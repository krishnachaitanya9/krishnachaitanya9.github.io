---
title: Latest Deep Learning Papers
date: 2020-12-15 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (206 Articles)</h1>
<h2>Robust One Shot Audio to Video Generation. (arXiv:2012.07842v1 [cs.CV])</h2>
<h3>Neeraj Kumar, Srishti Goel, Ankur Narang, Mujtaba Hasan</h3>
<p>Audio to Video generation is an interesting problem that has numerous
applications across industry verticals including film making, multi-media,
marketing, education and others. High-quality video generation with expressive
facial movements is a challenging problem that involves complex learning steps
for generative adversarial networks. Further, enabling one-shot learning for an
unseen single image increases the complexity of the problem while
simultaneously making it more applicable to practical scenarios. In the paper,
we propose a novel approach OneShotA2V to synthesize a talking person video of
arbitrary length using as input: an audio signal and a single unseen image of a
person. OneShotA2V leverages curriculum learning to learn movements of
expressive facial components and hence generates a high-quality talking-head
video of the given person. Further, it feeds the features generated from the
audio input directly into a generative adversarial network and it adapts to any
given unseen selfie by applying fewshot learning with only a few output
updation epochs. OneShotA2V leverages spatially adaptive normalization based
multi-level generator and multiple multi-level discriminators based
architecture. The input audio clip is not restricted to any specific language,
which gives the method multilingual applicability. Experimental evaluation
demonstrates superior performance of OneShotA2V as compared to Realistic
Speech-Driven Facial Animation with GANs(RSDGAN) [43], Speech2Vid [8], and
other approaches, on multiple quantitative metrics including: SSIM (structural
similarity index), PSNR (peak signal to noise ratio) and CPBD (image
sharpness). Further, qualitative evaluation and Online Turing tests demonstrate
the efficacy of our approach.
</p>
<a href="http://arxiv.org/abs/2012.07842" target="_blank">arXiv:2012.07842</a> [<a href="http://arxiv.org/pdf/2012.07842" target="_blank">pdf</a>]

<h2>Perceptron Theory for Predicting the Accuracy of Neural Networks. (arXiv:2012.07881v1 [cs.LG])</h2>
<h3>Denis Kleyko, Antonello Rosato, E. Paxon Frady, Massimo Panella, Friedrich T. Sommer</h3>
<p>Many neural network models have been successful at classification problems,
but their operation is still treated as a black box. Here, we developed a
theory for one-layer perceptrons that can predict performance on classification
tasks. This theory is a generalization of an existing theory for predicting the
performance of Echo State Networks and connectionist models for symbolic
reasoning known as Vector Symbolic Architectures. In this paper, we first show
that the proposed perceptron theory can predict the performance of Echo State
Networks, which could not be described by the previous theory. Second, we apply
our perceptron theory to the last layers of shallow randomly connected and deep
multi-layer networks. The full theory is based on Gaussian statistics, but it
is analytically intractable. We explore numerical methods to predict network
performance for problems with a small number of classes. For problems with a
large number of classes, we investigate stochastic sampling methods and a
tractable approximation to the full theory. The quality of predictions is
assessed in three experimental settings, using reservoir computing networks on
a memorization task, shallow randomly connected networks on a collection of
classification datasets, and deep convolutional networks with the ImageNet
dataset. This study offers a simple, bipartite approach to understand deep
neural networks: the input is encoded by the last-but-one layers into a
high-dimensional representation. This representation is mapped through the
weights of the last layer into the postsynaptic sums of the output neurons.
Specifically, the proposed perceptron theory uses the mean vector and
covariance matrix of the postsynaptic sums to compute classification accuracies
for the different classes. The first two moments of the distribution of the
postsynaptic sums can predict the overall network performance quite accurately.
</p>
<a href="http://arxiv.org/abs/2012.07881" target="_blank">arXiv:2012.07881</a> [<a href="http://arxiv.org/pdf/2012.07881" target="_blank">pdf</a>]

<h2>Bayesian Optimization -- Multi-Armed Bandit Problem. (arXiv:2012.07885v1 [cs.LG])</h2>
<h3>Abhilash Nandy, Chandan Kumar, Deepak Mewada, Soumya Sharma</h3>
<p>In this report, we survey Bayesian Optimization methods focussed on the
Multi-Armed Bandit Problem. We take the help of the paper "Portfolio Allocation
for Bayesian Optimization". We report a small literature survey on the
acquisition functions and the types of portfolio strategies used in papers
discussing Bayesian Optimization. We also replicate the experiments and report
our findings and compare them to the results in the paper. Code link:
https://colab.research.google.com/drive/1GZ14klEDoe3dcBeZKo5l8qqrKf_GmBDn?usp=sharing#scrollTo=XgIBau3O45_V.
</p>
<a href="http://arxiv.org/abs/2012.07885" target="_blank">arXiv:2012.07885</a> [<a href="http://arxiv.org/pdf/2012.07885" target="_blank">pdf</a>]

<h2>Adaptive Verifiable Training Using Pairwise Class Similarity. (arXiv:2012.07887v1 [cs.LG])</h2>
<h3>Shiqi Wang, Kevin Eykholt, Taesung Lee, Jiyong Jang, Ian Molloy</h3>
<p>Verifiable training has shown success in creating neural networks that are
provably robust to a given amount of noise. However, despite only enforcing a
single robustness criterion, its performance scales poorly with dataset
complexity. On CIFAR10, a non-robust LeNet model has a 21.63% error rate, while
a model created using verifiable training and a L-infinity robustness criterion
of 8/255, has an error rate of 57.10%. Upon examination, we find that when
labeling visually similar classes, the model's error rate is as high as 61.65%.
We attribute the loss in performance to inter-class similarity. Similar classes
(i.e., close in the feature space) increase the difficulty of learning a robust
model. While it's desirable to train a robust model for a large robustness
region, pairwise class similarities limit the potential gains. Also,
consideration must be made regarding the relative cost of mistaking similar
classes. In security or safety critical tasks, similar classes are likely to
belong to the same group, and thus are equally sensitive.

In this work, we propose a new approach that utilizes inter-class similarity
to improve the performance of verifiable training and create robust models with
respect to multiple adversarial criteria. First, we use agglomerate clustering
to group similar classes and assign robustness criteria based on the similarity
between clusters. Next, we propose two methods to apply our approach: (1)
Inter-Group Robustness Prioritization, which uses a custom loss term to create
a single model with multiple robustness guarantees and (2) neural decision
trees, which trains multiple sub-classifiers with different robustness
guarantees and combines them in a decision tree architecture. On Fashion-MNIST
and CIFAR10, our approach improves clean performance by 9.63% and 30.89%
respectively. On CIFAR100, our approach improves clean performance by 26.32%.
</p>
<a href="http://arxiv.org/abs/2012.07887" target="_blank">arXiv:2012.07887</a> [<a href="http://arxiv.org/pdf/2012.07887" target="_blank">pdf</a>]

<h2>Learning Collision-Free Space Detection from Stereo Images: Homography Matrix Brings Better Data Augmentation. (arXiv:2012.07890v1 [cs.RO])</h2>
<h3>Rui Fan, Hengli Wang, Peide Cai, Jin Wu, Mohammud Junaid Bocus, Lei Qiao, Ming Liu</h3>
<p>Collision-free space detection is a critical component of autonomous vehicle
perception. The state-of-the-art algorithms are typically based on supervised
learning. The performance of such approaches is always dependent on the quality
and amount of labeled training data. Additionally, it remains an open challenge
to train deep convolutional neural networks (DCNNs) using only a small quantity
of training samples. Therefore, this paper mainly explores an effective
training data augmentation approach that can be employed to improve the overall
DCNN performance, when additional images captured from different views are
available. Due to the fact that the pixels of the collision-free space
(generally regarded as a planar surface) between two images captured from
different views can be associated by a homography matrix, the scenario of the
target image can be transformed into the reference view. This provides a simple
but effective way of generating training data from additional multi-view
images. Extensive experimental results, conducted with six state-of-the-art
semantic segmentation DCNNs on three datasets, demonstrate the effectiveness of
our proposed training data augmentation algorithm for enhancing collision-free
space detection performance. When validated on the KITTI road benchmark, our
approach provides the best results for stereo vision-based collision-free space
detection.
</p>
<a href="http://arxiv.org/abs/2012.07890" target="_blank">arXiv:2012.07890</a> [<a href="http://arxiv.org/pdf/2012.07890" target="_blank">pdf</a>]

<h2>Distributed Sensor Networks Deployed Using Soft Growing Robots. (arXiv:2012.07899v1 [cs.RO])</h2>
<h3>Alexander M. Gruebele, Andrew C. Zerbe, Margaret M. Coad, Allison M. Okamura, Mark R. Cutkosky</h3>
<p>Due to their ability to move without sliding relative to their environment,
soft growing robots are attractive for deploying distributed sensor networks in
confined spaces. Sensing of the state of such robots would also add to their
capabilities as human-safe, adaptable manipulators. However, incorporation of
distributed sensors onto soft growing robots is challenging because it requires
an interface between stiff and soft materials, and the sensor network needs to
undergo significant strain. In this work, we present a method for adding
sensors to soft growing robots that uses flexible printed circuit boards with
self-contained units of microcontrollers and sensors encased in a laminate
armor that protects them from unsafe curvatures. We demonstrate the ability of
this system to relay directional temperature and humidity information in
hard-to-access spaces. We also demonstrate and characterize a method for
sensing the growing robot shape using inertial measurement units deployed along
its length, and develop a mathematical model to predict its accuracy. This work
advances the capabilities of soft growing robots, as well as the field of soft
robot sensing.
</p>
<a href="http://arxiv.org/abs/2012.07899" target="_blank">arXiv:2012.07899</a> [<a href="http://arxiv.org/pdf/2012.07899" target="_blank">pdf</a>]

<h2>Learning to Stop: Dynamic Simulation Monte-Carlo Tree Search. (arXiv:2012.07910v1 [cs.AI])</h2>
<h3>Li-Cheng Lan, Meng-Yu Tsai, Ti-Rong Wu, I-Chen Wu, Cho-Jui Hsieh</h3>
<p>Monte Carlo tree search (MCTS) has achieved state-of-the-art results in many
domains such as Go and Atari games when combining with deep neural networks
(DNNs). When more simulations are executed, MCTS can achieve higher performance
but also requires enormous amounts of CPU and GPU resources. However, not all
states require a long searching time to identify the best action that the agent
can find. For example, in 19x19 Go and NoGo, we found that for more than half
of the states, the best action predicted by DNN remains unchanged even after
searching 2 minutes. This implies that a significant amount of resources can be
saved if we are able to stop the searching earlier when we are confident with
the current searching result. In this paper, we propose to achieve this goal by
predicting the uncertainty of the current searching status and use the result
to decide whether we should stop searching. With our algorithm, called Dynamic
Simulation MCTS (DS-MCTS), we can speed up a NoGo agent trained by AlphaZero
2.5 times faster while maintaining a similar winning rate. Also, under the same
average simulation count, our method can achieve a 61% winning rate against the
original program.
</p>
<a href="http://arxiv.org/abs/2012.07910" target="_blank">arXiv:2012.07910</a> [<a href="http://arxiv.org/pdf/2012.07910" target="_blank">pdf</a>]

<h2>Reactive Temporal Logic Planning for Multiple Robots in Unknown Occupancy Grid Maps. (arXiv:2012.07912v1 [cs.RO])</h2>
<h3>Yiannis Kantaros, Matthew Malencia, George J. Pappas</h3>
<p>This paper proposes a new reactive temporal logic planning algorithm for
multiple robots that operate in environments with unknown geometry modeled
using occupancy grid maps. The robots are equipped with individual sensors that
allow them to continuously learn a grid map of the unknown environment using
existing Simultaneous Localization and Mapping (SLAM) methods. The goal of the
robots is to accomplish complex collaborative tasks, captured by global Linear
Temporal Logic (LTL) formulas. The majority of existing LTL planning approaches
rely on discrete abstractions of the robot dynamics operating in known
environments and, as a result, they cannot be applied to the more realistic
scenarios where the environment is initially unknown. In this paper, we address
this novel challenge by proposing the first reactive, abstraction-free, and
distributed LTL planning algorithm that can be applied for complex mission
planning of multiple robots operating in unknown environments. The proposed
algorithm is reactive, i.e., planning is adapting to the updated environmental
map and abstraction-free as it does not rely on designing abstractions of the
robot dynamics. Also, our algorithm is distributed in the sense that the global
LTL task is decomposed into single-agent reachability problems constructed
online based on the continuously learned map. The proposed algorithm is
complete under mild assumptions on the structure of the environment and the
sensor models. We provide extensive numerical simulations and hardware
experiments that illustrate the theoretical analysis and show that the proposed
algorithm can address complex planning tasks for large-scale multi-robot
systems in unknown environments.
</p>
<a href="http://arxiv.org/abs/2012.07912" target="_blank">arXiv:2012.07912</a> [<a href="http://arxiv.org/pdf/2012.07912" target="_blank">pdf</a>]

<h2>Quantizing data for distributed learning. (arXiv:2012.07913v1 [cs.LG])</h2>
<h3>Osama A. Hanna, Yahya H. Ezzeldin, Christina Fragouli, Suhas Diggavi</h3>
<p>We consider machine learning applications that train a model by leveraging
data distributed over a network, where communication constraints can create a
performance bottleneck. A number of recent approaches are proposing to overcome
this bottleneck through compression of gradient updates. However, as models
become larger, so does the size of the gradient updates. In this paper, we
propose an alternate approach, that quantizes data instead of gradients, and
can support learning over applications where the size of gradient updates is
prohibitive. Our approach combines aspects of: (1) sample selection; (2)
dataset quantization; and (3) gradient compensation. We analyze the convergence
of the proposed approach for smooth convex and non-convex objective functions
and show that we can achieve order optimal convergence rates with communication
that mostly depends on the data rather than the model (gradient) dimension. We
use our proposed algorithm to train ResNet models on the CIFAR-10 and ImageNet
datasets, and show that we can achieve an order of magnitude savings over
gradient compression methods.
</p>
<a href="http://arxiv.org/abs/2012.07913" target="_blank">arXiv:2012.07913</a> [<a href="http://arxiv.org/pdf/2012.07913" target="_blank">pdf</a>]

<h2>Improving model calibration with accuracy versus uncertainty optimization. (arXiv:2012.07923v1 [cs.LG])</h2>
<h3>Ranganath Krishnan, Omesh Tickoo</h3>
<p>Obtaining reliable and accurate quantification of uncertainty estimates from
deep neural networks is important in safety-critical applications. A
well-calibrated model should be accurate when it is certain about its
prediction and indicate high uncertainty when it is likely to be inaccurate.
Uncertainty calibration is a challenging problem as there is no ground truth
available for uncertainty estimates. We propose an optimization method that
leverages the relationship between accuracy and uncertainty as an anchor for
uncertainty calibration. We introduce a differentiable accuracy versus
uncertainty calibration (AvUC) loss function that allows a model to learn to
provide well-calibrated uncertainties, in addition to improved accuracy. We
also demonstrate the same methodology can be extended to post-hoc uncertainty
calibration on pretrained models. We illustrate our approach with mean-field
stochastic variational inference and compare with state-of-the-art methods.
Extensive experiments demonstrate our approach yields better model calibration
than existing methods on large-scale image classification tasks under
distributional shift.
</p>
<a href="http://arxiv.org/abs/2012.07923" target="_blank">arXiv:2012.07923</a> [<a href="http://arxiv.org/pdf/2012.07923" target="_blank">pdf</a>]

<h2>Automatic Vertebra Localization and Identification in CT by Spine Rectification and Anatomically-constrained Optimization. (arXiv:2012.07947v1 [cs.CV])</h2>
<h3>Fakai Wang, Kang Zheng, Le Lu, Jing Xiao, Min Wu, Shun Miao</h3>
<p>Accurate vertebra localization and identification are required in many
clinical applications of spine disorder diagnosis and surgery planning.
However, significant challenges are posed in this task by highly varying
pathologies (such as vertebral compression fracture, scoliosis, and vertebral
fixation) and imaging conditions (such as limited field of view and metal
streak artifacts). This paper proposes a robust and accurate method that
effectively exploits the anatomical knowledge of the spine to facilitate
vertebra localization and identification. A key point localization model is
trained to produce activation maps of vertebra centers. They are then
re-sampled along the spine centerline to produce spine-rectified activation
maps, which are further aggregated into 1-D activation signals. Following this,
an anatomically-constrained optimization module is introduced to jointly search
for the optimal vertebra centers under a soft constraint that regulates the
distance between vertebrae and a hard constraint on the consecutive vertebra
indices. When being evaluated on a major public benchmark of 302 highly
pathological CT images, the proposed method reports the state of the art
identification (id.) rate of 97.4%, and outperforms the best competing method
of 94.7% id. rate by reducing the relative id. error rate by half.
</p>
<a href="http://arxiv.org/abs/2012.07947" target="_blank">arXiv:2012.07947</a> [<a href="http://arxiv.org/pdf/2012.07947" target="_blank">pdf</a>]

<h2>SAT-MARL: Specification Aware Training in Multi-Agent Reinforcement Learning. (arXiv:2012.07949v1 [cs.LG])</h2>
<h3>Fabian Ritz, Thomy Phan, Robert M&#xfc;ller, Thomas Gabor, Andreas Sedlmeier, Marc Zeller, Jan Wieghardt, Reiner Schmid, Horst Sauer, Cornel Klein, Claudia Linnhoff-Popien</h3>
<p>A characteristic of reinforcement learning is the ability to develop
unforeseen strategies when solving problems. While such strategies sometimes
yield superior performance, they may also result in undesired or even dangerous
behavior. In industrial scenarios, a system's behavior also needs to be
predictable and lie within defined ranges. To enable the agents to learn (how)
to align with a given specification, this paper proposes to explicitly transfer
functional and non-functional requirements into shaped rewards. Experiments are
carried out on the smart factory, a multi-agent environment modeling an
industrial lot-size-one production facility, with up to eight agents and
different multi-agent reinforcement learning algorithms. Results indicate that
compliance with functional and non-functional constraints can be achieved by
the proposed approach.
</p>
<a href="http://arxiv.org/abs/2012.07949" target="_blank">arXiv:2012.07949</a> [<a href="http://arxiv.org/pdf/2012.07949" target="_blank">pdf</a>]

<h2>Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v1 [cs.LG])</h2>
<h3>Michalis Lazarou, Yannis Avrithis, Tania Stathaki</h3>
<p>Few-shot learning amounts to learning representations and acquiring knowledge
such that novel tasks may be solved with both supervision and data being
limited. Improved performance is possible by transductive inference, where the
entire test set is available concurrently, and semi-supervised learning, where
more unlabeled data is available. These problems are closely related because
there is little or no adaptation of the representation in novel tasks.

Focusing on these two settings, we introduce a new algorithm that leverages
the manifold structure of the labeled and unlabeled data distribution to
predict pseudo-labels, while balancing over classes and using the loss value
distribution of a limited-capacity classifier to select the cleanest labels,
iterately improving the quality of pseudo-labels. Our solution sets new state
of the art on four benchmark datasets, namely \emph{mini}ImageNet,
\emph{tiered}ImageNet, CUB and CIFAR-FS, while being robust over feature space
pre-processing and the quantity of available data.
</p>
<a href="http://arxiv.org/abs/2012.07962" target="_blank">arXiv:2012.07962</a> [<a href="http://arxiv.org/pdf/2012.07962" target="_blank">pdf</a>]

<h2>Odd-One-Out Representation Learning. (arXiv:2012.07966v1 [cs.LG])</h2>
<h3>Salman Mohammadi, Anders Kirk Uhrenholt, Bj&#xf8;rn Sand Jensen</h3>
<p>The effective application of representation learning to real-world problems
requires both techniques for learning useful representations, and also robust
ways to evaluate properties of representations. Recent work in disentangled
representation learning has shown that unsupervised representation learning
approaches rely on fully supervised disentanglement metrics, which assume
access to labels for ground-truth factors of variation. In many real-world
cases ground-truth factors are expensive to collect, or difficult to model,
such as for perception. Here we empirically show that a weakly-supervised
downstream task based on odd-one-out observations is suitable for model
selection by observing high correlation on a difficult downstream abstract
visual reasoning task. We also show that a bespoke metric-learning VAE model
which performs highly on this task also out-performs other standard
unsupervised and a weakly-supervised disentanglement model across several
metrics.
</p>
<a href="http://arxiv.org/abs/2012.07966" target="_blank">arXiv:2012.07966</a> [<a href="http://arxiv.org/pdf/2012.07966" target="_blank">pdf</a>]

<h2>FasteNet: A Fast Railway Fastener Detector. (arXiv:2012.07968v1 [cs.CV])</h2>
<h3>Jun Jet Tai, Mauro S. Innocente, Owais Mehmood</h3>
<p>In this work, a novel high-speed railway fastener detector is introduced.
This fully convolutional network, dubbed FasteNet, foregoes the notion of
bounding boxes and performs detection directly on a predicted saliency map.
Fastenet uses transposed convolutions and skip connections, the effective
receptive field of the network is 1.5$\times$ larger than the average size of a
fastener, enabling the network to make predictions with high confidence,
without sacrificing output resolution. In addition, due to the saliency map
approach, the network is able to vote for the presence of a fastener up to 30
times per fastener, boosting prediction accuracy. Fastenet is capable of
running at 110 FPS on an Nvidia GTX 1080, while taking in inputs of
1600$\times$512 with an average of 14 fasteners per image. Our source is open
here: https://github.com/jjshoots/DL\_FasteNet.git
</p>
<a href="http://arxiv.org/abs/2012.07968" target="_blank">arXiv:2012.07968</a> [<a href="http://arxiv.org/pdf/2012.07968" target="_blank">pdf</a>]

<h2>A case for new neural network smoothness constraints. (arXiv:2012.07969v1 [stat.ML])</h2>
<h3>Mihaela Rosca, Theophane Weber, Arthur Gretton, Shakir Mohamed</h3>
<p>How sensitive should machine learning models be to input changes? We tackle
the question of model smoothness and show that it is a useful inductive bias
which aids generalization, adversarial robustness, generative modeling and
reinforcement learning. We explore current methods of imposing smoothness
constraints and observe they lack the flexibility to adapt to new tasks, they
don't account for data modalities, they interact with losses, architectures and
optimization in ways not yet fully understood. We conclude that new advances in
the field are hinging on finding ways to incorporate data, tasks and learning
into our definitions of smoothness.
</p>
<a href="http://arxiv.org/abs/2012.07969" target="_blank">arXiv:2012.07969</a> [<a href="http://arxiv.org/pdf/2012.07969" target="_blank">pdf</a>]

<h2>A review of on-device fully neural end-to-end automatic speech recognition algorithms. (arXiv:2012.07974v1 [cs.LG])</h2>
<h3>Chanwoo Kim, Dhananjaya Gowda, Dongsoo Lee, Jiyeon Kim, Ankur Kumar, Sungsoo Kim, Abhinav Garg, Changwoo Han</h3>
<p>In this paper, we review various end-to-end automatic speech recognition
algorithms and their optimization techniques for on-device applications.
Conventional speech recognition systems comprise a large number of discrete
components such as an acoustic model, a language model, a pronunciation model,
a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted
Finite State Transducer (WFST), and so on. To obtain sufficiently high speech
recognition accuracy with such conventional speech recognition systems, a very
large language model (up to 100 GB) is usually needed. Hence, the corresponding
WFST size becomes enormous, which prohibits their on-device implementation.
Recently, fully neural network end-to-end speech recognition algorithms have
been proposed. Examples include speech recognition systems based on
Connectionist Temporal Classification (CTC), Recurrent Neural Network
Transducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic
Chunk-wise Attention (MoChA), transformer-based speech recognition systems, and
so on. These fully neural network-based systems require much smaller memory
footprints compared to conventional algorithms, therefore their on-device
implementation has become feasible. In this paper, we review such end-to-end
speech recognition models. We extensively discuss their structures,
performance, and advantages compared to conventional algorithms.
</p>
<a href="http://arxiv.org/abs/2012.07974" target="_blank">arXiv:2012.07974</a> [<a href="http://arxiv.org/pdf/2012.07974" target="_blank">pdf</a>]

<h2>A Framework for Efficient Robotic Manipulation. (arXiv:2012.07975v1 [cs.RO])</h2>
<h3>Albert Zhan, Philip Zhao, Lerrel Pinto, Pieter Abbeel, Michael Laskin</h3>
<p>Data-efficient learning of manipulation policies from visual observations is
an outstanding challenge for real-robot learning. While deep reinforcement
learning (RL) algorithms have shown success learning policies from visual
observations, they still require an impractical number of real-world data
samples to learn effective policies. However, recent advances in unsupervised
representation learning and data augmentation significantly improved the sample
efficiency of training RL policies on common simulated benchmarks. Building on
these advances, we present a Framework for Efficient Robotic Manipulation
(FERM) that utilizes data augmentation and unsupervised learning to achieve
extremely sample-efficient training of robotic manipulation policies with
sparse rewards. We show that, given only 10 demonstrations, a single robotic
arm can learn sparse-reward manipulation policies from pixels, such as
reaching, picking, moving, pulling a large object, flipping a switch, and
opening a drawer in just 15-50 minutes of real-world training time. We include
videos, code, and additional information on the project website --
https://sites.google.com/view/efficient-robotic-manipulation.
</p>
<a href="http://arxiv.org/abs/2012.07975" target="_blank">arXiv:2012.07975</a> [<a href="http://arxiv.org/pdf/2012.07975" target="_blank">pdf</a>]

<h2>NeurIPS 2020 Competition: Predicting Generalization in Deep Learning. (arXiv:2012.07976v1 [cs.LG])</h2>
<h3>Yiding Jiang (1), Pierre Foret (1), Scott Yak (1), Daniel M. Roy (2), Hossein Mobahi (1), Gintare Karolina Dziugaite (3), Samy Bengio (1), Suriya Gunasekar (4), Isabelle Guyon (5), Behnam Neyshabur (1) ((1) Google Research, (2) University of Toronto, (3) Element AI, (4) Microsoft Research, (5) University Paris-Saclay and ChaLearn)</h3>
<p>Understanding generalization in deep learning is arguably one of the most
important questions in deep learning. Deep learning has been successfully
adopted to a large number of problems ranging from pattern recognition to
complex decision making, but many recent researchers have raised many concerns
about deep learning, among which the most important is generalization. Despite
numerous attempts, conventional statistical learning approaches have yet been
able to provide a satisfactory explanation on why deep learning works. A recent
line of works aims to address the problem by trying to predict the
generalization performance through complexity measures. In this competition, we
invite the community to propose complexity measures that can accurately predict
generalization of models. A robust and general complexity measure would
potentially lead to a better understanding of deep learning's underlying
mechanism and behavior of deep models on unseen data, or shed light on better
generalization bounds. All these outcomes will be important for making deep
learning more robust and reliable.
</p>
<a href="http://arxiv.org/abs/2012.07976" target="_blank">arXiv:2012.07976</a> [<a href="http://arxiv.org/pdf/2012.07976" target="_blank">pdf</a>]

<h2>Feature Selection for Learning to Predict Outcomes of Compute Cluster Jobs with Application to Decision Support. (arXiv:2012.07982v1 [cs.LG])</h2>
<h3>Adedolapo Okanlawon, Huichen Yang, Avishek Bose, William Hsu, Dan Andresen, Mohammed Tanash</h3>
<p>We present a machine learning framework and a new test bed for data mining
from the Slurm Workload Manager for high-performance computing (HPC) clusters.
The focus was to find a method for selecting features to support decisions:
helping users decide whether to resubmit failed jobs with boosted CPU and
memory allocations or migrate them to a computing cloud. This task was cast as
both supervised classification and regression learning, specifically,
sequential problem solving suitable for reinforcement learning. Selecting
relevant features can improve training accuracy, reduce training time, and
produce a more comprehensible model, with an intelligent system that can
explain predictions and inferences. We present a supervised learning model
trained on a Simple Linux Utility for Resource Management (Slurm) data set of
HPC jobs using three different techniques for selecting features: linear
regression, lasso, and ridge regression. Our data set represented both HPC jobs
that failed and those that succeeded, so our model was reliable, less likely to
overfit, and generalizable. Our model achieved an R^2 of 95\% with 99\%
accuracy. We identified five predictors for both CPU and memory properties.
</p>
<a href="http://arxiv.org/abs/2012.07982" target="_blank">arXiv:2012.07982</a> [<a href="http://arxiv.org/pdf/2012.07982" target="_blank">pdf</a>]

<h2>On Continuous Local BDD-Based Search for Hybrid SAT Solving. (arXiv:2012.07983v1 [cs.AI])</h2>
<h3>Anastasios Kyrillidis, Moshe Y. Vardi, Zhiwei Zhang</h3>
<p>We explore the potential of continuous local search (CLS) in SAT solving by
proposing a novel approach for finding a solution of a hybrid system of Boolean
constraints. The algorithm is based on CLS combined with belief propagation on
binary decision diagrams (BDDs). Our framework accepts all Boolean constraints
that admit compact BDDs, including symmetric Boolean constraints and
small-coefficient pseudo-Boolean constraints as interesting families. We
propose a novel algorithm for efficiently computing the gradient needed by CLS.
We study the capabilities and limitations of our versatile CLS solver, GradSAT,
by applying it on many benchmark instances. The experimental results indicate
that GradSAT can be a useful addition to the portfolio of existing SAT and
MaxSAT solvers for solving Boolean satisfiability and optimization problems.
</p>
<a href="http://arxiv.org/abs/2012.07983" target="_blank">arXiv:2012.07983</a> [<a href="http://arxiv.org/pdf/2012.07983" target="_blank">pdf</a>]

<h2>GAN Ensemble for Anomaly Detection. (arXiv:2012.07988v1 [cs.LG])</h2>
<h3>Xu Han, Xiaohui Chen, Li-Ping Liu</h3>
<p>When formulated as an unsupervised learning problem, anomaly detection often
requires a model to learn the distribution of normal data. Previous works apply
Generative Adversarial Networks (GANs) to anomaly detection tasks and show good
performances from these models. Motivated by the observation that GAN ensembles
often outperform single GANs in generation tasks, we propose to construct GAN
ensembles for anomaly detection. In the proposed method, a group of generators
and a group of discriminators are trained together, so every generator gets
feedback from multiple discriminators, and vice versa. Compared to a single
GAN, a GAN ensemble can better model the distribution of normal data and thus
better detect anomalies. Our theoretical analysis of GANs and GAN ensembles
explains the role of a GAN discriminator in anomaly detection. In the empirical
study, we evaluate ensembles constructed from four types of base models, and
the results show that these ensembles clearly outperform single models in a
series of tasks of anomaly detection.
</p>
<a href="http://arxiv.org/abs/2012.07988" target="_blank">arXiv:2012.07988</a> [<a href="http://arxiv.org/pdf/2012.07988" target="_blank">pdf</a>]

<h2>FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction. (arXiv:2012.07999v1 [cs.CV])</h2>
<h3>ShahRukh Athar, Albert Pumarola, Francesc Moreno-Noguer, Dimitris Samaras</h3>
<p>Facial Expressions induce a variety of high-level details on the 3D face
geometry. For example, a smile causes the wrinkling of cheeks or the formation
of dimples, while being angry often causes wrinkling of the forehead. Morphable
Models (3DMMs) of the human face fail to capture such fine details in their
PCA-based representations and consequently cannot generate such details when
used to edit expressions. In this work, we introduce FaceDet3D, a
first-of-its-kind method that generates - from a single image - geometric
facial details that are consistent with any desired target expression. The
facial details are represented as a vertex displacement map and used then by a
Neural Renderer to photo-realistically render novel images of any single image
in any desired expression and view. The Project website is:
this http URL
</p>
<a href="http://arxiv.org/abs/2012.07999" target="_blank">arXiv:2012.07999</a> [<a href="http://arxiv.org/pdf/2012.07999" target="_blank">pdf</a>]

<h2>Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL. (arXiv:2012.08005v1 [cs.LG])</h2>
<h3>Andrea Zanette</h3>
<p>Several practical applications of reinforcement learning involve an agent
learning from past data without the possibility of further exploration. Often
these applications require us to 1) identify a near optimal policy or to 2)
estimate the value of a target policy. For both tasks we derive exponential
information-theoretic lower bounds in discounted infinite horizon MDPs with a
linear function representation for the action value function even if 1)
realizability holds, 2) the batch algorithm observes the exact reward and
transition functions, and 3) the batch algorithm is given the best a priori
data distribution for the problem class. Furthermore, if the dataset does not
come from policy rollouts then the lower bounds hold even if all policies admit
a linear representation.

If the objective is to find a near-optimal policy, we discover that these
hard instances are easily solved by an online algorithm, showing that there
exist RL problems where batch RL is exponentially harder than online RL even
under the most favorable batch data distribution. In other words, online
exploration is critical to enable sample efficient RL with function
approximation. A second corollary is the exponential separation between finite
and infinite horizon batch problems under our assumptions. On a technical
level, this work helps formalize the issue known as deadly triad and explains
that the bootstrapping problem is potentially more severe than the
extrapolation issue for RL because unlike the latter, bootstrapping cannot be
mitigated by adding more samples.
</p>
<a href="http://arxiv.org/abs/2012.08005" target="_blank">arXiv:2012.08005</a> [<a href="http://arxiv.org/pdf/2012.08005" target="_blank">pdf</a>]

<h2>Bandit-based Communication-Efficient Client Selection Strategies for Federated Learning. (arXiv:2012.08009v1 [cs.LG])</h2>
<h3>Yae Jee Cho, Samarth Gupta, Gauri Joshi, Osman Ya&#x11f;an</h3>
<p>Due to communication constraints and intermittent client availability in
federated learning, only a subset of clients can participate in each training
round. While most prior works assume uniform and unbiased client selection,
recent work on biased client selection has shown that selecting clients with
higher local losses can improve error convergence speed. However, previously
proposed biased selection strategies either require additional communication
cost for evaluating the exact local loss or utilize stale local loss, which can
even make the model diverge. In this paper, we present a bandit-based
communication-efficient client selection strategy UCB-CS that achieves faster
convergence with lower communication overhead. We also demonstrate how client
selection can be used to improve fairness.
</p>
<a href="http://arxiv.org/abs/2012.08009" target="_blank">arXiv:2012.08009</a> [<a href="http://arxiv.org/pdf/2012.08009" target="_blank">pdf</a>]

<h2>DeepGamble: Towards unlocking real-time player intelligence using multi-layer instance segmentation and attribute detection. (arXiv:2012.08011v1 [cs.CV])</h2>
<h3>Danish Syed, Naman Gandhi, Arushi Arora, Nilesh Kadam</h3>
<p>Annually the gaming industry spends approximately $15 billion in marketing
reinvestment. However, this amount is spent without any consideration for the
skill and luck of the player. For a casino, an unskilled player could fetch ~4
times more revenue than a skilled player. This paper describes a video
recognition system that is based on an extension of the Mask R-CNN model. Our
system digitizes the game of blackjack by detecting cards and player bets in
real-time and processes decisions they took in order to create accurate player
personas. Our proposed supervised learning approach consists of a specialized
three-stage pipeline that takes images from two viewpoints of the casino table
and does instance segmentation to generate masks on proposed regions of
interest. These predicted masks along with derivative features are used to
classify image attributes that are passed onto the next stage to assimilate the
gameplay understanding. Our end-to-end model yields an accuracy of ~95% for the
main bet detection and ~97% for card detection in a controlled environment
trained using transfer learning approach with 900 training examples. Our
approach is generalizable and scalable and shows promising results in varied
gaming scenarios and test data. Such granular level gathered data, helped in
understanding player's deviation from optimum strategy and thereby separate the
skill of the player from the luck of the game. Our system also assesses the
likelihood of card counting by correlating the player's betting pattern to the
deck's scaled count. Such a system lets casinos flag fraudulent activity and
calculate expected personalized profitability for each player and tailor their
marketing reinvestment decisions.
</p>
<a href="http://arxiv.org/abs/2012.08011" target="_blank">arXiv:2012.08011</a> [<a href="http://arxiv.org/pdf/2012.08011" target="_blank">pdf</a>]

<h2>Understanding graph embedding methods and their applications. (arXiv:2012.08019v1 [cs.LG])</h2>
<h3>Mengjia Xu</h3>
<p>Graph analytics can lead to better quantitative understanding and control of
complex networks, but traditional methods suffer from high computational cost
and excessive memory requirements associated with the high-dimensionality and
heterogeneous characteristics of industrial size networks. Graph embedding
techniques can be effective in converting high-dimensional sparse graphs into
low-dimensional, dense and continuous vector spaces, preserving maximally the
graph structure properties. Another type of emerging graph embedding employs
Gaussian distribution-based graph embedding with important uncertainty
estimation. The main goal of graph embedding methods is to pack every node's
properties into a vector with a smaller dimension, hence, node similarity in
the original complex irregular spaces can be easily quantified in the embedded
vector spaces using standard metrics. The generated nonlinear and highly
informative graph embeddings in the latent space can be conveniently used to
address different downstream graph analytics tasks (e.g., node classification,
link prediction, community detection, visualization, etc.). In this Review, we
present some fundamental concepts in graph analytics and graph embedding
methods, focusing in particular on random walk-based and neural network-based
methods. We also discuss the emerging deep learning-based dynamic graph
embedding methods. We highlight the distinct advantages of graph embedding
methods in four diverse applications, and present implementation details and
references to open-source software as well as available databases in the
Appendix for the interested readers to start their exploration into graph
analytics.
</p>
<a href="http://arxiv.org/abs/2012.08019" target="_blank">arXiv:2012.08019</a> [<a href="http://arxiv.org/pdf/2012.08019" target="_blank">pdf</a>]

<h2>Classification of Smoking and Calling using Deep Learning. (arXiv:2012.08026v1 [cs.CV])</h2>
<h3>Miaowei Wang, Alexander William Mohacey, Hongyu Wang, James Apfel</h3>
<p>Since 2014, very deep convolutional neural networks have been proposed and
become the must-have weapon for champions in all kinds of competition. In this
report, a pipeline is introduced to perform the classification of smoking and
calling by modifying the pretrained inception V3. Brightness enhancing based on
deep learning is implemented to improve the classification of this
classification task along with other useful training tricks. Based on the
quality and quantity results, it can be concluded that this pipeline with small
biased samples is practical and useful with high accuracy.
</p>
<a href="http://arxiv.org/abs/2012.08026" target="_blank">arXiv:2012.08026</a> [<a href="http://arxiv.org/pdf/2012.08026" target="_blank">pdf</a>]

<h2>General Policies, Serializations, and Planning Width. (arXiv:2012.08033v1 [cs.AI])</h2>
<h3>Blai Bonet, Hector Geffner</h3>
<p>It has been observed that in many of the benchmark planning domains, atomic
goals can be reached with a simple polynomial exploration procedure, called IW,
that runs in time exponential in the problem width. Such problems have indeed a
bounded width: a width that does not grow with the number of problem variables
and is often no greater than two. Yet, while the notion of width has become
part of the state-of-the-art planning algorithms like BFWS, there is still no
good explanation for why so many benchmark domains have bounded width. In this
work, we address this question by relating bounded width and serialized width
to ideas of generalized planning, where general policies aim to solve multiple
instances of a planning problem all at once. We show that bounded width is a
property of planning domains that admit optimal general policies in terms of
features that are explicitly or implicitly represented in the domain encoding.
The results are extended to much larger class of domains with bounded
serialized width where the general policies do not have to be optimal. The
study leads also to a new simple, meaningful, and expressive language for
specifying domain serializations in the form of policy sketches which can be
used for encoding domain control knowledge by hand or for learning it from
traces. The use of sketches and the meaning of the theoretical results are all
illustrated through a number of examples.
</p>
<a href="http://arxiv.org/abs/2012.08033" target="_blank">arXiv:2012.08033</a> [<a href="http://arxiv.org/pdf/2012.08033" target="_blank">pdf</a>]

<h2>Applications of multivariate quasi-random sampling with neural networks. (arXiv:2012.08036v1 [stat.ML])</h2>
<h3>Marius Hofert, Avinash Prasad, Mu Zhu</h3>
<p>Generative moment matching networks (GMMNs) are suggested for modeling the
cross-sectional dependence between stochastic processes. The stochastic
processes considered are geometric Brownian motions and ARMA-GARCH models.
Geometric Brownian motions lead to an application of pricing American basket
call options under dependence and ARMA-GARCH models lead to an application of
simulating predictive distributions. In both types of applications the benefit
of using GMMNs in comparison to parametric dependence models is highlighted and
the fact that GMMNs can produce dependent quasi-random samples with no
additional effort is exploited to obtain variance reduction.
</p>
<a href="http://arxiv.org/abs/2012.08036" target="_blank">arXiv:2012.08036</a> [<a href="http://arxiv.org/pdf/2012.08036" target="_blank">pdf</a>]

<h2>Proofs and additional experiments on Second order techniques for learning time-series with structural breaks. (arXiv:2012.08037v1 [cs.LG])</h2>
<h3>Takayuki Osogami</h3>
<p>We provide complete proofs of the lemmas about the properties of the
regularized loss function that is used in the second order techniques for
learning time-series with structural breaks in Osogami (2021). In addition, we
show experimental results that support the validity of the techniques.
</p>
<a href="http://arxiv.org/abs/2012.08037" target="_blank">arXiv:2012.08037</a> [<a href="http://arxiv.org/pdf/2012.08037" target="_blank">pdf</a>]

<h2>NUTA: Non-uniform Temporal Aggregation for Action Recognition. (arXiv:2012.08041v1 [cs.CV])</h2>
<h3>Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Hao Chen, Joseph Tighe</h3>
<p>In the world of action recognition research, one primary focus has been on
how to construct and train networks to model the spatial-temporal volume of an
input video. These methods typically uniformly sample a segment of an input
clip (along the temporal dimension). However, not all parts of a video are
equally important to determine the action in the clip. In this work, we focus
instead on learning where to extract features, so as to focus on the most
informative parts of the video. We propose a method called the non-uniform
temporal aggregation (NUTA), which aggregates features only from informative
temporal segments. We also introduce a synchronization method that allows our
NUTA features to be temporally aligned with traditional uniformly sampled video
features, so that both local and clip-level features can be combined. Our model
has achieved state-of-the-art performance on four widely used large-scale
action-recognition datasets (Kinetics400, Kinetics700, Something-something V2
and Charades). In addition, we have created a visualization to illustrate how
the proposed NUTA method selects only the most relevant parts of a video clip.
</p>
<a href="http://arxiv.org/abs/2012.08041" target="_blank">arXiv:2012.08041</a> [<a href="http://arxiv.org/pdf/2012.08041" target="_blank">pdf</a>]

<h2>Deep Bayesian Active Learning, A Brief Survey on Recent Advances. (arXiv:2012.08044v1 [cs.LG])</h2>
<h3>Salman Mohamadi, Hamidreza Amindavar</h3>
<p>Active learning frameworks offer efficient data annotation without remarkable
accuracy degradation. In other words, active learning starts training the model
with a small size of labeled data while exploring the space of unlabeled data
in order to select most informative samples to be labeled. Generally speaking,
representing the uncertainty is crucial in any active learning framework,
however, deep learning methods are not capable of either representing or
manipulating model uncertainty. On the other hand, from the real world
application perspective, uncertainty representation is getting more and more
attention in the machine learning community. Deep Bayesian active learning
frameworks and generally any Bayesian active learning settings, provide
practical consideration in the model which allows training with small data
while representing the model uncertainty for further efficient training. In
this paper, we briefly survey recent advances in Bayesian active learning and
in particular deep Bayesian active learning frameworks.
</p>
<a href="http://arxiv.org/abs/2012.08044" target="_blank">arXiv:2012.08044</a> [<a href="http://arxiv.org/pdf/2012.08044" target="_blank">pdf</a>]

<h2>Semantic-Guided Representation Enhancement for Self-supervised Monocular Trained Depth Estimation. (arXiv:2012.08048v1 [cs.CV])</h2>
<h3>Rui Li, Qing Mao, Pei Wang, Xiantuo He, Yu Zhu, Jinqiu Sun, Yanning Zhang</h3>
<p>Self-supervised depth estimation has shown its great effectiveness in
producing high quality depth maps given only image sequences as input. However,
its performance usually drops when estimating on border areas or objects with
thin structures due to the limited depth representation ability. In this paper,
we address this problem by proposing a semantic-guided depth representation
enhancement method, which promotes both local and global depth feature
representations by leveraging rich contextual information. In stead of a single
depth network as used in conventional paradigms, we propose an extra semantic
segmentation branch to offer extra contextual features for depth estimation.
Based on this framework, we enhance the local feature representation by
sampling and feeding the point-based features that locate on the semantic edges
to an individual Semantic-guided Edge Enhancement module (SEEM), which is
specifically designed for promoting depth estimation on the challenging
semantic borders. Then, we improve the global feature representation by
proposing a semantic-guided multi-level attention mechanism, which enhances the
semantic and depth features by exploring pixel-wise correlations in the
multi-level depth decoding scheme. Extensive experiments validate the distinct
superiority of our method in capturing highly accurate depth on the challenging
image areas such as semantic category borders and thin objects. Both
quantitative and qualitative experiments on KITTI show that our method
outperforms the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.08048" target="_blank">arXiv:2012.08048</a> [<a href="http://arxiv.org/pdf/2012.08048" target="_blank">pdf</a>]

<h2>Teach me to segment with mixed supervision: Confident students become masters. (arXiv:2012.08051v1 [cs.CV])</h2>
<h3>Jose Dolz, Christian Desrosiers, Ismail Ben Ayed</h3>
<p>Deep segmentation neural networks require large training datasets with
pixel-wise segmentations, which are expensive to obtain in practice. Mixed
supervision could mitigate this difficulty, with a small fraction of the data
containing complete pixel-wise annotations, while the rest being less
supervised, e.g., only a handful of pixels are labeled. In this work, we
propose a dual-branch architecture, where the upper branch (teacher) receives
strong annotations, while the bottom one (student) is driven by limited
supervision and guided by the upper branch. In conjunction with a standard
cross-entropy over the labeled pixels, our novel formulation integrates two
important terms: (i) a Shannon entropy loss defined over the less-supervised
images, which encourages confident student predictions at the bottom branch;
and (ii) a Kullback-Leibler (KL) divergence, which transfers the knowledge from
the predictions generated by the strongly supervised branch to the
less-supervised branch, and guides the entropy (student-confidence) term to
avoid trivial solutions. Very interestingly, we show that the synergy between
the entropy and KL divergence yields substantial improvements in performances.
Furthermore, we discuss an interesting link between Shannon-entropy
minimization and standard pseudo-mask generation and argue that the former
should be preferred over the latter for leveraging information from unlabeled
pixels. Through a series of quantitative and qualitative experiments, we show
the effectiveness of the proposed formulation in segmenting the left-ventricle
endocardium in MRI images. We demonstrate that our method significantly
outperforms other strategies to tackle semantic segmentation within a
mixed-supervision framework. More interestingly, and in line with recent
observations in classification, we show that the branch trained with reduced
supervision largely outperforms the teacher.
</p>
<a href="http://arxiv.org/abs/2012.08051" target="_blank">arXiv:2012.08051</a> [<a href="http://arxiv.org/pdf/2012.08051" target="_blank">pdf</a>]

<h2>Image Inpainting Guided by Coherence Priors of Semantics and Textures. (arXiv:2012.08054v1 [cs.CV])</h2>
<h3>Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, Shin&#x27;ichi Satoh</h3>
<p>Existing inpainting methods have achieved promising performance in recovering
defected images of specific scenes. However, filling holes involving multiple
semantic categories remains challenging due to the obscure semantic boundaries
and the mixture of different semantic textures. In this paper, we introduce
coherence priors between the semantics and textures which make it possible to
concentrate on completing separate textures in a semantic-wise manner.
Specifically, we adopt a multi-scale joint optimization framework to first
model the coherence priors and then accordingly interleavingly optimize image
inpainting and semantic segmentation in a coarse-to-fine manner. A
Semantic-Wise Attention Propagation (SWAP) module is devised to refine
completed image textures across scales by exploring non-local semantic
coherence, which effectively mitigates mix-up of textures. We also propose two
coherence losses to constrain the consistency between the semantics and the
inpainted image in terms of the overall structure and detailed textures.
Experimental results demonstrate the superiority of our proposed method for
challenging cases with complex holes.
</p>
<a href="http://arxiv.org/abs/2012.08054" target="_blank">arXiv:2012.08054</a> [<a href="http://arxiv.org/pdf/2012.08054" target="_blank">pdf</a>]

<h2>Fine-Grained Vehicle Perception via 3D Part-Guided Visual Data Augmentation. (arXiv:2012.08055v1 [cs.CV])</h2>
<h3>Feixiang Lu, Zongdai Liu, Hui Miao, Peng Wang, Liangjun Zhang, Ruigang Yang, Dinesh Manocha, Bin Zhou</h3>
<p>Holistically understanding an object and its 3D movable parts through visual
perception models is essential for enabling an autonomous agent to interact
with the world. For autonomous driving, the dynamics and states of vehicle
parts such as doors, the trunk, and the bonnet can provide meaningful semantic
information and interaction states, which are essential to ensure the safety of
the self-driving vehicle. Existing visual perception models mainly focus on
coarse parsing such as object bounding box detection or pose estimation and
rarely tackle these situations. In this paper, we address this important
problem for autonomous driving by solving two critical issues using visual data
augmentation. First, to deal with data scarcity, we propose an effective
training data generation process by fitting a 3D car model with dynamic parts
to vehicles in real images and then reconstructing human-vehicle interaction
scenarios. This allows us to directly edit the real images using the aligned 3D
parts, yielding effective training data generation for learning robust deep
neural networks (DNNs). Second, to benchmark the quality of 3D part
understanding, we collect a large dataset in real world driving scenarios with
vehicles in uncommon states (VUS), i.e. with the door or trunk opened, etc.
Experiments demonstrate our trained network with visual data augmentation
largely outperforms other baselines in terms of 2D detection and instance
segmentation accuracy. Our network yields large improvements in discovering and
understanding these uncommon cases. Moreover, we plan to release all of the
source code, the dataset, and the trained model on GitHub.
</p>
<a href="http://arxiv.org/abs/2012.08055" target="_blank">arXiv:2012.08055</a> [<a href="http://arxiv.org/pdf/2012.08055" target="_blank">pdf</a>]

<h2>Distributed Data Storage and Fusion for Collective Perception in Resource-Limited Mobile Robot Swarms. (arXiv:2012.08061v1 [cs.RO])</h2>
<h3>Nathalie Majcherczyk, Daniel Jeswin Nallathambi, Tim Antonelli, Carlo Pinciroli</h3>
<p>In this paper, we propose an approach to the distributed storage and fusion
of data for collective perception in resource-limited robot swarms. We
demonstrate our approach in a distributed semantic classification scenario. We
consider a team of mobile robots, in which each robot runs a pre-trained
classifier of known accuracy to annotate objects in the environment. We provide
two main contributions: (i) a decentralized, shared data structure for
efficient storage and retrieval of the semantic annotations, specifically
designed for low-resource mobile robots; and (ii) a voting-based, decentralized
algorithm to reduce the variance of the calculated annotations in presence of
imperfect classification. We discuss theory and implementation of both
contributions, and perform an extensive set of realistic simulated experiments
to evaluate the performance of our approach.
</p>
<a href="http://arxiv.org/abs/2012.08061" target="_blank">arXiv:2012.08061</a> [<a href="http://arxiv.org/pdf/2012.08061" target="_blank">pdf</a>]

<h2>Learning Parameters for Balanced Index Influence Maximization. (arXiv:2012.08067v1 [cs.AI])</h2>
<h3>Manqing Ma, Gyorgy Korniss, Boleslaw K. Szymanski</h3>
<p>Influence maximization is the task of finding the smallest set of nodes whose
activation in a social network can trigger an activation cascade that reaches
the targeted network coverage, where threshold rules determine the outcome of
influence. This problem is NP-hard and it has generated a significant amount of
recent research on finding efficient heuristics. We focus on a {\it Balance
Index} algorithm that relies on three parameters to tune its performance to the
given network structure. We propose using a supervised machine-learning
approach for such tuning. We select the most influential graph features for the
parameter tuning. Then, using random-walk-based graph-sampling, we create small
snapshots from the given synthetic and large-scale real-world networks. Using
exhaustive search, we find for these snapshots the high accuracy values of BI
parameters to use as a ground truth. Then, we train our machine-learning model
on the snapshots and apply this model to the real-word network to find the best
BI parameters. We apply these parameters to the sampled real-world network to
measure the quality of the sets of initiators found this way. We use various
real-world networks to validate our approach against other heuristic.
</p>
<a href="http://arxiv.org/abs/2012.08067" target="_blank">arXiv:2012.08067</a> [<a href="http://arxiv.org/pdf/2012.08067" target="_blank">pdf</a>]

<h2>Hypothesis Disparity Regularized Mutual Information Maximization. (arXiv:2012.08072v1 [cs.LG])</h2>
<h3>Qicheng Lao, Xiang Jiang, Mohammad Havaei</h3>
<p>We propose a hypothesis disparity regularized mutual information
maximization~(HDMI) approach to tackle unsupervised hypothesis transfer -- as
an effort towards unifying hypothesis transfer learning (HTL) and unsupervised
domain adaptation (UDA) -- where the knowledge from a source domain is
transferred solely through hypotheses and adapted to the target domain in an
unsupervised manner. In contrast to the prevalent HTL and UDA approaches that
typically use a single hypothesis, HDMI employs multiple hypotheses to leverage
the underlying distributions of the source and target hypotheses. To better
utilize the crucial relationship among different hypotheses -- as opposed to
unconstrained optimization of each hypothesis independently -- while adapting
to the unlabeled target domain through mutual information maximization, HDMI
incorporates a hypothesis disparity regularization that coordinates the target
hypotheses jointly learn better target representations while preserving more
transferable source knowledge with better-calibrated prediction uncertainty.
HDMI achieves state-of-the-art adaptation performance on benchmark datasets for
UDA in the context of HTL, without the need to access the source data during
the adaptation.
</p>
<a href="http://arxiv.org/abs/2012.08072" target="_blank">arXiv:2012.08072</a> [<a href="http://arxiv.org/pdf/2012.08072" target="_blank">pdf</a>]

<h2>Generalized Chernoff Sampling for Active Learning and Structured Bandit Algorithms. (arXiv:2012.08073v1 [stat.ML])</h2>
<h3>Subhojyoti Mukherjee, Ardhendu Tripathy, Robert Nowak</h3>
<p>Active learning and structured stochastic bandit problems are intimately
related to the classical problem of sequential experimental design. This paper
studies active learning and best-arm identification in structured bandit
settings from the viewpoint of active sequential hypothesis testing, a
framework initiated by Chernoff (1959). We first characterize the sample
complexity of Chernoff's original procedure by uncovering terms that reduce in
significance as the allowed error probability $\delta \rightarrow 0$, but are
nevertheless relevant at any fixed value of $\delta &gt; 0$. While initially
proposed for testing among finitely many hypotheses, we obtain the analogue of
Chernoff sampling for the case when the hypotheses belong to a compact space.
This makes it applicable to active learning and structured bandit problems,
where the unknown parameter specifying the arm means is often assumed to be an
element of Euclidean space. Empirically, we demonstrate the potential of our
proposed approach for active learning of neural network models and in the
linear bandit setting, where we observe that our general-purpose approach
compares favorably to state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.08073" target="_blank">arXiv:2012.08073</a> [<a href="http://arxiv.org/pdf/2012.08073" target="_blank">pdf</a>]

<h2>An exact solution in Markov decision process with multiplicative rewards as a general framework. (arXiv:2012.08074v1 [cs.LG])</h2>
<h3>Yuan Yao, Xiaolin Sun</h3>
<p>We develop an exactly solvable framework of Markov decision process with a
finite horizon, and continuous state and action spaces. We first review the
exact solution of conventional linear quadratic regulation with a linear
transition and a Gaussian noise, whose optimal policy does not depend on the
Gaussian noise, which is an undesired feature in the presence of significant
noises. It motivates us to investigate exact solutions which depend on noise.
To do so, we generalize the reward accumulation to be a general binary
commutative and associative operation. By a new multiplicative accumulation, we
obtain an exact solution of optimization assuming linear transitions with a
Gaussian noise and the optimal policy is noise dependent in contrast to the
additive accumulation. Furthermore, we also show that the multiplicative scheme
is a general framework that covers the additive one with an arbitrary
precision, which is a model-independent principle.
</p>
<a href="http://arxiv.org/abs/2012.08074" target="_blank">arXiv:2012.08074</a> [<a href="http://arxiv.org/pdf/2012.08074" target="_blank">pdf</a>]

<h2>Coupled Layer-wise Graph Convolution for Transportation Demand Prediction. (arXiv:2012.08080v1 [cs.LG])</h2>
<h3>Junchen Ye, Leilei Sun, Bowen Du, Yanjie Fu, Hui Xiong</h3>
<p>Graph Convolutional Network (GCN) has been widely applied in transportation
demand prediction due to its excellent ability to capture non-Euclidean spatial
dependence among station-level or regional transportation demands. However, in
most of the existing research, the graph convolution was implemented on a
heuristically generated adjacency matrix, which could neither reflect the real
spatial relationships of stations accurately, nor capture the multi-level
spatial dependence of demands adaptively. To cope with the above problems, this
paper provides a novel graph convolutional network for transportation demand
prediction. Firstly, a novel graph convolution architecture is proposed, which
has different adjacency matrices in different layers and all the adjacency
matrices are self-learned during the training process. Secondly, a layer-wise
coupling mechanism is provided, which associates the upper-level adjacency
matrix with the lower-level one. It also reduces the scale of parameters in our
model. Lastly, a unitary network is constructed to give the final prediction
result by integrating the hidden spatial states with gated recurrent unit,
which could capture the multi-level spatial dependence and temporal dynamics
simultaneously. Experiments have been conducted on two real-world datasets, NYC
Citi Bike and NYC Taxi, and the results demonstrate the superiority of our
model over the state-of-the-art ones.
</p>
<a href="http://arxiv.org/abs/2012.08080" target="_blank">arXiv:2012.08080</a> [<a href="http://arxiv.org/pdf/2012.08080" target="_blank">pdf</a>]

<h2>Product Graph Learning from Multi-domain Data with Sparsity and Rank Constraints. (arXiv:2012.08090v1 [cs.LG])</h2>
<h3>Sai Kiran Kadambari, Sundeep Prabhakar Chepuri</h3>
<p>In this paper, we focus on learning product graphs from multi-domain data. We
assume that the product graph is formed by the Cartesian product of two smaller
graphs, which we refer to as graph factors. We pose the product graph learning
problem as the problem of estimating the graph factor Laplacian matrices. To
capture local interactions in data, we seek sparse graph factors and assume a
smoothness model for data. We propose an efficient iterative solver for
learning sparse product graphs from data. We then extend this solver to infer
multi-component graph factors with applications to product graph clustering by
imposing rank constraints on the graph Laplacian matrices. Although working
with smaller graph factors is computationally more attractive, not all graphs
may readily admit an exact Cartesian product factorization. To this end, we
propose efficient algorithms to approximate a graph by a nearest Cartesian
product of two smaller graphs. The efficacy of the developed framework is
demonstrated using several numerical experiments on synthetic data and real
data.
</p>
<a href="http://arxiv.org/abs/2012.08090" target="_blank">arXiv:2012.08090</a> [<a href="http://arxiv.org/pdf/2012.08090" target="_blank">pdf</a>]

<h2>Automatic Speech Verification Spoofing Detection. (arXiv:2012.08095v1 [cs.LG])</h2>
<h3>Shentong Mo, Haofan Wang, Pinxu Ren, Ta-Chung Chi</h3>
<p>Automatic speech verification (ASV) is the technology to determine the
identity of a person based on their voice. While being convenient for identity
verification, we should aim for the highest system security standard given that
it is the safeguard of valuable digital assets. Bearing this in mind, we follow
the setup in ASVSpoof 2019 competition to develop potential countermeasures
that are robust and efficient. Two metrics, EER and t-DCF, will be used for
system evaluation.
</p>
<a href="http://arxiv.org/abs/2012.08095" target="_blank">arXiv:2012.08095</a> [<a href="http://arxiv.org/pdf/2012.08095" target="_blank">pdf</a>]

<h2>FAWA: Fast Adversarial Watermark Attack on Optical Character Recognition (OCR) Systems. (arXiv:2012.08096v1 [cs.CV])</h2>
<h3>Lu Chen, Jiao Sun, Wei Xu</h3>
<p>Deep neural networks (DNNs) significantly improved the accuracy of optical
character recognition (OCR) and inspired many important applications.
Unfortunately, OCRs also inherit the vulnerabilities of DNNs under adversarial
examples. Different from colorful vanilla images, text images usually have
clear backgrounds. Adversarial examples generated by most existing adversarial
attacks are unnatural and pollute the background severely. To address this
issue, we propose the Fast Adversarial Watermark Attack (FAWA) against
sequence-based OCR models in the white-box manner. By disguising the
perturbations as watermarks, we can make the resulting adversarial images
appear natural to human eyes and achieve a perfect attack success rate. FAWA
works with either gradient-based or optimization-based perturbation generation.
In both letter-level and word-level attacks, our experiments show that in
addition to natural appearance, FAWA achieves a 100% attack success rate with
60% less perturbations and 78% fewer iterations on average. In addition, we
further extend FAWA to support full-color watermarks, other languages, and even
the OCR accuracy-enhancing mechanism.
</p>
<a href="http://arxiv.org/abs/2012.08096" target="_blank">arXiv:2012.08096</a> [<a href="http://arxiv.org/pdf/2012.08096" target="_blank">pdf</a>]

<h2>Towards Improving Spatiotemporal Action Recognition in Videos. (arXiv:2012.08097v1 [cs.CV])</h2>
<h3>Shentong Mo, Xiaoqing Tan, Jingfei Xia, Pinxu Ren</h3>
<p>Spatiotemporal action recognition deals with locating and classifying actions
in videos. Motivated by the latest state-of-the-art real-time object detector
You Only Watch Once (YOWO), we aim to modify its structure to increase action
detection precision and reduce computational time. Specifically, we propose
four novel approaches in attempts to improve YOWO and address the imbalanced
class issue in videos by modifying the loss function. We consider two
moderate-sized datasets to apply our modification of YOWO - the popular
Joint-annotated Human Motion Data Base (J-HMDB-21) and a private dataset of
restaurant video footage provided by a Carnegie Mellon University-based
startup, Agot.AI. The latter involves fast-moving actions with small objects as
well as unbalanced data classes, making the task of action localization more
challenging. We implement our proposed methods in the GitHub repository
https://github.com/stoneMo/YOWOv2.
</p>
<a href="http://arxiv.org/abs/2012.08097" target="_blank">arXiv:2012.08097</a> [<a href="http://arxiv.org/pdf/2012.08097" target="_blank">pdf</a>]

<h2>Fast 3D Image Moments. (arXiv:2012.08099v1 [cs.CV])</h2>
<h3>William Diggin, Michael Diggin</h3>
<p>An algorithm to efficiently compute the moments of volumetric images is
disclosed. The approach demonstrates a reduction in processing time by reducing
the computational complexity significantly. Specifically, the algorithm reduces
multiplicative complexity from O(n^3) to O(n). Several 2D projection images of
the 3D volume are generated. The algorithm computes a set of 2D moments from
those 2D images. Those 2D moments are then used to derive the 3D volumetric
moments. Examples of use in MRI or CT and related analysis demonstrates the
benefit of the Discrete Projection Moment Algorithm. The approach is also
useful in computing the moments of a 3D object using a small set of 2D
tomographic images of that object.
</p>
<a href="http://arxiv.org/abs/2012.08099" target="_blank">arXiv:2012.08099</a> [<a href="http://arxiv.org/pdf/2012.08099" target="_blank">pdf</a>]

<h2>Anomaly Detection and Localization based on Double Kernelized Scoring and Matrix Kernels. (arXiv:2012.08100v1 [cs.LG])</h2>
<h3>Shunsuke Hirose, Tomotake Kozu, Yingzi Jin</h3>
<p>Anomaly detection is necessary for proper and safe operation of large-scale
systems consisting of multiple devices, networks, and/or plants. Those systems
are often characterized by a pair of multivariate datasets. To detect anomaly
in such a system and localize element(s) associated with anomaly, one would
need to estimate scores that quantify anomalousness of the entire system as
well as its elements. However, it is not trivial to estimate such scores by
considering changes of relationships between the elements, which strongly
correlate with each other. Moreover, it is necessary to estimate the scores for
the entire system and its elements from a single framework, in order to
identify relationships among the scores for localizing elements associated with
anomaly. Here, we developed a new method to quantify anomalousness of an entire
system and its elements simultaneously.

The purpose of this paper is threefold. The first one is to propose a new
anomaly detection method: Double Kernelized Scoring (DKS). DKS is a unified
framework for entire-system anomaly scoring and element-wise anomaly scoring.
Therefore, DKS allows for conducting simultaneously 1) anomaly detection for
the entire system and 2) localization for identifying faulty elements
responsible for the system anomaly. The second purpose is to propose a new
kernel function: Matrix Kernel. The Matrix Kernel is defined between general
matrices, which might have different dimensions, allowing for conducting
anomaly detection on systems where the number of elements change over time. The
third purpose is to demonstrate the effectiveness of the proposed method
experimentally. We evaluated the proposed method with synthetic and real time
series data. The results demonstrate that DKS is able to detect anomaly and
localize the elements associated with it successfully.
</p>
<a href="http://arxiv.org/abs/2012.08100" target="_blank">arXiv:2012.08100</a> [<a href="http://arxiv.org/pdf/2012.08100" target="_blank">pdf</a>]

<h2>Variational Beam Search for Online Learning with Distribution Shifts. (arXiv:2012.08101v1 [stat.ML])</h2>
<h3>Aodong Li, Alex Boyd, Padhraic Smyth, Stephan Mandt</h3>
<p>We consider the problem of online learning in the presence of sudden
distribution shifts as frequently encountered in applications such as
autonomous navigation. Distribution shifts require constant performance
monitoring and re-training. They may also be hard to detect and can lead to a
slow but steady degradation in model performance. To address this problem we
propose a new Bayesian meta-algorithm that can both (i) make inferences about
subtle distribution shifts based on minimal sequential observations and (ii)
accordingly adapt a model in an online fashion. The approach uses beam search
over multiple change point hypotheses to perform inference on a hierarchical
sequential latent variable modeling framework. Our proposed approach is
model-agnostic, applicable to both supervised and unsupervised learning, and
yields significant improvements over state-of-the-art Bayesian online learning
approaches.
</p>
<a href="http://arxiv.org/abs/2012.08101" target="_blank">arXiv:2012.08101</a> [<a href="http://arxiv.org/pdf/2012.08101" target="_blank">pdf</a>]

<h2>KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment. (arXiv:2012.08103v1 [cs.CV])</h2>
<h3>Soo Ye Kim, Hyeonjun Sim, Munchurl Kim</h3>
<p>Blind super-resolution (SR) methods aim to generate a high quality high
resolution image from a low resolution image containing unknown degradations.
However, natural images contain various types and amounts of blur: some may be
due to the inherent degradation characteristics of the camera, but some may
even be intentional, for aesthetic purposes (eg. Bokeh effect). In the case of
the latter, it becomes highly difficult for SR methods to disentangle the blur
to remove, and that to leave as is. In this paper, we propose a novel blind SR
framework based on kernel-oriented adaptive local adjustment (KOALA) of SR
features, called KOALAnet, which jointly learns spatially-variant degradation
and restoration kernels in order to adapt to the spatially-variant blur
characteristics in real images. Our KOALAnet outperforms recent blind SR
methods for synthesized LR images obtained with randomized degradations, and we
further show that the proposed KOALAnet produces the most natural results for
artistic photographs with intentional blur, which are not over-sharpened, by
effectively handling images mixed with in-focus and out-of-focus areas.
</p>
<a href="http://arxiv.org/abs/2012.08103" target="_blank">arXiv:2012.08103</a> [<a href="http://arxiv.org/pdf/2012.08103" target="_blank">pdf</a>]

<h2>Amata: An Annealing Mechanism for Adversarial Training Acceleration. (arXiv:2012.08112v1 [cs.LG])</h2>
<h3>Nanyang Ye, Qianxiao Li, Xiao-Yun Zhou, Zhanxing Zhu</h3>
<p>Despite the empirical success in various domains, it has been revealed that
deep neural networks are vulnerable to maliciously perturbed input data that
much degrade their performance. This is known as adversarial attacks. To
counter adversarial attacks, adversarial training formulated as a form of
robust optimization has been demonstrated to be effective. However, conducting
adversarial training brings much computational overhead compared with standard
training. In order to reduce the computational cost, we propose an annealing
mechanism, Amata, to reduce the overhead associated with adversarial training.
The proposed Amata is provably convergent, well-motivated from the lens of
optimal control theory and can be combined with existing acceleration methods
to further enhance performance. It is demonstrated that on standard datasets,
Amata can achieve similar or better robustness with around 1/3 to 1/2 the
computational time compared with traditional methods. In addition, Amata can be
incorporated into other adversarial training acceleration algorithms (e.g.
YOPO, Free, Fast, and ATTA), which leads to further reduction in computational
time on large-scale problems.
</p>
<a href="http://arxiv.org/abs/2012.08112" target="_blank">arXiv:2012.08112</a> [<a href="http://arxiv.org/pdf/2012.08112" target="_blank">pdf</a>]

<h2>Learning Energy-Based Models by Diffusion Recovery Likelihood. (arXiv:2012.08125v1 [cs.LG])</h2>
<h3>Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, Diederik P. Kingma</h3>
<p>While energy-based models (EBMs) exhibit a number of desirable properties,
training and sampling on high-dimensional datasets remains challenging.
Inspired by recent progress on diffusion probabilistic models, we present a
diffusion recovery likelihood method to tractably learn and sample from a
sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM
is trained by maximizing the recovery likelihood: the conditional probability
of the data at a certain noise level given their noisy versions at a higher
noise level. The recovery likelihood objective is more tractable than the
marginal likelihood objective, since it only requires MCMC sampling from a
relatively concentrated conditional distribution. Moreover, we show that this
estimation method is theoretically consistent: it learns the correct
conditional and marginal distributions at each noise level, given sufficient
data. After training, synthesized images can be generated efficiently by a
sampling process that initializes from a spherical Gaussian distribution and
progressively samples the conditional distributions at decreasingly lower noise
levels. Our method generates high fidelity samples on various image datasets.
On unconditional CIFAR-10 our method achieves FID 9.60 and inception score
8.58, superior to the majority of GANs. Moreover, we demonstrate that unlike
previous work on EBMs, our long-run MCMC samples from the conditional
distributions do not diverge and still represent realistic images, allowing us
to accurately estimate the normalized density of data even for high-dimensional
datasets.
</p>
<a href="http://arxiv.org/abs/2012.08125" target="_blank">arXiv:2012.08125</a> [<a href="http://arxiv.org/pdf/2012.08125" target="_blank">pdf</a>]

<h2>Relation-Aware Neighborhood Matching Model for Entity Alignment. (arXiv:2012.08128v1 [cs.AI])</h2>
<h3>Yao Zhu, Hongzhi Liu, Zhonghai Wu, Yingpeng Du</h3>
<p>Entity alignment which aims at linking entities with the same meaning from
different knowledge graphs (KGs) is a vital step for knowledge fusion. Existing
research focused on learning embeddings of entities by utilizing structural
information of KGs for entity alignment. These methods can aggregate
information from neighboring nodes but may also bring noise from neighbors.
Most recently, several researchers attempted to compare neighboring nodes in
pairs to enhance the entity alignment. However, they ignored the relations
between entities which are also important for neighborhood matching. In
addition, existing methods paid less attention to the positive interactions
between the entity alignment and the relation alignment. To deal with these
issues, we propose a novel Relation-aware Neighborhood Matching model named RNM
for entity alignment. Specifically, we propose to utilize the neighborhood
matching to enhance the entity alignment. Besides comparing neighbor nodes when
matching neighborhood, we also try to explore useful information from the
connected relations. Moreover, an iterative framework is designed to leverage
the positive interactions between the entity alignment and the relation
alignment in a semi-supervised manner. Experimental results on three real-world
datasets demonstrate that the proposed model RNM performs better than
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.08128" target="_blank">arXiv:2012.08128</a> [<a href="http://arxiv.org/pdf/2012.08128" target="_blank">pdf</a>]

<h2>Class-incremental Learning with Rectified Feature-Graph Preservation. (arXiv:2012.08129v1 [cs.CV])</h2>
<h3>Cheng-Hsun Lei, Yi-Hsin, Wen-Hsiao Peng, Wei-Chen Chiu</h3>
<p>In this paper, we address the problem of distillation-based class-incremental
learning with a single head. A central theme of this task is to learn new
classes that arrive in sequential phases over time while keeping the model's
capability of recognizing seen classes with only limited memory for preserving
seen data samples. Many regularization strategies have been proposed to
mitigate the phenomenon of catastrophic forgetting. To understand better the
essence of these regularizations, we introduce a feature-graph preservation
perspective. Insights into their merits and faults motivate our
weighted-Euclidean regularization for old knowledge preservation. We further
propose rectified cosine normalization and show how it can work with binary
cross-entropy to increase class separation for effective learning of new
classes. Experimental results on both CIFAR-100 and ImageNet datasets
demonstrate that our method outperforms the state-of-the-art approaches in
reducing classification error, easing catastrophic forgetting, and encouraging
evenly balanced accuracy over different classes. Our project page is at :
https://github.com/yhchen12101/FGP-ICL.
</p>
<a href="http://arxiv.org/abs/2012.08129" target="_blank">arXiv:2012.08129</a> [<a href="http://arxiv.org/pdf/2012.08129" target="_blank">pdf</a>]

<h2>Deep Layout of Custom-size Furniture through Multiple-domain Learning. (arXiv:2012.08131v1 [cs.CV])</h2>
<h3>Xinhan Di, Pengqian Yu, Danfeng Yang, Hong Zhu, Changyu Sun, YinDong Liu</h3>
<p>In this paper, we propose a multiple-domain model for producing a custom-size
furniture layout in the interior scene. This model is aimed to support
professional interior designers to produce interior decoration solutions with
custom-size furniture more quickly. The proposed model combines a deep layout
module, a domain attention module, a dimensional domain transfer module, and a
custom-size module in the end-end training. Compared with the prior work on
scene synthesis, our proposed model enhances the ability of auto-layout of
custom-size furniture in the interior room. We conduct our experiments on a
real-world interior layout dataset that contains $710,700$ designs from
professional designers. Our numerical results demonstrate that the proposed
model yields higher-quality layouts of custom-size furniture in comparison with
the state-of-art model.
</p>
<a href="http://arxiv.org/abs/2012.08131" target="_blank">arXiv:2012.08131</a> [<a href="http://arxiv.org/pdf/2012.08131" target="_blank">pdf</a>]

<h2>Efficient Trajectory Planning for Multiple Non-holonomic Mobile Robots via Prioritized Trajectory Optimization. (arXiv:2012.08135v1 [cs.RO])</h2>
<h3>Juncheng Li, Maopeng Ran, Lihua Xie</h3>
<p>In this paper, we present a novel approach to efficiently generate
collision-free optimal trajectories for multiple non-holonomic mobile robots in
obstacle-rich environments. Our approach first employs a graph-based
multi-agent path planner to find an initial discrete solution, and then refines
this solution into smooth trajectories using nonlinear optimization. We divide
the robot team into small groups and propose a prioritized trajectory
optimization method to improve the scalability of the algorithm. Infeasible
sub-problems may arise in some scenarios because of the decoupled optimization
framework. To handle this problem, a novel grouping and priority assignment
strategy is developed to increase the probability of finding feasible
trajectories. Compared to the coupled trajectory optimization, the proposed
approach reduces the computation time considerably with a small impact on the
optimality of the plans. Simulations and hardware experiments verified the
effectiveness and superiority of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2012.08135" target="_blank">arXiv:2012.08135</a> [<a href="http://arxiv.org/pdf/2012.08135" target="_blank">pdf</a>]

<h2>NeuralQAAD: An Efficient Differentiable Framework for High Resolution Point Cloud Compression. (arXiv:2012.08143v1 [cs.CV])</h2>
<h3>Nicolas Wagner, Ulrich Schwanecke</h3>
<p>In this paper, we propose NeuralQAAD, a differentiable point cloud
compression framework that is fast, robust to sampling, and applicable to high
resolutions. Previous work that is able to handle complex and non-smooth
topologies is hardly scaleable to more than just a few thousand points. We
tackle the task with a novel neural network architecture characterized by
weight sharing and autodecoding. Our architecture uses parameters much more
efficiently than previous work, allowing us to be deeper and scalable.
Futhermore, we show that the currently only tractable training criterion for
point cloud compression, the Chamfer distance, performances poorly for high
resolutions. To overcome this issue, we pair our architecture with a new
training procedure based upon a quadratic assignment problem (QAP) for which we
state two approximation algorithms. We solve the QAP in parallel to gradient
descent. This procedure acts as a surrogate loss and allows to implicitly
minimize the more expressive Earth Movers Distance (EMD) even for point clouds
with way more than $10^6$ points. As evaluating the EMD on high resolution
point clouds is intractable, we propose a divide-and-conquer approach based on
k-d trees, the EM-kD, as a scaleable and fast but still reliable upper bound
for the EMD. NeuralQAAD is demonstrated on COMA, D-FAUST, and Skulls to
significantly outperform the current state-of-the-art visually and in terms of
the EM-kD. Skulls is a novel dataset of skull CT-scans which we will make
publicly available together with our implementation of NeuralQAAD.
</p>
<a href="http://arxiv.org/abs/2012.08143" target="_blank">arXiv:2012.08143</a> [<a href="http://arxiv.org/pdf/2012.08143" target="_blank">pdf</a>]

<h2>Generation of complex database queries and API calls from natural language utterances. (arXiv:2012.08146v1 [cs.LG])</h2>
<h3>Amol Kelkar, Nachiketa Rajpurohit, Utkarsh Mittal, Peter Relan</h3>
<p>Generating queries corresponding to natural language questions is a long
standing problem. Traditional methods lack language flexibility, while newer
sequence-to-sequence models require large amount of data. Schema-agnostic
sequence-to-sequence models can be fine-tuned for a specific schema using a
small dataset but these models have relatively low accuracy. We present a
method that transforms the query generation problem into an intent
classification and slot filling problem. This method can work using small
datasets. For questions similar to the ones in the training dataset, it
produces complex queries with high accuracy. For other questions, it can use a
template-based approach or predict query pieces to construct the queries, still
at a higher accuracy than sequence-to-sequence models. On a real-world dataset,
a schema fine-tuned state-of-the-art generative model had 60\% exact match
accuracy for the query generation task, while our method resulted in 92\% exact
match accuracy.
</p>
<a href="http://arxiv.org/abs/2012.08146" target="_blank">arXiv:2012.08146</a> [<a href="http://arxiv.org/pdf/2012.08146" target="_blank">pdf</a>]

<h2>Dilated-Scale-Aware Attention ConvNet For Multi-Class Object Counting. (arXiv:2012.08149v1 [cs.CV])</h2>
<h3>Wei Xu, Dingkang Liang, Yixiao Zheng, Zhanyu Ma</h3>
<p>Object counting aims to estimate the number of objects in images. The leading
counting approaches focus on the single category counting task and achieve
impressive performance. Note that there are multiple categories of objects in
real scenes. Multi-class object counting expands the scope of application of
object counting task. The multi-target detection task can achieve multi-class
object counting in some scenarios. However, it requires the dataset annotated
with bounding boxes. Compared with the point annotations in mainstream object
counting issues, the coordinate box-level annotations are more difficult to
obtain. In this paper, we propose a simple yet efficient counting network based
on point-level annotations. Specifically, we first change the traditional
output channel from one to the number of categories to achieve multiclass
counting. Since all categories of objects use the same feature extractor in our
proposed framework, their features will interfere mutually in the shared
feature space. We further design a multi-mask structure to suppress harmful
interaction among objects. Extensive experiments on the challenging benchmarks
illustrate that the proposed method achieves state-of-the-art counting
performance.
</p>
<a href="http://arxiv.org/abs/2012.08149" target="_blank">arXiv:2012.08149</a> [<a href="http://arxiv.org/pdf/2012.08149" target="_blank">pdf</a>]

<h2>Modeling Heterogeneous Statistical Patterns in High-dimensional Data by Adversarial Distributions: An Unsupervised Generative Framework. (arXiv:2012.08153v1 [cs.LG])</h2>
<h3>Han Zhang, Wenhao Zheng, Charley Chen, Kevin Gao, Yao Hu, Ling Huang, Wei Xu</h3>
<p>Since the label collecting is prohibitive and time-consuming, unsupervised
methods are preferred in applications such as fraud detection. Meanwhile, such
applications usually require modeling the intrinsic clusters in
high-dimensional data, which usually displays heterogeneous statistical
patterns as the patterns of different clusters may appear in different
dimensions. Existing methods propose to model the data clusters on selected
dimensions, yet globally omitting any dimension may damage the pattern of
certain clusters. To address the above issues, we propose a novel unsupervised
generative framework called FIRD, which utilizes adversarial distributions to
fit and disentangle the heterogeneous statistical patterns. When applying to
discrete spaces, FIRD effectively distinguishes the synchronized fraudsters
from normal users. Besides, FIRD also provides superior performance on anomaly
detection datasets compared with SOTA anomaly detection methods (over 5%
average AUC improvement). The significant experiment results on various
datasets verify that the proposed method can better model the heterogeneous
statistical patterns in high-dimensional data and benefit downstream
applications.
</p>
<a href="http://arxiv.org/abs/2012.08153" target="_blank">arXiv:2012.08153</a> [<a href="http://arxiv.org/pdf/2012.08153" target="_blank">pdf</a>]

<h2>Confidential Machine Learning on Untrusted Platforms: A Survey. (arXiv:2012.08156v1 [cs.LG])</h2>
<h3>Sagar Sharma, Keke Chen</h3>
<p>With ever-growing data and the need for developing powerful machine learning
models, data owners increasingly depend on untrusted platforms (e.g., public
clouds, edges, and machine learning service providers). However, sensitive data
and models become susceptible to unauthorized access, misuse, and privacy
compromises. Recently, a body of research has been developed to train machine
learning models on encrypted outsourced data with untrusted platforms. In this
survey, we summarize the studies in this emerging area with a unified framework
to highlight the major challenges and approaches. We will focus on the
cryptographic approaches for confidential machine learning (CML), while also
covering other directions such as perturbation-based approaches and CML in the
hardware-assisted confidential computing environment. The discussion will take
a holistic way to consider a rich context of the related threat models,
security assumptions, attacks, design philosophies, and associated trade-offs
amongst data utility, cost, and confidentiality.
</p>
<a href="http://arxiv.org/abs/2012.08156" target="_blank">arXiv:2012.08156</a> [<a href="http://arxiv.org/pdf/2012.08156" target="_blank">pdf</a>]

<h2>Research on All-content Text Recognition Method for Financial Ticket Image. (arXiv:2012.08168v1 [cs.CV])</h2>
<h3>Fukang Tian, Haiyu Wu, Bo Xu</h3>
<p>With the development of the economy, the number of financial tickets
increases rapidly. The traditional manual invoice reimbursement and financial
accounting system bring more and more burden to financial accountants.
Therefore, based on the research and analysis of a large number of real
financial ticket data, we designed an accurate and efficient all contents text
detection and recognition method based on deep learning. This method has higher
recognition accuracy and recall rate and can meet the actual requirements of
financial accounting work. In addition, we propose a Financial Ticket Character
Recognition Framework (FTCRF). According to the characteristics of Chinese
character recognition, this framework contains a two-step information
extraction method, which can improve the speed of Chinese character
recognition. The experimental results show that the average recognition
accuracy of this method is 91.75\% for character sequence and 87\% for the
whole ticket. The availability and effectiveness of this method are verified by
a commercial application system, which significantly improves the efficiency of
the financial accounting system.
</p>
<a href="http://arxiv.org/abs/2012.08168" target="_blank">arXiv:2012.08168</a> [<a href="http://arxiv.org/pdf/2012.08168" target="_blank">pdf</a>]

<h2>Towards open and expandable cognitive AI architectures for large-scale multi-agent human-robot collaborative learning. (arXiv:2012.08174v1 [cs.RO])</h2>
<h3>Georgios Th. Papadopoulos, Margherita Antona, Constantine Stephanidis</h3>
<p>Learning from Demonstration (LfD) constitutes one of the most robust
methodologies for constructing efficient cognitive robotic systems. Current key
challenges in the field include those of multi-agent learning and long-term
autonomy. Towards this direction, a novel cognitive architecture for
multi-agent LfD robotic learning is introduced in this paper, targeting to
enable the reliable deployment of open, scalable and expandable robotic systems
in large-scale and complex environments. In particular, the designed
architecture capitalizes on the recent advances in the Artificial Intelligence
(AI) (and especially the Deep Learning (DL)) field, by establishing a Federated
Learning (FL)-based framework for incarnating a multi-human multi-robot
collaborative learning environment. The fundamental conceptualization relies on
employing multiple AI-empowered cognitive processes (implementing various
robotic tasks) that operate at the edge nodes of a network of robotic
platforms, while global AI models (underpinning the aforementioned robotic
tasks) are collectively created and shared among the network, by elegantly
combining information from a large number of human-robot interaction instances.
Pivotal novelties of the designed cognitive architecture include: a) it
introduces a new FL-based formalism that extends the conventional LfD learning
paradigm to support large-scale multi-agent operational settings, b) it
elaborates previous FL-based self-learning robotic schemes so as to incorporate
the human in the learning loop, and c) it consolidates the fundamental
principles of FL with additional sophisticated AI-enabled learning
methodologies for modelling the multi-level inter-dependencies among the
robotic tasks. The applicability of the proposed framework is explained using
an example of a real-world industrial case study for agile production-based
Critical Raw Materials (CRM) recovery.
</p>
<a href="http://arxiv.org/abs/2012.08174" target="_blank">arXiv:2012.08174</a> [<a href="http://arxiv.org/pdf/2012.08174" target="_blank">pdf</a>]

<h2>Squirrel: A Switching Hyperparameter Optimizer. (arXiv:2012.08180v1 [cs.LG])</h2>
<h3>Noor Awad, Gresa Shala, Difan Deng, Neeratyoy Mallik, Matthias Feurer, Katharina Eggensperger, Andre&#x27; Biedenkapp, Diederick Vermetten, Hao Wang, Carola Doerr, Marius Lindauer, Frank Hutter</h3>
<p>In this short note, we describe our submission to the NeurIPS 2020 BBO
challenge. Motivated by the fact that different optimizers work well on
different problems, our approach \emph{switches} between different
optimizers.\footnote{Switching between algorithms also relates to work on
\emph{chaining} or \emph{algorithm schedules}.} Since the team names on the
competition's leaderboard were randomly generated "alliteration nicknames",
consisting of an adjective and an animal with the same initial letter, we
called our approach the \emph{Switching Squirrel}, or here, short,
\emph{Squirrel}. Our reference implementation of Squirrel is available at
\url{https://github.com/automl/Squirrel-Optimizer-BBO-NeurIPS20-automlorg}.
</p>
<a href="http://arxiv.org/abs/2012.08180" target="_blank">arXiv:2012.08180</a> [<a href="http://arxiv.org/pdf/2012.08180" target="_blank">pdf</a>]

<h2>Scalable Verification of Quantized Neural Networks (Technical Report). (arXiv:2012.08185v1 [cs.AI])</h2>
<h3>Thomas A. Henzinger, Mathias Lechner, &#x110;or&#x111;e &#x17d;ikeli&#x107;</h3>
<p>Formal verification of neural networks is an active topic of research, and
recent advances have significantly increased the size of the networks that
verification tools can handle. However, most methods are designed for
verification of an idealized model of the actual network which works over real
arithmetic and ignores rounding imprecisions. This idealization is in stark
contrast to network quantization, which is a technique that trades numerical
precision for computational efficiency and is, therefore, often applied in
practice. Neglecting rounding errors of such low-bit quantized neural networks
has been shown to lead to wrong conclusions about the network's correctness.
Thus, the desired approach for verifying quantized neural networks would be one
that takes these rounding errors into account. In this paper, we show that
verifying the bit-exact implementation of quantized neural networks with
bit-vector specifications is PSPACE-hard, even though verifying idealized
real-valued networks and satisfiability of bit-vector specifications alone are
each in NP. Furthermore, we explore several practical heuristics toward closing
the complexity gap between idealized and bit-exact verification. In particular,
we propose three techniques for making SMT-based verification of quantized
neural networks more scalable. Our experiments demonstrate that our proposed
methods allow a speedup of up to three orders of magnitude over existing
approaches.
</p>
<a href="http://arxiv.org/abs/2012.08185" target="_blank">arXiv:2012.08185</a> [<a href="http://arxiv.org/pdf/2012.08185" target="_blank">pdf</a>]

<h2>docExtractor: An off-the-shelf historical document element extraction. (arXiv:2012.08191v1 [cs.CV])</h2>
<h3>Tom Monnier, Mathieu Aubry</h3>
<p>We present docExtractor, a generic approach for extracting visual elements
such as text lines or illustrations from historical documents without requiring
any real data annotation. We demonstrate it provides high-quality performances
as an off-the-shelf system across a wide variety of datasets and leads to
results on par with state-of-the-art when fine-tuned. We argue that the
performance obtained without fine-tuning on a specific dataset is critical for
applications, in particular in digital humanities, and that the line-level page
segmentation we address is the most relevant for a general purpose element
extraction engine. We rely on a fast generator of rich synthetic documents and
design a fully convolutional network, which we show to generalize better than a
detection-based approach. Furthermore, we introduce a new public dataset dubbed
IlluHisDoc dedicated to the fine evaluation of illustration segmentation in
historical documents.
</p>
<a href="http://arxiv.org/abs/2012.08191" target="_blank">arXiv:2012.08191</a> [<a href="http://arxiv.org/pdf/2012.08191" target="_blank">pdf</a>]

<h2>Bayesian neural network with pretrained protein embedding enhances prediction accuracy of drug-protein interaction. (arXiv:2012.08194v1 [cs.LG])</h2>
<h3>QHwan Kim, Joon-Hyuk Ko, Sunghoon Kim, Nojun Park, Wonho Jhe</h3>
<p>The characterization of drug-protein interactions is crucial in the
high-throughput screening for drug discovery. The deep learning-based
approaches have attracted attention because they can predict drug-protein
interactions without trial-and-error by humans. However, because data labeling
requires significant resources, the available protein data size is relatively
small, which consequently decreases model performance. Here we propose two
methods to construct a deep learning framework that exhibits superior
performance with a small labeled dataset. At first, we use transfer learning in
encoding protein sequences with a pretrained model, which trains general
sequence representations in an unsupervised manner. Second, we use a Bayesian
neural network to make a robust model by estimating the data uncertainty. As a
result, our model performs better than the previous baselines for predicting
drug-protein interactions. We also show that the quantified uncertainty from
the Bayesian inference is related to the confidence and can be used for
screening DPI data points.
</p>
<a href="http://arxiv.org/abs/2012.08194" target="_blank">arXiv:2012.08194</a> [<a href="http://arxiv.org/pdf/2012.08194" target="_blank">pdf</a>]

<h2>Representing Ambiguity in Registration Problems with Conditional Invertible Neural Networks. (arXiv:2012.08195v1 [cs.CV])</h2>
<h3>Darya Trofimova, Tim Adler, Lisa Kausch, Lynton Ardizzone, Klaus Maier-Hein, Ulrich K&#xf6;the, Carsten Rother, Lena Maier-Hein</h3>
<p>Image registration is the basis for many applications in the fields of
medical image computing and computer assisted interventions. One example is the
registration of 2D X-ray images with preoperative three-dimensional computed
tomography (CT) images in intraoperative surgical guidance systems. Due to the
high safety requirements in medical applications, estimating registration
uncertainty is of a crucial importance in such a scenario. However, previously
proposed methods, including classical iterative registration methods and deep
learning-based methods have one characteristic in common: They lack the
capacity to represent the fact that a registration problem may be inherently
ambiguous, meaning that multiple (substantially different) plausible solutions
exist. To tackle this limitation, we explore the application of invertible
neural networks (INN) as core component of a registration methodology. In the
proposed framework, INNs enable going beyond point estimates as network output
by representing the possible solutions to a registration problem by a
probability distribution that encodes different plausible solutions via
multiple modes. In a first feasibility study, we test the approach for a 2D 3D
registration setting by registering spinal CT volumes to X-ray images. To this
end, we simulate the X-ray images taken by a C-Arm with multiple orientations
using the principle of digitially reconstructed radiographs (DRRs). Due to the
symmetry of human spine, there are potentially multiple substantially different
poses of the C-Arm that can lead to similar projections. The hypothesis of this
work is that the proposed approach is able to identify multiple solutions in
such ambiguous registration problems.
</p>
<a href="http://arxiv.org/abs/2012.08195" target="_blank">arXiv:2012.08195</a> [<a href="http://arxiv.org/pdf/2012.08195" target="_blank">pdf</a>]

<h2>Explainable Recommendation Systems by Generalized Additive Models with Manifest and Latent Interactions. (arXiv:2012.08196v1 [cs.LG])</h2>
<h3>Yifeng Guo, Yu Su, Zebin Yang, Aijun Zhang</h3>
<p>In recent years, the field of recommendation systems has attracted increasing
attention to developing predictive models that provide explanations of why an
item is recommended to a user. The explanations can be either obtained by
post-hoc diagnostics after fitting a relatively complex model or embedded into
an intrinsically interpretable model. In this paper, we propose the explainable
recommendation systems based on a generalized additive model with manifest and
latent interactions (GAMMLI). This model architecture is intrinsically
interpretable, as it additively consists of the user and item main effects, the
manifest user-item interactions based on observed features, and the latent
interaction effects from residuals. Unlike conventional collaborative filtering
methods, the group effect of users and items are considered in GAMMLI. It is
beneficial for enhancing the model interpretability, and can also facilitate
the cold-start recommendation problem. A new Python package GAMMLI is developed
for efficient model training and visualized interpretation of the results. By
numerical experiments based on simulation data and real-world cases, the
proposed method is shown to have advantages in both predictive performance and
explainable recommendation.
</p>
<a href="http://arxiv.org/abs/2012.08196" target="_blank">arXiv:2012.08196</a> [<a href="http://arxiv.org/pdf/2012.08196" target="_blank">pdf</a>]

<h2>Seeing Behind Objects for 3D Multi-Object Tracking in RGB-D Sequences. (arXiv:2012.08197v1 [cs.CV])</h2>
<h3>Norman M&#xfc;ller, Yu-Shiang Wong, Niloy J. Mitra, Angela Dai, Matthias Nie&#xdf;ner</h3>
<p>Multi-object tracking from RGB-D video sequences is a challenging problem due
to the combination of changing viewpoints, motion, and occlusions over time. We
observe that having the complete geometry of objects aids in their tracking,
and thus propose to jointly infer the complete geometry of objects as well as
track them, for rigidly moving objects over time. Our key insight is that
inferring the complete geometry of the objects significantly helps in tracking.
By hallucinating unseen regions of objects, we can obtain additional
correspondences between the same instance, thus providing robust tracking even
under strong change of appearance. From a sequence of RGB-D frames, we detect
objects in each frame and learn to predict their complete object geometry as
well as a dense correspondence mapping into a canonical space. This allows us
to derive 6DoF poses for the objects in each frame, along with their
correspondence between frames, providing robust object tracking across the
RGB-D sequence. Experiments on both synthetic and real-world RGB-D data
demonstrate that we achieve state-of-the-art performance on dynamic object
tracking. Furthermore, we show that our object completion significantly helps
tracking, providing an improvement of $6.5\%$ in mean MOTA.
</p>
<a href="http://arxiv.org/abs/2012.08197" target="_blank">arXiv:2012.08197</a> [<a href="http://arxiv.org/pdf/2012.08197" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation from Synthetic to Real Images for Anchorless Object Detection. (arXiv:2012.08205v1 [cs.CV])</h2>
<h3>Tobias Scheck, Ana Perez Grassi, Gangolf Hirtz</h3>
<p>Synthetic images are one of the most promising solutions to avoid high costs
associated with generating annotated datasets to train supervised convolutional
neural networks (CNN). However, to allow networks to generalize knowledge from
synthetic to real images, domain adaptation methods are necessary. This paper
implements unsupervised domain adaptation (UDA) methods on an anchorless object
detector. Given their good performance, anchorless detectors are increasingly
attracting attention in the field of object detection. While their results are
comparable to the well-established anchor-based methods, anchorless detectors
are considerably faster. In our work, we use CenterNet, one of the most recent
anchorless architectures, for a domain adaptation problem involving synthetic
images. Taking advantage of the architecture of anchorless detectors, we
propose to adjust two UDA methods, viz., entropy minimization and maximum
squares loss, originally developed for segmentation, to object detection. Our
results show that the proposed UDA methods can increase the mAPfrom61 %to69
%with respect to direct transfer on the considered anchorless detector. The
code is available: https://github.com/scheckmedia/centernet-uda.
</p>
<a href="http://arxiv.org/abs/2012.08205" target="_blank">arXiv:2012.08205</a> [<a href="http://arxiv.org/pdf/2012.08205" target="_blank">pdf</a>]

<h2>FMODetect: Robust Detection and Trajectory Estimation of Fast Moving Objects. (arXiv:2012.08216v1 [cs.CV])</h2>
<h3>Denys Rozumnyi, Jiri Matas, Filip Sroubek, Marc Pollefeys, Martin R. Oswald</h3>
<p>We propose the first learning-based approach for detection and trajectory
estimation of fast moving objects. Such objects are highly blurred and move
over large distances within one video frame. Fast moving objects are associated
with a deblurring and matting problem, also called deblatting. Instead of
solving the complex deblatting problem jointly, we split the problem into
matting and deblurring and solve them separately. The proposed method first
detects all fast moving objects as a truncated distance function to the
trajectory. Subsequently, a matting and fitting network for each detected
object estimates the object trajectory and its blurred appearance without
background. For the sharp appearance estimation, we propose an energy
minimization based deblurring. The state-of-the-art methods are outperformed in
terms of trajectory estimation and sharp appearance reconstruction. Compared to
other methods, such as deblatting, the inference is of several orders of
magnitude faster and allows applications such as real-time fast moving object
detection and retrieval in large video collections.
</p>
<a href="http://arxiv.org/abs/2012.08216" target="_blank">arXiv:2012.08216</a> [<a href="http://arxiv.org/pdf/2012.08216" target="_blank">pdf</a>]

<h2>Policy Optimization as Online Learning with Mediator Feedback. (arXiv:2012.08225v1 [cs.LG])</h2>
<h3>Alberto Maria Metelli, Matteo Papini, Pierluca D&#x27;Oro, Marcello Restelli</h3>
<p>Policy Optimization (PO) is a widely used approach to address continuous
control tasks. In this paper, we introduce the notion of mediator feedback that
frames PO as an online learning problem over the policy space. The additional
available information, compared to the standard bandit feedback, allows reusing
samples generated by one policy to estimate the performance of other policies.
Based on this observation, we propose an algorithm, RANDomized-exploration
policy Optimization via Multiple Importance Sampling with Truncation
(RANDOMIST), for regret minimization in PO, that employs a randomized
exploration strategy, differently from the existing optimistic approaches. When
the policy space is finite, we show that under certain circumstances, it is
possible to achieve constant regret, while always enjoying logarithmic regret.
We also derive problem-dependent regret lower bounds. Then, we extend RANDOMIST
to compact policy spaces. Finally, we provide numerical simulations on finite
and compact policy spaces, in comparison with PO and bandit baselines.
</p>
<a href="http://arxiv.org/abs/2012.08225" target="_blank">arXiv:2012.08225</a> [<a href="http://arxiv.org/pdf/2012.08225" target="_blank">pdf</a>]

<h2>Cross-Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation. (arXiv:2012.08226v1 [cs.CV])</h2>
<h3>Minsu Kim, Sunghun Joung, Seungryong Kim, JungIn Park, Ig-Jae Kim, Kwanghoon Sohn</h3>
<p>Existing techniques to adapt semantic segmentation networks across the source
and target domains within deep convolutional neural networks (CNNs) deal with
all the samples from the two domains in a global or category-aware manner. They
do not consider an inter-class variation within the target domain itself or
estimated category, providing the limitation to encode the domains having a
multi-modal data distribution. To overcome this limitation, we introduce a
learnable clustering module, and a novel domain adaptation framework called
cross-domain grouping and alignment. To cluster the samples across domains with
an aim to maximize the domain alignment without forgetting precise segmentation
ability on the source domain, we present two loss functions, in particular, for
encouraging semantic consistency and orthogonality among the clusters. We also
present a loss so as to solve a class imbalance problem, which is the other
limitation of the previous methods. Our experiments show that our method
consistently boosts the adaptation performance in semantic segmentation,
outperforming the state-of-the-arts on various domain adaptation settings.
</p>
<a href="http://arxiv.org/abs/2012.08226" target="_blank">arXiv:2012.08226</a> [<a href="http://arxiv.org/pdf/2012.08226" target="_blank">pdf</a>]

<h2>Canny-VO: Visual Odometry with RGB-D Cameras based on Geometric 3D-2D Edge Alignment. (arXiv:2012.08228v1 [cs.CV])</h2>
<h3>Yi Zhou, Hongdong Li, Laurent Kneip</h3>
<p>The present paper reviews the classical problem of free-form curve
registration and applies it to an efficient RGBD visual odometry system called
Canny-VO, as it efficiently tracks all Canny edge features extracted from the
images. Two replacements for the distance transformation commonly used in edge
registration are proposed: Approximate Nearest Neighbour Fields and Oriented
Nearest Neighbour Fields. 3D2D edge alignment benefits from these alternative
formulations in terms of both efficiency and accuracy. It removes the need for
the more computationally demanding paradigms of datato-model registration,
bilinear interpolation, and sub-gradient computation. To ensure robustness of
the system in the presence of outliers and sensor noise, the registration is
formulated as a maximum a posteriori problem, and the resulting weighted least
squares objective is solved by the iteratively re-weighted least squares
method. A variety of robust weight functions are investigated and the optimal
choice is made based on the statistics of the residual errors. Efficiency is
furthermore boosted by an adaptively sampled definition of the nearest
neighbour fields. Extensive evaluations on public SLAM benchmark sequences
demonstrate state-of-the-art performance and an advantage over classical
Euclidean distance fields.
</p>
<a href="http://arxiv.org/abs/2012.08228" target="_blank">arXiv:2012.08228</a> [<a href="http://arxiv.org/pdf/2012.08228" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Global Factors in Deep Generative Models. (arXiv:2012.08234v1 [cs.LG])</h2>
<h3>Ignacio Peis, Pablo M. Olmos, Antonio Art&#xe9;s-Rodr&#xed;guez</h3>
<p>We present a novel deep generative model based on non i.i.d. variational
autoencoders that captures global dependencies among observations in a fully
unsupervised fashion. In contrast to the recent semi-supervised alternatives
for global modeling in deep generative models, our approach combines a mixture
model in the local or data-dependent space and a global Gaussian latent
variable, which lead us to obtain three particular insights. First, the induced
latent global space captures interpretable disentangled representations with no
user-defined regularization in the evidence lower bound (as in $\beta$-VAE and
its generalizations). Second, we show that the model performs domain alignment
to find correlations and interpolate between different databases. Finally, we
study the ability of the global space to discriminate between groups of
observations with non-trivial underlying structures, such as face images with
shared attributes or defined sequences of digits images.
</p>
<a href="http://arxiv.org/abs/2012.08234" target="_blank">arXiv:2012.08234</a> [<a href="http://arxiv.org/pdf/2012.08234" target="_blank">pdf</a>]

<h2>Point-Level Temporal Action Localization: Bridging Fully-supervised Proposals to Weakly-supervised Losses. (arXiv:2012.08236v1 [cs.CV])</h2>
<h3>Chen Ju, Peisen Zhao, Ya Zhang, Yanfeng Wang, Qi Tian</h3>
<p>Point-Level temporal action localization (PTAL) aims to localize actions in
untrimmed videos with only one timestamp annotation for each action instance.
Existing methods adopt the frame-level prediction paradigm to learn from the
sparse single-frame labels. However, such a framework inevitably suffers from a
large solution space. This paper attempts to explore the proposal-based
prediction paradigm for point-level annotations, which has the advantage of
more constrained solution space and consistent predictions among neighboring
frames. The point-level annotations are first used as the keypoint supervision
to train a keypoint detector. At the location prediction stage, a simple but
effective mapper module, which enables back-propagation of training errors, is
then introduced to bridge the fully-supervised framework with weak supervision.
To our best of knowledge, this is the first work to leverage the
fully-supervised paradigm for the point-level setting. Experiments on THUMOS14,
BEOID, and GTEA verify the effectiveness of our proposed method both
quantitatively and qualitatively, and demonstrate that our method outperforms
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.08236" target="_blank">arXiv:2012.08236</a> [<a href="http://arxiv.org/pdf/2012.08236" target="_blank">pdf</a>]

<h2>Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?. (arXiv:2012.08240v1 [cs.LG])</h2>
<h3>Antoine Grosnit, Alexander I. Cowen-Rivers, Rasul Tutunov, Ryan-Rhys Griffiths, Jun Wang, Haitham Bou-Ammar</h3>
<p>Bayesian optimisation presents a sample-efficient methodology for global
optimisation. Within this framework, a crucial performance-determining
subroutine is the maximisation of the acquisition function, a task complicated
by the fact that acquisition functions tend to be non-convex and thus
nontrivial to optimise. In this paper, we undertake a comprehensive empirical
study of approaches to maximise the acquisition function. Additionally, by
deriving novel, yet mathematically equivalent, compositional forms for popular
acquisition functions, we recast the maximisation task as a compositional
optimisation problem, allowing us to benefit from the extensive literature in
this field. We highlight the empirical advantages of the compositional approach
to acquisition function maximisation across 3958 individual experiments
comprising synthetic optimisation tasks as well as tasks from the 2020 NeurIPS
competition on Black-Box Optimisation for Machine Learning. Given the
generality of the acquisition function maximisation subroutine, we posit that
the adoption of compositional optimisers has the potential to yield performance
improvements across all domains in which Bayesian optimisation is currently
being applied.
</p>
<a href="http://arxiv.org/abs/2012.08240" target="_blank">arXiv:2012.08240</a> [<a href="http://arxiv.org/pdf/2012.08240" target="_blank">pdf</a>]

<h2>CosSGD: Nonlinear Quantization for Communication-efficient Federated Learning. (arXiv:2012.08241v1 [cs.LG])</h2>
<h3>Yang He, Maximilian Zenk, Mario Fritz</h3>
<p>Federated learning facilitates learning across clients without transferring
local data on these clients to a central server. Despite the success of the
federated learning method, it remains to improve further w.r.t communicating
the most critical information to update a model under limited communication
conditions, which can benefit this learning scheme into a wide range of
application scenarios. In this work, we propose a nonlinear quantization for
compressed stochastic gradient descent, which can be easily utilized in
federated learning. Based on the proposed quantization, our system
significantly reduces the communication cost by up to three orders of
magnitude, while maintaining convergence and accuracy of the training process
to a large extent. Extensive experiments are conducted on image classification
and brain tumor semantic segmentation using the MNIST, CIFAR-10 and BraTS
datasets where we show state-of-the-art effectiveness and impressive
communication efficiency.
</p>
<a href="http://arxiv.org/abs/2012.08241" target="_blank">arXiv:2012.08241</a> [<a href="http://arxiv.org/pdf/2012.08241" target="_blank">pdf</a>]

<h2>Robust Factorization Methods Using a Gaussian/Uniform Mixture Model. (arXiv:2012.08243v1 [cs.CV])</h2>
<h3>Andrei Zaharescu, Radu Horaud</h3>
<p>In this paper we address the problem of building a class of robust
factorization algorithms that solve for the shape and motion parameters with
both affine (weak perspective) and perspective camera models. We introduce a
Gaussian/uniform mixture model and its associated EM algorithm. This allows us
to address robust parameter estimation within a data clustering approach. We
propose a robust technique that works with any affine factorization method and
makes it robust to outliers. In addition, we show how such a framework can be
further embedded into an iterative perspective factorization scheme. We carry
out a large number of experiments to validate our algorithms and to compare
them with existing ones. We also compare our approach with factorization
methods that use M-estimators.
</p>
<a href="http://arxiv.org/abs/2012.08243" target="_blank">arXiv:2012.08243</a> [<a href="http://arxiv.org/pdf/2012.08243" target="_blank">pdf</a>]

<h2>Geometry Enhancements from Visual Content: Going Beyond Ground Truth. (arXiv:2012.08248v1 [cs.CV])</h2>
<h3>Liran Azaria, Dan Raviv</h3>
<p>This work presents a new cyclic architecture that extracts high-frequency
patterns from images and re-insert them as geometric features. This procedure
allows us to enhance the resolution of low-cost depth sensors capturing fine
details on the one hand and being loyal to the scanned ground truth on the
other. We present state-of-the-art results for depth super-resolution tasks and
as well as visually attractive, enhanced generated 3D models.
</p>
<a href="http://arxiv.org/abs/2012.08248" target="_blank">arXiv:2012.08248</a> [<a href="http://arxiv.org/pdf/2012.08248" target="_blank">pdf</a>]

<h2>Rigid chain in parallel kinematic positioning system. (arXiv:2012.08250v1 [cs.RO])</h2>
<h3>M Kubrikov, I Pikalov, M Saramud</h3>
<p>The article presents an analysis of the trends in the development of
kinematic structures of modern machine-building technological equipment. The
prospects of using machines with parallel kinematics in processing, measuring
and handling equipment, their advantages and disadvantages are demonstrated. It
is shown that it is inexpedient to use ball screw drives in machines with
parallel kinematics, performing tasks of low accuracy, but with displacements
of more than 3000 mm. The rigid chain system of the Serapid firm and the
possibility of its use in machines with parallel kinematics are considered. A
schematic solution of a three-coordinate manipulation robot based on parallel
kinematics with drive mechanisms on rigid chains is proposed.
</p>
<a href="http://arxiv.org/abs/2012.08250" target="_blank">arXiv:2012.08250</a> [<a href="http://arxiv.org/pdf/2012.08250" target="_blank">pdf</a>]

<h2>HeadGAN: Video-and-Audio-Driven Talking Head Synthesis. (arXiv:2012.08261v1 [cs.CV])</h2>
<h3>Michail Christos Doukas, Stefanos Zafeiriou, Viktoriia Sharmanska</h3>
<p>Recent attempts to solve the problem of talking head synthesis using a single
reference image have shown promising results. However, most of them fail to
meet the identity preservation problem, or perform poorly in terms of
photo-realism, especially in extreme head poses. We propose HeadGAN, a novel
reenactment approach that conditions synthesis on 3D face representations,
which can be extracted from any driving video and adapted to the facial
geometry of any source. We improve the plausibility of mouth movements, by
utilising audio features as a complementary input to the Generator.
Quantitative and qualitative experiments demonstrate the merits of our
approach.
</p>
<a href="http://arxiv.org/abs/2012.08261" target="_blank">arXiv:2012.08261</a> [<a href="http://arxiv.org/pdf/2012.08261" target="_blank">pdf</a>]

<h2>*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task. (arXiv:2012.08266v1 [cs.LG])</h2>
<h3>Dmitry Tsarkov, Tibor Tihon, Nathan Scales, Nikola Momchev, Danila Sinopalnikov, Nathanael Sch&#xe4;rli</h3>
<p>We present *-CFQ ("star-CFQ"): a suite of large-scale datasets of varying
scope based on the CFQ semantic parsing benchmark, designed for principled
investigation of the scalability of machine learning systems in a realistic
compositional task setting. Using this suite, we conduct a series of
experiments investigating the ability of Transformers to benefit from increased
training size under conditions of fixed computational cost. We show that
compositional generalization remains a challenge at all training sizes, and we
show that increasing the scope of natural language leads to consistently higher
error rates, which are only partially offset by increased training data. We
further show that while additional training data from a related domain improves
the accuracy in data-starved situations, this improvement is limited and
diminishes as the distance from the related domain to the target domain
increases.
</p>
<a href="http://arxiv.org/abs/2012.08266" target="_blank">arXiv:2012.08266</a> [<a href="http://arxiv.org/pdf/2012.08266" target="_blank">pdf</a>]

<h2>FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for Monocular Depth Completion. (arXiv:2012.08270v1 [cs.CV])</h2>
<h3>Lina Liu, Xibin Song, Xiaoyang Lyu, Junwei Diao, Mengmeng Wang, Yong Liu, Liangjun Zhang</h3>
<p>Depth completion aims to recover a dense depth map from a sparse depth map
with the corresponding color image as input. Recent approaches mainly formulate
the depth completion as a one-stage end-to-end learning task, which outputs
dense depth maps directly. However, the feature extraction and supervision in
one-stage frameworks are insufficient, limiting the performance of these
approaches. To address this problem, we propose a novel end-to-end residual
learning framework, which formulates the depth completion as a two-stage
learning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage.
First, a coarse dense depth map is obtained by a simple CNN framework. Then, a
refined depth map is further obtained using a residual learning strategy in the
coarse-to-fine stage with coarse depth map and color image as input. Specially,
in the coarse-to-fine stage, a channel shuffle extraction operation is utilized
to extract more representative features from color image and coarse depth map,
and an energy based fusion operation is exploited to effectively fuse these
features obtained by channel shuffle operation, thus leading to more accurate
and refined depth maps. We achieve SoTA performance in RMSE on KITTI benchmark.
Extensive experiments on other datasets future demonstrate the superiority of
our approach over current state-of-the-art depth completion approaches.
</p>
<a href="http://arxiv.org/abs/2012.08270" target="_blank">arXiv:2012.08270</a> [<a href="http://arxiv.org/pdf/2012.08270" target="_blank">pdf</a>]

<h2>Artificial Dummies for Urban Dataset Augmentation. (arXiv:2012.08274v1 [cs.CV])</h2>
<h3>Anton&#xed;n Vobeck&#xfd;, David Hurych, Michal U&#x159;i&#x10d;&#xe1;&#x159;, Patrick P&#xe9;rez, Josef &#x160;ivic</h3>
<p>Existing datasets for training pedestrian detectors in images suffer from
limited appearance and pose variation. The most challenging scenarios are
rarely included because they are too difficult to capture due to safety
reasons, or they are very unlikely to happen. The strict safety requirements in
assisted and autonomous driving applications call for an extra high detection
accuracy also in these rare situations. Having the ability to generate people
images in arbitrary poses, with arbitrary appearances and embedded in different
background scenes with varying illumination and weather conditions, is a
crucial component for the development and testing of such applications. The
contributions of this paper are three-fold. First, we describe an augmentation
method for controlled synthesis of urban scenes containing people, thus
producing rare or never-seen situations. This is achieved with a data generator
(called DummyNet) with disentangled control of the pose, the appearance, and
the target background scene. Second, the proposed generator relies on novel
network architecture and associated loss that takes into account the
segmentation of the foreground person and its composition into the background
scene. Finally, we demonstrate that the data generated by our DummyNet improve
performance of several existing person detectors across various datasets as
well as in challenging situations, such as night-time conditions, where only a
limited amount of training data is available. In the setup with only day-time
data available, we improve the night-time detector by $17\%$ log-average miss
rate over the detector trained with the day-time data only.
</p>
<a href="http://arxiv.org/abs/2012.08274" target="_blank">arXiv:2012.08274</a> [<a href="http://arxiv.org/pdf/2012.08274" target="_blank">pdf</a>]

<h2>High throughput screening with machine learning. (arXiv:2012.08275v1 [cs.LG])</h2>
<h3>Oleksandr Gurbych, Maksym Druchok, Dzvenymyra Yarish, Sofiya Garkot</h3>
<p>This study assesses the efficiency of several popular machine learning
approaches in the prediction of molecular binding affinity: CatBoost, Graph
Attention Neural Network, and Bidirectional Encoder Representations from
Transformers. The models were trained to predict binding affinities in terms of
inhibition constants $K_i$ for pairs of proteins and small organic molecules.
First two approaches use thoroughly selected physico-chemical features, while
the third one is based on textual molecular representations - it is one of the
first attempts to apply Transformer-based predictors for the binding affinity.
We also discuss the visualization of attention layers within the Transformer
approach in order to highlight the molecular sites responsible for
interactions. All approaches are free from atomic spatial coordinates thus
avoiding bias from known structures and being able to generalize for compounds
with unknown conformations. The achieved accuracy for all suggested approaches
prove their potential in high throughput screening.
</p>
<a href="http://arxiv.org/abs/2012.08275" target="_blank">arXiv:2012.08275</a> [<a href="http://arxiv.org/pdf/2012.08275" target="_blank">pdf</a>]

<h2>Cluster, Split, Fuse, and Update: Meta-Learning for Open Compound Domain Adaptive Semantic Segmentation. (arXiv:2012.08278v1 [cs.CV])</h2>
<h3>Rui Gong, Yuhua Chen, Danda Pani Paudel, Yawei Li, Ajad Chhatkuli, Wen Li, Dengxin Dai, Luc Van Gool</h3>
<p>Open compound domain adaptation (OCDA) is a domain adaptation setting, where
target domain is modeled as a compound of multiple unknown homogeneous domains,
which brings the advantage of improved generalization to unseen domains. In
this work, we propose a principled meta-learning based approach to OCDA for
semantic segmentation, MOCDA, by modeling the unlabeled target domain
continuously. Our approach consists of four key steps. First, we cluster target
domain into multiple sub-target domains by image styles, extracted in an
unsupervised manner. Then, different sub-target domains are split into
independent branches, for which batch normalization parameters are learnt to
treat them independently. A meta-learner is thereafter deployed to learn to
fuse sub-target domain-specific predictions, conditioned upon the style code.
Meanwhile, we learn to online update the model by model-agnostic meta-learning
(MAML) algorithm, thus to further improve generalization. We validate the
benefits of our approach by extensive experiments on synthetic-to-real
knowledge transfer benchmark datasets, where we achieve the state-of-the-art
performance in both compound and open domains.
</p>
<a href="http://arxiv.org/abs/2012.08278" target="_blank">arXiv:2012.08278</a> [<a href="http://arxiv.org/pdf/2012.08278" target="_blank">pdf</a>]

<h2>Robots Understanding Contextual Information in Human-Centered Environments using Weakly Supervised Mask Data Distillation. (arXiv:2012.08282v1 [cs.CV])</h2>
<h3>Daniel Dworakowski, Goldie Nejat</h3>
<p>Contextual information in human environments, such as signs, symbols, and
objects provide important information for robots to use for exploration and
navigation. To identify and segment contextual information from complex images
obtained in these environments, data-driven methods such as Convolutional
Neural Networks (CNNs) are used. However, these methods require large amounts
of human labeled data which are slow and time-consuming to obtain. Weakly
supervised methods address this limitation by generating pseudo segmentation
labels (PSLs). In this paper, we present the novel Weakly Supervised Mask Data
Distillation (WeSuperMaDD) architecture for autonomously generating PSLs using
CNNs not specifically trained for the task of context segmentation; i.e., CNNs
trained for object classification, image captioning, etc. WeSuperMaDD uniquely
generates PSLs using learned image features from sparse and limited diversity
data; common in robot navigation tasks in human-centred environments (malls,
grocery stores). Our proposed architecture uses a new mask refinement system
which automatically searches for the PSL with the fewest foreground pixels that
satisfies cost constraints. This removes the need for handcrafted heuristic
rules. Extensive experiments successfully validated the performance of
WeSuperMaDD in generating PSLs for datasets with text of various scales, fonts,
and perspectives in multiple indoor/outdoor environments. A comparison with
Naive, GrabCut, and Pyramid methods found a significant improvement in label
and segmentation quality. Moreover, a context segmentation CNN trained using
the WeSuperMaDD architecture achieved measurable improvements in accuracy
compared to one trained with Naive PSLs. Our method also had comparable
performance to existing state-of-the-art text detection and segmentation
methods on real datasets without requiring segmentation labels for training.
</p>
<a href="http://arxiv.org/abs/2012.08282" target="_blank">arXiv:2012.08282</a> [<a href="http://arxiv.org/pdf/2012.08282" target="_blank">pdf</a>]

<h2>BiSNN: Training Spiking Neural Networks with Binary Weights via Bayesian Learning. (arXiv:2012.08300v1 [cs.LG])</h2>
<h3>Hyeryung Jang, Nicolas Skatchkovsky, Osvaldo Simeone</h3>
<p>Artificial Neural Network (ANN)-based inference on battery-powered devices
can be made more energy-efficient by restricting the synaptic weights to be
binary, hence eliminating the need to perform multiplications. An alternative,
emerging, approach relies on the use of Spiking Neural Networks (SNNs),
biologically inspired, dynamic, event-driven models that enhance energy
efficiency via the use of binary, sparse, activations. In this paper, an SNN
model is introduced that combines the benefits of temporally sparse binary
activations and of binary weights. Two learning rules are derived, the first
based on the combination of straight-through and surrogate gradient techniques,
and the second based on a Bayesian paradigm. Experiments validate the
performance loss with respect to full-precision implementations, and
demonstrate the advantage of the Bayesian paradigm in terms of accuracy and
calibration.
</p>
<a href="http://arxiv.org/abs/2012.08300" target="_blank">arXiv:2012.08300</a> [<a href="http://arxiv.org/pdf/2012.08300" target="_blank">pdf</a>]

<h2>QUARC: Quaternion Multi-Modal Fusion Architecture For Hate Speech Classification. (arXiv:2012.08312v1 [cs.LG])</h2>
<h3>Deepak Kumar, Nalin Kumar, Subhankar Mishra</h3>
<p>Hate speech, quite common in the age of social media, at times harmless but
can also cause mental trauma to someone or even riots in communities. Image of
a religious symbol with derogatory comment or video of a man abusing a
particular community, all become hate speech with its every modality (such as
text, image, and audio) contributing towards it. Models based on a particular
modality of hate speech post on social media are not useful, rather, we need
models like multi-modal fusion models that consider both image and text while
classifying hate speech. Text-image fusion models are heavily parameterized,
hence we propose a quaternion neural network-based model having additional
fusion components for each pair of modalities. The model is tested on the
MMHS150K twitter dataset for hate speech classification. The model shows an
almost 75% reduction in parameters and also benefits us in terms of storage
space and training time while being at par in terms of performance as compared
to its real counterpart.
</p>
<a href="http://arxiv.org/abs/2012.08312" target="_blank">arXiv:2012.08312</a> [<a href="http://arxiv.org/pdf/2012.08312" target="_blank">pdf</a>]

<h2>Improved Image Matting via Real-time User Clicks and Uncertainty Estimation. (arXiv:2012.08323v1 [cs.CV])</h2>
<h3>Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Hanqing Zhao, Weiming Zhang, Nenghai Yu</h3>
<p>Image matting is a fundamental and challenging problem in computer vision and
graphics. Most existing matting methods leverage a user-supplied trimap as an
auxiliary input to produce good alpha matte. However, obtaining high-quality
trimap itself is arduous, thus restricting the application of these methods.
Recently, some trimap-free methods have emerged, however, the matting quality
is still far behind the trimap-based methods. The main reason is that, without
the trimap guidance in some cases, the target network is ambiguous about which
is the foreground target. In fact, choosing the foreground is a subjective
procedure and depends on the user's intention. To this end, this paper proposes
an improved deep image matting framework which is trimap-free and only needs
several user click interactions to eliminate the ambiguity. Moreover, we
introduce a new uncertainty estimation module that can predict which parts need
polishing and a following local refinement module. Based on the computation
budget, users can choose how many local parts to improve with the uncertainty
guidance. Quantitative and qualitative results show that our method performs
better than existing trimap-free methods and comparably to state-of-the-art
trimap-based methods with minimal user effort.
</p>
<a href="http://arxiv.org/abs/2012.08323" target="_blank">arXiv:2012.08323</a> [<a href="http://arxiv.org/pdf/2012.08323" target="_blank">pdf</a>]

<h2>Masksembles for Uncertainty Estimation. (arXiv:2012.08334v1 [cs.LG])</h2>
<h3>Nikita Durasov, Timur Bagautdinov, Pierre Baque, Pascal Fua</h3>
<p>Deep neural networks have amply demonstrated their prowess but estimating the
reliability of their predictions remains challenging. Deep Ensembles are widely
considered as being one of the best methods for generating uncertainty
estimates but are very expensive to train and evaluate. MC-Dropout is another
popular alternative, which is less expensive, but also less reliable. Our
central intuition is that there is a continuous spectrum of ensemble-like
models of which MC-Dropout and Deep Ensembles are extreme examples. The first
uses an effectively infinite number of highly correlated models while the
second relies on a finite number of independent models.

To combine the benefits of both, we introduce Masksembles. Instead of
randomly dropping parts of the network as in MC-dropout, Masksemble relies on a
fixed number of binary masks, which are parameterized in a way that allows to
change correlations between individual models. Namely, by controlling the
overlap between the masks and their density one can choose the optimal
configuration for the task at hand. This leads to a simple and easy to
implement method with performance on par with Ensembles at a fraction of the
cost. We experimentally validate Masksembles on two widely used datasets,
CIFAR10 and ImageNet.
</p>
<a href="http://arxiv.org/abs/2012.08334" target="_blank">arXiv:2012.08334</a> [<a href="http://arxiv.org/pdf/2012.08334" target="_blank">pdf</a>]

<h2>Cost-Effective Federated Learning Design. (arXiv:2012.08336v1 [cs.LG])</h2>
<h3>Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, Leandros Tassiulas</h3>
<p>Federated learning (FL) is a distributed learning paradigm that enables a
large number of devices to collaboratively learn a model without sharing their
raw data. Despite its practical efficiency and effectiveness, the iterative
on-device learning process incurs a considerable cost in terms of learning time
and energy consumption, which depends crucially on the number of selected
clients and the number of local iterations in each training round. In this
paper, we analyze how to design adaptive FL that optimally chooses these
essential control variables to minimize the total cost while ensuring
convergence. Theoretically, we analytically establish the relationship between
the total cost and the control variables with the convergence upper bound. To
efficiently solve the cost minimization problem, we develop a low-cost
sampling-based algorithm to learn the convergence related unknown parameters.
We derive important solution properties that effectively identify the design
principles for different metric preferences. Practically, we evaluate our
theoretical results both in a simulated environment and on a hardware
prototype. Experimental evidence verifies our derived properties and
demonstrates that our proposed solution achieves near-optimal performance for
various datasets, different machine learning models, and heterogeneous system
settings.
</p>
<a href="http://arxiv.org/abs/2012.08336" target="_blank">arXiv:2012.08336</a> [<a href="http://arxiv.org/pdf/2012.08336" target="_blank">pdf</a>]

<h2>Practical Auto-Calibration for Spatial Scene-Understanding from Crowdsourced Dashcamera Videos. (arXiv:2012.08375v1 [cs.CV])</h2>
<h3>Hemang Chawla, Matti Jukola, Shabbir Marzban, Elahe Arani, Bahram Zonooz</h3>
<p>Spatial scene-understanding, including dense depth and ego-motion estimation,
is an important problem in computer vision for autonomous vehicles and advanced
driver assistance systems. Thus, it is beneficial to design perception modules
that can utilize crowdsourced videos collected from arbitrary vehicular onboard
or dashboard cameras. However, the intrinsic parameters corresponding to such
cameras are often unknown or change over time. Typical manual calibration
approaches require objects such as a chessboard or additional scene-specific
information. On the other hand, automatic camera calibration does not have such
requirements. Yet, the automatic calibration of dashboard cameras is
challenging as forward and planar navigation results in critical motion
sequences with reconstruction ambiguities. Structure reconstruction of complete
visual-sequences that may contain tens of thousands of images is also
computationally untenable. Here, we propose a system for practical monocular
onboard camera auto-calibration from crowdsourced videos. We show the
effectiveness of our proposed system on the KITTI raw, Oxford RobotCar, and the
crowdsourced D$^2$-City datasets in varying conditions. Finally, we demonstrate
its application for accurate monocular dense depth and ego-motion estimation on
uncalibrated videos.
</p>
<a href="http://arxiv.org/abs/2012.08375" target="_blank">arXiv:2012.08375</a> [<a href="http://arxiv.org/pdf/2012.08375" target="_blank">pdf</a>]

<h2>mDALU: Multi-Source Domain Adaptation and Label Unification with Partial Datasets. (arXiv:2012.08385v1 [cs.CV])</h2>
<h3>Rui Gong, Dengxin Dai, Yuhua Chen, Wen Li, Luc Van Gool</h3>
<p>Object recognition advances very rapidly these days. One challenge is to
generalize existing methods to new domains, to more classes and/or to new data
modalities. In order to avoid annotating one dataset for each of these new
cases, one needs to combine and reuse existing datasets that may belong to
different domains, have partial annotations, and/or have different data
modalities. This paper treats this task as a multi-source domain adaptation and
label unification (mDALU) problem and proposes a novel method for it. Our
method consists of a partially-supervised adaptation stage and a
fully-supervised adaptation stage. In the former, partial knowledge is
transferred from multiple source domains to the target domain and fused
therein. Negative transfer between unmatched label space is mitigated via three
new modules: domain attention, uncertainty maximization and attention-guided
adversarial alignment. In the latter, knowledge is transferred in the unified
label space after a label completion process with pseudo-labels. We verify the
method on three different tasks, image classification, 2D semantic image
segmentation, and joint 2D-3D semantic segmentation. Extensive experiments show
that our method outperforms all competing methods significantly.
</p>
<a href="http://arxiv.org/abs/2012.08385" target="_blank">arXiv:2012.08385</a> [<a href="http://arxiv.org/pdf/2012.08385" target="_blank">pdf</a>]

<h2>FINED: Fast Inference Network for Edge Detection. (arXiv:2012.08392v1 [cs.CV])</h2>
<h3>Jan Kristanto Wibisono, Hsueh-Ming Hang</h3>
<p>In this paper, we address the design of lightweight deep learning-based edge
detection. The deep learning technology offers a significant improvement on the
edge detection accuracy. However, typical neural network designs have very high
model complexity, which prevents it from practical usage. In contrast, we
propose a Fast Inference Network for Edge Detection (FINED), which is a
lightweight neural net dedicated to edge detection. By carefully choosing
proper components for edge detection purpose, we can achieve the
state-of-the-art accuracy in edge detection while significantly reducing its
complexity. Another key contribution in increasing the inferencing speed is
introducing the training helper concept. The extra subnetworks (training
helper) are employed in training but not used in inferencing. It can further
reduce the model complexity and yet maintain the same level of accuracy. Our
experiments show that our systems outperform all the current edge detectors at
about the same model (parameter) size.
</p>
<a href="http://arxiv.org/abs/2012.08392" target="_blank">arXiv:2012.08392</a> [<a href="http://arxiv.org/pdf/2012.08392" target="_blank">pdf</a>]

<h2>Exploring Vicinal Risk Minimization for Lightweight Out-of-Distribution Detection. (arXiv:2012.08398v1 [cs.LG])</h2>
<h3>Deepak Ravikumar, Sangamesh Kodge, Isha Garg, Kaushik Roy</h3>
<p>Deep neural networks have found widespread adoption in solving complex tasks
ranging from image recognition to natural language processing. However, these
networks make confident mispredictions when presented with data that does not
belong to the training distribution, i.e. out-of-distribution (OoD) samples. In
this paper we explore whether the property of Vicinal Risk Minimization (VRM)
to smoothly interpolate between different class boundaries helps to train
better OoD detectors. We apply VRM to existing OoD detection techniques and
show their improved performance. We observe that existing OoD detectors have
significant memory and compute overhead, hence we leverage VRM to develop an
OoD detector with minimal overheard. Our detection method introduces an
auxiliary class for classifying OoD samples. We utilize mixup in two ways to
implement Vicinal Risk Minimization. First, we perform mixup within the same
class and second, we perform mixup with Gaussian noise when training the
auxiliary class. Our method achieves near competitive performance with
significantly less compute and memory overhead when compared to existing OoD
detection techniques. This facilitates the deployment of OoD detection on edge
devices and expands our understanding of Vicinal Risk Minimization for use in
training OoD detectors.
</p>
<a href="http://arxiv.org/abs/2012.08398" target="_blank">arXiv:2012.08398</a> [<a href="http://arxiv.org/pdf/2012.08398" target="_blank">pdf</a>]

<h2>Artificial Neural Networks for Sensor Data Classification on Small Embedded Systems. (arXiv:2012.08403v1 [cs.LG])</h2>
<h3>Marcus Venzke, Daniel Klisch, Philipp Kubik, Asad Ali, Jesper Dell Missier, Volker Turau</h3>
<p>In this paper we investigate the usage of machine learning for interpreting
measured sensor values in sensor modules. In particular we analyze the
potential of artificial neural networks (ANNs) on low-cost micro-controllers
with a few kilobytes of memory to semantically enrich data captured by sensors.
The focus is on classifying temporal data series with a high level of
reliability. Design and implementation of ANNs are analyzed considering Feed
Forward Neural Networks (FFNNs) and Recurrent Neural Networks (RNNs). We
validate the developed ANNs in a case study of optical hand gesture recognition
on an 8-bit micro-controller. The best reliability was found for an FFNN with
two layers and 1493 parameters requiring an execution time of 36 ms. We propose
a workflow to develop ANNs for embedded devices.
</p>
<a href="http://arxiv.org/abs/2012.08403" target="_blank">arXiv:2012.08403</a> [<a href="http://arxiv.org/pdf/2012.08403" target="_blank">pdf</a>]

<h2>Deep Learning Based Classification of Unsegmented Phonocardiogram Spectrograms Leveraging Transfer Learning. (arXiv:2012.08406v1 [cs.LG])</h2>
<h3>Kaleem Nawaz Khan, Faiq Ahmad Khan, Anam Abid, Tamer Olmez, Zumray Dokur, Amith Khandakar, Muhammad E. H. Chowdhury, Muhammad Salman Khan</h3>
<p>Cardiovascular diseases (CVDs) are the main cause of deaths all over the
world. Heart murmurs are the most common abnormalities detected during the
auscultation process. The two widely used publicly available phonocardiogram
(PCG) datasets are from the PhysioNet/CinC (2016) and PASCAL (2011) challenges.
The datasets are significantly different in terms of the tools used for data
acquisition, clinical protocols, digital storages and signal qualities, making
it challenging to process and analyze. In this work, we have used short-time
Fourier transform (STFT) based spectrograms to learn the representative
patterns of the normal and abnormal PCG signals. Spectrograms generated from
both the datasets are utilized to perform three different studies: (i) train,
validate and test different variants of convolutional neural network (CNN)
models with PhysioNet dataset, (ii) train, validate and test the best
performing CNN structure on combined PhysioNet-PASCAL dataset and (iii)
finally, transfer learning technique is employed to train the best performing
pre-trained network from the first study with PASCAL dataset. We propose a
novel, less complex and relatively light custom CNN model for the
classification of PhysioNet, combined and PASCAL datasets. The first study
achieves an accuracy, sensitivity, specificity, precision and F1 score of
95.4%, 96.3%, 92.4%, 97.6% and 96.98% respectively while the second study shows
accuracy, sensitivity, specificity, precision and F1 score of 94.2%, 95.5%,
90.3%, 96.8% and 96.1% respectively. Finally, the third study shows a precision
of 98.29% on the noisy PASCAL dataset with transfer learning approach. All the
three proposed approaches outperform most of the recent competing studies by
achieving comparatively high classification accuracy and precision, which make
them suitable for screening CVDs using PCG signals.
</p>
<a href="http://arxiv.org/abs/2012.08406" target="_blank">arXiv:2012.08406</a> [<a href="http://arxiv.org/pdf/2012.08406" target="_blank">pdf</a>]

<h2>SPOC learner's final grade prediction based on a novel sampling batch normalization embedded neural network method. (arXiv:2012.08408v1 [cs.CV])</h2>
<h3>Zhuonan Liang, Ziheng Liu, Huaze Shi, Yunlong Chen, Yanbin Cai, Yating Liang, Yafan Feng, Yuqing Yang, Jing Zhang, Peng Fu</h3>
<p>Recent years have witnessed the rapid growth of Small Private Online Courses
(SPOC) which is able to highly customized and personalized to adapt variable
educational requests, in which machine learning techniques are explored to
summarize and predict the learner's performance, mostly focus on the final
grade. However, the problem is that the final grade of learners on SPOC is
generally seriously imbalance which handicaps the training of prediction model.
To solve this problem, a sampling batch normalization embedded deep neural
network (SBNEDNN) method is developed in this paper. First, a combined
indicator is defined to measure the distribution of the data, then a rule is
established to guide the sampling process. Second, the batch normalization (BN)
modified layers are embedded into full connected neural network to solve the
data imbalanced problem. Experimental results with other three deep learning
methods demonstrates the superiority of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.08408" target="_blank">arXiv:2012.08408</a> [<a href="http://arxiv.org/pdf/2012.08408" target="_blank">pdf</a>]

<h2>Pedestrian Behavior Prediction for Automated Driving: Requirements, Metrics, and Relevant Features. (arXiv:2012.08418v1 [cs.RO])</h2>
<h3>Michael Herman, J&#xf6;rg Wagner, Vishnu Prabhakaran, Nicolas M&#xf6;ser, Hanna Ziesche, Waleed Ahmed, Lutz B&#xfc;rkle, Ernst Kloppenburg, Claudius Gl&#xe4;ser</h3>
<p>Automated vehicles require a comprehensive understanding of traffic
situations to ensure safe and comfortable driving. In this context, the
prediction of pedestrians is particularly challenging as pedestrian behavior
can be influenced by multiple factors. In this paper, we thoroughly analyze the
requirements on pedestrian behavior prediction for automated driving via a
system-level approach: to this end we investigate real-world pedestrian-vehicle
interactions with human drivers. Based on human driving behavior we then derive
appropriate reaction patterns of an automated vehicle. Finally, requirements
for the prediction of pedestrians are determined. This also includes a novel
metric tailored to measure prediction performance from a system-level
perspective. Furthermore, we present a pedestrian prediction model based on a
Conditional Variational Auto-Encoder (CVAE) which incorporates multiple
contextual cues to achieve accurate long-term prediction. The CVAE shows
superior performance over a baseline prediction model, where prediction
performance was evaluated on a large-scale data set comprising thousands of
real-world pedestrian-vehicle-interactions. Finally, we investigate the impact
of different contextual cues on prediction performance via an ablation study
whose results can guide future research on the perception of relevant
pedestrian attributes.
</p>
<a href="http://arxiv.org/abs/2012.08418" target="_blank">arXiv:2012.08418</a> [<a href="http://arxiv.org/pdf/2012.08418" target="_blank">pdf</a>]

<h2>Detecting Invisible People. (arXiv:2012.08419v1 [cs.CV])</h2>
<h3>Tarasha Khurana, Achal Dave, Deva Ramanan</h3>
<p>Monocular object detection and tracking have improved drastically in recent
years, but rely on a key assumption: that objects are visible to the camera.
Many offline tracking approaches reason about occluded objects post-hoc, by
linking together tracklets after the object re-appears, making use of
reidentification (ReID). However, online tracking in embodied robotic agents
(such as a self-driving vehicle) fundamentally requires object permanence,
which is the ability to reason about occluded objects before they re-appear. In
this work, we re-purpose tracking benchmarks and propose new metrics for the
task of detecting invisible objects, focusing on the illustrative case of
people. We demonstrate that current detection and tracking systems perform
dramatically worse on this task. We introduce two key innovations to recover
much of this performance drop. We treat occluded object detection in temporal
sequences as a short-term forecasting challenge, bringing to bear tools from
dynamic sequence prediction. Second, we build dynamic models that explicitly
reason in 3D, making use of observations produced by state-of-the-art monocular
depth estimation networks. To our knowledge, ours is the first work to
demonstrate the effectiveness of monocular depth estimation for the task of
tracking and detecting occluded objects. Our approach strongly improves by
11.4% over the baseline in ablations and by 5.0% over the state-of-the-art in
F1 score.
</p>
<a href="http://arxiv.org/abs/2012.08419" target="_blank">arXiv:2012.08419</a> [<a href="http://arxiv.org/pdf/2012.08419" target="_blank">pdf</a>]

<h2>Exploring Neural Networks Quantization via Layer-Wise Quantization Analysis. (arXiv:2012.08420v1 [cs.LG])</h2>
<h3>Shachar Gluska, Mark Grobman</h3>
<p>Quantization is an essential step in the efficient deployment of deep
learning models and as such is an increasingly popular research topic. An
important practical aspect that is not addressed in the current literature is
how to analyze and fix fail cases where the use of quantization results in
excessive degradation. In this paper, we present a simple analytic framework
that breaks down overall degradation to its per layer contributions. We analyze
many common networks and observe that a layer's contribution is determined by
both intrinsic (local) factors - the distribution of the layer's weights and
activations - and extrinsic (global) factors having to do with the the
interaction with the rest of the layers. Layer-wise analysis of existing
quantization schemes reveals local fail-cases of existing techniques which are
not reflected when inspecting their overall performance. As an example, we
consider ResNext26 on which SoTA post-training quantization methods perform
poorly. We show that almost all of the degradation stems from a single layer.
The same analysis also allows for local fixes - applying a common weight
clipping heuristic only to this layer reduces degradation to a minimum while
applying the same heuristic globally results in high degradation. More
generally, layer-wise analysis allows for a more nuanced examination of how
quantization affects the network, enabling the design of better performing
schemes.
</p>
<a href="http://arxiv.org/abs/2012.08420" target="_blank">arXiv:2012.08420</a> [<a href="http://arxiv.org/pdf/2012.08420" target="_blank">pdf</a>]

<h2>Detection of Anomalies in a Time Series Data using InfluxDB and Python. (arXiv:2012.08439v1 [cs.LG])</h2>
<h3>Tochukwu John Anih, Chika Amadi Bede, Chima Festus Umeokpala</h3>
<p>Analysis of water and environmental data is an important aspect of many
intelligent water and environmental system applications where inference from
such analysis plays a significant role in decision making. Quite often these
data that are collected through sensible sensors can be anomalous due to
different reasons such as systems breakdown, malfunctioning of sensor
detectors, and more. Regardless of their root causes, such data severely affect
the results of the subsequent analysis. This paper demonstrates data cleaning
and preparation for time-series data and further proposes cost-sensitive
machine learning algorithms as a solution to detect anomalous data points in
time-series data. The following models: Logistic Regression, Random Forest,
Support Vector Machines have been modified to support the cost-sensitive
learning which penalizes misclassified samples thereby minimizing the total
misclassification cost. Our results showed that Random Forest outperformed the
rest of the models at predicting the positive class (i.e anomalies). Applying
predictive model improvement techniques like data oversampling seems to provide
little or no improvement to the Random Forest model. Interestingly, with
recursive feature elimination, we achieved a better model performance thereby
reducing the dimensions in the data. Finally, with Influxdb and Kapacitor the
data was ingested and streamed to generate new data points to further evaluate
the model performance on unseen data, this will allow for early recognition of
undesirable changes in the drinking water quality and will enable the water
supply companies to rectify on a timely basis whatever undesirable changes
abound.
</p>
<a href="http://arxiv.org/abs/2012.08439" target="_blank">arXiv:2012.08439</a> [<a href="http://arxiv.org/pdf/2012.08439" target="_blank">pdf</a>]

<h2>Strong overall error analysis for the training of artificial neural networks via random initializations. (arXiv:2012.08443v1 [cs.LG])</h2>
<h3>Arnulf Jentzen, Adrian Riekert</h3>
<p>Although deep learning based approximation algorithms have been applied very
successfully to numerous problems, at the moment the reasons for their
performance are not entirely understood from a mathematical point of view.
Recently, estimates for the convergence of the overall error have been obtained
in the situation of deep supervised learning, but with an extremely slow rate
of convergence. In this note we partially improve on these estimates. More
specifically, we show that the depth of the neural network only needs to
increase much slower in order to obtain the same rate of approximation. The
results hold in the case of an arbitrary stochastic optimization algorithm with
i.i.d.\ random initializations.
</p>
<a href="http://arxiv.org/abs/2012.08443" target="_blank">arXiv:2012.08443</a> [<a href="http://arxiv.org/pdf/2012.08443" target="_blank">pdf</a>]

<h2>Geometric Surface Image Prediction for Image Recognition Enhancement. (arXiv:2012.08451v1 [cs.CV])</h2>
<h3>Tanasai Sucontphunt</h3>
<p>This work presents a method to predict a geometric surface image from a
photograph to assist in image recognition. To recognize objects, several images
from different conditions are required for training a model or fine-tuning a
pre-trained model. In this work, a geometric surface image is introduced as a
better representation than its color image counterpart to overcome lighting
conditions. The surface image is predicted from a color image. To do so, the
geometric surface image together with its color photographs are firstly trained
with Generative Adversarial Networks (GAN) model. The trained generator model
is then used to predict the geometric surface image from the input color image.
The evaluation on a case study of an amulet recognition shows that the
predicted geometric surface images contain less ambiguity than their color
images counterpart under different lighting conditions and can be used
effectively for assisting in image recognition task.
</p>
<a href="http://arxiv.org/abs/2012.08451" target="_blank">arXiv:2012.08451</a> [<a href="http://arxiv.org/pdf/2012.08451" target="_blank">pdf</a>]

<h2>Molecular machine learning with conformer ensembles. (arXiv:2012.08452v1 [cs.LG])</h2>
<h3>Simon Axelrod, Rafael Gomez-Bombarelli</h3>
<p>Virtual screening can accelerate drug discovery by identifying top candidates
for experimental testing. Machine learning is a powerful method for screening,
as it can learn complex structure-property relationships from experimental data
and make rapid predictions over virtual libraries. Although molecules are
inherently three-dimensional and their biological action typically occurs
through supramolecular recognition, most machine learning approaches use a 2D
graph representation of molecules as input; few use 3D information, and none
take into account the ensemble of conformers accessible to a species. Here we
investigate whether the 3D information of multiple conformers can improve
molecular property prediction. We introduce a number of new 3D-based models
that can take multiple conformers as input to predict drug activity, and find
that they learn interpretable weights for each conformer. The new architectures
perform significantly better than 2D models, but their performance is just as
strong with a single conformer as with many. From this analysis we identify the
best 3D architecture and examine its predictions on species without
experimental data.
</p>
<a href="http://arxiv.org/abs/2012.08452" target="_blank">arXiv:2012.08452</a> [<a href="http://arxiv.org/pdf/2012.08452" target="_blank">pdf</a>]

<h2>TACTO: A Fast, Flexible and Open-source Simulator for High-Resolution Vision-based Tactile Sensors. (arXiv:2012.08456v1 [cs.RO])</h2>
<h3>Shaoxiong Wang, Mike Lambeta, Po-Wei Chou, Roberto Calandra</h3>
<p>Simulators perform an important role in prototyping, debugging and
benchmarking new advances in robotics and learning for control. Although many
physics engines exist, some aspects of the real-world are harder than others to
simulate. One of the aspects that have so far eluded accurate simulation is
touch sensing. To address this gap, we present TACTO -- a fast, flexible and
open-source simulator for vision-based tactile sensors. This simulator allows
to render realistic high-resolution touch readings at hundreds of frames per
second, and can be easily configured to simulate different vision-based tactile
sensors, including GelSight, DIGIT and OmniTact. In this paper, we detail the
principles that drove the implementation of TACTO and how they are reflected in
its architecture. We demonstrate TACTO on a perceptual task, by learning to
predict grasp stability using touch from 1 million grasps, and on a marble
manipulation control task. We believe that TACTO is a step towards the
widespread adoption of touch sensing in robotic applications, and to enable
machine learning practitioners interested in multi-modal learning and control.
TACTO is open-source at https://github.com/facebookresearch/tacto.
</p>
<a href="http://arxiv.org/abs/2012.08456" target="_blank">arXiv:2012.08456</a> [<a href="http://arxiv.org/pdf/2012.08456" target="_blank">pdf</a>]

<h2>Rule Extraction from Binary Neural Networks with Convolutional Rules for Model Validation. (arXiv:2012.08459v1 [cs.LG])</h2>
<h3>Sophie Burkhardt, Jannis Brugger, Nicolas Wagner, Zahra Ahmadi, Kristian Kersting, Stefan Kramer</h3>
<p>Most deep neural networks are considered to be black boxes, meaning their
output is hard to interpret. In contrast, logical expressions are considered to
be more comprehensible since they use symbols that are semantically close to
natural language instead of distributed representations. However, for
high-dimensional input data such as images, the individual symbols, i.e.
pixels, are not easily interpretable. We introduce the concept of first-order
convolutional rules, which are logical rules that can be extracted using a
convolutional neural network (CNN), and whose complexity depends on the size of
the convolutional filter and not on the dimensionality of the input. Our
approach is based on rule extraction from binary neural networks with
stochastic local search. We show how to extract rules that are not necessarily
short, but characteristic of the input, and easy to visualize. Our experiments
show that the proposed approach is able to model the functionality of the
neural network while at the same time producing interpretable logical rules.
</p>
<a href="http://arxiv.org/abs/2012.08459" target="_blank">arXiv:2012.08459</a> [<a href="http://arxiv.org/pdf/2012.08459" target="_blank">pdf</a>]

<h2>Neural Collapse with Cross-Entropy Loss. (arXiv:2012.08465v1 [cs.LG])</h2>
<h3>Jianfeng Lu, Stefan Steinerberger</h3>
<p>We consider the variational problem of cross-entropy loss with $n$ feature
vectors on a unit hypersphere in $\mathbb{R}^d$. We prove that when $d \geq n -
1$, the global minimum is given by the simplex equiangular tight frame, which
justifies the neural collapse behavior. We also show a connection with the
frame potential of Benedetto &amp; Fickus.
</p>
<a href="http://arxiv.org/abs/2012.08465" target="_blank">arXiv:2012.08465</a> [<a href="http://arxiv.org/pdf/2012.08465" target="_blank">pdf</a>]

<h2>Objective-Based Hierarchical Clustering of Deep Embedding Vectors. (arXiv:2012.08466v1 [cs.LG])</h2>
<h3>Stanislav Naumov, Grigory Yaroslavtsev, Dmitrii Avdiukhin</h3>
<p>We initiate a comprehensive experimental study of objective-based
hierarchical clustering methods on massive datasets consisting of deep
embedding vectors from computer vision and NLP applications. This includes a
large variety of image embedding (ImageNet, ImageNetV2, NaBirds), word
embedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from
several popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our
study includes datasets with up to $4.5$ million entries with embedding
dimensions up to $2048$.

In order to address the challenge of scaling up hierarchical clustering to
such large datasets we propose a new practical hierarchical clustering
algorithm B++&amp;C. It gives a 5%/20% improvement on average for the popular
Moseley-Wang (MW) / Cohen-Addad et al. (CKMM) objectives (normalized) compared
to a wide range of classic methods and recent heuristics. We also introduce a
theoretical algorithm B2SAT&amp;C which achieves a $0.74$-approximation for the
CKMM objective in polynomial time. This is the first substantial improvement
over the trivial $2/3$-approximation achieved by a random binary tree. Prior to
this work, the best poly-time approximation of $\approx 2/3 + 0.0004$ was due
to Charikar et al. (SODA'19).
</p>
<a href="http://arxiv.org/abs/2012.08466" target="_blank">arXiv:2012.08466</a> [<a href="http://arxiv.org/pdf/2012.08466" target="_blank">pdf</a>]

<h2>Bayes Meets Entailment and Prediction: Commonsense Reasoning with Non-monotonicity, Paraconsistency and Predictive Accuracy. (arXiv:2012.08479v1 [cs.AI])</h2>
<h3>Hiroyuki Kido, Keishi Okamoto</h3>
<p>The recent success of Bayesian methods in neuroscience and artificial
intelligence gives rise to the hypothesis that the brain is a Bayesian machine.
Since logic and learning are both practices of the human brain, it leads to
another hypothesis that there is a Bayesian interpretation underlying both
logical reasoning and machine learning. In this paper, we introduce a
generative model of logical consequence relations. It formalises the process of
how the truth value of a sentence is probabilistically generated from the
probability distribution over states of the world. We show that the generative
model characterises a classical consequence relation, paraconsistent
consequence relation and nonmonotonic consequence relation. In particular, the
generative model gives a new consequence relation that outperforms them in
reasoning with inconsistent knowledge. We also show that the generative model
gives a new classification algorithm that outperforms several representative
algorithms in predictive accuracy and complexity on the Kaggle Titanic dataset.
</p>
<a href="http://arxiv.org/abs/2012.08479" target="_blank">arXiv:2012.08479</a> [<a href="http://arxiv.org/pdf/2012.08479" target="_blank">pdf</a>]

<h2>Learning Aggregation Functions. (arXiv:2012.08482v1 [cs.LG])</h2>
<h3>Giovanni Pellegrini, Alessandro Tibo, Paolo Frasconi, Andrea Passerini, Manfred Jaeger</h3>
<p>Learning on sets is increasingly gaining attention in the machine learning
community, due to its widespread applicability. Typically, representations over
sets are computed by using fixed aggregation functions such as sum or maximum.
However, recent results showed that universal function representation by sum-
(or max-) decomposition requires either highly discontinuous (and thus poorly
learnable) mappings, or a latent dimension equal to the maximum number of
elements in the set. To mitigate this problem, we introduce LAF (Learning
Aggregation Functions), a learnable aggregator for sets of arbitrary
cardinality. LAF can approximate several extensively used aggregators (such as
average, sum, maximum) as well as more complex functions (e.g. variance and
skewness). We report experiments on semi-synthetic and real data showing that
LAF outperforms state-of-the-art sum- (max-) decomposition architectures such
as DeepSets and library-based architectures like Principal Neighborhood
Aggregation.
</p>
<a href="http://arxiv.org/abs/2012.08482" target="_blank">arXiv:2012.08482</a> [<a href="http://arxiv.org/pdf/2012.08482" target="_blank">pdf</a>]

<h2>Amazon SageMaker Autopilot: a white box AutoML solution at scale. (arXiv:2012.08483v1 [cs.LG])</h2>
<h3>Piali Das, Valerio Perrone, Nikita Ivkin, Tanya Bansal, Zohar Karnin, Huibin Shen, Iaroslav Shcherbatyi, Yotam Elor, Wilton Wu, Aida Zolic, Thibaut Lienart, Alex Tang, Amr Ahmed, Jean Baptiste Faddoul, Rodolphe Jenatton, Fela Winkelmolen, Philip Gautier, Leo Dirac, Andre Perunicic, Miroslav Miladinovic, Giovanni Zappella, C&#xe9;dric Archambeau, Matthias Seeger, Bhaskar Dutt, Laurence Rouesnel</h3>
<p>AutoML systems provide a black-box solution to machine learning problems by
selecting the right way of processing features, choosing an algorithm and
tuning the hyperparameters of the entire pipeline. Although these systems
perform well on many datasets, there is still a non-negligible number of
datasets for which the one-shot solution produced by each particular system
would provide sub-par performance. In this paper, we present Amazon SageMaker
Autopilot: a fully managed system providing an automated ML solution that can
be modified when needed. Given a tabular dataset and the target column name,
Autopilot identifies the problem type, analyzes the data and produces a diverse
set of complete ML pipelines including feature preprocessing and ML algorithms,
which are tuned to generate a leaderboard of candidate models. In the scenario
where the performance is not satisfactory, a data scientist is able to view and
edit the proposed ML pipelines in order to infuse their expertise and business
knowledge without having to revert to a fully manual solution. This paper
describes the different components %in the eco-system of Autopilot, emphasizing
the infrastructure choices that allow scalability, high quality models,
editable ML pipelines, consumption of artifacts of offline meta-learning, and a
convenient integration with the entire SageMaker suite allowing these trained
models to be used in a production setting.
</p>
<a href="http://arxiv.org/abs/2012.08483" target="_blank">arXiv:2012.08483</a> [<a href="http://arxiv.org/pdf/2012.08483" target="_blank">pdf</a>]

<h2>Indecision Modeling. (arXiv:2012.08485v1 [cs.AI])</h2>
<h3>Duncan C McElfresh, Lok Chan, Kenzie Doyle, Walter Sinnott-Armstrong, Vincent Conitzer, Jana Schaich Borg, John P Dickerson</h3>
<p>AI systems are often used to make or contribute to important decisions in a
growing range of applications, including criminal justice, hiring, and
medicine. Since these decisions impact human lives, it is important that the AI
systems act in ways which align with human values. Techniques for preference
modeling and social choice help researchers learn and aggregate peoples'
preferences, which are used to guide AI behavior; thus, it is imperative that
these learned preferences are accurate. These techniques often assume that
people are willing to express strict preferences over alternatives; which is
not true in practice. People are often indecisive, and especially so when their
decision has moral implications. The philosophy and psychology literature shows
that indecision is a measurable and nuanced behavior -- and that there are
several different reasons people are indecisive. This complicates the task of
both learning and aggregating preferences, since most of the relevant
literature makes restrictive assumptions on the meaning of indecision. We begin
to close this gap by formalizing several mathematical \emph{indecision} models
based on theories from philosophy, psychology, and economics; these models can
be used to describe (indecisive) agent decisions, both when they are allowed to
express indecision and when they are not. We test these models using data
collected from an online survey where participants choose how to
(hypothetically) allocate organs to patients waiting for a transplant.
</p>
<a href="http://arxiv.org/abs/2012.08485" target="_blank">arXiv:2012.08485</a> [<a href="http://arxiv.org/pdf/2012.08485" target="_blank">pdf</a>]

<h2>Amazon SageMaker Automatic Model Tuning: Scalable Black-box Optimization. (arXiv:2012.08489v1 [cs.LG])</h2>
<h3>Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi, Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton, Jean Baptiste Faddoul, Barbara Pogorzelska, Miroslav Miladinovic, Krishnaram Kenthapadi, Matthias Seeger, C&#xe9;dric Archambeau</h3>
<p>Tuning complex machine learning systems is challenging. Machine learning
models typically expose a set of hyperparameters, be it regularization,
architecture, or optimization parameters, whose careful tuning is critical to
achieve good performance. To democratize access to such systems, it is
essential to automate this tuning process. This paper presents Amazon SageMaker
Automatic Model Tuning (AMT), a fully managed system for black-box optimization
at scale. AMT finds the best version of a machine learning model by repeatedly
training it with different hyperparameter configurations. It leverages either
random search or Bayesian optimization to choose the hyperparameter values
resulting in the best-performing model, as measured by the metric chosen by the
user. AMT can be used with built-in algorithms, custom algorithms, and Amazon
SageMaker pre-built containers for machine learning frameworks. We discuss the
core functionality, system architecture and our design principles. We also
describe some more advanced features provided by AMT, such as automated early
stopping and warm-starting, demonstrating their benefits in experiments.
</p>
<a href="http://arxiv.org/abs/2012.08489" target="_blank">arXiv:2012.08489</a> [<a href="http://arxiv.org/pdf/2012.08489" target="_blank">pdf</a>]

<h2>Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks. (arXiv:2012.08492v1 [cs.AI])</h2>
<h3>Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, Yan Zhan</h3>
<p>Large knowledge graphs often grow to store temporal facts that model the
dynamic relations or interactions of entities along the timeline. Since such
temporal knowledge graphs often suffer from incompleteness, it is important to
develop time-aware representation learning models that help to infer the
missing temporal facts. While the temporal facts are typically evolving, it is
observed that many facts often show a repeated pattern along the timeline, such
as economic crises and diplomatic activities. This observation indicates that a
model could potentially learn much from the known facts appeared in history. To
this end, we propose a new representation learning model for temporal knowledge
graphs, namely CyGNet, based on a novel timeaware copy-generation mechanism.
CyGNet is not only able to predict future facts from the whole entity
vocabulary, but also capable of identifying facts with repetition and
accordingly predicting such future facts with reference to the known facts in
the past. We evaluate the proposed method on the knowledge graph completion
task using five benchmark datasets. Extensive experiments demonstrate the
effectiveness of CyGNet for predicting future facts with repetition as well as
de novo fact prediction.
</p>
<a href="http://arxiv.org/abs/2012.08492" target="_blank">arXiv:2012.08492</a> [<a href="http://arxiv.org/pdf/2012.08492" target="_blank">pdf</a>]

<h2>Spectral Methods for Data Science: A Statistical Perspective. (arXiv:2012.08496v1 [stat.ML])</h2>
<h3>Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma</h3>
<p>Spectral methods have emerged as a simple yet surprisingly effective approach
for extracting information from massive, noisy and incomplete data. In a
nutshell, spectral methods refer to a collection of algorithms built upon the
eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors)
of some properly designed matrices constructed from data. A diverse array of
applications have been found in machine learning, data science, and signal
processing. Due to their simplicity and effectiveness, spectral methods are not
only used as a stand-alone estimator, but also frequently employed to
initialize other more sophisticated algorithms to improve performance.

While the studies of spectral methods can be traced back to classical matrix
perturbation theory and methods of moments, the past decade has witnessed
tremendous theoretical advances in demystifying their efficacy through the lens
of statistical modeling, with the aid of non-asymptotic random matrix theory.
This monograph aims to present a systematic, comprehensive, yet accessible
introduction to spectral methods from a modern statistical perspective,
highlighting their algorithmic implications in diverse large-scale
applications. In particular, our exposition gravitates around several central
questions that span various applications: how to characterize the sample
efficiency of spectral methods in reaching a target level of statistical
accuracy, and how to assess their stability in the face of random noise,
missing data, and adversarial corruptions? In addition to conventional $\ell_2$
perturbation analysis, we present a systematic $\ell_{\infty}$ and
$\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces,
which has only recently become available owing to a powerful "leave-one-out"
analysis framework.
</p>
<a href="http://arxiv.org/abs/2012.08496" target="_blank">arXiv:2012.08496</a> [<a href="http://arxiv.org/pdf/2012.08496" target="_blank">pdf</a>]

<h2>NAPA: Neural Art Human Pose Amplifier. (arXiv:2012.08501v1 [cs.CV])</h2>
<h3>Qingfu Wan, Oliver Lu</h3>
<p>This is the project report for CSCI-GA.2271-001. We target human pose
estimation in artistic images. For this goal, we design an end-to-end system
that uses neural style transfer for pose regression. We collect a 277-style set
for arbitrary style transfer and build an artistic 281-image test set. We
directly run pose regression on the test set and show promising results. For
pose regression, we propose a 2d-induced bone map from which pose is lifted. To
help such a lifting, we additionally annotate the pseudo 3d labels of the full
in-the-wild MPII dataset. Further, we append another style transfer as self
supervision to improve 2d. We perform extensive ablation studies to analyze the
introduced features. We also compare end-to-end with per-style training and
allude to the tradeoff between style transfer and pose regression. Lastly, we
generalize our model to the real-world human dataset and show its potentiality
as a generic pose model. We explain the theoretical foundation in Appendix. We
release code at https://github.com/strawberryfg/NAPA-NST-HPE, data, and video.
</p>
<a href="http://arxiv.org/abs/2012.08501" target="_blank">arXiv:2012.08501</a> [<a href="http://arxiv.org/pdf/2012.08501" target="_blank">pdf</a>]

<h2>Object-Centric Neural Scene Rendering. (arXiv:2012.08503v1 [cs.CV])</h2>
<h3>Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser</h3>
<p>We present a method for composing photorealistic scenes from captured images
of objects. Our work builds upon neural radiance fields (NeRFs), which
implicitly model the volumetric density and directionally-emitted radiance of a
scene. While NeRFs synthesize realistic pictures, they only model static scenes
and are closely tied to specific imaging conditions. This property makes NeRFs
hard to generalize to new scenarios, including new lighting or new arrangements
of objects. Instead of learning a scene radiance field as a NeRF does, we
propose to learn object-centric neural scattering functions (OSFs), a
representation that models per-object light transport implicitly using a
lighting- and view-dependent neural network. This enables rendering scenes even
when objects or lights move, without retraining. Combined with a volumetric
path tracing procedure, our framework is capable of rendering both intra- and
inter-object light transport effects including occlusions, specularities,
shadows, and indirect illumination. We evaluate our approach on scene
composition and show that it generalizes to novel illumination conditions,
producing photorealistic, physically accurate renderings of multi-object
scenes.
</p>
<a href="http://arxiv.org/abs/2012.08503" target="_blank">arXiv:2012.08503</a> [<a href="http://arxiv.org/pdf/2012.08503" target="_blank">pdf</a>]

<h2>Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes. (arXiv:2012.08507v1 [cs.LG])</h2>
<h3>Dongruo Zhou, Quanquan Gu, Csaba Szepesvari</h3>
<p>We study reinforcement learning (RL) with linear function approximation where
the underlying transition probability kernel of the Markov decision process
(MDP) is a linear mixture model (Jia et al., 2020; Ayoub et al., 2020; Zhou et
al., 2020) and the learning agent has access to either an integration or a
sampling oracle of the individual basis kernels. We propose a new
Bernstein-type concentration inequality for self-normalized martingales for
linear bandit problems with bounded noise. Based on the new inequality, we
propose a new, computationally efficient algorithm with linear function
approximation named $\text{UCRL-VTR}^{+}$ for the aforementioned linear mixture
MDPs in the episodic undiscounted setting. We show that $\text{UCRL-VTR}^{+}$
attains an $\tilde O(dH\sqrt{T})$ regret where $d$ is the dimension of feature
mapping, $H$ is the length of the episode and $T$ is the number of interactions
with the MDP. We also prove a matching lower bound $\Omega(dH\sqrt{T})$ for
this setting, which shows that $\text{UCRL-VTR}^{+}$ is minimax optimal up to
logarithmic factors. In addition, we propose the $\text{UCLK}^{+}$ algorithm
for the same family of MDPs under discounting and show that it attains an
$\tilde O(d\sqrt{T}/(1-\gamma)^{1.5})$ regret, where $\gamma\in [0,1)$ is the
discount factor. Our upper bound matches the lower bound
$\Omega(d\sqrt{T}/(1-\gamma)^{1.5})$ proved in Zhou et al. (2020) up to
logarithmic factors, suggesting that $\text{UCLK}^{+}$ is nearly minimax
optimal. To the best of our knowledge, these are the first computationally
efficient, nearly minimax optimal algorithms for RL with linear function
approximation.
</p>
<a href="http://arxiv.org/abs/2012.08507" target="_blank">arXiv:2012.08507</a> [<a href="http://arxiv.org/pdf/2012.08507" target="_blank">pdf</a>]

<h2>Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures. (arXiv:2012.08508v1 [cs.CV])</h2>
<h3>David Ding, Felix Hill, Adam Santoro, Matt Botvinick</h3>
<p>Neural networks have achieved success in a wide array of perceptual tasks,
but it is often stated that they are incapable of solving tasks that require
higher-level reasoning. Two new task domains, CLEVRER and CATER, have recently
been developed to focus on reasoning, as opposed to perception, in the context
of spatio-temporal interactions between objects. Initial experiments on these
domains found that neuro-symbolic approaches, which couple a logic engine and
language parser with a neural perceptual front-end, substantially outperform
fully-learned distributed networks, a finding that was taken to support the
above thesis. Here, we show on the contrary that a fully-learned neural network
with the right inductive biases can perform substantially better than all
previous neural-symbolic models on both of these tasks, particularly on
questions that most emphasize reasoning over perception. Our model makes
critical use of both self-attention and learned "soft" object-centric
representations, as well as BERT-style semi-supervised predictive losses. These
flexible biases allow our model to surpass the previous neuro-symbolic
state-of-the-art using less than 60% of available labelled data. Together,
these results refute the neuro-symbolic thesis laid out by previous work
involving these datasets, and they provide evidence that neural networks can
indeed learn to reason effectively about the causal, dynamic structure of
physical events.
</p>
<a href="http://arxiv.org/abs/2012.08508" target="_blank">arXiv:2012.08508</a> [<a href="http://arxiv.org/pdf/2012.08508" target="_blank">pdf</a>]

<h2>GTA: Global Temporal Attention for Video Action Understanding. (arXiv:2012.08510v1 [cs.CV])</h2>
<h3>Bo He, Xitong Yang, Zuxuan Wu, Hao Chen, Ser-Nam Lim, Abhinav Shrivastava</h3>
<p>Self-attention learns pairwise interactions via dot products to model
long-range dependencies, yielding great improvements for video action
recognition. In this paper, we seek a deeper understanding of self-attention
for temporal modeling in videos. In particular, we demonstrate that the
entangled modeling of spatial-temporal information by flattening all pixels is
sub-optimal, failing to capture temporal relationships among frames explicitly.
We introduce Global Temporal Attention (GTA), which performs global temporal
attention on top of spatial attention in a decoupled manner. Unlike
conventional self-attention that computes an instance-specific attention
matrix, GTA randomly initializes a global attention matrix that is intended to
learn stable temporal structures to generalize across different samples. GTA is
further augmented with a cross-channel multi-head fashion to exploit feature
interactions for better temporal modeling. We apply GTA not only on pixels but
also on semantically similar regions identified automatically by a learned
transformation matrix. Extensive experiments on 2D and 3D networks demonstrate
that our approach consistently enhances the temporal modeling and provides
state-of-the-art performance on three video action recognition datasets.
</p>
<a href="http://arxiv.org/abs/2012.08510" target="_blank">arXiv:2012.08510</a> [<a href="http://arxiv.org/pdf/2012.08510" target="_blank">pdf</a>]

<h2>FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation. (arXiv:2012.08512v1 [cs.CV])</h2>
<h3>Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran</h3>
<p>A majority of approaches solve the problem of video frame interpolation by
computing bidirectional optical flow between adjacent frames of a video
followed by a suitable warping algorithm to generate the output frames.
However, methods relying on optical flow often fail to model occlusions and
complex non-linear motions directly from the video and introduce additional
bottlenecks unsuitable for real time deployment. To overcome these limitations,
we propose a flexible and efficient architecture that makes use of 3D
space-time convolutions to enable end to end learning and inference for the
task of video frame interpolation. Our method efficiently learns to reason
about non-linear motions, complex occlusions and temporal abstractions
resulting in improved performance on video interpolation, while requiring no
additional inputs in the form of optical flow or depth maps. Due to its
simplicity, our proposed method improves the inference speed by 384x compared
to the current most accurate method and 23x compared to the current fastest on
8x interpolation. In addition, we evaluate our model on a wide range of
challenging settings and consistently demonstrate superior qualitative and
quantitative results compared with current methods on various popular
benchmarks including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we
demonstrate that video frame interpolation can serve as a useful
self-supervised pretext task for action recognition, optical flow estimation,
and motion magnification.
</p>
<a href="http://arxiv.org/abs/2012.08512" target="_blank">arXiv:2012.08512</a> [<a href="http://arxiv.org/pdf/2012.08512" target="_blank">pdf</a>]

<h2>Large Scale Holistic Video Understanding. (arXiv:1904.11451v3 [cs.CV] UPDATED)</h2>
<h3>Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri, Jurgen Gall, Rainer Stiefelhagen, Luc Van Gool</h3>
<p>Video recognition has been advanced in recent years by benchmarks with rich
annotations. However, research is still mainly limited to human action or
sports recognition - focusing on a highly specific video understanding task and
thus leaving a significant gap towards describing the overall content of a
video. We fill this gap by presenting a large-scale "Holistic Video
Understanding Dataset"~(HVU). HVU is organized hierarchically in a semantic
taxonomy that focuses on multi-label and multi-task video understanding as a
comprehensive problem that encompasses the recognition of multiple semantic
aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9
million annotations for training, validation, and test set spanning over 3142
labels. HVU encompasses semantic aspects defined on categories of scenes,
objects, actions, events, attributes, and concepts which naturally captures the
real-world scenarios.

We demonstrate the generalization capability of HVU on three challenging
tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering
tasks. In particular for video classification, we introduce a new
spatio-temporal deep neural network architecture called "Holistic Appearance
and Temporal Network"~(HATNet) that builds on fusing 2D and 3D architectures
into one by combining intermediate representations of appearance and temporal
cues. HATNet focuses on the multi-label and multi-task learning problem and is
trained in an end-to-end manner. Via our experiments, we validate the idea that
holistic representation learning is complementary, and can play a key role in
enabling many real-world applications.
</p>
<a href="http://arxiv.org/abs/1904.11451" target="_blank">arXiv:1904.11451</a> [<a href="http://arxiv.org/pdf/1904.11451" target="_blank">pdf</a>]

<h2>Panoptic Image Annotation with a Collaborative Assistant. (arXiv:1906.06798v4 [cs.CV] UPDATED)</h2>
<h3>Jasper R. R. Uijlings, Mykhaylo Andriluka, Vittorio Ferrari</h3>
<p>This paper aims to reduce the time to annotate images for panoptic
segmentation, which requires annotating segmentation masks and class labels for
all object instances and stuff regions. We formulate our approach as a
collaborative process between an annotator and an automated assistant who take
turns to jointly annotate an image using a predefined pool of segments. Actions
performed by the annotator serve as a strong contextual signal. The assistant
intelligently reacts to this signal by annotating other parts of the image on
its own, which reduces the amount of work required by the annotator. We perform
thorough experiments on the COCO panoptic dataset, both in simulation and with
human annotators. These demonstrate that our approach is significantly faster
than the recent machine-assisted interface of [4], and 2.4x to 5x faster than
manual polygon drawing. Finally, we show on ADE20k that our method can be used
to efficiently annotate new datasets, bootstrapping from a very small amount of
annotated data.
</p>
<a href="http://arxiv.org/abs/1906.06798" target="_blank">arXiv:1906.06798</a> [<a href="http://arxiv.org/pdf/1906.06798" target="_blank">pdf</a>]

<h2>Improved Schemes for Episodic Memory-based Lifelong Learning. (arXiv:1909.11763v7 [cs.LG] UPDATED)</h2>
<h3>Yunhui Guo, Mingrui Liu, Tianbao Yang, Tajana Rosing</h3>
<p>Current deep neural networks can achieve remarkable performance on a single
task. However, when the deep neural network is continually trained on a
sequence of tasks, it seems to gradually forget the previous learned knowledge.
This phenomenon is referred to as \textit{catastrophic forgetting} and
motivates the field called lifelong learning. Recently, episodic memory based
approaches such as GEM \cite{lopez2017gradient} and A-GEM
\cite{chaudhry2018efficient} have shown remarkable performance. In this paper,
we provide the first unified view of episodic memory based approaches from an
optimization's perspective. This view leads to two improved schemes for
episodic memory based lifelong learning, called MEGA-I and MEGA-II. MEGA-I and
MEGA-II modulate the balance between old tasks and the new task by integrating
the current gradient with the gradient computed on the episodic memory.
Notably, we show that GEM and A-GEM are degenerate cases of MEGA-I and MEGA-II
which consistently put the same emphasis on the current task, regardless of how
the loss changes over time. Our proposed schemes address this issue by using
novel loss-balancing updating rules, which drastically improve the performance
over GEM and A-GEM. Extensive experimental results show that the proposed
schemes significantly advance the state-of-the-art on four commonly used
lifelong learning benchmarks, reducing the error by up to 18\%.
</p>
<a href="http://arxiv.org/abs/1909.11763" target="_blank">arXiv:1909.11763</a> [<a href="http://arxiv.org/pdf/1909.11763" target="_blank">pdf</a>]

<h2>Noise as a Resource for Learning in Knowledge Distillation. (arXiv:1910.05057v2 [cs.LG] UPDATED)</h2>
<h3>Elahe Arani, Fahad Sarfraz, Bahram Zonooz</h3>
<p>While noise is commonly considered a nuisance in computing systems, a number
of studies in neuroscience have shown several benefits of noise in the nervous
system from enabling the brain to carry out computations such as probabilistic
inference as well as carrying additional information about the stimuli.
Similarly, noise has been shown to improve the performance of deep neural
networks. In this study, we further investigate the effect of adding noise in
the knowledge distillation framework because of its resemblance to
collaborative subnetworks in the brain regions. We empirically show that
injecting constructive noise at different levels in the collaborative learning
framework enables us to train the model effectively and distill desirable
characteristics in the student model. In doing so, we propose three different
methods that target the common challenges in deep neural networks: minimizing
the performance gap between a compact model and large model (Fickle Teacher),
training high performance compact adversarially robust models (Soft
Randomization), and training models efficiently under label noise (Messy
Collaboration). Our findings motivate further study in the role of noise as a
resource for learning in a collaborative learning framework.
</p>
<a href="http://arxiv.org/abs/1910.05057" target="_blank">arXiv:1910.05057</a> [<a href="http://arxiv.org/pdf/1910.05057" target="_blank">pdf</a>]

<h2>OffWorld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research. (arXiv:1910.08639v4 [cs.LG] UPDATED)</h2>
<h3>Ashish Kumar, Toby Buckley, John B. Lanier, Qiaozhi Wang, Alicia Kavelaars, Ilya Kuzovkin</h3>
<p>Success stories of applied machine learning can be traced back to the
datasets and environments that were put forward as challenges for the
community. The challenge that the community sets as a benchmark is usually the
challenge that the community eventually solves. The ultimate challenge of
reinforcement learning research is to train real agents to operate in the real
environment, but until now there has not been a common real-world RL benchmark.
In this work, we present a prototype real-world environment from OffWorld Gym
-- a collection of real-world environments for reinforcement learning in
robotics with free public remote access. Close integration into existing
ecosystem allows the community to start using OffWorld Gym without any prior
experience in robotics and takes away the burden of managing a physical
robotics system, abstracting it under a familiar API. We introduce a navigation
task, where a robot has to reach a visual beacon on an uneven terrain using
only the camera input and provide baseline results in both the real environment
and the simulated replica. To start training, visit https://gym.offworld.ai
</p>
<a href="http://arxiv.org/abs/1910.08639" target="_blank">arXiv:1910.08639</a> [<a href="http://arxiv.org/pdf/1910.08639" target="_blank">pdf</a>]

<h2>RGPNet: A Real-Time General Purpose Semantic Segmentation. (arXiv:1912.01394v2 [cs.CV] UPDATED)</h2>
<h3>Elahe Arani, Shabbir Marzban, Andrei Pata, Bahram Zonooz</h3>
<p>We propose a real-time general purpose semantic segmentation architecture,
RGPNet, which achieves significant performance gain in complex environments.
RGPNet consists of a light-weight asymmetric encoder-decoder and an adaptor.
The adaptor helps preserve and refine the abstract concepts from multiple
levels of distributed representations between the encoder and decoder. It also
facilitates the gradient flow from deeper layers to shallower layers. Our
experiments demonstrate that RGPNet can generate segmentation results in
real-time with comparable accuracy to the state-of-the-art non-real-time heavy
models. Moreover, towards green AI, we show that using an optimized
label-relaxation technique with progressive resizing can reduce the training
time by up to 60% while preserving the performance. We conclude that RGPNet
obtains a better speed-accuracy trade-off across multiple datasets.
</p>
<a href="http://arxiv.org/abs/1912.01394" target="_blank">arXiv:1912.01394</a> [<a href="http://arxiv.org/pdf/1912.01394" target="_blank">pdf</a>]

<h2>Multi-Modal Deep Clustering: Unsupervised Partitioning of Images. (arXiv:1912.02678v3 [cs.CV] UPDATED)</h2>
<h3>Guy Shiran, Daphna Weinshall</h3>
<p>The clustering of unlabeled raw images is a daunting task, which has recently
been approached with some success by deep learning methods. Here we propose an
unsupervised clustering framework, which learns a deep neural network in an
end-to-end fashion, providing direct cluster assignments of images without
additional processing. Multi-Modal Deep Clustering (MMDC), trains a deep
network to align its image embeddings with target points sampled from a
Gaussian Mixture Model distribution. The cluster assignments are then
determined by mixture component association of image embeddings.
Simultaneously, the same deep network is trained to solve an additional
self-supervised task of predicting image rotations. This pushes the network to
learn more meaningful image representations that facilitate a better
clustering. Experimental results show that MMDC achieves or exceeds
state-of-the-art performance on six challenging benchmarks. On natural image
datasets we improve on previous results with significant margins of up to 20%
absolute accuracy points, yielding an accuracy of 82% on CIFAR-10, 45% on
CIFAR-100 and 69% on STL-10.
</p>
<a href="http://arxiv.org/abs/1912.02678" target="_blank">arXiv:1912.02678</a> [<a href="http://arxiv.org/pdf/1912.02678" target="_blank">pdf</a>]

<h2>Capsule Attention for Multimodal EEG and EOG Spatiotemporal Representation Learning with Application to Driver Vigilance Estimation. (arXiv:1912.07812v3 [cs.LG] UPDATED)</h2>
<h3>Guangyi Zhang, Ali Etemad</h3>
<p>Driver vigilance estimation is an important task for transportation safety.
Wearable and portable brain-computer interface devices provide a powerful means
for real-time monitoring of the vigilance level of drivers to help with
avoiding distracted or impaired driving. In this paper, we propose a novel
multimodal architecture for in-vehicle vigilance estimation from
Electroencephalogram and Electrooculogram. To enable the system to focus on the
most salient parts of the learned multimodal representations, we propose an
architecture composed of a capsule attention mechanism following a deep Long
Short-Term Memory (LSTM) network. Our model learns both temporal and
hierarchical/spatial dependencies in the data through the LSTM and capsule
feature representation layers. To better explore the discriminative ability of
the learned representations, we study the effect of the proposed capsule
attention mechanism including the number of dynamic routing iterations as well
as other parameters. Experiments show the robustness of our method by
outperforming other solutions and baseline techniques, setting a new
state-of-the-art. Lastly, we investigate the brain activity patterns and
indicators of driving fatigue based on the analysis of brain attention
mechanism, followed by an analysis of the role of capsule attention and its
robust performance in multimodal EEG-EOG learning.
</p>
<a href="http://arxiv.org/abs/1912.07812" target="_blank">arXiv:1912.07812</a> [<a href="http://arxiv.org/pdf/1912.07812" target="_blank">pdf</a>]

<h2>Mastering Complex Control in MOBA Games with Deep Reinforcement Learning. (arXiv:1912.09729v3 [cs.AI] UPDATED)</h2>
<h3>Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, Qiaobo Chen, Yinyuting Yin, Hao Zhang, Tengfei Shi, Liang Wang, Qiang Fu, Wei Yang, Lanxiao Huang</h3>
<p>We study the reinforcement learning problem of complex action control in the
Multi-player Online Battle Arena (MOBA) 1v1 games. This problem involves far
more complicated state and action spaces than those of traditional 1v1 games,
such as Go and Atari series, which makes it very difficult to search any
policies with human-level performance. In this paper, we present a deep
reinforcement learning framework to tackle this problem from the perspectives
of both system and algorithm. Our system is of low coupling and high
scalability, which enables efficient explorations at large scale. Our algorithm
includes several novel strategies, including control dependency decoupling,
action mask, target attention, and dual-clip PPO, with which our proposed
actor-critic network can be effectively trained in our system. Tested on the
MOBA game Honor of Kings, our AI agent, called Tencent Solo, can defeat top
professional human players in full 1v1 games.
</p>
<a href="http://arxiv.org/abs/1912.09729" target="_blank">arXiv:1912.09729</a> [<a href="http://arxiv.org/pdf/1912.09729" target="_blank">pdf</a>]

<h2>Generalizing Emergent Communication. (arXiv:2001.01772v3 [cs.AI] UPDATED)</h2>
<h3>Thomas A. Unger, Elia Bruni</h3>
<p>We converted the recently developed BabyAI grid world platform to a
sender/receiver setup in order to test the hypothesis that established deep
reinforcement learning techniques are sufficient to incentivize the emergence
of a grounded discrete communication protocol between generalized agents. This
is in contrast to previous experiments that employed straight-through
estimation or specialized inductive biases. Our results show that these can
indeed be avoided, by instead providing proper environmental incentives.
Moreover, they show that a longer interval between communications incentivized
more abstract semantics. In some cases, the communicating agents adapted to new
environments more quickly than a monolithic agent, showcasing the potential of
emergent communication for transfer learning and generalization in general.
</p>
<a href="http://arxiv.org/abs/2001.01772" target="_blank">arXiv:2001.01772</a> [<a href="http://arxiv.org/pdf/2001.01772" target="_blank">pdf</a>]

<h2>Meta-learning framework with applications to zero-shot time-series forecasting. (arXiv:2002.02887v3 [cs.LG] UPDATED)</h2>
<h3>Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio</h3>
<p>Can meta-learning discover generic ways of processing time series (TS) from a
diverse dataset so as to greatly improve generalization on new TS coming from
different datasets? This work provides positive evidence to this using a broad
meta-learning framework which we show subsumes many existing meta-learning
algorithms. Our theoretical analysis suggests that residual connections act as
a meta-learning adaptation mechanism, generating a subset of task-specific
parameters based on a given TS input, thus gradually expanding the expressive
power of the architecture on-the-fly. The same mechanism is shown via
linearization analysis to have the interpretation of a sequential update of the
final linear layer. Our empirical results on a wide range of data emphasize the
importance of the identified meta-learning mechanisms for successful zero-shot
univariate forecasting, suggesting that it is viable to train a neural network
on a source TS dataset and deploy it on a different target TS dataset without
retraining, resulting in performance that is at least as good as that of
state-of-practice univariate forecasting models.
</p>
<a href="http://arxiv.org/abs/2002.02887" target="_blank">arXiv:2002.02887</a> [<a href="http://arxiv.org/pdf/2002.02887" target="_blank">pdf</a>]

<h2>Improved Consistency Regularization for GANs. (arXiv:2002.04724v2 [stat.ML] UPDATED)</h2>
<h3>Zhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, Augustus Odena, Han Zhang</h3>
<p>Recent work has increased the performance of Generative Adversarial Networks
(GANs) by enforcing a consistency cost on the discriminator. We improve on this
technique in several ways. We first show that consistency regularization can
introduce artifacts into the GAN samples and explain how to fix this issue. We
then propose several modifications to the consistency regularization procedure
designed to improve its performance. We carry out extensive experiments
quantifying the benefit of our improvements. For unconditional image synthesis
on CIFAR-10 and CelebA, our modifications yield the best known FID scores on
various GAN architectures. For conditional image synthesis on CIFAR-10, we
improve the state-of-the-art FID score from 11.48 to 9.21. Finally, on
ImageNet-2012, we apply our technique to the original BigGAN model and improve
the FID from 6.66 to 5.38, which is the best score at that model size.
</p>
<a href="http://arxiv.org/abs/2002.04724" target="_blank">arXiv:2002.04724</a> [<a href="http://arxiv.org/pdf/2002.04724" target="_blank">pdf</a>]

<h2>Stein Self-Repulsive Dynamics: Benefits From Past Samples. (arXiv:2002.09070v2 [cs.LG] UPDATED)</h2>
<h3>Mao Ye, Tongzheng Ren, Qiang Liu</h3>
<p>We propose a new Stein self-repulsive dynamics for obtaining diversified
samples from intractable un-normalized distributions. Our idea is to introduce
Stein variational gradient as a repulsive force to push the samples of Langevin
dynamics away from the past trajectories. This simple idea allows us to
significantly decrease the auto-correlation in Langevin dynamics and hence
increase the effective sample size. Importantly, as we establish in our
theoretical analysis, the asymptotic stationary distribution remains correct
even with the addition of the repulsive force, thanks to the special properties
of the Stein variational gradient. We perform extensive empirical studies of
our new algorithm, showing that our method yields much higher sample efficiency
and better uncertainty estimation than vanilla Langevin dynamics.
</p>
<a href="http://arxiv.org/abs/2002.09070" target="_blank">arXiv:2002.09070</a> [<a href="http://arxiv.org/pdf/2002.09070" target="_blank">pdf</a>]

<h2>Generative Latent Implicit Conditional Optimization when Learning from Small Sample. (arXiv:2003.14297v5 [cs.LG] UPDATED)</h2>
<h3>Idan Azuri, Daphna Weinshall</h3>
<p>We revisit the long-standing problem of learning from a small sample, to
which end we propose a novel method called GLICO (Generative Latent Implicit
Conditional Optimization). GLICO learns a mapping from the training examples to
a latent space and a generator that generates images from vectors in the latent
space. Unlike most recent works, which rely on access to large amounts of
unlabeled data, GLICO does not require access to any additional data other than
the small set of labeled points. In fact, GLICO learns to synthesize completely
new samples for every class using as little as 5 or 10 examples per class, with
as few as 10 such classes without imposing any prior. GLICO is then used to
augment the small training set while training a classifier on the small sample.
To this end, our proposed method samples the learned latent space using
spherical interpolation, and generates new examples using the trained
generator. Empirical results show that the new sampled set is diverse enough,
leading to improvement in image classification in comparison with the state of
the art, when trained on small samples obtained from CIFAR-10, CIFAR-100, and
CUB-200.
</p>
<a href="http://arxiv.org/abs/2003.14297" target="_blank">arXiv:2003.14297</a> [<a href="http://arxiv.org/pdf/2003.14297" target="_blank">pdf</a>]

<h2>Neural Analogical Matching. (arXiv:2004.03573v5 [cs.AI] UPDATED)</h2>
<h3>Maxwell Crouse, Constantine Nakos, Ibrahim Abdelaziz, Kenneth Forbus</h3>
<p>Analogy is core to human cognition. It allows us to solve problems based on
prior experience, it governs the way we conceptualize new information, and it
even influences our visual perception. The importance of analogy to humans has
made it an active area of research in the broader field of artificial
intelligence, resulting in data-efficient models that learn and reason in
human-like ways. While cognitive perspectives of analogy and deep learning have
generally been studied independently of one another, the integration of the two
lines of research is a promising step towards more robust and efficient
learning techniques. As part of a growing body of research on such an
integration, we introduce the Analogical Matching Network: a neural
architecture that learns to produce analogies between structured, symbolic
representations that are largely consistent with the principles of
Structure-Mapping Theory.
</p>
<a href="http://arxiv.org/abs/2004.03573" target="_blank">arXiv:2004.03573</a> [<a href="http://arxiv.org/pdf/2004.03573" target="_blank">pdf</a>]

<h2>Fitting the Search Space of Weight-sharing NAS with Graph Convolutional Networks. (arXiv:2004.08423v2 [cs.LG] UPDATED)</h2>
<h3>Xin Chen, Lingxi Xie, Jun Wu, Longhui Wei, Yuhui Xu, Qi Tian</h3>
<p>Neural architecture search has attracted wide attentions in both academia and
industry. To accelerate it, researchers proposed weight-sharing methods which
first train a super-network to reuse computation among different operators,
from which exponentially many sub-networks can be sampled and efficiently
evaluated. These methods enjoy great advantages in terms of computational
costs, but the sampled sub-networks are not guaranteed to be estimated
precisely unless an individual training process is taken. This paper owes such
inaccuracy to the inevitable mismatch between assembled network layers, so that
there is a random error term added to each estimation. We alleviate this issue
by training a graph convolutional network to fit the performance of sampled
sub-networks so that the impact of random errors becomes minimal. With this
strategy, we achieve a higher rank correlation coefficient in the selected set
of candidates, which consequently leads to better performance of the final
architecture. In addition, our approach also enjoys the flexibility of being
used under different hardware constraints, since the graph convolutional
network has provided an efficient lookup table of the performance of
architectures in the entire search space.
</p>
<a href="http://arxiv.org/abs/2004.08423" target="_blank">arXiv:2004.08423</a> [<a href="http://arxiv.org/pdf/2004.08423" target="_blank">pdf</a>]

<h2>Federated Transfer Learning for EEG Signal Classification. (arXiv:2004.12321v4 [cs.LG] UPDATED)</h2>
<h3>Ce Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, Cuntai Guan</h3>
<p>The success of deep learning (DL) methods in the Brain-Computer Interfaces
(BCI) field for classification of electroencephalographic (EEG) recordings has
been restricted by the lack of large datasets. Privacy concerns associated with
EEG signals limit the possibility of constructing a large EEG-BCI dataset by
the conglomeration of multiple small ones for jointly training machine learning
models. Hence, in this paper, we propose a novel privacy-preserving DL
architecture named federated transfer learning (FTL) for EEG classification
that is based on the federated learning framework. Working with the
single-trial covariance matrix, the proposed architecture extracts common
discriminative information from multi-subject EEG data with the help of domain
adaptation techniques. We evaluate the performance of the proposed architecture
on the PhysioNet dataset for 2-class motor imagery classification. While
avoiding the actual data sharing, our FTL approach achieves 2% higher
classification accuracy in a subject-adaptive analysis. Also, in the absence of
multi-subject data, our architecture provides 6% better accuracy compared to
other state-of-the-art DL architectures.
</p>
<a href="http://arxiv.org/abs/2004.12321" target="_blank">arXiv:2004.12321</a> [<a href="http://arxiv.org/pdf/2004.12321" target="_blank">pdf</a>]

<h2>Better scalability under potentially heavy-tailed gradients. (arXiv:2006.00784v2 [stat.ML] UPDATED)</h2>
<h3>Matthew J. Holland</h3>
<p>We study a scalable alternative to robust gradient descent (RGD) techniques
that can be used when the gradients can be heavy-tailed, though this will be
unknown to the learner. The core technique is simple: instead of trying to
robustly aggregate gradients at each step, which is costly and leads to
sub-optimal dimension dependence in risk bounds, we choose a candidate which
does not diverge too far from the majority of cheap stochastic sub-processes
run for a single pass over partitioned data. In addition to formal guarantees,
we also provide empirical analysis of robustness to perturbations to
experimental conditions, under both sub-Gaussian and heavy-tailed data. The
result is a procedure that is simple to implement, trivial to parallelize,
which keeps the formal strength of RGD methods but scales much better to large
learning problems.
</p>
<a href="http://arxiv.org/abs/2006.00784" target="_blank">arXiv:2006.00784</a> [<a href="http://arxiv.org/pdf/2006.00784" target="_blank">pdf</a>]

<h2>Improved scalability under heavy tails, without strong convexity. (arXiv:2006.01364v2 [stat.ML] UPDATED)</h2>
<h3>Matthew J. Holland</h3>
<p>Real-world data is laden with outlying values. The challenge for machine
learning is that the learner typically has no prior knowledge of whether the
feedback it receives (losses, gradients, etc.) will be heavy-tailed or not. In
this work, we study a simple algorithmic strategy that can be leveraged when
both losses and gradients can be heavy-tailed. The core technique introduces a
simple robust validation sub-routine, which is used to boost the confidence of
inexpensive gradient-based sub-processes. Compared with recent robust gradient
descent methods from the literature, dimension dependence (both risk bounds and
cost) is substantially improved, without relying upon strong convexity or
expensive per-step robustification. Empirically, we also show that under
heavy-tailed losses, the proposed procedure cannot simply be replaced with
naive cross-validation. Taken together, we have a scalable method with
transparent guarantees, which performs well without prior knowledge of how
"convenient" the feedback it receives will be.
</p>
<a href="http://arxiv.org/abs/2006.01364" target="_blank">arXiv:2006.01364</a> [<a href="http://arxiv.org/pdf/2006.01364" target="_blank">pdf</a>]

<h2>Incorporating Pragmatic Reasoning Communication into Emergent Language. (arXiv:2006.04109v2 [cs.AI] UPDATED)</h2>
<h3>Yipeng Kang, Tonghan Wang, Gerard de Melo</h3>
<p>Emergentism and pragmatics are two research fields that study the dynamics of
linguistic communication along substantially different timescales and
intelligence levels. From the perspective of multi-agent reinforcement
learning, they correspond to stochastic games with reinforcement training and
stage games with opponent awareness. Given that their combination has been
explored in linguistics, we propose computational models that combine
short-term mutual reasoning-based pragmatics with long-term language
emergentism. We explore this for agent communication referential games as well
as in Starcraft II, assessing the relative merits of different kinds of mutual
reasoning pragmatics models both empirically and theoretically. Our results
shed light on their importance for making inroads towards getting more natural,
accurate, robust, fine-grained, and succinct utterances.
</p>
<a href="http://arxiv.org/abs/2006.04109" target="_blank">arXiv:2006.04109</a> [<a href="http://arxiv.org/pdf/2006.04109" target="_blank">pdf</a>]

<h2>Liquid Time-constant Networks. (arXiv:2006.04439v4 [cs.LG] UPDATED)</h2>
<h3>Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, Radu Grosu</h3>
<p>We introduce a new class of time-continuous recurrent neural network models.
Instead of declaring a learning system's dynamics by implicit nonlinearities,
we construct networks of linear first-order dynamical systems modulated via
nonlinear interlinked gates. The resulting models represent dynamical systems
with varying (i.e., liquid) time-constants coupled to their hidden state, with
outputs being computed by numerical differential equation solvers. These neural
networks exhibit stable and bounded behavior, yield superior expressivity
within the family of neural ordinary differential equations, and give rise to
improved performance on time-series prediction tasks. To demonstrate these
properties, we first take a theoretical approach to find bounds over their
dynamics and compute their expressive power by the trajectory length measure in
latent trajectory space. We then conduct a series of time-series prediction
experiments to manifest the approximation capability of Liquid Time-Constant
Networks (LTCs) compared to classical and modern RNNs. Code and data are
available at https://github.com/raminmh/liquid_time_constant_networks
</p>
<a href="http://arxiv.org/abs/2006.04439" target="_blank">arXiv:2006.04439</a> [<a href="http://arxiv.org/pdf/2006.04439" target="_blank">pdf</a>]

<h2>STL-SGD: Speeding Up Local SGD with Stagewise Communication Period. (arXiv:2006.06377v2 [cs.LG] UPDATED)</h2>
<h3>Shuheng Shen, Yifei Cheng, Jingchang Liu, Linli Xu</h3>
<p>Distributed parallel stochastic gradient descent algorithms are workhorses
for large scale machine learning tasks. Among them, local stochastic gradient
descent (Local SGD) has attracted significant attention due to its low
communication complexity. Previous studies prove that the communication
complexity of Local SGD with a fixed or an adaptive communication period is in
the order of $O (N^{\frac{3}{2}} T^{\frac{1}{2}})$ and $O (N^{\frac{3}{4}}
T^{\frac{3}{4}})$ when the data distributions on clients are identical (IID) or
otherwise (Non-IID), where $N$ is the number of clients and $T$ is the number
of iterations. In this paper, to accelerate the convergence by reducing the
communication complexity, we propose \textit{ST}agewise \textit{L}ocal
\textit{SGD} (STL-SGD), which increases the communication period gradually
along with decreasing learning rate. We prove that STL-SGD can keep the same
convergence rate and linear speedup as mini-batch SGD. In addition, as the
benefit of increasing the communication period, when the objective is strongly
convex or satisfies the Polyak-\L ojasiewicz condition, the communication
complexity of STL-SGD is $O (N \log{T})$ and $O (N^{\frac{1}{2}}
T^{\frac{1}{2}})$ for the IID case and the Non-IID case respectively, achieving
significant improvements over Local SGD. Experiments on both convex and
non-convex problems demonstrate the superior performance of STL-SGD.
</p>
<a href="http://arxiv.org/abs/2006.06377" target="_blank">arXiv:2006.06377</a> [<a href="http://arxiv.org/pdf/2006.06377" target="_blank">pdf</a>]

<h2>Combinatorial Pure Exploration with Full-Bandit or Partial Linear Feedback. (arXiv:2006.07905v2 [cs.LG] UPDATED)</h2>
<h3>Yihan Du, Yuko Kuroki, Wei Chen</h3>
<p>In this paper, we first study the problem of combinatorial pure exploration
with full-bandit feedback (CPE-BL), where a learner is given a combinatorial
action space $\mathcal{X} \subseteq \{0,1\}^d$, and in each round the learner
pulls an action $x \in \mathcal{X}$ and receives a random reward with
expectation $x^{\top} \theta$, with $\theta \in \mathbb{R}^d$ a latent and
unknown environment vector. The objective is to identify the optimal action
with the highest expected reward, using as few samples as possible. For CPE-BL,
we design the first {\em polynomial-time adaptive} algorithm, whose sample
complexity matches the lower bound (within a logarithmic factor) for a family
of instances and has a light dependence of $\Delta_{\min}$ (the smallest gap
between the optimal action and sub-optimal actions). Furthermore, we propose a
novel generalization of CPE-BL with flexible feedback structures, called
combinatorial pure exploration with partial linear feedback (CPE-PL), which
encompasses several families of sub-problems including full-bandit feedback,
semi-bandit feedback, partial feedback and nonlinear reward functions. In
CPE-PL, each pull of action $x$ reports a random feedback vector with
expectation of $M_{x} \theta $, where $M_x \in \mathbb{R}^{m_x \times d}$ is a
transformation matrix for $x$, and gains a random (possibly nonlinear) reward
related to $x$. For CPE-PL, we develop the first {\em polynomial-time}
algorithm, which simultaneously addresses limited feedback, general reward
function and combinatorial action space, and provide its sample complexity
analysis. Our empirical evaluation demonstrates that our algorithms run orders
of magnitude faster than the existing ones, and our CPE-BL algorithm is robust
across different $\Delta_{\min}$ settings while our CPE-PL algorithm is the
only one returning correct answers for nonlinear reward functions.
</p>
<a href="http://arxiv.org/abs/2006.07905" target="_blank">arXiv:2006.07905</a> [<a href="http://arxiv.org/pdf/2006.07905" target="_blank">pdf</a>]

<h2>How isotropic kernels perform on simple invariants. (arXiv:2006.09754v5 [cs.LG] UPDATED)</h2>
<h3>Jonas Paccolat, Stefano Spigler, Matthieu Wyart</h3>
<p>We investigate how the training curve of isotropic kernel methods depends on
the symmetry of the task to be learned, in several settings. (i) We consider a
regression task, where the target function is a Gaussian random field that
depends only on $d_\parallel$ variables, fewer than the input dimension $d$. We
compute the expected test error $\epsilon$ that follows $\epsilon\sim
p^{-\beta}$ where $p$ is the size of the training set. We find that $\beta\sim
1/d$ independently of $d_\parallel$, supporting previous findings that the
presence of invariants does not resolve the curse of dimensionality for kernel
regression. (ii) Next we consider support-vector binary classification and
introduce the stripe model where the data label depends on a single coordinate
$y(\underline{x}) = y(x_1)$, corresponding to parallel decision boundaries
separating labels of different signs, and consider that there is no margin at
these interfaces. We argue and confirm numerically that for large bandwidth,
$\beta = \frac{d-1+\xi}{3d-3+\xi}$, where $\xi\in (0,2)$ is the exponent
characterizing the singularity of the kernel at the origin. This estimation
improves classical bounds obtainable from Rademacher complexity. In this
setting there is no curse of dimensionality since $\beta\rightarrow 1 / 3$ as
$d\rightarrow\infty$. (iii) We confirm these findings for the spherical model
for which $y(\underline{x}) = y(|\underline{x}|)$. (iv) In the stripe model, we
show that if the data are compressed along their invariants by some factor
$\lambda$ (an operation believed to take place in deep networks), the test
error is reduced by a factor $\lambda^{-\frac{2(d-1)}{3d-3+\xi}}$.
</p>
<a href="http://arxiv.org/abs/2006.09754" target="_blank">arXiv:2006.09754</a> [<a href="http://arxiv.org/pdf/2006.09754" target="_blank">pdf</a>]

<h2>Privacy-Preserving Technology to Help Millions of People: Federated Prediction Model for Stroke Prevention. (arXiv:2006.10517v2 [cs.LG] UPDATED)</h2>
<h3>Ce Ju, Ruihui Zhao, Jichao Sun, Xiguang Wei, Bo Zhao, Yang Liu, Hongshan Li, Tianjian Chen, Xinwei Zhang, Dashan Gao, Ben Tan, Han Yu, Chuning He, Yuan Jin</h3>
<p>Prevention of stroke with its associated risk factors has been one of the
public health priorities worldwide. Emerging artificial intelligence technology
is being increasingly adopted to predict stroke. Because of privacy concerns,
patient data are stored in distributed electronic health record (EHR)
databases, voluminous clinical datasets, which prevent patient data from being
aggregated and restrains AI technology to boost the accuracy of stroke
prediction with centralized training data. In this work, our scientists and
engineers propose a privacy-preserving scheme to predict the risk of stroke and
deploy our federated prediction model on cloud servers. Our system of federated
prediction model asynchronously supports any number of client connections and
arbitrary local gradient iterations in each communication round. It adopts
federated averaging during the model training process, without patient data
being taken out of the hospitals during the whole process of model training and
forecasting. With the privacy-preserving mechanism, our federated prediction
model trains over all the healthcare data from hospitals in a certain city
without actual data sharing among them. Therefore, it is not only secure but
also more accurate than any single prediction model that trains over the data
only from one single hospital. Especially for small hospitals with few
confirmed stroke cases, our federated model boosts model performance by 10%~20%
in several machine learning metrics. To help stroke experts comprehend the
advantage of our prediction system more intuitively, we developed a mobile app
that collects the key information of patients' statistics and demonstrates
performance comparisons between the federated prediction model and the single
prediction model during the federated training process.
</p>
<a href="http://arxiv.org/abs/2006.10517" target="_blank">arXiv:2006.10517</a> [<a href="http://arxiv.org/pdf/2006.10517" target="_blank">pdf</a>]

<h2>Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning. (arXiv:2006.11485v3 [cs.LG] UPDATED)</h2>
<h3>Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen</h3>
<p>Goal-conditioned hierarchical reinforcement learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is often large. Searching in a large goal space poses
difficulties for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that the proposed adjacency constraint
preserves the optimal hierarchical policy in deterministic MDPs, and show that
this constraint can be practically implemented by training an adjacency network
that can discriminate between adjacent and non-adjacent subgoals. Experimental
results on discrete and continuous control tasks show that incorporating the
adjacency constraint improves the performance of state-of-the-art HRL
approaches in both deterministic and stochastic environments.
</p>
<a href="http://arxiv.org/abs/2006.11485" target="_blank">arXiv:2006.11485</a> [<a href="http://arxiv.org/pdf/2006.11485" target="_blank">pdf</a>]

<h2>Gaining Insight into SARS-CoV-2 Infection and COVID-19 Severity Using Self-supervised Edge Features and Graph Neural Networks. (arXiv:2006.12971v2 [cs.LG] UPDATED)</h2>
<h3>Arijit Sehanobish, Neal G. Ravindra, David van Dijk</h3>
<p>A molecular and cellular understanding of how SARS-CoV-2 variably infects and
causes severe COVID-19 remains a bottleneck in developing interventions to end
the pandemic. We sought to use deep learning to study the biology of SARS-CoV-2
infection and COVID-19 severity by identifying transcriptomic patterns and cell
types associated with SARS-CoV-2 infection and COVID-19 severity. To do this,
we developed a new approach to generating self-supervised edge features. We
propose a model that builds on Graph Attention Networks (GAT), creates edge
features using self-supervised learning, and ingests these edge features via a
Set Transformer. This model achieves significant improvements in predicting the
disease state of individual cells, given their transcriptome. We apply our
model to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung
organoids and bronchoalveolar lavage fluid samples of patients with COVID-19,
achieving state-of-the-art performance on both datasets with our model. We then
borrow from the field of explainable AI (XAI) to identify the features (genes)
and cell types that discriminate bystander vs. infected cells across time and
moderate vs. severe COVID-19 disease. To the best of our knowledge, this
represents the first application of deep learning to identifying the molecular
and cellular determinants of SARS-CoV-2 infection and COVID-19 severity using
single-cell omics data.
</p>
<a href="http://arxiv.org/abs/2006.12971" target="_blank">arXiv:2006.12971</a> [<a href="http://arxiv.org/pdf/2006.12971" target="_blank">pdf</a>]

<h2>PaMIR: Parametric Model-Conditioned Implicit Representation for Image-based Human Reconstruction. (arXiv:2007.03858v2 [cs.CV] UPDATED)</h2>
<h3>Zerong Zheng, Tao Yu, Yebin Liu, Qionghai Dai</h3>
<p>Modeling 3D humans accurately and robustly from a single image is very
challenging, and the key for such an ill-posed problem is the 3D representation
of the human models. To overcome the limitations of regular 3D representations,
we propose Parametric Model-Conditioned Implicit Representation (PaMIR), which
combines the parametric body model with the free-form deep implicit function.
In our PaMIR-based reconstruction framework, a novel deep neural network is
proposed to regularize the free-form deep implicit function using the semantic
features of the parametric model, which improves the generalization ability
under the scenarios of challenging poses and various clothing topologies.
Moreover, a novel depth-ambiguity-aware training loss is further integrated to
resolve depth ambiguities and enable successful surface detail reconstruction
with imperfect body reference. Finally, we propose a body reference
optimization method to improve the parametric model estimation accuracy and to
enhance the consistency between the parametric model and the implicit function.
With the PaMIR representation, our framework can be easily extended to
multi-image input scenarios without the need of multi-camera calibration and
pose synchronization. Experimental results demonstrate that our method achieves
state-of-the-art performance for image-based 3D human reconstruction in the
cases of challenging poses and clothing types.
</p>
<a href="http://arxiv.org/abs/2007.03858" target="_blank">arXiv:2007.03858</a> [<a href="http://arxiv.org/pdf/2007.03858" target="_blank">pdf</a>]

<h2>Blur Invariant Kernel-Adaptive Network for Single Image Blind deblurring. (arXiv:2007.04543v3 [cs.CV] UPDATED)</h2>
<h3>Sungkwon An, Hyungmin Roh, Myungjoo Kang</h3>
<p>We present a novel, blind, single image deblurring method that utilizes
information regarding blur kernels. Our model solves the deblurring problem by
dividing it into two successive tasks: (1) blur kernel estimation and (2) sharp
image restoration. We first introduce a kernel estimation network that produces
adaptive blur kernels based on the analysis of the blurred image. The network
learns the blur pattern of the input image and trains to generate the
estimation of image-specific blur kernels. Subsequently, we propose a
deblurring network that restores sharp images using the estimated blur kernel.
To use the kernel efficiently, we propose a kernel-adaptive AE block that
encodes features from both blurred images and blur kernels into a low
dimensional space and then decodes them simultaneously to obtain an
appropriately synthesized feature representation. We evaluate our model on
REDS, GOPRO and Flickr2K datasets using various Gaussian blur kernels.
Experiments show that our model can achieve state-of-the-art results on each
dataset.
</p>
<a href="http://arxiv.org/abs/2007.04543" target="_blank">arXiv:2007.04543</a> [<a href="http://arxiv.org/pdf/2007.04543" target="_blank">pdf</a>]

<h2>Solving the Clustered Traveling Salesman Problem via TSP methods. (arXiv:2007.05254v2 [cs.AI] UPDATED)</h2>
<h3>Yongliang Lu, Jin-Kao Hao, Qinghua Wu</h3>
<p>The Clustered Traveling Salesman Problem (CTSP) is a variant of the popular
Traveling Salesman Problem (TSP) arising from a number of real-life
applications. In this work, we explore an uncharted solution approach that
solves the CTSP by transforming it to the well-studied TSP. For this purpose,
we first investigate a technique to convert a CTSP instance to a TSP and then
apply popular TSP solvers (including exact and heuristic solvers) to solve the
resulting TSP instance. We want to answer the following questions: How do
state-of-the-art TSP solvers perform on clustered instances converted from the
CTSP? Do state-of-the-art TSP solvers compete well with the best performing
methods specifically designed for the CTSP? For this purpose, we present
intensive computational experiments on various benchmark instances to draw
conclusions.
</p>
<a href="http://arxiv.org/abs/2007.05254" target="_blank">arXiv:2007.05254</a> [<a href="http://arxiv.org/pdf/2007.05254" target="_blank">pdf</a>]

<h2>FC-GAGA: Fully Connected Gated Graph Architecture for Spatio-Temporal Traffic Forecasting. (arXiv:2007.15531v2 [cs.LG] UPDATED)</h2>
<h3>Boris N. Oreshkin, Arezou Amini, Lucy Coyle, Mark J. Coates</h3>
<p>Forecasting of multivariate time-series is an important problem that has
applications in traffic management, cellular network configuration, and
quantitative finance. A special case of the problem arises when there is a
graph available that captures the relationships between the time-series. In
this paper we propose a novel learning architecture that achieves performance
competitive with or better than the best existing algorithms, without requiring
knowledge of the graph. The key element of our proposed architecture is the
learnable fully connected hard graph gating mechanism that enables the use of
the state-of-the-art and highly computationally efficient fully connected
time-series forecasting architecture in traffic forecasting applications.
Experimental results for two public traffic network datasets illustrate the
value of our approach, and ablation studies confirm the importance of each
element of the architecture. The code is available here:
https://github.com/boreshkinai/fc-gaga.
</p>
<a href="http://arxiv.org/abs/2007.15531" target="_blank">arXiv:2007.15531</a> [<a href="http://arxiv.org/pdf/2007.15531" target="_blank">pdf</a>]

<h2>Canopy Density Estimation in Perennial Horticulture Crops Using 3D Spinning Lidar SLAM. (arXiv:2007.15652v2 [cs.RO] UPDATED)</h2>
<h3>Thomas Lowe, Peyman Moghadam, Everard Edwards, Jason Williams</h3>
<p>We propose a novel, canopy density estimation solution using a 3D ray cloud
representation for perennial horticultural crops at the field scale. To attain
high spatial and temporal fidelity in field conditions, we propose the
application of continuous-time 3D SLAM (Simultaneous Localisation and Mapping)
to a spinning lidar payload (AgScan3D) mounted on a moving farm vehicle. The
AgScan3D data is processed through a Continuous-Time SLAM algorithm into a
globally registered 3D ray cloud. The global ray cloud is a canonical data
format (a digital twin) from which we can compare vineyard snapshots over
multiple times within a season and across seasons. Then, the vineyard rows are
automatically extracted from the ray cloud and a novel density calculation is
performed to estimate the maximum likelihood canopy densities of the vineyard.
This combination of digital twinning, together with the accurate extraction of
canopy structure information, allows entire vineyards to be analysed and
compared, across the growing season and from year to year. The proposed method
is evaluated both in simulation and field experiments. Field experiments were
performed at four sites, which varied in vineyard structure and vine
management, over two growing seasons and 64 data collection campaigns,
resulting in a total traversal of 160 kilometres, 42.4 scanned hectares of
vines with a combined total of approximately 93,000 scanned vines. Our
experiments show canopy density repeatability of 3.8% (Relative RMSE) per
vineyard panel, for acquisition speeds of 5-6 km/h, and under half the standard
deviation in estimated densities when compared to an industry standard
gap-fraction based solution. The code and field datasets are available at
https://github.com/csiro-robotics/agscan3d.
</p>
<a href="http://arxiv.org/abs/2007.15652" target="_blank">arXiv:2007.15652</a> [<a href="http://arxiv.org/pdf/2007.15652" target="_blank">pdf</a>]

<h2>Self-supervised learning through the eyes of a child. (arXiv:2007.16189v3 [cs.CV] UPDATED)</h2>
<h3>A. Emin Orhan, Vaibhav V. Gupta, Brenden M. Lake</h3>
<p>Within months of birth, children develop meaningful expectations about the
world around them. How much of this early knowledge can be explained through
generic learning mechanisms applied to sensory data, and how much of it
requires more substantive innate inductive biases? Addressing this fundamental
question in its full generality is currently infeasible, but we can hope to
make real progress in more narrowly defined domains, such as the development of
high-level visual categories, thanks to improvements in data collecting
technology and recent progress in deep learning. In this paper, our goal is
precisely to achieve such progress by utilizing modern self-supervised deep
learning methods and a recent longitudinal, egocentric video dataset recorded
from the perspective of three young children (Sullivan et al., 2020). Our
results demonstrate the emergence of powerful, high-level visual
representations from developmentally realistic natural videos using generic
self-supervised learning objectives.
</p>
<a href="http://arxiv.org/abs/2007.16189" target="_blank">arXiv:2007.16189</a> [<a href="http://arxiv.org/pdf/2007.16189" target="_blank">pdf</a>]

<h2>Beyond Pointwise Submodularity: Non-Monotone Adaptive Submodular Maximization in Linear Time. (arXiv:2008.05004v4 [cs.LG] UPDATED)</h2>
<h3>Shaojie Tang</h3>
<p>In this paper, we study the non-monotone adaptive submodular maximization
problem subject to a cardinality constraint. We first revisit the adaptive
random greedy algorithm proposed in \citep{gotovos2015non}, where they show
that this algorithm achieves a $1/e$ approximation ratio if the objective
function is adaptive submodular and pointwise submodular. It is not clear
whether the same guarantee holds under adaptive submodularity (without
resorting to pointwise submodularity) or not. Our first contribution is to show
that the adaptive random greedy algorithm achieves a $1/e$ approximation ratio
under adaptive submodularity. One limitation of the adaptive random greedy
algorithm is that it requires $O(n\times k)$ value oracle queries, where $n$ is
the size of the ground set and $k$ is the cardinality constraint. Our second
contribution is to develop the first linear-time algorithm for the non-monotone
adaptive submodular maximization problem. Our algorithm achieves a
$1/e-\epsilon$ approximation ratio (this bound is improved to $1-1/e-\epsilon$
for monotone case), using only $O(n\epsilon^{-2}\log \epsilon^{-1})$ value
oracle queries. Notably, $O(n\epsilon^{-2}\log \epsilon^{-1})$ is independent
of the cardinality constraint. For the monotone case, we propose a faster
algorithm that achieves a $1-1/e-\epsilon$ approximation ratio in expectation
with $O(n \log \frac{1}{\epsilon})$ value oracle queries. We also generalize
our study by considering a partition matroid constraint, and develop a
linear-time algorithm for monotone and fully adaptive submodular functions.
</p>
<a href="http://arxiv.org/abs/2008.05004" target="_blank">arXiv:2008.05004</a> [<a href="http://arxiv.org/pdf/2008.05004" target="_blank">pdf</a>]

<h2>Addressing Class Imbalance in Federated Learning. (arXiv:2008.06217v2 [cs.LG] UPDATED)</h2>
<h3>Lixu Wang, Shichao Xu, Xiao Wang, Qi Zhu</h3>
<p>Federated learning (FL) is a promising approach for training decentralized
data located on local client devices while improving efficiency and privacy.
However, the distribution and quantity of the training data on the clients'
side may lead to significant challenges such as class imbalance and non-IID
(non-independent and identically distributed) data, which could greatly impact
the performance of the common model. While much effort has been devoted to
helping FL models converge when encountering non-IID data, the imbalance issue
has not been sufficiently addressed. In particular, as FL training is executed
by exchanging gradients in an encrypted form, the training data is not
completely observable to either clients or servers, and previous methods for
class imbalance do not perform well for FL. Therefore, it is crucial to design
new methods for detecting class imbalance in FL and mitigating its impact. In
this work, we propose a monitoring scheme that can infer the composition of
training data for each FL round, and design a new loss function --
\textbf{Ratio Loss} to mitigate the impact of the imbalance. Our experiments
demonstrate the importance of acknowledging class imbalance and taking measures
as early as possible in FL training, and the effectiveness of our method in
mitigating the impact. Our method is shown to significantly outperform previous
methods, while maintaining client privacy.
</p>
<a href="http://arxiv.org/abs/2008.06217" target="_blank">arXiv:2008.06217</a> [<a href="http://arxiv.org/pdf/2008.06217" target="_blank">pdf</a>]

<h2>Linear Disentangled Representations and Unsupervised Action Estimation. (arXiv:2008.07922v2 [cs.LG] UPDATED)</h2>
<h3>Matthew Painter, Jonathon Hare, Adam Prugel-Bennett</h3>
<p>Disentangled representation learning has seen a surge in interest over recent
times, generally focusing on new models which optimise one of many disparate
disentanglement metrics. Symmetry Based Disentangled Representation learning
introduced a robust mathematical framework that defined precisely what is meant
by a "linear disentangled representation". This framework determined that such
representations would depend on a particular decomposition of the symmetry
group acting on the data, showing that actions would manifest through
irreducible group representations acting on independent representational
subspaces. Caselles-Dupre et al [2019] subsequently proposed the first model to
induce and demonstrate a linear disentangled representation in a VAE model. In
this work we empirically show that linear disentangled representations are not
generally present in standard VAE models and that they instead require altering
the loss landscape to induce them. We proceed to show that such representations
are a desirable property with regard to classical disentanglement metrics.
Finally we propose a method to induce irreducible representations which forgoes
the need for labelled action sequences, as was required by prior work. We
explore a number of properties of this method, including the ability to learn
from action sequences without knowledge of intermediate states and robustness
under visual noise. We also demonstrate that it can successfully learn 4
independent symmetries directly from pixels.
</p>
<a href="http://arxiv.org/abs/2008.07922" target="_blank">arXiv:2008.07922</a> [<a href="http://arxiv.org/pdf/2008.07922" target="_blank">pdf</a>]

<h2>Weakly Supervised Learning with Region and Box-level Annotations for Salient Instance Segmentation. (arXiv:2008.08246v2 [cs.CV] UPDATED)</h2>
<h3>Jialun Pei, He Tang, Chuanbo Chen</h3>
<p>Salient instance segmentation is a new challenging task that received
widespread attention in saliency detection area. Due to the limited scale of
the existing dataset and the high mask annotations cost, it is difficult to
train a salient instance neural network completely. In this paper, we appeal to
train a salient instance segmentation framework by a weakly supervised source
without resorting to laborious labeling. We present a cyclic global context
salient instance segmentation network (CGCNet), which is supervised by the
combination of the binary salient regions and bounding boxes from the existing
saliency detection datasets. For a precise pixel-level location, a global
feature refining layer is introduced that dilates the context features of each
salient instance to the global context in the image. Meanwhile, a labeling
updating scheme is embedded in the proposed framework to online update the weak
annotations for next iteration. Experiment results demonstrate that the
proposed end-to-end network trained by weakly supervised annotations can be
competitive to the existing fully supervised salient instance segmentation
methods. Without bells and whistles, our proposed method achieves a mask AP of
57.13%, which outperforms the best fully supervised methods and establishes new
states of the art for weakly supervised salient instance segmentation.
</p>
<a href="http://arxiv.org/abs/2008.08246" target="_blank">arXiv:2008.08246</a> [<a href="http://arxiv.org/pdf/2008.08246" target="_blank">pdf</a>]

<h2>Online Class-Incremental Continual Learning with Adversarial Shapley Value. (arXiv:2009.00093v2 [cs.LG] UPDATED)</h2>
<h3>Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, Jongseong Jang</h3>
<p>As image-based deep learning becomes pervasive on every device, from cell
phones to smart watches, there is a growing need to develop methods that
continually learn from data while minimizing memory footprint and power
consumption. While memory replay techniques have shown exceptional promise for
this task of continual learning, the best method for selecting which buffered
images to replay is still an open question. In this paper, we specifically
focus on the online class-incremental setting where a model needs to learn new
classes continually from an online data stream. To this end, we contribute a
novel Adversarial Shapley value scoring method that scores memory data samples
according to their ability to preserve latent decision boundaries for
previously observed classes (to maintain learning stability and avoid
forgetting) while interfering with latent decision boundaries of current
classes being learned (to encourage plasticity and optimal learning of new
class boundaries). Overall, we observe that our proposed ASER method provides
competitive or improved performance compared to state-of-the-art replay-based
continual learning methods on a variety of datasets.
</p>
<a href="http://arxiv.org/abs/2009.00093" target="_blank">arXiv:2009.00093</a> [<a href="http://arxiv.org/pdf/2009.00093" target="_blank">pdf</a>]

<h2>Can AutoML outperform humans? An evaluation on popular OpenML datasets using AutoML Benchmark. (arXiv:2009.01564v2 [cs.LG] UPDATED)</h2>
<h3>Marc Hanussek, Matthias Blohm, Maximilien Kintz</h3>
<p>In the last few years, Automated Machine Learning (AutoML) has gained much
attention. With that said, the question arises whether AutoML can outperform
results achieved by human data scientists. This paper compares four AutoML
frameworks on 12 different popular datasets from OpenML; six of them supervised
classification tasks and the other six supervised regression ones.
Additionally, we consider a real-life dataset from one of our recent projects.
The results show that the automated frameworks perform better or equal than the
machine learning community in 7 out of 12 OpenML tasks.
</p>
<a href="http://arxiv.org/abs/2009.01564" target="_blank">arXiv:2009.01564</a> [<a href="http://arxiv.org/pdf/2009.01564" target="_blank">pdf</a>]

<h2>Dual-Mandate Patrols: Multi-Armed Bandits for Green Security. (arXiv:2009.06560v2 [cs.LG] UPDATED)</h2>
<h3>Lily Xu, Elizabeth Bondi, Fei Fang, Andrew Perrault, Kai Wang, Milind Tambe</h3>
<p>Conservation efforts in green security domains to protect wildlife and
forests are constrained by the limited availability of defenders (i.e.,
patrollers), who must patrol vast areas to protect from attackers (e.g.,
poachers or illegal loggers). Defenders must choose how much time to spend in
each region of the protected area, balancing exploration of infrequently
visited regions and exploitation of known hotspots. We formulate the problem as
a stochastic multi-armed bandit, where each action represents a patrol
strategy, enabling us to guarantee the rate of convergence of the patrolling
policy. However, a naive bandit approach would compromise short-term
performance for long-term optimality, resulting in animals poached and forests
destroyed. To speed up performance, we leverage smoothness in the reward
function and decomposability of actions. We show a synergy between
Lipschitz-continuity and decomposition as each aids the convergence of the
other. In doing so, we bridge the gap between combinatorial and Lipschitz
bandits, presenting a no-regret approach that tightens existing guarantees
while optimizing for short-term performance. We demonstrate that our algorithm,
LIZARD, improves performance on real-world poaching data from Cambodia.
</p>
<a href="http://arxiv.org/abs/2009.06560" target="_blank">arXiv:2009.06560</a> [<a href="http://arxiv.org/pdf/2009.06560" target="_blank">pdf</a>]

<h2>ExGAN: Adversarial Generation of Extreme Samples. (arXiv:2009.08454v2 [cs.LG] UPDATED)</h2>
<h3>Siddharth Bhatia, Arjit Jain, Bryan Hooi</h3>
<p>Mitigating the risk arising from extreme events is a fundamental goal with
many applications, such as the modelling of natural disasters, financial
crashes, epidemics, and many others. To manage this risk, a vital step is to be
able to understand or generate a wide range of extreme scenarios. Existing
approaches based on Generative Adversarial Networks (GANs) excel at generating
realistic samples, but seek to generate typical samples, rather than extreme
samples. Hence, in this work, we propose ExGAN, a GAN-based approach to
generate realistic and extreme samples. To model the extremes of the training
distribution in a principled way, our work draws from Extreme Value Theory
(EVT), a probabilistic approach for modelling the extreme tails of
distributions. For practical utility, our framework allows the user to specify
both the desired extremeness measure, as well as the desired extremeness
probability they wish to sample at. Experiments on real US Precipitation data
show that our method generates realistic samples, based on visual inspection
and quantitative measures, in an efficient manner. Moreover, generating
increasingly extreme examples using ExGAN can be done in constant time (with
respect to the extremeness probability $\tau$), as opposed to the
$\mathcal{O}(\frac{1}{\tau})$ time required by the baseline approach.
</p>
<a href="http://arxiv.org/abs/2009.08454" target="_blank">arXiv:2009.08454</a> [<a href="http://arxiv.org/pdf/2009.08454" target="_blank">pdf</a>]

<h2>Objective, Probabilistic, and Generalized Noise Level Dependent Classifications of sets of more or less 2D Periodic Images into Plane Symmetry Groups. (arXiv:2009.08539v2 [cs.CV] UPDATED)</h2>
<h3>Andrew Dempsey, Peter Moeck</h3>
<p>Crystallographic symmetry classifications from real-world images with
periodicities in two dimensions (2D) are of interest to crystallographers and
practitioners of computer vision studies alike. Currently, these
classifications are typically made by both communities in a subjective manner
that relies on arbitrary thresholds for judgments, and are reported under the
pretense of being definitive, which is impossible. Moreover, the computer
vision community tends to use direct space methods to make such classifications
instead of more powerful and computationally efficient Fourier space methods.
This is because the proper functioning of those methods requires more periodic
repeats of a unit cell motif than are commonly present in images analyzed by
the computer vision community. We demonstrate a novel approach to plane
symmetry group classifications that is enabled by Kenichi Kanatani's Geometric
Akaike Information Criterion and associated Geometric Akaike weights. Our
approach leverages the advantages of working in Fourier space, is well suited
for handling the hierarchic nature of crystallographic symmetries, and yields
probabilistic results that are generalized noise level dependent. The latter
feature means crystallographic symmetry classifications can be updated when
less noisy image data and more accurate processing algorithms become available.
We demonstrate the ability of our approach to objectively estimate the plane
symmetry and pseudosymmetries of sets of synthetic 2D-periodic images with
varying amounts of red-green-blue and spread noise. Additionally, we suggest a
simple solution to the problem of too few periodic repeats in an input image
for practical application of Fourier space methods. In doing so, we effectively
solve the decades-old and heretofore intractable problem from computer vision
of symmetry detection and classification from images in the presence of noise.
</p>
<a href="http://arxiv.org/abs/2009.08539" target="_blank">arXiv:2009.08539</a> [<a href="http://arxiv.org/pdf/2009.08539" target="_blank">pdf</a>]

<h2>Deep N-ary Error Correcting Output Codes. (arXiv:2009.10465v4 [cs.CV] UPDATED)</h2>
<h3>Hao Zhang, Joey Tianyi Zhou, Tianying Wang, Ivor W. Tsang, Rick Siow Mong Goh</h3>
<p>Ensemble learning consistently improves the performance of multi-class
classification through aggregating a series of base classifiers. To this end,
data-independent ensemble methods like Error Correcting Output Codes (ECOC)
attract increasing attention due to its easiness of implementation and
parallelization. Specifically, traditional ECOCs and its general extension
N-ary ECOC decompose the original multi-class classification problem into a
series of independent simpler classification subproblems. Unfortunately,
integrating ECOCs, especially N-ary ECOC with deep neural networks, termed as
deep N-ary ECOC, is not straightforward and yet fully exploited in the
literature, due to the high expense of training base learners. To facilitate
the training of N-ary ECOC with deep learning base learners, we further propose
three different variants of parameter sharing architectures for deep N-ary
ECOC. To verify the generalization ability of deep N-ary ECOC, we conduct
experiments by varying the backbone with different deep neural network
architectures for both image and text classification tasks. Furthermore,
extensive ablation studies on deep N-ary ECOC show its superior performance
over other deep data-independent ensemble methods.
</p>
<a href="http://arxiv.org/abs/2009.10465" target="_blank">arXiv:2009.10465</a> [<a href="http://arxiv.org/pdf/2009.10465" target="_blank">pdf</a>]

<h2>Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning. (arXiv:2009.13891v3 [cs.LG] UPDATED)</h2>
<h3>Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, Wulong Liu</h3>
<p>Context, the embedding of previous collected trajectories, is a powerful
construct for Meta-Reinforcement Learning (Meta-RL) algorithms. By conditioning
on an effective context, Meta-RL policies can easily generalize to new tasks
within a few adaptation steps. We argue that improving the quality of context
involves answering two questions: 1. How to train a compact and sufficient
encoder that can embed the task-specific information contained in prior
trajectories? 2. How to collect informative trajectories of which the
corresponding context reflects the specification of tasks? To this end, we
propose a novel Meta-RL framework called CCM (Contrastive learning augmented
Context-based Meta-RL). We first focus on the contrastive nature behind
different tasks and leverage it to train a compact and sufficient context
encoder. Further, we train a separate exploration policy and theoretically
derive a new information-gain-based objective which aims to collect informative
trajectories in a few steps. Empirically, we evaluate our approaches on common
benchmarks as well as several complex sparse-reward environments. The
experimental results show that CCM outperforms state-of-the-art algorithms by
addressing previously mentioned problems respectively.
</p>
<a href="http://arxiv.org/abs/2009.13891" target="_blank">arXiv:2009.13891</a> [<a href="http://arxiv.org/pdf/2009.13891" target="_blank">pdf</a>]

<h2>CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG. (arXiv:2010.00104v2 [cs.LG] UPDATED)</h2>
<h3>Pritam Sarkar, Ali Etemad</h3>
<p>Electrocardiogram (ECG) is the electrical measurement of cardiac activity,
whereas Photoplethysmogram (PPG) is the optical measurement of volumetric
changes in blood circulation. While both signals are used for heart rate
monitoring, from a medical perspective, ECG is more useful as it carries
additional cardiac information. Despite many attempts toward incorporating ECG
sensing in smartwatches or similar wearable devices for continuous and reliable
cardiac monitoring, PPG sensors are the main feasible sensing solution
available. In order to tackle this problem, we propose CardioGAN, an
adversarial model which takes PPG as input and generates ECG as output. The
proposed network utilizes an attention-based generator to learn local salient
features, as well as dual discriminators to preserve the integrity of generated
data in both time and frequency domains. Our experiments show that the ECG
generated by CardioGAN provides more reliable heart rate measurements compared
to the original input PPG, reducing the error from 9.74 beats per minute
(measured from the PPG) to 2.89 (measured from the generated ECG).
</p>
<a href="http://arxiv.org/abs/2010.00104" target="_blank">arXiv:2010.00104</a> [<a href="http://arxiv.org/pdf/2010.00104" target="_blank">pdf</a>]

<h2>Understanding Catastrophic Overfitting in Single-step Adversarial Training. (arXiv:2010.01799v2 [cs.LG] UPDATED)</h2>
<h3>Hoki Kim, Woojin Lee, Jaewook Lee</h3>
<p>Although fast adversarial training has demonstrated both robustness and
efficiency, the problem of "catastrophic overfitting" has been observed. This
is a phenomenon in which, during single-step adversarial training, the robust
accuracy against projected gradient descent (PGD) suddenly decreases to 0%
after a few epochs, whereas the robust accuracy against fast gradient sign
method (FGSM) increases to 100%. In this paper, we demonstrate that
catastrophic overfitting is very closely related to the characteristic of
single-step adversarial training which uses only adversarial examples with the
maximum perturbation, and not all adversarial examples in the adversarial
direction, which leads to decision boundary distortion and a highly curved loss
surface. Based on this observation, we propose a simple method that not only
prevents catastrophic overfitting, but also overrides the belief that it is
difficult to prevent multi-step adversarial attacks with single-step
adversarial training.
</p>
<a href="http://arxiv.org/abs/2010.01799" target="_blank">arXiv:2010.01799</a> [<a href="http://arxiv.org/pdf/2010.01799" target="_blank">pdf</a>]

<h2>Flow-based Anomaly Detection. (arXiv:2010.03002v2 [cs.LG] UPDATED)</h2>
<h3>&#x141;ukasz Maziarka, Marek &#x15a;mieja, Marcin Sendera, &#x141;ukasz Struski, Jacek Tabor, Przemys&#x142;aw Spurek</h3>
<p>We propose OneFlow - a flow-based one-class classifier for anomaly (outliers)
detection that finds a minimal volume bounding region. Contrary to
density-based methods, OneFlow is constructed in such a way that its result
typically does not depend on the structure of outliers. This is caused by the
fact that during training the gradient of the cost function is propagated only
over the points located near to the decision boundary (behavior similar to the
support vectors in SVM). The combination of flow models and Bernstein quantile
estimator allows OneFlow to find a parametric form of bounding region, which
can be useful in various applications including describing shapes from 3D point
clouds. Experiments show that the proposed model outperforms related methods on
real-world anomaly detection problems.
</p>
<a href="http://arxiv.org/abs/2010.03002" target="_blank">arXiv:2010.03002</a> [<a href="http://arxiv.org/pdf/2010.03002" target="_blank">pdf</a>]

<h2>Hierarchical Relational Inference. (arXiv:2010.03635v2 [cs.LG] UPDATED)</h2>
<h3>Aleksandar Stani&#x107;, Sjoerd van Steenkiste, J&#xfc;rgen Schmidhuber</h3>
<p>Common-sense physical reasoning in the real world requires learning about the
interactions of objects and their dynamics. The notion of an abstract object,
however, encompasses a wide variety of physical objects that differ greatly in
terms of the complex behaviors they support. To address this, we propose a
novel approach to physical reasoning that models objects as hierarchies of
parts that may locally behave separately, but also act more globally as a
single whole. Unlike prior approaches, our method learns in an unsupervised
fashion directly from raw visual images to discover objects, parts, and their
relations. It explicitly distinguishes multiple levels of abstraction and
improves over a strong baseline at modeling synthetic and real-world videos.
</p>
<a href="http://arxiv.org/abs/2010.03635" target="_blank">arXiv:2010.03635</a> [<a href="http://arxiv.org/pdf/2010.03635" target="_blank">pdf</a>]

<h2>SWIFT: Scalable Wasserstein Factorization for Sparse Nonnegative Tensors. (arXiv:2010.04081v2 [cs.LG] UPDATED)</h2>
<h3>Ardavan Afshar, Kejing Yin, Sherry Yan, Cheng Qian, Joyce C. Ho, Haesun Park, Jimeng Sun</h3>
<p>Existing tensor factorization methods assume that the input tensor follows
some specific distribution (i.e. Poisson, Bernoulli, and Gaussian), and solve
the factorization by minimizing some empirical loss functions defined based on
the corresponding distribution. However, it suffers from several drawbacks: 1)
In reality, the underlying distributions are complicated and unknown, making it
infeasible to be approximated by a simple distribution. 2) The correlation
across dimensions of the input tensor is not well utilized, leading to
sub-optimal performance. Although heuristics were proposed to incorporate such
correlation as side information under Gaussian distribution, they can not
easily be generalized to other distributions. Thus, a more principled way of
utilizing the correlation in tensor factorization models is still an open
challenge. Without assuming any explicit distribution, we formulate the tensor
factorization as an optimal transport problem with Wasserstein distance, which
can handle non-negative inputs.

We introduce SWIFT, which minimizes the Wasserstein distance that measures
the distance between the input tensor and that of the reconstruction. In
particular, we define the N-th order tensor Wasserstein loss for the widely
used tensor CP factorization and derive the optimization algorithm that
minimizes it. By leveraging sparsity structure and different equivalent
formulations for optimizing computational efficiency, SWIFT is as scalable as
other well-known CP algorithms. Using the factor matrices as features, SWIFT
achieves up to 9.65% and 11.31% relative improvement over baselines for
downstream prediction tasks. Under the noisy conditions, SWIFT achieves up to
15% and 17% relative improvements over the best competitors for the prediction
tasks.
</p>
<a href="http://arxiv.org/abs/2010.04081" target="_blank">arXiv:2010.04081</a> [<a href="http://arxiv.org/pdf/2010.04081" target="_blank">pdf</a>]

<h2>Explaining Neural Matrix Factorization with Gradient Rollback. (arXiv:2010.05516v4 [cs.LG] UPDATED)</h2>
<h3>Carolin Lawrence, Timo Sztyler, Mathias Niepert</h3>
<p>Explaining the predictions of neural black-box models is an important
problem, especially when such models are used in applications where user trust
is crucial. Estimating the influence of training examples on a learned neural
model's behavior allows us to identify training examples most responsible for a
given prediction and, therefore, to faithfully explain the output of a
black-box model. The most generally applicable existing method is based on
influence functions, which scale poorly for larger sample sizes and models.

We propose gradient rollback, a general approach for influence estimation,
applicable to neural models where each parameter update step during gradient
descent touches a smaller number of parameters, even if the overall number of
parameters is large. Neural matrix factorization models trained with gradient
descent are part of this model class. These models are popular and have found a
wide range of applications in industry. Especially knowledge graph embedding
methods, which belong to this class, are used extensively. We show that
gradient rollback is highly efficient at both training and test time. Moreover,
we show theoretically that the difference between gradient rollback's influence
approximation and the true influence on a model's behavior is smaller than
known bounds on the stability of stochastic gradient descent. This establishes
that gradient rollback is robustly estimating example influence. We also
conduct experiments which show that gradient rollback provides faithful
explanations for knowledge base completion and recommender datasets.
</p>
<a href="http://arxiv.org/abs/2010.05516" target="_blank">arXiv:2010.05516</a> [<a href="http://arxiv.org/pdf/2010.05516" target="_blank">pdf</a>]

<h2>Vision-Aided Radio: User Identity Match in Radio and Video Domains Using Machine Learning. (arXiv:2010.07219v3 [cs.CV] UPDATED)</h2>
<h3>Vinicius M. de Pinho, Marcello L. R. de Campos, Luis Uzeda Garcia, Dalia Popescu</h3>
<p>5G is designed to be an essential enabler and a leading infrastructure
provider in the communication technology industry by supporting the demand for
the growing data traffic and a variety of services with distinct requirements.
The use of deep learning and computer vision tools has the means to increase
the environmental awareness of the network with information from visual data.
Information extracted via computer vision tools such as user position, movement
direction, and speed can be promptly available for the network. However, the
network must have a mechanism to match the identity of a user in both visual
and radio systems. This mechanism is absent in the present literature.
Therefore, we propose a framework to match the information from both visual and
radio domains. This is an essential step to practical applications of computer
vision tools in communications. We detail the proposed framework training and
deployment phases for a presented setup. We carried out practical experiments
using data collected in different types of environments. The work compares the
use of Deep Neural Network and Random Forest classifiers and shows that the
former performed better across all experiments, achieving classification
accuracy greater than 99%.
</p>
<a href="http://arxiv.org/abs/2010.07219" target="_blank">arXiv:2010.07219</a> [<a href="http://arxiv.org/pdf/2010.07219" target="_blank">pdf</a>]

<h2>Theoretical Foundations of Hyperdimensional Computing. (arXiv:2010.07426v2 [cs.LG] UPDATED)</h2>
<h3>Anthony Thomas, Sanjoy Dasgupta, Tajana Rosing</h3>
<p>Hyperdimensional (HD) computing is a set of neurally inspired methods for
obtaining high-dimensional, low-precision, distributed representations of data.
These representations can be combined with simple, neurally plausible
algorithms to effect a variety of information processing tasks. HD computing
has recently garnered significant interest from the computer hardware community
as an energy-efficient, low-latency, and noise-robust tool for solving learning
problems. In this review, we present a unified treatment of the theoretical
foundations of HD computing with a focus on the suitability of representations
for learning.
</p>
<a href="http://arxiv.org/abs/2010.07426" target="_blank">arXiv:2010.07426</a> [<a href="http://arxiv.org/pdf/2010.07426" target="_blank">pdf</a>]

<h2>Tight Second-Order Certificates for Randomized Smoothing. (arXiv:2010.10549v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Levine, Aounon Kumar, Thomas Goldstein, Soheil Feizi</h3>
<p>Randomized smoothing is a popular way of providing robustness guarantees
against adversarial attacks: randomly-smoothed functions have a universal
Lipschitz-like bound, allowing for robustness certificates to be easily
computed. In this work, we show that there also exists a universal
curvature-like bound for Gaussian random smoothing: given the exact value and
gradient of a smoothed function, we compute a lower bound on the distance of a
point to its closest adversarial example, called the Second-order Smoothing
(SoS) robustness certificate. In addition to proving the correctness of this
novel certificate, we show that SoS certificates are realizable and therefore
tight. Interestingly, we show that the maximum achievable benefits, in terms of
certified robustness, from using the additional information of the gradient
norm are relatively small: because our bounds are tight, this is a fundamental
negative result. The gain of SoS certificates further diminishes if we consider
the estimation error of the gradient norms, for which we have developed an
estimator. We therefore additionally develop a variant of Gaussian smoothing,
called Gaussian dipole smoothing, which provides similar bounds to randomized
smoothing with gradient information, but with much-improved sample efficiency.
This allows us to achieve (marginally) improved robustness certificates on
high-dimensional datasets such as CIFAR-10 and ImageNet. Code is available at
https://github.com/alevine0/smoothing_second_order.
</p>
<a href="http://arxiv.org/abs/2010.10549" target="_blank">arXiv:2010.10549</a> [<a href="http://arxiv.org/pdf/2010.10549" target="_blank">pdf</a>]

<h2>Face Hallucination Using Split-Attention in Split-Attention Network. (arXiv:2010.11575v2 [cs.CV] UPDATED)</h2>
<h3>Yuanzhi Wang, Tao Lu, Yu Wang, Yanduo Zhang, Wei Liu, Zhongyuan Wang</h3>
<p>Recently, attention mechanism has been applied into convolutional neural
networks(CNNs) based super-resolution (SR) tasks for exploring internal feature
map correlation. However, most of them ignore the correlation between
multi-path features channels for coarse-to-fine attention focusing. In this
paper, we propose a split-attention in split-attention network (SISN) to fuse
internal channel features and external (cross) multi-path features for
exploring face structure information. First, internal-feature split attention
block maintains the fidelity of facial local details. Then external-internal
split attention group provides cross-features interaction to finetune
multi-path features for stabilizing facial structure information.
External-feature fusion module is designed to fuse face structure and local
detail features for preserving the consistency of images from coarse-to-fine.
Experimental results demonstrate that the proposed approach consistently and
significantly improves the subjective and objective performances for face
hallucination over some state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2010.11575" target="_blank">arXiv:2010.11575</a> [<a href="http://arxiv.org/pdf/2010.11575" target="_blank">pdf</a>]

<h2>Multi-Graph Tensor Networks. (arXiv:2010.13209v3 [cs.LG] UPDATED)</h2>
<h3>Yao Lei Xu, Kriton Konstantinidis, Danilo P. Mandic</h3>
<p>The irregular and multi-modal nature of numerous modern data sources poses
serious challenges for traditional deep learning algorithms. To this end,
recent efforts have generalized existing algorithms to irregular domains
through graphs, with the aim to gain additional insights from data through the
underlying graph topology. At the same time, tensor-based methods have
demonstrated promising results in bypassing the bottlenecks imposed by the
Curse of Dimensionality. In this paper, we introduce a novel Multi-Graph Tensor
Network (MGTN) framework, which exploits both the ability of graphs to handle
irregular data sources and the compression properties of tensor networks in a
deep learning setting. The potential of the proposed framework is demonstrated
through an MGTN based deep Q agent for Foreign Exchange (FOREX) algorithmic
trading. By virtue of the MGTN, a FOREX currency graph is leveraged to impose
an economically meaningful structure on this demanding task, resulting in a
highly superior performance against three competing models and at a drastically
lower complexity.
</p>
<a href="http://arxiv.org/abs/2010.13209" target="_blank">arXiv:2010.13209</a> [<a href="http://arxiv.org/pdf/2010.13209" target="_blank">pdf</a>]

<h2>On the intrinsic robustness to noise of some leading classifiers and symetric loss function -- an empirical evaluation. (arXiv:2010.13570v3 [cs.LG] UPDATED)</h2>
<h3>Hugo Le Baher (1), Vincent Lemaire (2), Romain Trinquart (2) ((1) Polytech Nantes (France), (2) Orange Labs (France))</h3>
<p>In some industrial applications such as fraud detection, the performance of
common supervision techniques may be affected by the poor quality of the
available labels : in actual operational use-cases, these labels may be weak in
quantity, quality or trustworthiness. We propose a benchmark to evaluate the
natural robustness of different algorithms taken from various paradigms on
artificially corrupted datasets, with a focus on noisy labels. This paper
studies the intrinsic robustness of some leading classifiers. The algorithms
under scrutiny include SVM, logistic regression, random forests, XGBoost,
Khiops. Furthermore, building on results from recent literature, the study is
supplemented with an investigation into the opportunity to enhance some
algorithms with symmetric loss functions.
</p>
<a href="http://arxiv.org/abs/2010.13570" target="_blank">arXiv:2010.13570</a> [<a href="http://arxiv.org/pdf/2010.13570" target="_blank">pdf</a>]

<h2>Proceedings of the AI-HRI Symposium at AAAI-FSS 2020. (arXiv:2010.13830v4 [cs.RO] UPDATED)</h2>
<h3>Shelly Bagchi, Jason R. Wilson, Muneeb I. Ahmad, Christian Dondrup, Zhao Han, Justin W. Hart, Matteo Leonetti, Katrin Lohan, Ross Mead, Emmanuel Senft, Jivko Sinapov, Megan L. Zimmerman</h3>
<p>The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. In that
time, the related topic of trust in robotics has been rapidly growing, with
major research efforts at universities and laboratories across the world.
Indeed, many of the past participants in AI-HRI have been or are now involved
with research into trust in HRI. While trust has no consensus definition, it is
regularly associated with predictability, reliability, inciting confidence, and
meeting expectations. Furthermore, it is generally believed that trust is
crucial for adoption of both AI and robotics, particularly when transitioning
technologies from the lab to industrial, social, and consumer applications.
However, how does trust apply to the specific situations we encounter in the
AI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a
growing need for research that lives directly at the intersection of AI and HRI
that is serviced by this symposium. Over the course of the two-day meeting, we
propose to create a collaborative forum for discussion of current efforts in
trust for AI-HRI, with a sub-session focused on the related topic of
explainable AI (XAI) for HRI.
</p>
<a href="http://arxiv.org/abs/2010.13830" target="_blank">arXiv:2010.13830</a> [<a href="http://arxiv.org/pdf/2010.13830" target="_blank">pdf</a>]

<h2>Experimental design for MRI by greedy policy search. (arXiv:2010.16262v2 [cs.CV] UPDATED)</h2>
<h3>Tim Bakker, Herke van Hoof, Max Welling</h3>
<p>In today's clinical practice, magnetic resonance imaging (MRI) is routinely
accelerated through subsampling of the associated Fourier domain. Currently,
the construction of these subsampling strategies - known as experimental design
- relies primarily on heuristics. We propose to learn experimental design
strategies for accelerated MRI with policy gradient methods. Unexpectedly, our
experiments show that a simple greedy approximation of the objective leads to
solutions nearly on-par with the more general non-greedy approach. We offer a
partial explanation for this phenomenon rooted in greater variance in the
non-greedy objective's gradient estimates, and experimentally verify that this
variance hampers non-greedy models in adapting their policies to individual MR
images. We empirically show that this adaptivity is key to improving
subsampling designs.
</p>
<a href="http://arxiv.org/abs/2010.16262" target="_blank">arXiv:2010.16262</a> [<a href="http://arxiv.org/pdf/2010.16262" target="_blank">pdf</a>]

<h2>A Comprehensive Study of Class Incremental Learning Algorithms for Visual Tasks. (arXiv:2011.01844v4 [cs.LG] UPDATED)</h2>
<h3>Eden Belouadah, Adrian Popescu, Ioannis Kanellos</h3>
<p>The ability of artificial agents to increment their capabilities when
confronted with new data is an open challenge in artificial intelligence. The
main challenge faced in such cases is catastrophic forgetting, i.e., the
tendency of neural networks to underfit past data when new ones are ingested. A
first group of approaches tackles forgetting by increasing deep model capacity
to accommodate new knowledge. A second type of approaches fix the deep model
size and introduce a mechanism whose objective is to ensure a good compromise
between stability and plasticity of the model. While the first type of
algorithms were compared thoroughly, this is not the case for methods which
exploit a fixed size model. Here, we focus on the latter, place them in a
common conceptual and experimental framework and propose the following
contributions: (1) define six desirable properties of incremental learning
algorithms and analyze them according to these properties, (2) introduce a
unified formalization of the class-incremental learning problem, (3) propose a
common evaluation framework which is more thorough than existing ones in terms
of number of datasets, size of datasets, size of bounded memory and number of
incremental states, (4) investigate the usefulness of herding for past
exemplars selection, (5) provide experimental evidence that it is possible to
obtain competitive performance without the use of knowledge distillation to
tackle catastrophic forgetting and (6) facilitate reproducibility by
integrating all tested methods in a common open-source repository. The main
experimental finding is that none of the existing algorithms achieves the best
results in all evaluated settings. Important differences arise notably if a
bounded memory of past classes is allowed or not.
</p>
<a href="http://arxiv.org/abs/2011.01844" target="_blank">arXiv:2011.01844</a> [<a href="http://arxiv.org/pdf/2011.01844" target="_blank">pdf</a>]

<h2>A Few Shot Adaptation of Visual Navigation Skills to New Observations using Meta-Learning. (arXiv:2011.03609v2 [cs.RO] UPDATED)</h2>
<h3>Qian Luo, Maks Sorokin, Sehoon Ha</h3>
<p>Target-driven visual navigation is a challenging problem that requires a
robot to find the goal using only visual inputs. Many researchers have
demonstrated promising results using deep reinforcement learning (deep RL) on
various robotic platforms, but typical end-to-end learning is known for its
poor extrapolation capability to new scenarios. Therefore, learning a
navigation policy for a new robot with a new sensor configuration or a new
target still remains a challenging problem. In this paper, we introduce a
learning algorithm that enables rapid adaptation to new sensor configurations
or target objects with a few shots. We design a policy architecture with latent
features between perception and inference networks and quickly adapt the
perception network via meta-learning while freezing the inference network. Our
experiments show that our algorithm adapts the learned navigation policy with
only three shots for unseen situations with different sensor configurations or
different target colors. We also analyze the proposed algorithm by
investigating various hyperparameters.
</p>
<a href="http://arxiv.org/abs/2011.03609" target="_blank">arXiv:2011.03609</a> [<a href="http://arxiv.org/pdf/2011.03609" target="_blank">pdf</a>]

<h2>Recursive Tree Grammar Autoencoders. (arXiv:2012.02097v2 [cs.LG] UPDATED)</h2>
<h3>Benjamin Paassen, Irena Koprinska, Kalina Yacef</h3>
<p>Machine learning on tree data has been mostly focused on trees as input. Much
less research has investigates trees as output, like in molecule optimization
for drug discovery or hint generation for intelligent tutoring systems. In this
work, we propose a novel autoencoder approach, called recursive tree grammar
autoencoder (RTG-AE), which encodes trees via a bottom-up parser and decodes
trees via a tree grammar, both controlled by neural networks that minimize the
variational autoencoder loss. The resulting encoding and decoding functions can
then be employed in subsequent tasks, such as optimization and time series
prediction. RTG-AE combines variational autoencoders, grammatical knowledge,
and recursive processing. Our key message is that this combination improves
performance compared to only combining two of these three components. In
particular, we show experimentally that our proposed method improves the
autoencoding error, training time, and optimization score on four benchmark
datasets compared to baselines from the literature.
</p>
<a href="http://arxiv.org/abs/2012.02097" target="_blank">arXiv:2012.02097</a> [<a href="http://arxiv.org/pdf/2012.02097" target="_blank">pdf</a>]

<h2>Logic Synthesis Meets Machine Learning: Trading Exactness for Generalization. (arXiv:2012.02530v2 [cs.LG] UPDATED)</h2>
<h3>Shubham Rai, Walter Lau Neto, Yukio Miyasaka, Xinpei Zhang, Mingfei Yu, Qingyang Yi Masahiro Fujita, Guilherme B. Manske, Matheus F. Pontes, Leomar S. da Rosa Junior, Marilton S. de Aguiar, Paulo F. Butzen, Po-Chun Chien, Yu-Shan Huang, Hoa-Ren Wang, Jie-Hong R. Jiang, Jiaqi Gu, Zheng Zhao, Zixuan Jiang, David Z. Pan, Brunno A. de Abreu, Isac de Souza Campos, Augusto Berndt, Cristina Meinhardt, Jonata T. Carvalho, Mateus Grellert, Sergio Bampi, Aditya Lohana, Akash Kumar, Wei Zeng, Azadeh Davoodi, Rasit O. Topaloglu, Yuan Zhou, Jordan Dotzel, Yichi Zhang, Hanyu Wang, Zhiru Zhang, Valerio Tenace, Pierre-Emmanuel Gaillardon, Alan Mishchenko, Satrajit Chatterjee</h3>
<p>Logic synthesis is a fundamental step in hardware design whose goal is to
find structural representations of Boolean functions while minimizing delay and
area. If the function is completely-specified, the implementation accurately
represents the function. If the function is incompletely-specified, the
implementation has to be true only on the care set. While most of the
algorithms in logic synthesis rely on SAT and Boolean methods to exactly
implement the care set, we investigate learning in logic synthesis, attempting
to trade exactness for generalization. This work is directly related to machine
learning where the care set is the training set and the implementation is
expected to generalize on a validation set. We present learning
incompletely-specified functions based on the results of a competition
conducted at IWLS 2020. The goal of the competition was to implement 100
functions given by a set of care minterms for training, while testing the
implementation using a set of validation minterms sampled from the same
function. We make this benchmark suite available and offer a detailed
comparative analysis of the different approaches to learning
</p>
<a href="http://arxiv.org/abs/2012.02530" target="_blank">arXiv:2012.02530</a> [<a href="http://arxiv.org/pdf/2012.02530" target="_blank">pdf</a>]

<h2>Dynamic Anchor Learning for Arbitrary-Oriented Object Detection. (arXiv:2012.04150v2 [cs.CV] UPDATED)</h2>
<h3>Qi Ming, Zhiqiang Zhou, Lingjuan Miao, Hongwei Zhang, Linhao Li</h3>
<p>Arbitrary-oriented objects widely appear in natural scenes, aerial
photographs, remote sensing images, etc., thus arbitrary-oriented object
detection has received considerable attention. Many current rotation detectors
use plenty of anchors with different orientations to achieve spatial alignment
with ground truth boxes, then Intersection-over-Union (IoU) is applied to
sample the positive and negative candidates for training. However, we observe
that the selected positive anchors cannot always ensure accurate detections
after regression, while some negative samples can achieve accurate
localization. It indicates that the quality assessment of anchors through IoU
is not appropriate, and this further lead to inconsistency between
classification confidence and localization accuracy. In this paper, we propose
a dynamic anchor learning (DAL) method, which utilizes the newly defined
matching degree to comprehensively evaluate the localization potential of the
anchors and carry out a more efficient label assignment process. In this way,
the detector can dynamically select high-quality anchors to achieve accurate
object detection, and the divergence between classification and regression will
be alleviated. With the newly introduced DAL, we achieve superior detection
performance for arbitrary-oriented objects with only a few horizontal preset
anchors. Experimental results on three remote sensing datasets HRSC2016, DOTA,
UCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method
achieves substantial improvement compared with the baseline model. Besides, our
approach is also universal for object detection using horizontal bound box. The
code and models are available at https://github.com/ming71/DAL.
</p>
<a href="http://arxiv.org/abs/2012.04150" target="_blank">arXiv:2012.04150</a> [<a href="http://arxiv.org/pdf/2012.04150" target="_blank">pdf</a>]

<h2>Concept Drift and Covariate Shift Detection Ensemble with Lagged Labels. (arXiv:2012.04759v3 [cs.AI] UPDATED)</h2>
<h3>Yiming Xu, Diego Klabjan</h3>
<p>In model serving, having one fixed model during the entire often life-long
inference process is usually detrimental to model performance, as data
distribution evolves over time, resulting in lack of reliability of the model
trained on historical data. It is important to detect changes and retrain the
model in time. The existing methods generally have three weaknesses: 1) using
only classification error rate as signal, 2) assuming ground truth labels are
immediately available after features from samples are received and 3) unable to
decide what data to use to retrain the model when change occurs. We address the
first problem by utilizing six different signals to capture a wide range of
characteristics of data, and we address the second problem by allowing lag of
labels, where labels of corresponding features are received after a lag in
time. For the third problem, our proposed method automatically decides what
data to use to retrain based on the signals. Extensive experiments on
structured and unstructured data for different type of data changes establish
that our method consistently outperforms the state-of-the-art methods by a
large margin.
</p>
<a href="http://arxiv.org/abs/2012.04759" target="_blank">arXiv:2012.04759</a> [<a href="http://arxiv.org/pdf/2012.04759" target="_blank">pdf</a>]

<h2>Implicit Regularization in ReLU Networks with the Square Loss. (arXiv:2012.05156v2 [cs.LG] UPDATED)</h2>
<h3>Gal Vardi, Ohad Shamir</h3>
<p>Understanding the implicit regularization (or implicit bias) of gradient
descent has recently been a very active research area. However, the implicit
regularization in nonlinear neural networks is still poorly understood,
especially for regression losses such as the square loss. Perhaps surprisingly,
we prove that even for a single ReLU neuron, it is impossible to characterize
the implicit regularization with the square loss by any explicit function of
the model parameters (although on the positive side, we show it can be
characterized approximately). For one hidden-layer networks, we prove a similar
result, where in general it is impossible to characterize implicit
regularization properties in this manner, except for the "balancedness"
property identified in Du et al. [2018]. Our results suggest that a more
general framework than the one considered so far may be needed to understand
implicit regularization for nonlinear predictors, and provides some clues on
what this framework should be.
</p>
<a href="http://arxiv.org/abs/2012.05156" target="_blank">arXiv:2012.05156</a> [<a href="http://arxiv.org/pdf/2012.05156" target="_blank">pdf</a>]

<h2>SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains. (arXiv:2012.05535v3 [cs.CV] UPDATED)</h2>
<h3>Yuanqi Chen, Ge Li, Cece Jin, Shan Liu, Thomas Li</h3>
<p>This paper observes that there is an issue of high frequencies missing in the
discriminator of standard GAN, and we reveal it stems from downsampling layers
employed in the network architecture. This issue makes the generator lack the
incentive from the discriminator to learn high-frequency content of data,
resulting in a significant spectrum discrepancy between generated images and
real images. Since the Fourier transform is a bijective mapping, we argue that
reducing this spectrum discrepancy would boost the performance of GANs. To this
end, we introduce SSD-GAN, an enhancement of GANs to alleviate the spectral
information loss in the discriminator. Specifically, we propose to embed a
frequency-aware classifier into the discriminator to measure the realness of
the input in both the spatial and spectral domains. With the enhanced
discriminator, the generator of SSD-GAN is encouraged to learn high-frequency
content of real data and generate exact details. The proposed method is general
and can be easily integrated into most existing GANs framework without
excessive cost. The effectiveness of SSD-GAN is validated on various network
architectures, objective functions, and datasets. Code will be available at
https://github.com/cyq373/SSD-GAN.
</p>
<a href="http://arxiv.org/abs/2012.05535" target="_blank">arXiv:2012.05535</a> [<a href="http://arxiv.org/pdf/2012.05535" target="_blank">pdf</a>]

<h2>Slimmable Generative Adversarial Networks. (arXiv:2012.05660v2 [cs.LG] UPDATED)</h2>
<h3>Liang Hou, Zehuan Yuan, Lei Huang, Huawei Shen, Xueqi Cheng, Changhu Wang</h3>
<p>Generative adversarial networks (GANs) have achieved remarkable progress in
recent years, but the continuously growing scale of models make them
challenging to deploy widely in practical applications. In particular, for
real-time generation tasks, different devices require generators of different
sizes due to varying computing power. In this paper, we introduce slimmable
GANs (SlimGANs), which can flexibly switch the width of the generator to
accommodate various quality-efficiency trade-offs at runtime. Specifically, we
leverage multiple discriminators that share partial parameters to train the
slimmable generator. To facilitate the consistency between generators of
different widths, we present a stepwise inplace distillation technique that
encourages narrow generators to learn from wide ones. As for class-conditional
generation, we propose a sliceable conditional batch normalization that
incorporates the label information into different widths. Our methods are
validated, both quantitatively and qualitatively, by extensive experiments and
a detailed ablation study.
</p>
<a href="http://arxiv.org/abs/2012.05660" target="_blank">arXiv:2012.05660</a> [<a href="http://arxiv.org/pdf/2012.05660" target="_blank">pdf</a>]

<h2>Efficient Human Pose Estimation by Learning Deeply Aggregated Representations. (arXiv:2012.07033v2 [cs.CV] UPDATED)</h2>
<h3>Zhengxiong Luo, Zhicheng Wang, Yuanhao Cai, Guanan Wang, Yan Huang, Liang Wang, Erjin Zhou, Tieniu Tan, Jian Sun</h3>
<p>In this paper, we propose an efficient human pose estimation network (DANet)
by learning deeply aggregated representations. Most existing models explore
multi-scale information mainly from features with different spatial sizes.
Powerful multi-scale representations usually rely on the cascaded pyramid
framework. This framework largely boosts the performance but in the meanwhile
makes networks very deep and complex. Instead, we focus on exploiting
multi-scale information from layers with different receptive-field sizes and
then making full of use this information by improving the fusion method.
Specifically, we propose an orthogonal attention block (OAB) and a second-order
fusion unit (SFU). The OAB learns multi-scale information from different layers
and enhances them by encouraging them to be diverse. The SFU adaptively selects
and fuses diverse multi-scale information and suppress the redundant ones. This
could maximize the effective information in final fused representations. With
the help of OAB and SFU, our single pyramid network may be able to generate
deeply aggregated representations that contain even richer multi-scale
information and have a larger representing capacity than that of cascaded
networks. Thus, our networks could achieve comparable or even better accuracy
with much smaller model complexity. Specifically, our \mbox{DANet-72} achieves
$70.5$ in AP score on COCO test-dev set with only $1.0G$ FLOPs. Its speed on a
CPU platform achieves $58$ Persons-Per-Second~(PPS).
</p>
<a href="http://arxiv.org/abs/2012.07033" target="_blank">arXiv:2012.07033</a> [<a href="http://arxiv.org/pdf/2012.07033" target="_blank">pdf</a>]

<h2>Monitoring multimode processes: a modified PCA algorithm with continual learning ability. (arXiv:2012.07044v2 [stat.ML] UPDATED)</h2>
<h3>Jingxin Zhang, Donghua Zhou, Maoyin Chen</h3>
<p>For multimode processes, one has to establish local monitoring models
corresponding to local modes. However, the significant features of previous
modes may be catastrophically forgotten when a monitoring model for the current
mode is built. It would result in an abrupt performance decrease. Is it
possible to make local monitoring model remember the features of previous
modes? Choosing the principal component analysis (PCA) as a basic monitoring
model, we try to resolve this problem. A modified PCA algorithm is built with
continual learning ability for monitoring multimode processes, which adopts
elastic weight consolidation (EWC) to overcome catastrophic forgetting of PCA
for successive modes. It is called PCA-EWC, where the significant features of
previous modes are preserved when a PCA model is established for the current
mode. The computational complexity and key parameters are discussed to further
understand the relationship between PCA and the proposed algorithm. Numerical
case study and a practical industrial system in China are employed to
illustrate the effectiveness of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2012.07044" target="_blank">arXiv:2012.07044</a> [<a href="http://arxiv.org/pdf/2012.07044" target="_blank">pdf</a>]

<h2>Adaptive Algorithms for Multi-armed Bandit with Composite and Anonymous Feedback. (arXiv:2012.07048v2 [cs.LG] UPDATED)</h2>
<h3>Siwei Wang, Haoyun Wang, Longbo Huang</h3>
<p>We study the multi-armed bandit (MAB) problem with composite and anonymous
feedback. In this model, the reward of pulling an arm spreads over a period of
time (we call this period as reward interval) and the player receives partial
rewards of the action, convoluted with rewards from pulling other arms,
successively. Existing results on this model require prior knowledge about the
reward interval size as an input to their algorithms. In this paper, we propose
adaptive algorithms for both the stochastic and the adversarial cases, without
requiring any prior information about the reward interval. For the stochastic
case, we prove that our algorithm guarantees a regret that matches the lower
bounds (in order). For the adversarial case, we propose the first algorithm to
jointly handle non-oblivious adversary and unknown reward interval size. We
also conduct simulations based on real-world dataset. The results show that our
algorithms outperform existing benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.07048" target="_blank">arXiv:2012.07048</a> [<a href="http://arxiv.org/pdf/2012.07048" target="_blank">pdf</a>]

<h2>A Memory-Augmented Neural Network Model of Abstract Rule Learning. (arXiv:2012.07172v2 [cs.AI] UPDATED)</h2>
<h3>Ishan Sinha, Taylor W. Webb, Jonathan D. Cohen</h3>
<p>Human intelligence is characterized by a remarkable ability to infer abstract
rules from experience and apply these rules to novel domains. As such,
designing neural network algorithms with this capacity is an important step
toward the development of deep learning systems with more human-like
intelligence. However, doing so is a major outstanding challenge, one that some
argue will require neural networks to use explicit symbol-processing
mechanisms. In this work, we focus on neural networks' capacity for arbitrary
role-filler binding, the ability to associate abstract "roles" to
context-specific "fillers," which many have argued is an important mechanism
underlying the ability to learn and apply rules abstractly. Using a simplified
version of Raven's Progressive Matrices, a hallmark test of human intelligence,
we introduce a sequential formulation of a visual problem-solving task that
requires this form of binding. Further, we introduce the Emergent Symbol
Binding Network (ESBN), a recurrent neural network model that learns to use an
external memory as a binding mechanism. This mechanism enables symbol-like
variable representations to emerge through the ESBN's training process without
the need for explicit symbol-processing machinery. We empirically demonstrate
that the ESBN successfully learns the underlying abstract rule structure of our
task and perfectly generalizes this rule structure to novel fillers.
</p>
<a href="http://arxiv.org/abs/2012.07172" target="_blank">arXiv:2012.07172</a> [<a href="http://arxiv.org/pdf/2012.07172" target="_blank">pdf</a>]

<h2>A One-Size-Fits-All Solution to Conservative Bandit Problems. (arXiv:2012.07341v2 [cs.LG] UPDATED)</h2>
<h3>Yihan Du, Siwei Wang, Longbo Huang</h3>
<p>In this paper, we study a family of conservative bandit problems (CBPs) with
sample-path reward constraints, i.e., the learner's reward performance must be
at least as well as a given baseline at any time. We propose a
One-Size-Fits-All solution to CBPs and present its applications to three
encompassed problems, i.e. conservative multi-armed bandits (CMAB),
conservative linear bandits (CLB) and conservative contextual combinatorial
bandits (CCCB). Different from previous works which consider high probability
constraints on the expected reward, we focus on a sample-path constraint on the
actually received reward, and achieve better theoretical guarantees
($T$-independent additive regrets instead of $T$-dependent) and empirical
performance. Furthermore, we extend the results and consider a novel
conservative mean-variance bandit problem (MV-CBP), which measures the learning
performance with both the expected reward and variability. For this extended
problem, we provide a novel algorithm with $O(1/T)$ normalized additive regrets
($T$-independent in the cumulative form) and validate this result through
empirical evaluation.
</p>
<a href="http://arxiv.org/abs/2012.07341" target="_blank">arXiv:2012.07341</a> [<a href="http://arxiv.org/pdf/2012.07341" target="_blank">pdf</a>]

<h2>Phase Retrieval with Holography and Untrained Priors: Tackling the Challenges of Low-Photon Nanoscale Imaging. (arXiv:2012.07386v2 [cs.LG] UPDATED)</h2>
<h3>Hannah Lawrence, David A. Barmherzig, Henry Li, Michael Eickenberg, Marylou Gabri&#xe9;</h3>
<p>Phase retrieval is the inverse problem of recovering a signal from
magnitude-only Fourier measurements, and underlies numerous imaging modalities,
such as Coherent Diffraction Imaging (CDI). A variant of this setup, known as
holography, includes a reference object that is placed adjacent to the specimen
of interest before measurements are collected. The resulting inverse problem,
known as holographic phase retrieval, is well-known to have improved problem
conditioning relative to the original. This innovation, i.e. Holographic CDI,
becomes crucial at the nanoscale, where imaging specimens such as viruses,
proteins, and crystals require low-photon measurements. This data is highly
corrupted by Poisson shot noise, and often lacks low-frequency content as well.
In this work, we introduce a dataset-free deep learning framework for
holographic phase retrieval adapted to these challenges. The key ingredients of
our approach are the explicit and flexible incorporation of the physical
forward model into an automatic differentiation procedure, the Poisson
log-likelihood objective function, and an optional untrained deep image prior.
We perform extensive evaluation under realistic conditions. Compared to
competing classical methods, our method recovers signal from higher noise
levels and is more resilient to suboptimal reference design, as well as to
large missing regions of low frequencies in the observations. To the best of
our knowledge, this is the first work to consider a dataset-free machine
learning approach for holographic phase retrieval.
</p>
<a href="http://arxiv.org/abs/2012.07386" target="_blank">arXiv:2012.07386</a> [<a href="http://arxiv.org/pdf/2012.07386" target="_blank">pdf</a>]

<h2>Aggregative Self-Supervised Feature Learning. (arXiv:2012.07477v2 [cs.CV] UPDATED)</h2>
<h3>Jiuwen Zhu, Yuexiang Li, S. Kevin Zhou</h3>
<p>Self-supervised learning (SSL) is an efficient approach that addresses the
issue of annotation shortage. The key part in SSL is its proxy task that
defines the supervisory signals and drives the learning toward effective
feature representations. However, most SSL approaches usually focus on a single
proxy task, which greatly limits the expressive power of the learned features
and therefore deteriorates the network generalization capacity. In this regard,
we hereby propose three strategies of aggregation in terms of complementarity
of various forms to boost the robustness of self-supervised learned features.
In spatial context aggregative SSL, we contribute a heuristic SSL method that
integrates two ad-hoc proxy tasks with spatial context complementarity,
modeling global and local contextual features, respectively. We then propose a
principled framework of multi-task aggregative self-supervised learning to form
a unified representation, with an intent of exploiting feature complementarity
among different tasks. Finally, in self-aggregative SSL, we propose to
self-complement an existing proxy task with an auxiliary loss function based on
a linear centered kernel alignment metric, which explicitly promotes the
exploring of where are uncovered by the features learned from a proxy task at
hand to further boost the modeling capability. Our extensive experiments on 2D
natural image and 3D medical image classification tasks under limited
annotation scenarios confirm that the proposed aggregation strategies
successfully boost the classification accuracy.
</p>
<a href="http://arxiv.org/abs/2012.07477" target="_blank">arXiv:2012.07477</a> [<a href="http://arxiv.org/pdf/2012.07477" target="_blank">pdf</a>]

<h2>FlowMOT: 3D Multi-Object Tracking by Scene Flow Association. (arXiv:2012.07541v2 [cs.CV] UPDATED)</h2>
<h3>Guangyao Zhai, Xin Kong, Jinhao Cui, Yong Liu, Zhen Yang</h3>
<p>Most end-to-end Multi-Object Tracking (MOT) methods face the problems of low
accuracy and poor generalization ability. Although traditional filter-based
methods can achieve better results, they are difficult to be endowed with
optimal hyperparameters and often fail in varying scenarios. To alleviate these
drawbacks, we propose a LiDAR-based 3D MOT framework named FlowMOT, which
integrates point-wise motion information into the traditional matching
algorithm, enhancing the robustness of the data association. We firstly utilize
a scene flow estimation network to obtain implicit motion information between
two adjacent frames and calculate the predicted detection for each old tracklet
in the previous frame. Then we use Hungarian algorithm to generate optimal
matching relations with the ID propagation strategy to finish the tracking
task. Experiments on KITTI MOT dataset show that our approach outperforms
recent end-to-end methods and achieves competitive performance with the
state-of-the-art filter-based method. In addition, ours can work steadily in
the various-speed scenes where the filter-based methods may fail.
</p>
<a href="http://arxiv.org/abs/2012.07541" target="_blank">arXiv:2012.07541</a> [<a href="http://arxiv.org/pdf/2012.07541" target="_blank">pdf</a>]

<h2>WDNet: Watermark-Decomposition Network for Visible Watermark Removal. (arXiv:2012.07616v2 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Zhen Zhu, Xiang Bai</h3>
<p>Visible watermarks are widely-used in images to protect copyright ownership.
Analyzing watermark removal helps to reinforce the anti-attack techniques in an
adversarial way. Current removal methods normally leverage image-to-image
translation techniques. Nevertheless, the uncertainty of the size, shape, color
and transparency of the watermarks set a huge barrier for these methods. To
combat this, we combine traditional watermarked image decomposition into a
two-stage generator, called Watermark-Decomposition Network (WDNet), where the
first stage predicts a rough decomposition from the whole watermarked image and
the second stage specifically centers on the watermarked area to refine the
removal results. The decomposition formulation enables WDNet to separate
watermarks from the images rather than simply removing them. We further show
that these separated watermarks can serve as extra nutrients for building a
larger training dataset and further improving removal performance. Besides, we
construct a large-scale dataset named CLWD, which mainly contains colored
watermarks, to fill the vacuum of colored watermark removal dataset. Extensive
experiments on the public gray-scale dataset LVW and CLWD consistently show
that the proposed WDNet outperforms the state-of-the-art approaches both in
accuracy and efficiency. The code and CLWD dataset are publicly available at
https://github.com/MRUIL/WDNet.
</p>
<a href="http://arxiv.org/abs/2012.07616" target="_blank">arXiv:2012.07616</a> [<a href="http://arxiv.org/pdf/2012.07616" target="_blank">pdf</a>]

<h2>Decoupled Self Attention for Accurate One Stage Object Detection. (arXiv:2012.07630v2 [cs.CV] UPDATED)</h2>
<h3>Kehe WU, Zuge Chen, Qi MA, Xiaoliang Zhang, Wei Li</h3>
<p>As the scale of object detection dataset is smaller than that of image
recognition dataset ImageNet, transfer learning has become a basic training
method for deep learning object detection models, which will pretrain the
backbone network of object detection model on ImageNet dataset to extract
features for classification and localization subtasks. However, the
classification task focuses on the salient region features of object, while the
location task focuses on the edge features of object, so there is certain
deviation between the features extracted by pretrained backbone network and the
features used for localization task. In order to solve this problem, a
decoupled self attention(DSA) module is proposed for one stage object detection
models in this paper. DSA includes two decoupled self-attention branches, so it
can extract appropriate features for different tasks. It is located between FPN
and head networks of subtasks, so it is used to extract global features based
on FPN fused features for different tasks independently. Although the network
of DSA module is simple, but it can effectively improve the performance of
object detection, also it can be easily embedded in many detection models. Our
experiments are based on the representative one-stage detection model
RetinaNet. In COCO dataset, when ResNet50 and ResNet101 are used as backbone
networks, the detection performances can be increased by 0.4% AP and 0.5% AP
respectively. When DSA module and object confidence task are applied in
RetinaNet together, the detection performances based on ResNet50 and ResNet101
can be increased by 1.0% AP and 1.4% AP respectively. The experiment results
show the effectiveness of DSA module. Code is at:
https://github.com/chenzuge1/DSANet.git.
</p>
<a href="http://arxiv.org/abs/2012.07630" target="_blank">arXiv:2012.07630</a> [<a href="http://arxiv.org/pdf/2012.07630" target="_blank">pdf</a>]

<h2>Evolutionary learning of interpretable decision trees. (arXiv:2012.07723v2 [cs.LG] UPDATED)</h2>
<h3>Leonardo Lucio Custode, Giovanni Iacca</h3>
<p>Reinforcement learning techniques achieved human-level performance in several
tasks in the last decade. However, in recent years, the need for
interpretability emerged: we want to be able to understand how a system works
and the reasons behind its decisions. Not only we need interpretability to
assess the safety of the produced systems, we also need it to extract knowledge
about unknown problems. While some techniques that optimize decision trees for
reinforcement learning do exist, they usually employ greedy algorithms or they
do not exploit the rewards given by the environment. This means that these
techniques may easily get stuck in local optima. In this work, we propose a
novel approach to interpretable reinforcement learning that uses decision
trees. We present a two-level optimization scheme that combines the advantages
of evolutionary algorithms with the advantages of Q-learning. This way we
decompose the problem into two sub-problems: the problem of finding a
meaningful and useful decomposition of the state space, and the problem of
associating an action to each state. We test the proposed method on three
well-known reinforcement learning benchmarks, on which it results competitive
with respect to the state-of-the-art in both performance and interpretability.
Finally, we perform an ablation study that confirms that using the two-level
optimization scheme gives a boost in performance in non-trivial environments
with respect to a one-layer optimization technique.
</p>
<a href="http://arxiv.org/abs/2012.07723" target="_blank">arXiv:2012.07723</a> [<a href="http://arxiv.org/pdf/2012.07723" target="_blank">pdf</a>]

<h2>The Monte Carlo Transformer: a stochastic self-attention model for sequence prediction. (arXiv:2007.08620v2 [cs.LG] CROSS LISTED)</h2>
<h3>Alice Martin (CMAP, IP Paris, CITI, TIPIC-SAMOVAR), Charles Ollion (CMAP), Florian Strub, Sylvain Le Corff (IP Paris, CITI, TIPIC-SAMOVAR), Olivier Pietquin</h3>
<p>This paper introduces the Sequential Monte Carlo Transformer, an original
approach that naturally captures the observations distribution in a transformer
architecture. The keys, queries, values and attention vectors of the network
are considered as the unobserved stochastic states of its hidden structure.
This generative model is such that at each time step the received observation
is a random function of its past states in a given attention window. In this
general state-space setting, we use Sequential Monte Carlo methods to
approximate the posterior distributions of the states given the observations,
and to estimate the gradient of the log-likelihood. We hence propose a
generative model giving a predictive distribution, instead of a single-point
estimate.
</p>
<a href="http://arxiv.org/abs/2007.08620" target="_blank">arXiv:2007.08620</a> [<a href="http://arxiv.org/pdf/2007.08620" target="_blank">pdf</a>]

<h2>The Monte Carlo Transformer: a stochastic self-attention model for sequence prediction. (arXiv:2007.08620v2 [cs.LG] UPDATED)</h2>
<h3>Alice Martin (CMAP, IP Paris, CITI, TIPIC-SAMOVAR), Charles Ollion (CMAP), Florian Strub, Sylvain Le Corff (IP Paris, CITI, TIPIC-SAMOVAR), Olivier Pietquin</h3>
<p>This paper introduces the Sequential Monte Carlo Transformer, an original
approach that naturally captures the observations distribution in a transformer
architecture. The keys, queries, values and attention vectors of the network
are considered as the unobserved stochastic states of its hidden structure.
This generative model is such that at each time step the received observation
is a random function of its past states in a given attention window. In this
general state-space setting, we use Sequential Monte Carlo methods to
approximate the posterior distributions of the states given the observations,
and to estimate the gradient of the log-likelihood. We hence propose a
generative model giving a predictive distribution, instead of a single-point
estimate.
</p>
<a href="http://arxiv.org/abs/2007.08620" target="_blank">arXiv:2007.08620</a> [<a href="http://arxiv.org/pdf/2007.08620" target="_blank">pdf</a>]

