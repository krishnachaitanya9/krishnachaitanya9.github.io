---
title: Latest Deep Learning Papers
date: 2020-11-15 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (140 Articles)</h1>
<h2>Learning Latent Representations to Influence Multi-Agent Interaction. (arXiv:2011.06619v1 [cs.RO])</h2>
<h3>Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh</h3>
<p>Seamlessly interacting with humans or robots is hard because these agents are
non-stationary. They update their policy in response to the ego agent's
behavior, and the ego agent must anticipate these changes to co-adapt. Inspired
by humans, we recognize that robots do not need to explicitly model every
low-level action another agent will make; instead, we can capture the latent
strategy of other agents through high-level representations. We propose a
reinforcement learning-based framework for learning latent representations of
an agent's policy, where the ego agent identifies the relationship between its
behavior and the other agent's future strategy. The ego agent then leverages
these latent dynamics to influence the other agent, purposely guiding them
towards policies suitable for co-adaptation. Across several simulated domains
and a real-world air hockey game, our approach outperforms the alternatives and
learns to influence the other agent.
</p>
<a href="http://arxiv.org/abs/2011.06619" target="_blank">arXiv:2011.06619</a> [<a href="http://arxiv.org/pdf/2011.06619" target="_blank">pdf</a>]

<h2>Steady State Analysis of Episodic Reinforcement Learning. (arXiv:2011.06631v1 [cs.LG])</h2>
<h3>Huang Bojun</h3>
<p>This paper proves that the episodic learning environment of every
finite-horizon decision task has a unique steady state under any behavior
policy, and that the marginal distribution of the agent's input indeed
approaches to the steady-state distribution in essentially all episodic
learning processes. This observation supports an interestingly reversed mindset
against conventional wisdom: While steady states are usually presumed to exist
in continual learning and are considered less relevant in episodic learning, it
turns out they are guaranteed to exist for the latter. Based on this insight,
the paper further develops connections between episodic and continual RL for
several important concepts that have been separately treated in the two RL
formalisms. Practically, the existence of unique and approachable steady state
enables a general, reliable, and efficient way to collect data in episodic RL
tasks, which the paper applies to policy gradient algorithms as a
demonstration, based on a new steady-state policy gradient theorem. The paper
also proposes and empirically evaluates a perturbation method that facilitates
rapid mixing in real-world tasks.
</p>
<a href="http://arxiv.org/abs/2011.06631" target="_blank">arXiv:2011.06631</a> [<a href="http://arxiv.org/pdf/2011.06631" target="_blank">pdf</a>]

<h2>Empirical Performance Analysis of Conventional Deep Learning Models for Recognition of Objects in 2-D Images. (arXiv:2011.06639v1 [cs.CV])</h2>
<h3>Sangeeta Satish Rao, Nikunj Phutela, V R Badri Prasad</h3>
<p>Artificial Neural Networks, an essential part of Deep Learning, are derived
from the structure and functionality of the human brain. It has a broad range
of applications ranging from medical analysis to automated driving. Over the
past few years, deep learning techniques have improved drastically - models can
now be customized to a much greater extent by varying the network architecture,
network parameters, among others. We have varied parameters like learning rate,
filter size, the number of hidden layers, stride size and the activation
function among others to analyze the performance of the model and thus produce
a model with the highest performance. The model classifies images into 3
categories, namely, cars, faces and aeroplanes.
</p>
<a href="http://arxiv.org/abs/2011.06639" target="_blank">arXiv:2011.06639</a> [<a href="http://arxiv.org/pdf/2011.06639" target="_blank">pdf</a>]

<h2>Utilizing Ensemble Learning for Performance and Power Modeling and Improvement of Parallel Cancer Deep Learning CANDLE Benchmarks. (arXiv:2011.06654v1 [cs.LG])</h2>
<h3>Xingfu Wu, Valerie Taylor</h3>
<p>Machine learning (ML) continues to grow in importance across nearly all
domains and is a natural tool in modeling to learn from data. Often a tradeoff
exists between a model's ability to minimize bias and variance. In this paper,
we utilize ensemble learning to combine linear, nonlinear, and tree-/rule-based
ML methods to cope with the bias-variance tradeoff and result in more accurate
models. Hardware performance counter values are correlated with properties of
applications that impact performance and power on the underlying system. We use
the datasets collected for two parallel cancer deep learning CANDLE benchmarks,
NT3 (weak scaling) and P1B2 (strong scaling), to build performance and power
models based on hardware performance counters using single-object and
multiple-objects ensemble learning to identify the most important counters for
improvement. Based on the insights from these models, we improve the
performance and energy of P1B2 and NT3 by optimizing the deep learning
environments TensorFlow, Keras, Horovod, and Python under the huge page size of
8 MB on the Cray XC40 Theta at Argonne National Laboratory. Experimental
results show that ensemble learning not only produces more accurate models but
also provides more robust performance counter ranking. We achieve up to 61.15%
performance improvement and up to 62.58% energy saving for P1B2 and up to
55.81% performance improvement and up to 52.60% energy saving for NT3 on up to
24,576 cores.
</p>
<a href="http://arxiv.org/abs/2011.06654" target="_blank">arXiv:2011.06654</a> [<a href="http://arxiv.org/pdf/2011.06654" target="_blank">pdf</a>]

<h2>Performance and Power Modeling and Prediction Using MuMMI and Ten Machine Learning Methods. (arXiv:2011.06655v1 [cs.LG])</h2>
<h3>Xingfu Wu, Valerie Taylor, Zhiling Lan</h3>
<p>In this paper, we use modeling and prediction tool MuMMI (Multiple Metrics
Modeling Infrastructure) and ten machine learning methods to model and predict
performance and power and compare their prediction error rates. We use a
fault-tolerant linear algebra code and a fault-tolerant heat distribution code
to conduct our modeling and prediction study on the Cray XC40 Theta and IBM
BG/Q Mira at Argonne National Laboratory and the Intel Haswell cluster Shepard
at Sandia National Laboratories. Our experiment results show that the
prediction error rates in performance and power using MuMMI are less than 10%
for most cases. Based on the models for runtime, node power, CPU power, and
memory power, we identify the most significant performance counters for
potential optimization efforts associated with the application characteristics
and the target architectures, and we predict theoretical outcomes of the
potential optimizations. When we compare the prediction accuracy using MuMMI
with that using 10 machine learning methods, we observe that MuMMI not only
results in more accurate prediction in both performance and power but also
presents how performance counters impact the performance and power models. This
provides some insights about how to fine-tune the applications and/or systems
for energy efficiency.
</p>
<a href="http://arxiv.org/abs/2011.06655" target="_blank">arXiv:2011.06655</a> [<a href="http://arxiv.org/pdf/2011.06655" target="_blank">pdf</a>]

<h2>Domain-Level Explainability -- A Challenge for Creating Trust in Superhuman AI Strategies. (arXiv:2011.06665v1 [cs.AI])</h2>
<h3>Jonas Andrulis, Ole Meyer, Gr&#xe9;gory Schott, Samuel Weinbach, Volker Gruhn</h3>
<p>For strategic problems, intelligent systems based on Deep Reinforcement
Learning (DRL) have demonstrated an impressive ability to learn advanced
solutions that can go far beyond human capabilities, especially when dealing
with complex scenarios. While this creates new opportunities for the
development of intelligent assistance systems with groundbreaking
functionalities, applying this technology to real-world problems carries
significant risks and therefore requires trust in their transparency and
reliability. With superhuman strategies being non-intuitive and complex by
definition and real-world scenarios prohibiting a reliable performance
evaluation, the key components for trust in these systems are difficult to
achieve. Explainable AI (XAI) has successfully increased transparency for
modern AI systems through a variety of measures, however, XAI research has not
yet provided approaches enabling domain level insights for expert users in
strategic situations. In this paper, we discuss the existence of superhuman
DRL-based strategies, their properties, the requirements and challenges for
transforming them into real-world environments, and the implications for trust
through explainability as a key technology.
</p>
<a href="http://arxiv.org/abs/2011.06665" target="_blank">arXiv:2011.06665</a> [<a href="http://arxiv.org/pdf/2011.06665" target="_blank">pdf</a>]

<h2>Attitude and Thrust Strategies for Fully-Actuated Multirotors: The Fast-Track to Real-World Applications. (arXiv:2011.06666v1 [cs.RO])</h2>
<h3>Azarakhsh Keipour, Mohammadreza Mousaei, Andrew T Ashley, Sebastian Scherer</h3>
<p>The introduction of fully-actuated multirotors has opened the door to new
possibilities and more efficient solutions to many real-world applications.
However, their integration had been slower than expected, partly due to the
need for new tools to take full advantage of these robots.

As far as we know, all the groups currently working on the fully-actuated
multirotors develop new full-pose (6-D) tools and methods to use their robots,
which is inefficient, time-consuming, and requires many resources.

We propose methods that extend the existing flight controllers to support the
new fully-actuated robots and bridge the gap between the tools already
available for underactuated robots and the new fully-actuated vehicles. We
introduce attitude strategies that work with the underactuated planners,
controllers, tools, and remote control interfaces, all while allowing taking
advantage of the full actuation. Moreover, new methods are proposed that can
properly handle the limited lateral thrust suffered by many fully-actuated UAV
designs. The strategies are lightweight, simple, and allow rapid integration of
the available tools with these new vehicles for the fast development of new
real-world applications.

The real experiments on our robots and simulations on several UAV
architectures show how the strategies can be utilized. The source code of the
PX4 firmware enhanced with the proposed methods and its simulator with our
fully-actuated hexarotor model are provided with this paper. For more
information, please visit https://theairlab.org/fully-actuated/.
</p>
<a href="http://arxiv.org/abs/2011.06666" target="_blank">arXiv:2011.06666</a> [<a href="http://arxiv.org/pdf/2011.06666" target="_blank">pdf</a>]

<h2>Symbolically Solving Partial Differential Equations using Deep Learning. (arXiv:2011.06673v1 [cs.LG])</h2>
<h3>Maysum Panju, Kourosh Parand, Ali Ghodsi</h3>
<p>We describe a neural-based method for generating exact or approximate
solutions to differential equations in the form of mathematical expressions.
Unlike other neural methods, our system returns symbolic expressions that can
be interpreted directly. Our method uses a neural architecture for learning
mathematical expressions to optimize a customizable objective, and is scalable,
compact, and easily adaptable for a variety of tasks and configurations. The
system has been shown to effectively find exact or approximate symbolic
solutions to various differential equations with applications in natural
sciences. In this work, we highlight how our method applies to partial
differential equations over multiple variables and more complex boundary and
initial value conditions.
</p>
<a href="http://arxiv.org/abs/2011.06673" target="_blank">arXiv:2011.06673</a> [<a href="http://arxiv.org/pdf/2011.06673" target="_blank">pdf</a>]

<h2>Trajectory Prediction in Autonomous Driving with a Lane Heading Auxiliary Loss. (arXiv:2011.06679v1 [cs.CV])</h2>
<h3>Ross Greer, Nachiket Deo, Mohan Trivedi</h3>
<p>Predicting a vehicle's trajectory is an essential ability for autonomous
vehicles navigating through complex urban traffic scenes. Bird's-eye-view
roadmap information provides valuable information for making trajectory
predictions, and while state-of-the-art models extract this information via
image convolution, auxiliary loss functions can augment patterns inferred from
deep learning by further encoding common knowledge of social and legal driving
behaviors. Since human driving behavior is inherently multimodal, models which
allow for multimodal output tend to outperform single-prediction models on
standard metrics; the proposed loss function benefits such models, as all
predicted modes must follow the same expected driving rules. Our contribution
to trajectory prediction is twofold; we propose a new metric which addresses
failure cases of the off-road rate metric by penalizing trajectories that
contain driving behavior that opposes the ascribed heading (flow direction) of
a driving lane, and we show this metric to be differentiable and therefore
suitable as an auxiliary loss function. We then use this auxiliary loss to
extend the the standard multiple trajectory prediction (MTP) and MultiPath
models, achieving improved results on the nuScenes prediction benchmark by
predicting trajectories which better conform to the lane-following rules of the
road.
</p>
<a href="http://arxiv.org/abs/2011.06679" target="_blank">arXiv:2011.06679</a> [<a href="http://arxiv.org/pdf/2011.06679" target="_blank">pdf</a>]

<h2>Adversarial Robustness Against Image Color Transformation within Parametric Filter Space. (arXiv:2011.06690v1 [cs.CV])</h2>
<h3>Zhengyu Zhao, Zhuoran Liu, Martha Larson</h3>
<p>We propose Adversarial Color Enhancement (ACE), a novel approach to
generating non-suspicious adversarial images by optimizing a color
transformation within a parametric filter space. The filter we use approximates
human-understandable color curve adjustment, constraining ACE with a single,
continuous function. This property gives rise to a principled adversarial
action space explicitly controlled by filter parameters. Existing color
transformation attacks are not guided by a parametric space, and, consequently,
additional pixel-related constraints such as regularization and sampling are
necessary. These constraints make methodical analysis difficult. In this paper,
we carry out a systematic robustness analysis of ACE from both the attack and
defense perspectives by varying the bound of the color filter parameters. We
investigate a general formulation of ACE and also a variant targeting
particularly appealing color styles, as achieved with popular image filters.
From the attack perspective, we provide extensive experiments on the
vulnerability of image classifiers, but also explore the vulnerability of
segmentation and aesthetic quality assessment algorithms, in both the white-box
and black-box scenarios. From the defense perspective, more experiments provide
insight into the stability of ACE against input transformation-based defenses
and show the potential of adversarial training for improving model robustness
against ACE.
</p>
<a href="http://arxiv.org/abs/2011.06690" target="_blank">arXiv:2011.06690</a> [<a href="http://arxiv.org/pdf/2011.06690" target="_blank">pdf</a>]

<h2>Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation. (arXiv:2011.06698v1 [cs.RO])</h2>
<h3>Bryan Chen, Alexander Sax, Gene Lewis, Iro Armeni, Silvio Savarese, Amir Zamir, Jitendra Malik, Lerrel Pinto</h3>
<p>Vision-based robotics often separates the control loop into one module for
perception and a separate module for control. It is possible to train the whole
system end-to-end (e.g. with deep RL), but doing it "from scratch" comes with a
high sample complexity cost and the final result is often brittle, failing
unexpectedly if the test environment differs from that of training.

We study the effects of using mid-level visual representations (features
learned asynchronously for traditional computer vision objectives), as a
generic and easy-to-decode perceptual state in an end-to-end RL framework.
Mid-level representations encode invariances about the world, and we show that
they aid generalization, improve sample complexity, and lead to a higher final
performance. Compared to other approaches for incorporating invariances, such
as domain randomization, asynchronously trained mid-level representations scale
better: both to harder problems and to larger domain shifts. In practice, this
means that mid-level representations could be used to successfully train
policies for tasks where domain randomization and learning-from-scratch failed.
We report results on both manipulation and navigation tasks, and for navigation
include zero-shot sim-to-real experiments on real robots.
</p>
<a href="http://arxiv.org/abs/2011.06698" target="_blank">arXiv:2011.06698</a> [<a href="http://arxiv.org/pdf/2011.06698" target="_blank">pdf</a>]

<h2>Neural Network Training Techniques Regularize Optimization Trajectory: An Empirical Study. (arXiv:2011.06702v1 [cs.LG])</h2>
<h3>Cheng Chen, Junjie Yang, Yi Zhou</h3>
<p>Modern deep neural network (DNN) trainings utilize various training
techniques, e.g., nonlinear activation functions, batch normalization,
skip-connections, etc. Despite their effectiveness, it is still mysterious how
they help accelerate DNN trainings in practice. In this paper, we provide an
empirical study of the regularization effect of these training techniques on
DNN optimization. Specifically, we find that the optimization trajectories of
successful DNN trainings consistently obey a certain regularity principle that
regularizes the model update direction to be aligned with the trajectory
direction. Theoretically, we show that such a regularity principle leads to a
convergence guarantee in nonconvex optimization and the convergence rate
depends on a regularization parameter. Empirically, we find that DNN trainings
that apply the training techniques achieve a fast convergence and obey the
regularity principle with a large regularization parameter, implying that the
model updates are well aligned with the trajectory. On the other hand, DNN
trainings without the training techniques have slow convergence and obey the
regularity principle with a small regularization parameter, implying that the
model updates are not well aligned with the trajectory. Therefore, different
training techniques regularize the model update direction via the regularity
principle to facilitate the convergence.
</p>
<a href="http://arxiv.org/abs/2011.06702" target="_blank">arXiv:2011.06702</a> [<a href="http://arxiv.org/pdf/2011.06702" target="_blank">pdf</a>]

<h2>Diffusion models for Handwriting Generation. (arXiv:2011.06704v1 [cs.LG])</h2>
<h3>Troy Luhman, Eric Luhman</h3>
<p>In this paper, we propose a diffusion probabilistic model for handwriting
generation. Diffusion models are a class of generative models where samples
start from Gaussian noise and are gradually denoised to produce output. Our
method of handwriting generation does not require using any text-recognition
based, writer-style based, or adversarial loss functions, nor does it require
training of auxiliary networks. Our model is able to incorporate writer
stylistic features directly from image data, eliminating the need for user
interaction during sampling. Experiments reveal that our model is able to
generate realistic , high quality images of handwritten text in a similar style
to a given writer. Our implementation can be found at
https://github.com/tcl9876/Diffusion-Handwriting-Generation
</p>
<a href="http://arxiv.org/abs/2011.06704" target="_blank">arXiv:2011.06704</a> [<a href="http://arxiv.org/pdf/2011.06704" target="_blank">pdf</a>]

<h2>Active Reinforcement Learning: Observing Rewards at a Cost. (arXiv:2011.06709v1 [cs.LG])</h2>
<h3>David Krueger, Jan Leike, Owain Evans, John Salvatier</h3>
<p>Active reinforcement learning (ARL) is a variant on reinforcement learning
where the agent does not observe the reward unless it chooses to pay a query
cost c &gt; 0. The central question of ARL is how to quantify the long-term value
of reward information. Even in multi-armed bandits, computing the value of this
information is intractable and we have to rely on heuristics. We propose and
evaluate several heuristic approaches for ARL in multi-armed bandits and
(tabular) Markov decision processes, and discuss and illustrate some
challenging aspects of the ARL problem.
</p>
<a href="http://arxiv.org/abs/2011.06709" target="_blank">arXiv:2011.06709</a> [<a href="http://arxiv.org/pdf/2011.06709" target="_blank">pdf</a>]

<h2>Dependency-based Anomaly Detection: Framework, Methods and Benchmark. (arXiv:2011.06716v1 [cs.LG])</h2>
<h3>Sha Lu, Lin Liu, Jiuyong Li, Thuc Duy Le, Jixue Liu</h3>
<p>Anomaly detection is an important research problem because anomalies often
contain critical insights for understanding the unusual behavior in data. One
type of anomaly detection approach is dependency-based, which identifies
anomalies by examining the violations of the normal dependency among variables.
These methods can discover subtle and meaningful anomalies with better
interpretation. Existing dependency-based methods adopt different
implementations and show different strengths and weaknesses. However, the
theoretical fundamentals and the general process behind them have not been well
studied. This paper proposes a general framework, DepAD, to provide a unified
process for dependency-based anomaly detection. DepAD decomposes unsupervised
anomaly detection tasks into feature selection and prediction problems.
Utilizing off-the-shelf techniques, the DepAD framework can have various
instantiations to suit different application domains. Comprehensive experiments
have been conducted over one hundred instantiated DepAD methods with 32
real-world datasets to evaluate the performance of representative techniques in
DepAD. To show the effectiveness of DepAD, we compare two DepAD methods with
nine state-of-the-art anomaly detection methods, and the results show that
DepAD methods outperform comparison methods in most cases. Through the DepAD
framework, this paper gives guidance and inspiration for future research of
dependency-based anomaly detection and provides a benchmark for its evaluation.
</p>
<a href="http://arxiv.org/abs/2011.06716" target="_blank">arXiv:2011.06716</a> [<a href="http://arxiv.org/pdf/2011.06716" target="_blank">pdf</a>]

<h2>Coordinated Motion Control and Event-based Obstacle-crossing for Four Wheel-leg Independent Motor-driven Robotic System via MPC. (arXiv:2011.06717v1 [cs.RO])</h2>
<h3>Liu Dongchen, Wang Junzheng, Wang Shoukun</h3>
<p>This work presents the coordinated motion control and obstacle-crossing
problem for the four wheel-leg independent motor-driven robotic systems via a
model predictive control (MPC) approach based on an event-triggering mechanism.
The modeling of a wheel-leg robotic control system with a dynamic supporting
polygon is organized. The system dynamic model is 3 degrees of freedom (DOF)
ignoring the pitch, roll and vertical motions. The single wheel dynamic is
analyzed considering the characteristics of motor-driven and the Burckhardt
nonlinear tire model. As a result, an over-actuated predictive model is
proposed with the motor torques as inputs and the system states as outputs. As
the supporting polygon is only adjusted at certain conditions, an event-based
triggering mechanism is designed to save hardware resources and energy. The MPC
controller is evaluated on a virtual prototype as well as a physical prototype.
The simulation results guide the parameter tuning for the controller
implementation in the physical prototype. The experimental results on these two
prototypes verify the efficiency of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2011.06717" target="_blank">arXiv:2011.06717</a> [<a href="http://arxiv.org/pdf/2011.06717" target="_blank">pdf</a>]

<h2>Power System Event Identification based on Deep Neural Network with Information Loading. (arXiv:2011.06718v1 [cs.LG])</h2>
<h3>Jie Shi, Brandon Foggo, Nanpeng Yu</h3>
<p>Online power system event identification and classification is crucial to
enhancing the reliability of transmission systems. In this paper, we develop a
deep neural network (DNN) based approach to identify and classify power system
events by leveraging real-world measurements from hundreds of phasor
measurement units (PMUs) and labels from thousands of events. Two innovative
designs are embedded into the baseline model built on convolutional neural
networks (CNNs) to improve the event classification accuracy. First, we propose
a graph signal processing based PMU sorting algorithm to improve the learning
efficiency of CNNs. Second, we deploy information loading based regularization
to strike the right balance between memorization and generalization for the
DNN. Numerical studies results based on real-world dataset from the Eastern
Interconnection of the U.S power transmission grid show that the combination of
PMU based sorting and the information loading based regularization techniques
help the proposed DNN approach achieve highly accurate event identification and
classification results.
</p>
<a href="http://arxiv.org/abs/2011.06718" target="_blank">arXiv:2011.06718</a> [<a href="http://arxiv.org/pdf/2011.06718" target="_blank">pdf</a>]

<h2>Grasping with Chopsticks: Combating Covariate Shift in Model-free Imitation Learning for Fine Manipulation. (arXiv:2011.06719v1 [cs.RO])</h2>
<h3>Liyiming Ke, Jingqiang Wang, Tapomayukh Bhattacharjee, Byron Boots, Siddhartha Srinivasa</h3>
<p>Billions of people use chopsticks, a simple yet versatile tool, for fine
manipulation of everyday objects. The small, curved, and slippery tips of
chopsticks pose a challenge for picking up small objects, making them a
suitably complex test case. This paper leverages human demonstrations to
develop an autonomous chopsticks-equipped robotic manipulator. Due to the lack
of accurate models for fine manipulation, we explore model-free imitation
learning, which traditionally suffers from the covariate shift phenomenon that
causes poor generalization. We propose two approaches to reduce covariate
shift, neither of which requires access to an interactive expert or a model,
unlike previous approaches. First, we alleviate single-step prediction errors
by applying an invariant operator to increase the data support at critical
steps for grasping. Second, we generate synthetic corrective labels by adding
bounded noise and combining parametric and non-parametric methods to prevent
error accumulation. We demonstrate our methods on a real chopstick-equipped
robot that we built, and observe the agent's success rate increase from 37.3%
to 80%, which is comparable to the human expert performance of 82.6%.
</p>
<a href="http://arxiv.org/abs/2011.06719" target="_blank">arXiv:2011.06719</a> [<a href="http://arxiv.org/pdf/2011.06719" target="_blank">pdf</a>]

<h2>Local Anomaly Detection in Videos using Object-Centric Adversarial Learning. (arXiv:2011.06722v1 [cs.CV])</h2>
<h3>Pankaj Raj Roy, Guillaume-Alexandre Bilodeau, Lama Seoud</h3>
<p>We propose a novel unsupervised approach based on a two-stage object-centric
adversarial framework that only needs object regions for detecting frame-level
local anomalies in videos. The first stage consists in learning the
correspondence between the current appearance and past gradient images of
objects in scenes deemed normal, allowing us to either generate the past
gradient from current appearance or the reverse. The second stage extracts the
partial reconstruction errors between real and generated images (appearance and
past gradient) with normal object behaviour, and trains a discriminator in an
adversarial fashion. In inference mode, we employ the trained image generators
with the adversarially learned binary classifier for outputting region-level
anomaly detection scores. We tested our method on four public benchmarks, UMN,
UCSD, Avenue and ShanghaiTech and our proposed object-centric adversarial
approach yields competitive or even superior results compared to
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.06722" target="_blank">arXiv:2011.06722</a> [<a href="http://arxiv.org/pdf/2011.06722" target="_blank">pdf</a>]

<h2>A GAN-based Approach for Mitigating Inference Attacks in Smart Home Environment. (arXiv:2011.06725v1 [cs.LG])</h2>
<h3>Olakunle Ibitoye, Ashraf Matrawy, M. Omair Shafiq</h3>
<p>The proliferation of smart, connected, always listening devices have
introduced significant privacy risks to users in a smart home environment.
Beyond the notable risk of eavesdropping, intruders can adopt machine learning
techniques to infer sensitive information from audio recordings on these
devices, resulting in a new dimension of privacy concerns and attack variables
to smart home users. Techniques such as sound masking and microphone jamming
have been effectively used to prevent eavesdroppers from listening in to
private conversations. In this study, we explore the problem of adversaries
spying on smart home users to infer sensitive information with the aid of
machine learning techniques. We then analyze the role of randomness in the
effectiveness of sound masking for mitigating sensitive information leakage. We
propose a Generative Adversarial Network (GAN) based approach for privacy
preservation in smart homes which generates random noise to distort the
unwanted machine learning-based inference. Our experimental results demonstrate
that GANs can be used to generate more effective sound masking noise signals
which exhibit more randomness and effectively mitigate deep learning-based
inference attacks while preserving the semantics of the audio samples.
</p>
<a href="http://arxiv.org/abs/2011.06725" target="_blank">arXiv:2011.06725</a> [<a href="http://arxiv.org/pdf/2011.06725" target="_blank">pdf</a>]

<h2>3-D Motion Capture of an Unmodified Drone with Single-chip Millimeter Wave Radar. (arXiv:2011.06730v1 [cs.RO])</h2>
<h3>Peijun Zhao, Chris Xiaoxuan Lu, Bing Wang, Niki Trigoni, Andrew Markham</h3>
<p>Accurate motion capture of aerial robots in 3-D is a key enabler for
autonomous operation in indoor environments such as warehouses or factories, as
well as driving forward research in these areas. The most commonly used
solutions at present are optical motion capture (e.g. VICON) and Ultrawideband
(UWB), but these are costly and cumbersome to deploy, due to their requirement
of multiple cameras/sensors spaced around the tracking area. They also require
the drone to be modified to carry an active or passive marker. In this work, we
present an inexpensive system that can be rapidly installed, based on
single-chip millimeter wave (mmWave) radar. Importantly, the drone does not
need to be modified or equipped with any markers, as we exploit the Doppler
signals from the rotating propellers. Furthermore, 3-D tracking is possible
from a single point, greatly simplifying deployment. We develop a novel deep
neural network and demonstrate decimeter level 3-D tracking at 10Hz, achieving
better performance than classical baselines. Our hope is that this low-cost
system will act to catalyse inexpensive drone research and increased autonomy.
</p>
<a href="http://arxiv.org/abs/2011.06730" target="_blank">arXiv:2011.06730</a> [<a href="http://arxiv.org/pdf/2011.06730" target="_blank">pdf</a>]

<h2>Structured Attention Graphs for Understanding Deep Image Classifications. (arXiv:2011.06733v1 [cs.CV])</h2>
<h3>Vivswan Shitole, Fuxin Li, Minsuk Kahng, Prasad Tadepalli, Alan Fern</h3>
<p>Attention maps are a popular way of explaining the decisions of convolutional
networks for image classification. Typically, for each image of interest, a
single attention map is produced, which assigns weights to pixels based on
their importance to the classification. A single attention map, however,
provides an incomplete understanding since there are often many other maps that
explain a classification equally well. In this paper, we introduce structured
attention graphs (SAGs), which compactly represent sets of attention maps for
an image by capturing how different combinations of image regions impact a
classifier's confidence. We propose an approach to compute SAGs and a
visualization for SAGs so that deeper insight can be gained into a classifier's
decisions. We conduct a user study comparing the use of SAGs to traditional
attention maps for answering counterfactual questions about image
classifications. Our results show that the users are more correct when
answering comparative counterfactual questions based on SAGs compared to the
baselines.
</p>
<a href="http://arxiv.org/abs/2011.06733" target="_blank">arXiv:2011.06733</a> [<a href="http://arxiv.org/pdf/2011.06733" target="_blank">pdf</a>]

<h2>Investigating Learning in Deep Neural Networks using Layer-Wise Weight Change. (arXiv:2011.06735v1 [cs.LG])</h2>
<h3>Ayush Manish Agrawal, Atharva Tendle, Harshvardhan Sikka, Sahib Singh, Amr Kayid</h3>
<p>Understanding the per-layer learning dynamics of deep neural networks is of
significant interest as it may provide insights into how neural networks learn
and the potential for better training regimens. We investigate learning in Deep
Convolutional Neural Networks (CNNs) by measuring the relative weight change of
layers while training. Several interesting trends emerge in a variety of CNN
architectures across various computer vision classification tasks, including
the overall increase in relative weight change of later layers as compared to
earlier ones.
</p>
<a href="http://arxiv.org/abs/2011.06735" target="_blank">arXiv:2011.06735</a> [<a href="http://arxiv.org/pdf/2011.06735" target="_blank">pdf</a>]

<h2>Metric-Free Individual Fairness with Cooperative Contextual Bandits. (arXiv:2011.06738v1 [cs.LG])</h2>
<h3>Qian Hu, Huzefa Rangwala</h3>
<p>Data mining algorithms are increasingly used in automated decision making
across all walks of daily life. Unfortunately, as reported in several studies
these algorithms inject bias from data and environment leading to inequitable
and unfair solutions. To mitigate bias in machine learning, different
formalizations of fairness have been proposed that can be categorized into
group fairness and individual fairness. Group fairness requires that different
groups should be treated similarly which might be unfair to some individuals
within a group. On the other hand, individual fairness requires that similar
individuals be treated similarly. However, individual fairness remains
understudied due to its reliance on problem-specific similarity metrics. We
propose a metric-free individual fairness and a cooperative contextual bandits
(CCB) algorithm. The CCB algorithm utilizes fairness as a reward and attempts
to maximize it. The advantage of treating fairness as a reward is that the
fairness criterion does not need to be differentiable. The proposed algorithm
is tested on multiple real-world benchmark datasets. The results show the
effectiveness of the proposed algorithm at mitigating bias and at achieving
both individual and group fairness.
</p>
<a href="http://arxiv.org/abs/2011.06738" target="_blank">arXiv:2011.06738</a> [<a href="http://arxiv.org/pdf/2011.06738" target="_blank">pdf</a>]

<h2>Rebounding Bandits for Modeling Satiation Effects. (arXiv:2011.06741v1 [cs.LG])</h2>
<h3>Liu Leqi, Fatma Kilinc-Karzan, Zachary C. Lipton, Alan L. Montgomery</h3>
<p>Psychological research shows that enjoyment of many goods is subject to
satiation, with enjoyment declining after repeated exposures to the same item.
Nevertheless, proposed algorithms for powering recommender systems seldom model
these dynamics, instead proceeding as though user preferences were fixed in
time. In this work, we adopt a multi-armed bandit setup, modeling satiation
dynamics as a time-invariant linear dynamical system. In our model, the
expected rewards for each arm decline monotonically with consecutive exposures
and rebound towards the initial reward whenever that arm is not pulled. We
analyze this model, showing that, when the arms exhibit deterministic identical
dynamics, our problem is equivalent to a specific instance of Max K-Cut. In
this case, a greedy policy, which plays the arms in a cyclic order, is optimal.
In the general setting, where each arm's satiation dynamics are stochastic and
governed by different (unknown) parameters, we propose an algorithm that first
uses offline data to estimate each arm's reward model and then plans using a
generalization of the greedy policy.
</p>
<a href="http://arxiv.org/abs/2011.06741" target="_blank">arXiv:2011.06741</a> [<a href="http://arxiv.org/pdf/2011.06741" target="_blank">pdf</a>]

<h2>Encoded Value-at-Risk: A Predictive Machine for Financial Risk Management. (arXiv:2011.06742v1 [cs.LG])</h2>
<h3>Hamidreza Arian, Mehrdad Moghimi, Ehsan Tabatabaei, Shiva Zamani</h3>
<p>Measuring risk is at the center of modern financial risk management. As the
world economy is becoming more complex and standard modeling assumptions are
violated, the advanced artificial intelligence solutions may provide the right
tools to analyze the global market. In this paper, we provide a novel approach
for measuring market risk called Encoded Value-at-Risk (Encoded VaR), which is
based on a type of artificial neural network, called Variational Auto-encoders
(VAEs). Encoded VaR is a generative model which can be used to reproduce market
scenarios from a range of historical cross-sectional stock returns, while
increasing the signal-to-noise ratio present in the financial data, and
learning the dependency structure of the market without any assumptions about
the joint distribution of stock returns. We compare Encoded VaR out-of-sample
results with eleven other methods and show that it is competitive to many other
well-known VaR algorithms presented in the literature.
</p>
<a href="http://arxiv.org/abs/2011.06742" target="_blank">arXiv:2011.06742</a> [<a href="http://arxiv.org/pdf/2011.06742" target="_blank">pdf</a>]

<h2>Safe and Robust Motion Planning for Dynamic Robotics via Control Barrier Functions. (arXiv:2011.06748v1 [cs.RO])</h2>
<h3>Aniketh Manjunath, Quan Nguyen</h3>
<p>Control Barrier Functions (CBF) are widely used to enforce the
safety-critical constraints on nonlinear systems. Recently, these functions are
being incorporated into a path planning framework to design a safety-critical
path planner. However, these methods fall short of providing a realistic path
considering both run-time complexity and safety-critical constraints. This
paper proposes a novel motion planning approach using Rapidly exploring Random
Trees (RRT) algorithm to enforce the robust CBF and kinodynamic constraints to
generate a safety-critical path that is free of any obstacles while taking into
account the model uncertainty from robot dynamics as well as perception. Result
analysis indicates that the proposed method outperforms various conventional
RRT based path planners, guaranteeing a safety-critical path with reduced
computational overhead. We present numerical validation of the algorithm on the
Hamster V7 robot car, a micro autonomous Unmanned Ground Vehicle, where it
performs dynamic navigation on an obstacle-ridden path with various
uncertainties in perception noises, and robot dynamics.
</p>
<a href="http://arxiv.org/abs/2011.06748" target="_blank">arXiv:2011.06748</a> [<a href="http://arxiv.org/pdf/2011.06748" target="_blank">pdf</a>]

<h2>A Legged Soft Robot Platform for Dynamic Locomotion. (arXiv:2011.06749v1 [cs.RO])</h2>
<h3>Boxi Xia, Jiaming Fu, Hongbo Zhu, Zhicheng Song, Yibo Jiang, Hod Lipson</h3>
<p>We present an open-source untethered quadrupedal soft robot platform for
dynamic locomotion (e.g., high-speed running and backflipping). The robot is
mostly soft (80 vol.%) while driven by four geared servo motors. The robot's
soft body and soft legs were 3D printed with gyroid infill using a flexible
material, enabling it to conform to the environment and passively stabilize
during locomotion on multi-terrain environments. In addition, we simulated the
robot in a real-time soft body simulation. With tuned gaits in simulation, the
real robot can locomote at a speed of 0.9 m/s (2.5 body length/second),
substantially faster than most untethered legged soft robots published to date.
We hope this platform, along with its verified simulator, can catalyze the
development of soft robotics.
</p>
<a href="http://arxiv.org/abs/2011.06749" target="_blank">arXiv:2011.06749</a> [<a href="http://arxiv.org/pdf/2011.06749" target="_blank">pdf</a>]

<h2>Filter Pre-Pruning for Improved Fine-tuning of Quantized Deep Neural Networks. (arXiv:2011.06751v1 [cs.CV])</h2>
<h3>Jun Nishikawa, Ryoji Ikegaya</h3>
<p>Deep Neural Networks(DNNs) have many parameters and activation data, and
these both are expensive to implement. One method to reduce the size of the DNN
is to quantize the pre-trained model by using a low-bit expression for weights
and activations, using fine-tuning to recover the drop in accuracy. However, it
is generally difficult to train neural networks which use low-bit expressions.
One reason is that the weights in the middle layer of the DNN have a wide
dynamic range and so when quantizing the wide dynamic range into a few bits,
the step size becomes large, which leads to a large quantization error and
finally a large degradation in accuracy. To solve this problem, this paper
makes the following three contributions without using any additional learning
parameters and hyper-parameters. First, we analyze how batch normalization,
which causes the aforementioned problem, disturbs the fine-tuning of the
quantized DNN. Second, based on these results, we propose a new pruning method
called Pruning for Quantization (PfQ) which removes the filters that disturb
the fine-tuning of the DNN while not affecting the inferred result as far as
possible. Third, we propose a workflow of fine-tuning for quantized DNNs using
the proposed pruning method(PfQ). Experiments using well-known models and
datasets confirmed that the proposed method achieves higher performance with a
similar model size than conventional quantization methods including
fine-tuning.
</p>
<a href="http://arxiv.org/abs/2011.06751" target="_blank">arXiv:2011.06751</a> [<a href="http://arxiv.org/pdf/2011.06751" target="_blank">pdf</a>]

<h2>Critic PI2: Master Continuous Planning via Policy Improvement with Path Integrals and Deep Actor-Critic Reinforcement Learning. (arXiv:2011.06752v1 [cs.LG])</h2>
<h3>Jiajun Fan, He Ba, Xian Guo, Jianye Hao</h3>
<p>Constructing agents with planning capabilities has long been one of the main
challenges in the pursuit of artificial intelligence. Tree-based planning
methods from AlphaGo to Muzero have enjoyed huge success in discrete domains,
such as chess and Go. Unfortunately, in real-world applications like robot
control and inverted pendulum, whose action space is normally continuous, those
tree-based planning techniques will be struggling. To address those
limitations, in this paper, we present a novel model-based reinforcement
learning frameworks called Critic PI2, which combines the benefits from
trajectory optimization, deep actor-critic learning, and model-based
reinforcement learning. Our method is evaluated for inverted pendulum models
with applicability to many continuous control systems. Extensive experiments
demonstrate that Critic PI2 achieved a new state of the art in a range of
challenging continuous domains. Furthermore, we show that planning with a
critic significantly increases the sample efficiency and real-time performance.
Our work opens a new direction toward learning the components of a model-based
planning system and how to use them.
</p>
<a href="http://arxiv.org/abs/2011.06752" target="_blank">arXiv:2011.06752</a> [<a href="http://arxiv.org/pdf/2011.06752" target="_blank">pdf</a>]

<h2>Scaffolding Reflection in Reinforcement Learning Framework for Confinement Escape Problem. (arXiv:2011.06764v1 [cs.RO])</h2>
<h3>Nishant Mohanty, Suresh Sundaram</h3>
<p>This paper formulates an application of reinforcement learning for an evader
in a confinement escape problem. An evader's objective is to attempt escaping a
confinement region patrolled by multiple defenders, with minimum use of energy.
Meanwhile, the defenders aim to reach and capture the evader without any
communication between them. The problem formulation uses the actor-critic
approach for the defender. In this paper, the novel Scaffolding Reflection in
Reinforcement Learning (SR2L) framework is proposed, using a potential field
method as a scaffold to assist the actor's action-reflection. Through the
user's clearly articulated intent, the action-reflection enables the actor to
learn by observing the probable actions and their values based on experience.
Extensive Monte-Carlo simulations show the performance of a trained SR2L
against the baseline approach. The SR2L framework achieves at least one order
fewer episodes to learn the policy than the conventional RL framework.
</p>
<a href="http://arxiv.org/abs/2011.06764" target="_blank">arXiv:2011.06764</a> [<a href="http://arxiv.org/pdf/2011.06764" target="_blank">pdf</a>]

<h2>Formation-based Selection of Drone Swarm Services. (arXiv:2011.06766v1 [cs.RO])</h2>
<h3>Balsam Alkouz, Athman Bouguettaya</h3>
<p>Swarm of drones are increasingly being asked to carry out missions that can't
be completed by one drone. Particularly, in delivery, issues arise due to the
swarm's limited flight endurance. Hence, we propose a novel formation-guided
framework for selecting Swarm-based Drone-as-a-Service (SDaaS) for delivery. A
detailed study is carried out to highlight the effect of swarm formations on
energy consumption. Two SDaaS selection approaches, i.e. Fixed and Adaptive,
are designed considering the different formation decisions a swarm can take.
The proposed framework considers extrinsic constraints including wind speed and
direction. We propose SDaaS selection algorithms for each approach.
Experimental results prove the efficiency of the proposed algorithms.
</p>
<a href="http://arxiv.org/abs/2011.06766" target="_blank">arXiv:2011.06766</a> [<a href="http://arxiv.org/pdf/2011.06766" target="_blank">pdf</a>]

<h2>Toward the Fully Physics-Informed Echo State Network -- an ODE Approximator Based on Recurrent Artificial Neurons. (arXiv:2011.06769v1 [cs.LG])</h2>
<h3>Dong Keun Oh</h3>
<p>Inspired by recent theoretical arguments, physics-informed echo state network
(ESN) is discussed on the attempt to train a reservoir model absolutely in
physics-informed manner. As the plainest work on such a purpose, an ODE
(ordinary differential equation) approximator is designed to replicate the
solution in sequence with respect to the recurrent evaluations. On the
principal invariance of differential equations, the constraint in recurrence
just takes shape to secure a proper regression method for the ESN-based ODE
approximator. After then, the actual training process is established on the
idea of two-pass strategy for regression. Aiming at the fully physics-informed
reservoir model, a couple of nonlinear dynamical problems are demonstrated as
the computations obtained from the proposed method in this study.
</p>
<a href="http://arxiv.org/abs/2011.06769" target="_blank">arXiv:2011.06769</a> [<a href="http://arxiv.org/pdf/2011.06769" target="_blank">pdf</a>]

<h2>Lightweight Single-Image Super-Resolution Network with Attentive Auxiliary Feature Learning. (arXiv:2011.06773v1 [cs.CV])</h2>
<h3>Xuehui Wang, Qing Wang, Yuzhi Zhao, Junchi Yan, Lei Fan, Long Chen</h3>
<p>Despite convolutional network-based methods have boosted the performance of
single image super-resolution (SISR), the huge computation costs restrict their
practical applicability. In this paper, we develop a computation efficient yet
accurate network based on the proposed attentive auxiliary features (A$^2$F)
for SISR. Firstly, to explore the features from the bottom layers, the
auxiliary feature from all the previous layers are projected into a common
space. Then, to better utilize these projected auxiliary features and filter
the redundant information, the channel attention is employed to select the most
important common feature based on current layer feature. We incorporate these
two modules into a block and implement it with a lightweight network.
Experimental results on large-scale dataset demonstrate the effectiveness of
the proposed model against the state-of-the-art (SOTA) SR methods. Notably,
when parameters are less than 320k, A$^2$F outperforms SOTA methods for all
scales, which proves its ability to better utilize the auxiliary features.
Codes are available at https://github.com/wxxxxxxh/A2F-SR.
</p>
<a href="http://arxiv.org/abs/2011.06773" target="_blank">arXiv:2011.06773</a> [<a href="http://arxiv.org/pdf/2011.06773" target="_blank">pdf</a>]

<h2>Learning Scalable Self-Driving Policies for Generic Traffic Scenarios. (arXiv:2011.06775v1 [cs.RO])</h2>
<h3>Peide Cai, Hengli Wang, Yuxiang Sun, Ming Liu</h3>
<p>Robust and safe self-driving in complex and dynamic environments is quite
challenging due to the requirement of scalable driving policies against the
wide variety of traffic scenarios (e,g., road topologies, traffic rules and
interaction with road agents). In this area, traditional modular frameworks
scale poorly in new environments, and require tremendous and iterative
hand-tuning of rules and parameters to maintain performance in all foreseeable
scenarios. Recently, deep-learning based self-driving methods have shown
promising results with better generalization capability but less hand
engineering effort. However, most of the previous methods are trained and
evaluated in limited and simple environments with scattered tasks, such as
lane-following, autonomous braking and conditional driving. In this paper, we
propose a graph-based deep network to achieve unified and scalable self-driving
in diverse dynamic environments. The extensive evaluation results show that our
model can safely navigate the vehicle in a large variety of urban, rural, and
highway areas with dense traffic while obeying traffic rules. Specifically,
more than 7,500 km of closed-loop driving evaluation is conducted in dynamic
simulation environments, in which our method can handle complex driving
situations, and achieve higher success rates (73.5%-83.2%) and driving scores
than the baselines.
</p>
<a href="http://arxiv.org/abs/2011.06775" target="_blank">arXiv:2011.06775</a> [<a href="http://arxiv.org/pdf/2011.06775" target="_blank">pdf</a>]

<h2>Fast and Scalable Earth Texture Synthesis using Spatially Assembled Generative Adversarial Neural Networks. (arXiv:2011.06776v1 [cs.CV])</h2>
<h3>Sung Eun Kim, Hongkyu Yoon, Jonghyun Lee</h3>
<p>The earth texture with complex morphological geometry and compositions such
as shale and carbonate rocks, is typically characterized with sparse field
samples because of an expensive and time-consuming characterization process.
Accordingly, generating arbitrary large size of the geological texture with
similar topological structures at a low computation cost has become one of the
key tasks for realistic geomaterial reconstruction. Recently, generative
adversarial neural networks (GANs) have demonstrated a potential of
synthesizing input textural images and creating equiprobable geomaterial
images. However, the texture synthesis with the GANs framework is often limited
by the computational cost and scalability of the output texture size. In this
study, we proposed a spatially assembled GANs (SAGANs) that can generate output
images of an arbitrary large size regardless of the size of training images
with computational efficiency. The performance of the SAGANs was evaluated with
two and three dimensional (2D and 3D) rock image samples widely used in
geostatistical reconstruction of the earth texture. We demonstrate SAGANs can
generate the arbitrary large size of statistical realizations with connectivity
and structural properties similar to training images, and also can generate a
variety of realizations even on a single training image. In addition, the
computational time was significantly improved compared to standard GANs
frameworks.
</p>
<a href="http://arxiv.org/abs/2011.06776" target="_blank">arXiv:2011.06776</a> [<a href="http://arxiv.org/pdf/2011.06776" target="_blank">pdf</a>]

<h2>ROLL: Visual Self-Supervised Reinforcement Learning with Object Reasoning. (arXiv:2011.06777v1 [cs.LG])</h2>
<h3>Yufei Wang, Gautham Narayan Narasimhan, Xingyu Lin, Brian Okorn, David Held</h3>
<p>Current image-based reinforcement learning (RL) algorithms typically operate
on the whole image without performing object-level reasoning. This leads to
inefficient goal sampling and ineffective reward functions. In this paper, we
improve upon previous visual self-supervised RL by incorporating object-level
reasoning and occlusion reasoning. Specifically, we use unknown object
segmentation to ignore distractors in the scene for better reward computation
and goal generation; we further enable occlusion reasoning by employing a novel
auxiliary loss and training scheme. We demonstrate that our proposed algorithm,
ROLL (Reinforcement learning with Object Level Learning), learns dramatically
faster and achieves better final performance compared with previous methods in
several simulated visual control tasks. Project video and code are available at
https://sites.google.com/andrew.cmu.edu/roll.
</p>
<a href="http://arxiv.org/abs/2011.06777" target="_blank">arXiv:2011.06777</a> [<a href="http://arxiv.org/pdf/2011.06777" target="_blank">pdf</a>]

<h2>A differential evolution-based optimization tool for interplanetary transfer trajectory design. (arXiv:2011.06780v1 [cs.AI])</h2>
<h3>Mingcheng Zuo, Guangming Dai, Lei Peng, Zhe Tang</h3>
<p>The extremely sensitive and highly nonlinear search space of interplanetary
transfer trajectory design bring about big challenges on global optimization.
As a representative, the current known best solution of the global trajectory
optimization problem (GTOP) designed by the European space agency (ESA) is very
hard to be found. To deal with this difficulty, a powerful differential
evolution-based optimization tool named \textbf{CO}operative
\textbf{D}ifferential \textbf{E}volution (CODE) is proposed in this paper. CODE
employs a two-stage evolutionary process, which concentrates on learning global
structure in the earlier process, and tends to self-adaptively learn the
structures of different local spaces. Besides, considering the spatial
distribution of global optimum on different problems and the gradient
information on different variables, a multiple boundary check technique has
been employed. Also, Covariance Matrix Adaptation Evolutionary Strategies
(CMA-ES) is used as a local optimizer. The previous studies have shown that a
specific swarm intelligent optimization algorithm usually can solve only one or
two GTOP problems. However, the experimental test results show that CODE can
find the current known best solutions of Cassini1 and Sagas directly, and the
cooperation with CMA-ES can solve Cassini2, GTOC1, Messenger (reduced) and
Rosetta. For the most complicated Messenger (full) problem, even though CODE
cannot find the current known best solution, the found best solution with
objective function equaling to 3.38 km/s is still a level that other swarm
intelligent algorithms cannot easily reach.
</p>
<a href="http://arxiv.org/abs/2011.06780" target="_blank">arXiv:2011.06780</a> [<a href="http://arxiv.org/pdf/2011.06780" target="_blank">pdf</a>]

<h2>A Reweighted Meta Learning Framework for Robust Few Shot Learning. (arXiv:2011.06782v1 [cs.LG])</h2>
<h3>Krishnateja Killamsetty, Changbin Li, Chen Zhao, Rishabh Iyer, Feng Chen</h3>
<p>Model-Agnostic Meta-Learning (MAML) is a popular gradient-based meta-learning
framework that tries to find an optimal initialization to minimize the expected
loss across all tasks during meta-training. However, it inherently assumes that
the contribution of each instance/task to the meta-learner is equal. Therefore,
it fails to address the problem of domain differences between base and novel
classes in few-shot learning. In this work, we propose a novel and robust
meta-learning algorithm, called RW-MAML, which learns to assign weights to
training instances or tasks. We consider these weights to be hyper-parameters.
Hence, we iteratively optimize the weights using a small set of validation
tasks and an online approximation in a \emph{bi-bi-level} optimization
framework, in contrast to the standard bi-level optimization in MAML.
Therefore, we investigate a practical evaluation setting to demonstrate the
scalability of our RW-MAML in two scenarios: (1) out-of-distribution tasks and
(2) noisy labels in the meta-training stage. Extensive experiments on synthetic
and real-world datasets demonstrate that our framework efficiently mitigates
the effects of "unwanted" instances, showing that our proposed technique
significantly outperforms state-of-the-art robust meta-learning methods.
</p>
<a href="http://arxiv.org/abs/2011.06782" target="_blank">arXiv:2011.06782</a> [<a href="http://arxiv.org/pdf/2011.06782" target="_blank">pdf</a>]

<h2>Adaptive Future Frame Prediction with Ensemble Network. (arXiv:2011.06788v1 [cs.CV])</h2>
<h3>Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki</h3>
<p>Future frame prediction in videos is a challenging problem because videos
include complicated movements and large appearance changes. Learning-based
future frame prediction approaches have been proposed in kinds of literature. A
common limitation of the existing learning-based approaches is a mismatch of
training data and test data. In the future frame prediction task, we can obtain
the ground truth data by just waiting for a few frames. It means we can update
the prediction model online in the test phase. Then, we propose an adaptive
update framework for the future frame prediction task. The proposed adaptive
updating framework consists of a pre-trained prediction network, a
continuous-updating prediction network, and a weight estimation network. We
also show that our pre-trained prediction model achieves comparable performance
to the existing state-of-the-art approaches. We demonstrate that our approach
outperforms existing methods especially for dynamically changing scenes.
</p>
<a href="http://arxiv.org/abs/2011.06788" target="_blank">arXiv:2011.06788</a> [<a href="http://arxiv.org/pdf/2011.06788" target="_blank">pdf</a>]

<h2>High-Dimensional Multi-Task Averaging and Application to Kernel Mean Embedding. (arXiv:2011.06794v1 [stat.ML])</h2>
<h3>Hannah Marienwald (TUB), Jean-Baptiste Fermanian (ENS Rennes), Gilles Blanchard (DATASHAPE, LMO, CNRS)</h3>
<p>We propose an improved estimator for the multi-task averaging problem, whose
goal is the joint estimation of the means of multiple distributions using
separate, independent data sets. The naive approach is to take the empirical
mean of each data set individually, whereas the proposed method exploits
similarities between tasks, without any related information being known in
advance. First, for each data set, similar or neighboring means are determined
from the data by multiple testing. Then each naive estimator is shrunk towards
the local average of its neighbors. We prove theoretically that this approach
provides a reduction in mean squared error. This improvement can be significant
when the dimension of the input space is large, demonstrating a "blessing of
dimensionality" phenomenon. An application of this approach is the estimation
of multiple kernel mean embeddings, which plays an important role in many
modern applications. The theoretical results are verified on artificial and
real world data.
</p>
<a href="http://arxiv.org/abs/2011.06794" target="_blank">arXiv:2011.06794</a> [<a href="http://arxiv.org/pdf/2011.06794" target="_blank">pdf</a>]

<h2>Wisdom of the Ensemble: Improving Consistency of Deep Learning Models. (arXiv:2011.06796v1 [cs.LG])</h2>
<h3>Lijing Wang, Dipanjan Ghosh, Maria Teresa Gonzalez Diaz, Ahmed Farahat, Mahbubul Alam, Chetan Gupta, Jiangzhuo Chen, Madhav Marathe</h3>
<p>Deep learning classifiers are assisting humans in making decisions and hence
the user's trust in these models is of paramount importance. Trust is often a
function of constant behavior. From an AI model perspective it means given the
same input the user would expect the same output, especially for correct
outputs, or in other words consistently correct outputs. This paper studies a
model behavior in the context of periodic retraining of deployed models where
the outputs from successive generations of the models might not agree on the
correct labels assigned to the same input. We formally define consistency and
correct-consistency of a learning model. We prove that consistency and
correct-consistency of an ensemble learner is not less than the average
consistency and correct-consistency of individual learners and
correct-consistency can be improved with a probability by combining learners
with accuracy not less than the average accuracy of ensemble component
learners. To validate the theory using three datasets and two state-of-the-art
deep learning classifiers we also propose an efficient dynamic snapshot
ensemble method and demonstrate its value.
</p>
<a href="http://arxiv.org/abs/2011.06796" target="_blank">arXiv:2011.06796</a> [<a href="http://arxiv.org/pdf/2011.06796" target="_blank">pdf</a>]

<h2>Deep Template Matching for Pedestrian Attribute Recognition with the Auxiliary Supervision of Attribute-wise Keypoints. (arXiv:2011.06798v1 [cs.CV])</h2>
<h3>Jiajun Zhang, Pengyuan Ren, Jianmin Li</h3>
<p>Pedestrian Attribute Recognition (PAR) has aroused extensive attention due to
its important role in video surveillance scenarios. In most cases, the
existence of a particular attribute is strongly related to a partial region.
Recent works design complicated modules, e.g., attention mechanism and proposal
of body parts to localize the attribute corresponding region. These works
further prove that localization of attribute specific regions precisely will
help in improving performance. However, these part-information-based methods
are still not accurate as well as increasing model complexity which makes it
hard to deploy on realistic applications. In this paper, we propose a Deep
Template Matching based method to capture body parts features with less
computation. Further, we also proposed an auxiliary supervision method that use
human pose keypoints to guide the learning toward discriminative local cues.
Extensive experiments show that the proposed method outperforms and has lower
computational complexity, compared with the state-of-the-art approaches on
large-scale pedestrian attribute datasets, including PETA, PA-100K, RAP, and
RAPv2 zs.
</p>
<a href="http://arxiv.org/abs/2011.06798" target="_blank">arXiv:2011.06798</a> [<a href="http://arxiv.org/pdf/2011.06798" target="_blank">pdf</a>]

<h2>Federated Learning System without Model Sharing through Integration of Dimensional Reduced Data Representations. (arXiv:2011.06803v1 [cs.LG])</h2>
<h3>Anna Bogdanova, Akie Nakai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</h3>
<p>Dimensionality Reduction is a commonly used element in a machine learning
pipeline that helps to extract important features from high-dimensional data.
In this work, we explore an alternative federated learning system that enables
integration of dimensionality reduced representations of distributed data prior
to a supervised learning task, thus avoiding model sharing among the parties.
We compare the performance of this approach on image classification tasks to
three alternative frameworks: centralized machine learning, individual machine
learning, and Federated Averaging, and analyze potential use cases for a
federated learning system without model sharing. Our results show that our
approach can achieve similar accuracy as Federated Averaging and performs
better than Federated Averaging in a small-user setting.
</p>
<a href="http://arxiv.org/abs/2011.06803" target="_blank">arXiv:2011.06803</a> [<a href="http://arxiv.org/pdf/2011.06803" target="_blank">pdf</a>]

<h2>Learning Object Manipulation Skills via Approximate State Estimation from Real Videos. (arXiv:2011.06813v1 [cs.RO])</h2>
<h3>Vladim&#xed;r Petr&#xed;k, Makarand Tapaswi, Ivan Laptev, Josef Sivic</h3>
<p>Humans are adept at learning new tasks by watching a few instructional
videos. On the other hand, robots that learn new actions either require a lot
of effort through trial and error, or use expert demonstrations that are
challenging to obtain. In this paper, we explore a method that facilitates
learning object manipulation skills directly from videos. Leveraging recent
advances in 2D visual recognition and differentiable rendering, we develop an
optimization based method to estimate a coarse 3D state representation for the
hand and the manipulated object(s) without requiring any supervision. We use
these trajectories as dense rewards for an agent that learns to mimic them
through reinforcement learning. We evaluate our method on simple single- and
two-object actions from the Something-Something dataset. Our approach allows an
agent to learn actions from single videos, while watching multiple
demonstrations makes the policy more robust. We show that policies learned in a
simulated environment can be easily transferred to a real robot.
</p>
<a href="http://arxiv.org/abs/2011.06813" target="_blank">arXiv:2011.06813</a> [<a href="http://arxiv.org/pdf/2011.06813" target="_blank">pdf</a>]

<h2>SHAD3S: : A model to Sketch, Shade and Shadow. (arXiv:2011.06822v1 [cs.CV])</h2>
<h3>Raghav Brahmadesam Venkataramaiyer, Abhishek Joshi, Saisha Narang, Vinay P. Namboodiri</h3>
<p>Hatching is a common method used by artists to accentuate the third dimension
of a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete
with a human at hatching generic three-dimensional (3D) shapes, and also tries
to assist her in a form exploration exercise. The novelty of our approach lies
in the fact that we make no assumptions about the input other than that it
represents a 3D shape, and yet, given a contextual information of illumination
and texture, we synthesise an accurate hatch pattern over the sketch, without
access to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet
effective method to synthesise a sufficiently large high fidelity dataset,
pertinent to task; b) creating a pipeline with conditional generative
adversarial network (CGAN); and c) creating an interactive utility with GIMP,
that is a tool for artists to engage with automated hatching or a
form-exploration exercise. User evaluation of the tool suggests that the model
performance does generalise satisfactorily over diverse input, both in terms of
style as well as shape. A simple comparison of inception scores suggest that
the generated distribution is as diverse as the ground truth.
</p>
<a href="http://arxiv.org/abs/2011.06822" target="_blank">arXiv:2011.06822</a> [<a href="http://arxiv.org/pdf/2011.06822" target="_blank">pdf</a>]

<h2>LULC classification by semantic segmentation of satellite images using FastFCN. (arXiv:2011.06825v1 [cs.CV])</h2>
<h3>Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, Amreen Anbar, A. K. M. Nazrul Islam, A. K. M. Mahbubur Rahman</h3>
<p>This paper analyses how well a Fast Fully Convolu-tional Network (FastFCN)
semantically segments satellite images and thus classifies Land Use/Land
Cover(LULC) classes. Fast-FCN was used on Gaofen-2 Image Dataset (GID-2) to
segment them in five different classes: BuiltUp, Meadow, Farmland, Water and
Forest. The results showed better accuracy (0.93), precision (0.99), recall
(0.98) and mean Intersection over Union (mIoU)(0.97) than other approaches like
using FCN-8 or eCognition, a readily available software. We presented a
comparison between the results. We propose FastFCN to be both faster and more
accurate automated method than other existing methods for LULC classification.
</p>
<a href="http://arxiv.org/abs/2011.06825" target="_blank">arXiv:2011.06825</a> [<a href="http://arxiv.org/pdf/2011.06825" target="_blank">pdf</a>]

<h2>End-to-End Learning from Noisy Crowd to Supervised Machine Learning Models. (arXiv:2011.06833v1 [cs.LG])</h2>
<h3>Taraneh Younesian, Chi Hong, Amirmasoud Ghiassi, Robert Birke, Lydia Y. Chen</h3>
<p>Labeling real-world datasets is time consuming but indispensable for
supervised machine learning models. A common solution is to distribute the
labeling task across a large number of non-expert workers via crowd-sourcing.
Due to the varying background and experience of crowd workers, the obtained
labels are highly prone to errors and even detrimental to the learning models.
In this paper, we advocate using hybrid intelligence, i.e., combining deep
models and human experts, to design an end-to-end learning framework from noisy
crowd-sourced data, especially in an on-line scenario. We first summarize the
state-of-the-art solutions that address the challenges of noisy labels from
non-expert crowd and learn from multiple annotators. We show how label
aggregation can benefit from estimating the annotators' confusion matrices to
improve the learning process. Moreover, with the help of an expert labeler as
well as classifiers, we cleanse aggregated labels of highly informative samples
to enhance the final classification accuracy. We demonstrate the effectiveness
of our strategies on several image datasets, i.e. UCI and CIFAR-10, using SVM
and deep neural networks. Our evaluation shows that our on-line label
aggregation with confusion matrix estimation reduces the error rate of labels
by over 30%. Furthermore, relabeling only 10% of the data using the expert's
results in over 90% classification accuracy with SVM.
</p>
<a href="http://arxiv.org/abs/2011.06833" target="_blank">arXiv:2011.06833</a> [<a href="http://arxiv.org/pdf/2011.06833" target="_blank">pdf</a>]

<h2>Improving Offline Contextual Bandits with Distributional Robustness. (arXiv:2011.06835v1 [stat.ML])</h2>
<h3>Otmane Sakhi, Louis Faury, Flavian Vasile</h3>
<p>This paper extends the Distributionally Robust Optimization (DRO) approach
for offline contextual bandits. Specifically, we leverage this framework to
introduce a convex reformulation of the Counterfactual Risk Minimization
principle. Besides relying on convex programs, our approach is compatible with
stochastic optimization, and can therefore be readily adapted tothe large data
regime. Our approach relies on the construction of asymptotic confidence
intervals for offline contextual bandits through the DRO framework. By
leveraging known asymptotic results of robust estimators, we also show how to
automatically calibrate such confidence intervals, which in turn removes the
burden of hyper-parameter selection for policy optimization. We present
preliminary empirical results supporting the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2011.06835" target="_blank">arXiv:2011.06835</a> [<a href="http://arxiv.org/pdf/2011.06835" target="_blank">pdf</a>]

<h2>Unified Multi-Modal Landmark Tracking for Tightly Coupled Lidar-Visual-Inertial Odometry. (arXiv:2011.06838v1 [cs.RO])</h2>
<h3>David Wisth, Marco Camurri, Sandipan Das, Maurice Fallon</h3>
<p>We present an efficient multi-sensor odometry system for mobile platforms
that jointly optimizes visual, lidar, and inertial information within a single
integrated factor graph. This runs in real-time at full framerate using fixed
lag smoothing. To perform such tight integration, a new method to extract 3D
line and planar primitives from lidar point clouds is presented. This approach
overcomes the suboptimality of typical frame-to-frame tracking methods by
treating the primitives as landmarks and tracking them over multiple scans.
True integration of lidar features with standard visual features and IMU is
made possible using a subtle passive synchronization of lidar and camera
frames. The lightweight formulation of the 3D features allows for real-time
execution on a single CPU. Our proposed system has been tested on a variety of
platforms and scenarios, including underground exploration with a legged robot
and outdoor scanning with a dynamically moving handheld device, for a total
duration of 96 min and 2.4 km traveled distance. In these test sequences, using
only one exteroceptive sensor leads to failure due to either underconstrained
geometry (affecting lidar) and textureless areas caused by aggressive lighting
changes (affecting vision). In these conditions, our factor graph naturally
uses the best information available from each sensor modality without any hard
switches.
</p>
<a href="http://arxiv.org/abs/2011.06838" target="_blank">arXiv:2011.06838</a> [<a href="http://arxiv.org/pdf/2011.06838" target="_blank">pdf</a>]

<h2>A Homotopy Coordinate Descent Optimization Method for $l_0$-Norm Regularized Least Square Problem. (arXiv:2011.06841v1 [cs.LG])</h2>
<h3>Zhenzhen Sun, Yuanlong Yu</h3>
<p>This paper proposes a homotopy coordinate descent (HCD) method to solve the
$l_0$-norm regularized least square ($l_0$-LS) problem for compressed sensing,
which combine the homotopy technique with a variant of coordinate descent
method. Differs from the classical coordinate descent algorithms, HCD provides
three strategies to speed up the convergence: warm start initialization, active
set updating, and strong rule for active set initialization. The active set is
pre-selected using a strong rule, then the coordinates of the active set are
updated while those of inactive set are unchanged. The homotopy strategy
provides a set of warm start initial solutions for a sequence of decreasing
values of the regularization factor, which ensures all iterations along the
homotopy solution path are sparse. Computational experiments on simulate
signals and natural signals demonstrate effectiveness of the proposed
algorithm, in accurately and efficiently reconstructing sparse solutions of the
$l_0$-LS problem, whether the observation is noisy or not.
</p>
<a href="http://arxiv.org/abs/2011.06841" target="_blank">arXiv:2011.06841</a> [<a href="http://arxiv.org/pdf/2011.06841" target="_blank">pdf</a>]

<h2>Low-activity supervised convolutional spiking neural networks applied to speech commands recognition. (arXiv:2011.06846v1 [cs.LG])</h2>
<h3>Thomas Pellegrini, Romain Zimmer, Timoth&#xe9;e Masquelier</h3>
<p>Deep Neural Networks (DNNs) are the current state-of-the-art models in many
speech related tasks. There is a growing interest, though, for more
biologically realistic, hardware friendly and energy efficient models, named
Spiking Neural Networks (SNNs). Recently, it has been shown that SNNs can be
trained efficiently, in a supervised manner, using backpropagation with a
surrogate gradient trick. In this work, we report speech command (SC)
recognition experiments using supervised SNNs. We explored the
Leaky-Integrate-Fire (LIF) neuron model for this task, and show that a model
comprised of stacked dilated convolution spiking layers can reach an error rate
very close to standard DNNs on the Google SC v1 dataset: 5.5%, while keeping a
very sparse spiking activity, below 5%, thank to a new regularization term. We
also show that modeling the leakage of the neuron membrane potential is useful,
since the LIF model outperformed its non-leaky model counterpart significantly.
</p>
<a href="http://arxiv.org/abs/2011.06846" target="_blank">arXiv:2011.06846</a> [<a href="http://arxiv.org/pdf/2011.06846" target="_blank">pdf</a>]

<h2>Transductive Zero-Shot Learning using Cross-Modal CycleGAN. (arXiv:2011.06850v1 [cs.CV])</h2>
<h3>Patrick Bordes, Eloi Zablocki, Benjamin Piwowarski, Patrick Gallinari</h3>
<p>In Computer Vision, Zero-Shot Learning (ZSL) aims at classifying unseen
classes -- classes for which no matching training image exists. Most of ZSL
works learn a cross-modal mapping between images and class labels for seen
classes. However, the data distribution of seen and unseen classes might
differ, causing a domain shift problem. Following this observation,
transductive ZSL (T-ZSL) assumes that unseen classes and their associated
images are known during training, but not their correspondence. As current
T-ZSL approaches do not scale efficiently when the number of seen classes is
high, we tackle this problem with a new model for T-ZSL based upon CycleGAN.
Our model jointly (i) projects images on their seen class labels with a
supervised objective and (ii) aligns unseen class labels and visual exemplars
with adversarial and cycle-consistency objectives. We show the efficiency of
our Cross-Modal CycleGAN model (CM-GAN) on the ImageNet T-ZSL task where we
obtain state-of-the-art results. We further validate CM-GAN on a language
grounding task, and on a new task that we propose: zero-shot sentence-to-image
matching on MS COCO.
</p>
<a href="http://arxiv.org/abs/2011.06850" target="_blank">arXiv:2011.06850</a> [<a href="http://arxiv.org/pdf/2011.06850" target="_blank">pdf</a>]

<h2>Population synthesis for urban resident modeling using deep generative models. (arXiv:2011.06851v1 [cs.LG])</h2>
<h3>Martin Johnsen, Oliver Brandt, Sergio Garrido, Francisco C. Pereira</h3>
<p>The impacts of new real estate developments are strongly associated to its
population distribution (types and compositions of households, incomes, social
demographics) conditioned on aspects such as dwelling typology, price,
location, and floor level. This paper presents a Machine Learning based method
to model the population distribution of upcoming developments of new buildings
within larger neighborhood/condo settings.

We use a real data set from Ecopark Township, a real estate development
project in Hanoi, Vietnam, where we study two machine learning algorithms from
the deep generative models literature to create a population of synthetic
agents: Conditional Variational Auto-Encoder (CVAE) and Conditional Generative
Adversarial Networks (CGAN). A large experimental study was performed, showing
that the CVAE outperforms both the empirical distribution, a non-trivial
baseline model, and the CGAN in estimating the population distribution of new
real estate development projects.
</p>
<a href="http://arxiv.org/abs/2011.06851" target="_blank">arXiv:2011.06851</a> [<a href="http://arxiv.org/pdf/2011.06851" target="_blank">pdf</a>]

<h2>Discriminative Feature Representation with Spatio-temporal Cues for Vehicle Re-identification. (arXiv:2011.06852v1 [cs.CV])</h2>
<h3>J. Tu, C. Chen, X. Huang, J. He, X. Guan</h3>
<p>Vehicle re-identification (re-ID) aims to discover and match the target
vehicles from a gallery image set taken by different cameras on a wide range of
road networks. It is crucial for lots of applications such as security
surveillance and traffic management. The remarkably similar appearances of
distinct vehicles and the significant changes of viewpoints and illumination
conditions take grand challenges to vehicle re-ID. Conventional solutions focus
on designing global visual appearances without sufficient consideration of
vehicles' spatiotamporal relationships in different images. In this paper, we
propose a novel discriminative feature representation with spatiotemporal clues
(DFR-ST) for vehicle re-ID. It is capable of building robust features in the
embedding space by involving appearance and spatio-temporal information. Based
on this multi-modal information, the proposed DFR-ST constructs an appearance
model for a multi-grained visual representation by a two-stream architecture
and a spatio-temporal metric to provide complementary information. Experimental
results on two public datasets demonstrate DFR-ST outperforms the
state-of-the-art methods, which validate the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2011.06852" target="_blank">arXiv:2011.06852</a> [<a href="http://arxiv.org/pdf/2011.06852" target="_blank">pdf</a>]

<h2>DANAE: a denoising autoencoder for underwater attitude estimation. (arXiv:2011.06853v1 [cs.RO])</h2>
<h3>Paolo Russo, Fabiana Di Ciaccio, Salvatore Troisi</h3>
<p>One of the main issues for underwater robots navigation is their accurate
positioning, which heavily depends on the orientation estimation phase. The
systems employed to this scope are affected by different noise typologies,
mainly related to the sensors and to the irregular noise of the underwater
environment. Filtering algorithms can reduce their effect if opportunely
configured, but this process usually requires fine techniques and time. In this
paper we propose DANAE, a deep Denoising AutoeNcoder for Attitude Estimation
which works on Kalman filter IMU/AHRS data integration with the aim of reducing
any kind of noise, independently of its nature. This deep learning-based
architecture showed to be robust and reliable, significantly improving the
Kalman filter results. Further tests could make this method suitable for
real-time applications on navigation tasks.
</p>
<a href="http://arxiv.org/abs/2011.06853" target="_blank">arXiv:2011.06853</a> [<a href="http://arxiv.org/pdf/2011.06853" target="_blank">pdf</a>]

<h2>Collaborative Robotic Manipulation: A Use Case of Articulated Objects in Three-dimensions with Gravity. (arXiv:2011.06865v1 [cs.RO])</h2>
<h3>Riccardo Bertolucci, Alessio Capitanelli, Marco Maratea, Fulvio Mastrogiovanni, Mauro Vallati</h3>
<p>This paper addresses two intertwined needs for collaborative robots operating
in shop-floor environments. The first is the ability to perform complex
manipulation operations, such as those on articulated or even flexible objects,
in a way robust to a high degree of variability in the actions possibly carried
out by human operators during collaborative tasks. The second is encoding in
such operations a basic knowledge about physical laws (e.g., gravity), and
their effects on the models used by the robot to plan its actions, to generate
more robust plans. We adopt the manipulation in three-dimensional space of
articulated objects as an effective use case to ground both needs, and we use a
variant of the Planning Domain Definition Language to integrate the planning
process with a notion of gravity. Different complexity levels in modelling
gravity are evaluated, which trade-off model faithfulness and performance. A
thorough validation of the framework is done in simulation using a dual-arm
Baxter manipulator.
</p>
<a href="http://arxiv.org/abs/2011.06865" target="_blank">arXiv:2011.06865</a> [<a href="http://arxiv.org/pdf/2011.06865" target="_blank">pdf</a>]

<h2>Hybrid Federated and Centralized Learning. (arXiv:2011.06892v1 [cs.LG])</h2>
<h3>Ahmet M. Elbir</h3>
<p>Many of the machine learning (ML) tasks are focused on centralized learning
(CL), which requires the transmission of local datasets from the clients to a
parameter server (PS), which entails huge communication overhead. To overcome
this issue, federated learning (FL) has been a promising tool, wherein the
clients send only the model updates to the PS instead of the whole dataset.
Thus, FL brings the learning task into the edge level, which demands powerful
computational resources from the clients. This requirement may not be satisfied
in all ML applications due to diversity of the edge devices in terms of
computation power. In this work, we propose a hybrid federated and centralized
learning (HFCL) framework to effectively train a learning model by exploiting
the computational capability of the clients. In HFCL, only the clients who have
sufficient resources employ FL while the ones who do not resort to CL by
transmitting their local dataset to the PS. We also propose a sequential data
transmission approach with HFCL (HFCL-SDT) to sequentially transmit the
datasets in order to reduce the duration of the training. The proposed method
is advantageous since all the clients collaborate on the learning process
regardless of their computational resources. Via numerical simulations, the
proposed HFCL scheme is shown to be superior than FL with a moderate
communication overhead between FL and CL.
</p>
<a href="http://arxiv.org/abs/2011.06892" target="_blank">arXiv:2011.06892</a> [<a href="http://arxiv.org/pdf/2011.06892" target="_blank">pdf</a>]

<h2>Online Object-Oriented Semantic Mapping and Map Updating with Modular Representations. (arXiv:2011.06895v1 [cs.RO])</h2>
<h3>Nils Dengler, Tobias Zaenker, Francesco Verdoja, Maren Bennewitz</h3>
<p>Creating and maintaining an accurate representation of the environment is an
essential capability for every service robot. Especially semantic information
is important for household robots acting in indoor environments. In this paper,
we present a semantic mapping framework with modular map representations. Our
system is capable of online mapping and object updating given object detections
from RGB-D~data and provides various 2D and 3D~representations of the mapped
objects. To undo wrong data association, we perform a refinement step when
updating object shapes. Furthermore, we maintain a likelihood for each object
to deal with false positive and false negative detections and keep the map
updated. Our mapping system is highly efficient and achieves a run time of more
than 10 Hz. We evaluated our approach in various environments using two
different robots, i.e., a HSR by Toyota and a \mbox{Care-O-Bot-4} by
Fraunhofer. As the experimental results demonstrate, our system is able to
generate maps that are close to the ground truth and outperforms an existing
approach in terms of intersection over union, different distance metrics, and
the number of correct object mappings. We plan to publish the code of our
system for the final submission.
</p>
<a href="http://arxiv.org/abs/2011.06895" target="_blank">arXiv:2011.06895</a> [<a href="http://arxiv.org/pdf/2011.06895" target="_blank">pdf</a>]

<h2>Real time implementation of CTRNN and BPTT algorithm to learn on-line biped robot balance: Experiments on the standing posture. (arXiv:2011.06910v1 [cs.RO])</h2>
<h3>Patrick Henaff (ETIS - UMR 8051), Vincent Scesa, Fethi Ben Ouezdou (LISV), Olivier Bruneau (LISV)</h3>
<p>This paper describes experimental results regarding the real time
implementation of continuous time recurrent neural networks (CTRNN) and the
dynamic back-propagation through time (BPTT) algorithm for the on-line learning
control laws. Experiments are carried out to control the balance of a biped
robot prototype in its standing posture. The neural controller is trained to
compensate for external perturbations by controlling the torso's joint motions.
Algorithms are embedded in the real time electronic unit of the robot. On-line
learning implementations are presented in detail. The results on learning
behavior and control performance demonstrate the strength and the efficiency of
the proposed approach.
</p>
<a href="http://arxiv.org/abs/2011.06910" target="_blank">arXiv:2011.06910</a> [<a href="http://arxiv.org/pdf/2011.06910" target="_blank">pdf</a>]

<h2>Mechanics of compliant serial manipulator composed of dual-triangle segments. (arXiv:2011.06912v1 [cs.RO])</h2>
<h3>Damien Chablat (ReV, LS2N), Wanda Zhao (LS2N, ReV), Anatol Pashkevich (LS2N, ReV, IMT Atlantique - DAPI), Alexandr Klimchik</h3>
<p>The paper focuses on the mechanics of a compliant serial manipulator composed
of new type of dual-triangle elastic segments. Both the analytical and
numerical methods were used to find the manipulator stable and unstable
equilibrium configurations, as well as to predict corresponding manipulator
shapes. The stiffness analysis was carried on for both loaded and unloaded
modes, the stiffness matrices were computed using the Virtual Joint Method
(VJM). The results demonstrate that either buckling or quasi-buckling
phenomenon may occur under the loading, if the manipulator corresponding
initial configuration is straight or non-straight one. Relevant simulation
results are presented that confirm the theoretical study.
</p>
<a href="http://arxiv.org/abs/2011.06912" target="_blank">arXiv:2011.06912</a> [<a href="http://arxiv.org/pdf/2011.06912" target="_blank">pdf</a>]

<h2>Image Animation with Perturbed Masks. (arXiv:2011.06922v1 [cs.CV])</h2>
<h3>Yoav Shalev, Lior Wolf</h3>
<p>We present a novel approach for image-animation of a source image by a
driving video, both depicting the same type of object. We do not assume the
existence of pose models and our method is able to animate arbitrary objects
without knowledge of the object's structure. Furthermore, both the driving
video and the course image are only seen during test-time. Our method is based
on a shared mask generator, which separates the foreground object from its
background, and captures the object's general pose and shape. A mask-refinement
module then replaces, in the mask extracted from the driver image, the identity
of the driver with the identity of the source. Conditioned on the source image,
the transformed mask is then decoded by a multi-scale generator that renders a
realistic image, in which the content of the source frame is animated by the
pose in the driving video. Due to lack of fully supervised data, we train on
the task of reconstructing frames from the same video the source image is taken
from. In order to control source of the identity of the output frame, we employ
during training perturbations that remove the unwanted identity information.
Our method is shown to greatly outperform the state of the art methods on
multiple benchmarks. Our code and samples are available at
https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks.
</p>
<a href="http://arxiv.org/abs/2011.06922" target="_blank">arXiv:2011.06922</a> [<a href="http://arxiv.org/pdf/2011.06922" target="_blank">pdf</a>]

<h2>LEAN: graph-based pruning for convolutional neural networks by extracting longest chains. (arXiv:2011.06923v1 [cs.LG])</h2>
<h3>Richard Schoonhoven, Allard A. Hendriksen, Dani&#xeb;l M. Pelt, K. Joost Batenburg</h3>
<p>Convolutional neural networks (CNNs) have proven to be highly successful at a
range of image-to-image tasks. CNNs can be computationally expensive, which can
limit their applicability in practice. Model pruning can improve computational
efficiency by sparsifying trained networks. Common methods for pruning CNNs
determine what convolutional filters to remove by ranking filters on an
individual basis. However, filters are not independent, as CNNs consist of
chains of convolutions, which can result in sub-optimal filter selection.

We propose a novel pruning method, LongEst-chAiN (LEAN) pruning, which takes
the interdependency between the convolution operations into account. We propose
to prune CNNs by using graph-based algorithms to select relevant chains of
convolutions. A CNN is interpreted as a graph, with the operator norm of each
convolution as distance metric for the edges. LEAN pruning iteratively extracts
the highest value path from the graph to keep. In our experiments, we test LEAN
pruning for several image-to-image tasks, including the well-known CamVid
dataset. LEAN pruning enables us to keep just 0.5%-2% of the convolutions
without significant loss of accuracy. When pruning CNNs with LEAN, we achieve a
higher accuracy than pruning filters individually, and different pruned
substructures emerge.
</p>
<a href="http://arxiv.org/abs/2011.06923" target="_blank">arXiv:2011.06923</a> [<a href="http://arxiv.org/pdf/2011.06923" target="_blank">pdf</a>]

<h2>A Study of Image Pre-processing for Faster Object Recognition. (arXiv:2011.06928v1 [cs.CV])</h2>
<h3>Md Tanzil Shahriar, Huyue Li</h3>
<p>Quality of image always plays a vital role in in-creasing object recognition
or classification rate. A good quality image gives better recognition or
classification rate than any unprocessed noisy images. It is more difficult to
extract features from such unprocessed images which in-turn reduces object
recognition or classification rate. To overcome problems occurred due to low
quality image, typically pre-processing is done before extracting features from
the image. Our project proposes an image pre-processing method, so that the
performance of selected Machine Learning algorithms or Deep Learning algorithms
increases in terms of increased accuracy or reduced the number of training
images. In the later part, we compare the performance results by using our
method with the previous used approaches.
</p>
<a href="http://arxiv.org/abs/2011.06928" target="_blank">arXiv:2011.06928</a> [<a href="http://arxiv.org/pdf/2011.06928" target="_blank">pdf</a>]

<h2>Non-stationary Online Regression. (arXiv:2011.06957v1 [cs.LG])</h2>
<h3>Anant Raj, Pierre Gaillard (SIERRA, Thoth), Christophe Saad (CMU)</h3>
<p>Online forecasting under a changing environment has been a problem of
increasing importance in many real-world applications. In this paper, we
consider the meta-algorithm presented in \citet{zhang2017dynamic} combined with
different subroutines. We show that an expected cumulative error of order
$\tilde{O}(n^{1/3} C_n^{2/3})$ can be obtained for non-stationary online linear
regression where the total variation of parameter sequence is bounded by $C_n$.
Our paper extends the result of online forecasting of one-dimensional
time-series as proposed in \cite{baby2019online} to general $d$-dimensional
non-stationary linear regression. We improve the rate $O(\sqrt{n C_n})$
obtained by Zhang et al. 2017 and Besbes et al. 2015. We further extend our
analysis to non-stationary online kernel regression. Similar to the
non-stationary online regression case, we use the meta-procedure of Zhang et
al. 2017 combined with Kernel-AWV (Jezequel et al. 2020) to achieve an expected
cumulative controlled by the effective dimension of the RKHS and the total
variation of the sequence. To the best of our knowledge, this work is the first
extension of non-stationary online regression to non-stationary kernel
regression. Lastly, we evaluate our method empirically with several existing
benchmarks and also compare it with the theoretical bound obtained in this
paper.
</p>
<a href="http://arxiv.org/abs/2011.06957" target="_blank">arXiv:2011.06957</a> [<a href="http://arxiv.org/pdf/2011.06957" target="_blank">pdf</a>]

<h2>SALAD: Self-Assessment Learning for Action Detection. (arXiv:2011.06958v1 [cs.LG])</h2>
<h3>Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard</h3>
<p>Literature on self-assessment in machine learning mainly focuses on the
production of well-calibrated algorithms through consensus frameworks i.e.
calibration is seen as a problem. Yet, we observe that learning to be properly
confident could behave like a powerful regularization and thus, could be an
opportunity to improve performance.Precisely, we show that used within a
framework of action detection, the learning of a self-assessment score is able
to improve the whole action localization process.Experimental results show that
our approach outperforms the state-of-the-art on two action detection
benchmarks. On THUMOS14 dataset, the mAP at tIoU@0.5 is improved from 42.8\% to
44.6\%, and from 50.4\% to 51.7\% on ActivityNet1.3 dataset. For lower tIoU
values, we achieve even more significant improvements on both datasets.
</p>
<a href="http://arxiv.org/abs/2011.06958" target="_blank">arXiv:2011.06958</a> [<a href="http://arxiv.org/pdf/2011.06958" target="_blank">pdf</a>]

<h2>Efficient Subspace Search in Data Streams. (arXiv:2011.06959v1 [cs.LG])</h2>
<h3>Edouard Fouch&#xe9;, Florian Kalinke, Klemens B&#xf6;hm</h3>
<p>In the real world, data streams are ubiquitous -- think of network traffic or
sensor data. Mining patterns, e.g., outliers or clusters, from such data must
take place in real time. This is challenging because (1) streams often have
high dimensionality, and (2) the data characteristics may change over time.
Existing approaches tend to focus on only one aspect, either high
dimensionality or the specifics of the streaming setting. For static data, a
common approach to deal with high dimensionality -- known as subspace search --
extracts low-dimensional, `interesting' projections (subspaces), in which
patterns are easier to find. In this paper, we address both Challenge (1) and
(2) by generalising subspace search to data streams. Our approach, Streaming
Greedy Maximum Random Deviation (SGMRD), monitors interesting subspaces in
high-dimensional data streams. It leverages novel multivariate dependency
estimators and monitoring techniques based on bandit theory. We show that the
benefits of SGMRD are twofold: (i) It monitors subspaces efficiently, and (ii)
this improves the results of downstream data mining tasks, such as outlier
detection. Our experiments, performed against synthetic and real-world data,
demonstrate that SGMRD outperforms its competitors by a large margin.
</p>
<a href="http://arxiv.org/abs/2011.06959" target="_blank">arXiv:2011.06959</a> [<a href="http://arxiv.org/pdf/2011.06959" target="_blank">pdf</a>]

<h2>Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis. (arXiv:2011.06961v1 [cs.CV])</h2>
<h3>Daniel Seichter, Mona K&#xf6;hler, Benjamin Lewandowski, Tim Wengefeld, Horst-Michael Gross</h3>
<p>Analyzing scenes thoroughly is crucial for mobile robots acting in different
environments. Semantic segmentation can enhance various subsequent tasks, such
as (semantically assisted) person perception, (semantic) free space detection,
(semantic) mapping, and (semantic) navigation. In this paper, we propose an
efficient and robust RGB-D segmentation approach that can be optimized to a
high degree using NVIDIA TensorRT and, thus, is well suited as a common initial
processing step in a complex system for scene analysis on mobile robots. We
show that RGB-D segmentation is superior to processing RGB images solely and
that it can still be performed in real time if the network architecture is
carefully designed. We evaluate our proposed Efficient Scene Analysis Network
(ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that it
reaches state-of-the-art performance when considering both segmentation
performance and runtime. Furthermore, our evaluation on the outdoor dataset
Cityscapes shows that our approach is suitable for other areas of application
as well. Finally, instead of presenting benchmark results only, we show
qualitative results in one of our indoor application scenarios.
</p>
<a href="http://arxiv.org/abs/2011.06961" target="_blank">arXiv:2011.06961</a> [<a href="http://arxiv.org/pdf/2011.06961" target="_blank">pdf</a>]

<h2>Determinantal Point Processes Implicitly Regularize Semi-parametric Regression Problems. (arXiv:2011.06964v1 [cs.LG])</h2>
<h3>Micha&#xeb;l Fanuel, Joachim Schreurs, Johan A.K. Suykens</h3>
<p>Semi-parametric regression models are used in several applications which
require comprehensibility without sacrificing accuracy. Examples are spline
interpolation in geophysics, or non-linear time series problems, where the
system includes for instance a linear and non-linear component. We discuss here
the use of a finite Determinantal Point Process (DPP) sampling for
approximating semi-parametric models in two cases. On the one hand, in the case
of large training data sets, DPP sampling is used to reduce the number of model
parameters. On the other hand, DPPs can determine experimental designs in the
case of the optimal interpolation models. Recently, Barthelm\'e, Tremblay,
Usevich, and Amblard introduced a novel representation of finite DPP's. They
formulated extended $L$-ensembles that can conveniently represent for instance
partial-projection DPPs and suggest their use for optimal interpolation. With
the help of this formalism, we derive a key identity illustrating the implicit
regularization effect of determinantal sampling for semi-parametric regression
and interpolation. Also, a novel projected Nystr\"om approximation is defined
and used to derive a bound on the expected risk for the corresponding
approximation of semi-parametric regression. This work naturally extends
similar results obtained for kernel ridge regression.
</p>
<a href="http://arxiv.org/abs/2011.06964" target="_blank">arXiv:2011.06964</a> [<a href="http://arxiv.org/pdf/2011.06964" target="_blank">pdf</a>]

<h2>Transformer-Encoder Detector Module: Using Context to Improve Robustness to Adversarial Attacks on Object Detection. (arXiv:2011.06978v1 [cs.CV])</h2>
<h3>Faisal Alamri, Sinan Kalkan, Nicolas Pugeault</h3>
<p>Deep neural network approaches have demonstrated high performance in object
recognition (CNN) and detection (Faster-RCNN) tasks, but experiments have shown
that such architectures are vulnerable to adversarial attacks (FFF, UAP): low
amplitude perturbations, barely perceptible by the human eye, can lead to a
drastic reduction in labeling performance. This article proposes a new context
module, called \textit{Transformer-Encoder Detector Module}, that can be
applied to an object detector to (i) improve the labeling of object instances;
and (ii) improve the detector's robustness to adversarial attacks. The proposed
model achieves higher mAP, F1 scores and AUC average score of up to 13\%
compared to the baseline Faster-RCNN detector, and an mAP score 8 points higher
on images subjected to FFF or UAP attacks due to the inclusion of both
contextual and visual features extracted from scene and encoded into the model.
The result demonstrates that a simple ad-hoc context module can improve the
reliability of object detectors significantly.
</p>
<a href="http://arxiv.org/abs/2011.06978" target="_blank">arXiv:2011.06978</a> [<a href="http://arxiv.org/pdf/2011.06978" target="_blank">pdf</a>]

<h2>Multi-layered tensor networks for image classification. (arXiv:2011.06982v1 [cs.CV])</h2>
<h3>Raghavendra Selvan, Silas &#xd8;rting, Erik B Dam</h3>
<p>The recently introduced locally orderless tensor network (LoTeNet) for
supervised image classification uses matrix product state (MPS) operations on
grids of transformed image patches. The resulting patch representations are
combined back together into the image space and aggregated hierarchically using
multiple MPS blocks per layer to obtain the final decision rules. In this work,
we propose a non-patch based modification to LoTeNet that performs one MPS
operation per layer, instead of several patch-level operations. The spatial
information in the input images to MPS blocks at each layer is squeezed into
the feature dimension, similar to LoTeNet, to maximise retained spatial
correlation between pixels when images are flattened into 1D vectors. The
proposed multi-layered tensor network (MLTN) is capable of learning linear
decision boundaries in high dimensional spaces in a multi-layered setting,
which results in a reduction in the computation cost compared to LoTeNet
without any degradation in performance.
</p>
<a href="http://arxiv.org/abs/2011.06982" target="_blank">arXiv:2011.06982</a> [<a href="http://arxiv.org/pdf/2011.06982" target="_blank">pdf</a>]

<h2>Robotic self-representation improves manipulation skills and transfer learning. (arXiv:2011.06985v1 [cs.RO])</h2>
<h3>Phuong D.H. Nguyen, Manfred Eppe, Stefan Wermter</h3>
<p>Cognitive science suggests that the self-representation is critical for
learning and problem-solving. However, there is a lack of computational methods
that relate this claim to cognitively plausible robots and reinforcement
learning. In this paper, we bridge this gap by developing a model that learns
bidirectional action-effect associations to encode the representations of body
schema and the peripersonal space from multisensory information, which is named
multimodal BidAL. Through three different robotic experiments, we demonstrate
that this approach significantly stabilizes the learning-based problem-solving
under noisy conditions and that it improves transfer learning of robotic
manipulation skills.
</p>
<a href="http://arxiv.org/abs/2011.06985" target="_blank">arXiv:2011.06985</a> [<a href="http://arxiv.org/pdf/2011.06985" target="_blank">pdf</a>]

<h2>Learning Predictive Models for Ergonomic Control of Prosthetic Devices. (arXiv:2011.07005v1 [cs.RO])</h2>
<h3>Geoffrey Clark, Joseph Campbell, Heni Ben Amor</h3>
<p>We present Model-Predictive Interaction Primitives -- a robot learning
framework for assistive motion in human-machine collaboration tasks which
explicitly accounts for biomechanical impact on the human musculoskeletal
system. First, we extend Interaction Primitives to enable predictive
biomechanics: the prediction of future biomechanical states of a human partner
conditioned on current observations and intended robot control signals. In
turn, we leverage this capability within a model-predictive control strategy to
identify the future ergonomic and biomechanical ramifications of potential
robot actions. Optimal control trajectories are selected so as to minimize
future physical impact on the human musculoskeletal system. We empirically
demonstrate that our approach minimizes knee or muscle forces via generated
control actions selected according to biomechanical cost functions. Experiments
are performed in synthetic and real-world experiments involving powered
prosthetic devices.
</p>
<a href="http://arxiv.org/abs/2011.07005" target="_blank">arXiv:2011.07005</a> [<a href="http://arxiv.org/pdf/2011.07005" target="_blank">pdf</a>]

<h2>Federated Multi-Mini-Batch: An Efficient Training Approach to Federated Learning in Non-IID Environments. (arXiv:2011.07006v1 [cs.LG])</h2>
<h3>Mohammad Bakhtiari, Reza Nasirigerdeh, Reihaneh Torkzadehmahani, Amirhossein Bayat, David B. Blumenthal, Markus List, Jan Baumbach</h3>
<p>Federated learning is a well-established approach to privacy-preserving
training of a joint model on heavily distributed data. Federated averaging
(FedAvg) is a well-known communication-efficient algorithm for federated
learning, which performs well if the data distribution across the clients is
independently and identically distributed (IID). However, FedAvg provides a
lower accuracy and still requires a large number of communication rounds to
achieve a target accuracy when it comes to Non-IID environments. To address the
former limitation, we present federated single mini-batch (FedSMB), where the
clients train the model on a single mini-batch from their dataset in each
iteration. We show that FedSMB achieves the accuracy of the centralized
training in Non-IID configurations, but in a considerable number of iterations.
To address the latter limitation, we introduce federated multi-mini-batch
(FedMMB) as a generalization of FedSMB, where the clients train the model on
multiple mini-batches (specified by the batch count) in each communication
round. FedMMB decouples the batch size from the batch count and provides a
trade-off between the accuracy and communication efficiency in Non-IID
settings. This is not possible with FedAvg, in which a single parameter
determines both the batch size and batch count. The simulation results
illustrate that FedMMB outperforms FedAvg in terms of the accuracy,
communication efficiency, as well as computational efficiency and is an
efficient training approach to federated learning in Non-IID environments.
</p>
<a href="http://arxiv.org/abs/2011.07006" target="_blank">arXiv:2011.07006</a> [<a href="http://arxiv.org/pdf/2011.07006" target="_blank">pdf</a>]

<h2>Relative Drone -- Ground Vehicle Localization using LiDAR and Fisheye Cameras through Direct and Indirect Observations. (arXiv:2011.07008v1 [cs.RO])</h2>
<h3>Jan Hausberg, Ryoichi Ishikawa, Menandro Roxas, Takeshi Oishi</h3>
<p>Estimating the pose of an unmanned aerial vehicle (UAV) or drone is a
challenging task. It is useful for many applications such as navigation,
surveillance, tracking objects on the ground, and 3D reconstruction. In this
work, we present a LiDAR-camera-based relative pose estimation method between a
drone and a ground vehicle, using a LiDAR sensor and a fisheye camera on the
vehicle's roof and another fisheye camera mounted under the drone. The LiDAR
sensor directly observes the drone and measures its position, and the two
cameras estimate the relative orientation using indirect observation of the
surrounding objects. We propose a dynamically adaptive kernel-based method for
drone detection and tracking using the LiDAR. We detect vanishing points in
both cameras and find their correspondences to estimate the relative
orientation. Additionally, we propose a rotation correction technique by
relying on the observed motion of the drone through the LiDAR. In our
experiments, we were able to achieve very fast initial detection and real-time
tracking of the drone. Our method is fully automatic.
</p>
<a href="http://arxiv.org/abs/2011.07008" target="_blank">arXiv:2011.07008</a> [<a href="http://arxiv.org/pdf/2011.07008" target="_blank">pdf</a>]

<h2>Monitoring and Diagnosability of Perception Systems. (arXiv:2011.07010v1 [cs.RO])</h2>
<h3>Pasquale Antonante, David I. Spivak, Luca Carlone</h3>
<p>Perception is a critical component of high-integrity applications of robotics
and autonomous systems, such as self-driving vehicles. In these applications,
failure of perception systems may put human life at risk, and a broad adoption
of these technologies requires the development of methodologies to guarantee
and monitor safe operation. Despite the paramount importance of perception
systems, currently there is no formal approach for system-level monitoring. In
this work, we propose a mathematical model for runtime monitoring and fault
detection and identification in perception systems. Towards this goal, we draw
connections with the literature on diagnosability in multiprocessor systems,
and generalize it to account for modules with heterogeneous outputs that
interact over time. The resulting temporal diagnostic graphs (i) provide a
framework to reason over the consistency of perception outputs -- across
modules and over time -- thus enabling fault detection, (ii) allow us to
establish formal guarantees on the maximum number of faults that can be
uniquely identified in a given perception systems, and (iii) enable the design
of efficient algorithms for fault identification. We demonstrate our monitoring
system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving
simulator and the Apollo Auto autonomy software stack, and show that PerSyS is
able to detect failures in challenging scenarios (including scenarios that have
caused self-driving car accidents in recent years), and is able to correctly
identify faults while entailing a minimal computation overhead (&lt; 5ms on a
single-core CPU).
</p>
<a href="http://arxiv.org/abs/2011.07010" target="_blank">arXiv:2011.07010</a> [<a href="http://arxiv.org/pdf/2011.07010" target="_blank">pdf</a>]

<h2>Convex Optimization with an Interpolation-based Projection and its Application to Deep Learning. (arXiv:2011.07016v1 [cs.LG])</h2>
<h3>Riad Akrour, Asma Atamna, Jan Peters</h3>
<p>Convex optimizers have known many applications as differentiable layers
within deep neural architectures. One application of these convex layers is to
project points into a convex set. However, both forward and backward passes of
these convex layers are significantly more expensive to compute than those of a
typical neural network. We investigate in this paper whether an inexact, but
cheaper projection, can drive a descent algorithm to an optimum. Specifically,
we propose an interpolation-based projection that is computationally cheap and
easy to compute given a convex, domain defining, function. We then propose an
optimization algorithm that follows the gradient of the composition of the
objective and the projection and prove its convergence for linear objectives
and arbitrary convex and Lipschitz domain defining inequality constraints. In
addition to the theoretical contributions, we demonstrate empirically the
practical interest of the interpolation projection when used in conjunction
with neural networks in a reinforcement learning and a supervised learning
setting.
</p>
<a href="http://arxiv.org/abs/2011.07016" target="_blank">arXiv:2011.07016</a> [<a href="http://arxiv.org/pdf/2011.07016" target="_blank">pdf</a>]

<h2>NightVision: Generating Nighttime Satellite Imagery from Infra-Red Observations. (arXiv:2011.07017v1 [cs.CV])</h2>
<h3>Paula Harder, William Jones, Redouane Lguensat, Shahine Bouabid, James Fulton, D&#xe1;nell Quesada-Chac&#xf3;n, Aris Marcolongo, Sofija Stefanovi&#x107;, Yuhan Rao, Peter Manshausen, Duncan Watson-Parris</h3>
<p>The recent explosion in applications of machine learning to satellite imagery
often rely on visible images and therefore suffer from a lack of data during
the night. The gap can be filled by employing available infra-red observations
to generate visible images. This work presents how deep learning can be applied
successfully to create those images by using U-Net based architectures. The
proposed methods show promising results, achieving a structural similarity
index (SSIM) up to 86\% on an independent test set and providing visually
convincing output images, generated from infra-red observations.
</p>
<a href="http://arxiv.org/abs/2011.07017" target="_blank">arXiv:2011.07017</a> [<a href="http://arxiv.org/pdf/2011.07017" target="_blank">pdf</a>]

<h2>Synthetic Data -- A Privacy Mirage. (arXiv:2011.07018v1 [cs.LG])</h2>
<h3>Theresa Stadler, Bristena Oprisanu, Carmela Troncoso</h3>
<p>Synthetic datasets produced by generative models are advertised as a
silver-bullet solution to privacy-preserving data sharing. Claims about the
privacy benefits of synthetic data, however, have not been supported by a
rigorous privacy analysis. In this paper, we introduce an evaluation framework
that enables data holders to (I) quantify the privacy gain of publishing a
synthetic dataset instead of the raw data, and (II) compare the privacy
properties of generative model training algorithms. We illustrate the utility
of the framework and quantify privacy gain with respect to two concerns, the
risk of re-identification via linkage and the risk of attribute disclosure, on
synthetic data produced by a range of generative models, from simple
independent histograms to differentially private GANs. We find that, across the
board, synthetic data provides little privacy gain even under a black-box
adversary with access to a single synthetic dataset only. Moreover, we observe
that some target records receive substantially less protection than others and
that the more complex the generative model, the more difficult it is to predict
which targets will remain vulnerable to privacy attacks. Our findings highlight
the need to re-consider whether synthetic data is an appropriate strategy to
privacy-preserving data publishing.
</p>
<a href="http://arxiv.org/abs/2011.07018" target="_blank">arXiv:2011.07018</a> [<a href="http://arxiv.org/pdf/2011.07018" target="_blank">pdf</a>]

<h2>A Study of Domain Generalization on Ultrasound-based Multi-Class Segmentation of Arteries, Veins, Ligaments, and Nerves Using Transfer Learning. (arXiv:2011.07019v1 [cs.CV])</h2>
<h3>Edward Chen, Tejas Sudharshan Mathai, Vinit Sarode, Howie Choset, John Galeotti</h3>
<p>Identifying landmarks in the femoral area is crucial for ultrasound (US)
-based robot-guided catheter insertion, and their presentation varies when
imaged with different scanners. As such, the performance of past deep
learning-based approaches is also narrowly limited to the training data
distribution; this can be circumvented by fine-tuning all or part of the model,
yet the effects of fine-tuning are seldom discussed. In this work, we study the
US-based segmentation of multiple classes through transfer learning by
fine-tuning different contiguous blocks within the model, and evaluating on a
gamut of US data from different scanners and settings. We propose a simple
method for predicting generalization on unseen datasets and observe
statistically significant differences between the fine-tuning methods while
working towards domain generalization.
</p>
<a href="http://arxiv.org/abs/2011.07019" target="_blank">arXiv:2011.07019</a> [<a href="http://arxiv.org/pdf/2011.07019" target="_blank">pdf</a>]

<h2>Enabling the Sense of Self in a Dual-Arm Robot. (arXiv:2011.07026v1 [cs.RO])</h2>
<h3>Ali AlQallaf, Gerardo Aragon-Camarasa</h3>
<p>While humans are aware of their body and capabilities, robots are not. To
address this, we present in this paper a neural network architecture that
enables a dual-arm robot to get a sense of itself in an environment. Our
approach is inspired by human self-awareness developmental levels and serves as
the underlying building block for a robot to achieve awareness of itself while
carrying out tasks in an environment. We assume that a robot has to know itself
before interacting with the environment in order to be able to support
different robotic tasks. Hence, we implemented a neural network architecture to
enable a robot to differentiate its limbs from the environment using visual and
proprioception sensory inputs. We demonstrate experimentally that a robot can
distinguish itself with an accuracy of 88.7% on average in cluttered
environmental settings and under confounding input signals.
</p>
<a href="http://arxiv.org/abs/2011.07026" target="_blank">arXiv:2011.07026</a> [<a href="http://arxiv.org/pdf/2011.07026" target="_blank">pdf</a>]

<h2>DeepMind Lab2D. (arXiv:2011.07027v1 [cs.AI])</h2>
<h3>Charles Beattie, Thomas K&#xf6;ppe, Edgar A. Du&#xe9;&#xf1;ez-Guzm&#xe1;n, Joel Z. Leibo</h3>
<p>We present DeepMind Lab2D, a scalable environment simulator for artificial
intelligence research that facilitates researcher-led experimentation with
environment design. DeepMind Lab2D was built with the specific needs of
multi-agent deep reinforcement learning researchers in mind, but it may also be
useful beyond that particular subfield.
</p>
<a href="http://arxiv.org/abs/2011.07027" target="_blank">arXiv:2011.07027</a> [<a href="http://arxiv.org/pdf/2011.07027" target="_blank">pdf</a>]

<h2>Continual Learning with Deep Artificial Neurons. (arXiv:2011.07035v1 [cs.AI])</h2>
<h3>Blake Camp, Jaya Krishna Mandivarapu, Rolando Estrada</h3>
<p>Neurons in real brains are enormously complex computational units. Among
other things, they're responsible for transforming inbound electro-chemical
vectors into outbound action potentials, updating the strengths of intermediate
synapses, regulating their own internal states, and modulating the behavior of
other nearby neurons. One could argue that these cells are the only things
exhibiting any semblance of real intelligence. It is odd, therefore, that the
machine learning community has, for so long, relied upon the assumption that
this complexity can be reduced to a simple sum and fire operation. We ask,
might there be some benefit to substantially increasing the computational power
of individual neurons in artificial systems? To answer this question, we
introduce Deep Artificial Neurons (DANs), which are themselves realized as deep
neural networks. Conceptually, we embed DANs inside each node of a traditional
neural network, and we connect these neurons at multiple synaptic sites,
thereby vectorizing the connections between pairs of cells. We demonstrate that
it is possible to meta-learn a single parameter vector, which we dub a neuronal
phenotype, shared by all DANs in the network, which facilitates a
meta-objective during deployment. Here, we isolate continual learning as our
meta-objective, and we show that a suitable neuronal phenotype can endow a
single network with an innate ability to update its synapses with minimal
forgetting, using standard backpropagation, without experience replay, nor
separate wake/sleep phases. We demonstrate this ability on sequential
non-linear regression tasks.
</p>
<a href="http://arxiv.org/abs/2011.07035" target="_blank">arXiv:2011.07035</a> [<a href="http://arxiv.org/pdf/2011.07035" target="_blank">pdf</a>]

<h2>Tactile SLAM: Real-time inference of shape and pose from planar pushing. (arXiv:2011.07044v1 [cs.RO])</h2>
<h3>Sudharshan Suresh, Maria Bauza, Kuan-Ting Yu, Joshua G. Mangelson, Alberto Rodriguez, Michael Kaess</h3>
<p>Tactile perception is central to robot manipulation in unstructured
environments. However, it requires contact, and a mature implementation must
infer object models while also accounting for the motion induced by the
interaction. In this work, we present a method to estimate both object shape
and pose in real-time from a stream of tactile measurements. This is applied
towards tactile exploration of an unknown object by planar pushing. We consider
this as an online SLAM problem with a nonparametric shape representation. Our
formulation of tactile inference alternates between Gaussian process implicit
surface regression and pose estimation on a factor graph. Through a combination
of local Gaussian processes and fixed-lag smoothing, we infer object shape and
pose in real-time. We evaluate our system across different objects in both
simulated and real-world planar pushing tasks.
</p>
<a href="http://arxiv.org/abs/2011.07044" target="_blank">arXiv:2011.07044</a> [<a href="http://arxiv.org/pdf/2011.07044" target="_blank">pdf</a>]

<h2>Using Graph Neural Networks to Reconstruct Ancient Documents. (arXiv:2011.07048v1 [cs.CV])</h2>
<h3>Cecilia Ostertag, Marie Beurton-Aimar</h3>
<p>In recent years, machine learning and deep learning approaches such as
artificial neural networks have gained in popularity for the resolution of
automatic puzzle resolution problems. Indeed, these methods are able to extract
high-level representations from images, and then can be trained to separate
matching image pieces from non-matching ones. These applications have many
similarities to the problem of ancient document reconstruction from partially
recovered fragments. In this work we present a solution based on a Graph Neural
Network, using pairwise patch information to assign labels to edges
representing the spatial relationships between pairs. This network classifies
the relationship between a source and a target patch as being one of Up, Down,
Left, Right or None. By doing so for all edges, our model outputs a new graph
representing a reconstruction proposal. Finally, we show that our model is not
only able to provide correct classifications at the edge-level, but also to
generate partial or full reconstruction graphs from a set of patches.
</p>
<a href="http://arxiv.org/abs/2011.07048" target="_blank">arXiv:2011.07048</a> [<a href="http://arxiv.org/pdf/2011.07048" target="_blank">pdf</a>]

<h2>Learning to Drop: Robust Graph Neural Network via Topological Denoising. (arXiv:2011.07057v1 [cs.LG])</h2>
<h3>Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, Xiang Zhang</h3>
<p>Graph Neural Networks (GNNs) have shown to be powerful tools for graph
analytics. The key idea is to recursively propagate and aggregate information
along edges of the given graph. Despite their success, however, the existing
GNNs are usually sensitive to the quality of the input graph. Real-world graphs
are often noisy and contain task-irrelevant edges, which may lead to suboptimal
generalization performance in the learned GNN models. In this paper, we propose
PTDNet, a parameterized topological denoising network, to improve the
robustness and generalization performance of GNNs by learning to drop
task-irrelevant edges. PTDNet prunes task-irrelevant edges by penalizing the
number of edges in the sparsified graph with parameterized networks. To take
into consideration of the topology of the entire graph, the nuclear norm
regularization is applied to impose the low-rank constraint on the resulting
sparsified graph for better generalization. PTDNet can be used as a key
component in GNN models to improve their performances on various tasks, such as
node classification and link prediction. Experimental studies on both synthetic
and benchmark datasets show that PTDNet can improve the performance of GNNs
significantly and the performance gain becomes larger for more noisy datasets.
</p>
<a href="http://arxiv.org/abs/2011.07057" target="_blank">arXiv:2011.07057</a> [<a href="http://arxiv.org/pdf/2011.07057" target="_blank">pdf</a>]

<h2>Block Stability for MAP Inference. (arXiv:1810.05305v2 [stat.ML] UPDATED)</h2>
<h3>Hunter Lang, David Sontag, Aravindan Vijayaraghavan</h3>
<p>To understand the empirical success of approximate MAP inference, recent work
(Lang et al., 2018) has shown that some popular approximation algorithms
perform very well when the input instance is stable. The simplest stability
condition assumes that the MAP solution does not change at all when some of the
pairwise potentials are (adversarially) perturbed. Unfortunately, this strong
condition does not seem to be satisfied in practice. In this paper, we
introduce a significantly more relaxed condition that only requires blocks
(portions) of an input instance to be stable. Under this block stability
condition, we prove that the pairwise LP relaxation is persistent on the stable
blocks. We complement our theoretical results with an empirical evaluation of
real-world MAP inference instances from computer vision. We design an algorithm
to find stable blocks, and find that these real instances have large stable
regions. Our work gives a theoretical explanation for the widespread empirical
phenomenon of persistency for this LP relaxation.
</p>
<a href="http://arxiv.org/abs/1810.05305" target="_blank">arXiv:1810.05305</a> [<a href="http://arxiv.org/pdf/1810.05305" target="_blank">pdf</a>]

<h2>Rank consistent ordinal regression for neural networks with application to age estimation. (arXiv:1901.07884v7 [cs.LG] UPDATED)</h2>
<h3>Wenzhi Cao, Vahid Mirjalili, Sebastian Raschka</h3>
<p>In many real-world prediction tasks, class labels include information about
the relative ordering between labels, which is not captured by commonly-used
loss functions such as multi-category cross-entropy. Recently, the deep
learning community adopted ordinal regression frameworks to take such ordering
information into account. Neural networks were equipped with ordinal regression
capabilities by transforming ordinal targets into binary classification
subtasks. However, this method suffers from inconsistencies among the different
binary classifiers. To resolve these inconsistencies, we propose the COnsistent
RAnk Logits (CORAL) framework with strong theoretical guarantees for
rank-monotonicity and consistent confidence scores. Moreover, the proposed
method is architecture-agnostic and can extend arbitrary state-of-the-art deep
neural network classifiers for ordinal regression tasks. The empirical
evaluation of the proposed rank-consistent method on a range of face-image
datasets for age prediction shows a substantial reduction of the prediction
error compared to the reference ordinal regression network.
</p>
<a href="http://arxiv.org/abs/1901.07884" target="_blank">arXiv:1901.07884</a> [<a href="http://arxiv.org/pdf/1901.07884" target="_blank">pdf</a>]

<h2>Learning Nonsymmetric Determinantal Point Processes. (arXiv:1905.12962v3 [cs.LG] UPDATED)</h2>
<h3>Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, Syrine Krichene</h3>
<p>Determinantal point processes (DPPs) have attracted substantial attention as
an elegant probabilistic model that captures the balance between quality and
diversity within sets. DPPs are conventionally parameterized by a positive
semi-definite kernel matrix, and this symmetric kernel encodes only repulsive
interactions between items. These so-called symmetric DPPs have significant
expressive power, and have been successfully applied to a variety of machine
learning tasks, including recommendation systems, information retrieval, and
automatic summarization, among many others. Efficient algorithms for learning
symmetric DPPs and sampling from these models have been reasonably well
studied. However, relatively little attention has been given to nonsymmetric
DPPs, which relax the symmetric constraint on the kernel. Nonsymmetric DPPs
allow for both repulsive and attractive item interactions, which can
significantly improve modeling power, resulting in a model that may better fit
for some applications. We present a method that enables a tractable algorithm,
based on maximum likelihood estimation, for learning nonsymmetric DPPs from
data composed of observed subsets. Our method imposes a particular
decomposition of the nonsymmetric kernel that enables such tractable learning
algorithms, which we analyze both theoretically and experimentally. We evaluate
our model on synthetic and real-world datasets, demonstrating improved
predictive performance compared to symmetric DPPs, which have previously shown
strong performance on modeling tasks associated with these datasets.
</p>
<a href="http://arxiv.org/abs/1905.12962" target="_blank">arXiv:1905.12962</a> [<a href="http://arxiv.org/pdf/1905.12962" target="_blank">pdf</a>]

<h2>The Intrinsic Robustness of Stochastic Bandits to Strategic Manipulation. (arXiv:1906.01528v2 [cs.LG] UPDATED)</h2>
<h3>Zhe Feng, David C. Parkes, Haifeng Xu</h3>
<p>Motivated by economic applications such as recommender systems, we study the
behavior of stochastic bandits algorithms under \emph{strategic behavior}
conducted by rational actors, i.e., the arms. Each arm is a
\emph{self-interested} strategic player who can modify its own reward whenever
pulled, subject to a cross-period budget constraint, in order to maximize its
own expected number of times of being pulled. We analyze the robustness of
three popular bandit algorithms: UCB, $\varepsilon$-Greedy, and Thompson
Sampling. We prove that all three algorithms achieve a regret upper bound
$\mathcal{O}(\max \{ B, K\ln T\})$ where $B$ is the total budget across arms,
$K$ is the total number of arms and $T$ is length of the time horizon. This
regret guarantee holds under \emph{arbitrary adaptive} manipulation strategy of
arms. Our second set of main results shows that this regret bound is
\emph{tight} -- in fact for UCB it is tight even when we restrict the arms'
manipulation strategies to form a \emph{Nash equilibrium}. The lower bound
makes use of a simple manipulation strategy, the same for all three algorithms,
yielding a bound of $\Omega(\max \{ B, K\ln T\})$. Our results illustrate the
robustness of classic bandits algorithms against strategic manipulations as
long as $B=o(T)$.
</p>
<a href="http://arxiv.org/abs/1906.01528" target="_blank">arXiv:1906.01528</a> [<a href="http://arxiv.org/pdf/1906.01528" target="_blank">pdf</a>]

<h2>Heterogeneous Deep Graph Infomax. (arXiv:1911.08538v5 [cs.LG] UPDATED)</h2>
<h3>Yuxiang Ren, Bo Liu, Chao Huang, Peng Dai, Liefeng Bo, Jiawei Zhang</h3>
<p>Graph representation learning is to learn universal node representations that
preserve both node attributes and structural information. The derived node
representations can be used to serve various downstream tasks, such as node
classification and node clustering. When a graph is heterogeneous, the problem
becomes more challenging than the homogeneous graph node learning problem.
Inspired by the emerging information theoretic-based learning algorithm, in
this paper we propose an unsupervised graph neural network Heterogeneous Deep
Graph Infomax (HDGI) for heterogeneous graph representation learning. We use
the meta-path structure to analyze the connections involving semantics in
heterogeneous graphs and utilize graph convolution module and semantic-level
attention mechanism to capture local representations. By maximizing
local-global mutual information, HDGI effectively learns high-level node
representations that can be utilized in downstream graph-related tasks.
Experiment results show that HDGI remarkably outperforms state-of-the-art
unsupervised graph representation learning methods on both classification and
clustering tasks. By feeding the learned representations into a parametric
model, such as logistic regression, we even achieve comparable performance in
node classification tasks when comparing with state-of-the-art supervised
end-to-end GNN models.
</p>
<a href="http://arxiv.org/abs/1911.08538" target="_blank">arXiv:1911.08538</a> [<a href="http://arxiv.org/pdf/1911.08538" target="_blank">pdf</a>]

<h2>Matrix Completion using Kronecker Product Approximation. (arXiv:1911.11774v3 [stat.ML] UPDATED)</h2>
<h3>Chencheng Cai, Rong Chen, Han Xiao</h3>
<p>A matrix completion problem is to recover the missing entries in a partially
observed matrix. Most of the existing matrix completion methods assume a low
rank structure of the underlying complete matrix. In this paper, we introduce
an alternative and more general form of the underlying complete matrix, which
assumes a low Kronecker rank instead of a low regular rank, but includes the
latter as a special case. The extra flexibility allows for a much more
parsimonious representation of the underlying matrix, but also raises the
challenge of determining the proper Kronecker product configuration to be used.
We find that the configuration can be identified using the mean squared error
criterion as well as a modified cross-validation criterion. We establish the
consistency of this procedure under suitable conditions on the signal-to-noise
ratio. A aggregation procedure is also proposed to deal with special missing
patterns and complex underlying structures. Both numerical and empirical
studies are carried out to demonstrate the performance of the new method.
</p>
<a href="http://arxiv.org/abs/1911.11774" target="_blank">arXiv:1911.11774</a> [<a href="http://arxiv.org/pdf/1911.11774" target="_blank">pdf</a>]

<h2>A Survey on String Constraint Solving. (arXiv:2002.02376v5 [cs.AI] UPDATED)</h2>
<h3>Roberto Amadini</h3>
<p>String constraint solving refers to solving combinatorial problems involving
constraints over string variables. String solving approaches have become
popular over the last years given the massive use of strings in different
application domains like formal analysis, automated testing, database query
processing, and cybersecurity. This paper reports a comprehensive survey on
string constraint solving by exploring the large number of approaches that have
been proposed over the last decades to solve string constraints.
</p>
<a href="http://arxiv.org/abs/2002.02376" target="_blank">arXiv:2002.02376</a> [<a href="http://arxiv.org/pdf/2002.02376" target="_blank">pdf</a>]

<h2>Compositional Embeddings for Multi-Label One-Shot Learning. (arXiv:2002.04193v5 [cs.LG] UPDATED)</h2>
<h3>Zeqian Li, Michael C. Mozer, Jacob Whitehill</h3>
<p>We present a compositional embedding framework that infers not just a single
class per input image, but a set of classes, in the setting of one-shot
learning. Specifically, we propose and evaluate several novel models consisting
of (1) an embedding function f trained jointly with a "composition" function g
that computes set union operations between the classes encoded in two embedding
vectors; and (2) embedding f trained jointly with a "query" function h that
computes whether the classes encoded in one embedding subsume the classes
encoded in another embedding. In contrast to prior work, these models must both
perceive the classes associated with the input examples and encode the
relationships between different class label sets, and they are trained using
only weak one-shot supervision consisting of the label-set relationships among
training examples. Experiments on the OmniGlot, Open Images, and COCO datasets
show that the proposed compositional embedding models outperform existing
embedding methods. Our compositional embedding models have applications to
multi-label object recognition for both one-shot and supervised learning.
</p>
<a href="http://arxiv.org/abs/2002.04193" target="_blank">arXiv:2002.04193</a> [<a href="http://arxiv.org/pdf/2002.04193" target="_blank">pdf</a>]

<h2>Amplifying The Uncanny. (arXiv:2002.06890v3 [cs.CV] UPDATED)</h2>
<h3>Terence Broad, Frederic Fol Leymarie, Mick Grierson</h3>
<p>Deep neural networks have become remarkably good at producing realistic
deepfakes, images of people that (to the untrained eye) are indistinguishable
from real images. Deepfakes are produced by algorithms that learn to
distinguish between real and fake images and are optimised to generate samples
that the system deems realistic. This paper, and the resulting series of
artworks Being Foiled explore the aesthetic outcome of inverting this process,
instead optimising the system to generate images that it predicts as being
fake. This maximises the unlikelihood of the data and in turn, amplifies the
uncanny nature of these machine hallucinations.
</p>
<a href="http://arxiv.org/abs/2002.06890" target="_blank">arXiv:2002.06890</a> [<a href="http://arxiv.org/pdf/2002.06890" target="_blank">pdf</a>]

<h2>Sparse Graphical Memory for Robust Planning. (arXiv:2003.06417v3 [cs.LG] UPDATED)</h2>
<h3>Scott Emmons, Ajay Jain, Michael Laskin, Thanard Kurutach, Pieter Abbeel, Deepak Pathak</h3>
<p>To operate effectively in the real world, agents should be able to act from
high-dimensional raw sensory input such as images and achieve diverse goals
across long time-horizons. Current deep reinforcement and imitation learning
methods can learn directly from high-dimensional inputs but do not scale well
to long-horizon tasks. In contrast, classical graphical methods like A* search
are able to solve long-horizon tasks, but assume that the state space is
abstracted away from raw sensory input. Recent works have attempted to combine
the strengths of deep learning and classical planning; however, dominant
methods in this domain are still quite brittle and scale poorly with the size
of the environment. We introduce Sparse Graphical Memory (SGM), a new data
structure that stores states and feasible transitions in a sparse memory. SGM
aggregates states according to a novel two-way consistency objective, adapting
classic state aggregation criteria to goal-conditioned RL: two states are
redundant when they are interchangeable both as goals and as starting states.
Theoretically, we prove that merging nodes according to two-way consistency
leads to an increase in shortest path lengths that scales only linearly with
the merging threshold. Experimentally, we show that SGM significantly
outperforms current state of the art methods on long horizon, sparse-reward
visual navigation tasks. Project video and code are available at
https://mishalaskin.github.io/sgm/
</p>
<a href="http://arxiv.org/abs/2003.06417" target="_blank">arXiv:2003.06417</a> [<a href="http://arxiv.org/pdf/2003.06417" target="_blank">pdf</a>]

<h2>A Simple Probabilistic Method for Deep Classification under Input-Dependent Label Noise. (arXiv:2003.06778v3 [cs.LG] UPDATED)</h2>
<h3>Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, Jesse Berent</h3>
<p>Datasets with noisy labels are a common occurrence in practical applications
of classification methods. We propose a simple probabilistic method for
training deep classifiers under input-dependent (heteroscedastic) label noise.
We assume an underlying heteroscedastic generative process for noisy labels. To
make gradient based training feasible we use a temperature parameterized
softmax as a smooth approximation to the assumed generative process. We
illustrate that the softmax temperature controls a bias-variance trade-off for
the approximation. By tuning the softmax temperature, we improve accuracy,
log-likelihood and calibration on both image classification benchmarks with
controlled label noise as well as Imagenet-21k which has naturally occurring
label noise. For image segmentation, our method increases the mean IoU on the
PASCAL VOC and Cityscapes datasets by more than 1% over the state-of-the-art
model.
</p>
<a href="http://arxiv.org/abs/2003.06778" target="_blank">arXiv:2003.06778</a> [<a href="http://arxiv.org/pdf/2003.06778" target="_blank">pdf</a>]

<h2>A Low-cost Fault Corrector for Deep Neural Networks through Range Restriction. (arXiv:2003.13874v3 [cs.LG] UPDATED)</h2>
<h3>Zitao Chen, Guanpeng Li, Karthik Pattabiraman</h3>
<p>With the increasing adoption of deep neural networks (DNNs) in
safety-critical domains such as autonomous vehicles and industrial robotics,
their safety and reliability is also becoming critical. On the other hand,
hardware transient faults are growing in frequency due to the progressive
technology scaling and can lead to failures in DNNs. Existing error-resilience
techniques for DNNs often suffer from expensive re-computation, or requires
significant implementation effort.

In this work, we propose Ranger, a novel, low-cost fault correction
technique, which directly rectifies the faulty output due to transient faults
without any re-computation. We leverage the insight that DNNs are inherently
resilient to benign faults (whose occurrence will not corrupt the program
output), and introduce a technique to transform the critical faults (which can
result in erroneous output) into benign faults. Ranger is an automated
transformation applied to restrict the ranges of values in particular DNN
layers, which can dampen the large deviations typically caused by critical
faults to smaller ones (the reduced deviation can be tolerated by DNNs). Our
evaluation on 8 DNNs (including two used in autonomous vehicles applications)
demonstrates that Ranger is highly effective in rectifying the faulty outputs
due to transient faults with negligible overheads.
</p>
<a href="http://arxiv.org/abs/2003.13874" target="_blank">arXiv:2003.13874</a> [<a href="http://arxiv.org/pdf/2003.13874" target="_blank">pdf</a>]

<h2>Flows for simultaneous manifold learning and density estimation. (arXiv:2003.13913v3 [stat.ML] UPDATED)</h2>
<h3>Johann Brehmer, Kyle Cranmer</h3>
<p>We introduce manifold-learning flows (M-flows), a new class of generative
models that simultaneously learn the data manifold as well as a tractable
probability density on that manifold. Combining aspects of normalizing flows,
GANs, autoencoders, and energy-based models, they have the potential to
represent datasets with a manifold structure more faithfully and provide
handles on dimensionality reduction, denoising, and out-of-distribution
detection. We argue why such models should not be trained by maximum likelihood
alone and present a new training algorithm that separates manifold and density
updates. In a range of experiments we demonstrate how M-flows learn the data
manifold and allow for better inference than standard flows in the ambient data
space.
</p>
<a href="http://arxiv.org/abs/2003.13913" target="_blank">arXiv:2003.13913</a> [<a href="http://arxiv.org/pdf/2003.13913" target="_blank">pdf</a>]

<h2>A rigorous method to compare interpretability. (arXiv:2004.01570v3 [stat.ML] UPDATED)</h2>
<h3>Vincent Margot</h3>
<p>Interpretability is becoming increasingly important in predictive model
analysis. Unfortunately, as mentioned by many authors, there is still no
consensus on that idea. The aim of this article is to propose a rigorous
mathematical definition of the concept of interpretability, allowing fair
comparisons between any rule-based algorithms. This definition is built from
three notions, each of which being quantitatively measured by a simple formula:
predictivity, stability and simplicity. While predictivity has been widely
studied to measure the accuracy of predictive algorithms, stability is based on
the Dice-Sorensen index to compare two sets of rules generated by an algorithm
using two independent samples. Simplicity is based on the sum of the length of
the rules deriving from the generated model. The final objective measure of the
interpretability of any rule-based algorithm ends up as a weighted sum of the
three aforementioned concepts. This paper concludes with the comparison of the
interpretability between four rule-based algorithms.
</p>
<a href="http://arxiv.org/abs/2004.01570" target="_blank">arXiv:2004.01570</a> [<a href="http://arxiv.org/pdf/2004.01570" target="_blank">pdf</a>]

<h2>Supervised Contrastive Learning. (arXiv:2004.11362v3 [cs.LG] UPDATED)</h2>
<h3>Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan</h3>
<p>Contrastive learning applied to self-supervised representation learning has
seen a resurgence in recent years, leading to state of the art performance in
the unsupervised training of deep image models. Modern batch contrastive
approaches subsume or significantly outperform traditional contrastive losses
such as triplet, max-margin and the N-pairs loss. In this work, we extend the
self-supervised batch contrastive approach to the fully-supervised setting,
allowing us to effectively leverage label information. Clusters of points
belonging to the same class are pulled together in embedding space, while
simultaneously pushing apart clusters of samples from different classes. We
analyze two possible versions of the supervised contrastive (SupCon) loss,
identifying the best-performing formulation of the loss. On ResNet-200, we
achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above
the best number reported for this architecture. We show consistent
outperformance over cross-entropy on other datasets and two ResNet variants.
The loss shows benefits for robustness to natural corruptions and is more
stable to hyperparameter settings such as optimizers and data augmentations.
Our loss function is simple to implement, and reference TensorFlow code is
released at https://t.ly/supcon.
</p>
<a href="http://arxiv.org/abs/2004.11362" target="_blank">arXiv:2004.11362</a> [<a href="http://arxiv.org/pdf/2004.11362" target="_blank">pdf</a>]

<h2>Object-Centric Representation and Rendering of 3D Scenes. (arXiv:2006.06130v2 [cs.LG] UPDATED)</h2>
<h3>Chang Chen, Fei Deng, Sungjin Ahn</h3>
<p>A crucial ability of human intelligence is to build up models of individual
3D objects from partial scene observations. Recent works achieve object-centric
generation but without the ability to infer the representation, or achieve 3D
scene representation learning but without object-centric compositionality.
Therefore, learning to represent and render 3D scenes with object-centric
compositionality remains elusive. In this paper, we propose a probabilistic
generative model for learning to build modular and compositional 3D object
models from partial observations of a multi-object scene. The proposed model
can (i) infer the 3D object representations by learning to search and group
object areas and also (ii) render from an arbitrary viewpoint not only
individual objects but also the full scene by compositing the objects. The
entire learning process is unsupervised and end-to-end. In experiments, in
addition to generation quality, we also demonstrate that the learned
representation permits object-wise manipulation and novel scene generation, and
generalizes to various settings. Results can be found on our project website:
https://sites.google.com/view/roots3d
</p>
<a href="http://arxiv.org/abs/2006.06130" target="_blank">arXiv:2006.06130</a> [<a href="http://arxiv.org/pdf/2006.06130" target="_blank">pdf</a>]

<h2>Stochastic Optimization for Performative Prediction. (arXiv:2006.06887v3 [cs.LG] UPDATED)</h2>
<h3>Celestine Mendler-D&#xfc;nner, Juan C. Perdomo, Tijana Zrnic, Moritz Hardt</h3>
<p>In performative prediction, the choice of a model influences the distribution
of future data, typically through actions taken based on the model's
predictions.

We initiate the study of stochastic optimization for performative prediction.
What sets this setting apart from traditional stochastic optimization is the
difference between merely updating model parameters and deploying the new
model. The latter triggers a shift in the distribution that affects future
data, while the former keeps the distribution as is.

Assuming smoothness and strong convexity, we prove rates of convergence for
both greedily deploying models after each stochastic update (greedy deploy) as
well as for taking several updates before redeploying (lazy deploy). In both
cases, our bounds smoothly recover the optimal $O(1/k)$ rate as the strength of
performativity decreases. Furthermore, they illustrate how depending on the
strength of performative effects, there exists a regime where either approach
outperforms the other. We experimentally explore the trade-off on both
synthetic data and a strategic classification simulator.
</p>
<a href="http://arxiv.org/abs/2006.06887" target="_blank">arXiv:2006.06887</a> [<a href="http://arxiv.org/pdf/2006.06887" target="_blank">pdf</a>]

<h2>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization. (arXiv:2006.13258v4 [cs.LG] UPDATED)</h2>
<h3>Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai</h3>
<p>Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.
</p>
<a href="http://arxiv.org/abs/2006.13258" target="_blank">arXiv:2006.13258</a> [<a href="http://arxiv.org/pdf/2006.13258" target="_blank">pdf</a>]

<h2>Predicting Length of Stay in the Intensive Care Unit with Temporal Pointwise Convolutional Networks. (arXiv:2006.16109v2 [cs.LG] UPDATED)</h2>
<h3>Emma Rocheteau, Pietro Li&#xf2;, Stephanie Hyland</h3>
<p>The pressure of ever-increasing patient demand and budget restrictions make
hospital bed management a daily challenge for clinical staff. Most critical is
the efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to
the patients who need life support. Central to solving this problem is knowing
for how long the current set of ICU patients are likely to stay in the unit. In
this work, we propose a new deep learning model based on the combination of
temporal convolution and pointwise (1x1) convolution, to solve the length of
stay prediction task on the eICU critical care dataset. The model - which we
refer to as Temporal Pointwise Convolution (TPC) - is specifically designed to
mitigate for common challenges with Electronic Health Records, such as
skewness, irregular sampling and missing data. In doing so, we have achieved
significant performance benefits of 18-51% (metric dependent) over the commonly
used Long-Short Term Memory (LSTM) network, and the multi-head self-attention
network known as the Transformer.
</p>
<a href="http://arxiv.org/abs/2006.16109" target="_blank">arXiv:2006.16109</a> [<a href="http://arxiv.org/pdf/2006.16109" target="_blank">pdf</a>]

<h2>Object Files and Schemata: Factorizing Declarative and Procedural Knowledge in Dynamical Systems. (arXiv:2006.16225v5 [cs.LG] UPDATED)</h2>
<h3>Anirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Sergey Levine, Charles Blundell, Yoshua Bengio, Michael Mozer</h3>
<p>Modeling a structured, dynamic environment like a video game requires keeping
track of the objects and their states declarative knowledge) as well as
predicting how objects behave (procedural knowledge). Black-box models with a
monolithic hidden state often fail to apply procedural knowledge consistently
and uniformly, i.e., they lack systematicity. For example, in a video game,
correct prediction of one enemy's trajectory does not ensure correct prediction
of another's. We address this issue via an architecture that factorizes
declarative and procedural knowledge and that imposes modularity within each
form of knowledge. The architecture consists of active modules called object
files that maintain the state of a single object and invoke passive external
knowledge sources called schemata that prescribe state updates. To use a video
game as an illustration, two enemies of the same type will share schemata but
will have separate object files to encode their distinct state (e.g., health,
position). We propose to use attention to determine which object files to
update, the selection of schemata, and the propagation of information between
object files. The resulting architecture is a drop-in replacement conforming to
the same input-output interface as normal recurrent networks (e.g., LSTM, GRU)
yet achieves substantially better generalization on environments that have
multiple object tokens of the same type, including a challenging intuitive
physics benchmark.
</p>
<a href="http://arxiv.org/abs/2006.16225" target="_blank">arXiv:2006.16225</a> [<a href="http://arxiv.org/pdf/2006.16225" target="_blank">pdf</a>]

<h2>Noticing Motion Patterns: Temporal CNN with a Novel Convolution Operator for Human Trajectory Prediction. (arXiv:2007.00862v2 [cs.CV] UPDATED)</h2>
<h3>Dapeng Zhao, Jean Oh</h3>
<p>We propose a Convolutional Neural Network-based approach to learn, detect,and
extract patterns in sequential trajectory data, known here as Social Pattern
Extraction Convolution (Social-PEC). A set of experiments carried out on the
human trajectory prediction problem shows that our model performs comparably to
the state of the art and outperforms in some cases. More importantly,the
proposed approach unveils the obscurity in the previous use of pooling layer,
presenting a way to intuitively explain the decision-making process.
</p>
<a href="http://arxiv.org/abs/2007.00862" target="_blank">arXiv:2007.00862</a> [<a href="http://arxiv.org/pdf/2007.00862" target="_blank">pdf</a>]

<h2>Certifiably Adversarially Robust Detection of Out-of-Distribution Data. (arXiv:2007.08473v2 [cs.LG] UPDATED)</h2>
<h3>Julian Bitterwolf, Alexander Meinke, Matthias Hein</h3>
<p>Deep neural networks are known to be overconfident when applied to
out-of-distribution (OOD) inputs which clearly do not belong to any class. This
is a problem in safety-critical applications since a reliable assessment of the
uncertainty of a classifier is a key property, allowing the system to trigger
human intervention or to transfer into a safe state. In this paper, we aim for
certifiable worst case guarantees for OOD detection by enforcing not only low
confidence at the OOD point but also in an $l_\infty$-ball around it. For this
purpose, we use interval bound propagation (IBP) to upper bound the maximal
confidence in the $l_\infty$-ball and minimize this upper bound during training
time. We show that non-trivial bounds on the confidence for OOD data
generalizing beyond the OOD dataset seen at training time are possible.
Moreover, in contrast to certified adversarial robustness which typically comes
with significant loss in prediction performance, certified guarantees for worst
case OOD detection are possible without much loss in accuracy.
</p>
<a href="http://arxiv.org/abs/2007.08473" target="_blank">arXiv:2007.08473</a> [<a href="http://arxiv.org/pdf/2007.08473" target="_blank">pdf</a>]

<h2>FaceHop: A Light-Weight Low-Resolution Face Gender Classification Method. (arXiv:2007.09510v3 [cs.CV] UPDATED)</h2>
<h3>Mozhdeh Rouhsedaghat, Yifan Wang, Xiou Ge, Shuowen Hu, Suya You, C.-C. Jay Kuo</h3>
<p>A light-weight low-resolution face gender classification method, called
FaceHop, is proposed in this research. We have witnessed rapid progress in face
gender classification accuracy due to the adoption of deep learning (DL)
technology. Yet, DL-based systems are not suitable for resource-constrained
environments with limited networking and computing. FaceHop offers an
interpretable non-parametric machine learning solution. It has desired
characteristics such as a small model size, a small training data amount, low
training complexity, and low-resolution input images. FaceHop is developed with
the successive subspace learning (SSL) principle and built upon the foundation
of PixelHop++. The effectiveness of the FaceHop method is demonstrated by
experiments. For gray-scale face images of resolution $32 \times 32$ in the LFW
and the CMU Multi-PIE datasets, FaceHop achieves correct gender classification
rates of 94.63% and 95.12% with model sizes of 16.9K and 17.6K parameters,
respectively. It outperforms LeNet-5 in classification accuracy while LeNet-5
has a model size of 75.8K parameters.
</p>
<a href="http://arxiv.org/abs/2007.09510" target="_blank">arXiv:2007.09510</a> [<a href="http://arxiv.org/pdf/2007.09510" target="_blank">pdf</a>]

<h2>Robot Action Selection Learning via Layered Dimension Informed Program Synthesis. (arXiv:2008.04133v2 [cs.AI] UPDATED)</h2>
<h3>Jarrett Holtz, Arjun Guha, Joydeep Biswas</h3>
<p>Action selection policies (ASPs), used to compose low-level robot skills into
complex high-level tasks are commonly represented as neural networks (NNs) in
the state of the art. Such a paradigm, while very effective, suffers from a few
key problems: 1) NNs are opaque to the user and hence not amenable to
verification, 2) they require significant amounts of training data, and 3) they
are hard to repair when the domain changes. We present two key insights about
ASPs for robotics. First, ASPs need to reason about physically meaningful
quantities derived from the state of the world, and second, there exists a
layered structure for composing these policies. Leveraging these insights, we
introduce layered dimension-informed program synthesis (LDIPS) - by reasoning
about the physical dimensions of state variables, and dimensional constraints
on operators, LDIPS directly synthesizes ASPs in a human-interpretable
domain-specific language that is amenable to program repair. We present
empirical results to demonstrate that LDIPS 1) can synthesize effective ASPs
for robot soccer and autonomous driving domains, 2) requires two orders of
magnitude fewer training examples than a comparable NN representation, and 3)
can repair the synthesized ASPs with only a small number of corrections when
transferring from simulation to real robots.
</p>
<a href="http://arxiv.org/abs/2008.04133" target="_blank">arXiv:2008.04133</a> [<a href="http://arxiv.org/pdf/2008.04133" target="_blank">pdf</a>]

<h2>Derived metrics for the game of Go -- intrinsic network strength assessment and cheat-detection. (arXiv:2009.01606v3 [cs.AI] UPDATED)</h2>
<h3>Attila Egri-Nagy, Antti T&#xf6;rm&#xe4;nen</h3>
<p>The widespread availability of superhuman AI engines is changing how we play
the ancient game of Go. The open-source software packages developed after the
AlphaGo series shifted focus from producing strong playing entities to
providing tools for analyzing games. Here we describe two ways of how the
innovations of the second generation engines (e.g.~score estimates, variable
komi) can be used for defining new metrics that help deepen our understanding
of the game. First, we study how much information the search component
contributes in addition to the raw neural network policy output. This gives an
intrinsic strength measurement for the neural network. Second, we define the
effect of a move by the difference in score estimates. This gives a
fine-grained, move-by-move performance evaluation of a player. We use this in
combating the new challenge of detecting online cheating.
</p>
<a href="http://arxiv.org/abs/2009.01606" target="_blank">arXiv:2009.01606</a> [<a href="http://arxiv.org/pdf/2009.01606" target="_blank">pdf</a>]

<h2>Deep multi-stations weather forecasting: explainable recurrent convolutional neural networks. (arXiv:2009.11239v5 [cs.LG] UPDATED)</h2>
<h3>Ismail Alaoui Abdellaoui, Siamak Mehrkanoon</h3>
<p>Deep learning applied to weather forecasting has started gaining popularity
because of the progress achieved by data-driven models. The present paper
compares four different deep learning architectures to perform weather
prediction on daily data gathered from 18 cities across Europe and spanned over
a period of 15 years. The four proposed models investigate the different type
of input representations (i.e. tensorial unistream vs. multi-stream matrices)
as well as the combination of convolutional neural networks and LSTM (i.e.
cascaded vs. ConvLSTM). In particular, we show that a model that uses a
multi-stream input representation and that processes each lag individually
combined with a cascaded convolution and LSTM is capable of better forecasting
than the other compared models. In addition, we show that visualization
techniques such as occlusion analysis and score maximization can give an
additional insight on the most important features and cities for predicting a
particular target feature and city.
</p>
<a href="http://arxiv.org/abs/2009.11239" target="_blank">arXiv:2009.11239</a> [<a href="http://arxiv.org/pdf/2009.11239" target="_blank">pdf</a>]

<h2>LiRaNet: End-to-End Trajectory Prediction using Spatio-Temporal Radar Fusion. (arXiv:2010.00731v3 [cs.CV] UPDATED)</h2>
<h3>Meet Shah, Zhiling Huang, Ankit Laddha, Matthew Langford, Blake Barber, Sidney Zhang, Carlos Vallespi-Gonzalez, Raquel Urtasun</h3>
<p>In this paper, we present LiRaNet, a novel end-to-end trajectory prediction
method which utilizes radar sensor information along with widely used lidar and
high definition (HD) maps. Automotive radar provides rich, complementary
information, allowing for longer range vehicle detection as well as
instantaneous radial velocity measurements. However, there are factors that
make the fusion of lidar and radar information challenging, such as the
relatively low angular resolution of radar measurements, their sparsity and the
lack of exact time synchronization with lidar. To overcome these challenges, we
propose an efficient spatio-temporal radar feature extraction scheme which
achieves state-of-the-art performance on multiple large-scale datasets.Further,
by incorporating radar information, we show a 52% reduction in prediction error
for objects with high acceleration and a 16% reduction in prediction error for
objects at longer range.
</p>
<a href="http://arxiv.org/abs/2010.00731" target="_blank">arXiv:2010.00731</a> [<a href="http://arxiv.org/pdf/2010.00731" target="_blank">pdf</a>]

<h2>Online Knowledge Distillation via Multi-branch Diversity Enhancement. (arXiv:2010.00795v3 [cs.CV] UPDATED)</h2>
<h3>Zheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan</h3>
<p>Knowledge distillation is an effective method to transfer the knowledge from
the cumbersome teacher model to the lightweight student model. Online knowledge
distillation uses the ensembled prediction results of multiple student models
as soft targets to train each student model. However, the homogenization
problem will lead to difficulty in further improving model performance. In this
work, we propose a new distillation method to enhance the diversity among
multiple student models. We introduce Feature Fusion Module (FFM), which
improves the performance of the attention mechanism in the network by
integrating rich semantic information contained in the last block of multiple
student models. Furthermore, we use the Classifier Diversification(CD) loss
function to strengthen the differences between the student models and deliver a
better ensemble result. Extensive experiments proved that our method
significantly enhances the diversity among student models and brings better
distillation performance. We evaluate our method on three image classification
datasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves
state-of-the-art performance on these datasets.
</p>
<a href="http://arxiv.org/abs/2010.00795" target="_blank">arXiv:2010.00795</a> [<a href="http://arxiv.org/pdf/2010.00795" target="_blank">pdf</a>]

<h2>Disentangle-based Continual Graph Representation Learning. (arXiv:2010.02565v2 [cs.LG] UPDATED)</h2>
<h3>Xiaoyu Kou, Yankai Lin, Shaobo Liu, Peng Li, Jie Zhou, Yan Zhang</h3>
<p>Graph embedding (GE) methods embed nodes (and/or edges) in graph into a
low-dimensional semantic space, and have shown its effectiveness in modeling
multi-relational data. However, existing GE models are not practical in
real-world applications since it overlooked the streaming nature of incoming
data. To address this issue, we study the problem of continual graph
representation learning which aims to continually train a GE model on new data
to learn incessantly emerging multi-relational data while avoiding
catastrophically forgetting old learned knowledge. Moreover, we propose a
disentangle-based continual graph representation learning (DiCGRL) framework
inspired by the human's ability to learn procedural knowledge. The experimental
results show that DiCGRL could effectively alleviate the catastrophic
forgetting problem and outperform state-of-the-art continual learning models.
</p>
<a href="http://arxiv.org/abs/2010.02565" target="_blank">arXiv:2010.02565</a> [<a href="http://arxiv.org/pdf/2010.02565" target="_blank">pdf</a>]

<h2>Noise in Classification. (arXiv:2010.05080v2 [cs.LG] UPDATED)</h2>
<h3>Maria-Florina Balcan, Nika Haghtalab</h3>
<p>This chapter considers the computational and statistical aspects of learning
linear thresholds in presence of noise. When there is no noise, several
algorithms exist that efficiently learn near-optimal linear thresholds using a
small amount of data. However, even a small amount of adversarial noise makes
this problem notoriously hard in the worst-case. We discuss approaches for
dealing with these negative results by exploiting natural assumptions on the
data-generating process.
</p>
<a href="http://arxiv.org/abs/2010.05080" target="_blank">arXiv:2010.05080</a> [<a href="http://arxiv.org/pdf/2010.05080" target="_blank">pdf</a>]

<h2>Optimal quantisation of probability measures using maximum mean discrepancy. (arXiv:2010.07064v3 [stat.ML] UPDATED)</h2>
<h3>Onur Teymur, Jackson Gorham, Marina Riabiz, Chris. J. Oates</h3>
<p>Several researchers have proposed minimisation of maximum mean discrepancy
(MMD) as a method to quantise probability measures, i.e., to approximate a
target distribution by a representative point set. Here we consider sequential
algorithms that greedily minimise MMD over a discrete candidate set. We propose
a novel non-myopic algorithm and, in order to both improve statistical
efficiency and reduce computational cost, we investigate a variant that applies
this technique to a mini-batch of the candidate set at each iteration. When the
candidate points are sampled from the target, the consistency of these new
algorithm - and their mini-batch variants - is established. We demonstrate the
algorithms on a range of important computational problems, including
optimisation of nodes in Bayesian cubature and the thinning of Markov chain
output.
</p>
<a href="http://arxiv.org/abs/2010.07064" target="_blank">arXiv:2010.07064</a> [<a href="http://arxiv.org/pdf/2010.07064" target="_blank">pdf</a>]

<h2>A Backbone Replaceable Fine-tuning Framework for Stable Face Alignment. (arXiv:2010.09501v2 [cs.CV] UPDATED)</h2>
<h3>Xu Sun, Zhenfeng Fan, Zihao Zhang, Yingjie Guo, Shihong Xia</h3>
<p>Heatmap regression based face alignment has achieved prominent performance on
static images. However, the stability and accuracy are remarkably discounted
when applying the existing methods on dynamic videos. We attribute the
degradation to random noise and motion blur, which are common in videos. The
temporal information is critical to address this issue yet not fully considered
in the existing works. In this paper, we visit the video-oriented face
alignment problem in two perspectives: detection accuracy prefers lower error
for a single frame, and detection consistency forces better stability between
adjacent frames. On this basis, we propose a Jitter loss function that
leverages temporal information to suppress inaccurate as well as jittered
landmarks. The Jitter loss is involved in a novel framework with a fine-tuning
ConvLSTM structure over a backbone replaceable network. We further demonstrate
that accurate and stable landmarks are associated with different regions with
overlaps in a canonical coordinate, based on which the proposed Jitter loss
facilitates the optimization process during training. The proposed framework
achieves at least 40% improvement on stability evaluation metrics while
enhancing detection accuracy versus state-of-the-art methods. Generally, it can
swiftly convert a landmark detector for facial images to a better-performing
one for videos without retraining the entire model.
</p>
<a href="http://arxiv.org/abs/2010.09501" target="_blank">arXiv:2010.09501</a> [<a href="http://arxiv.org/pdf/2010.09501" target="_blank">pdf</a>]

<h2>Advantages of Bilinear Koopman Realizations for the Modeling and Control of Systems with Unknown Dynamics. (arXiv:2010.09961v3 [cs.RO] UPDATED)</h2>
<h3>Daniel Bruder, Xun Fu, Ram Vasudevan</h3>
<p>Nonlinear dynamical systems can be made easier to control by lifting them
into the space of observable functions, where their evolution is described by
the linear Koopman operator. This paper describes how the Koopman operator can
be used to generate approximate linear, bilinear, and nonlinear model
realizations from data, and argues in favor of bilinear realizations for
characterizing systems with unknown dynamics. Necessary and sufficient
conditions for a dynamical system to have a valid linear or bilinear
realization over a given set of observable functions are presented and used to
show that every control-affine system admits an infinite-dimensional bilinear
realization, but does not necessarily admit a linear one. Therefore,
approximate bilinear realizations constructed from generic sets of basis
functions tend to improve as the number of basis functions increases, whereas
approximate linear realizations may not. To demonstrate the advantages of
bilinear Koopman realizations for control, a linear, bilinear, and nonlinear
Koopman model realization of a simulated robot arm are constructed from data.
In a trajectory following task, the bilinear realization exceeds the prediction
accuracy of the linear realization and the computational efficiency of the
nonlinear realization when incorporated into a model predictive control
framework.
</p>
<a href="http://arxiv.org/abs/2010.09961" target="_blank">arXiv:2010.09961</a> [<a href="http://arxiv.org/pdf/2010.09961" target="_blank">pdf</a>]

<h2>Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image Pair. (arXiv:2010.10052v2 [cs.CV] UPDATED)</h2>
<h3>S Anupama, Prasan Shedligeri, Abhishek Pal, Kaushik Mitra</h3>
<p>Learning-based methods have enabled the recovery of a video sequence from a
single motion-blurred image or a single coded exposure image. Recovering video
from a single motion-blurred image is a very ill-posed problem and the
recovered video usually has many artifacts. In addition to this, the direction
of motion is lost and it results in motion ambiguity. However, it has the
advantage of fully preserving the information in the static parts of the scene.
The traditional coded exposure framework is better-posed but it only samples a
fraction of the space-time volume, which is at best 50% of the space-time
volume. Here, we propose to use the complementary information present in the
fully-exposed (blurred) image along with the coded exposure image to recover a
high fidelity video without any motion ambiguity. Our framework consists of a
shared encoder followed by an attention module to selectively combine the
spatial information from the fully-exposed image with the temporal information
from the coded image, which is then super-resolved to recover a non-ambiguous
high-quality video. The input to our algorithm is a fully-exposed and coded
image pair. Such an acquisition system already exists in the form of a
Coded-two-bucket (C2B) camera. We demonstrate that our proposed deep learning
approach using blurred-coded image pair produces much better results than those
from just a blurred image or just a coded image.
</p>
<a href="http://arxiv.org/abs/2010.10052" target="_blank">arXiv:2010.10052</a> [<a href="http://arxiv.org/pdf/2010.10052" target="_blank">pdf</a>]

<h2>Exploring search space trees using an adapted version of Monte Carlo tree search for combinatorial optimization problems. (arXiv:2010.11523v2 [cs.AI] UPDATED)</h2>
<h3>Jorik Jooken, Pieter Leyman, Patrick De Causmaecker, Tony Wauters</h3>
<p>In this article, a novel approach to solve combinatorial optimization
problems is proposed. This approach makes use of a heuristic algorithm to
explore the search space tree of a problem instance. The algorithm is based on
Monte Carlo tree search, a popular algorithm in game playing that is used to
explore game trees. By leveraging the combinatorial structure of a problem,
several enhancements to the algorithm are proposed. These enhancements aim to
efficiently explore the search space tree by pruning subtrees, using a
heuristic simulation policy, reducing the domains of variables by eliminating
dominated value assignments and using a beam width. They are demonstrated for
two specific combinatorial optimization problems: the quay crane scheduling
problem with non-crossing constraints and the 0-1 knapsack problem.
Computational results show that the algorithm achieves promising results for
both problems and eight new best solutions for a benchmark set of instances are
found for the former problem. These results indicate that the algorithm is
competitive with the state-of-the-art. Apart from this, the results also show
evidence that the algorithm is able to learn to correct the incorrect choices
made by constructive heuristics.
</p>
<a href="http://arxiv.org/abs/2010.11523" target="_blank">arXiv:2010.11523</a> [<a href="http://arxiv.org/pdf/2010.11523" target="_blank">pdf</a>]

<h2>The Analysis of Facial Feature Deformation using Optical Flow Algorithm. (arXiv:2010.12199v2 [cs.CV] UPDATED)</h2>
<h3>Dayang Nur Zulhijah Awang Jesemi, Hamimah Ujir, Irwandi Hipiny, Sarah Flora Samson Juan</h3>
<p>Facial features deformed according to the intended facial expression.
Specific facial features are associated with specific facial expression, i.e.
happy means the deformation of mouth. This paper presents the study of facial
feature deformation for each facial expression by using an optical flow
algorithm and segmented into three different regions of interest. The
deformation of facial features shows the relation between facial the and facial
expression. Based on the experiments, the deformations of eye and mouth are
significant in all expressions except happy. For happy expression, cheeks and
mouths are the significant regions. This work also suggests that different
facial features' intensity varies in the way that they contribute to the
recognition of the different facial expression intensity. The maximum magnitude
across all expressions is shown by the mouth for surprise expression which is
9x10-4. While the minimum magnitude is shown by the mouth for angry expression
which is 0.4x10-4.
</p>
<a href="http://arxiv.org/abs/2010.12199" target="_blank">arXiv:2010.12199</a> [<a href="http://arxiv.org/pdf/2010.12199" target="_blank">pdf</a>]

<h2>Model Interpretability through the Lens of Computational Complexity. (arXiv:2010.12265v2 [cs.AI] UPDATED)</h2>
<h3>Pablo Barcel&#xf3;, Mika&#xeb;l Monet, Jorge P&#xe9;rez, Bernardo Subercaseaux</h3>
<p>In spite of several claims stating that some models are more interpretable
than others -- e.g., "linear models are more interpretable than deep neural
networks" -- we still lack a principled notion of interpretability to formally
compare among different classes of models. We make a step towards such a notion
by studying whether folklore interpretability claims have a correlate in terms
of computational complexity theory. We focus on local post-hoc explainability
queries that, intuitively, attempt to answer why individual inputs are
classified in a certain way by a given model. In a nutshell, we say that a
class $\mathcal{C}_1$ of models is more interpretable than another class
$\mathcal{C}_2$, if the computational complexity of answering post-hoc queries
for models in $\mathcal{C}_2$ is higher than for those in $\mathcal{C}_1$. We
prove that this notion provides a good theoretical counterpart to current
beliefs on the interpretability of models; in particular, we show that under
our definition and assuming standard complexity-theoretical assumptions (such
as P$\neq$NP), both linear and tree-based models are strictly more
interpretable than neural networks. Our complexity analysis, however, does not
provide a clear-cut difference between linear and tree-based models, as we
obtain different results depending on the particular post-hoc explanations
considered. Finally, by applying a finer complexity analysis based on
parameterized complexity, we are able to prove a theoretical result suggesting
that shallow neural networks are more interpretable than deeper ones.
</p>
<a href="http://arxiv.org/abs/2010.12265" target="_blank">arXiv:2010.12265</a> [<a href="http://arxiv.org/pdf/2010.12265" target="_blank">pdf</a>]

<h2>Shapley Flow: A Graph-based Approach to Interpreting Model Predictions. (arXiv:2010.14592v2 [cs.LG] UPDATED)</h2>
<h3>Jiaxuan Wang, Jenna Wiens, Scott Lundberg</h3>
<p>Many existing approaches for estimating feature importance are problematic
because they ignore or hide dependencies among features. A causal graph, which
encodes the relationships among input variables, can aid in assigning feature
importance. However, current approaches that assign credit to nodes in the
causal graph fail to explain the entire graph. In light of these limitations,
we propose Shapley Flow, a novel approach to interpreting machine learning
models. It considers the entire causal graph, and assigns credit to
\textit{edges} instead of treating nodes as the fundamental unit of credit
assignment. Shapley Flow is the unique solution to a generalization of the
Shapley value axioms to directed acyclic graphs. We demonstrate the benefit of
using Shapley Flow to reason about the impact of a model's input on its output.
In addition to maintaining insights from existing approaches, Shapley Flow
extends the flat, set-based, view prevalent in game theory based explanation
methods to a deeper, \textit{graph-based}, view. This graph-based view enables
users to understand the flow of importance through a system, and reason about
potential interventions.
</p>
<a href="http://arxiv.org/abs/2010.14592" target="_blank">arXiv:2010.14592</a> [<a href="http://arxiv.org/pdf/2010.14592" target="_blank">pdf</a>]

<h2>Health improvement framework for planning actionable treatment process using surrogate Bayesian model. (arXiv:2010.16087v2 [cs.LG] UPDATED)</h2>
<h3>Kazuki Nakamura, Ryosuke Kojima, Eiichiro Uchino, Koichi Murashita, Ken Itoh, Shigeyuki Nakaji, Yasushi Okuno</h3>
<p>Clinical decision making regarding treatments based on personal
characteristics leads to effective health improvements. Machine learning (ML)
has been the primary concern of diagnosis support according to comprehensive
patient information. However, the remaining prominent issue is the development
of objective treatment processes in clinical situations. This study proposes a
novel framework to plan treatment processes in a data-driven manner. A key
point of the framework is the evaluation of the "actionability" for personal
health improvements by using a surrogate Bayesian model in addition to a
high-performance nonlinear ML model. We first evaluated the framework from the
viewpoint of its methodology using a synthetic dataset. Subsequently, the
framework was applied to an actual health checkup dataset comprising data from
3,132 participants, to improve systolic blood pressure values at the individual
level. We confirmed that the computed treatment processes are actionable and
consistent with clinical knowledge for lowering blood pressure. These results
demonstrate that our framework could contribute toward decision making in the
medical field, providing clinicians with deeper insights.
</p>
<a href="http://arxiv.org/abs/2010.16087" target="_blank">arXiv:2010.16087</a> [<a href="http://arxiv.org/pdf/2010.16087" target="_blank">pdf</a>]

<h2>Toward Force Estimation in Robot-Assisted Surgery using Deep Learning with Vision and Robot State. (arXiv:2011.02112v2 [cs.RO] UPDATED)</h2>
<h3>Zonghe Chua, Anthony M. Jarc, Allison M. Okamura</h3>
<p>Knowledge of interaction forces during teleoperated robot-assisted surgery
could be used to enable force feedback to human operators and evaluate tissue
handling skill. However, direct force sensing at the end-effector is
challenging because it requires biocompatible, sterilizable, and cost-effective
sensors. Vision-based deep learning using convolutional neural networks is a
promising approach for providing useful force estimates, though questions
remain about generalization to new scenarios and real-time inference. We
present a force estimation neural network that uses RGB images and robot state
as inputs. Using a self-collected dataset, we compared the network to variants
that included only a single input type, and evaluated how they generalized to
new viewpoints, workspace positions, materials, and tools. We found that
vision-based networks were sensitive to shifts in viewpoints, while state-only
networks were robust to changes in workspace. The network with both state and
vision inputs had the highest accuracy for an unseen tool, and was moderately
robust to changes in viewpoints. Through feature removal studies, we found that
using only position features produced better accuracy than using only force
features as input. The network with both state and vision inputs outperformed a
physics-based baseline model in accuracy. It showed comparable accuracy but
faster computation times than a baseline recurrent neural network, making it
better suited for real-time applications.
</p>
<a href="http://arxiv.org/abs/2011.02112" target="_blank">arXiv:2011.02112</a> [<a href="http://arxiv.org/pdf/2011.02112" target="_blank">pdf</a>]

<h2>Measuring Data Collection Quality for Community Healthcare. (arXiv:2011.02962v4 [cs.LG] UPDATED)</h2>
<h3>Ramesha Karunasena, Mohammad Sarparajul Ambiya, Arunesh Sinha, Ruchit Nagar, Saachi Dalal, Divy Thakkar, Milind Tambe</h3>
<p>Machine learning has tremendous potential to provide targeted interventions
in low-resource communities, however the availability of high-quality public
health data is a significant challenge. In this work, we partner with field
experts at a non-governmental organization (NGO) in India to define and test a
data collection quality score for each health worker who collects data. This
challenging unlabeled data problem is handled by building upon domain-expert's
guidance to design a useful data representation that is then clustered to infer
a data quality score. We also provide a more interpretable version of the
score. These scores already provide for a measurement of data collection
quality; in addition, we also predict the quality for future time steps and
find our results to be very accurate. Our work was successfully field tested
and is in the final stages of deployment in Rajasthan, India.
</p>
<a href="http://arxiv.org/abs/2011.02962" target="_blank">arXiv:2011.02962</a> [<a href="http://arxiv.org/pdf/2011.02962" target="_blank">pdf</a>]

<h2>Revisiting Model-Agnostic Private Learning: Faster Rates and Active Learning. (arXiv:2011.03186v2 [cs.LG] UPDATED)</h2>
<h3>Chong Liu, Yuqing Zhu, Kamalika Chaudhuri, Yu-Xiang Wang</h3>
<p>The Private Aggregation of Teacher Ensembles (PATE) framework is one of the
most promising recent approaches in differentially private learning. Existing
theoretical analysis shows that PATE consistently learns any VC-classes in the
realizable setting, but falls short in explaining its success in more general
cases where the error rate of the optimal classifier is bounded away from zero.
We fill in this gap by introducing the Tsybakov Noise Condition (TNC) and
establish stronger and more interpretable learning bounds. These bounds provide
new insights into when PATE works and improve over existing results even in the
narrower realizable setting. We also investigate the compelling idea of using
active learning for saving privacy budget. The novel components in the proofs
include a more refined analysis of the majority voting classifier -- which
could be of independent interest -- and an observation that the synthetic
"student" learning problem is nearly realizable by construction under the
Tsybakov noise condition.
</p>
<a href="http://arxiv.org/abs/2011.03186" target="_blank">arXiv:2011.03186</a> [<a href="http://arxiv.org/pdf/2011.03186" target="_blank">pdf</a>]

<h2>Learning to Orient Surfaces by Self-supervised Spherical CNNs. (arXiv:2011.03298v2 [cs.CV] UPDATED)</h2>
<h3>Riccardo Spezialetti, Federico Stella, Marlon Marcon, Luciano Silva, Samuele Salti, Luigi Di Stefano</h3>
<p>Defining and reliably finding a canonical orientation for 3D surfaces is key
to many Computer Vision and Robotics applications. This task is commonly
addressed by handcrafted algorithms exploiting geometric cues deemed as
distinctive and robust by the designer. Yet, one might conjecture that humans
learn the notion of the inherent orientation of 3D objects from experience and
that machines may do so alike. In this work, we show the feasibility of
learning a robust canonical orientation for surfaces represented as point
clouds. Based on the observation that the quintessential property of a
canonical orientation is equivariance to 3D rotations, we propose to employ
Spherical CNNs, a recently introduced machinery that can learn equivariant
representations defined on the Special Orthogonal group SO(3). Specifically,
spherical correlations compute feature maps whose elements define 3D rotations.
Our method learns such feature maps from raw data by a self-supervised training
procedure and robustly selects a rotation to transform the input point cloud
into a learned canonical orientation. Thereby, we realize the first end-to-end
learning approach to define and extract the canonical orientation of 3D shapes,
which we aptly dub Compass. Experiments on several public datasets prove its
effectiveness at orienting local surface patches as well as whole objects.
</p>
<a href="http://arxiv.org/abs/2011.03298" target="_blank">arXiv:2011.03298</a> [<a href="http://arxiv.org/pdf/2011.03298" target="_blank">pdf</a>]

<h2>Testbeds for Reinforcement Learning. (arXiv:2011.04590v2 [cs.AI] UPDATED)</h2>
<h3>Banafsheh Rafiee</h3>
<p>We present three problems modeled after animal learning experiments designed
to test online state construction or representation learning algorithms. Our
test problems require the learner to construct compact summaries of their past
interaction with the world in order to predict the future, updating online and
incrementally on each time step without an explicit training-testing split. The
majority of recent work in Deep Reinforcement Learning focuses on either fully
observable tasks, or games where stacking a handful of recent frames is
sufficient for good performance. Current benchmarks used for evaluating memory
and recurrent learning make use of 3D visual environments (e.g., DeepMind Lab)
which require billions of training samples, complex agent architectures, and
cloud-scale compute. These domains are thus not well suited for rapid
prototyping, hyper-parameter study, or extensive replication study. In this
paper, we contribute a set of test problems and benchmark results to fill this
gap. Our test problems are designed to be the simplest instantiation and test
of learning capabilities which animals readily exhibit, including (1) trace
conditioning (remembering a cue in order to predict another far in the future),
(2) positive/negative patterning (a combination of cues predict another), (3)
and combinations of both with additional non-relevant distracting signals. We
provide baselines for the first problem including heuristics from the early
days of neural network learning and simple ideas inspired by computational
models of animal learning. Our results highlight the difficulty of our test
problems for online recurrent learning systems and how the agent's performance
often exhibits substantial sensitivity to the choice of key problem and agent
parameters.
</p>
<a href="http://arxiv.org/abs/2011.04590" target="_blank">arXiv:2011.04590</a> [<a href="http://arxiv.org/pdf/2011.04590" target="_blank">pdf</a>]

<h2>Relation-weighted Link Prediction for Disease Gene Identification. (arXiv:2011.05138v3 [cs.LG] UPDATED)</h2>
<h3>Srivamshi Pittala, William Koehler, Jonathan Deans, Daniel Salinas, Martin Bringmann, Katharina Sophia Volz, Berk Kapicioglu</h3>
<p>Identification of disease genes, which are a set of genes associated with a
disease, plays an important role in understanding and curing diseases. In this
paper, we present a biomedical knowledge graph designed specifically for this
problem, propose a novel machine learning method that identifies disease genes
on such graphs by leveraging recent advances in network biology and graph
representation learning, study the effects of various relation types on
prediction performance, and empirically demonstrate that our algorithms
outperform its closest state-of-the-art competitor in disease gene
identification by 24.1%. We also show that we achieve higher precision than
Open Targets, the leading initiative for target identification, with respect to
predicting drug targets in clinical trials for Parkinson's disease.
</p>
<a href="http://arxiv.org/abs/2011.05138" target="_blank">arXiv:2011.05138</a> [<a href="http://arxiv.org/pdf/2011.05138" target="_blank">pdf</a>]

<h2>When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making. (arXiv:2011.06167v2 [cs.LG] UPDATED)</h2>
<h3>Sean McGrath, Parth Mehta, Alexandra Zytek, Isaac Lage, Himabindu Lakkaraju</h3>
<p>As machine learning (ML) models are increasingly being employed to assist
human decision makers, it becomes critical to provide these decision makers
with relevant inputs which can help them decide if and how to incorporate model
predictions into their decision making. For instance, communicating the
uncertainty associated with model predictions could potentially be helpful in
this regard. However, there is little to no research that systematically
explores if and how conveying predictive uncertainty impacts decision making.
In this work, we carry out user studies to systematically assess how people
respond to different types of predictive uncertainty i.e., posterior predictive
distributions with different shapes and variances, in the context of ML
assisted decision making. To the best of our knowledge, this work marks one of
the first attempts at studying this question. Our results demonstrate that
people are more likely to agree with a model prediction when they observe the
corresponding uncertainty associated with the prediction. This finding holds
regardless of the properties (shape or variance) of predictive uncertainty
(posterior predictive distribution), suggesting that uncertainty is an
effective tool for persuading humans to agree with model predictions.
Furthermore, we also find that other factors such as domain expertise and
familiarity with ML also play a role in determining how someone interprets and
incorporates predictive uncertainty into their decision making.
</p>
<a href="http://arxiv.org/abs/2011.06167" target="_blank">arXiv:2011.06167</a> [<a href="http://arxiv.org/pdf/2011.06167" target="_blank">pdf</a>]

<h2>Motion Generation Using Bilateral Control-Based Imitation Learning with Autoregressive Learning. (arXiv:2011.06192v2 [cs.RO] UPDATED)</h2>
<h3>Ayumu Sasagawa, Sho Sakaino, Toshiaki Tsuji</h3>
<p>Robots that can execute various tasks automatically on behalf of humans are
becoming an increasingly important focus of research in the field of robotics.
Imitation learning has been studied as an efficient and high-performance
method, and imitation learning based on bilateral control has been proposed as
a method that can realize fast motion. However, because this method cannot
implement autoregressive learning, this method may not generate desirable
long-term behavior. Therefore, in this paper, we propose a method of
autoregressive learning for bilateral control-based imitation learning. A new
neural network model for implementing autoregressive learning is proposed. In
this study, three types of experiments are conducted to verify the
effectiveness of the proposed method. The performance is improved compared to
conventional approaches; the proposed method has the highest rate of success.
Owing to the structure and autoregressive learning of the proposed model, the
proposed method can generate the desirable motion for successful tasks and have
a high generalization ability for environmental changes.
</p>
<a href="http://arxiv.org/abs/2011.06192" target="_blank">arXiv:2011.06192</a> [<a href="http://arxiv.org/pdf/2011.06192" target="_blank">pdf</a>]

<h2>StrObe: Streaming Object Detection from LiDAR Packets. (arXiv:2011.06425v2 [cs.CV] UPDATED)</h2>
<h3>Davi Frossard, Simon Suo, Sergio Casas, James Tu, Rui Hu, Raquel Urtasun</h3>
<p>Many modern robotics systems employ LiDAR as their main sensing modality due
to its geometrical richness. Rolling shutter LiDARs are particularly common, in
which an array of lasers scans the scene from a rotating base. Points are
emitted as a stream of packets, each covering a sector of the 360{\deg}
coverage. Modern perception algorithms wait for the full sweep to be built
before processing the data, which introduces an additional latency. For typical
10Hz LiDARs this will be 100ms. As a consequence, by the time an output is
produced, it no longer accurately reflects the state of the world. This poses a
challenge, as robotics applications require minimal reaction times, such that
maneuvers can be quickly planned in the event of a safety-critical situation.
In this paper we propose StrObe, a novel approach that minimizes latency by
ingesting LiDAR packets and emitting a stream of detections without waiting for
the full sweep to be built. StrObe reuses computations from previous packets
and iteratively updates a latent spatial representation of the scene, which
acts as a memory, as new evidence comes in, resulting in accurate low-latency
perception. We demonstrate the effectiveness of our approach on a large scale
real-world dataset, showing that StrObe far outperforms the state-of-the-art
when latency is taken into account, and matches the performance in the
traditional setting.
</p>
<a href="http://arxiv.org/abs/2011.06425" target="_blank">arXiv:2011.06425</a> [<a href="http://arxiv.org/pdf/2011.06425" target="_blank">pdf</a>]

<h2>Same Object, Different Grasps: Data and Semantic Knowledge for Task-Oriented Grasping. (arXiv:2011.06431v2 [cs.RO] UPDATED)</h2>
<h3>Adithyavairavan Murali, Weiyu Liu, Kenneth Marino, Sonia Chernova, Abhinav Gupta</h3>
<p>Despite the enormous progress and generalization in robotic grasping in
recent years, existing methods have yet to scale and generalize task-oriented
grasping to the same extent. This is largely due to the scale of the datasets
both in terms of the number of objects and tasks studied. We address these
concerns with the TaskGrasp dataset which is more diverse both in terms of
objects and tasks, and an order of magnitude larger than previous datasets. The
dataset contains 250K task-oriented grasps for 56 tasks and 191 objects along
with their RGB-D information. We take advantage of this new breadth and
diversity in the data and present the GCNGrasp framework which uses the
semantic knowledge of objects and tasks encoded in a knowledge graph to
generalize to new object instances, classes and even new tasks. Our framework
shows a significant improvement of around 12% on held-out settings compared to
baseline methods which do not use semantics. We demonstrate that our dataset
and model are applicable for the real world by executing task-oriented grasps
on a real robot on unknown objects. Code, data and supplementary video could be
found at https://sites.google.com/view/taskgrasp
</p>
<a href="http://arxiv.org/abs/2011.06431" target="_blank">arXiv:2011.06431</a> [<a href="http://arxiv.org/pdf/2011.06431" target="_blank">pdf</a>]

<h2>Image analysis for Alzheimer's disease prediction: Embracing pathological hallmarks for model architecture design. (arXiv:2011.06531v2 [cs.LG] UPDATED)</h2>
<h3>Sarah C. Br&#xfc;ningk, Felix Hensel, Catherine R. Jutzeler, Bastian Rieck</h3>
<p>Alzheimer's disease (AD) is associated with local (e.g. brain tissue atrophy)
and global brain changes (loss of cerebral connectivity), which can be detected
by high-resolution structural magnetic resonance imaging. Conventionally, these
changes and their relation to AD are investigated independently. Here, we
introduce a novel, highly-scalable approach that simultaneously captures
$\textit{local}$ and $\textit{global}$ changes in the diseased brain. It is
based on a neural network architecture that combines patch-based,
high-resolution 3D-CNNs with global topological features, evaluating
multi-scale brain tissue connectivity. Our local-global approach reached
competitive results with an average precision score of $0.95\pm0.03$ for the
classification of cognitively normal subjects and AD patients (prevalence
$\approx 55\%$).
</p>
<a href="http://arxiv.org/abs/2011.06531" target="_blank">arXiv:2011.06531</a> [<a href="http://arxiv.org/pdf/2011.06531" target="_blank">pdf</a>]

<h2>Shared Prior Learning of Energy-Based Models for Image Reconstruction. (arXiv:2011.06539v2 [cs.CV] UPDATED)</h2>
<h3>Thomas Pinetz, Erich Kobler, Thomas Pock, Alexander Effland</h3>
<p>We propose a novel learning-based framework for image reconstruction
particularly designed for training without ground truth data, which has three
major building blocks: energy-based learning, a patch-based Wasserstein loss
functional, and shared prior learning. In energy-based learning, the parameters
of an energy functional composed of a learned data fidelity term and a
data-driven regularizer are computed in a mean-field optimal control problem.
In the absence of ground truth data, we change the loss functional to a
patch-based Wasserstein functional, in which local statistics of the output
images are compared to uncorrupted reference patches. Finally, in shared prior
learning, both aforementioned optimal control problems are optimized
simultaneously with shared learned parameters of the regularizer to further
enhance unsupervised image reconstruction. We derive several time
discretization schemes of the gradient flow and verify their consistency in
terms of Mosco convergence. In numerous numerical experiments, we demonstrate
that the proposed method generates state-of-the-art results for various image
reconstruction applications--even if no ground truth images are available for
training.
</p>
<a href="http://arxiv.org/abs/2011.06539" target="_blank">arXiv:2011.06539</a> [<a href="http://arxiv.org/pdf/2011.06539" target="_blank">pdf</a>]

<h2>Randomized fast no-loss expert system to play tic tac toe like a human. (arXiv:2009.11225v2 [cs.AI] CROSS LISTED)</h2>
<h3>Aditya Jyoti Paul</h3>
<p>This paper introduces a blazingly fast, no-loss expert system for Tic Tac Toe
using Decision Trees called T3DT, that tries to emulate human gameplay as
closely as possible. It does not make use of any brute force, minimax or
evolutionary techniques, but is still always unbeatable. In order to make the
gameplay more human-like, randomization is prioritized and T3DT randomly
chooses one of the multiple optimal moves at each step. Since it does not need
to analyse the complete game tree at any point, T3DT is exceptionally faster
than any brute force or minimax algorithm, this has been shown theoretically as
well as empirically from clock-time analyses in this paper. T3DT also doesn't
need the data sets or the time to train an evolutionary model, making it a
practical no-loss approach to play Tic Tac Toe.
</p>
<a href="http://arxiv.org/abs/2009.11225" target="_blank">arXiv:2009.11225</a> [<a href="http://arxiv.org/pdf/2009.11225" target="_blank">pdf</a>]

<h2>Parameterized Explainer for Graph Neural Network. (arXiv:2011.04573v1 [cs.LG] CROSS LISTED)</h2>
<h3>Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang</h3>
<p>Despite recent progress in Graph Neural Networks (GNNs), explaining
predictions made by GNNs remains a challenging open problem. The leading method
independently addresses the local explanations (i.e., important subgraph
structure and node features) to interpret why a GNN model makes the prediction
for a single instance, e.g. a node or a graph. As a result, the explanation
generated is painstakingly customized for each instance. The unique explanation
interpreting each instance independently is not sufficient to provide a global
understanding of the learned GNN model, leading to a lack of generalizability
and hindering it from being used in the inductive setting. Besides, as it is
designed for explaining a single instance, it is challenging to explain a set
of instances naturally (e.g., graphs of a given class). In this study, we
address these key challenges and propose PGExplainer, a parameterized explainer
for GNNs. PGExplainer adopts a deep neural network to parameterize the
generation process of explanations, which enables PGExplainer a natural
approach to explaining multiple instances collectively. Compared to the
existing work, PGExplainer has better generalization ability and can be
utilized in an inductive setting easily. Experiments on both synthetic and
real-life datasets show highly competitive performance with up to 24.7\%
relative improvement in AUC on explaining graph classification over the leading
baseline.
</p>
<a href="http://arxiv.org/abs/2011.04573" target="_blank">arXiv:2011.04573</a> [<a href="http://arxiv.org/pdf/2011.04573" target="_blank">pdf</a>]

<h2>Sparse Longitudinal Representations of Electronic Health Record Data for the Early Detection of Chronic Kidney Disease in Diabetic Patients. (arXiv:2011.04802v1 [cs.LG] CROSS LISTED)</h2>
<h3>Jinghe Zhang, Kamran Kowsari, Mehdi Boukhechba, James Harrison, Jennifer Lobo, Laura Barnes</h3>
<p>Chronic kidney disease (CKD) is a gradual loss of renal function over time,
and it increases the risk of mortality, decreased quality of life, as well as
serious complications. The prevalence of CKD has been increasing in the last
couple of decades, which is partly due to the increased prevalence of diabetes
and hypertension. To accurately detect CKD in diabetic patients, we propose a
novel framework to learn sparse longitudinal representations of patients'
medical records. The proposed method is also compared with widely used
baselines such as Aggregated Frequency Vector and Bag-of-Pattern in Sequences
on real EHR data, and the experimental results indicate that the proposed model
achieves higher predictive performance. Additionally, the learned
representations are interpreted and visualized to bring clinical insights.
</p>
<a href="http://arxiv.org/abs/2011.04802" target="_blank">arXiv:2011.04802</a> [<a href="http://arxiv.org/pdf/2011.04802" target="_blank">pdf</a>]

