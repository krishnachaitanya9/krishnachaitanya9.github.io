---
title: Latest Deep Learning Papers
date: 2020-10-27 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>A Path-Dependent Variational Framework for Incremental Information Gathering. (arXiv:2010.13813v1 [cs.RO])</h2>
<h3>William Clark, Maani Ghaffari</h3>
<p>Information gathered along a path is inherently submodular; the incremental
amount of information gained along a path decreases due to redundant
observations. In addition to submodularity, the incremental amount of
information gained is a function of not only the current state but also the
entire history as well. This paper presents the construction of the first-order
necessary optimality conditions for memory (history-dependent) Lagrangians.
Path-dependent problems frequently appear in robotics and artificial
intelligence, where the state such as a map is partially observable, and
information can only be obtained along a trajectory by local sensing. Robotic
exploration and environmental monitoring has numerous real-world applications
and can be formulated using the proposed approach.
</p>
<a href="http://arxiv.org/abs/2010.13813" target="_blank">arXiv:2010.13813</a> [<a href="http://arxiv.org/pdf/2010.13813" target="_blank">pdf</a>]

<h2>Statistical Learning Based Joint Antenna Selection and User Scheduling for Single-Cell Massive MIMO Systems. (arXiv:2010.13848v1 [cs.IT])</h2>
<h3>Mangqing Guo, M. Cenk Gursoy</h3>
<p>Large number of antennas and radio frequency (RF) chains at the base stations
(BSs) lead to high energy consumption in massive MIMO systems. Thus, how to
improve the energy efficiency (EE) with a computationally efficient approach is
a significant challenge in the design of massive MIMO systems. With this
motivation, a learning-based stochastic gradient descent algorithm is proposed
in this paper to obtain the optimal joint uplink and downlink EE with joint
antenna selection and user scheduling in single-cell massive MIMO systems.
Using Jensen's inequality and the characteristics of wireless channels, a lower
bound on the system throughput is obtained. Subsequently, incorporating the
power consumption model, the corresponding lower bound on the EE of the system
is identified. Finally, learning-based stochastic gradient descent method is
used to solve the joint antenna selection and user scheduling problem, which is
a combinatorial optimization problem. Rare event simulation is embedded in the
learning-based stochastic gradient descent method to generate samples with very
small probabilities. In the analysis, both perfect and imperfect channel side
information (CSI) at the BS are considered. Minimum mean-square error (MMSE)
channel estimation is employed in the study of the imperfect CSI case. In
addition, the effect of a constraint on the number of available RF chains in
massive MIMO system is investigated considering both perfect and imperfect CSI
at the BS.
</p>
<a href="http://arxiv.org/abs/2010.13848" target="_blank">arXiv:2010.13848</a> [<a href="http://arxiv.org/pdf/2010.13848" target="_blank">pdf</a>]

<h2>An exploratory study on machine learning to couple numerical solutions of partial differential equations. (arXiv:2010.13917v1 [math.NA])</h2>
<h3>H. S. Tang, L. Li, M. Grossberg, Y. J. Liu, Y. M. Jia, S. S. Li, W. B. Dong</h3>
<p>As further progress in the accurate and efficient computation of coupled
partial differential equations (PDEs) becomes increasingly difficult, it has
become highly desired to develop new methods for such computation. In deviation
from conventional approaches, this short communication paper explores a
computational paradigm that couples numerical solutions of PDEs via
machine-learning (ML) based methods, together with a preliminary study on the
paradigm. Particularly, it solves PDEs in subdomains as in a conventional
approach but develops and trains artificial neural networks (ANN) to couple the
PDEs' solutions at their interfaces, leading to solutions to the PDEs in the
whole domains. The concepts and algorithms for the ML coupling are discussed
using coupled Poisson equations and coupled advection-diffusion equations.
Preliminary numerical examples illustrate the feasibility and performance of
the ML coupling. Although preliminary, the results of this exploratory study
indicate that the ML paradigm is promising and deserves further research.
</p>
<a href="http://arxiv.org/abs/2010.13917" target="_blank">arXiv:2010.13917</a> [<a href="http://arxiv.org/pdf/2010.13917" target="_blank">pdf</a>]

<h2>Hausdorff dimension of sets of numbers with large L\"uroth elements. (arXiv:2010.13932v1 [math.NT])</h2>
<h3>Aubin Arroyo, Gerardo Gonz&#xe1;lez Robert</h3>
<p>L\"uroth series, like regular continued fractions, provide an interesting
identification of irrational numbers with infinite sequence of integers. These
sequences give deep arithmetic and measure-theoretic properties of subsets of
numbers according to their growth. Although different, regular continued
fractions and L\"uroth series share several properties. In this paper, we
explore one similarity by estimating the Hausdorff dimension of subsets of real
numbers whose L\"uroth expansion grows at a definite rate. This is an extension
of a result of Y. Sun and J. Wu to the context of L\"uroth series.
</p>
<a href="http://arxiv.org/abs/2010.13932" target="_blank">arXiv:2010.13932</a> [<a href="http://arxiv.org/pdf/2010.13932" target="_blank">pdf</a>]

<h2>Interior Point Solving for LP-based prediction+optimisation. (arXiv:2010.13943v1 [cs.NE])</h2>
<h3>Jayanta Mandi, Tias Guns</h3>
<p>Solving optimization problems is the key to decision making in many real-life
analytics applications. However, the coefficients of the optimization problems
are often uncertain and dependent on external factors, such as future demand or
energy or stock prices. Machine learning (ML) models, especially neural
networks, are increasingly being used to estimate these coefficients in a
data-driven way. Hence, end-to-end predict-and-optimize approaches, which
consider how effective the predicted values are to solve the optimization
problem, have received increasing attention. In case of integer linear
programming problems, a popular approach to overcome their non-differentiabilty
is to add a quadratic penalty term to the continuous relaxation, such that
results from differentiating over quadratic programs can be used. Instead we
investigate the use of the more principled logarithmic barrier term, as widely
used in interior point solvers for linear programming. Specifically, instead of
differentiating the KKT conditions, we consider the homogeneous self-dual
formulation of the LP and we show the relation between the interior point step
direction and corresponding gradients needed for learning. Finally our
empirical experiments demonstrate our approach performs as good as if not
better than the state-of-the-art QPTL (Quadratic Programming task loss)
formulation of Wilder et al. and SPO approach of Elmachtoub and Grigas.
</p>
<a href="http://arxiv.org/abs/2010.13943" target="_blank">arXiv:2010.13943</a> [<a href="http://arxiv.org/pdf/2010.13943" target="_blank">pdf</a>]

<h2>Epidemic Dynamics via Wavelet Theory and Machine Learning, with Applications to Covid-19. (arXiv:2010.14004v1 [math.OC])</h2>
<h3>T&#xf4; Tat Dat, Protin Fr&#xe9;d&#xe9;ric, Nguyen T.T. Hang, Martel Jules, Nguyen Duc Thang, Charles Piffault, Rodr&#xed;guez Willy, Figueroa Susely, H&#xf4;ng V&#xe2;n L&#xea;, Wilderich Tuschmann, Nguyen Tien Zung</h3>
<p>We introduce the concept of epidemic-fitted wavelets which comprise, in
particular, as special cases the number $I(t)$ of infectious individuals at
time $t$ in classical SIR models and their derivatives. We present a novel
method for modelling epidemic dynamics by a model selection method using
wavelet theory and, for its applications, machine learning based curve fitting
techniques. Our universal models are functions that are finite linear
combinations of epidemic-fitted wavelets. We apply our method by modelling and
forecasting, based on the John Hopkins University dataset, the spread of the
current Covid-19 (SARS-CoV-2) epidemic in France, Germany, Italy and the Czech
Republic, as well as in the US federal states New York and Florida.
</p>
<a href="http://arxiv.org/abs/2010.14004" target="_blank">arXiv:2010.14004</a> [<a href="http://arxiv.org/pdf/2010.14004" target="_blank">pdf</a>]

<h2>A Probabilistic Representation of Deep Learning for Improving The Information Theoretic Interpretability. (arXiv:2010.14054v1 [cs.LG])</h2>
<h3>Xinjie Lan, Kenneth E. Barner</h3>
<p>In this paper, we propose a probabilistic representation of MultiLayer
Perceptrons (MLPs) to improve the information-theoretic interpretability. Above
all, we demonstrate that the activations being i.i.d. is not valid for all the
hidden layers of MLPs, thus the existing mutual information estimators based on
non-parametric inference methods, e.g., empirical distributions and Kernel
Density Estimate (KDE), are invalid for measuring the information flow in MLPs.
Moreover, we introduce explicit probabilistic explanations for MLPs: (i) we
define the probability space (Omega_F, t, P_F) for a fully connected layer f
and demonstrate the great effect of an activation function on the probability
measure P_F ; (ii) we prove the entire architecture of MLPs as a Gibbs
distribution P; and (iii) the back-propagation aims to optimize the sample
space Omega_F of all the fully connected layers of MLPs for learning an optimal
Gibbs distribution P* to express the statistical connection between the input
and the label. Based on the probabilistic explanations for MLPs, we improve the
information-theoretic interpretability of MLPs in three aspects: (i) the random
variable of f is discrete and the corresponding entropy is finite; (ii) the
information bottleneck theory cannot correctly explain the information flow in
MLPs if we take into account the back-propagation; and (iii) we propose novel
information-theoretic explanations for the generalization of MLPs. Finally, we
demonstrate the proposed probabilistic representation and information-theoretic
explanations for MLPs in a synthetic dataset and benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2010.14054" target="_blank">arXiv:2010.14054</a> [<a href="http://arxiv.org/pdf/2010.14054" target="_blank">pdf</a>]

<h2>Hamilton-Jacobi Deep Q-Learning for Deterministic Continuous-Time Systems with Lipschitz Continuous Controls. (arXiv:2010.14087v1 [cs.LG])</h2>
<h3>Jeongho Kim, Jaeuk Shin, Insoon Yang</h3>
<p>In this paper, we propose Q-learning algorithms for continuous-time
deterministic optimal control problems with Lipschitz continuous controls. Our
method is based on a new class of Hamilton-Jacobi-Bellman (HJB) equations
derived from applying the dynamic programming principle to continuous-time
Q-functions. A novel semi-discrete version of the HJB equation is proposed to
design a Q-learning algorithm that uses data collected in discrete time without
discretizing or approximating the system dynamics. We identify the condition
under which the Q-function estimated by this algorithm converges to the optimal
Q-function. For practical implementation, we propose the Hamilton-Jacobi DQN,
which extends the idea of deep Q-networks (DQN) to our continuous control
setting. This approach does not require actor networks or numerical solutions
to optimization problems for greedy actions since the HJB equation provides a
simple characterization of optimal controls via ordinary differential
equations. We empirically demonstrate the performance of our method through
benchmark tasks and high-dimensional linear-quadratic problems.
</p>
<a href="http://arxiv.org/abs/2010.14087" target="_blank">arXiv:2010.14087</a> [<a href="http://arxiv.org/pdf/2010.14087" target="_blank">pdf</a>]

<h2>Meta-MgNet: Meta Multigrid Networks for Solving Parameterized Partial Differential Equations. (arXiv:2010.14088v1 [math.NA])</h2>
<h3>Yuyan Chen, Bin Dong, Jinchao Xu</h3>
<p>This paper studies numerical solutions for parameterized partial differential
equations (P-PDEs) with deep learning (DL). P-PDEs arise in many important
application areas and the computational cost using traditional numerical
schemes can be exorbitant, especially when the parameters fall into a
particular range and the underlying PDE is required to be solved with high
accuracy. Recently, solving PDEs with DL has become an emerging field. Existing
works demonstrate great potentials of the DL based approach in speeding up
numerical solutions of PDEs. However, there is still limited research on the DL
approach for P-PDEs. If we directly apply existing supervised learning models
to P-PDEs, the models need to be constantly fine-tuned or retrained when the
parameters change. This drastically limits the applicability and utility of
these models in practice. To resolve this issue, we propose a
meta-learning-based method that can efficiently solve P-PDEs with a wide range
of parameters without retraining. Our key observation is to regard training a
solver for the P-PDE with a given set of parameters as a learning task. Then,
training a solver for the P-PDEs with varied parameters can be viewed as a
multi-task learning problem, to which meta-learning is one of the most
effective approaches. This new perspective can be applied to many existing PDE
solvers. As an example, we adopt the Multigrid Network (MgNet) as the base
solver. To achieve multi-task learning, we introduce a new hypernetwork, called
Meta-NN, in MgNet and refer to the entire network as the Meta-MgNet. Meta-NN
takes the differential operators and the right-hand-side of the underlying
P-PDEs as inputs and generates appropriate smoothers for MgNet which can
significantly affect the convergent speed. Finally, extensive numerical
experiments demonstrate that Meta-MgNet is more efficient in solving P-PDEs
than the MG methods and MgNet.
</p>
<a href="http://arxiv.org/abs/2010.14088" target="_blank">arXiv:2010.14088</a> [<a href="http://arxiv.org/pdf/2010.14088" target="_blank">pdf</a>]

<h2>Physics-Based Deep Learning for Fiber-Optic Communication Systems. (arXiv:2010.14258v1 [eess.SP])</h2>
<h3>Christian H&#xe4;ger, Henry D. Pfister</h3>
<p>We propose a new machine-learning approach for fiber-optic communication
systems whose signal propagation is governed by the nonlinear Schr\"odinger
equation (NLSE). Our main observation is that the popular split-step method
(SSM) for numerically solving the NLSE has essentially the same functional form
as a deep multi-layer neural network; in both cases, one alternates linear
steps and pointwise nonlinearities. We exploit this connection by
parameterizing the SSM and viewing the linear steps as general linear
functions, similar to the weight matrices in a neural network. The resulting
physics-based machine-learning model has several advantages over "black-box"
function approximators. For example, it allows us to examine and interpret the
learned solutions in order to understand why they perform well. As an
application, low-complexity nonlinear equalization is considered, where the
task is to efficiently invert the NLSE. This is commonly referred to as digital
backpropagation (DBP). Rather than employing neural networks, the proposed
algorithm, dubbed learned DBP (LDBP), uses the physics-based model with
trainable filters in each step and its complexity is reduced by progressively
pruning filter taps during gradient descent. Our main finding is that the
filters can be pruned to remarkably short lengths-as few as 3 taps/step-without
sacrificing performance. As a result, the complexity can be reduced by orders
of magnitude in comparison to prior work. By inspecting the filter responses,
an additional theoretical justification for the learned parameter
configurations is provided. Our work illustrates that combining data-driven
optimization with existing domain knowledge can generate new insights into old
communications problems.
</p>
<a href="http://arxiv.org/abs/2010.14258" target="_blank">arXiv:2010.14258</a> [<a href="http://arxiv.org/pdf/2010.14258" target="_blank">pdf</a>]

<h2>An efficient nonconvex reformulation of stagewise convex optimization problems. (arXiv:2010.14322v1 [math.OC])</h2>
<h3>Rudy Bunel, Oliver Hinder, Srinadh Bhojanapalli, Krishnamurthy (Dj) Dvijotham</h3>
<p>Convex optimization problems with staged structure appear in several
contexts, including optimal control, verification of deep neural networks, and
isotonic regression. Off-the-shelf solvers can solve these problems but may
scale poorly. We develop a nonconvex reformulation designed to exploit this
staged structure. Our reformulation has only simple bound constraints, enabling
solution via projected gradient methods and their accelerated variants. The
method automatically generates a sequence of primal and dual feasible solutions
to the original convex problem, making optimality certification easy. We
establish theoretical properties of the nonconvex formulation, showing that it
is (almost) free of spurious local minima and has the same global optimum as
the convex problem. We modify PGD to avoid spurious local minimizers so it
always converges to the global minimizer. For neural network verification, our
approach obtains small duality gaps in only a few gradient steps. Consequently,
it can quickly solve large-scale verification problems faster than both
off-the-shelf and specialized solvers.
</p>
<a href="http://arxiv.org/abs/2010.14322" target="_blank">arXiv:2010.14322</a> [<a href="http://arxiv.org/pdf/2010.14322" target="_blank">pdf</a>]

<h2>Compound conditionals, Fr\'echet-Hoeffding bounds, and Frank t-norms. (arXiv:2010.14382v1 [math.PR])</h2>
<h3>Angelo Gilio, Giuseppe Sanfilippo</h3>
<p>We consider compound conditionals, Fr\'echet-Hoeffding bounds and the
probabilistic interpretation of Frank t-norms. We show under logical
independence the sharpness of the Fr\'echet-Hoeffding bounds for the prevision
of conjunctions and disjunctions of $n$ conditional events. We discuss the case
where the prevision of conjunctions is assessed by Lukasiewicz t-norms and we
give explicit solutions for the linear systems; then, we analyze a selected
example. We obtain a probabilistic interpretation of Frank t-norms and
t-conorms as prevision of conjunctions and disjunctions of conditional events,
respectively. Then, we characterize the sets of coherent prevision assessments
on a family containing $n$ conditional events and their conjunction, or their
disjunction, by using Frank t-norms, or Frank t-conorms. By assuming logical
independence, we show that any Frank t-norm (resp., t-conorm) of two
conditional events $A|H$ and $B|K$, $T_{\lambda}(A|H,B|K)$ (resp.,
$S_{\lambda}(A|H,B|K)$), is a conjunction $(A|H)\wedge (B|K)$ (resp., a
disjunction $(A|H)\vee (B|K)$). Then, we analyze the case of logical dependence
where $A=B$. We give some results on Frank t-norms and coherence of the
prevision assessments on a family of three conditional events. By assuming
logical independence, we show that it is coherent to assess the previsions of
all the conjunctions by means of Minimum and Product t-norms. In this case all
the conjunctions coincide with the t-norms of the corresponding conditional
events. We verify by a counterexample that, when using the Lukasiewicz t-norm
to assess the previsions of conjunctions, coherence is not assured; thus, the
Lukasiewicz t-norm of conditional events may not be interpreted as their
conjunction. Finally, we give two sufficient conditions for coherence and
incoherence when using the Lukasiewicz t-norm.
</p>
<a href="http://arxiv.org/abs/2010.14382" target="_blank">arXiv:2010.14382</a> [<a href="http://arxiv.org/pdf/2010.14382" target="_blank">pdf</a>]

<h2>Deep Network Approximation Characterized by Number of Neurons. (arXiv:1906.05497v4 [math.NA] UPDATED)</h2>
<h3>Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>This paper quantitatively characterizes the approximation power of deep
feed-forward neural networks (FNNs) in terms of the number of neurons. It is
shown by construction that ReLU FNNs with width $\mathcal{O}\big(\max\{d\lfloor
N^{1/d}\rfloor,\, N+1\}\big)$ and depth $\mathcal{O}(L)$ can approximate an
arbitrary H\"older continuous function of order $\alpha\in (0,1]$ on $[0,1]^d$
with a nearly tight approximation rate $\mathcal{O}\big(\sqrt{d}
N^{-2\alpha/d}L^{-2\alpha/d}\big)$ measured in $L^p$-norm for any $N,L\in
\mathbb{N}^+$ and $p\in[1,\infty]$. More generally for an arbitrary continuous
function $f$ on $[0,1]^d$ with a modulus of continuity $\omega_f(\cdot)$, the
constructive approximation rate is $\mathcal{O}\big(\sqrt{d}\,\omega_f(
N^{-2/d}L^{-2/d})\big)$. We also extend our analysis to $f$ on irregular
domains or those localized in an $\varepsilon$-neighborhood of a
$d_{\mathcal{M}}$-dimensional smooth manifold $\mathcal{M}\subseteq [0,1]^d$
with $d_{\mathcal{M}}\ll d$. Especially, in the case of an essentially
low-dimensional domain, we show an approximation rate
$\mathcal{O}\big(\omega_f(\tfrac{\varepsilon}{1-\delta}\sqrt{\tfrac{d}{d_\delta}}+\varepsilon)+\sqrt{d}\,\omega_f(\tfrac{\sqrt{d}}{(1-\delta)\sqrt{d_\delta}}N^{-2/d_\delta}L^{-2/d_\delta})\big)$
for ReLU FNNs to approximate $f$ in the $\varepsilon$-neighborhood, where
$d_\delta=\mathcal{O}\big(d_{\mathcal{M}}\tfrac{\ln (d/\delta)}{\delta^2}\big)$
for any $\delta\in(0,1)$ as a relative error for a projection to approximate an
isometry when projecting $\mathcal{M}$ to a $d_{\delta}$-dimensional domain.
</p>
<a href="http://arxiv.org/abs/1906.05497" target="_blank">arXiv:1906.05497</a> [<a href="http://arxiv.org/pdf/1906.05497" target="_blank">pdf</a>]

<h2>Distributed Optimization for Smart Cyber-Physical Networks. (arXiv:1906.10760v3 [eess.SY] UPDATED)</h2>
<h3>Giuseppe Notarstefano, Ivano Notarnicola, Andrea Camisa</h3>
<p>The presence of embedded electronics and communication capabilities as well
as sensing and control in smart devices has given rise to the novel concept of
cyber-physical networks, in which agents aim at cooperatively solving complex
tasks by local computation and communication. Numerous estimation, learning,
decision and control tasks in smart networks involve the solution of
large-scale, structured optimization problems in which network agents have only
a partial knowledge of the whole problem. Distributed optimization aims at
designing local computation and communication rules for the network processors
allowing them to cooperatively solve the global optimization problem without
relying on any central unit. The purpose of this survey is to provide an
introduction to distributed optimization methodologies. Principal approaches,
namely (primal) consensus-based, duality-based and constraint exchange methods,
are formalized. An analysis of the basic schemes is supplied, and
state-of-the-art extensions are reviewed.
</p>
<a href="http://arxiv.org/abs/1906.10760" target="_blank">arXiv:1906.10760</a> [<a href="http://arxiv.org/pdf/1906.10760" target="_blank">pdf</a>]

<h2>Kinetic Theory for Residual Neural Networks. (arXiv:2001.04294v2 [math.OC] UPDATED)</h2>
<h3>M. Herty, T. Trimborn, G. Visconti</h3>
<p>Deep residual neural networks are performing very well for many data science
applications. We use kinetic theory to improve understanding of existing
methods. A simplified residual neural network (SimResNet) model, in which each
layer consists of one neuron per input dimension at most, is studied in the
limit of infinitely many inputs. This leads to a Vlasov type equation for the
distribution of data, and we analyze it with respect to sensitivities and
steady states. In the simple case of a linear activation function we can study
moment model properties for one-dimensional input data. Further, a modification
of the microscopic dynamics leads to a Fokker-Planck type formulation of the
SimResNet, in which the concept of network training is replaced by the task of
fitting distributions. The performed analysis is validated by numerical
simulations. In particular, results on clustering and regression problems are
presented.
</p>
<a href="http://arxiv.org/abs/2001.04294" target="_blank">arXiv:2001.04294</a> [<a href="http://arxiv.org/pdf/2001.04294" target="_blank">pdf</a>]

<h2>The gap between theory and practice in function approximation with deep neural networks. (arXiv:2001.07523v2 [cs.LG] UPDATED)</h2>
<h3>Ben Adcock, Nick Dexter</h3>
<p>Deep learning (DL) is transforming industry as decision-making processes are
being automated by deep neural networks (DNNs) trained on real-world data.
Driven partly by rapidly-expanding literature on DNN approximation theory
showing they can approximate a rich variety of functions, such tools are
increasingly being considered for problems in scientific computing. Yet, unlike
traditional algorithms in this field, little is known about DNNs from the
principles of numerical analysis, e.g., stability, accuracy, computational
efficiency and sample complexity. In this paper we introduce a computational
framework for examining DNNs in practice, and use it to study empirical
performance with regard to these issues. We study performance of DNNs of
different widths &amp; depths on test functions in various dimensions, including
smooth and piecewise smooth functions. We also compare DL against best-in-class
methods for smooth function approx. based on compressed sensing (CS). Our main
conclusion from these experiments is that there is a crucial gap between the
approximation theory of DNNs and their practical performance, with trained DNNs
performing relatively poorly on functions for which there are strong
approximation results (e.g. smooth functions), yet performing well in
comparison to best-in-class methods for other functions. To analyze this gap
further, we provide some theoretical insights. We establish a practical
existence theorem, asserting existence of a DNN architecture and training
procedure that offers the same performance as CS. This establishes a key
theoretical benchmark, showing the gap can be closed, albeit via a strategy
guaranteed to perform as well as, but no better than, current best-in-class
schemes. Nevertheless, it demonstrates the promise of practical DNN approx., by
highlighting potential for better schemes through careful design of DNN
architectures and training strategies.
</p>
<a href="http://arxiv.org/abs/2001.07523" target="_blank">arXiv:2001.07523</a> [<a href="http://arxiv.org/pdf/2001.07523" target="_blank">pdf</a>]

<h2>The nucleus of an adjunction and the Street monad on monads. (arXiv:2004.07353v2 [math.CT] UPDATED)</h2>
<h3>Dusko Pavlovic, Dominic J.D.Hughes</h3>
<p>An adjunction is a pair of functors related by a pair of natural
transformations, and relating a pair of categories. It displays how a
structure, or a concept, projects from each category to the other, and back.
Adjunctions are the common denominator of Galois connections, representation
theories, spectra, and generalized quantifiers. We call an adjunction nuclear
when its categories determine each other. We show that every adjunction can be
resolved into a nuclear adjunction. The resolution is idempotent in a strict
sense. The resulting nucleus displays the concept that was implicit in the
original adjunction, just as the singular value decomposition of an adjoint
pair of linear operators displays their canonical bases.

[snip]

In his seminal early work, Ross Street described an adjunction between monads
and comonads in 2-categories. Lifting the nucleus construction, we show that
the resulting Street monad on monads is strictly idempotent, and extracts the
nucleus of a monad. A dual treatment achieves the same for comonads. This
uncovers remarkably concrete applications behind a notable fragment of pure
2-category theory. The other way around, driven by the tasks and methods of
machine learning and data analysis, the nucleus construction also seems to
uncover remarkably pure and general mathematical content lurking behind the
daily practices of network computation and data analysis.
</p>
<a href="http://arxiv.org/abs/2004.07353" target="_blank">arXiv:2004.07353</a> [<a href="http://arxiv.org/pdf/2004.07353" target="_blank">pdf</a>]

<h2>Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model. (arXiv:2006.08212v2 [cs.LG] UPDATED)</h2>
<h3>Rapha&#xeb;l Berthier (PSL, SIERRA), Francis Bach (SIERRA, PSL), Pierre Gaillard (SIERRA, PSL, Thoth)</h3>
<p>In the context of statistical supervised learning, the noiseless linear model
assumes that there exists a deterministic linear relation $Y = \langle
\theta_*, X \rangle$ between the random output $Y$ and the random feature
vector $\Phi(U)$, a potentially non-linear transformation of the inputs $U$. We
analyze the convergence of single-pass, fixed step-size stochastic gradient
descent on the least-square risk under this model. The convergence of the
iterates to the optimum $\theta_*$ and the decay of the generalization error
follow polynomial convergence rates with exponents that both depend on the
regularities of the optimum $\theta_*$ and of the feature vectors $\Phi(u)$. We
interpret our result in the reproducing kernel Hilbert space framework. As a
special case, we analyze an online algorithm for estimating a real function on
the unit interval from the noiseless observation of its value at randomly
sampled points; the convergence depends on the Sobolev smoothness of the
function and of a chosen kernel. Finally, we apply our analysis beyond the
supervised learning setting to obtain convergence rates for the averaging
process (a.k.a. gossip algorithm) on a graph depending on its spectral
dimension.
</p>
<a href="http://arxiv.org/abs/2006.08212" target="_blank">arXiv:2006.08212</a> [<a href="http://arxiv.org/pdf/2006.08212" target="_blank">pdf</a>]

<h2>Deep Network with Approximation Error Being Reciprocal of Width to Power of Square Root of Depth. (arXiv:2006.12231v6 [cs.LG] UPDATED)</h2>
<h3>Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>A new network with super approximation power is introduced. This network is
built with Floor ($\lfloor x\rfloor$) or ReLU ($\max\{0,x\}$) activation
function in each neuron and hence we call such networks Floor-ReLU networks.
For any hyper-parameters $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, it is shown
that Floor-ReLU networks with width $\max\{d,\, 5N+13\}$ and depth $64dL+3$ can
uniformly approximate a H\"older function $f$ on $[0,1]^d$ with an
approximation error $3\lambda d^{\alpha/2}N^{-\alpha\sqrt{L}}$, where $\alpha
\in(0,1]$ and $\lambda$ are the H\"older order and constant, respectively. More
generally for an arbitrary continuous function $f$ on $[0,1]^d$ with a modulus
of continuity $\omega_f(\cdot)$, the constructive approximation rate is
$\omega_f(\sqrt{d}\,N^{-\sqrt{L}})+2\omega_f(\sqrt{d}){N^{-\sqrt{L}}}$. As a
consequence, this new class of networks overcomes the curse of dimensionality
in approximation power when the variation of $\omega_f(r)$ as $r\to 0$ is
moderate (e.g., $\omega_f(r) \lesssim r^\alpha$ for H\"older continuous
functions), since the major term to be considered in our approximation rate is
essentially $\sqrt{d}$ times a function of $N$ and $L$ independent of $d$
within the modulus of continuity.
</p>
<a href="http://arxiv.org/abs/2006.12231" target="_blank">arXiv:2006.12231</a> [<a href="http://arxiv.org/pdf/2006.12231" target="_blank">pdf</a>]

<h2>On The Convergence of First Order Methods for Quasar-Convex Optimization. (arXiv:2010.04937v3 [math.OC] UPDATED)</h2>
<h3>Jikai Jin</h3>
<p>In recent years, the success of deep learning has inspired many researchers
to study the optimization of general smooth non-convex functions. However,
recent works have established pessimistic worst-case complexities for this
class functions, which is in stark contrast with their superior performance in
real-world applications (e.g. training deep neural networks). On the other
hand, it is found that many popular non-convex optimization problems enjoy
certain structured properties which bear some similarities to convexity. In
this paper, we study the class of \textit{quasar-convex functions} to close the
gap between theory and practice. We study the convergence of first order
methods in a variety of different settings and under different optimality
criterions. We prove complexity upper bounds that are similar to standard
results established for convex functions and much better that state-of-the-art
convergence rates of non-convex functions. Overall, this paper suggests that
\textit{quasar-convexity} allows efficient optimization procedures, and we are
looking forward to seeing more problems that demonstrate similar properties in
practice.
</p>
<a href="http://arxiv.org/abs/2010.04937" target="_blank">arXiv:2010.04937</a> [<a href="http://arxiv.org/pdf/2010.04937" target="_blank">pdf</a>]

<h2>Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis. (arXiv:2010.13272v2 [cs.LG] UPDATED)</h2>
<h3>Shaocong Ma, Yi Zhou, Shaofeng Zou</h3>
<p>Variance reduction techniques have been successfully applied to
temporal-difference (TD) learning and help to improve the sample complexity in
policy evaluation. However, the existing work applied variance reduction to
either the less popular one time-scale TD algorithm or the two time-scale GTD
algorithm but with a finite number of i.i.d.\ samples, and both algorithms
apply to only the on-policy setting. In this work, we develop a variance
reduction scheme for the two time-scale TDC algorithm in the off-policy setting
and analyze its non-asymptotic convergence rate over both i.i.d.\ and Markovian
samples. In the i.i.d.\ setting, our algorithm achieves a sample complexity
$O(\epsilon^{-\frac{3}{5}} \log{\epsilon}^{-1})$ that is lower than the
state-of-the-art result $O(\epsilon^{-1} \log {\epsilon}^{-1})$. In the
Markovian setting, our algorithm achieves the state-of-the-art sample
complexity $O(\epsilon^{-1} \log {\epsilon}^{-1})$ that is near-optimal.
Experiments demonstrate that the proposed variance-reduced TDC achieves a
smaller asymptotic convergence error than both the conventional TDC and the
variance-reduced TD.
</p>
<a href="http://arxiv.org/abs/2010.13272" target="_blank">arXiv:2010.13272</a> [<a href="http://arxiv.org/pdf/2010.13272" target="_blank">pdf</a>]

<h2>PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction. (arXiv:2010.13816v1 [cs.CL])</h2>
<h3>Xinyao Ma, Maarten Sap, Hannah Rashkin, Yejin Choi</h3>
<p>Unconscious biases continue to be prevalent in modern text and media, calling
for algorithms that can assist writers with bias correction. For example, a
female character in a story is often portrayed as passive and powerless ("She
daydreams about being a doctor") while a man is portrayed as more proactive and
powerful ("He pursues his dream of being a doctor").

We formulate *Controllable Debiasing*, a new revision task that aims to
rewrite a given text to correct the implicit and potentially undesirable bias
in character portrayals. We then introduce PowerTransformer as an approach that
debiases text through the lens of connotation frames (Sap et al., 2017), which
encode pragmatic knowledge of implied power dynamics with respect to verb
predicates. One key challenge of our task is the lack of parallel corpora. To
address this challenge, we adopt an unsupervised approach using auxiliary
supervision with related tasks such as paraphrasing and self-supervision based
on a reconstruction loss, building on pretrained language models.

Through comprehensive experiments based on automatic and human evaluations,
we demonstrate that our approach outperforms ablations and existing methods
from related tasks. Furthermore, we demonstrate the use of PowerTransformer as
a step toward mitigating the well-documented gender bias in character portrayal
in movie scripts.
</p>
<a href="http://arxiv.org/abs/2010.13816" target="_blank">arXiv:2010.13816</a> [<a href="http://arxiv.org/pdf/2010.13816" target="_blank">pdf</a>]

<h2>Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining. (arXiv:2010.13826v1 [cs.CL])</h2>
<h3>Cheng-I Lai, Yung-Sung Chuang, Hung-Yi Lee, Shang-Wen Li, James Glass</h3>
<p>Much recent work on Spoken Language Understanding (SLU) is limited in at
least one of three ways: models were trained on oracle text input and neglected
ASR errors, models were trained to predict only intents without the slot
values, or models were trained on a large amount of in-house data. In this
paper, we propose a clean and general framework to learn semantics directly
from speech with semi-supervision from transcribed or untranscribed speech to
address these issues. Our framework is built upon pretrained end-to-end (E2E)
ASR and self-supervised language models, such as BERT, and fine-tuned on a
limited amount of target SLU data. We study two semi-supervised settings for
the ASR component: supervised pretraining on transcribed speech, and
unsupervised pretraining by replacing the ASR encoder with self-supervised
speech representations, such as wav2vec. In parallel, we identify two essential
criteria for evaluating SLU models: environmental noise-robustness and E2E
semantics evaluation. Experiments on ATIS show that our SLU framework with
speech as input can perform on par with those using oracle text as input in
semantics understanding, even though environmental noise is present and a
limited amount of labeled semantics data is available for training.
</p>
<a href="http://arxiv.org/abs/2010.13826" target="_blank">arXiv:2010.13826</a> [<a href="http://arxiv.org/pdf/2010.13826" target="_blank">pdf</a>]

<h2>BEAR: Sketching BFGS Algorithm for Ultra-High Dimensional Feature Selection in Sublinear Memory. (arXiv:2010.13829v1 [cs.LG])</h2>
<h3>Amirali Aghazadeh, Vipul Gupta, Alex DeWeese, O. Ozan Koyluoglu, Kannan Ramchandran</h3>
<p>We consider feature selection for applications in machine learning where the
dimensionality of the data is so large that it exceeds the working memory of
the (local) computing machine. Unfortunately, current large-scale sketching
algorithms show poor memory-accuracy trade-off due to the irreversible
collision and accumulation of the stochastic gradient noise in the sketched
domain. Here, we develop a second-order ultra-high dimensional feature
selection algorithm, called BEAR, which avoids the extra collisions by storing
the second-order gradients in the celebrated Broyden-Fletcher-Goldfarb-Shannon
(BFGS) algorithm in Count Sketch, a sublinear memory data structure from the
streaming literature. Experiments on real-world data sets demonstrate that BEAR
requires up to three orders of magnitude less memory space to achieve the same
classification accuracy compared to the first-order sketching algorithms.
Theoretical analysis proves convergence of BEAR with rate O(1/t) in t
iterations of the sketched algorithm. Our algorithm reveals an unexplored
advantage of second-order optimization for memory-constrained sketching of
models trained on ultra-high dimensional data sets.
</p>
<a href="http://arxiv.org/abs/2010.13829" target="_blank">arXiv:2010.13829</a> [<a href="http://arxiv.org/pdf/2010.13829" target="_blank">pdf</a>]

<h2>Proceedings of the AI-HRI Symposium at AAAI-FSS 2020. (arXiv:2010.13830v1 [cs.RO])</h2>
<h3>Shelly Bagchi, Jason R. Wilson, Muneeb I. Ahmad, Christian Dondrup, Zhao Han, Justin W. Hart, Matteo Leonetti, Katrin Lohan, Ross Mead, Emmanuel Senft, Jivko Sinapov, Megan L. Zimmerman</h3>
<p>The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. In that
time, the related topic of trust in robotics has been rapidly growing, with
major research efforts at universities and laboratories across the world.
Indeed, many of the past participants in AI-HRI have been or are now involved
with research into trust in HRI. While trust has no consensus definition, it is
regularly associated with predictability, reliability, inciting confidence, and
meeting expectations. Furthermore, it is generally believed that trust is
crucial for adoption of both AI and robotics, particularly when transitioning
technologies from the lab to industrial, social, and consumer applications.
However, how does trust apply to the specific situations we encounter in the
AI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a
growing need for research that lives directly at the intersection of AI and HRI
that is serviced by this symposium. Over the course of the two-day meeting, we
propose to create a collaborative forum for discussion of current efforts in
trust for AI-HRI, with a sub-session focused on the related topic of
explainable AI (XAI) for HRI.
</p>
<a href="http://arxiv.org/abs/2010.13830" target="_blank">arXiv:2010.13830</a> [<a href="http://arxiv.org/pdf/2010.13830" target="_blank">pdf</a>]

<h2>End-to-End Learning and Intervention in Games. (arXiv:2010.13834v1 [cs.LG])</h2>
<h3>Jiayang Li, Jing Yu, Yu (Marco)Nie, Zhaoran Wang</h3>
<p>In a social system, the self-interest of agents can be detrimental to the
collective good, sometimes leading to social dilemmas. To resolve such a
conflict, a central designer may intervene by either redesigning the system or
incentivizing the agents to change their behaviors. To be effective, the
designer must anticipate how the agents react to the intervention, which is
dictated by their often unknown payoff functions. Therefore, learning about the
agents is a prerequisite for intervention. In this paper, we provide a unified
framework for learning and intervention in games. We cast the equilibria of
games as individual layers and integrate them into an end-to-end optimization
framework. To enable the backward propagation through the equilibria of games,
we propose two approaches, respectively based on explicit and implicit
differentiation. Specifically, we cast the equilibria as the solutions to
variational inequalities (VIs). The explicit approach unrolls the projection
method for solving VIs, while the implicit approach exploits the sensitivity of
the solutions to VIs. At the core of both approaches is the differentiation
through a projection operator. Moreover, we establish the correctness of both
approaches and identify the conditions under which one approach is more
desirable than the other. The analytical results are validated using several
real-world problems.
</p>
<a href="http://arxiv.org/abs/2010.13834" target="_blank">arXiv:2010.13834</a> [<a href="http://arxiv.org/pdf/2010.13834" target="_blank">pdf</a>]

<h2>Application of sequential processing of computer vision methods for solving the problem of detecting the edges of a honeycomb block. (arXiv:2010.13837v1 [cs.CV])</h2>
<h3>M V Kubrikov, I A Paulin, M V Saramud, A S Kubrikova</h3>
<p>The article describes the application of the Hough transform to a honeycomb
block image. The problem of cutting a mold from a honeycomb block is described.
A number of image transformations are considered to increase the efficiency of
the Hough algorithm. A method for obtaining a binary image using a simple
threshold, a method for obtaining a binary image using Otsu binarization, and
the Canny Edge Detection algorithm are considered. The method of binary
skeleton (skeletonization) is considered, in which the skeleton is obtained
using 2 main morphological operations: Dilation and Erosion. As a result of a
number of experiments, the optimal sequence of processing the original image
was revealed, which allows obtaining the coordinates of the maximum number of
faces. This result allows one to choose the optimal places for cutting a
honeycomb block, which will improve the quality of the resulting shapes.
</p>
<a href="http://arxiv.org/abs/2010.13837" target="_blank">arXiv:2010.13837</a> [<a href="http://arxiv.org/pdf/2010.13837" target="_blank">pdf</a>]

<h2>VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning. (arXiv:2010.13839v1 [cs.LG])</h2>
<h3>Thomas Carta, Subhajit Chaudhury, Kartik Talamadupula, Michiaki Tatsubori</h3>
<p>We present VisualHints, a novel environment for multimodal reinforcement
learning (RL) involving text-based interactions along with visual hints
(obtained from the environment). Real-life problems often demand that agents
interact with the environment using both natural language information and
visual perception towards solving a goal. However, most traditional RL
environments either solve pure vision-based tasks like Atari games or
video-based robotic manipulation; or entirely use natural language as a mode of
interaction, like Text-based games and dialog systems. In this work, we aim to
bridge this gap and unify these two approaches in a single environment for
multimodal RL. We introduce an extension of the TextWorld cooking environment
with the addition of visual clues interspersed throughout the environment. The
goal is to force an RL agent to use both text and visual features to predict
natural language action commands for solving the final task of cooking a meal.
We enable variations and difficulties in our environment to emulate various
interactive real-world scenarios. We present a baseline multimodal agent for
solving such problems using CNN-based feature extraction from visual hints and
LSTMs for textual feature extraction. We believe that our proposed
visual-lingual environment will facilitate novel problem settings for the RL
community.
</p>
<a href="http://arxiv.org/abs/2010.13839" target="_blank">arXiv:2010.13839</a> [<a href="http://arxiv.org/pdf/2010.13839" target="_blank">pdf</a>]

<h2>LEAD: Least-Action Dynamics for Min-Max Optimization. (arXiv:2010.13846v1 [cs.LG])</h2>
<h3>Reyhane Askari Hemmat, Amartya Mitra, Guillaume Lajoie, Ioannis Mitliagkas</h3>
<p>Adversarial formulations in machine learning have rekindled interest in
differentiable games. The development of efficient optimization methods for
two-player min-max games is an active area of research with a timely impact on
adversarial formulations including generative adversarial networks (GANs).
Existing methods for this type of problem typically employ intuitive, carefully
hand-designed mechanisms for controlling the problematic rotational dynamics
commonly encountered during optimization. In this work, we take a novel
approach to address this issue by casting min-max optimization as a physical
system. We propose LEAD (Least-Action Dynamics), a second-order optimizer that
uses the principle of least-action from physics to discover an efficient
optimizer for min-max games. We subsequently provide convergence analysis of
our optimizer in quadratic min-max games using the Lyapunov theory. Finally, we
empirically test our method on synthetic problems and GANs to demonstrate
improvements over baseline methods.
</p>
<a href="http://arxiv.org/abs/2010.13846" target="_blank">arXiv:2010.13846</a> [<a href="http://arxiv.org/pdf/2010.13846" target="_blank">pdf</a>]

<h2>Multi-Class Zero-Shot Learning for Artistic Material Recognition. (arXiv:2010.13850v1 [cs.CV])</h2>
<h3>Alexander W Olson, Andreea Cucu, Tom Bock</h3>
<p>Zero-Shot Learning (ZSL) is an extreme form of transfer learning, where no
labelled examples of the data to be classified are provided during the training
stage. Instead, ZSL uses additional information learned about the domain, and
relies upon transfer learning algorithms to infer knowledge about the missing
instances. ZSL approaches are an attractive solution for sparse datasets. Here
we outline a model to identify the materials with which a work of art was
created, by learning the relationship between English descriptions of the
subject of a piece and its composite materials. After experimenting with a
range of hyper-parameters, we produce a model which is capable of correctly
identifying the materials used on pieces from an entirely distinct museum
dataset. This model returned a classification accuracy of 48.42% on 5,000
artworks taken from the Tate collection, which is distinct from the Rijksmuseum
network used to create and train our model.
</p>
<a href="http://arxiv.org/abs/2010.13850" target="_blank">arXiv:2010.13850</a> [<a href="http://arxiv.org/pdf/2010.13850" target="_blank">pdf</a>]

<h2>A State-of-the-Art Review on IoT botnet Attack Detection. (arXiv:2010.13852v1 [cs.NI])</h2>
<h3>Zainab Al-Othman, Mouhammd Alkasassbeh, Sherenaz AL-Haj Baddar</h3>
<p>The Internet as we know it Today, comprises several fundamental interrelated
networks, among which is the Internet of Things (IoT). Despite their
versatility, several IoT devices are vulnerable from a security perspective,
which renders them as a favorable target for multiple security breaches,
especially botnet attacks. In this study, the conceptual frameworks of IoT
botnet attacks will be explored, alongside several machinelearning based botnet
detection techniques. This study also analyzes and contrasts several botnet
Detection techniques based on the Bot-IoT Dataset; a recent realistic IoT
dataset that comprises state-of-the-art IoT botnet attack scenarios.
</p>
<a href="http://arxiv.org/abs/2010.13852" target="_blank">arXiv:2010.13852</a> [<a href="http://arxiv.org/pdf/2010.13852" target="_blank">pdf</a>]

<h2>Improved Supervised Training of Physics-Guided Deep Learning Image Reconstruction with Multi-Masking. (arXiv:2010.13868v1 [eess.IV])</h2>
<h3>Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Mehmet Ak&#xe7;akaya</h3>
<p>Physics-guided deep learning (PG-DL) via algorithm unrolling has received
significant interest for improved image reconstruction, including MRI
applications. These methods unroll an iterative optimization algorithm into a
series of regularizer and data consistency units. The unrolled networks are
typically trained end-to-end using a supervised approach. Current supervised
PG-DL approaches use all of the available sub-sampled measurements in their
data consistency units. Thus, the network learns to fit the rest of the
measurements. In this study, we propose to improve the performance and
robustness of supervised training by utilizing randomness by retrospectively
selecting only a subset of all the available measurements for data consistency
units. The process is repeated multiple times using different random masks
during training for further enhancement. Results on knee MRI show that the
proposed multi-mask supervised PG-DL enhances reconstruction performance
compared to conventional supervised PG-DL approaches.
</p>
<a href="http://arxiv.org/abs/2010.13868" target="_blank">arXiv:2010.13868</a> [<a href="http://arxiv.org/pdf/2010.13868" target="_blank">pdf</a>]

<h2>Examining the causal structures of deep neural networks using information theory. (arXiv:2010.13871v1 [cs.LG])</h2>
<h3>Simon Mattsson, Eric J. Michaud, Erik Hoel</h3>
<p>Deep Neural Networks (DNNs) are often examined at the level of their response
to input, such as analyzing the mutual information between nodes and data sets.
Yet DNNs can also be examined at the level of causation, exploring "what does
what" within the layers of the network itself. Historically, analyzing the
causal structure of DNNs has received less attention than understanding their
responses to input. Yet definitionally, generalizability must be a function of
a DNN's causal structure since it reflects how the DNN responds to unseen or
even not-yet-defined future inputs. Here, we introduce a suite of metrics based
on information theory to quantify and track changes in the causal structure of
DNNs during training. Specifically, we introduce the effective information (EI)
of a feedforward DNN, which is the mutual information between layer input and
output following a maximum-entropy perturbation. The EI can be used to assess
the degree of causal influence nodes and edges have over their downstream
targets in each layer. We show that the EI can be further decomposed in order
to examine the sensitivity of a layer (measured by how well edges transmit
perturbations) and the degeneracy of a layer (measured by how edge overlap
interferes with transmission), along with estimates of the amount of integrated
information of a layer. Together, these properties define where each layer lies
in the "causal plane" which can be used to visualize how layer connectivity
becomes more sensitive or degenerate over time, and how integration changes
during training, revealing how the layer-by-layer causal structure
differentiates. These results may help in understanding the generalization
capabilities of DNNs and provide foundational tools for making DNNs both more
generalizable and more explainable.
</p>
<a href="http://arxiv.org/abs/2010.13871" target="_blank">arXiv:2010.13871</a> [<a href="http://arxiv.org/pdf/2010.13871" target="_blank">pdf</a>]

<h2>Q-FIT: The Quantifiable Feature Importance Technique for Explainable Machine Learning. (arXiv:2010.13872v1 [stat.ML])</h2>
<h3>Kamil Adamczewski, Frederik Harder, Mijung Park</h3>
<p>We introduce a novel framework to quantify the importance of each input
feature for model explainability. A user of our framework can choose between
two modes: (a) global explanation: providing feature importance globally across
all the data points; and (b) local explanation: providing feature importance
locally for each individual data point. The core idea of our method comes from
utilizing the Dirichlet distribution to define a distribution over the
importance of input features. This particular distribution is useful in ranking
the importance of the input features as a sample from this distribution is a
probability vector (i.e., the vector components sum to 1), Thus, the ranking
uncovered by our framework which provides a \textit{quantifiable explanation}
of how significant each input feature is to a model's output. This quantifiable
explainability differentiates our method from existing feature-selection
methods, which simply determine whether a feature is relevant or not.
Furthermore, a distribution over the explanation allows to define a closed-form
divergence to measure the similarity between learned feature importance under
different models. We use this divergence to study how the feature importance
trade-offs with essential notions in modern machine learning, such as privacy
and fairness. We show the effectiveness of our method on a variety of synthetic
and real datasets, taking into account both tabular and image datasets.
</p>
<a href="http://arxiv.org/abs/2010.13872" target="_blank">arXiv:2010.13872</a> [<a href="http://arxiv.org/pdf/2010.13872" target="_blank">pdf</a>]

<h2>MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection. (arXiv:2010.13886v1 [eess.AS])</h2>
<h3>Fei Jia, Somshubra Majumdar, Boris Ginsburg</h3>
<p>We present MarbleNet, an end-to-end neural network for Voice Activity
Detection (VAD). MarbleNet is a deep residual network composed from blocks of
1D time-channel separable convolution, batch-normalization, ReLU and dropout
layers. When compared to a state-of-the-art VAD model, MarbleNet is able to
achieve similar performance with roughly 1/10-th the parameter cost. We further
conduct extensive ablation studies on different training methods and choices of
parameters in order to study the robustness of MarbleNet in real-world VAD
tasks.
</p>
<a href="http://arxiv.org/abs/2010.13886" target="_blank">arXiv:2010.13886</a> [<a href="http://arxiv.org/pdf/2010.13886" target="_blank">pdf</a>]

<h2>How We Refactor and How We Document it? On the Use of Supervised Machine Learning Algorithms to Classify Refactoring Documentation. (arXiv:2010.13890v1 [cs.SE])</h2>
<h3>Eman Abdullah AlOmar, Anthony Peruma, Mohamed Wiem Mkaouer, Christian Newman, Ali Ouni, Marouane Kessentini</h3>
<p>Refactoring is the art of improving the design of a system without altering
its external behavior. Refactoring has become a well established and
disciplined software engineering practice that has attracted a significant
amount of research presuming that refactoring is primarily motivated by the
need to improve system structures. However, recent studies have shown that
developers may incorporate refactorings in other development activities that go
beyond improving the design. Unfortunately, these studies are limited to
developer interviews and a reduced set of projects. To cope with the
above-mentioned limitations, we aim to better understand what motivates
developers to apply refactoring by mining and classifying a large set of
111,884 commits containing refactorings, extracted from 800 Java projects. We
trained a multi-class classifier to categorize these commits into 3 categories,
namely, Internal QA, External QA, and Code Smell Resolution, along with the
traditional BugFix and Functional categories. This classification challenges
the original definition of refactoring, being exclusive to improving the design
and fixing code smells. Further, to better understand our classification
results, we analyzed commit messages to extract textual patterns that
developers regularly use to describe their refactorings. The results show that
(1) fixing code smells is not the main driver for developers to refactoring
their codebases. Refactoring is solicited for a wide variety of reasons, going
beyond its traditional definition; (2) the distribution of refactorings differs
between production and test files; (3) developers use several patterns to
purposefully target refactoring; (4) the textual patterns, extracted from
commit messages, provide better coverage for how developers document their
refactorings.
</p>
<a href="http://arxiv.org/abs/2010.13890" target="_blank">arXiv:2010.13890</a> [<a href="http://arxiv.org/pdf/2010.13890" target="_blank">pdf</a>]

<h2>Financial Data Analysis Using Expert Bayesian Framework For Bankruptcy Prediction. (arXiv:2010.13892v1 [q-fin.ST])</h2>
<h3>Amir Mukeri, Habibullah Shaikh, Dr. D.P. Gaikwad</h3>
<p>In recent years, bankruptcy forecasting has gained lot of attention from
researchers as well as practitioners in the field of financial risk management.
For bankruptcy prediction, various approaches proposed in the past and
currently in practice relies on accounting ratios and using statistical
modeling or machine learning methods.These models have had varying degrees of
successes. Models such as Linear Discriminant Analysis or Artificial Neural
Network employ discriminative classification techniques. They lack explicit
provision to include prior expert knowledge. In this paper, we propose another
route of generative modeling using Expert Bayesian framework. The biggest
advantage of the proposed framework is an explicit inclusion of expert judgment
in the modeling process. Also the proposed methodology provides a way to
quantify uncertainty in prediction. As a result the model built using Bayesian
framework is highly flexible, interpretable and intuitive in nature. The
proposed approach is well suited for highly regulated or safety critical
applications such as in finance or in medical diagnosis. In such cases accuracy
in the prediction is not the only concern for decision makers. Decision makers
and other stakeholders are also interested in uncertainty in the prediction as
well as interpretability of the model. We empirically demonstrate these
benefits of proposed framework using Stan, a probabilistic programming
language. We found that the proposed model is either comparable or superior to
other existing methods. Also resulting model has much less False Positive Rate
compared to many existing state of the art methods. The corresponding R code
for the experiments is available at Github repository.
</p>
<a href="http://arxiv.org/abs/2010.13892" target="_blank">arXiv:2010.13892</a> [<a href="http://arxiv.org/pdf/2010.13892" target="_blank">pdf</a>]

<h2>Incorporating Symbolic Domain Knowledge into Graph Neural Networks. (arXiv:2010.13900v1 [cs.LG])</h2>
<h3>Tirtharaj Dash, Ashwin Srinivasan, Lovekesh Vig</h3>
<p>Our interest is in scientific problems with the following characteristics:
(1) Data are naturally represented as graphs; (2) The amount of data available
is typically small; and (3) There is significant domain-knowledge, usually
expressed in some symbolic form. These kinds of problems have been addressed
effectively in the past by Inductive Logic Programming (ILP), by virtue of 2
important characteristics: (a) The use of a representation language that easily
captures the relation encoded in graph-structured data, and (b) The inclusion
of prior information encoded as domain-specific relations, that can alleviate
problems of data scarcity, and construct new relations. Recent advances have
seen the emergence of deep neural networks specifically developed for
graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have
been shown to be able to handle graph-structured data, less has been done to
investigate the inclusion of domain-knowledge. Here we investigate this aspect
of GNNs empirically by employing an operation we term "vertex-enrichment" and
denote the corresponding GNNs as "VEGNNs". Using over 70 real-world datasets
and substantial amounts of symbolic domain-knowledge, we examine the result of
vertex-enrichment across 5 different variants of GNNs. Our results provide
support for the following: (a) Inclusion of domain-knowledge by
vertex-enrichment can significantly improve the performance of a GNN. That is,
the performance VEGNNs is significantly better than GNNs across all GNN
variants; (b) The inclusion of domain-specific relations constructed using ILP
improves the performance of VEGNNs, across all GNN variants. Taken together,
the results provide evidence that it is possible to incorporate symbolic domain
knowledge into a GNN, and that ILP can play an important role in providing
high-level relationships that are not easily discovered by a GNN.
</p>
<a href="http://arxiv.org/abs/2010.13900" target="_blank">arXiv:2010.13900</a> [<a href="http://arxiv.org/pdf/2010.13900" target="_blank">pdf</a>]

<h2>An investigation of Modern Foreign Language (MFL) teachers and their cognitions of Computer Assisted Language Learning (CALL) amid the COVID-19 health pandemic. (arXiv:2010.13901v1 [cs.HC])</h2>
<h3>Louise Hanna, David Barr, Helen Hou, Shauna McGill</h3>
<p>A study was performed with 33 Modern Foreign Language (MFL) teachers to
afford insight into how classroom practitioners interact with Computer Assisted
Language Learning (CALL) in Second Language (L2) pedagogy. A questionnaire with
CALL specific statements was completed by MFL teachers who were recruited via
UK based Facebook groups. Significantly, participants acknowledged a gap in
practice from the expectation of CALL in the MFL classroom. Overall,
respondents were shown to be interested and regular consumers of CALL who
perceived its ease and importance in L2 teaching and learning.
</p>
<a href="http://arxiv.org/abs/2010.13901" target="_blank">arXiv:2010.13901</a> [<a href="http://arxiv.org/pdf/2010.13901" target="_blank">pdf</a>]

<h2>Graph Contrastive Learning with Augmentations. (arXiv:2010.13902v1 [cs.LG])</h2>
<h3>Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen</h3>
<p>Generalizable, transferrable, and robust representation learning on
graph-structured data remains a challenge for current graph neural networks
(GNNs). Unlike what has been developed for convolutional neural networks (CNNs)
for image data, self-supervised learning and pre-training are less explored for
GNNs. In this paper, we propose a graph contrastive learning (GraphCL)
framework for learning unsupervised representations of graph data. We first
design four types of graph augmentations to incorporate various priors. We then
systematically study the impact of various combinations of graph augmentations
on multiple datasets, in four different settings: semi-supervised,
unsupervised, and transfer learning as well as adversarial attacks. The results
show that, even without tuning augmentation extents nor using sophisticated GNN
architectures, our GraphCL framework can produce graph representations of
similar or better generalizability, transferrability, and robustness compared
to state-of-the-art methods. We also investigate the impact of parameterized
graph augmentation extents and patterns, and observe further performance gains
in preliminary experiments. Our codes are available at
https://github.com/Shen-Lab/GraphCL.
</p>
<a href="http://arxiv.org/abs/2010.13902" target="_blank">arXiv:2010.13902</a> [<a href="http://arxiv.org/pdf/2010.13902" target="_blank">pdf</a>]

<h2>T$^2$-Net: A Semi-supervised Deep Model for Turbulence Forecasting. (arXiv:2010.13903v1 [cs.LG])</h2>
<h3>Denghui Zhang, Yanchi Liu, Wei Cheng, Bo Zong, Jingchao Ni, Zhengzhang Chen, Haifeng Chen, Hui Xiong</h3>
<p>Accurate air turbulence forecasting can help airlines avoid hazardous
turbulence, guide the routes that keep passengers safe, maximize efficiency,
and reduce costs. Traditional turbulence forecasting approaches heavily rely on
painstakingly customized turbulence indexes, which are less effective in
dynamic and complex weather conditions. The recent availability of
high-resolution weather data and turbulence records allows more accurate
forecasting of the turbulence in a data-driven way. However, it is a
non-trivial task for developing a machine learning based turbulence forecasting
system due to two challenges: (1) Complex spatio-temporal correlations,
turbulence is caused by air movement with complex spatio-temporal patterns, (2)
Label scarcity, very limited turbulence labels can be obtained. To this end, in
this paper, we develop a unified semi-supervised framework, T$^2$-Net, to
address the above challenges. Specifically, we first build an encoder-decoder
paradigm based on the convolutional LSTM to model the spatio-temporal
correlations. Then, to tackle the label scarcity problem, we propose a novel
Dual Label Guessing method to take advantage of massive unlabeled turbulence
data. It integrates complementary signals from the main Turbulence Forecasting
task and the auxiliary Turbulence Detection task to generate pseudo-labels,
which are dynamically utilized as additional training data. Finally, extensive
experimental results on a real-world turbulence dataset validate the
superiority of our method on turbulence forecasting.
</p>
<a href="http://arxiv.org/abs/2010.13903" target="_blank">arXiv:2010.13903</a> [<a href="http://arxiv.org/pdf/2010.13903" target="_blank">pdf</a>]

<h2>Improving Limited Labeled Dialogue State Tracking with Self-Supervision. (arXiv:2010.13920v1 [cs.CL])</h2>
<h3>Chien-Sheng Wu, Steven Hoi, Caiming Xiong</h3>
<p>Existing dialogue state tracking (DST) models require plenty of labeled data.
However, collecting high-quality labels is costly, especially when the number
of domains increases. In this paper, we address a practical DST problem that is
rarely discussed, i.e., learning efficiently with limited labeled data. We
present and investigate two self-supervised objectives: preserving latent
consistency and modeling conversational behavior. We encourage a DST model to
have consistent latent distributions given a perturbed input, making it more
robust to an unseen scenario. We also add an auxiliary utterance generation
task, modeling a potential correlation between conversational behavior and
dialogue states. The experimental results show that our proposed
self-supervised signals can improve joint goal accuracy by 8.95\% when only 1\%
labeled data is used on the MultiWOZ dataset. We can achieve an additional
1.76\% improvement if some unlabeled data is jointly trained as semi-supervised
learning. We analyze and visualize how our proposed self-supervised signals
help the DST task and hope to stimulate future data-efficient DST research.
</p>
<a href="http://arxiv.org/abs/2010.13920" target="_blank">arXiv:2010.13920</a> [<a href="http://arxiv.org/pdf/2010.13920" target="_blank">pdf</a>]

<h2>Benchmarking Deep Learning Interpretability in Time Series Predictions. (arXiv:2010.13924v1 [cs.LG])</h2>
<h3>Aya Abdelsalam Ismail, Mohamed Gunady, H&#xe9;ctor Corrada Bravo, Soheil Feizi</h3>
<p>Saliency methods are used extensively to highlight the importance of input
features in model predictions. These methods are mostly used in vision and
language tasks, and their applications to time series data is relatively
unexplored. In this paper, we set out to extensively compare the performance of
various saliency-based interpretability methods across diverse neural
architectures, including Recurrent Neural Network, Temporal Convolutional
Networks, and Transformers in a new benchmark of synthetic time series data. We
propose and report multiple metrics to empirically evaluate the performance of
saliency methods for detecting feature importance over time using both
precision (i.e., whether identified features contain meaningful signals) and
recall (i.e., the number of features with signal identified as important).
Through several experiments, we show that (i) in general, network architectures
and saliency methods fail to reliably and accurately identify feature
importance over time in time series data, (ii) this failure is mainly due to
the conflation of time and feature domains, and (iii) the quality of saliency
maps can be improved substantially by using our proposed two-step temporal
saliency rescaling (TSR) approach that first calculates the importance of each
time step before calculating the importance of each feature at a time step.
</p>
<a href="http://arxiv.org/abs/2010.13924" target="_blank">arXiv:2010.13924</a> [<a href="http://arxiv.org/pdf/2010.13924" target="_blank">pdf</a>]

<h2>HarperValleyBank: A Domain-Specific Spoken Dialog Corpus. (arXiv:2010.13929v1 [cs.LG])</h2>
<h3>Mike Wu, Jonathan Nafziger, Anthony Scodary, Andrew Maas</h3>
<p>We introduce HarperValleyBank, a free, public domain spoken dialog corpus.
The data simulate simple consumer banking interactions, containing about 23
hours of audio from 1,446 human-human conversations between 59 unique speakers.
We selected intents and utterance templates to allow realistic variation while
controlling overall task complexity and limiting vocabulary size to about 700
unique words. We provide audio data along with transcripts and annotations for
speaker ID, caller intent, dialog actions, and emotional valence. The size and
domain specificity of this data makes for quick experiments with modern
end-to-end neural approaches. Further, we provide baselines for representation
learning and transfer tasks. These experiments adapt recent work to embed
utterances and use the resulting representations in prediction tasks. Our
experiments show that tasks using our annotations are sensitive to both the
model choice and corpus size for representation learning approaches.
</p>
<a href="http://arxiv.org/abs/2010.13929" target="_blank">arXiv:2010.13929</a> [<a href="http://arxiv.org/pdf/2010.13929" target="_blank">pdf</a>]

<h2>Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models. (arXiv:2010.13933v1 [stat.ML])</h2>
<h3>Jason W. Rocks, Pankaj Mehta</h3>
<p>The bias-variance trade-off is a central concept in supervised learning. In
classical statistics, increasing the complexity of a model (e.g., number of
parameters) reduces bias but also increases variance. Until recently, it was
commonly believed that optimal performance is achieved at intermediate model
complexities which strike a balance between bias and variance. Modern Deep
Learning methods flout this dogma, achieving state-of-the-art performance using
"over-parameterized models" where the number of fit parameters is large enough
to perfectly fit the training data. As a result, understanding bias and
variance in over-parameterized models has emerged as a fundamental problem in
machine learning. Here, we use methods from statistical physics to derive
analytic expressions for bias and variance in three minimal models for
over-parameterization (linear regression and two-layer neural networks with
linear and nonlinear activation functions), allowing us to disentangle
properties stemming from the model architecture and random sampling of data.
All three models exhibit a phase transition to an interpolation regime where
the training error is zero, with linear neural-networks possessing an
additional phase transition between regimes with zero and nonzero bias. The
test error diverges at the interpolation transition for all three models.
However, beyond the transition, it decreases again for the neural network
models due to a decrease in both bias and variance with model complexity. We
also show that over-parameterized models can overfit even in the absence of
noise. We synthesize these results to construct a holistic understanding of
generalization error and the bias-variance trade-off in over-parameterized
models.
</p>
<a href="http://arxiv.org/abs/2010.13933" target="_blank">arXiv:2010.13933</a> [<a href="http://arxiv.org/pdf/2010.13933" target="_blank">pdf</a>]

<h2>Neural Unsigned Distance Fields for Implicit Function Learning. (arXiv:2010.13938v1 [cs.CV])</h2>
<h3>Julian Chibane, Aymen Mir, Gerard Pons-Moll</h3>
<p>In this work we target a learnable output representation that allows
continuous, high resolution outputs of arbitrary shape. Recent works represent
3D surfaces implicitly with a Neural Network, thereby breaking previous
barriers in resolution, and ability to represent diverse topologies. However,
neural implicit representations are limited to closed surfaces, which divide
the space into inside and outside. Many real world objects such as walls of a
scene scanned by a sensor, clothing, or a car with inner structures are not
closed. This constitutes a significant barrier, in terms of data pre-processing
(objects need to be artificially closed creating artifacts), and the ability to
output open surfaces. In this work, we propose Neural Distance Fields (NDF), a
neural network based model which predicts the unsigned distance field for
arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high
resolutions as prior implicit models, but do not require closed surface data,
and significantly broaden the class of representable shapes in the output. NDF
allow to extract the surface as very dense point clouds and as meshes. We also
show that NDF allow for surface normal calculation and can be rendered using a
slight modification of sphere tracing. We find NDF can be used for multi-target
regression (multiple outputs for one input) with techniques that have been
exclusively used for rendering in graphics. Experiments on ShapeNet show that
NDF, while simple, is the state-of-the art, and allows to reconstruct shapes
with inner structures, such as the chairs inside a bus. Notably, we show that
NDF are not restricted to 3D shapes, and can approximate more general open
surfaces such as curves, manifolds, and functions. Code is available for
research at https://virtualhumans.mpi-inf.mpg.de/ndf/.
</p>
<a href="http://arxiv.org/abs/2010.13938" target="_blank">arXiv:2010.13938</a> [<a href="http://arxiv.org/pdf/2010.13938" target="_blank">pdf</a>]

<h2>An Adversarial Domain Separation Framework for Septic Shock Early Prediction Across EHR Systems. (arXiv:2010.13952v1 [cs.LG])</h2>
<h3>Farzaneh Khoshnevisan, Min Chi</h3>
<p>Modeling patient disease progression using Electronic Health Records (EHRs)
is critical to assist clinical decision making. While most of prior work has
mainly focused on developing effective disease progression models using EHRs
collected from an individual medical system, relatively little work has
investigated building robust yet generalizable diagnosis models across
different systems. In this work, we propose a general domain adaptation (DA)
framework that tackles two categories of discrepancies in EHRs collected from
different medical systems: one is caused by heterogeneous patient populations
(covariate shift) and the other is caused by variations in data collection
procedures (systematic bias). Prior research in DA has mainly focused on
addressing covariate shift but not systematic bias. In this work, we propose an
adversarial domain separation framework that addresses both categories of
discrepancies by maintaining one globally-shared invariant latent
representation across all systems} through an adversarial learning process,
while also allocating a domain-specific model for each system to extract local
latent representations that cannot and should not be unified across systems.
Moreover, our proposed framework is based on variational recurrent neural
network (VRNN) because of its ability to capture complex temporal dependencies
and handling missing values in time-series data. We evaluate our framework for
early diagnosis of an extremely challenging condition, septic shock, using two
real-world EHRs from distinct medical systems in the U.S. The results show that
by separating globally-shared from domain-specific representations, our
framework significantly improves septic shock early prediction performance in
both EHRs and outperforms the current state-of-the-art DA models.
</p>
<a href="http://arxiv.org/abs/2010.13952" target="_blank">arXiv:2010.13952</a> [<a href="http://arxiv.org/pdf/2010.13952" target="_blank">pdf</a>]

<h2>MELD: Meta-Reinforcement Learning from Images via Latent State Models. (arXiv:2010.13957v1 [cs.LG])</h2>
<h3>Tony Z. Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, Sergey Levine</h3>
<p>Meta-reinforcement learning algorithms can enable autonomous agents, such as
robots, to quickly acquire new behaviors by leveraging prior experience in a
set of related training tasks. However, the onerous data requirements of
meta-training compounded with the challenge of learning from sensory inputs
such as images have made meta-RL challenging to apply to real robotic systems.
Latent state models, which learn compact state representations from a sequence
of observations, can accelerate representation learning from visual inputs. In
this paper, we leverage the perspective of meta-learning as task inference to
show that latent state models can \emph{also} perform meta-learning given an
appropriately defined observation space. Building on this insight, we develop
meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that
performs inference in a latent state model to quickly acquire new skills given
observations and rewards. MELD outperforms prior meta-RL methods on several
simulated image-based robotic control problems, and enables a real WidowX
robotic arm to insert an Ethernet cable into new locations given a sparse task
completion signal after only $8$ hours of real world meta-training. To our
knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic
control setting from images.
</p>
<a href="http://arxiv.org/abs/2010.13957" target="_blank">arXiv:2010.13957</a> [<a href="http://arxiv.org/pdf/2010.13957" target="_blank">pdf</a>]

<h2>Impact of Spherical Coordinates Transformation Pre-processing in Deep Convolution Neural Networks for Brain Tumor Segmentation and Survival Prediction. (arXiv:2010.13967v1 [eess.IV])</h2>
<h3>Carlo Russo, Sidong Liu, Antonio Di Ieva</h3>
<p>Pre-processing and Data Augmentation play an important role in Deep
Convolutional Neural Networks (DCNN). Whereby several methods aim for
standardization and augmentation of the dataset, we here propose a novel method
aimed to feed DCNN with spherical space transformed input data that could
better facilitate feature learning compared to standard Cartesian space images
and volumes. In this work, the spherical coordinates transformation has been
applied as a preprocessing method that, used in conjunction with normal MRI
volumes, improves the accuracy of brain tumor segmentation and patient overall
survival (OS) prediction on Brain Tumor Segmentation (BraTS) Challenge 2020
dataset. The LesionEncoder framework has been then applied to automatically
extract features from DCNN models, achieving 0.586 accuracy of OS prediction on
the validation data set, which is one of the best results according to BraTS
2020 leaderboard.
</p>
<a href="http://arxiv.org/abs/2010.13967" target="_blank">arXiv:2010.13967</a> [<a href="http://arxiv.org/pdf/2010.13967" target="_blank">pdf</a>]

<h2>GPUTreeShap: Fast Parallel Tree Interpretability. (arXiv:2010.13972v1 [cs.LG])</h2>
<h3>Rory Mitchell, Eibe Frank, Geoffrey Holmes</h3>
<p>SHAP (SHapley Additive exPlanation) values provide a game theoretic
interpretation of the predictions of machine learning models based on Shapley
values. While SHAP values are intractable in general, a recursive polynomial
time algorithm specialised for decision tree models is available, named
TreeShap. Despite its polynomial time complexity, TreeShap can become a
significant bottleneck in practical machine learning pipelines when applied to
large decision tree ensembles. We present GPUTreeShap, a software package
implementing a modified TreeShap algorithm in CUDA for Nvidia GPUs. Our
approach first preprocesses the input model to isolate variable sized
sub-problems from the original recursive algorithm, then solves a bin packing
problem, and finally maps sub-problems to streaming multiprocessors for
parallel execution with specialised hardware instructions. With a single GPU,
we achieve speedups of up to 19x for SHAP values, and 340x for SHAP interaction
values, over a state-of-the-art multi-core CPU implementation. We also
experiment with an 8 GPU DGX-1 system, demonstrating throughput of 1.2M rows
per second---equivalent CPU-based performance is estimated to require 6850 CPU
cores.
</p>
<a href="http://arxiv.org/abs/2010.13972" target="_blank">arXiv:2010.13972</a> [<a href="http://arxiv.org/pdf/2010.13972" target="_blank">pdf</a>]

<h2>Wearing a MASK: Compressed Representations of Variable-Length Sequences Using Recurrent Neural Tangent Kernels. (arXiv:2010.13975v1 [eess.SP])</h2>
<h3>Sina Alemohammad, Hossein Babaei, Randall Balestriero, Matt Y. Cheung, Ahmed Imtiaz Humayun, Daniel LeJeune, Naiming Liu, Lorenzo Luzi, Jasper Tan, Zichao Wang, Richard G. Baraniuk</h3>
<p>High dimensionality poses many challenges to the use of data, from
visualization and interpretation, to prediction and storage for historical
preservation. Techniques abound to reduce the dimensionality of fixed-length
sequences, yet these methods rarely generalize to variable-length sequences. To
address this gap, we extend existing methods that rely on the use of kernels to
variable-length sequences via use of the Recurrent Neural Tangent Kernel
(RNTK). Since a deep neural network with ReLu activation is a Max-Affine Spline
Operator (MASO), we dub our approach Max-Affine Spline Kernel (MASK). We
demonstrate how MASK can be used to extend principal components analysis (PCA)
and t-distributed stochastic neighbor embedding (t-SNE) and apply these new
algorithms to separate synthetic time series data sampled from second-order
differential equations.
</p>
<a href="http://arxiv.org/abs/2010.13975" target="_blank">arXiv:2010.13975</a> [<a href="http://arxiv.org/pdf/2010.13975" target="_blank">pdf</a>]

<h2>A Neuro-Symbolic Humanlike Arm Controller for Sophia the Robot. (arXiv:2010.13983v1 [cs.RO])</h2>
<h3>David Hanson (Hanson Robotics), Alishba Imran (Hanson Robotics), Abhinandan Vellanki (Hanson Robotics), Sanjeew Kanagaraj (Hanson Robotics)</h3>
<p>We outline the design and construction of novel robotic arms using machine
perception, convolutional neural networks, and symbolic AI for logical control
and affordance indexing. We describe our robotic arms built with a humanlike
mechanical configuration and aesthetic, with 28 degrees of freedom, touch
sensors, and series elastic actuators. The arms were modelled in Roodle and
Gazebo with URDF models, as well as Unity, and implement motion control
solutions for solving live games of Baccarat (the casino card game), rock paper
scissors, handshaking, and drawing. This includes live interactions with
people, incorporating both social control of the hands and facial gestures, and
physical inverse kinematics (IK) for grasping and manipulation tasks. The
resulting framework is an integral part of the Sophia 2020 alpha platform,
which is being used with ongoing research in the authors work with team AHAM,
an ANA Avatar Xprize effort towards human-AI hybrid telepresence. These results
are available to test on the broadly released Hanson Robotics Sophia 2020 robot
platform, for users to try and extend.
</p>
<a href="http://arxiv.org/abs/2010.13983" target="_blank">arXiv:2010.13983</a> [<a href="http://arxiv.org/pdf/2010.13983" target="_blank">pdf</a>]

<h2>Interpretation of NLP models through input marginalization. (arXiv:2010.13984v1 [cs.CL])</h2>
<h3>Siwon Kim, Jihun Yi, Eunji Kim, Sungroh Yoon</h3>
<p>To demystify the "black box" property of deep neural networks for natural
language processing (NLP), several methods have been proposed to interpret
their predictions by measuring the change in prediction probability after
erasing each token of an input. Since existing methods replace each token with
a predefined value (i.e., zero), the resulting sentence lies out of the
training data distribution, yielding misleading interpretations. In this study,
we raise the out-of-distribution problem induced by the existing interpretation
methods and present a remedy; we propose to marginalize each token out. We
interpret various NLP models trained for sentiment analysis and natural
language inference using the proposed method.
</p>
<a href="http://arxiv.org/abs/2010.13984" target="_blank">arXiv:2010.13984</a> [<a href="http://arxiv.org/pdf/2010.13984" target="_blank">pdf</a>]

<h2>Toward Better Generalization Bounds with Locally Elastic Stability. (arXiv:2010.13988v1 [cs.LG])</h2>
<h3>Zhun Deng, Hangfeng He, Weijie J. Su</h3>
<p>Classical approaches in learning theory are often seen to yield very loose
generalization bounds for deep neural networks. Using the example of "stability
and generalization" \citep{bousquet2002stability}, however, we demonstrate that
generalization bounds can be significantly improved by taking into account
refined characteristics of modern neural networks. Specifically, this paper
proposes a new notion of algorithmic stability termed \textit{locally elastic
stability} in light of a certain phenomenon in the training of neural networks
\citep{he2020local}. We prove that locally elastic stability implies a tighter
generalization bound than that derived based on uniform stability in many
situations. When applied to deep neural networks, our new generalization bound
attaches much more meaningful confidence statements to the performance on
unseen data than existing algorithmic stability notions, thereby shedding light
on the effectiveness of modern neural networks in real-world applications.
</p>
<a href="http://arxiv.org/abs/2010.13988" target="_blank">arXiv:2010.13988</a> [<a href="http://arxiv.org/pdf/2010.13988" target="_blank">pdf</a>]

<h2>Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning. (arXiv:2010.13991v1 [cs.CL])</h2>
<h3>Dongwei Jiang, Wubo Li, Miao Cao, Ruixiong Zhang, Wei Zou, Kun Han, Xiangang Li</h3>
<p>Self-supervised visual pretraining has shown significant progress recently.
Among those methods, SimCLR greatly advanced the state of the art in
self-supervised and semi-supervised learning on ImageNet. The input feature
representations for speech and visual tasks are both continuous, so it is
natural to consider applying similar objective on speech representation
learning. In this paper, we propose Speech SimCLR, a new self-supervised
objective for speech representation learning. During training, Speech SimCLR
applies augmentation on raw speech and its spectrogram. Its objective is the
combination of contrastive loss that maximizes agreement between differently
augmented samples in the latent space and reconstruction loss of input
representation. The proposed method achieved competitive results on speech
emotion recognition and speech recognition. When used as feature extractor, our
best model achieved 5.89% word error rate on LibriSpeech test-clean set using
LibriSpeech 960 hours as pretraining data and LibriSpeech train-clean-100 set
as fine-tuning data, which is the lowest error rate obtained in this setup to
the best of our knowledge.
</p>
<a href="http://arxiv.org/abs/2010.13991" target="_blank">arXiv:2010.13991</a> [<a href="http://arxiv.org/pdf/2010.13991" target="_blank">pdf</a>]

<h2>Combining Label Propagation and Simple Models Out-performs Graph Neural Networks. (arXiv:2010.13993v1 [cs.LG])</h2>
<h3>Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, Austin R. Benson</h3>
<p>Graph Neural Networks (GNNs) are the predominant technique for learning over
graphs. However, there is relatively little understanding of why GNNs are
successful in practice and whether they are necessary for good performance.
Here, we show that for many standard transductive node classification
benchmarks, we can exceed or match the performance of state-of-the-art GNNs by
combining shallow models that ignore the graph structure with two simple
post-processing steps that exploit correlation in the label structure: (i) an
"error correlation" that spreads residual errors in training data to correct
errors in test data and (ii) a "prediction correlation" that smooths the
predictions on the test data. We call this overall procedure Correct and Smooth
(C&amp;S), and the post-processing steps are implemented via simple modifications
to standard label propagation techniques from early graph-based semi-supervised
learning methods. Our approach exceeds or nearly matches the performance of
state-of-the-art GNNs on a wide variety of benchmarks, with just a small
fraction of the parameters and orders of magnitude faster runtime. For
instance, we exceed the best known GNN performance on the OGB-Products dataset
with 137 times fewer parameters and greater than 100 times less training time.
The performance of our methods highlights how directly incorporating label
information into the learning algorithm (as was done in traditional techniques)
yields easy and substantial performance gains. We can also incorporate our
techniques into big GNN models, providing modest gains. Our code for the OGB
results is at https://github.com/Chillee/CorrectAndSmooth.
</p>
<a href="http://arxiv.org/abs/2010.13993" target="_blank">arXiv:2010.13993</a> [<a href="http://arxiv.org/pdf/2010.13993" target="_blank">pdf</a>]

<h2>One-class learning towards generalized voice spoofing detection. (arXiv:2010.13995v1 [eess.AS])</h2>
<h3>You Zhang, Fei Jiang, Zhiyao Duan</h3>
<p>Human voices can be used to authenticate the identity of the speaker, but the
automatic speaker verification (ASV) systems are vulnerable to voice spoofing
attacks, such as impersonation, replay, text-to-speech, and voice conversion.
Recently, researchers developed anti-spoofing techniques to improve the
reliability of ASV systems against spoofing attacks. However, most methods
encounter difficulties in detecting unknown attacks in practical use, which
often have different statistical distributions from known attacks. In this
work, we propose an anti-spoofing system to detect unknown logical access
attacks (i.e., synthetic speech) using one-class learning. The key idea is to
compact the genuine speech representation and inject an angular margin to
separate the spoofing attacks in the embedding space. Our system achieves an
equal error rate of 2.19% on the evaluation set of ASVspoof 2019 Challenge,
outperforming all existing single systems.
</p>
<a href="http://arxiv.org/abs/2010.13995" target="_blank">arXiv:2010.13995</a> [<a href="http://arxiv.org/pdf/2010.13995" target="_blank">pdf</a>]

<h2>A Computationally Efficient Approach to Black-box Optimization using Gaussian Process Models. (arXiv:2010.13997v1 [stat.ML])</h2>
<h3>Sudeep Salgia, Sattar Vakili, Qing Zhao</h3>
<p>We consider the sequential optimization of an unknown function from noisy
feedback using Gaussian process modeling. A prevailing approach to this problem
involves choosing query points based on finding the maximum of an upper
confidence bound (UCB) score over the entire domain of the function. Due to the
multi-modal nature of the UCB, this maximization can only be approximated,
usually using an increasingly fine sequence of discretizations of the entire
domain, making such methods computationally prohibitive. We propose a general
approach that reduces the computational complexity of this class of algorithms
by a factor of $O(T^{2d-1})$ (where $T$ is the time horizon and $d$ the
dimension of the function domain), while preserving the same regret order. The
significant reduction in computational complexity results from two key features
of the proposed approach: (i) a tree-based localized search strategy rooted in
the methodology of domain shrinking to achieve increasing accuracy with a
constant-size discretization; (ii) a localized optimization with the objective
relaxed from a global maximizer to any point with value exceeding a given
threshold, where the threshold is updated iteratively to approach the maximum
as the search deepens. More succinctly, the proposed optimization strategy is a
sequence of localized searches in the domain of the function guided by an
iterative search in the range of the function to approach the maximum.
</p>
<a href="http://arxiv.org/abs/2010.13997" target="_blank">arXiv:2010.13997</a> [<a href="http://arxiv.org/pdf/2010.13997" target="_blank">pdf</a>]

<h2>Graph-based Reinforcement Learning for Active Learning in Real Time: An Application in Modeling River Networks. (arXiv:2010.14000v1 [cs.LG])</h2>
<h3>Xiaowei Jia, Beiyu Lin, Jacob Zwart, Jeffery Sadler, Alison Appling, Samantha Oliver, Jordan Read</h3>
<p>Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.
</p>
<a href="http://arxiv.org/abs/2010.14000" target="_blank">arXiv:2010.14000</a> [<a href="http://arxiv.org/pdf/2010.14000" target="_blank">pdf</a>]

<h2>ByteCover: Cover Song Identification via Multi-Loss Training. (arXiv:2010.14022v1 [cs.SD])</h2>
<h3>Xingjian Du, Zhesong Yu, Bilei Zhu, Xiaoou Chen, Zejun Ma</h3>
<p>We present in this paper ByteCover, which is a new feature learning method
for cover song identification (CSI). ByteCover is built based on the classical
ResNet model, and two major improvements are designed to further enhance the
capability of the model for CSI. In the first improvement, we introduce the
integration of instance normalization (IN) and batch normalization (BN) to
build IBN blocks, which are major components of our ResNet-IBN model. With the
help of the IBN blocks, our CSI model can learn features that are invariant to
the changes of musical attributes such as key, tempo, timbre and genre, while
preserving the version information. In the second improvement, we employ the
BNNeck method to allow a multi-loss training and encourage our method to
jointly optimize a classification loss and a triplet loss, and by this means,
the inter-class discrimination and intra-class compactness of cover songs, can
be ensured at the same time. A set of experiments demonstrated the
effectiveness and efficiency of ByteCover on multiple datasets, and in the
Da-TACOS dataset, ByteCover outperformed the best competitive system by 20.9\%.
</p>
<a href="http://arxiv.org/abs/2010.14022" target="_blank">arXiv:2010.14022</a> [<a href="http://arxiv.org/pdf/2010.14022" target="_blank">pdf</a>]

<h2>FaceLeaks: Inference Attacks against Transfer Learning Models via Black-box Queries. (arXiv:2010.14023v1 [cs.LG])</h2>
<h3>Seng Pei Liew, Tsubasa Takahashi</h3>
<p>Transfer learning is a useful machine learning framework that allows one to
build task-specific models (student models) without significantly incurring
training costs using a single powerful model (teacher model) pre-trained with a
large amount of data. The teacher model may contain private data, or interact
with private inputs. We investigate if one can leak or infer such private
information without interacting with the teacher model directly. We describe
such inference attacks in the context of face recognition, an application of
transfer learning that is highly sensitive to personal privacy.

Under black-box and realistic settings, we show that existing inference
techniques are ineffective, as interacting with individual training instances
through the student models does not reveal information about the teacher. We
then propose novel strategies to infer from aggregate-level information.
Consequently, membership inference attacks on the teacher model are shown to be
possible, even when the adversary has access only to the student models.

We further demonstrate that sensitive attributes can be inferred, even in the
case where the adversary has limited auxiliary information. Finally, defensive
strategies are discussed and evaluated. Our extensive study indicates that
information leakage is a real privacy threat to the transfer learning framework
widely used in real-life situations.
</p>
<a href="http://arxiv.org/abs/2010.14023" target="_blank">arXiv:2010.14023</a> [<a href="http://arxiv.org/pdf/2010.14023" target="_blank">pdf</a>]

<h2>Synthetic Training for Monocular Human Mesh Recovery. (arXiv:2010.14036v1 [cs.CV])</h2>
<h3>Yu Sun, Qian Bao, Wu Liu, Wenpeng Gao, Yili Fu, Chuang Gan, Tao Mei</h3>
<p>Recovering 3D human mesh from monocular images is a popular topic in computer
vision and has a wide range of applications. This paper aims to estimate 3D
mesh of multiple body parts (e.g., body, hands) with large-scale differences
from a single RGB image. Existing methods are mostly based on iterative
optimization, which is very time-consuming. We propose to train a single-shot
model to achieve this goal. The main challenge is lacking training data that
have complete 3D annotations of all body parts in 2D images. To solve this
problem, we design a multi-branch framework to disentangle the regression of
different body properties, enabling us to separate each component's training in
a synthetic training manner using unpaired data available. Besides, to
strengthen the generalization ability, most existing methods have used
in-the-wild 2D pose datasets to supervise the estimated 3D pose via 3D-to-2D
projection. However, we observe that the commonly used weak-perspective model
performs poorly in dealing with the external foreshortening effect of camera
projection. Therefore, we propose a depth-to-scale (D2S) projection to
incorporate the depth difference into the projection function to derive
per-joint scale variants for more proper supervision. The proposed method
outperforms previous methods on the CMU Panoptic Studio dataset according to
the evaluation results and achieves comparable results on the Human3.6M body
and STB hand benchmarks. More impressively, the performance in close shot
images gets significantly improved using the proposed D2S projection for weak
supervision, while maintains obvious superiority in computational efficiency.
</p>
<a href="http://arxiv.org/abs/2010.14036" target="_blank">arXiv:2010.14036</a> [<a href="http://arxiv.org/pdf/2010.14036" target="_blank">pdf</a>]

<h2>To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging. (arXiv:2010.14042v1 [cs.CL])</h2>
<h3>Kasturi Bhattacharjee, Miguel Ballesteros, Rishita Anubhai, Smaranda Muresan, Jie Ma, Faisal Ladhak, Yaser Al-Onaizan</h3>
<p>Leveraging large amounts of unlabeled data using Transformer-like
architectures, like BERT, has gained popularity in recent times owing to their
effectiveness in learning general representations that can then be further
fine-tuned for downstream tasks to much success. However, training these models
can be costly both from an economic and environmental standpoint. In this work,
we investigate how to effectively use unlabeled data: by exploring the
task-specific semi-supervised approach, Cross-View Training (CVT) and comparing
it with task-agnostic BERT in multiple settings that include domain and task
relevant English data. CVT uses a much lighter model architecture and we show
that it achieves similar performance to BERT on a set of sequence tagging
tasks, with lesser financial and environmental impact.
</p>
<a href="http://arxiv.org/abs/2010.14042" target="_blank">arXiv:2010.14042</a> [<a href="http://arxiv.org/pdf/2010.14042" target="_blank">pdf</a>]

<h2>Enhanced Classification Accuracy for Cardiotocogram Data with Ensemble Feature Selection and Classifier Ensemble. (arXiv:2010.14051v1 [cs.LG])</h2>
<h3>Tipawan Silwattananusarn, Wanida Kanarkard, Kulthida Tuamsuk</h3>
<p>In this paper ensemble learning based feature selection and classifier
ensemble model is proposed to improve classification accuracy. The hypothesis
is that good feature sets contain features that are highly correlated with the
class from ensemble feature selection to SVM ensembles which can be achieved on
the performance of classification accuracy. The proposed approach consists of
two phases: (i) to select feature sets that are likely to be the support
vectors by applying ensemble based feature selection methods; and (ii) to
construct an SVM ensemble using the selected features. The proposed approach
was evaluated by experiments on Cardiotocography dataset. Four feature
selection techniques were used: (i) Correlation-based, (ii) Consistency-based,
(iii) ReliefF and (iv) Information Gain. Experimental results showed that using
the ensemble of Information Gain feature selection and Correlation-based
feature selection with SVM ensembles achieved higher classification accuracy
than both single SVM classifier and ensemble feature selection with SVM
classifier.
</p>
<a href="http://arxiv.org/abs/2010.14051" target="_blank">arXiv:2010.14051</a> [<a href="http://arxiv.org/pdf/2010.14051" target="_blank">pdf</a>]

<h2>Nonlinear Monte Carlo Method for Imbalanced Data Learning. (arXiv:2010.14060v1 [cs.LG])</h2>
<h3>Xuli Shen, Qing Xu, Xiangyang Xue</h3>
<p>For basic machine learning problems, expected error is used to evaluate model
performance. Since the distribution of data is usually unknown, we can make
simple hypothesis that the data are sampled independently and identically
distributed (i.i.d.) and the mean value of loss function is used as the
empirical risk by Law of Large Numbers (LLN). This is known as the Monte Carlo
method. However, when LLN is not applicable, such as imbalanced data problems,
empirical risk will cause overfitting and might decrease robustness and
generalization ability. Inspired by the framework of nonlinear expectation
theory, we substitute the mean value of loss function with the maximum value of
subgroup mean loss. We call it nonlinear Monte Carlo method. In order to use
numerical method of optimization, we linearize and smooth the functional of
maximum empirical risk and get the descent direction via quadratic programming.
With the proposed method, we achieve better performance than SOTA backbone
models with less training steps, and more robustness for basic regression and
imbalanced classification tasks.
</p>
<a href="http://arxiv.org/abs/2010.14060" target="_blank">arXiv:2010.14060</a> [<a href="http://arxiv.org/pdf/2010.14060" target="_blank">pdf</a>]

<h2>Structure-based Optical Logics Without Using Transistors. (arXiv:2010.14073v1 [cs.ET])</h2>
<h3>Jonghyeon Lee, Taewon Kang</h3>
<p>The commercialization of transistors capable of both switching and
amplification in 1960 resulted in the development of second-generation
computers, which resulted in the miniaturization and lightening while
accelerating the reduction and development of production costs. However, the
self-resistance and the resistance used in conjunction with semiconductors,
which are the basic principles of computers, generate a lot of heat, which
results in semiconductor obsolescence, and limits the computation speed (clock
rate). In implementing logic operation, this paper proposes the concept of
Structure-based Computer which can implement NOT gate made of semiconductor
transistor only by Structure-based twist of cable without resistance. In
Structure-based computer, the theory of 'inverse signal pair' of digital
signals was introduced so that it could operate in a different way than
semiconductor-based transistors. In this paper, we propose a new hardware
called Structure-based computer that can solve various problems in
semiconductor computers only with the wiring structure of the conductor itself,
not with the silicon-based semiconductor. Furthermore, we propose a
deep-priority exploration-based simulation method that can easily implement and
test complex Structure-based computer circuits. Furthermore, this paper
suggests a mechanism to implement optical computers currently under development
and research based on structures rather than devices.
</p>
<a href="http://arxiv.org/abs/2010.14073" target="_blank">arXiv:2010.14073</a> [<a href="http://arxiv.org/pdf/2010.14073" target="_blank">pdf</a>]

<h2>Lattice-based IBE with Equality Test Supporting Flexible Authorization in the Standard Model. (arXiv:2010.14077v1 [cs.CR])</h2>
<h3>Giang L. D. Nguyen, Willy Susilo, Dung Hoang Duong, Huy Quoc Le, Fuchun Guo</h3>
<p>Identity-based encryption with equality test supporting flexible
authorization (IBEET-FA) allows the equality test of underlying messages of two
ciphertexts while strengthens privacy protection by allowing users (identities)
to control the comparison of their ciphertexts with others. IBEET by itself has
a wide range of useful applicable domain such as keyword search on encrypted
data, database partitioning for efficient encrypted data management, personal
health record systems, and spam filtering in encrypted email systems. The
flexible authorization will enhance privacy protection of IBEET. In this paper,
we propose an efficient construction of IBEET-FA system based on the hardness
of learning with error (LWE) problem. Our security proof holds in the standard
model.
</p>
<a href="http://arxiv.org/abs/2010.14077" target="_blank">arXiv:2010.14077</a> [<a href="http://arxiv.org/pdf/2010.14077" target="_blank">pdf</a>]

<h2>Triple-view Convolutional Neural Networks for COVID-19 Diagnosis with Chest X-ray. (arXiv:2010.14091v1 [eess.IV])</h2>
<h3>Jianjia Zhang</h3>
<p>The Coronavirus Disease 2019 (COVID-19) is affecting increasingly large
number of people worldwide, posing significant stress to the health care
systems. Early and accurate diagnosis of COVID-19 is critical in screening of
infected patients and breaking the person-to-person transmission. Chest X-ray
(CXR) based computer-aided diagnosis of COVID-19 using deep learning becomes a
promising solution to this end. However, the diverse and various radiographic
features of COVID-19 make it challenging, especially when considering each CXR
scan typically only generates one single image. Data scarcity is another issue
since collecting large-scale medical CXR data set could be difficult at
present. Therefore, how to extract more informative and relevant features from
the limited samples available becomes essential. To address these issues,
unlike traditional methods processing each CXR image from a single view, this
paper proposes triple-view convolutional neural networks for COVID-19 diagnosis
with CXR images. Specifically, the proposed networks extract individual
features from three views of each CXR image, i.e., the left lung view, the
right lung view and the overall view, in three streams and then integrate them
for joint diagnosis. The proposed network structure respects the anatomical
structure of human lungs and is well aligned with clinical diagnosis of
COVID-19 in practice. In addition, the labeling of the views does not require
experts' domain knowledge, which is needed by many existing methods. The
experimental results show that the proposed method achieves state-of-the-art
performance, especially in the more challenging three class classification
task, and admits wide generality and high flexibility.
</p>
<a href="http://arxiv.org/abs/2010.14091" target="_blank">arXiv:2010.14091</a> [<a href="http://arxiv.org/pdf/2010.14091" target="_blank">pdf</a>]

<h2>A Multi-task Two-stream Spatiotemporal Convolutional Neural Network for Convective Storm Nowcasting. (arXiv:2010.14100v1 [cs.CV])</h2>
<h3>W. Zhang, H. Liu, P. Li, L. Han</h3>
<p>The goal of convective storm nowcasting is local prediction of severe and
imminent convective storms. Here, we consider the convective storm nowcasting
problem from the perspective of machine learning. First, we use a pixel-wise
sampling method to construct spatiotemporal features for nowcasting, and
flexibly adjust the proportions of positive and negative samples in the
training set to mitigate class-imbalance issues. Second, we employ a concise
two-stream convolutional neural network to extract spatial and temporal cues
for nowcasting. This simplifies the network structure, reduces the training
time requirement, and improves classification accuracy. The two-stream network
used both radar and satellite data. In the resulting two-stream, fused
convolutional neural network, some of the parameters are entered into a
single-stream convolutional neural network, but it can learn the features of
many data. Further, considering the relevance of classification and regression
tasks, we develop a multi-task learning strategy that predicts the labels used
in such tasks. We integrate two-stream multi-task learning into a single
convolutional neural network. Given the compact architecture, this network is
more efficient and easier to optimize than existing recurrent neural networks.
</p>
<a href="http://arxiv.org/abs/2010.14100" target="_blank">arXiv:2010.14100</a> [<a href="http://arxiv.org/pdf/2010.14100" target="_blank">pdf</a>]

<h2>Co-attentional Transformers for Story-Based Video Understanding. (arXiv:2010.14104v1 [cs.CV])</h2>
<h3>Bj&#xf6;rn Bebensee, Byoung-Tak Zhang</h3>
<p>Inspired by recent trends in vision and language learning, we explore
applications of attention mechanisms for visio-lingual fusion within an
application to story-based video understanding. Like other video-based QA
tasks, video story understanding requires agents to grasp complex temporal
dependencies. However, as it focuses on the narrative aspect of video it also
requires understanding of the interactions between different characters, as
well as their actions and their motivations. We propose a novel co-attentional
transformer model to better capture long-term dependencies seen in visual
stories such as dramas and measure its performance on the video question
answering task. We evaluate our approach on the recently introduced DramaQA
dataset which features character-centered video story understanding questions.
Our model outperforms the baseline model by 8 percentage points overall, at
least 4.95 and up to 12.8 percentage points on all difficulty levels and
manages to beat the winner of the DramaQA challenge.
</p>
<a href="http://arxiv.org/abs/2010.14104" target="_blank">arXiv:2010.14104</a> [<a href="http://arxiv.org/pdf/2010.14104" target="_blank">pdf</a>]

<h2>Micro-CT Synthesis and Inner Ear Super Resolution via Bayesian Generative Adversarial Networks. (arXiv:2010.14105v1 [eess.IV])</h2>
<h3>Hongwei Li, Rameshwara G. N. Prasad, Anjany Sekuboyina, Chen Niu, Siwei Bai, Werner Hemmert, Bjoern Menze</h3>
<p>Existing medical image super-resolution methods rely on pairs of low- and
high- resolution images to learn a mapping in a fully supervised manner.
However, such image pairs are often not available in clinical practice. In this
paper, we address super resolution problem in a real-world scenario using
unpaired data and synthesize linearly \textbf{eight times} higher resolved
Micro-CT images of temporal bone structure, which is embedded in the inner ear.
We explore cycle-consistency generative adversarial networks for
super-resolution task and equip the translation approach with Bayesian
inference. We further introduce \emph{Hu Moment} the evaluation metric to
quantify the structure of the temporal bone. We evaluate our method on a public
inner ear CT dataset and have seen both visual and quantitative improvement
over state-of-the-art deep-learning based methods. In addition, we perform a
multi-rater visual evaluation experiment and find that trained experts
consistently rate the proposed method highest quality scores among all methods.
Implementing our approach as an end-to-end learning task, we are able to
quantify uncertainty in the unpaired translation tasks and find that the
uncertainty mask can provide structural information of the temporal bone.
</p>
<a href="http://arxiv.org/abs/2010.14105" target="_blank">arXiv:2010.14105</a> [<a href="http://arxiv.org/pdf/2010.14105" target="_blank">pdf</a>]

<h2>A robust low data solution: dimension prediction of semiconductor nanorods. (arXiv:2010.14111v1 [cs.LG])</h2>
<h3>Xiaoli Liu, Yang Xu, Jiali Li, Xuanwei Ong, Salwa Ali Ibrahim, Tonio Buonassisi, Xiaonan Wang</h3>
<p>Precise control over dimension of nanocrystals is critical to tune the
properties for various applications. However, the traditional control through
experimental optimization is slow, tedious and time consuming. Herein a robust
deep neural network-based regression algorithm has been developed for precise
prediction of length, width, and aspect ratios of semiconductor nanorods (NRs).
Given there is limited experimental data available (28 samples), a Synthetic
Minority Oversampling Technique for regression (SMOTE-REG) has been employed
for the first time for data generation. Deep neural network is further applied
to develop regression model which demonstrated the well performed prediction on
both the original and generated data with a similar distribution. The
prediction model is further validated with additional experimental data,
showing accurate prediction results. Additionally, Local Interpretable
Model-Agnostic Explanations (LIME) is used to interpret the weight for each
variable, which corresponds to its importance towards the target dimension,
which is approved to be well correlated well with experimental observations.
</p>
<a href="http://arxiv.org/abs/2010.14111" target="_blank">arXiv:2010.14111</a> [<a href="http://arxiv.org/pdf/2010.14111" target="_blank">pdf</a>]

<h2>Energy Consumption and Battery Aging Minimization Using a Q-learning Strategy for a Battery/Ultracapacitor Electric Vehicle. (arXiv:2010.14115v1 [eess.SY])</h2>
<h3>Bin Xu, Junzhe Shi, Sixu Li, Huayi Li, Zhe Wang</h3>
<p>Propulsion system electrification revolution has been undergoing in the
automotive industry. The electrified propulsion system improves energy
efficiency and reduces the dependence on fossil fuel. However, the batteries of
electric vehicles experience degradation process during vehicle operation.
Research considering both battery degradation and energy consumption in
battery/ supercapacitor electric vehicles is still lacking. This study proposes
a Q-learning-based strategy to minimize battery degradation and energy
consumption. Besides Q-learning, two heuristic energy management methods are
also proposed and optimized using Particle Swarm Optimization algorithm. A
vehicle propulsion system model is first presented, where the severity factor
battery degradation model is considered and experimentally validated with the
help of Genetic Algorithm. In the results analysis, Q-learning is first
explained with the optimal policy map after learning. Then, the result from a
vehicle without ultracapacitor is used as the baseline, which is compared with
the results from the vehicle with ultracapacitor using Q-learning, and two
heuristic methods as the energy management strategies. At the learning and
validation driving cycles, the results indicate that the Q-learning strategy
slows down the battery degradation by 13-20% and increases the vehicle range by
1.5-2% compared with the baseline vehicle without ultracapacitor.
</p>
<a href="http://arxiv.org/abs/2010.14115" target="_blank">arXiv:2010.14115</a> [<a href="http://arxiv.org/pdf/2010.14115" target="_blank">pdf</a>]

<h2>Phase Aware Speech Enhancement using Realisation of Complex-valued LSTM. (arXiv:2010.14122v1 [eess.AS])</h2>
<h3>Raktim Gautam Goswami, Sivaganesh Andhavarapu, K Sri Rama Murty</h3>
<p>Most of the deep learning based speech enhancement (SE) methods rely on
estimating the magnitude spectrum of the clean speech signal from the observed
noisy speech signal, either by magnitude spectral masking or regression. These
methods reuse the noisy phase while synthesizing the time-domain waveform from
the estimated magnitude spectrum. However, there have been recent works
highlighting the importance of phase in SE. There was an attempt to estimate
the complex ratio mask taking phase into account using complex-valued
feed-forward neural network (FFNN). But FFNNs cannot capture the sequential
information essential for phase estimation. In this work, we propose a
realisation of complex-valued long short-term memory (RCLSTM) network to
estimate the complex ratio mask (CRM) using sequential information along time.
The proposed RCLSTM is designed to process the complex-valued sequences using
complex arithmetic, and hence it preserves the dependencies between the real
and imaginary parts of CRM and thereby the phase. The proposed method is
evaluated on the noisy speech mixtures formed from the Voice-Bank corpus and
DEMAND database. When compared to real value based masking methods, the
proposed RCLSTM improves over them in several objective measures including
perceptual evaluation of speech quality (PESQ), in which it improves by over
4.3%
</p>
<a href="http://arxiv.org/abs/2010.14122" target="_blank">arXiv:2010.14122</a> [<a href="http://arxiv.org/pdf/2010.14122" target="_blank">pdf</a>]

<h2>Selective Classification Can Magnify Disparities Across Groups. (arXiv:2010.14134v1 [cs.LG])</h2>
<h3>Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, Percy Liang</h3>
<p>Selective classification, in which models are allowed to abstain on uncertain
predictions, is a natural approach to improving accuracy in settings where
errors are costly but abstentions are manageable. In this paper, we find that
while selective classification can improve average accuracies, it can
simultaneously magnify existing accuracy disparities between various groups
within a population, especially in the presence of spurious correlations. We
observe this behavior consistently across five datasets from computer vision
and NLP. Surprisingly, increasing the abstention rate can even decrease
accuracies on some groups. To better understand when selective classification
improves or worsens accuracy on a group, we study its margin distribution,
which captures the model's confidences over all predictions. For example, when
the margin distribution is symmetric, we prove that whether selective
classification monotonically improves or worsens accuracy is fully determined
by the accuracy at full coverage (i.e., without any abstentions) and whether
the distribution satisfies a property we term left-log-concavity. Our analysis
also shows that selective classification tends to magnify accuracy disparities
that are present at full coverage. Fortunately, we find that it uniformly
improves each group when applied to distributionally-robust models that achieve
similar full-coverage accuracies across groups. Altogether, our results imply
selective classification should be used with care and underscore the importance
of models that perform equally well across groups at full coverage.
</p>
<a href="http://arxiv.org/abs/2010.14134" target="_blank">arXiv:2010.14134</a> [<a href="http://arxiv.org/pdf/2010.14134" target="_blank">pdf</a>]

<h2>Active Learning for Noisy Data Streams Using Weak and Strong Labelers. (arXiv:2010.14149v1 [cs.LG])</h2>
<h3>Taraneh Younesian, Dick Epema, Lydia Y. Chen</h3>
<p>Labeling data correctly is an expensive and challenging task in machine
learning, especially for on-line data streams. Deep learning models especially
require a large number of clean labeled data that is very difficult to acquire
in real-world problems. Choosing useful data samples to label while minimizing
the cost of labeling is crucial to maintain efficiency in the training process.
When confronted with multiple labelers with different expertise and respective
labeling costs, deciding which labeler to choose is nontrivial. In this paper,
we consider a novel weak and strong labeler problem inspired by humans natural
ability for labeling, in the presence of data streams with noisy labels and
constrained by a limited budget. We propose an on-line active learning
algorithm that consists of four steps: filtering, adding diversity, informative
sample selection, and labeler selection. We aim to filter out the suspicious
noisy samples and spend the budget on the diverse informative data using strong
and weak labelers in a cost-effective manner. We derive a decision function
that measures the information gain by combining the informativeness of
individual samples and model confidence. We evaluate our proposed algorithm on
the well-known image classification datasets CIFAR10 and CIFAR100 with up to
60% noise. Experiments show that by intelligently deciding which labeler to
query, our algorithm maintains the same accuracy compared to the case of having
only one of the labelers available while spending less of the budget.
</p>
<a href="http://arxiv.org/abs/2010.14149" target="_blank">arXiv:2010.14149</a> [<a href="http://arxiv.org/pdf/2010.14149" target="_blank">pdf</a>]

<h2>Federated Learning From Big Data Over Networks. (arXiv:2010.14159v1 [cs.LG])</h2>
<h3>Y. Sarcheshmehpour, M. Leinonen, A. Jung</h3>
<p>This paper formulates and studies a novel algorithm for federated learning
from large collections of local datasets. This algorithm capitalizes on an
intrinsic network structure that relates the local datasets via an undirected
"empirical" graph. We model such big data over networks using a networked
linear regression model. Each local dataset has individual regression weights.
The weights of close-knit sub-collections of local datasets are enforced to
deviate only little. This lends naturally to a network Lasso problem which we
solve using a primal-dual method. We obtain a distributed federated learning
algorithm via a message passing implementation of this primal-dual method. We
provide a detailed analysis of the statistical and computational properties of
the resulting federated learning algorithm.
</p>
<a href="http://arxiv.org/abs/2010.14159" target="_blank">arXiv:2010.14159</a> [<a href="http://arxiv.org/pdf/2010.14159" target="_blank">pdf</a>]

<h2>Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio and Tags. (arXiv:2010.14171v1 [cs.SD])</h2>
<h3>Xavier Favory, Konstantinos Drossos, Tuomas Virtanen, Xavier Serra</h3>
<p>Self-supervised audio representation learning offers an attractive
alternative for obtaining generic audio embeddings, capable to be employed into
various downstream tasks. Published approaches that consider both audio and
words/tags associated with audio do not employ text processing models that are
capable to generalize to tags unknown during training. In this work we propose
a method for learning audio representations using an audio autoencoder (AAE), a
general word embeddings model (WEM), and a multi-head self-attention (MHA)
mechanism. MHA attends on the output of the WEM, providing a contextualized
representation of the tags associated with the audio, and we align the output
of MHA with the output of the encoder of AAE using a contrastive loss. We
jointly optimize AAE and MHA and we evaluate the audio representations (i.e.
the output of the encoder of AAE) by utilizing them in three different
downstream tasks, namely sound, music genre, and music instrument
classification. Our results show that employing multi-head self-attention with
multiple heads in the tag-based network can induce better learned audio
representations.
</p>
<a href="http://arxiv.org/abs/2010.14171" target="_blank">arXiv:2010.14171</a> [<a href="http://arxiv.org/pdf/2010.14171" target="_blank">pdf</a>]

<h2>Spatio-temporal encoding improves neuromorphic tactile texture classification. (arXiv:2010.14184v1 [cs.RO])</h2>
<h3>Anupam K. Gupta, Andrei Nakagawa, Nathan F. Lepora, Nitish V. Thakor</h3>
<p>With the increase in interest in deployment of robots in unstructured
environments to work alongside humans, the development of human-like sense of
touch for robots becomes important. In this work, we implement a multi-channel
neuromorphic tactile system that encodes contact events as discrete spike
events that mimic the behavior of slow adapting mechanoreceptors. We study the
impact of information pooling across artificial mechanoreceptors on
classification performance of spatially non-uniform naturalistic textures. We
encoded the spatio-temporal activation patterns of mechanoreceptors through
gray-level co-occurrence matrix computed from time-varying mean spiking
rate-based tactile response volume. We found that this approach greatly
improved texture classification in comparison to use of individual
mechanoreceptor response alone. In addition, the performance was also more
robust to changes in sliding velocity. The importance of exploiting precise
spatial and temporal correlations between sensory channels is evident from the
fact that on either removal of precise temporal information or altering of
spatial structure of response pattern, a significant performance drop was
observed. This study thus demonstrates the superiority of population coding
approaches that can exploit the precise spatio-temporal information encoded in
activation patterns of mechanoreceptor populations. It, therefore, makes an
advance in the direction of development of bio-inspired tactile systems
required for realistic touch applications in robotics and prostheses.
</p>
<a href="http://arxiv.org/abs/2010.14184" target="_blank">arXiv:2010.14184</a> [<a href="http://arxiv.org/pdf/2010.14184" target="_blank">pdf</a>]

<h2>Learning Financial Asset-Specific Trading Rules via Deep Reinforcement Learning. (arXiv:2010.14194v1 [cs.AI])</h2>
<h3>Mehran Taghian, Ahmad Asadi, Reza Safabakhsh</h3>
<p>Generating asset-specific trading signals based on the financial conditions
of the assets is one of the challenging problems in automated trading. Various
asset trading rules are proposed experimentally based on different technical
analysis techniques. However, these kind of trading strategies are profitable,
extracting new asset-specific trading rules from vast historical data to
increase total return and decrease the risk of portfolios is difficult for
human experts. Recently, various deep reinforcement learning (DRL) methods are
employed to learn the new trading rules for each asset. In this paper, a novel
DRL model with various feature extraction modules is proposed. The effect of
different input representations on the performance of the models is
investigated and the performance of DRL-based models in different markets and
asset situations is studied. The proposed model in this work outperformed the
other state-of-the-art models in learning single asset-specific trading rules
and obtained a total return of almost 262% in two years on a specific asset
while the best state-of-the-art model get 78% on the same asset in the same
time period.
</p>
<a href="http://arxiv.org/abs/2010.14194" target="_blank">arXiv:2010.14194</a> [<a href="http://arxiv.org/pdf/2010.14194" target="_blank">pdf</a>]

<h2>Spiking Neural Networks -- Part I: Detecting Spatial Patterns. (arXiv:2010.14208v1 [cs.NE])</h2>
<h3>Hyeryung Jang, Nicolas Skatchkovsky, Osvaldo Simeone</h3>
<p>Spiking Neural Networks (SNNs) are biologically inspired machine learning
models that build on dynamic neuronal models processing binary and sparse
spiking signals in an event-driven, online, fashion. SNNs can be implemented on
neuromorphic computing platforms that are emerging as energy-efficient
co-processors for learning and inference. This is the first of a series of
three papers that introduce SNNs to an audience of engineers by focusing on
models, algorithms, and applications. In this first paper, we first cover
neural models used for conventional Artificial Neural Networks (ANNs) and SNNs.
Then, we review learning algorithms and applications for SNNs that aim at
mimicking the functionality of ANNs by detecting or generating spatial patterns
in rate-encoded spiking signals. We specifically discuss ANN-to-SNN conversion
and neural sampling. Finally, we validate the capabilities of SNNs for
detecting and generating spatial patterns through experiments.
</p>
<a href="http://arxiv.org/abs/2010.14208" target="_blank">arXiv:2010.14208</a> [<a href="http://arxiv.org/pdf/2010.14208" target="_blank">pdf</a>]

<h2>Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns. (arXiv:2010.14217v1 [cs.NE])</h2>
<h3>Nicolas Skatchkovsky, Hyeryung Jang, Osvaldo Simeone</h3>
<p>Inspired by the operation of biological brains, Spiking Neural Networks
(SNNs) have the unique ability to detect information encoded in spatio-temporal
patterns of spiking signals. Examples of data types requiring spatio-temporal
processing include logs of time stamps, e.g., of tweets, and outputs of neural
prostheses and neuromorphic sensors. In this paper, the second of a series of
three review papers on SNNs, we first review models and training algorithms for
the dominant approach that considers SNNs as a Recurrent Neural Network (RNN)
and adapt learning rules based on backpropagation through time to the
requirements of SNNs. In order to tackle the non-differentiability of the
spiking mechanism, state-of-the-art solutions use surrogate gradients that
approximate the threshold activation function with a differentiable function.
Then, we describe an alternative approach that relies on probabilistic models
for spiking neurons, allowing the derivation of local learning rules via
stochastic estimates of the gradient. Finally, experiments are provided for
neuromorphic data sets, yielding insights on accuracy and convergence under
different SNN models.
</p>
<a href="http://arxiv.org/abs/2010.14217" target="_blank">arXiv:2010.14217</a> [<a href="http://arxiv.org/pdf/2010.14217" target="_blank">pdf</a>]

<h2>Spiking Neural Networks -- Part III: Neuromorphic Communications. (arXiv:2010.14220v1 [cs.NE])</h2>
<h3>Nicolas Skatchkovsky, Hyeryung Jang, Osvaldo Simeone</h3>
<p>Synergies between wireless communications and artificial intelligence are
increasingly motivating research at the intersection of the two fields. On the
one hand, the presence of more and more wirelessly connected devices, each with
its own data, is driving efforts to export advances in machine learning (ML)
from high performance computing facilities, where information is stored and
processed in a single location, to distributed, privacy-minded, processing at
the end user. On the other hand, ML can address algorithm and model deficits in
the optimization of communication protocols. However, implementing ML models
for learning and inference on battery-powered devices that are connected via
bandwidth-constrained channels remains challenging. This paper explores two
ways in which Spiking Neural Networks (SNNs) can help address these open
problems. First, we discuss federated learning for the distributed training of
SNNs, and then describe the integration of neuromorphic sensing, SNNs, and
impulse radio technologies for low-power remote inference.
</p>
<a href="http://arxiv.org/abs/2010.14220" target="_blank">arXiv:2010.14220</a> [<a href="http://arxiv.org/pdf/2010.14220" target="_blank">pdf</a>]

<h2>Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding. (arXiv:2010.14227v1 [cs.LG])</h2>
<h3>Yongqi Zhang, Quanming Yao, Lei Chen</h3>
<p>Negative sampling, which samples negative triplets from non-observed ones in
knowledge graph (KG), is an essential step in KG embedding. Recently,
generative adversarial network (GAN), has been introduced in negative sampling.
By sampling negative triplets with large gradients, these methods avoid the
problem of vanishing gradient and thus obtain better performance. However, they
make the original model more complex and harder to train. In this paper,
motivated by the observation that negative triplets with large gradients are
important but rare, we propose to directly keep track of them with the cache.
In this way, our method acts as a "distilled" version of previous GAN-based
methods, which does not waste training time on additional parameters to fit the
full distribution of negative triplets. However, how to sample from and update
the cache are two critical questions. We propose to solve these issues by
automated machine learning techniques. The automated version also covers
GAN-based methods as special cases. Theoretical explanation of NSCaching is
also provided, justifying the superior over fixed sampling scheme. Besides, we
further extend NSCaching with skip-gram model for graph embedding. Finally,
extensive experiments show that our method can gain significant improvements on
various KG embedding models and the skip-gram model, and outperforms the
state-of-the-art negative sampling methods.
</p>
<a href="http://arxiv.org/abs/2010.14227" target="_blank">arXiv:2010.14227</a> [<a href="http://arxiv.org/pdf/2010.14227" target="_blank">pdf</a>]

<h2>A Comparison of Discrete Latent Variable Models for Speech Representation Learning. (arXiv:2010.14230v1 [eess.AS])</h2>
<h3>Henry Zhou, Alexei Baevski, Michael Auli</h3>
<p>Neural latent variable models enable the discovery of interesting structure
in speech audio data. This paper presents a comparison of two different
approaches which are broadly based on predicting future time-steps or
auto-encoding the input signal. Our study compares the representations learned
by vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme
recognition performance. Results show that future time-step prediction with
vq-wav2vec achieves better performance. The best system achieves an error rate
of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge
</p>
<a href="http://arxiv.org/abs/2010.14230" target="_blank">arXiv:2010.14230</a> [<a href="http://arxiv.org/pdf/2010.14230" target="_blank">pdf</a>]

<h2>Global Sentiment Analysis Of COVID-19 Tweets Over Time. (arXiv:2010.14234v1 [cs.CL])</h2>
<h3>Muvazima Mansoor, Kirthika Gurumurthy, Anantharam R U, V R Badri Prasad</h3>
<p>The Coronavirus pandemic has affected the normal course of life. People
around the world have taken to social media to express their opinions and
general emotions regarding this phenomenon that has taken over the world by
storm. The social networking site, Twitter showed an unprecedented increase in
tweets related to the novel Coronavirus in a very short span of time. This
paper presents the global sentiment analysis of tweets related to Coronavirus
and how the sentiment of people in different countries has changed over time.
Furthermore, to determine the impact of Coronavirus on daily aspects of life,
tweets related to Work From Home (WFH) and Online Learning were scraped and the
change in sentiment over time was observed. In addition, various Machine
Learning models such as Long Short Term Memory (LSTM) and Artificial Neural
Networks (ANN) were implemented for sentiment classification and their
accuracies were determined. Exploratory data analysis was also performed for a
dataset providing information about the number of confirmed cases on a per-day
basis in a few of the worst-hit countries to provide a comparison between the
change in sentiment with the change in cases since the start of this pandemic
till June 2020.
</p>
<a href="http://arxiv.org/abs/2010.14234" target="_blank">arXiv:2010.14234</a> [<a href="http://arxiv.org/pdf/2010.14234" target="_blank">pdf</a>]

<h2>Scientific intuition inspired by machine learning generated hypotheses. (arXiv:2010.14236v1 [cs.LG])</h2>
<h3>Pascal Friederich, Mario Krenn, Isaac Tamblyn, Alan Aspuru-Guzik</h3>
<p>Machine learning with application to questions in the physical sciences has
become a widely used tool, successfully applied to classification, regression
and optimization tasks in many areas. Research focus mostly lies in improving
the accuracy of the machine learning models in numerical predictions, while
scientific understanding is still almost exclusively generated by human
researchers analysing numerical results and drawing conclusions. In this work,
we shift the focus on the insights and the knowledge obtained by the machine
learning models themselves. In particular, we study how it can be extracted and
used to inspire human scientists to increase their intuitions and understanding
of natural systems. We apply gradient boosting in decision trees to extract
human interpretable insights from big data sets from chemistry and physics. In
chemistry, we not only rediscover widely know rules of thumb but also find new
interesting motifs that tell us how to control solubility and energy levels of
organic molecules. At the same time, in quantum physics, we gain new
understanding on experiments for quantum entanglement. The ability to go beyond
numerics and to enter the realm of scientific insight and hypothesis generation
opens the door to use machine learning to accelerate the discovery of
conceptual understanding in some of the most challenging domains of science.
</p>
<a href="http://arxiv.org/abs/2010.14236" target="_blank">arXiv:2010.14236</a> [<a href="http://arxiv.org/pdf/2010.14236" target="_blank">pdf</a>]

<h2>Deep generative factorization for speech signal. (arXiv:2010.14242v1 [cs.SD])</h2>
<h3>Haoran Sun, Lantian Li, Yunqi Cai, Yang Zhang, Thomas Fang Zheng, Dong Wang</h3>
<p>Various information factors are blended in speech signals, which forms the
primary difficulty for most speech information processing tasks. An intuitive
idea is to factorize speech signal into individual information factors (e.g.,
phonetic content and speaker trait), though it turns out to be highly
challenging. This paper presents a speech factorization approach based on a
novel factorial discriminative normalization flow model (factorial DNF).
Experiments conducted on a two-factor case that involves phonetic content and
speaker trait demonstrates that the proposed factorial DNF has powerful
capability to factorize speech signals and outperforms several comparative
models in terms of information representation and manipulation.
</p>
<a href="http://arxiv.org/abs/2010.14242" target="_blank">arXiv:2010.14242</a> [<a href="http://arxiv.org/pdf/2010.14242" target="_blank">pdf</a>]

<h2>{\mu}NAS: Constrained Neural Architecture Search for Microcontrollers. (arXiv:2010.14246v1 [cs.LG])</h2>
<h3>Edgar Liberis, &#x141;ukasz Dudziak, Nicholas D. Lane</h3>
<p>IoT devices are powered by microcontroller units (MCUs) which are extremely
resource-scarce: a typical MCU may have an underpowered processor and around 64
KB of memory and persistent storage, which is orders of magnitude fewer
computational resources than is typically required for deep learning. Designing
neural networks for such a platform requires an intricate balance between
keeping high predictive performance (accuracy) while achieving low memory and
storage usage and inference latency. This is extremely challenging to achieve
manually, so in this work, we build a neural architecture search (NAS) system,
called {\mu}NAS, to automate the design of such small-yet-powerful MCU-level
networks. {\mu}NAS explicitly targets the three primary aspects of resource
scarcity of MCUs: the size of RAM, persistent storage and processor speed.
{\mu}NAS represents a significant advance in resource-efficient models,
especially for "mid-tier" MCUs with memory requirements ranging from 0.5 KB to
64 KB. We show that on a variety of image classification datasets {\mu}NAS is
able to (a) improve top-1 classification accuracy by up to 4.8%, or (b) reduce
memory footprint by 4--13x, or (c) reduce the number of multiply-accumulate
operations by approx. 1700x, compared to existing MCU specialist literature and
resource-efficient models.
</p>
<a href="http://arxiv.org/abs/2010.14246" target="_blank">arXiv:2010.14246</a> [<a href="http://arxiv.org/pdf/2010.14246" target="_blank">pdf</a>]

<h2>Neural Networked Assisted Tree Search for the Personnel Rostering Problem. (arXiv:2010.14252v1 [cs.AI])</h2>
<h3>Ziyi Chen, Patrick De Causmaecker, Yajie Dou</h3>
<p>The personnel rostering problem is the problem of finding an optimal way to
assign employees to shifts, subject to a set of hard constraints which all
valid solutions must follow, and a set of soft constraints which define the
relative quality of valid solutions. The problem has received significant
attention in the literature and is addressed by a large number of exact and
metaheuristic methods. In order to make the complex and costly design of
heuristics for the personnel rostering problem automatic, we propose a new
method combined Deep Neural Network and Tree Search. By treating schedules as
matrices, the neural network can predict the distance between the current
solution and the optimal solution. It can select solution strategies by
analyzing existing (near-)optimal solutions to personnel rostering problem
instances. Combined with branch and bound, the network can give every node a
probability which indicates the distance between it and the optimal one, so
that a well-informed choice can be made on which branch to choose next and to
prune the search tree.
</p>
<a href="http://arxiv.org/abs/2010.14252" target="_blank">arXiv:2010.14252</a> [<a href="http://arxiv.org/pdf/2010.14252" target="_blank">pdf</a>]

<h2>Improving Reinforcement Learning for Neural Relation Extraction with Hierarchical Memory Extractor. (arXiv:2010.14255v1 [cs.CL])</h2>
<h3>Jianing Wang, Chong Su</h3>
<p>Distant supervision relation extraction (DSRE) is an efficient method to
extract semantic relations on a large-scale heuristic labeling corpus. However,
it usually brings in a massive noisy data. In order to alleviate this problem,
many recent approaches adopt reinforcement learning (RL), which aims to select
correct data autonomously before relation classification. Although these RL
methods outperform conventional multi-instance learning-based methods, there
are still two neglected problems: 1) the existing RL methods ignore the
feedback of noisy data, 2) the reduction of training corpus exacerbates
long-tail problem. In this paper, we propose a novel framework to solve the two
problems mentioned above. Firstly, we design a novel reward function to obtain
feedback from both correct and noisy data. In addition, we use implicit
relations information to improve RL. Secondly, we propose the hierarchical
memory extractor (HME), which utilizes the gating mechanism to share the
semantics from correlative instances between data-rich and data-poor classes.
Moreover, we define a hierarchical weighted ranking loss function to implement
top-down search processing. Extensive experiments conducted on the widely used
NYT dataset show significant improvement over state-of-the-art baseline
methods.
</p>
<a href="http://arxiv.org/abs/2010.14255" target="_blank">arXiv:2010.14255</a> [<a href="http://arxiv.org/pdf/2010.14255" target="_blank">pdf</a>]

<h2>Leveraging speaker attribute information using multi task learning for speaker verification and diarization. (arXiv:2010.14269v1 [cs.SD])</h2>
<h3>Chau Luu, Peter Bell, Steve Renals</h3>
<p>Deep speaker embeddings have become the leading method for encoding speaker
identity in speaker recognition tasks. The embedding space should ideally
capture the variations between all possible speakers, encoding the multiple
aspects that make up speaker identity. In this work, utilizing speaker age as
an auxiliary variable in US Supreme Court recordings and speaker nationality
with VoxCeleb, we show that by leveraging additional speaker attribute
information in a multi task learning setting, deep speaker embedding
performance can be increased for verification and diarization tasks, achieving
a relative improvement of 17.8% in DER and 8.9% in EER for Supreme Court audio
compared to omitting the auxiliary task. Experimental code has been made
publicly available.
</p>
<a href="http://arxiv.org/abs/2010.14269" target="_blank">arXiv:2010.14269</a> [<a href="http://arxiv.org/pdf/2010.14269" target="_blank">pdf</a>]

<h2>Cross-lingual Machine Reading Comprehension with Language Branch Knowledge Distillation. (arXiv:2010.14271v1 [cs.CL])</h2>
<h3>Junhao Liu, Linjun Shou, Jian Pei, Ming Gong, Min Yang, Daxin Jiang</h3>
<p>Cross-lingual Machine Reading Comprehension (CLMRC) remains a challenging
problem due to the lack of large-scale annotated datasets in low-source
languages, such as Arabic, Hindi, and Vietnamese. Many previous approaches use
translation data by translating from a rich-source language, such as English,
to low-source languages as auxiliary supervision. However, how to effectively
leverage translation data and reduce the impact of noise introduced by
translation remains onerous. In this paper, we tackle this challenge and
enhance the cross-lingual transferring performance by a novel augmentation
approach named Language Branch Machine Reading Comprehension (LBMRC). A
language branch is a group of passages in one single language paired with
questions in all target languages. We train multiple machine reading
comprehension (MRC) models proficient in individual language based on LBMRC.
Then, we devise a multilingual distillation approach to amalgamate knowledge
from multiple language branch models to a single model for all target
languages. Combining the LBMRC and multilingual distillation can be more robust
to the data noises, therefore, improving the model's cross-lingual ability.
Meanwhile, the produced single multilingual model is applicable to all target
languages, which saves the cost of training, inference, and maintenance for
multiple models. Extensive experiments on two CLMRC benchmarks clearly show the
effectiveness of our proposed method.
</p>
<a href="http://arxiv.org/abs/2010.14271" target="_blank">arXiv:2010.14271</a> [<a href="http://arxiv.org/pdf/2010.14271" target="_blank">pdf</a>]

<h2>Behavior Priors for Efficient Reinforcement Learning. (arXiv:2010.14274v1 [cs.AI])</h2>
<h3>Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh, Nicolas Heess</h3>
<p>As we deploy reinforcement learning agents to solve increasingly challenging
problems, methods that allow us to inject prior knowledge about the structure
of the world and effective solution strategies becomes increasingly important.
In this work we consider how information and architectural constraints can be
combined with ideas from the probabilistic modeling literature to learn
behavior priors that capture the common movement and interaction patterns that
are shared across a set of related tasks or contexts. For example the day-to
day behavior of humans comprises distinctive locomotion and manipulation
patterns that recur across many different situations and goals. We discuss how
such behavior patterns can be captured using probabilistic trajectory models
and how these can be integrated effectively into reinforcement learning
schemes, e.g.\ to facilitate multi-task and transfer learning. We then extend
these ideas to latent variable models and consider a formulation to learn
hierarchical priors that capture different aspects of the behavior in reusable
modules. We discuss how such latent variable formulations connect to related
work on hierarchical reinforcement learning (HRL) and mutual information and
curiosity based objectives, thereby offering an alternative perspective on
existing ideas. We demonstrate the effectiveness of our framework by applying
it to a range of simulated continuous control domains.
</p>
<a href="http://arxiv.org/abs/2010.14274" target="_blank">arXiv:2010.14274</a> [<a href="http://arxiv.org/pdf/2010.14274" target="_blank">pdf</a>]

<h2>Take a Chance: Managing the Exploitation-Exploration Dilemma in Customs Fraud Detection via Online Active Learning. (arXiv:2010.14282v1 [cs.LG])</h2>
<h3>Sundong Kim, Tung-Duong Mai, Thi Nguyen Duc Khanh, Sungwon Han, Sungwon Park, Karandeep Singh, Meeyoung Cha</h3>
<p>Continual labeling of training examples is a costly task in supervised
learning. Active learning strategies mitigate this cost by identifying
unlabeled data that are considered the most useful for training a predictive
model. However, sample selection via active learning may lead to an
exploitation-exploration dilemma. In online settings, profitable items can be
neglected when uncertain items are annotated instead. To illustrate this
dilemma, we study a human-in-the-loop customs selection scenario where an
AI-based system supports customs officers by providing a set of imports to be
inspected. If the inspected items are fraud, officers levy extra duties, and
these items will be used as additional training data for the next iterations.
Inspecting highly suspicious items will inevitably lead to additional customs
revenue, yet they may not give any extra knowledge to customs officers. On the
other hand, inspecting uncertain items will help customs officers to acquire
new knowledge, which will be used as supplementary training resources to update
their selection systems. Through years of customs selection simulation, we show
that some exploration is needed to cope with the domain shift, and our hybrid
strategy of selecting fraud and uncertain items will eventually outperform the
performance of the exploitation strategy.
</p>
<a href="http://arxiv.org/abs/2010.14282" target="_blank">arXiv:2010.14282</a> [<a href="http://arxiv.org/pdf/2010.14282" target="_blank">pdf</a>]

<h2>Affordance as general value function: A computational model. (arXiv:2010.14289v1 [cs.AI])</h2>
<h3>Daniel Graves, Johannes G&#xfc;nther, Jun Luo</h3>
<p>General value functions (GVFs) in the reinforcement learning (RL) literature
are long-term predictive summaries of the outcomes of agents following specific
policies in the environment. Affordances as perceived valences of action
possibilities may be cast into predicted policy-relative goodness and modelled
as GVFs. A systematic explication of this connection shows that GVFs and
especially their deep learning embodiments (1) realize affordance prediction as
a form of direct perception, (2) illuminate the fundamental connection between
action and perception in affordance, and (3) offer a scalable way to learn
affordances using RL methods. Through a comprehensive review of existing
literature on recent successes of GVF applications in robotics, rehabilitation,
industrial automation, and autonomous driving, we demonstrate that GVFs provide
the right framework for learning affordances in real-world applications. In
addition, we highlight a few new avenues of research opened up by the
perspective of "affordance as GVF", including using GVFs for orchestrating
complex behaviors.
</p>
<a href="http://arxiv.org/abs/2010.14289" target="_blank">arXiv:2010.14289</a> [<a href="http://arxiv.org/pdf/2010.14289" target="_blank">pdf</a>]

<h2>Fast Local Attack: Generating Local Adversarial Examples for Object Detectors. (arXiv:2010.14291v1 [cs.CV])</h2>
<h3>Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu</h3>
<p>The deep neural network is vulnerable to adversarial examples. Adding
imperceptible adversarial perturbations to images is enough to make them fail.
Most existing research focuses on attacking image classifiers or anchor-based
object detectors, but they generate globally perturbation on the whole image,
which is unnecessary. In our work, we leverage higher-level semantic
information to generate high aggressive local perturbations for anchor-free
object detectors. As a result, it is less computationally intensive and
achieves a higher black-box attack as well as transferring attack performance.
The adversarial examples generated by our method are not only capable of
attacking anchor-free object detectors, but also able to be transferred to
attack anchor-based object detector.
</p>
<a href="http://arxiv.org/abs/2010.14291" target="_blank">arXiv:2010.14291</a> [<a href="http://arxiv.org/pdf/2010.14291" target="_blank">pdf</a>]

<h2>Fit to Measure: Reasoning about Sizes for Robust Object Recognition. (arXiv:2010.14296v1 [cs.RO])</h2>
<h3>Agnese Chiatti, Enrico Motta, Enrico Daga, Gianluca Bardaro</h3>
<p>Service robots can help with many of our daily tasks, especially in those
cases where it is inconvenient or unsafe for us to intervene: e.g., under
extreme weather conditions or when social distance needs to be maintained.
However, before we can successfully delegate complex tasks to robots, we need
to enhance their ability to make sense of dynamic, real world environments. In
this context, the first prerequisite to improving the Visual Intelligence of a
robot is building robust and reliable object recognition systems. While object
recognition solutions are traditionally based on Machine Learning methods,
augmenting them with knowledge based reasoners has been shown to improve their
performance. In particular, based on our prior work on identifying the
epistemic requirements of Visual Intelligence, we hypothesise that knowledge of
the typical size of objects could significantly improve the accuracy of an
object recognition system. To verify this hypothesis, in this paper we present
an approach to integrating knowledge about object sizes in a ML based
architecture. Our experiments in a real world robotic scenario show that this
combined approach ensures a significant performance increase over state of the
art Machine Learning methods.
</p>
<a href="http://arxiv.org/abs/2010.14296" target="_blank">arXiv:2010.14296</a> [<a href="http://arxiv.org/pdf/2010.14296" target="_blank">pdf</a>]

<h2>A Statistical Framework for Low-bitwidth Training of Deep Neural Networks. (arXiv:2010.14298v1 [cs.LG])</h2>
<h3>Jianfei Chen, Yu Gai, Zhewei Yao, Michael W. Mahoney, Joseph E. Gonzalez</h3>
<p>Fully quantized training (FQT), which uses low-bitwidth hardware by
quantizing the activations, weights, and gradients of a neural network model,
is a promising approach to accelerate the training of deep neural networks. One
major challenge with FQT is the lack of theoretical understanding, in
particular of how gradient quantization impacts convergence properties. In this
paper, we address this problem by presenting a statistical framework for
analyzing FQT algorithms. We view the quantized gradient of FQT as a stochastic
estimator of its full precision counterpart, a procedure known as
quantization-aware training (QAT). We show that the FQT gradient is an unbiased
estimator of the QAT gradient, and we discuss the impact of gradient
quantization on its variance. Inspired by these theoretical results, we develop
two novel gradient quantizers, and we show that these have smaller variance
than the existing per-tensor quantizer. For training ResNet-50 on ImageNet, our
5-bit block Householder quantizer achieves only 0.5% validation accuracy loss
relative to QAT, comparable to the existing INT8 baseline.
</p>
<a href="http://arxiv.org/abs/2010.14298" target="_blank">arXiv:2010.14298</a> [<a href="http://arxiv.org/pdf/2010.14298" target="_blank">pdf</a>]

<h2>Ice Monitoring in Swiss Lakes from Optical Satellites and Webcams using Machine Learning. (arXiv:2010.14300v1 [cs.CV])</h2>
<h3>Manu Tom, Rajanie Prabha, Tianyu Wu, Emmanuel Baltsavias, Laura Leal-Taixe, Konrad Schindler</h3>
<p>Continuous observation of climate indicators, such as trends in lake
freezing, is important to understand the dynamics of the local and global
climate system. Consequently, lake ice has been included among the Essential
Climate Variables (ECVs) of the Global Climate Observing System (GCOS), and
there is a need to set up operational monitoring capabilities. Multi-temporal
satellite images and publicly available webcam streams are among the viable
data sources to monitor lake ice. In this work we investigate machine
learning-based image analysis as a tool to determine the spatio-temporal extent
of ice on Swiss Alpine lakes as well as the ice-on and ice-off dates, from both
multispectral optical satellite images (VIIRS and MODIS) and RGB webcam images.
We model lake ice monitoring as a pixel-wise semantic segmentation problem,
i.e., each pixel on the lake surface is classified to obtain a spatially
explicit map of ice cover. We show experimentally that the proposed system
produces consistently good results when tested on data from multiple winters
and lakes. Our satellite-based method obtains mean Intersection-over-Union
(mIoU) scores &gt;93%, for both sensors. It also generalises well across lakes and
winters with mIoU scores &gt;78% and &gt;80% respectively. On average, our webcam
approach achieves mIoU values of 87% (approx.) and generalisation scores of 71%
(approx.) and 69% (approx.) across different cameras and winters respectively.
Additionally, we put forward a new benchmark dataset of webcam images
(Photi-LakeIce) which includes data from two winters and three cameras.
</p>
<a href="http://arxiv.org/abs/2010.14300" target="_blank">arXiv:2010.14300</a> [<a href="http://arxiv.org/pdf/2010.14300" target="_blank">pdf</a>]

<h2>What Color is this? Explaining Art Restoration Research Methods using Interactive Museum Installations. (arXiv:2010.14307v1 [cs.HC])</h2>
<h3>Franziska Hann&#xdf;, Esther Lapczyna, Mathias M&#xfc;ller, Rainer Groh</h3>
<p>This case study describes an approach to designing interactive museum
installations as a student project with the aim of presenting the research
results of the restoration process of paintings to a wide range of visitors.
During one and a half years, the Chair of Media Design created five interactive
media stations in two lectures to enrich the special exhibition "Veronese: The
Cuccina Cycle. The Restored Masterpiece". The project was realised in close
communication with the conservators of the Dresden State Art Collections and
the employees of the Science and Archaeometric Laboratory of the Dresden
University of Fine Arts. The students had to learn about the foreign content
and how to translate it into a media-related environment. With suitable
teaching methods, we pushed the students towards a deeper understanding of the
matter.
</p>
<a href="http://arxiv.org/abs/2010.14307" target="_blank">arXiv:2010.14307</a> [<a href="http://arxiv.org/pdf/2010.14307" target="_blank">pdf</a>]

<h2>Probabilistic learning on manifolds constrained by nonlinear partial differential equations for small datasets. (arXiv:2010.14324v1 [stat.ML])</h2>
<h3>Christian Soize, Roger Ghanem</h3>
<p>A novel extension of the Probabilistic Learning on Manifolds (PLoM) is
presented. It makes it possible to synthesize solutions to a wide range of
nonlinear stochastic boundary value problems described by partial differential
equations (PDEs) for which a stochastic computational model (SCM) is available
and depends on a vector-valued random control parameter. The cost of a single
numerical evaluation of this SCM is assumed to be such that only a limited
number of points can be computed for constructing the training dataset (small
data). Each point of the training dataset is made up realizations from a
vector-valued stochastic process (the stochastic solution) and the associated
random control parameter on which it depends. The presented PLoM constrained by
PDE allows for generating a large number of learned realizations of the
stochastic process and its corresponding random control parameter. These
learned realizations are generated so as to minimize the vector-valued random
residual of the PDE in the mean-square sense. Appropriate novel methods are
developed to solve this challenging problem. Three applications are presented.
The first one is a simple uncertain nonlinear dynamical system with a
nonstationary stochastic excitation. The second one concerns the 2D nonlinear
unsteady Navier-Stokes equations for incompressible flows in which the Reynolds
number is the random control parameter. The last one deals with the nonlinear
dynamics of a 3D elastic structure with uncertainties. The results obtained
make it possible to validate the PLoM constrained by stochastic PDE but also
provide further validation of the PLoM without constraint.
</p>
<a href="http://arxiv.org/abs/2010.14324" target="_blank">arXiv:2010.14324</a> [<a href="http://arxiv.org/pdf/2010.14324" target="_blank">pdf</a>]

<h2>Learning to Infer Unseen Attribute-Object Compositions. (arXiv:2010.14343v1 [cs.CV])</h2>
<h3>Hui Chen, Zhixiong Nan, Jiang Jingjing, Nanning Zheng</h3>
<p>The composition recognition of unseen attribute-object is critical to make
machines learn to decompose and compose complex concepts like people. Most of
the existing methods are limited to the composition recognition of
single-attribute-object, and can hardly distinguish the compositions with
similar appearances. In this paper, a graph-based model is proposed that can
flexibly recognize both single- and multi-attribute-object compositions. The
model maps the visual features of images and the attribute-object category
labels represented by word embedding vectors into a latent space. Then,
according to the constraints of the attribute-object semantic association,
distances are calculated between visual features and the corresponding label
semantic features in the latent space. During the inference, the composition
that is closest to the given image feature among all compositions is used as
the reasoning result. In addition, we build a large-scale Multi-Attribute
Dataset (MAD) with 116,099 images and 8,030 composition categories. Experiments
on MAD and two other single-attribute-object benchmark datasets demonstrate the
effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2010.14343" target="_blank">arXiv:2010.14343</a> [<a href="http://arxiv.org/pdf/2010.14343" target="_blank">pdf</a>]

<h2>CT Reconstruction with PDF: Parameter-Dependent Framework for Multiple Scanning Geometries and Dose Levels. (arXiv:2010.14350v1 [physics.med-ph])</h2>
<h3>Wenjun Xia, Zexin Lu, Yongqiang Huang, Yan Liu, Hu Chen, Jiliu Zhou, Yi Zhang</h3>
<p>Current mainstream of CT reconstruction methods based on deep learning
usually needs to fix the scanning geometry and dose level, which will
significantly aggravate the training cost and need more training data for
clinical application. In this paper, we propose a parameter-dependent framework
(PDF) which trains data with multiple scanning geometries and dose levels
simultaneously. In the proposed PDF, the geometry and dose level are
parameterized and fed into two multi-layer perceptrons (MLPs). The MLPs are
leveraged to modulate the feature maps of CT reconstruction network, which
condition the network outputs on different scanning geometries and dose levels.
The experiments show that our proposed method can obtain competing performance
similar to the original network trained with specific geometry and dose level,
which can efficiently save the extra training cost for multiple scanning
geometries and dose levels.
</p>
<a href="http://arxiv.org/abs/2010.14350" target="_blank">arXiv:2010.14350</a> [<a href="http://arxiv.org/pdf/2010.14350" target="_blank">pdf</a>]

<h2>Implementing efficient balanced networks with mixed-signal spike-based learning circuits. (arXiv:2010.14353v1 [cs.ET])</h2>
<h3>Julian B&#xfc;chel, Jonathan Kakon, Michel Perez, Giacomo Indiveri</h3>
<p>Efficient Balanced Networks (EBNs) are networks of spiking neurons in which
excitatory and inhibitory synaptic currents are balanced on a short timescale,
leading to desirable coding properties such as high encoding precision, low
firing rates, and distributed information representation. It is for these
benefits that it would be desirable to implement such networks in low-power
neuromorphic processors. However, the degree of device mismatch in analog
mixed-signal neuromorphic circuits renders the use of pre-trained EBNs
challenging, if not impossible. To overcome this issue, we developed a novel
local learning rule suitable for on-chip implementation that drives a randomly
connected network of spiking neurons into a tightly balanced regime. Here we
present the integrated circuits that implement this rule and demonstrate their
expected behaviour in low-level circuit simulations. Our proposed method paves
the way towards a system-level implementation of tightly balanced networks on
analog mixed-signal neuromorphic hardware. Thanks to their coding properties
and sparse activity, neuromorphic electronic EBNs will be ideally suited for
extreme-edge computing applications that require low-latency, ultra-low power
consumption and which cannot rely on cloud computing for data processing.
</p>
<a href="http://arxiv.org/abs/2010.14353" target="_blank">arXiv:2010.14353</a> [<a href="http://arxiv.org/pdf/2010.14353" target="_blank">pdf</a>]

<h2>Upsampling artifacts in neural audio synthesis. (arXiv:2010.14356v1 [cs.SD])</h2>
<h3>Jordi Pons, Santiago Pascual, Giulio Cengarle, Joan Serr&#xe0;</h3>
<p>A number of recent advances in audio synthesis rely on neural upsamplers,
which can introduce undesired artifacts. In computer vision, upsampling
artifacts have been studied and are known as checkerboard artifacts (due to
their characteristic visual pattern). However, their effect has been overlooked
so far in audio processing. Here, we address this gap by studying this problem
from the audio signal processing perspective. We first show that the main
sources of upsampling artifacts are: (i) the tonal and filtering artifacts
introduced by problematic upsampling operators, and (ii) the spectral replicas
that emerge while upsampling. We then compare different neural upsamplers,
showing that nearest neighbor interpolation upsamplers can be an alternative to
the problematic (but state-of-the-art) transposed and subpixel convolutions
which are prone to introduce tonal artifacts.
</p>
<a href="http://arxiv.org/abs/2010.14356" target="_blank">arXiv:2010.14356</a> [<a href="http://arxiv.org/pdf/2010.14356" target="_blank">pdf</a>]

<h2>Matrix Engines for High Performance Computing:A Paragon of Performance or Grasping at Straws?. (arXiv:2010.14373v1 [cs.DC])</h2>
<h3>Jens Domke, Emil Vatai, Aleksandr Drozd, Peng Chen, Yosuke Oyama, Lingqi Zhang, Shweta Salaria, Daichi Mukunoki, Artur Podobas, Mohamed Wahib, Satoshi Matsuoka</h3>
<p>Matrix engines or units, in different forms and affinities, are becoming a
reality in modern processors; CPUs and otherwise. The current and dominant
algorithmic approach to Deep Learning merits the commercial investments in
these units, and deduced from the No. 1 benchmark in supercomputing, namely
High Performance Linpack, one would expect an awakened enthusiasm by the HPC
community, too. Hence, our goal is to identify the practical added benefits for
HPC and machine learning applications by having access to matrix engines. For
this purpose, we perform an in-depth survey of software stacks, proxy
applications and benchmarks, and historical batch job records. We provide a
cost-benefit analysis of matrix engines, both asymptotically and in conjunction
with state-of-the-art processors. While our empirical data will temper the
enthusiasm, we also outline opportunities to "misuse" these dense
matrix-multiplication engines if they come for free.
</p>
<a href="http://arxiv.org/abs/2010.14373" target="_blank">arXiv:2010.14373</a> [<a href="http://arxiv.org/pdf/2010.14373" target="_blank">pdf</a>]

<h2>Explainable Machine Learning for Public Policy: Use Cases, Gaps, and Research Directions. (arXiv:2010.14374v1 [cs.LG])</h2>
<h3>Kasun Amarasinghe, Kit Rodolfa, Hemank Lamba, Rayid Ghani</h3>
<p>In Machine Learning (ML) models used for supporting decisions in high-stakes
domains such as public policy, explainability is crucial for adoption and
effectiveness. While the field of explainable ML has expanded in recent years,
much of this work does not take real-world needs into account. A majority of
proposed methods use benchmark ML problems with generic explainability goals
without clear use-cases or intended end-users. As a result, the effectiveness
of this large body of theoretical and methodological work on real-world
applications is unclear. This paper focuses on filling this void for the domain
of public policy. We develop a taxonomy of explainability use-cases within
public policy problems; for each use-case, we define the end-users of
explanations and the specific goals explainability has to fulfill; third, we
map existing work to these use-cases, identify gaps, and propose research
directions to fill those gaps in order to have practical policy impact through
ML.
</p>
<a href="http://arxiv.org/abs/2010.14374" target="_blank">arXiv:2010.14374</a> [<a href="http://arxiv.org/pdf/2010.14374" target="_blank">pdf</a>]

<h2>The DigitalTwin from an Artificial Intelligence Perspective. (arXiv:2010.14376v1 [cs.AI])</h2>
<h3>Oliver Niggemann, Alexander Diedrich, Christian Kuehnert, Erik Pfannstiel, Joshua Schraven</h3>
<p>Services for Cyber-Physical Systems based on Artificial Intelligence and
Machine Learning require a virtual representation of the physical. To reduce
modeling efforts and to synchronize results, for each system, a common and
unique virtual representation used by all services during the whole system
life-cycle is needed, i.e. a DigitalTwin. In this paper such a DigitalTwin,
namely the AI reference model AITwin, is defined. This reference model is
verified by using a running example from process industry and by analyzing the
work done in recent projects.
</p>
<a href="http://arxiv.org/abs/2010.14376" target="_blank">arXiv:2010.14376</a> [<a href="http://arxiv.org/pdf/2010.14376" target="_blank">pdf</a>]

<h2>An Experimentation Platform for Explainable Coalition Situational Understanding. (arXiv:2010.14388v1 [cs.AI])</h2>
<h3>Katie Barrett-Powell, Jack Furby, Liam Hiley, Marc Roig Vilamala, Harrison Taylor, Federico Cerutti, Alun Preece, Tianwei Xing, Luis Garcia, Mani Srivastava, Dave Braines</h3>
<p>We present an experimentation platform for coalition situational
understanding research that highlights capabilities in explainable artificial
intelligence/machine learning (AI/ML) and integration of symbolic and
subsymbolic AI/ML approaches for event processing. The Situational
Understanding Explorer (SUE) platform is designed to be lightweight, to easily
facilitate experiments and demonstrations, and open. We discuss our
requirements to support coalition multi-domain operations with emphasis on
asset interoperability and ad hoc human-machine teaming in a dense urban
terrain setting. We describe the interface functionality and give examples of
SUE applied to coalition situational understanding tasks.
</p>
<a href="http://arxiv.org/abs/2010.14388" target="_blank">arXiv:2010.14388</a> [<a href="http://arxiv.org/pdf/2010.14388" target="_blank">pdf</a>]

<h2>Succinct and Robust Multi-Agent Communication With Temporal Message Control. (arXiv:2010.14391v1 [cs.AI])</h2>
<h3>Sai Qian Zhang, Jieyu Lin, Qi Zhang</h3>
<p>Recent studies have shown that introducing communication between agents can
significantly improve overall performance in cooperative Multi-agent
reinforcement learning (MARL). However, existing communication schemes often
require agents to exchange an excessive number of messages at run-time under a
reliable communication channel, which hinders its practicality in many
real-world situations. In this paper, we present \textit{Temporal Message
Control} (TMC), a simple yet effective approach for achieving succinct and
robust communication in MARL. TMC applies a temporal smoothing technique to
drastically reduce the amount of information exchanged between agents.
Experiments show that TMC can significantly reduce inter-agent communication
overhead without impacting accuracy. Furthermore, TMC demonstrates much better
robustness against transmission loss than existing approaches in lossy
networking environments.
</p>
<a href="http://arxiv.org/abs/2010.14391" target="_blank">arXiv:2010.14391</a> [<a href="http://arxiv.org/pdf/2010.14391" target="_blank">pdf</a>]

<h2>Transporter Networks: Rearranging the Visual World for Robotic Manipulation. (arXiv:2010.14406v1 [cs.RO])</h2>
<h3>Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Johnny Lee</h3>
<p>Robotic manipulation can be formulated as inducing a sequence of spatial
displacements: where the space being moved can encompass an object, part of an
object, or end effector. In this work, we propose the Transporter Network, a
simple model architecture that rearranges deep features to infer spatial
displacements from visual input - which can parameterize robot actions. It
makes no assumptions of objectness (e.g. canonical poses, models, or
keypoints), it exploits spatial symmetries, and is orders of magnitude more
sample efficient than our benchmarked alternatives in learning vision-based
manipulation tasks: from stacking a pyramid of blocks, to assembling kits with
unseen objects; from manipulating deformable ropes, to pushing piles of small
objects with closed-loop feedback. Our method can represent complex multi-modal
policy distributions and generalizes to multi-step sequential tasks, as well as
6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns
faster and generalizes better than a variety of end-to-end baselines, including
policies that use ground-truth object poses. We validate our methods with
hardware in the real world. Experiment videos and code will be made available
at https://transporternets.github.io
</p>
<a href="http://arxiv.org/abs/2010.14406" target="_blank">arXiv:2010.14406</a> [<a href="http://arxiv.org/pdf/2010.14406" target="_blank">pdf</a>]

<h2>On the Transfer of Disentangled Representations in Realistic Settings. (arXiv:2010.14407v1 [cs.LG])</h2>
<h3>Andrea Dittadi, Frederik Tr&#xe4;uble, Francesco Locatello, Manuel W&#xfc;thrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, Bernhard Sch&#xf6;lkopf</h3>
<p>Learning meaningful representations that disentangle the underlying structure
of the data generating process is considered to be of key importance in machine
learning. While disentangled representations were found to be useful for
diverse tasks such as abstract reasoning and fair classification, their
scalability and real-world impact remain questionable. We introduce a new
high-resolution dataset with 1M simulated images and over 1,800 annotated
real-world images of the same robotic setup. In contrast to previous work, this
new dataset exhibits correlations, a complex underlying structure, and allows
to evaluate transfer to unseen simulated and real-world settings where the
encoder i) remains in distribution or ii) is out of distribution. We propose
new architectures in order to scale disentangled representation learning to
realistic high-resolution settings and conduct a large-scale empirical study of
disentangled representations on this dataset. We observe that disentanglement
is a good predictor for out-of-distribution (OOD) task performance.
</p>
<a href="http://arxiv.org/abs/2010.14407" target="_blank">arXiv:2010.14407</a> [<a href="http://arxiv.org/pdf/2010.14407" target="_blank">pdf</a>]

<h2>Improving Word Recognition using Multiple Hypotheses and Deep Embeddings. (arXiv:2010.14411v1 [cs.CV])</h2>
<h3>Siddhant Bansal, Praveen Krishnan, C.V. Jawahar</h3>
<p>We propose a novel scheme for improving the word recognition accuracy using
word image embeddings. We use a trained text recognizer, which can predict
multiple text hypothesis for a given word image. Our fusion scheme improves the
recognition process by utilizing the word image and text embeddings obtained
from a trained word image embedding network. We propose EmbedNet, which is
trained using a triplet loss for learning a suitable embedding space where the
embedding of the word image lies closer to the embedding of the corresponding
text transcription. The updated embedding space thus helps in choosing the
correct prediction with higher confidence. To further improve the accuracy, we
propose a plug-and-play module called Confidence based Accuracy Booster (CAB).
The CAB module takes in the confidence scores obtained from the text recognizer
and Euclidean distances between the embeddings to generate an updated distance
vector. The updated distance vector has lower distance values for the correct
words and higher distance values for the incorrect words. We rigorously
evaluate our proposed method systematically on a collection of books in the
Hindi language. Our method achieves an absolute improvement of around 10
percent in terms of word recognition accuracy.
</p>
<a href="http://arxiv.org/abs/2010.14411" target="_blank">arXiv:2010.14411</a> [<a href="http://arxiv.org/pdf/2010.14411" target="_blank">pdf</a>]

<h2>Structured Visual Search via Composition-aware Learning. (arXiv:2010.14438v1 [cs.CV])</h2>
<h3>Mert Kilickaya, Arnold W.M. Smeulders</h3>
<p>This paper studies visual search using structured queries. The structure is
in the form of a 2D composition that encodes the position and the category of
the objects. The transformation of the position and the category of the objects
leads to a continuous-valued relationship between visual compositions, which
carries highly beneficial information, although not leveraged by previous
techniques. To that end, in this work, our goal is to leverage these continuous
relationships by using the notion of symmetry in equivariance. Our model output
is trained to change symmetrically with respect to the input transformations,
leading to a sensitive feature space. Doing so leads to a highly efficient
search technique, as our approach learns from fewer data using a smaller
feature space. Experiments on two large-scale benchmarks of MS-COCO and
HICO-DET demonstrates that our approach leads to a considerable gain in the
performance against competing techniques.
</p>
<a href="http://arxiv.org/abs/2010.14438" target="_blank">arXiv:2010.14438</a> [<a href="http://arxiv.org/pdf/2010.14438" target="_blank">pdf</a>]

<h2>Generating 3D Molecular Structures Conditional on a Receptor Binding Site with Deep Generative Models. (arXiv:2010.14442v1 [physics.chem-ph])</h2>
<h3>Tomohide Masuda, Matthew Ragoza, David Ryan Koes</h3>
<p>Deep generative models have been applied with increasing success to the
generation of two dimensional molecules as SMILES strings and molecular graphs.
In this work we describe for the first time a deep generative model that can
generate 3D molecular structures conditioned on a three-dimensional (3D)
binding pocket. Using convolutional neural networks, we encode atomic density
grids into separate receptor and ligand latent spaces. The ligand latent space
is variational to support sampling of new molecules. A decoder network
generates atomic densities of novel ligands conditioned on the receptor.
Discrete atoms are then fit to these continuous densities to create molecular
structures. We show that valid and unique molecules can be readily sampled from
the variational latent space defined by a reference `seed' structure and
generated structures have reasonable interactions with the binding site. As
structures are sampled farther in latent space from the seed structure, the
novelty of the generated structures increases, but the predicted binding
affinity decreases. Overall, we demonstrate the feasibility of conditional 3D
molecular structure generation and provide a starting point for methods that
also explicitly optimize for desired molecular properties, such as high binding
affinity.
</p>
<a href="http://arxiv.org/abs/2010.14442" target="_blank">arXiv:2010.14442</a> [<a href="http://arxiv.org/pdf/2010.14442" target="_blank">pdf</a>]

<h2>Can Reinforcement Learning for Continuous Control Generalize Across Physics Engines?. (arXiv:2010.14444v1 [cs.LG])</h2>
<h3>Aaqib Parvez Mohammed, Matias Valdenegro-Toro</h3>
<p>Reinforcement learning (RL) algorithms should learn as much as possible about
the environment but not the properties of the physics engines that generate the
environment. There are multiple algorithms that solve the task in a physics
engine based environment but there is no work done so far to understand if the
RL algorithms can generalize across physics engines. In this work, we compare
the generalization performance of various deep reinforcement learning
algorithms on a variety of control tasks. Our results show that MuJoCo is the
best engine to transfer the learning to other engines. On the other hand, none
of the algorithms generalize when trained on PyBullet. We also found out that
various algorithms have a promising generalizability if the effect of random
seeds can be minimized on their performance.
</p>
<a href="http://arxiv.org/abs/2010.14444" target="_blank">arXiv:2010.14444</a> [<a href="http://arxiv.org/pdf/2010.14444" target="_blank">pdf</a>]

<h2>Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging. (arXiv:2010.14462v1 [cs.LG])</h2>
<h3>He Sun, Katherine L. Bouman</h3>
<p>Computational image reconstruction algorithms generally produce a single
image without any measure of uncertainty or confidence. Regularized Maximum
Likelihood (RML) and feed-forward deep learning approaches for inverse problems
typically focus on recovering a point estimate. This is a serious limitation
when working with underdetermined imaging systems, where it is conceivable that
multiple image modes would be consistent with the measured data. Characterizing
the space of probable images that explain the observational data is therefore
crucial. In this paper, we propose a variational deep probabilistic imaging
approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging
(DPI) employs an untrained deep generative model to estimate a posterior
distribution of an unobserved image. This approach does not require any
training data; instead, it optimizes the weights of a neural network to
generate image samples that fit a particular measurement dataset. Once the
network weights have been learned, the posterior distribution can be
efficiently sampled. We demonstrate this approach in the context of
interferometric radio imaging, which is used for black hole imaging with the
Event Horizon Telescope.
</p>
<a href="http://arxiv.org/abs/2010.14462" target="_blank">arXiv:2010.14462</a> [<a href="http://arxiv.org/pdf/2010.14462" target="_blank">pdf</a>]

<h2>It's All in the Name: A Character Based Approach To Infer Religion. (arXiv:2010.14479v1 [cs.CL])</h2>
<h3>Rochana Chaturvedi, Sugat Chaturvedi</h3>
<p>Demographic inference from text has received a surge of attention in the
field of natural language processing in the last decade. In this paper, we use
personal names to infer religion in South Asia - where religion is a salient
social division, and yet, disaggregated data on it remains scarce. Existing
work predicts religion using dictionary based method, and therefore, can not
classify unseen names. We use character based models which learn character
patterns and, therefore, can classify unseen names as well with high accuracy.
These models are also much faster and can easily be scaled to large data sets.
We improve our classifier by combining the name of an individual with that of
their parent/spouse and achieve remarkably high accuracy. Finally, we trace the
classification decisions of a convolutional neural network model using
layer-wise relevance propagation which can explain the predictions of complex
non-linear classifiers and circumvent their purported black box nature. We show
how character patterns learned by the classifier are rooted in the linguistic
origins of names.
</p>
<a href="http://arxiv.org/abs/2010.14479" target="_blank">arXiv:2010.14479</a> [<a href="http://arxiv.org/pdf/2010.14479" target="_blank">pdf</a>]

<h2>One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. (arXiv:2010.14484v1 [cs.LG])</h2>
<h3>Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn</h3>
<p>While reinforcement learning algorithms can learn effective policies for
complex tasks, these policies are often brittle to even minor task variations,
especially when variations are not explicitly provided during training. One
natural approach to this problem is to train agents with manually specified
variation in the training task or environment. However, this may be infeasible
in practical situations, either because making perturbations is not possible,
or because it is unclear how to choose suitable perturbation strategies without
sacrificing performance. The key insight of this work is that learning diverse
behaviors for accomplishing a task can directly lead to behavior that
generalizes to varying environments, without needing to perform explicit
perturbations during training. By identifying multiple solutions for the task
in a single environment during training, our approach can generalize to new
situations by abandoning solutions that are no longer effective and adopting
those that are. We theoretically characterize a robustness set of environments
that arises from our algorithm and empirically find that our diversity-driven
approach can extrapolate to various changes in the environment and task.
</p>
<a href="http://arxiv.org/abs/2010.14484" target="_blank">arXiv:2010.14484</a> [<a href="http://arxiv.org/pdf/2010.14484" target="_blank">pdf</a>]

<h2>Examining Deep Learning Models with Multiple Data Sources for COVID-19 Forecasting. (arXiv:2010.14491v1 [cs.LG])</h2>
<h3>Lijing Wang, Aniruddha Adiga, Srinivasan Venkatramanan, Jiangzhuo Chen, Bryan Lewis, Madhav Marathe</h3>
<p>The COVID-19 pandemic represents the most significant public health disaster
since the 1918 influenza pandemic. During pandemics such as COVID-19, timely
and reliable spatio-temporal forecasting of epidemic dynamics is crucial. Deep
learning-based time series models for forecasting have recently gained
popularity and have been successfully used for epidemic forecasting. Here we
focus on the design and analysis of deep learning-based models for COVID-19
forecasting. We implement multiple recurrent neural network-based deep learning
models and combine them using the stacking ensemble technique. In order to
incorporate the effects of multiple factors in COVID-19 spread, we consider
multiple sources such as COVID-19 testing data and human mobility data for
better predictions. To overcome the sparsity of training data and to address
the dynamic correlation of the disease, we propose clustering-based training
for high-resolution forecasting. The methods help us to identify the similar
trends of certain groups of regions due to various spatio-temporal effects. We
examine the proposed method for forecasting weekly COVID-19 new confirmed cases
at county-, state-, and country-level. A comprehensive comparison between
different time series models in COVID-19 context is conducted and analyzed. The
results show that simple deep learning models can achieve comparable or better
performance when compared with more complicated models. We are currently
integrating our methods as a part of our weekly forecasts that we provide state
and federal authorities.
</p>
<a href="http://arxiv.org/abs/2010.14491" target="_blank">arXiv:2010.14491</a> [<a href="http://arxiv.org/pdf/2010.14491" target="_blank">pdf</a>]

<h2>$\gamma$-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction. (arXiv:2010.14496v1 [cs.LG])</h2>
<h3>Michael Janner, Igor Mordatch, Sergey Levine</h3>
<p>We introduce the $\gamma$-model, a predictive model of environment dynamics
with an infinite probabilistic horizon. Replacing standard single-step models
with $\gamma$-models leads to generalizations of the procedures that form the
foundation of model-based control, including the model rollout and model-based
value estimation. The $\gamma$-model, trained with a generative
reinterpretation of temporal difference learning, is a natural continuous
analogue of the successor representation and a hybrid between model-free and
model-based mechanisms. Like a value function, it contains information about
the long-term future; like a standard predictive model, it is independent of
task reward. We instantiate the $\gamma$-model as both a generative adversarial
network and normalizing flow, discuss how its training reflects an inescapable
tradeoff between training-time and testing-time compounding errors, and
empirically investigate its utility for prediction and control.
</p>
<a href="http://arxiv.org/abs/2010.14496" target="_blank">arXiv:2010.14496</a> [<a href="http://arxiv.org/pdf/2010.14496" target="_blank">pdf</a>]

<h2>Conservative Safety Critics for Exploration. (arXiv:2010.14497v1 [cs.LG])</h2>
<h3>Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, Animesh Garg</h3>
<p>Safe exploration presents a major challenge in reinforcement learning (RL):
when active data collection requires deploying partially trained policies, we
must ensure that these policies avoid catastrophically unsafe regions, while
still enabling trial and error learning. In this paper, we target the problem
of safe exploration in RL by learning a conservative safety estimate of
environment states through a critic, and provably upper bound the likelihood of
catastrophic failures at every training iteration. We theoretically
characterize the tradeoff between safety and policy improvement, show that the
safety constraints are likely to be satisfied with high probability during
training, derive provable convergence guarantees for our approach, which is no
worse asymptotically than standard RL, and demonstrate the efficacy of the
proposed approach on a suite of challenging navigation, manipulation, and
locomotion tasks. Empirically, we show that the proposed approach can achieve
competitive task performance while incurring significantly lower catastrophic
failure rates during training than prior methods. Videos are at this url
https://sites.google.com/view/conservative-safety-critics/home
</p>
<a href="http://arxiv.org/abs/2010.14497" target="_blank">arXiv:2010.14497</a> [<a href="http://arxiv.org/pdf/2010.14497" target="_blank">pdf</a>]

<h2>Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning. (arXiv:2010.14498v1 [cs.LG])</h2>
<h3>Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, Sergey Levine</h3>
<p>We identify an implicit under-parameterization phenomenon in value-based deep
RL methods that use bootstrapping: when value functions, approximated using
deep neural networks, are trained with gradient descent using iterated
regression onto target values generated by previous instances of the value
network, more gradient updates decrease the expressivity of the current value
network. We characterize this loss of expressivity in terms of a drop in the
rank of the learned value network features, and show that this corresponds to a
drop in performance. We demonstrate this phenomenon on widely studies domains,
including Atari and Gym benchmarks, in both offline and online RL settings. We
formally analyze this phenomenon and show that it results from a pathological
interaction between bootstrapping and gradient-based optimization. We further
show that mitigating implicit under-parameterization by controlling rank
collapse improves performance.
</p>
<a href="http://arxiv.org/abs/2010.14498" target="_blank">arXiv:2010.14498</a> [<a href="http://arxiv.org/pdf/2010.14498" target="_blank">pdf</a>]

<h2>A Bayesian Perspective on Training Speed and Model Selection. (arXiv:2010.14499v1 [cs.LG])</h2>
<h3>Clare Lyle, Lisa Schut, Binxin Ru, Yarin Gal, Mark van der Wilk</h3>
<p>We take a Bayesian perspective to illustrate a connection between training
speed and the marginal likelihood in linear models. This provides two major
insights: first, that a measure of a model's training speed can be used to
estimate its marginal likelihood. Second, that this measure, under certain
conditions, predicts the relative weighting of models in linear model
combinations trained to minimize a regression loss. We verify our results in
model selection tasks for linear models and for the infinite-width limit of
deep neural networks. We further provide encouraging empirical evidence that
the intuition developed in these settings also holds for deep neural networks
trained with stochastic gradient descent. Our results suggest a promising new
direction towards explaining why neural networks trained with stochastic
gradient descent are biased towards functions that generalize well.
</p>
<a href="http://arxiv.org/abs/2010.14499" target="_blank">arXiv:2010.14499</a> [<a href="http://arxiv.org/pdf/2010.14499" target="_blank">pdf</a>]

<h2>COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning. (arXiv:2010.14500v1 [cs.LG])</h2>
<h3>Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, Sergey Levine</h3>
<p>Reinforcement learning has been applied to a wide variety of robotics
problems, but most of such applications involve collecting data from scratch
for each new task. Since the amount of robot data we can collect for any single
task is limited by time and cost considerations, the learned behavior is
typically narrow: the policy can only execute the task in a handful of
scenarios that it was trained on. What if there was a way to incorporate a
large amount of prior data, either from previously solved tasks or from
unsupervised or undirected environment interaction, to extend and generalize
learned behaviors? While most prior work on extending robotic skills using
pre-collected data focuses on building explicit hierarchies or skill
decompositions, we show in this paper that we can reuse prior data to extend
new skills simply through dynamic programming. We show that even when the prior
data does not actually succeed at solving the new task, it can still be
utilized for learning a better policy, by providing the agent with a broader
understanding of the mechanics of its environment. We demonstrate the
effectiveness of our approach by chaining together several behaviors seen in
prior datasets for solving a new task, with our hardest experimental setting
involving composing four robotic skills in a row: picking, placing, drawer
opening, and grasping, where a +1/0 sparse reward is provided only on task
completion. We train our policies in an end-to-end fashion, mapping
high-dimensional image observations to low-level robot control commands, and
present results in both simulated and real world domains. Additional materials
and source code can be found on our project website:
https://sites.google.com/view/cog-rl
</p>
<a href="http://arxiv.org/abs/2010.14500" target="_blank">arXiv:2010.14500</a> [<a href="http://arxiv.org/pdf/2010.14500" target="_blank">pdf</a>]

<h2>Memory Optimization for Deep Networks. (arXiv:2010.14501v1 [cs.LG])</h2>
<h3>Aashaka Shah, Chao-Yuan Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Kr&#xe4;henb&#xfc;hl</h3>
<p>Deep learning is slowly, but steadily, hitting a memory bottleneck. While the
tensor computation in top-of-the-line GPUs increased by 32x over the last five
years, the total available memory only grew by 2.5x. This prevents researchers
from exploring larger architectures, as training large networks requires more
memory for storing intermediate outputs. In this paper, we present MONeT, an
automatic framework that minimizes both the memory footprint and computational
overhead of deep networks. MONeT jointly optimizes the checkpointing schedule
and the implementation of various operators. MONeT is able to outperform all
prior hand-tuned operations as well as automated checkpointing. MONeT reduces
the overall memory requirement by 3x for various PyTorch models, with a 9-16%
overhead in computation. For the same computation cost, \sysname requires
1.2-1.8x less memory than current state-of-the-art automated checkpointing
frameworks. Our code is available at https://github.com/utsaslab/MONeT.
</p>
<a href="http://arxiv.org/abs/2010.14501" target="_blank">arXiv:2010.14501</a> [<a href="http://arxiv.org/pdf/2010.14501" target="_blank">pdf</a>]

<h2>Denoising Prior Driven Deep Neural Network for Image Restoration. (arXiv:1801.06756v2 [cs.CV] UPDATED)</h2>
<h3>Weisheng Dong, Peiyao Wang, Wotao Yin, Guangming Shi, Fangfang Wu, Xiaotong Lu</h3>
<p>Deep neural networks (DNNs) have shown very promising results for various
image restoration (IR) tasks. However, the design of network architectures
remains a major challenging for achieving further improvements. While most
existing DNN-based methods solve the IR problems by directly mapping low
quality images to desirable high-quality images, the observation models
characterizing the image degradation processes have been largely ignored. In
this paper, we first propose a denoising-based IR algorithm, whose iterative
steps can be computed efficiently. Then, the iterative process is unfolded into
a deep neural network, which is composed of multiple denoisers modules
interleaved with back-projection (BP) modules that ensure the observation
consistencies. A convolutional neural network (CNN) based denoiser that can
exploit the multi-scale redundancies of natural images is proposed. As such,
the proposed network not only exploits the powerful denoising ability of DNNs,
but also leverages the prior of the observation model. Through end-to-end
training, both the denoisers and the BP modules can be jointly optimized.
Experimental results on several IR tasks, e.g., image denoisig,
super-resolution and deblurring show that the proposed method can lead to very
competitive and often state-of-the-art results on several IR tasks, including
image denoising, deblurring and super-resolution.
</p>
<a href="http://arxiv.org/abs/1801.06756" target="_blank">arXiv:1801.06756</a> [<a href="http://arxiv.org/pdf/1801.06756" target="_blank">pdf</a>]

<h2>Theory of higher order interpretations and application to Basic Feasible Functions. (arXiv:1801.08350v3 [cs.LO] UPDATED)</h2>
<h3>Emmanuel Hainry, Romain P&#xe9;choux</h3>
<p>Interpretation methods and their restrictions to polynomials have been deeply
used to control the termination and complexity of first-order term rewrite
systems. This paper extends interpretation methods to a pure higher order
functional language. We develop a theory of higher order functions that is
well-suited for the complexity analysis of this programming language. The
interpretation domain is a complete lattice and, consequently, we express
program interpretation in terms of a least fixpoint. As an application, by
bounding interpretations by higher order polynomials, we characterize Basic
Feasible Functions at any order.
</p>
<a href="http://arxiv.org/abs/1801.08350" target="_blank">arXiv:1801.08350</a> [<a href="http://arxiv.org/pdf/1801.08350" target="_blank">pdf</a>]

<h2>Regularized Loss Minimizers with Local Data Perturbation: Consistency and Data Irrecoverability. (arXiv:1805.07645v6 [cs.LG] UPDATED)</h2>
<h3>Zitao Li, Jean Honorio</h3>
<p>We introduce a new concept, data irrecoverability, and show that the
well-studied concept of data privacy is sufficient but not necessary for data
irrecoverability. We show that there are several regularized loss minimization
problems that can use perturbed data with theoretical guarantees of
generalization, i.e., loss consistency. Our results quantitatively connect the
convergence rates of the learning problems to the impossibility for any
adversary for recovering the original data from perturbed observations. In
addition, we show several examples where the convergence rates with perturbed
data only increase the convergence rates with original data within a constant
factor related to the amount of perturbation, i.e., noise.
</p>
<a href="http://arxiv.org/abs/1805.07645" target="_blank">arXiv:1805.07645</a> [<a href="http://arxiv.org/pdf/1805.07645" target="_blank">pdf</a>]

<h2>Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing. (arXiv:1807.00914v3 [cs.CL] UPDATED)</h2>
<h3>Edoardo Maria Ponti, Helen O&#x27;Horan, Yevgeni Berzak, Ivan Vuli&#x107;, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, Anna Korhonen</h3>
<p>Linguistic typology aims to capture structural and semantic variation across
the world's languages. A large-scale typology could provide excellent guidance
for multilingual Natural Language Processing (NLP), particularly for languages
that suffer from the lack of human labeled resources. We present an extensive
literature survey on the use of typological information in the development of
NLP techniques. Our survey demonstrates that to date, the use of information in
existing typological databases has resulted in consistent but modest
improvements in system performance. We show that this is due to both intrinsic
limitations of databases (in terms of coverage and feature granularity) and
under-employment of the typological features included in them. We advocate for
a new approach that adapts the broad and discrete nature of typological
categories to the contextual and continuous nature of machine learning
algorithms used in contemporary NLP. In particular, we suggest that such
approach could be facilitated by recent developments in data-driven induction
of typological knowledge.
</p>
<a href="http://arxiv.org/abs/1807.00914" target="_blank">arXiv:1807.00914</a> [<a href="http://arxiv.org/pdf/1807.00914" target="_blank">pdf</a>]

<h2>F-BLEAU: Fast Black-box Leakage Estimation. (arXiv:1902.01350v2 [cs.CR] UPDATED)</h2>
<h3>Giovanni Cherubin, Konstantinos Chatzikokolakis, Catuscia Palamidessi</h3>
<p>We consider the problem of measuring how much a system reveals about its
secret inputs. We work under the black-box setting: we assume no prior
knowledge of the system's internals, and we run the system for choices of
secrets and measure its leakage from the respective outputs. Our goal is to
estimate the Bayes risk, from which one can derive some of the most popular
leakage measures (e.g., min-entropy, additive, and multiplicative leakage). The
state-of-the-art method for estimating these leakage measures is the
frequentist paradigm, which approximates the system's internals by looking at
the frequencies of its inputs and outputs. Unfortunately, this does not scale
for systems with large output spaces, where it would require too many
input-output examples. Consequently, it also cannot be applied to systems with
continuous outputs (e.g., time side channels, network traffic). In this paper,
we exploit an analogy between Machine Learning (ML) and black-box leakage
estimation to show that the Bayes risk of a system can be estimated by using a
class of ML methods: the universally consistent learning rules; these rules can
exploit patterns in the input-output examples to improve the estimates'
convergence, while retaining formal optimality guarantees. We focus on a set of
them, the nearest neighbor rules; we show that they significantly reduce the
number of black-box queries required for a precise estimation whenever nearby
outputs tend to be produced by the same secret; furthermore, some of them can
tackle systems with continuous outputs. We illustrate the applicability of
these techniques on both synthetic and real-world data, and we compare them
with the state-of-the-art tool, leakiEst, which is based on the frequentist
approach.
</p>
<a href="http://arxiv.org/abs/1902.01350" target="_blank">arXiv:1902.01350</a> [<a href="http://arxiv.org/pdf/1902.01350" target="_blank">pdf</a>]

<h2>Topology of Learning in Artificial Neural Networks. (arXiv:1902.08160v4 [cs.LG] UPDATED)</h2>
<h3>Maxime Gabella</h3>
<p>Understanding how neural networks learn remains one of the central challenges
in machine learning research. From random at the start of training, the weights
of a neural network evolve in such a way as to be able to perform a variety of
tasks, like classifying images. Here we study the emergence of structure in the
weights by applying methods from topological data analysis. We train simple
feedforward neural networks on the MNIST dataset and monitor the evolution of
the weights. When initialized to zero, the weights follow trajectories that
branch off recurrently, thus generating trees that describe the growth of the
effective capacity of each layer. When initialized to tiny random values, the
weights evolve smoothly along two-dimensional surfaces. We show that natural
coordinates on these learning surfaces correspond to important factors of
variation.
</p>
<a href="http://arxiv.org/abs/1902.08160" target="_blank">arXiv:1902.08160</a> [<a href="http://arxiv.org/pdf/1902.08160" target="_blank">pdf</a>]

<h2>Guided Meta-Policy Search. (arXiv:1904.00956v2 [cs.LG] UPDATED)</h2>
<h3>Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, Chelsea Finn</h3>
<p>Reinforcement learning (RL) algorithms have demonstrated promising results on
complex tasks, yet often require impractical numbers of samples since they
learn from scratch. Meta-RL aims to address this challenge by leveraging
experience from previous tasks so as to more quickly solve new tasks. However,
in practice, these algorithms generally also require large amounts of on-policy
experience during the meta-training process, making them impractical for use in
many problems. To this end, we propose to learn a reinforcement learning
procedure in a federated way, where individual off-policy learners can solve
the individual meta-training tasks, and then consolidate these solutions into a
single meta-learner. Since the central meta-learner learns by imitating the
solutions to the individual tasks, it can accommodate either the standard
meta-RL problem setting or a hybrid setting where some or all tasks are
provided with example demonstrations. The former results in an approach that
can leverage policies learned for previous tasks without significant amounts of
on-policy data during meta-training, whereas the latter is particularly useful
in cases where demonstrations are easy for a person to provide. Across a number
of continuous control meta-RL problems, we demonstrate significant improvements
in meta-RL sample efficiency in comparison to prior work as well as the ability
to scale to domains with visual observations.
</p>
<a href="http://arxiv.org/abs/1904.00956" target="_blank">arXiv:1904.00956</a> [<a href="http://arxiv.org/pdf/1904.00956" target="_blank">pdf</a>]

<h2>Memorized Sparse Backpropagation. (arXiv:1905.10194v3 [cs.LG] UPDATED)</h2>
<h3>Zhiyuan Zhang, Pengcheng Yang, Xuancheng Ren, Qi Su, Xu Sun</h3>
<p>Neural network learning is usually time-consuming since backpropagation needs
to compute full gradients and backpropagate them across multiple layers.
Despite its success of existing works in accelerating propagation through
sparseness, the relevant theoretical characteristics remain under-researched
and empirical studies found that they suffer from the loss of information
contained in unpropagated gradients. To tackle these problems, this paper
presents a unified sparse backpropagation framework and provides a detailed
analysis of its theoretical characteristics. Analysis reveals that when applied
to a multilayer perceptron, our framework essentially performs gradient descent
using an estimated gradient similar enough to the true gradient, resulting in
convergence in probability under certain conditions. Furthermore, a simple yet
effective algorithm named memorized sparse backpropagation (MSBP) is proposed
to remedy the problem of information loss by storing unpropagated gradients in
memory for learning in the next steps. Experimental results demonstrate that
the proposed MSBP is effective to alleviate the information loss in traditional
sparse backpropagation while achieving comparable acceleration.
</p>
<a href="http://arxiv.org/abs/1905.10194" target="_blank">arXiv:1905.10194</a> [<a href="http://arxiv.org/pdf/1905.10194" target="_blank">pdf</a>]

<h2>ATRW: A Benchmark for Amur Tiger Re-identification in the Wild. (arXiv:1906.05586v3 [cs.CV] UPDATED)</h2>
<h3>Shuyuan Li, Jianguo Li, Weiyao Lin, Hanlin Tang</h3>
<p>Monitoring the population and movements of endangered species is an important
task to wildlife conversation. Traditional tagging methods do not scale to
large populations, while applying computer vision methods to camera sensor data
requires re-identification (re-ID) algorithms to obtain accurate counts and
moving trajectory of wildlife. However, existing re-ID methods are largely
targeted at persons and cars, which have limited pose variations and
constrained capture environments. This paper tries to fill the gap by
introducing a novel large-scale dataset, the Amur Tiger Re-identification in
the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur
tigers, with bounding box, pose keypoint, and tiger identity annotations. In
contrast to typical re-ID datasets, the tigers are captured in a diverse set of
unconstrained poses and lighting conditions. We demonstrate with a set of
baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we
propose a novel method for tiger re-identification, which introduces precise
pose parts modeling in deep neural networks to handle large pose variation of
tigers, and reaches notable performance improvement over existing re-ID
methods. The dataset is public available at https://cvwc2019.github.io/ .
</p>
<a href="http://arxiv.org/abs/1906.05586" target="_blank">arXiv:1906.05586</a> [<a href="http://arxiv.org/pdf/1906.05586" target="_blank">pdf</a>]

<h2>Selection via Proxy: Efficient Data Selection for Deep Learning. (arXiv:1906.11829v4 [cs.LG] UPDATED)</h2>
<h3>Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, Matei Zaharia</h3>
<p>Data selection methods, such as active learning and core-set selection, are
useful tools for machine learning on large datasets. However, they can be
prohibitively expensive to apply in deep learning because they depend on
feature representations that need to be learned. In this work, we show that we
can greatly improve the computational efficiency by using a small proxy model
to perform data selection (e.g., selecting data points to label for active
learning). By removing hidden layers from the target model, using smaller
architectures, and training for fewer epochs, we create proxies that are an
order of magnitude faster to train. Although these small proxy models have
higher error rates, we find that they empirically provide useful signals for
data selection. We evaluate this "selection via proxy" (SVP) approach on
several data selection tasks across five datasets: CIFAR10, CIFAR100, ImageNet,
Amazon Review Polarity, and Amazon Review Full. For active learning, applying
SVP can give an order of magnitude improvement in data selection runtime (i.e.,
the time it takes to repeatedly train and select points) without significantly
increasing the final error (often within 0.1%). For core-set selection on
CIFAR10, proxies that are over 10x faster to train than their larger, more
accurate targets can remove up to 50% of the data without harming the final
accuracy of the target, leading to a 1.6x end-to-end training time improvement.
</p>
<a href="http://arxiv.org/abs/1906.11829" target="_blank">arXiv:1906.11829</a> [<a href="http://arxiv.org/pdf/1906.11829" target="_blank">pdf</a>]

<h2>Cascade-BGNN: Toward Efficient Self-supervised Representation Learning on Large-scale Bipartite Graphs. (arXiv:1906.11994v3 [cs.SI] UPDATED)</h2>
<h3>Chaoyang He, Tian Xie, Yu Rong, Wenbing Huang, Junzhou Huang, Xiang Ren, Cyrus Shahabi</h3>
<p>Bipartite graphs have been used to represent data relationships in many
data-mining applications such as in E-commerce recommendation systems. Since
learning in graph space is more complicated than in Euclidian space, recent
studies have extensively utilized neural nets to effectively and efficiently
embed a graph's nodes into a multidimensional space. However, this embedding
method has not yet been applied to large-scale bipartite graphs. Existing
techniques either cannot be scaled to large-scale bipartite graphs that have
limited labels or cannot exploit the unique structure of bipartite graphs,
which have distinct node features in two domains. Thus, we propose Cascade
Bipartite Graph Neural Networks, Cascade-BGNN, a novel node representation
learning for bipartite graphs that is domain-consistent, self-supervised, and
efficient. To efficiently aggregate information both across and within the two
partitions of a bipartite graph, BGNN utilizes a customized Inter-domain
Message Passing (IDMP) and Intra-domain Alignment (IDA), which is our
adaptation of adversarial learning, for message aggregation across and within
partitions, respectively. BGNN is trained in a self-supervised manner.
Moreover, we formulate a multi-layer BGNN in a cascaded training manner to
enable multi-hop relationship modeling while improving training efficiency.
Extensive experiments on several datasets of varying scales verify the
effectiveness and efficiency of BGNN over baselines. Our design is further
affirmed through theoretical analysis for domain alignment. The scalability of
BGNN is additionally verified through its demonstrated rapid training speed and
low memory cost over a large-scale real-world bipartite graph.
</p>
<a href="http://arxiv.org/abs/1906.11994" target="_blank">arXiv:1906.11994</a> [<a href="http://arxiv.org/pdf/1906.11994" target="_blank">pdf</a>]

<h2>Multi-Agent Safe Policy Learning for Power Management of Networked Microgrids. (arXiv:1907.02091v4 [eess.SY] UPDATED)</h2>
<h3>Qianzhi Zhang, Kaveh Dehghanpour, Zhaoyu Wang, Feng Qiu, Dongbo Zhao</h3>
<p>This paper presents a supervised multi-agent safe policy learning (SMAS-PL)
method for optimal power management of networked microgrids (MGs) in
distribution systems. While conventional reinforcement learning (RL) algorithms
are black-box decision models that could fail to satisfy grid operational
constraints, our proposed method is constrained by AC power flow equations and
other operational limits. Accordingly, the training process employs the
gradient information of operational constraints to ensure that the optimal
control policy functions generate safe and feasible decisions. Furthermore, we
have developed a distributed consensus-based optimization approach to train the
agents' policy functions while maintaining MGs' privacy and data ownership
boundaries. After training, the learned optimal policy functions can be safely
used by the MGs to dispatch their local resources, without the need to solve a
complex optimization problem from scratch. Numerical experiments have been
devised to verify the performance of the proposed method.
</p>
<a href="http://arxiv.org/abs/1907.02091" target="_blank">arXiv:1907.02091</a> [<a href="http://arxiv.org/pdf/1907.02091" target="_blank">pdf</a>]

<h2>Warfarin dose estimation on multiple datasets with automated hyperparameter optimisation and a novel software framework. (arXiv:1907.05363v4 [q-bio.QM] UPDATED)</h2>
<h3>Gianluca Truda, Patrick Marais</h3>
<p>Warfarin is an effective preventative treatment for arterial and venous
thromboembolism, but requires individualised dosing due to its narrow
therapeutic range and high individual variation. Many machine learning
techniques have been demonstrated in this domain. This study evaluated the
accuracy of the most promising algorithms on the International Warfarin
Pharmacogenetics Consortium dataset and a novel clinical dataset of South
African patients. Support vectors and linear regression were amongst the top
performers in both datasets and performed comparably to recent stacked ensemble
approaches, whilst neural networks were one of the worst performers in both
datasets. We also introduced genetic programming to automatically optimise
model architectures and hyperparameters without human guidance. Remarkably, the
generated models were found to match the performance of the best models
hand-crafted by human experts. Finally, we present a novel software framework
(Warfit-learn) for warfarin dosing research. It leverages the most successful
techniques in preprocessing, imputation, and parallel evaluation, with the goal
of accelerating research and making results in this domain more reproducible.
</p>
<a href="http://arxiv.org/abs/1907.05363" target="_blank">arXiv:1907.05363</a> [<a href="http://arxiv.org/pdf/1907.05363" target="_blank">pdf</a>]

<h2>SubTSBR to tackle high noise and outliers for data-driven discovery of differential equations. (arXiv:1907.07788v4 [stat.ML] UPDATED)</h2>
<h3>Sheng Zhang, Guang Lin</h3>
<p>Data-driven discovery of differential equations has been an emerging research
topic. We propose a novel algorithm subsampling-based threshold sparse Bayesian
regression (SubTSBR) to tackle high noise and outliers. The subsampling
technique is used for improving the accuracy of the Bayesian learning
algorithm. It has two parameters: subsampling size and the number of
subsamples. When the subsampling size increases with fixed total sample size,
the accuracy of our algorithm goes up and then down. When the number of
subsamples increases, the accuracy of our algorithm keeps going up. We
demonstrate how to use our algorithm step by step and compare our algorithm
with threshold sparse Bayesian regression (TSBR) for the discovery of
differential equations. We show that our algorithm produces better results. We
also discuss the merits of discovering differential equations from data and
demonstrate how to discover models with random initial and boundary condition
as well as models with bifurcations. The numerical examples are: (1)
predator-prey model with noise, (2) shallow water equations with outliers, (3)
heat diffusion with random initial and boundary condition, and (4)
fish-harvesting problem with bifurcations.
</p>
<a href="http://arxiv.org/abs/1907.07788" target="_blank">arXiv:1907.07788</a> [<a href="http://arxiv.org/pdf/1907.07788" target="_blank">pdf</a>]

<h2>Rank aggregation for non-stationary data streams. (arXiv:1910.08795v3 [stat.ML] UPDATED)</h2>
<h3>Ekhine Irurozki, Jesus Lobo, Aritz Perez, Javier Del Ser</h3>
<p>We consider the problem of learning over non-stationary ranking streams. The
rankings can be interpreted as the preferences of a population and the
non-stationarity means that the distribution of preferences changes over time.
Our goal is to learn, in an online manner, the current distribution of
rankings. The bottleneck of this process is a rank aggregation problem.

We propose a generalization of the Borda algorithm for non-stationary ranking
streams. Moreover, we give bounds on the minimum number of samples required to
output the ground truth with high probability. Besides, we show how the optimal
parameters are set. Then, we generalize the whole family of weighted voting
rules (the family to which Borda belongs) to situations in which some rankings
are more \textit{reliable} than others and show that this generalization can
solve the problem of rank aggregation over non-stationary data streams.
</p>
<a href="http://arxiv.org/abs/1910.08795" target="_blank">arXiv:1910.08795</a> [<a href="http://arxiv.org/pdf/1910.08795" target="_blank">pdf</a>]

<h2>Autoencoding with a Classifier System. (arXiv:1910.10579v5 [cs.NE] UPDATED)</h2>
<h3>Richard J. Preen, Stewart W. Wilson, Larry Bull</h3>
<p>Autoencoders enable data dimensionality reduction and are a key component of
many learning systems. This article explores the use of a learning classifier
system to perform autoencoding. Initial results using a neural network
representation and combining artificial evolution with stochastic gradient
descent, suggest it is an effective approach to data reduction. The approach
adaptively subdivides the input domain into local approximations that are
simpler than a global neural network solution. By allowing the number of
neurons in the autoencoders to evolve, this further enables the emergence of an
ensemble of structurally heterogeneous solutions to cover the problem space. In
this case, networks of differing complexity are typically seen to cover
different areas of the problem space. Furthermore, the rate of gradient descent
applied to each layer is tuned via self-adaptive mutation, thereby reducing the
parameter optimisation task.
</p>
<a href="http://arxiv.org/abs/1910.10579" target="_blank">arXiv:1910.10579</a> [<a href="http://arxiv.org/pdf/1910.10579" target="_blank">pdf</a>]

<h2>IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary. (arXiv:1910.12903v4 [cs.CR] UPDATED)</h2>
<h3>Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong</h3>
<p>A deep neural network (DNN) classifier represents a model owner's
intellectual property as training a DNN classifier often requires lots of
resource. Watermarking was recently proposed to protect the intellectual
property of DNN classifiers. However, watermarking suffers from a key
limitation: it sacrifices the utility/accuracy of the model owner's classifier
because it tampers the classifier's training or fine-tuning process. In this
work, we propose IPGuard, the first method to protect intellectual property of
DNN classifiers that provably incurs no accuracy loss for the classifiers. Our
key observation is that a DNN classifier can be uniquely represented by its
classification boundary. Based on this observation, IPGuard extracts some data
points near the classification boundary of the model owner's classifier and
uses them to fingerprint the classifier. A DNN classifier is said to be a
pirated version of the model owner's classifier if they predict the same labels
for most fingerprinting data points. IPGuard is qualitatively different from
watermarking. Specifically, IPGuard extracts fingerprinting data points near
the classification boundary of a classifier that is already trained, while
watermarking embeds watermarks into a classifier during its training or
fine-tuning process. We extensively evaluate IPGuard on CIFAR-10, CIFAR-100,
and ImageNet datasets. Our results show that IPGuard can robustly identify
post-processed versions of the model owner's classifier as pirated versions of
the classifier, and IPGuard can identify classifiers, which are not the model
owner's classifier nor its post-processed versions, as non-pirated versions of
the classifier.
</p>
<a href="http://arxiv.org/abs/1910.12903" target="_blank">arXiv:1910.12903</a> [<a href="http://arxiv.org/pdf/1910.12903" target="_blank">pdf</a>]

<h2>Towards Security Threats of Deep Learning Systems: A Survey. (arXiv:1911.12562v2 [cs.CR] UPDATED)</h2>
<h3>Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, Jinwen He</h3>
<p>Deep learning has gained tremendous success and great popularity in the past
few years. However, deep learning systems are suffering several inherent
weaknesses, which can threaten the security of learning models. Deep learning's
wide use further magnifies the impact and consequences. To this end, lots of
research has been conducted with the purpose of exhaustively identifying
intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few
are clear about how these weaknesses are incurred and how effective these
attack approaches are in assaulting deep learning. In order to unveil the
security weaknesses and aid in the development of a robust deep learning
system, we undertake an investigation on attacks towards deep learning, and
analyze these attacks to conclude some findings in multiple views. In
particular, we focus on four types of attacks associated with security threats
of deep learning: model extraction attack, model inversion attack, poisoning
attack and adversarial attack. For each type of attack, we construct its
essential workflow as well as adversary capabilities and attack goals. Pivot
metrics are devised for comparing the attack approaches, by which we perform
quantitative and qualitative analyses. From the analysis, we have identified
significant and indispensable factors in an attack vector, e.g., how to reduce
queries to target models, what distance should be used for measuring
perturbation. We shed light on 18 findings covering these approaches' merits
and demerits, success probability, deployment complexity and prospects.
Moreover, we discuss other potential security weaknesses and possible
mitigation which can inspire relevant research in this area.
</p>
<a href="http://arxiv.org/abs/1911.12562" target="_blank">arXiv:1911.12562</a> [<a href="http://arxiv.org/pdf/1911.12562" target="_blank">pdf</a>]

<h2>Feature-aware Adaptation and Density Alignment for Crowd Counting in Video Surveillance. (arXiv:1912.03672v2 [cs.CV] UPDATED)</h2>
<h3>Junyu Gao, Yuan Yuan, Qi Wang</h3>
<p>With the development of deep neural networks, the performance of crowd
counting and pixel-wise density estimation are continually being refreshed.
Despite this, there are still two challenging problems in this field: 1)
current supervised learning needs a large amount of training data, but
collecting and annotating them is difficult; 2) existing methods can not
generalize well to the unseen domain. A recently released synthetic crowd
dataset alleviates these two problems. However, the domain gap between the
real-world data and synthetic images decreases the models' performance. To
reduce the gap, in this paper, we propose a domain-adaptation-style crowd
counting method, which can effectively adapt the model from synthetic data to
the specific real-world scenes. It consists of Multi-level Featureaware
Adaptation (MFA) and Structured Density map Alignment (SDA). To be specific,
MFA boosts the model to extract domain-invariant features from multiple layers.
SDA guarantees the network outputs fine density maps with a reasonable
distribution on the real domain. Finally, we evaluate the proposed method on
four mainstream surveillance crowd datasets, Shanghai Tech Part B,
WorldExpo'10, Mall and UCSD. Extensive experiments evidence that our approach
outperforms the state-of-the-art methods for the same cross-domain counting
problem.
</p>
<a href="http://arxiv.org/abs/1912.03672" target="_blank">arXiv:1912.03672</a> [<a href="http://arxiv.org/pdf/1912.03672" target="_blank">pdf</a>]

<h2>FootAndBall: Integrated player and ball detector. (arXiv:1912.05445v2 [cs.CV] UPDATED)</h2>
<h3>Jacek Komorowski, Grzegorz Kurzejamski, Grzegorz Sarwas</h3>
<p>The paper describes a deep neural network-based detector dedicated for ball
and players detection in high resolution, long shot, video recordings of soccer
matches. The detector, dubbed FootAndBall, has an efficient fully convolutional
architecture and can operate on input video stream with an arbitrary
resolution. It produces ball confidence map encoding the position of the
detected ball, player confidence map and player bounding boxes tensor encoding
players' positions and bounding boxes. The network uses Feature Pyramid Network
desing pattern, where lower level features with higher spatial resolution are
combined with higher level features with bigger receptive field. This improves
discriminability of small objects (the ball) as larger visual context around
the object of interest is taken into account for the classification. Due to its
specialized design, the network has two orders of magnitude less parameters
than a generic deep neural network-based object detector, such as SSD or YOLO.
This allows real-time processing of high resolution input video stream. Our
code and pre-trained model can be found on the project website:
https://github.com/jac99/FootAndBall .
</p>
<a href="http://arxiv.org/abs/1912.05445" target="_blank">arXiv:1912.05445</a> [<a href="http://arxiv.org/pdf/1912.05445" target="_blank">pdf</a>]

<h2>Learning Deep Attribution Priors Based On Prior Knowledge. (arXiv:1912.10065v3 [cs.LG] UPDATED)</h2>
<h3>Ethan Weinberger, Joseph Janizek, Su-In Lee</h3>
<p>Feature attribution methods, which explain an individual prediction made by a
model as a sum of attributions for each input feature, are an essential tool
for understanding the behavior of complex deep learning models. However,
ensuring that models produce meaningful explanations, rather than ones that
rely on noise, is not straightforward. Exacerbating this problem is the fact
that attribution methods do not provide insight as to why features are assigned
their attribution values, leading to explanations that are difficult to
interpret. In real-world problems we often have sets of additional information
for each feature that are predictive of that feature's importance to the task
at hand. Here, we propose the deep attribution prior (DAPr) framework to
exploit such information to overcome the limitations of attribution methods.
Our framework jointly learns a relationship between prior information and
feature importance, as well as biases models to have explanations that rely on
features predicted to be important. We find that our framework both results in
networks that generalize better to out of sample data and admits new methods
for interpreting model behavior.
</p>
<a href="http://arxiv.org/abs/1912.10065" target="_blank">arXiv:1912.10065</a> [<a href="http://arxiv.org/pdf/1912.10065" target="_blank">pdf</a>]

<h2>Deep neural network models for computational histopathology: A survey. (arXiv:1912.12378v2 [eess.IV] UPDATED)</h2>
<h3>Chetan L. Srinidhi, Ozan Ciga, Anne L. Martel</h3>
<p>Histopathological images contain rich phenotypic information that can be used
to monitor underlying mechanisms contributing to diseases progression and
patient survival outcomes. Recently, deep learning has become the mainstream
methodological choice for analyzing and interpreting cancer histology images.
In this paper, we present a comprehensive review of state-of-the-art deep
learning approaches that have been used in the context of histopathological
image analysis. From the survey of over 130 papers, we review the fields
progress based on the methodological aspect of different machine learning
strategies such as supervised, weakly supervised, unsupervised, transfer
learning and various other sub-variants of these methods. We also provide an
overview of deep learning based survival models that are applicable for
disease-specific prognosis tasks. Finally, we summarize several existing open
datasets and highlight critical challenges and limitations with current deep
learning approaches, along with possible avenues for future research.
</p>
<a href="http://arxiv.org/abs/1912.12378" target="_blank">arXiv:1912.12378</a> [<a href="http://arxiv.org/pdf/1912.12378" target="_blank">pdf</a>]

<h2>How to Support Users in Understanding Intelligent Systems? Structuring the Discussion. (arXiv:2001.08301v4 [cs.HC] UPDATED)</h2>
<h3>Malin Eiband, Daniel Buschek, Heinrich Hussmann</h3>
<p>The opaque nature of many intelligent systems violates established usability
principles and thus presents a challenge for human-computer interaction.
Research in the field therefore highlights the need for transparency,
scrutability, intelligibility, interpretability and explainability, among
others. While all of these terms carry a vision of supporting users in
understanding intelligent systems, the underlying notions and assumptions about
users and their interaction with the system often remain unclear. We review the
literature in HCI through the lens of implied user questions to synthesise a
conceptual framework integrating user mindsets, user involvement, and knowledge
outcomes to reveal, differentiate and classify current notions in prior work.
This framework aims to resolve conceptual ambiguity in the field and enables
researchers to clarify their assumptions and become aware of those made in
prior work. We thus hope to advance and structure the dialogue in the HCI
research community on supporting users in understanding intelligent systems.
</p>
<a href="http://arxiv.org/abs/2001.08301" target="_blank">arXiv:2001.08301</a> [<a href="http://arxiv.org/pdf/2001.08301" target="_blank">pdf</a>]

<h2>LaFurca: Iterative Refined Speech Separation Based on Context-Aware Dual-Path Parallel Bi-LSTM. (arXiv:2001.08998v4 [cs.SD] UPDATED)</h2>
<h3>Ziqiang Shi, Rujie Liu, Jiqing Han</h3>
<p>Deep neural network with dual-path bi-directional long short-term memory
(BiLSTM) block has been proved to be very effective in sequence modeling,
especially in speech separation, e.g. DPRNN-TasNet \cite{luo2019dual}. In this
paper, we propose several improvements of dual-path BiLSTM based network for
end-to-end approach to monaural speech separation. Firstly a dual-path network
with intra-parallel BiLSTM and inter-parallel BiLSTM components is introduced
to reduce performance sub-variances among different branches. Secondly, we
propose to use global context aware inter-intra cross-parallel BiLSTM to
further perceive the global contextual information. Finally, a spiral
multi-stage dual-path BiLSTM is proposed to iteratively refine the separation
results of the previous stages. All these networks take the mixed utterance of
two speakers and map it to two separate utterances, where each utterance
contains only one speaker's voice. For the objective, we propose to train the
network by directly optimizing the utterance level scale-invariant
signal-to-distortion ratio (SI-SDR) in a permutation invariant training (PIT)
style. Our experiments on the public WSJ0-2mix data corpus results in 20.55dB
SDR improvement, 20.35dB SI-SDR improvement, 3.69 of PESQ, and 94.86\% of
ESTOI, which shows our proposed networks can lead to performance improvement on
the speaker separation task. We have open-sourced our re-implementation of the
DPRNN-TasNet in
https://github.com/ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separation,
and our LaFurca is realized based on this implementation of DPRNN-TasNet, it is
believed that the results in this paper can be reproduced with ease.
</p>
<a href="http://arxiv.org/abs/2001.08998" target="_blank">arXiv:2001.08998</a> [<a href="http://arxiv.org/pdf/2001.08998" target="_blank">pdf</a>]

<h2>Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages. (arXiv:2001.11453v2 [cs.CL] UPDATED)</h2>
<h3>Edoardo M. Ponti, Ivan Vuli&#x107;, Ryan Cotterell, Marinela Parovic, Roi Reichart, Anna Korhonen</h3>
<p>Most combinations of NLP tasks and language varieties lack in-domain examples
for supervised training because of the paucity of annotated data. How can
neural models make sample-efficient generalizations from task--language
combinations with available data to low-resource ones? In this work, we propose
a Bayesian generative model for the space of neural parameters. We assume that
this space can be factorized into latent variables for each language and each
task. We infer the posteriors over such latent variables based on data from
seen task--language combinations through variational inference. This enables
zero-shot classification on unseen combinations at prediction time. For
instance, given training data for named entity recognition (NER) in Vietnamese
and for part-of-speech (POS) tagging in Wolof, our model can perform accurate
predictions for NER in Wolof. In particular, we experiment with a typologically
diverse sample of 33 languages from 4 continents and 11 families, and show that
our model yields comparable or better results than state-of-the-art, zero-shot
cross-lingual transfer methods. Moreover, we demonstrate that approximate
Bayesian model averaging results in smoother predictive distributions, whose
entropy strongly correlates with accuracy. Hence, the proposed framework also
offers robust estimates of uncertainty.
</p>
<a href="http://arxiv.org/abs/2001.11453" target="_blank">arXiv:2001.11453</a> [<a href="http://arxiv.org/pdf/2001.11453" target="_blank">pdf</a>]

<h2>Multi-Vehicle Routing Problems with Soft Time Windows: A Multi-Agent Reinforcement Learning Approach. (arXiv:2002.05513v2 [cs.AI] UPDATED)</h2>
<h3>Ke Zhang, Meng Li, Zhengchao Zhang, Xi Lin, Fang He</h3>
<p>Multi-vehicle routing problem with soft time windows (MVRPSTW) is an
indispensable constituent in urban logistics distribution systems. Over the
past decade, numerous methods for MVRPSTW have been proposed, but most are
based on heuristic rules that require a large amount of computation time. With
the current rapid increase of logistics demands, traditional methods incur the
dilemma between computational efficiency and solution quality. To efficiently
solve the problem, we propose a novel reinforcement learning algorithm called
the Multi-Agent Attention Model that can solve routing problem instantly
benefit from lengthy offline training. Specifically, the vehicle routing
problem is regarded as a vehicle tour generation process, and an
encoder-decoder framework with attention layers is proposed to generate tours
of multiple vehicles iteratively. Furthermore, a multi-agent reinforcement
learning method with an unsupervised auxiliary network is developed for the
model training. By evaluated on four synthetic networks with different scales,
the results demonstrate that the proposed method consistently outperforms
Google OR-Tools and traditional methods with little computation time. In
addition, we validate the robustness of the well-trained model by varying the
number of customers and the capacities of vehicles.
</p>
<a href="http://arxiv.org/abs/2002.05513" target="_blank">arXiv:2002.05513</a> [<a href="http://arxiv.org/pdf/2002.05513" target="_blank">pdf</a>]

<h2>Generating Automatic Curricula via Self-Supervised Active Domain Randomization. (arXiv:2002.07911v2 [cs.LG] UPDATED)</h2>
<h3>Sharath Chandra Raparthy, Bhairav Mehta, Florian Golemo, Liam Paull</h3>
<p>Goal-directed Reinforcement Learning (RL) traditionally considers an agent
interacting with an environment, prescribing a real-valued reward to an agent
proportional to the completion of some goal. Goal-directed RL has seen large
gains in sample efficiency, due to the ease of reusing or generating new
experience by proposing goals. One approach,self-play, allows an agent to
"play" against itself by alternatively setting and accomplishing goals,
creating a learned curriculum through which an agent can learn to accomplish
progressively more difficult goals. However, self-play has been limited to goal
curriculum learning or learning progressively harder goals within a single
environment. Recent work on robotic agents has shown that varying the
environment during training, for example with domain randomization, leads to
more robust transfer. As a result, we extend the self-play framework to jointly
learn a goal and environment curriculum, leading to an approach that learns the
most fruitful domain randomization strategy with self-play. Our method,
Self-Supervised Active Domain Randomization(SS-ADR), generates a coupled
goal-task curriculum, where agents learn through progressively more difficult
tasks and environment variations. By encouraging the agent to try tasks that
are just outside of its current capabilities, SS-ADR builds a domain
randomization curriculum that enables state-of-the-art results on
varioussim2real transfer tasks. Our results show that a curriculum of
co-evolving the environment difficulty together with the difficulty of goals
set in each environment provides practical benefits in the goal-directed tasks
tested.
</p>
<a href="http://arxiv.org/abs/2002.07911" target="_blank">arXiv:2002.07911</a> [<a href="http://arxiv.org/pdf/2002.07911" target="_blank">pdf</a>]

<h2>Multi-Agent Reinforcement Learning as a Computational Tool for Language Evolution Research: Historical Context and Future Challenges. (arXiv:2002.08878v2 [cs.MA] UPDATED)</h2>
<h3>Cl&#xe9;ment Moulin-Frier, Pierre-Yves Oudeyer</h3>
<p>Computational models of emergent communication in agent populations are
currently gaining interest in the machine learning community due to recent
advances in Multi-Agent Reinforcement Learning (MARL). Current contributions
are however still relatively disconnected from the earlier theoretical and
computational literature aiming at understanding how language might have
emerged from a prelinguistic substance. The goal of this paper is to position
recent MARL contributions within the historical context of language evolution
research, as well as to extract from this theoretical and computational
background a few challenges for future research.
</p>
<a href="http://arxiv.org/abs/2002.08878" target="_blank">arXiv:2002.08878</a> [<a href="http://arxiv.org/pdf/2002.08878" target="_blank">pdf</a>]

<h2>Fusing Physics-based and Deep Learning Models for Prognostics. (arXiv:2003.00732v2 [eess.SY] UPDATED)</h2>
<h3>Manuel Arias Chao, Chetan Kulkarni, Kai Goebel, Olga Fink</h3>
<p>Physics-based and data-driven models for remaining useful lifetime (RUL)
prediction typically suffer from two major challenges that limit their
applicability to complex real-world domains: (1) incompleteness of
physics-based models and (2) limited representativeness of the training dataset
for data-driven models. Combining the advantages of these two directions while
overcoming some of their limitations, we propose a novel hybrid framework for
fusing the information from physics-based performance models with deep learning
algorithms for prognostics of complex safety-critical systems under real-world
scenarios. In the proposed framework, we use physics-based performance models
to infer unobservable model parameters related to a system's components health
solving a calibration problem. These parameters are subsequently combined with
sensor readings and used as input to a deep neural network to generate a
data-driven prognostics model with physics-augmented features. The performance
of the hybrid framework is evaluated on an extensive case study comprising
run-to-failure degradation trajectories from a fleet of nine turbofan engines
under real flight conditions. The experimental results show that the hybrid
framework outperforms purely data-driven approaches by extending the prediction
horizon by nearly 127\%. Furthermore, it requires less training data and is
less sensitive to the limited representativeness of the dataset compared to
purely data-driven approaches.
</p>
<a href="http://arxiv.org/abs/2003.00732" target="_blank">arXiv:2003.00732</a> [<a href="http://arxiv.org/pdf/2003.00732" target="_blank">pdf</a>]

<h2>A Compiler Assisted Scheduler for Detecting and Mitigating Cache-Based Side Channel Attacks. (arXiv:2003.03850v3 [cs.CR] UPDATED)</h2>
<h3>Sharjeel Khan, Girish Mururu, Santosh Pande</h3>
<p>Side channel attacks steal secret keys by cleverly leveraging information
leakages and can, therefore, break encryption. Thus, detection and mitigation
of side channel attacks is a very important problem, but the solutions proposed
in the literature have limitations in that they do not work in a real-world
multi-tenancy setting on servers, have high false positives, or have high
overheads. In this work, we demonstrate a compiler guided scheduler, Biscuit,
that detects cache-based side channel attacks for processes scheduled on
multi-tenancy server farms. A key element of this solution involves the use of
a cache-miss model which is inserted by the compiler at the entrances of loop
nests to predict the cache misses of the corresponding loop. Such inserted
library calls, or beacons, convey the cache miss information to the scheduler
at run time, which uses it to co-schedule processes such that their combined
cache footprint does not exceed the maximum capacity of the last level cache.
The scheduled processes are then monitored for actual vs predicted cache
misses, and when an anomaly is detected, the scheduler performs a search to
isolate the attacker. We show that Biscuit is able to detect and mitigate
Prime+Probe, Flush+Reload, and Flush+Flush attacks on OpenSSL cryptography
algorithms with an F-score of 1, and also to detect and mitigate degradation of
service on a vision application suite with an F-score of 0.9375. Under a
no-attack scenario, the scheme poses low overheads (up to a maximum of 6
percent). In the case of an attack, the scheme ends up with less than 11
percent overhead and is able to reduce the degradation of service in some cases
by 40 percent. With these many desirable features such as an ability to deal
with multi-tenancy, its ability to detect attacks early, its ability to
mitigate those attacks, and low runtime overheads, Biscuit is a practical
solution.
</p>
<a href="http://arxiv.org/abs/2003.03850" target="_blank">arXiv:2003.03850</a> [<a href="http://arxiv.org/pdf/2003.03850" target="_blank">pdf</a>]

<h2>Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule. (arXiv:2003.03977v2 [cs.LG] UPDATED)</h2>
<h3>Nikhil Iyer, V Thejas, Nipun Kwatra, Ramachandran Ramjee, Muthian Sivathanu</h3>
<p>Several papers argue that wide minima generalize better than narrow minima.
In this paper, through detailed experiments that not only corroborate the
generalization properties of wide minima, we also provide empirical evidence
for a new hypothesis that the density of wide minima is likely lower than the
density of narrow minima. Further, motivated by this hypothesis, we design a
novel explore-exploit learning rate schedule. On a variety of image and natural
language datasets, compared to their original hand-tuned learning rate
baselines, we show that our explore-exploit schedule can result in either up to
0.84\% higher absolute accuracy using the original training budget or up to
57\% reduced training time while achieving the original reported accuracy. For
example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) and
WMT'14 (DE-EN) datasets by just modifying the learning rate schedule of a high
performing model.
</p>
<a href="http://arxiv.org/abs/2003.03977" target="_blank">arXiv:2003.03977</a> [<a href="http://arxiv.org/pdf/2003.03977" target="_blank">pdf</a>]

<h2>Understanding Global Feature Contributions With Additive Importance Measures. (arXiv:2004.00668v2 [cs.LG] UPDATED)</h2>
<h3>Ian Covert, Scott Lundberg, Su-In Lee</h3>
<p>Understanding the inner workings of complex machine learning models is a
long-standing problem and most recent research has focused on local
interpretability. To assess the role of individual input features in a global
sense, we explore the perspective of defining feature importance through the
predictive power associated with each feature. We introduce two notions of
predictive power (model-based and universal) and formalize this approach with a
framework of additive importance measures, which unifies numerous methods in
the literature. We then propose SAGE, a model-agnostic method that quantifies
predictive power while accounting for feature interactions. Our experiments
show that SAGE can be calculated efficiently and that it assigns more accurate
importance values than other methods.
</p>
<a href="http://arxiv.org/abs/2004.00668" target="_blank">arXiv:2004.00668</a> [<a href="http://arxiv.org/pdf/2004.00668" target="_blank">pdf</a>]

<h2>Benchmarking Deep Spiking Neural Networks on Neuromorphic Hardware. (arXiv:2004.01656v3 [cs.NE] UPDATED)</h2>
<h3>Christoph Ostrau, Jonas Homburg, Christian Klarhorst, Michael Thies, Ulrich R&#xfc;ckert</h3>
<p>With more and more event-based neuromorphic hardware systems being developed
at universities and in industry, there is a growing need for assessing their
performance with domain specific measures. In this work, we use the methodology
of converting pre-trained non-spiking to spiking neural networks to evaluate
the performance loss and measure the energy-per-inference for three
neuromorphic hardware systems (BrainScaleS, Spikey, SpiNNaker) and common
simulation frameworks for CPU (NEST) and CPU/GPU (GeNN). For analog hardware we
further apply a re-training technique known as hardware-in-the-loop training to
cope with device mismatch. This analysis is performed for five different
networks, including three networks that have been found by an automated
optimization with a neural architecture search framework. We demonstrate that
the conversion loss is usually below one percent for digital implementations,
and moderately higher for analog systems with the benefit of much lower
energy-per-inference costs.
</p>
<a href="http://arxiv.org/abs/2004.01656" target="_blank">arXiv:2004.01656</a> [<a href="http://arxiv.org/pdf/2004.01656" target="_blank">pdf</a>]

<h2>Dendrite Net: A White-Box Module for Classification, Regression, and System Identification. (arXiv:2004.03955v5 [cs.LG] UPDATED)</h2>
<h3>Gang Liu, Jing Wang</h3>
<p>This paper presents a basic machine learning algorithm, named Dendrite Net or
DD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's
main concept is that the algorithm can recognize this class after learning, if
the output's logical expression contains the corresponding class's logical
relationship among inputs ($ and \backslash or \backslash not $). Experiments
and results: DD, the first white-box machine learning algorithm, showed
excellent system identification performance for the black-box system. Secondly,
it was verified by nine real-world applications that DD brought better
generalization capability relative to MLP architecture that imitated neurons'
cell body (Cell body Net) for regression. Thirdly, by MNIST and FASHION-MNIST
datasets, it was verified that DD showed higher testing accuracy under greater
training loss than Cell body Net for classification. The number of modules can
effectively adjust DD's logical expression capacity, which avoids over-fitting
and makes it easy to get a model with outstanding generalization capability.
Finally, repeated experiments in $ MATLAB $ and $ PyTorch $ ($ Python $)
demonstrated that DD was faster than Cell body Net both in epoch and
forward-propagation. We highlight DD's white-box attribute, controllable
precision for better generalization capability, and lower computational
complexity. Not only can DD be used for generalized engineering, but DD has
vast development potential as a module for deep learning. DD code is available
at https://github.com/liugang1234567/Gang-neuron.
</p>
<a href="http://arxiv.org/abs/2004.03955" target="_blank">arXiv:2004.03955</a> [<a href="http://arxiv.org/pdf/2004.03955" target="_blank">pdf</a>]

<h2>An Adaptive Intelligence Algorithm for Undersampled Knee MRI Reconstruction. (arXiv:2004.07339v2 [eess.IV] UPDATED)</h2>
<h3>Nicola Pezzotti, Sahar Yousefi, Mohamed S. Elmahdy, Jeroen van Gemert, Christophe Sch&#xfc;lke, Mariya Doneva, Tim Nielsen, Sergey Kastryulin, Boudewijn P.F. Lelieveldt, Matthias J.P. van Osch, Elwin de Weerdt, Marius Staring</h3>
<p>Adaptive intelligence aims at empowering machine learning techniques with the
additional use of domain knowledge. In this work, we present the application of
adaptive intelligence to accelerate MR acquisition. Starting from undersampled
k-space data, an iterative learning-based reconstruction scheme inspired by
compressed sensing theory is used to reconstruct the images. We adopt deep
neural networks to refine and correct prior reconstruction assumptions given
the training data. The network was trained and tested on a knee MRI dataset
from the 2019 fastMRI challenge organized by Facebook AI Research and NYU
Langone Health. All submissions to the challenge were initially ranked based on
similarity with a known groundtruth, after which the top 4 submissions were
evaluated radiologically. Our method was evaluated by the fastMRI organizers on
an independent challenge dataset. It ranked #1, shared #1, and #3 on
respectively the 8x accelerated multi-coil, the 4x multi-coil, and the 4x
single-coil track. This demonstrates the superior performance and wide
applicability of the method.
</p>
<a href="http://arxiv.org/abs/2004.07339" target="_blank">arXiv:2004.07339</a> [<a href="http://arxiv.org/pdf/2004.07339" target="_blank">pdf</a>]

<h2>Modeling Extent-of-Texture Information for Ground Terrain Recognition. (arXiv:2004.08141v2 [cs.CV] UPDATED)</h2>
<h3>Shuvozit Ghose, Pinaki Nath Chowdhury, Partha Pratim Roy, Umapada Pal</h3>
<p>Ground Terrain Recognition is a difficult task as the context information
varies significantly over the regions of a ground terrain image. In this paper,
we propose a novel approach towards ground-terrain recognition via modeling the
Extent-of-Texture information to establish a balance between the order-less
texture component and ordered-spatial information locally. At first, the
proposed method uses a CNN backbone feature extractor network to capture
meaningful information of a ground terrain image, and model the extent of
texture and shape information locally. Then, the order-less texture information
and ordered shape information are encoded in a patch-wise manner, which is
utilized by intra-domain message passing module to make every patch aware of
each other for rich feature learning. Next, the Extent-of-Texture (EoT) Guided
Inter-domain Message Passing module combines the extent of texture and shape
information with the encoded texture and shape information in a patch-wise
fashion for sharing knowledge to balance out the order-less texture information
with ordered shape information. Further, Bilinear model generates a pairwise
correlation between the order-less texture information and ordered shape
information. Finally, the ground-terrain image classification is performed by a
fully connected layer. The experimental results indicate superior performance
of the proposed model over existing state-of-the-art techniques on publicly
available datasets like DTD, MINC and GTOS-mobile.
</p>
<a href="http://arxiv.org/abs/2004.08141" target="_blank">arXiv:2004.08141</a> [<a href="http://arxiv.org/pdf/2004.08141" target="_blank">pdf</a>]

<h2>Advanced Equivalence Checking for Quantum Circuits. (arXiv:2004.08420v2 [quant-ph] UPDATED)</h2>
<h3>Lukas Burgholzer, Robert Wille</h3>
<p>Quantum computing will change the way we tackle certain problems. It promises
to dramatically speed-up many chemical, financial, and machine-learning
applications. However, to capitalize on those promises, complex design flows
composed of steps such as compilation, decomposition, or mapping need to be
employed before being able to execute a conceptual quantum algorithm on an
actual device. This results in descriptions at various levels of abstraction
which may significantly differ from each other. The complexity of the
underlying design problems necessitates to not only provide efficient solutions
for the single steps, but also to verify that the originally intended
functionality is preserved throughout all levels of abstraction. This motivates
methods for equivalence checking of quantum circuits. However, most existing
methods are inspired by the classical realm and have merely been extended to
support quantum circuits (i.e., circuits which do not only rely on 0's and 1's,
but also employ superposition and entanglement). In this work, we propose an
advanced methodology which takes the different paradigms of quantum circuits
not only as a burden, but as an opportunity. In fact, the proposed methodology
explicitly utilizes characteristics unique to quantum computing in order to
overcome the shortcomings of existing approaches. We show that, by exploiting
the reversibility of quantum circuits, complexity can be kept feasible in many
cases. Moreover, we show that, in contrast to the classical realm, simulation
is very powerful in verifying quantum circuits. Experimental evaluations
confirm that the resulting methodology allows one to conduct equivalence
checking dramatically faster than ever before--in many cases just a single
simulation run is sufficient. An implementation of the proposed methodology is
publicly available at https://iic.jku.at/eda/research/quantum_verification/.
</p>
<a href="http://arxiv.org/abs/2004.08420" target="_blank">arXiv:2004.08420</a> [<a href="http://arxiv.org/pdf/2004.08420" target="_blank">pdf</a>]

<h2>A Comprehensive Study on Challenges in Deploying Deep Learning Based Software. (arXiv:2005.00760v3 [cs.SE] UPDATED)</h2>
<h3>Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, Xuanzhe Liu</h3>
<p>Deep learning (DL) becomes increasingly pervasive, being used in a wide range
of software applications. These software applications, named as DL based
software (in short as DL software), integrate DL models trained using a large
data corpus with DL programs written based on DL frameworks such as TensorFlow
and Keras. A DL program encodes the network structure of a desirable DL model
and the process by which the model is trained using the training data. To help
developers of DL software meet the new challenges posed by DL, enormous
research efforts in software engineering have been devoted. Existing studies
focus on the development of DL software and extensively analyze faults in DL
programs. However, the deployment of DL software has not been comprehensively
studied. To fill this knowledge gap, this paper presents a comprehensive study
on understanding challenges in deploying DL software. We mine and analyze 3,023
relevant posts from Stack Overflow, a popular Q&amp;A website for developers, and
show the increasing popularity and high difficulty of DL software deployment
among developers. We build a taxonomy of specific challenges encountered by
developers in the process of DL software deployment through manual inspection
of 769 sampled posts and report a series of actionable implications for
researchers, developers, and DL framework vendors.
</p>
<a href="http://arxiv.org/abs/2005.00760" target="_blank">arXiv:2005.00760</a> [<a href="http://arxiv.org/pdf/2005.00760" target="_blank">pdf</a>]

<h2>Learning to Recognize Actionable Static Code Warnings (is Intrinsically Easy). (arXiv:2006.00444v2 [cs.SE] UPDATED)</h2>
<h3>Xueqi Yang, Jianfeng Chen, Rahul Yedida, Zhe Yu, Tim Menzies</h3>
<p>Static code warning tools often generate warnings that programmers ignore.
Such tools can be made more useful via data mining algorithms that select the
"actionable" warnings; i.e. the warnings that are usually not ignored.

In this paper, we look for actionable warnings within a sample of 5,675
actionable warnings seen in 31,058 static code warnings from FindBugs. We find
that data mining algorithms can find actionable warnings with remarkable ease.
Specifically, a range of data mining methods (deep learners, random forests,
decision tree learners, and support vector machines) all achieved very good
results (recalls and AUC (TRN, TPR) measures usually over 95% and false alarms
usually under 5%).

Given that all these learners succeeded so easily, it is appropriate to ask
if there is something about this task that is inherently easy. We report that
while our data sets have up to 58 raw features, those features can be
approximated by less than two underlying dimensions. For such intrinsically
simple data, many different kinds of learners can generate useful models with
similar performance.

Based on the above, we conclude that learning to recognize actionable static
code warnings is easy, using a wide range of learning algorithms, since the
underlying data is intrinsically simple. If we had to pick one particular
learner for this task, we would suggest linear SVMs (since, at least in our
sample, that learner ran relatively quickly and achieved the best median
performance) and we would not recommend deep learning (since this data is
intrinsically very simple).
</p>
<a href="http://arxiv.org/abs/2006.00444" target="_blank">arXiv:2006.00444</a> [<a href="http://arxiv.org/pdf/2006.00444" target="_blank">pdf</a>]

<h2>Equality of Learning Opportunity via Individual Fairness in Personalized Recommendations. (arXiv:2006.04282v2 [cs.IR] UPDATED)</h2>
<h3>Mirko Marras, Ludovico Boratto, Guilherme Ramos, Gianni Fenu</h3>
<p>Online educational platforms are playing a primary role in mediating the
success of individuals' careers. Therefore, while building overlying content
recommendation services, it becomes essential to guarantee that learners are
provided with equal recommended learning opportunities, according to the
platform values, context, and pedagogy. Though the importance of ensuring
equality of learning opportunities has been well investigated in traditional
institutions, how this equality can be operationalized in online learning
ecosystems through recommender systems is still under-explored. In this paper,
we formalize educational principles that model recommendations' learning
properties, and a novel fairness metric that combines them in order to monitor
the equality of recommended learning opportunities among learners. Then, we
envision a scenario wherein an educational platform should be arranged in such
a way that the generated recommendations meet each principle to a certain
degree for all learners, constrained to their individual preferences. Under
this view, we explore the learning opportunities provided by recommender
systems in a large-scale course platform, uncovering systematic inequalities.
To reduce this effect, we propose a novel post-processing approach that
balances personalization and equality of recommended opportunities. Experiments
show that our approach leads to higher equality, with a negligible loss in
personalization. Our study moves a step forward in operationalizing the ethics
of human learning in recommendations, a core unit of intelligent educational
systems.
</p>
<a href="http://arxiv.org/abs/2006.04282" target="_blank">arXiv:2006.04282</a> [<a href="http://arxiv.org/pdf/2006.04282" target="_blank">pdf</a>]

<h2>Gaussian Processes on Graphs via Spectral Kernel Learning. (arXiv:2006.07361v2 [cs.LG] UPDATED)</h2>
<h3>Yin-Cong Zhi, Yin Cheng Ng, Xiaowen Dong</h3>
<p>We propose a graph spectrum-based Gaussian process for prediction of signals
defined on nodes of the graph. The model is designed to capture various graph
signal structures through a highly adaptive kernel that incorporates a flexible
polynomial function in the graph spectral domain. Unlike most existing
approaches, we propose to learn such a spectral kernel, where the polynomial
setup enables learning without the need for eigen-decomposition of the graph
Laplacian. In addition, this kernel has the interpretability of graph filtering
achieved by a bespoke maximum likelihood learning algorithm that enforces the
positivity of the spectrum. We demonstrate the interpretability of the model in
synthetic experiments from which we show the various ground truth spectral
filters can be accurately recovered, and the adaptability translates to
superior performances in the prediction of real-world graph data of various
characteristics.
</p>
<a href="http://arxiv.org/abs/2006.07361" target="_blank">arXiv:2006.07361</a> [<a href="http://arxiv.org/pdf/2006.07361" target="_blank">pdf</a>]

<h2>Semantic Visual Navigation by Watching YouTube Videos. (arXiv:2006.10034v2 [cs.CV] UPDATED)</h2>
<h3>Matthew Chang, Arjun Gupta, Saurabh Gupta</h3>
<p>Semantic cues and statistical regularities in real-world environment layouts
can improve efficiency for navigation in novel environments. This paper learns
and leverages such semantic cues for navigating to objects of interest in novel
environments, by simply watching YouTube videos. This is challenging because
YouTube videos don't come with labels for actions or goals, and may not even
showcase optimal behavior. Our method tackles these challenges through the use
of Q-learning on pseudo-labeled transition quadruples (image, action, next
image, reward). We show that such off-policy Q-learning from passive data is
able to learn meaningful semantic cues for navigation. These cues, when used in
a hierarchical navigation policy, lead to improved efficiency at the ObjectGoal
task in visually realistic simulations. We observe a relative improvement of
15-83% over end-to-end RL, behavior cloning, and classical methods, while using
minimal direct interaction.
</p>
<a href="http://arxiv.org/abs/2006.10034" target="_blank">arXiv:2006.10034</a> [<a href="http://arxiv.org/pdf/2006.10034" target="_blank">pdf</a>]

<h2>On the role of data in PAC-Bayes bounds. (arXiv:2006.10929v2 [cs.LG] UPDATED)</h2>
<h3>Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, Daniel M. Roy</h3>
<p>The dominant term in PAC-Bayes bounds is often the Kullback--Leibler
divergence between the posterior and prior. For so-called linear PAC-Bayes risk
bounds based on the empirical risk of a fixed posterior kernel, it is possible
to minimize the expected value of the bound by choosing the prior to be the
expected posterior, which we call the oracle prior on the account that it is
distribution dependent. In this work, we show that the bound based on the
oracle prior can be suboptimal: In some cases, a stronger bound is obtained by
using a data-dependent oracle prior, i.e., a conditional expectation of the
posterior, given a subset of the training data that is then excluded from the
empirical risk term. While using data to learn a prior is a known heuristic,
its essential role in optimal bounds is new. In fact, we show that using data
can mean the difference between vacuous and nonvacuous bounds. We apply this
new principle in the setting of nonconvex learning, simulating data-dependent
oracle priors on MNIST and Fashion MNIST with and without held-out data, and
demonstrating new nonvacuous bounds in both cases.
</p>
<a href="http://arxiv.org/abs/2006.10929" target="_blank">arXiv:2006.10929</a> [<a href="http://arxiv.org/pdf/2006.10929" target="_blank">pdf</a>]

<h2>DISK: Learning local features with policy gradient. (arXiv:2006.13566v2 [cs.CV] UPDATED)</h2>
<h3>Micha&#x142; J. Tyszkiewicz, Pascal Fua, Eduard Trulls</h3>
<p>Local feature frameworks are difficult to learn in an end-to-end fashion, due
to the discreteness inherent to the selection and matching of sparse keypoints.
We introduce DISK (DIScrete Keypoints), a novel method that overcomes these
obstacles by leveraging principles from Reinforcement Learning (RL), optimizing
end-to-end for a high number of correct feature matches. Our simple yet
expressive probabilistic model lets us keep the training and inference regimes
close, while maintaining good enough convergence properties to reliably train
from scratch. Our features can be extracted very densely while remaining
discriminative, challenging commonly held assumptions about what constitutes a
good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on
three public benchmarks.
</p>
<a href="http://arxiv.org/abs/2006.13566" target="_blank">arXiv:2006.13566</a> [<a href="http://arxiv.org/pdf/2006.13566" target="_blank">pdf</a>]

<h2>Relative gradient optimization of the Jacobian term in unsupervised deep learning. (arXiv:2006.15090v2 [stat.ML] UPDATED)</h2>
<h3>Luigi Gresele, Giancarlo Fissore, Adri&#xe1;n Javaloy, Bernhard Sch&#xf6;lkopf, Aapo Hyv&#xe4;rinen</h3>
<p>Learning expressive probabilistic models correctly describing the data is a
ubiquitous problem in machine learning. A popular approach for solving it is
mapping the observations into a representation space with a simple joint
distribution, which can typically be written as a product of its marginals --
thus drawing a connection with the field of nonlinear independent component
analysis. Deep density models have been widely used for this task, but their
maximum likelihood based training requires estimating the log-determinant of
the Jacobian and is computationally expensive, thus imposing a trade-off
between computation and expressive power. In this work, we propose a new
approach for exact training of such neural networks. Based on relative
gradients, we exploit the matrix structure of neural network parameters to
compute updates efficiently even in high-dimensional spaces; the computational
cost of the training is quadratic in the input size, in contrast with the cubic
scaling of naive approaches. This allows fast training with objective functions
involving the log-determinant of the Jacobian, without imposing constraints on
its structure, in stark contrast to autoregressive normalizing flows.
</p>
<a href="http://arxiv.org/abs/2006.15090" target="_blank">arXiv:2006.15090</a> [<a href="http://arxiv.org/pdf/2006.15090" target="_blank">pdf</a>]

<h2>Guiding Deep Molecular Optimization with Genetic Exploration. (arXiv:2007.04897v3 [q-bio.QM] UPDATED)</h2>
<h3>Sungsoo Ahn, Junsu Kim, Hankook Lee, Jinwoo Shin</h3>
<p>De novo molecular design attempts to search over the chemical space for
molecules with the desired property. Recently, deep learning has gained
considerable attention as a promising approach to solve the problem. In this
paper, we propose genetic expert-guided learning (GEGL), a simple yet novel
framework for training a deep neural network (DNN) to generate highly-rewarding
molecules. Our main idea is to design a "genetic expert improvement" procedure,
which generates high-quality targets for imitation learning of the DNN.
Extensive experiments show that GEGL significantly improves over
state-of-the-art methods. For example, GEGL manages to solve the penalized
octanol-water partition coefficient optimization with a score of 31.40, while
the best-known score in the literature is 27.22. Besides, for the GuacaMol
benchmark with 20 tasks, our method achieves the highest score for 19 tasks, in
comparison with state-of-the-art methods, and newly obtains the perfect score
for three tasks.
</p>
<a href="http://arxiv.org/abs/2007.04897" target="_blank">arXiv:2007.04897</a> [<a href="http://arxiv.org/pdf/2007.04897" target="_blank">pdf</a>]

<h2>Generalization bound of globally optimal non-convex neural network training: Transportation map estimation by infinite dimensional Langevin dynamics. (arXiv:2007.05824v2 [cs.LG] UPDATED)</h2>
<h3>Taiji Suzuki</h3>
<p>We introduce a new theoretical framework to analyze deep learning
optimization with connection to its generalization error. Existing frameworks
such as mean field theory and neural tangent kernel theory for neural network
optimization analysis typically require taking limit of infinite width of the
network to show its global convergence. This potentially makes it difficult to
directly deal with finite width network; especially in the neural tangent
kernel regime, we cannot reveal favorable properties of neural networks beyond
kernel methods. To realize more natural analysis, we consider a completely
different approach in which we formulate the parameter training as a
transportation map estimation and show its global convergence via the theory of
the infinite dimensional Langevin dynamics. This enables us to analyze narrow
and wide networks in a unifying manner. Moreover, we give generalization gap
and excess risk bounds for the solution obtained by the dynamics. The excess
risk bound achieves the so-called fast learning rate. In particular, we show an
exponential convergence for a classification problem and a minimax optimal rate
for a regression problem.
</p>
<a href="http://arxiv.org/abs/2007.05824" target="_blank">arXiv:2007.05824</a> [<a href="http://arxiv.org/pdf/2007.05824" target="_blank">pdf</a>]

<h2>Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware Clues. (arXiv:2007.09355v2 [cs.CV] UPDATED)</h2>
<h3>Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, Jing Shao</h3>
<p>As realistic facial manipulation technologies have achieved remarkable
progress, social concerns about potential malicious abuse of these technologies
bring out an emerging research topic of face forgery detection. However, it is
extremely challenging since recent advances are able to forge faces beyond the
perception ability of human eyes, especially in compressed images and videos.
We find that mining forgery patterns with the awareness of frequency could be a
cure, as frequency provides a complementary viewpoint where either subtle
forgery artifacts or compression errors could be well described. To introduce
frequency into the face forgery detection, we propose a novel Frequency in Face
Forgery Network (F3-Net), taking advantages of two different but complementary
frequency-aware clues, 1) frequency-aware decomposed image components, and 2)
local frequency statistics, to deeply mine the forgery patterns via our
two-stream collaborative learning framework. We apply DCT as the applied
frequency-domain transformation. Through comprehensive studies, we show that
the proposed F3-Net significantly outperforms competing state-of-the-art
methods on all compression qualities in the challenging FaceForensics++
dataset, especially wins a big lead upon low-quality media.
</p>
<a href="http://arxiv.org/abs/2007.09355" target="_blank">arXiv:2007.09355</a> [<a href="http://arxiv.org/pdf/2007.09355" target="_blank">pdf</a>]

<h2>DEAL: Deep Evidential Active Learning for Image Classification. (arXiv:2007.11344v2 [cs.LG] UPDATED)</h2>
<h3>Patrick Hemmer, Niklas K&#xfc;hl, Jakob Sch&#xf6;ffer</h3>
<p>Convolutional Neural Networks (CNNs) have proven to be state-of-the-art
models for supervised computer vision tasks, such as image classification.
However, large labeled data sets are generally needed for the training and
validation of such models. In many domains, unlabeled data is available but
labeling is expensive, for instance when specific expert knowledge is required.
Active Learning (AL) is one approach to mitigate the problem of limited labeled
data. Through selecting the most informative and representative data instances
for labeling, AL can contribute to more efficient learning of the model. Recent
AL methods for CNNs propose different solutions for the selection of instances
to be labeled. However, they do not perform consistently well and are often
computationally expensive. In this paper, we propose a novel AL algorithm that
efficiently learns from unlabeled data by capturing high prediction
uncertainty. By replacing the softmax standard output of a CNN with the
parameters of a Dirichlet density, the model learns to identify data instances
that contribute efficiently to improving model performance during training. We
demonstrate in several experiments with publicly available data that our method
consistently outperforms other state-of-the-art AL approaches. It can be easily
implemented and does not require extensive computational resources for
training. Additionally, we are able to show the benefits of the approach on a
real-world medical use case in the field of automated detection of visual
signals for pneumonia on chest radiographs.
</p>
<a href="http://arxiv.org/abs/2007.11344" target="_blank">arXiv:2007.11344</a> [<a href="http://arxiv.org/pdf/2007.11344" target="_blank">pdf</a>]

<h2>A Systematic Literature Review on Federated Machine Learning: From A Software Engineering Perspective. (arXiv:2007.11354v6 [cs.SE] UPDATED)</h2>
<h3>Sin Kit Lo, Qinghua Lu, Chen Wang, Hye-Young Paik, Liming Zhu</h3>
<p>Federated learning is an emerging machine learning paradigm where multiple
clients train models locally and formulate a global model based on the local
model updates. To identify the state-of-the-art in federated learning from a
software engineering perspective, we performed a systematic literature review
with the extracted 231 primary studies. The results show that most of the known
motivations of federated learning appear to be the most studied federated
learning challenges, such as communication efficiency and statistical
heterogeneity. Also, there are only a few real-world applications of federated
learning. Hence, more studies in this area are needed before the actual
industrial-level adoption of federated learning.
</p>
<a href="http://arxiv.org/abs/2007.11354" target="_blank">arXiv:2007.11354</a> [<a href="http://arxiv.org/pdf/2007.11354" target="_blank">pdf</a>]

<h2>Bootstrapping Neural Processes. (arXiv:2008.02956v2 [cs.LG] UPDATED)</h2>
<h3>Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, Yee Whye Teh</h3>
<p>Unlike in the traditional statistical modeling for which a user typically
hand-specify a prior, Neural Processes (NPs) implicitly define a broad class of
stochastic processes with neural networks. Given a data stream, NP learns a
stochastic process that best describes the data. While this "data-driven" way
of learning stochastic processes has proven to handle various types of data,
NPs still rely on an assumption that uncertainty in stochastic processes is
modeled by a single latent variable, which potentially limits the flexibility.
To this end, we propose the Boostrapping Neural Process (BNP), a novel
extension of the NP family using the bootstrap. The bootstrap is a classical
data-driven technique for estimating uncertainty, which allows BNP to learn the
stochasticity in NPs without assuming a particular form. We demonstrate the
efficacy of BNP on various types of data and its robustness in the presence of
model-data mismatch.
</p>
<a href="http://arxiv.org/abs/2008.02956" target="_blank">arXiv:2008.02956</a> [<a href="http://arxiv.org/pdf/2008.02956" target="_blank">pdf</a>]

<h2>A Functional Perspective on Learning Symmetric Functions with Neural Networks. (arXiv:2008.06952v2 [cs.LG] UPDATED)</h2>
<h3>Aaron Zweig, Joan Bruna</h3>
<p>Symmetric functions, which take as input an unordered, fixed-size set, are
known to be universally representable by neural networks that enforce
permutation invariance. These architectures only give guarantees for fixed
input sizes, yet in many practical applications, including point clouds and
particle physics, a relevant notion of generalization should include varying
the input size. In this work we treat symmetric functions (of any size) as
functions over probability measures, and study the learning and representation
of neural networks defined on measures. By focusing on shallow architectures,
we establish approximation and generalization bounds under different choices of
regularization (such as RKHS and variation norms), that capture a hierarchy of
functional spaces with increasing degree of non-linear learning. The resulting
models can be learned efficiently and enjoy generalization guarantees that
extend across input sizes, as we verify empirically.
</p>
<a href="http://arxiv.org/abs/2008.06952" target="_blank">arXiv:2008.06952</a> [<a href="http://arxiv.org/pdf/2008.06952" target="_blank">pdf</a>]

<h2>AIPerf: Automated machine learning as an AI-HPC benchmark. (arXiv:2008.07141v6 [cs.DC] UPDATED)</h2>
<h3>Zhixiang Ren, Yongheng Liu, Tianhui Shi, Lei Xie, Yue Zhou, Jidong Zhai, Youhui Zhang, Yunquan Zhang, Wenguang Chen</h3>
<p>The plethora of complex artificial intelligence (AI) algorithms and available
high performance computing (HPC) power stimulates the expeditious development
of AI components with heterogeneous designs. Consequently, the need for
cross-stack performance benchmarking of AI-HPC systems emerges rapidly. The
current HPC benchmarks can not reflect AI computing power without
representative workloads and the current AI benchmarks have fixed problem size
therefore limited scalability. To address these issues, we propose an
end-to-end benchmark suite utilizing automated machine learning (AutoML) that
represents real AI scenarios. More importantly, AutoML is auto-adaptive to
various scales of machines with an extreme computational cost therefore a
desired workload. We implement the algorithms in a highly parallel and flexible
way to ensure the efficiency and optimization potential on diverse systems with
customizable configurations. The major metric to quantify the performance is
floating-point operations per second (FLOPS) that is measured in an analytical
and systematic approach. We verify the benchmark's stability at discrete
timestamps and the linear scalability on various numbers of machines equipped
with up to 400 AI accelerators. With flexible workload size as well as single
metric measurement, our benchmark can scale from small clusters to large AI-HPC
and rank them easily. The source code, specifications and detailed procedures
are publicly accessible on GitHub.
</p>
<a href="http://arxiv.org/abs/2008.07141" target="_blank">arXiv:2008.07141</a> [<a href="http://arxiv.org/pdf/2008.07141" target="_blank">pdf</a>]

<h2>AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. (arXiv:2008.11869v3 [cs.CL] UPDATED)</h2>
<h3>Xinsong Zhang, Hang Li</h3>
<p>Pre-trained language models such as BERT have exhibited remarkable
performances in many tasks in natural language understanding (NLU). The tokens
in the models are usually fine-grained in the sense that for languages like
English they are words or sub-words and for languages like Chinese they are
characters. In English, for example, there are multi-word expressions which
form natural lexical units and thus the use of coarse-grained tokenization also
appears to be reasonable. In fact, both fine-grained and coarse-grained
tokenizations have advantages and disadvantages for learning of pre-trained
language models. In this paper, we propose a novel pre-trained language model,
referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained
and coarse-grained tokenizations. For English, AMBERT takes both the sequence
of words (fine-grained tokens) and the sequence of phrases (coarse-grained
tokens) as input after tokenization, employs one encoder for processing the
sequence of words and the other encoder for processing the sequence of the
phrases, utilizes shared parameters between the two encoders, and finally
creates a sequence of contextualized representations of the words and a
sequence of contextualized representations of the phrases. Experiments have
been conducted on benchmark datasets for Chinese and English, including CLUE,
GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing
best performing models in almost all cases, particularly the improvements are
significant for Chinese.
</p>
<a href="http://arxiv.org/abs/2008.11869" target="_blank">arXiv:2008.11869</a> [<a href="http://arxiv.org/pdf/2008.11869" target="_blank">pdf</a>]

<h2>Benchmarking Metric Ground Navigation. (arXiv:2008.13315v2 [cs.RO] UPDATED)</h2>
<h3>Daniel Perille, Abigail Truong, Xuesu Xiao, Peter Stone</h3>
<p>Metric ground navigation addresses the problem of autonomously moving a robot
from one point to another in an obstacle-occupied planar environment in a
collision-free manner. It is one of the most fundamental capabilities of
intelligent mobile robots. This paper presents a standardized testbed with a
set of environments and metrics to benchmark difficulty of different scenarios
and performance of different systems of metric ground navigation. Current
benchmarks focus on individual components of mobile robot navigation, such as
perception and state estimation, but the navigation performance as a whole is
rarely measured in a systematic and standardized fashion. As a result,
navigation systems are usually tested and compared in an ad hoc manner, such as
in one or two manually chosen environments. The introduced benchmark provides a
general testbed for ground robot navigation in a metric world. The Benchmark
for Autonomous Robot Navigation (BARN) dataset includes 300 navigation
environments, which are ordered by a set of difficulty metrics. Navigation
performance can be tested and compared in those environments in a systematic
and objective fashion. This benchmark can be used to predict navigation
difficulty of a new environment, compare navigation systems, and potentially
serve as a cost function and a curriculum for planning-based and learning-based
navigation systems. We have published our dataset and the source code to
generate datasets for different robot footprints at
www.cs.utexas.edu/attruong/metrics_dataset.html.
</p>
<a href="http://arxiv.org/abs/2008.13315" target="_blank">arXiv:2008.13315</a> [<a href="http://arxiv.org/pdf/2008.13315" target="_blank">pdf</a>]

<h2>FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning. (arXiv:2009.01974v2 [cs.LG] UPDATED)</h2>
<h3>Hong-You Chen, Wei-Lun Chao</h3>
<p>Federated learning aims to collaboratively train a strong global model by
accessing users' locally trained models but not their own data. A crucial step
is therefore to aggregate local models into a global model, which has been
shown challenging when users have non-i.i.d. data. In this paper, we propose a
novel aggregation algorithm named FedBE, which takes a Bayesian inference
perspective by sampling higher-quality global models and combining them via
Bayesian model Ensemble, leading to much robust aggregation. We show that an
effective model distribution can be constructed by simply fitting a Gaussian or
Dirichlet distribution to the local models. Our empirical studies validate
FedBE's superior performance, especially when users' data are not i.i.d. and
when the neural networks go deeper. Moreover, FedBE is compatible with recent
efforts in regularizing users' model training, making it an easily applicable
module: you only need to replace the aggregation method but leave other parts
of your federated learning algorithm intact.
</p>
<a href="http://arxiv.org/abs/2009.01974" target="_blank">arXiv:2009.01974</a> [<a href="http://arxiv.org/pdf/2009.01974" target="_blank">pdf</a>]

<h2>Distilled One-Shot Federated Learning. (arXiv:2009.07999v2 [cs.LG] UPDATED)</h2>
<h3>Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, Dapeng Wu</h3>
<p>Current federated learning algorithms take tens of communication rounds
transmitting unwieldy model weights under ideal circumstances and hundreds when
data is poorly distributed. Inspired by recent work on dataset distillation and
distributed one-shot learning, we propose Distilled One-Shot Federated Learning
to reduce the number of communication rounds required to train a performant
model to only one. Each client distills their private dataset and sends the
synthetic data (e.g. images or sentences) to the server. The distilled data
look like noise and become useless after model fitting. We empirically show
that our method can achieve 96\% test accuracy on federated MNIST with LeNet
(centralized 99\%), 81\% on federated IMDB with a CNN (centralized 86\%), 84\%
on federated TREC-6 with a Bi-LSTM (centralized 89\%), and 81\% on federated
SENT140 with a CNN (centralized 84\%). Using only one round, DOSFL can match
the centralized baseline on all four tasks. By evading the need for model-wise
updates (i.e., weights, gradients, loss, etc.), the total communication cost of
DOSFL is reduced by over an order of magnitude. Notably, DOSFL beats FedAvg on
three tasks using the same amount of communication resources. We believe that
DOSFL represents a new direction orthogonal to previous work, towards
weight-less and gradient-less federated learning.
</p>
<a href="http://arxiv.org/abs/2009.07999" target="_blank">arXiv:2009.07999</a> [<a href="http://arxiv.org/pdf/2009.07999" target="_blank">pdf</a>]

<h2>MFIF-GAN: A New Generative Adversarial Network for Multi-Focus Image Fusion. (arXiv:2009.09718v3 [cs.CV] UPDATED)</h2>
<h3>Yicheng Wang, Shuang Xu, Junmin Liu, Zixiang Zhao, Chunxia Zhang, Jiangshe Zhang</h3>
<p>Multi-Focus Image Fusion (MFIF) is one of the promising techniques to obtain
all-in-focus images meeting visual needs and it is a precondition of other
computer vision tasks. One of the research trends of MFIF is to avoid the
defocus spread effect (DSE) around the focus/defocus boundary (FDB). In this
paper,we propose a network termed MFIF-GAN to attenuate the DSE by generating
focus maps in which the foreground region are correctly larger than the
corresponding objects. The Squeeze and Excitation Residual module as an
attention mechanism is employed in the network. By combining the prior
knowledge of training condition, this network is trained on a synthetic dataset
based on an {\alpha}-matte model. In addition, the reconstruction and gradient
regularization terms are combined in the loss functions to enhance the boundary
details and improve the quality of fused images. Experimental results
demonstrate that the MFIF-GAN outperforms several state-of-the-art (SOTA)
methods in visual perception, quantitative analysis as well as efficiency.
Moreover, the edge diffusion and contraction module is firstly proposed to
verify that focus maps generated by our method are accurate at the pixel level.
</p>
<a href="http://arxiv.org/abs/2009.09718" target="_blank">arXiv:2009.09718</a> [<a href="http://arxiv.org/pdf/2009.09718" target="_blank">pdf</a>]

<h2>Exploiting Vietnamese Social Media Characteristics for Textual Emotion Recognition in Vietnamese. (arXiv:2009.11005v3 [cs.CL] UPDATED)</h2>
<h3>Khang Phuoc-Quy Nguyen, Kiet Van Nguyen</h3>
<p>Textual emotion recognition has been a promising research topic in recent
years. Many researchers aim to build more accurate and robust emotion detection
systems. In this paper, we conduct several experiments to indicate how data
pre-processing affects a machine learning method on textual emotion
recognition. These experiments are performed on the Vietnamese Social Media
Emotion Corpus (UIT-VSMEC) as the benchmark dataset. We explore Vietnamese
social media characteristics to propose different pre-processing techniques,
and key-clause extraction with emotional context to improve the machine
performance on UIT-VSMEC. Our experimental evaluation shows that with
appropriate pre-processing techniques based on Vietnamese social media
characteristics, Multinomial Logistic Regression (MLR) achieves the best
F1-score of 64.40%, a significant improvement of 4.66% over the CNN model built
by the authors of UIT-VSMEC (59.74%).
</p>
<a href="http://arxiv.org/abs/2009.11005" target="_blank">arXiv:2009.11005</a> [<a href="http://arxiv.org/pdf/2009.11005" target="_blank">pdf</a>]

<h2>Deep multi-stations weather forecasting: explainable recurrent convolutional neural networks. (arXiv:2009.11239v4 [cs.LG] UPDATED)</h2>
<h3>Ismail Alaoui Abdellaoui, Siamak Mehrkanoon</h3>
<p>Deep learning applied to weather forecasting has started gaining popularity
because of the progress achieved by data-driven models. The present paper
compares four different deep learning architectures to perform weather
prediction on daily data gathered from 18 cities across Europe and spanned over
a period of 15 years. The four proposed models investigate the different type
of input representations (i.e. tensorial unistream vs. multi-stream matrices)
as well as the combination of convolutional neural networks and LSTM (i.e.
cascaded vs. ConvLSTM). In particular, we show that a model that uses a
multi-stream input representation and that processes each lag individually
combined with a cascaded convolution and LSTM is capable of better forecasting
than the other compared models. In addition, we show that visualization
techniques such as occlusion analysis and score maximization can give an
additional insight on the most important features and cities for predicting a
particular target feature and city.
</p>
<a href="http://arxiv.org/abs/2009.11239" target="_blank">arXiv:2009.11239</a> [<a href="http://arxiv.org/pdf/2009.11239" target="_blank">pdf</a>]

<h2>Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions. (arXiv:2009.14259v2 [cs.CL] UPDATED)</h2>
<h3>Peter A. Jansen</h3>
<p>The recently proposed ALFRED challenge task aims for a virtual robotic agent
to complete complex multi-step everyday tasks in a virtual home environment
from high-level natural language directives, such as "put a hot piece of bread
on a plate". Currently, the best-performing models are able to complete less
than 5% of these tasks successfully. In this work we focus on modeling the
translation problem of converting natural language directives into detailed
multi-step sequences of actions that accomplish those goals in the virtual
environment. We empirically demonstrate that it is possible to generate gold
multi-step plans from language directives alone without any visual input in 26%
of unseen cases. When a small amount of visual information is incorporated,
namely the starting location in the virtual environment, our best-performing
GPT-2 model successfully generates gold command sequences in 58% of cases. Our
results suggest that contextualized language models may provide strong visual
semantic planning modules for grounded virtual agents.
</p>
<a href="http://arxiv.org/abs/2009.14259" target="_blank">arXiv:2009.14259</a> [<a href="http://arxiv.org/pdf/2009.14259" target="_blank">pdf</a>]

<h2>Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples. (arXiv:2010.03593v2 [stat.ML] UPDATED)</h2>
<h3>Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, Pushmeet Kohli</h3>
<p>Adversarial training and its variants have become de facto standards for
learning robust deep neural networks. In this paper, we explore the landscape
around adversarial training in a bid to uncover its limits. We systematically
study the effect of different training losses, model sizes, activation
functions, the addition of unlabeled data (through pseudo-labeling) and other
factors on adversarial robustness. We discover that it is possible to train
robust models that go well beyond state-of-the-art results by combining larger
models, Swish/SiLU activations and model weight averaging. We demonstrate large
improvements on CIFAR-10 and CIFAR-100 against $\ell_\infty$ and $\ell_2$
norm-bounded perturbations of size $8/255$ and $128/255$, respectively. In the
setting with additional unlabeled data, we obtain an accuracy under attack of
65.88% against $\ell_\infty$ perturbations of size $8/255$ on CIFAR-10 (+6.35%
with respect to prior art). Without additional data, we obtain an accuracy
under attack of 57.20% (+3.46%). To test the generality of our findings and
without any additional modifications, we obtain an accuracy under attack of
80.53% (+7.62%) against $\ell_2$ perturbations of size $128/255$ on CIFAR-10,
and of 36.88% (+8.46%) against $\ell_\infty$ perturbations of size $8/255$ on
CIFAR-100.
</p>
<a href="http://arxiv.org/abs/2010.03593" target="_blank">arXiv:2010.03593</a> [<a href="http://arxiv.org/pdf/2010.03593" target="_blank">pdf</a>]

<h2>An Open Review of OpenReview: A Critical Analysis of the Machine Learning Conference Review Process. (arXiv:2010.05137v2 [cs.LG] UPDATED)</h2>
<h3>David Tran, Alex Valtchanov, Keshav Ganapathy, Raymond Feng, Eric Slud, Micah Goldblum, Tom Goldstein</h3>
<p>Mainstream machine learning conferences have seen a dramatic increase in the
number of participants, along with a growing range of perspectives, in recent
years. Members of the machine learning community are likely to overhear
allegations ranging from randomness of acceptance decisions to institutional
bias. In this work, we critically analyze the review process through a
comprehensive study of papers submitted to ICLR between 2017 and 2020. We
quantify reproducibility/randomness in review scores and acceptance decisions,
and examine whether scores correlate with paper impact. Our findings suggest
strong institutional bias in accept/reject decisions, even after controlling
for paper quality. Furthermore, we find evidence for a gender gap, with female
authors receiving lower scores, lower acceptance rates, and fewer citations per
paper than their male counterparts. We conclude our work with recommendations
for future conference organizers.
</p>
<a href="http://arxiv.org/abs/2010.05137" target="_blank">arXiv:2010.05137</a> [<a href="http://arxiv.org/pdf/2010.05137" target="_blank">pdf</a>]

<h2>Interpretable Neural Networks for Panel Data Analysis in Economics. (arXiv:2010.05311v2 [econ.EM] UPDATED)</h2>
<h3>Yucheng Yang, Zhong Zheng, Weinan E</h3>
<p>The lack of interpretability and transparency are preventing economists from
using advanced tools like neural networks in their empirical work. In this
paper, we propose a new class of interpretable neural network models that can
achieve both high prediction accuracy and interpretability in regression
problems with time series cross-sectional data. Our model can essentially be
written as a simple function of a limited number of interpretable features. In
particular, we incorporate a class of interpretable functions named persistent
change filters as part of the neural network. We apply this model to predicting
individual's monthly employment status using high-dimensional administrative
data in China. We achieve an accuracy of 94.5% on the out-of-sample test set,
which is comparable to the most accurate conventional machine learning methods.
Furthermore, the interpretability of the model allows us to understand the
mechanism that underlies the ability for predicting employment status using
administrative data: an individual's employment status is closely related to
whether she pays different types of insurances. Our work is a useful step
towards overcoming the "black box" problem of neural networks, and provide a
promising new tool for economists to study administrative and proprietary big
data.
</p>
<a href="http://arxiv.org/abs/2010.05311" target="_blank">arXiv:2010.05311</a> [<a href="http://arxiv.org/pdf/2010.05311" target="_blank">pdf</a>]

<h2>Active learning with RESSPECT: Resource allocation for extragalactic astronomical transients. (arXiv:2010.05941v2 [astro-ph.IM] UPDATED)</h2>
<h3>Noble Kennamer, Emille E. O. Ishida, Santiago Gonzalez-Gaitan, Rafael S. de Souza, Alexander Ihler, Kara Ponder, Ricardo Vilalta, Anais Moller, David O. Jones, Mi Dai, Alberto Krone-Martins, Bruno Quint, Sreevarsha Sreejith, Alex I. Malz, Lluis Galbany (The LSST Dark Energy Science Collaboration and the COIN collaboration)</h3>
<p>The recent increase in volume and complexity of available astronomical data
has led to a wide use of supervised machine learning techniques. Active
learning strategies have been proposed as an alternative to optimize the
distribution of scarce labeling resources. However, due to the specific
conditions in which labels can be acquired, fundamental assumptions, such as
sample representativeness and labeling cost stability cannot be fulfilled. The
Recommendation System for Spectroscopic follow-up (RESSPECT) project aims to
enable the construction of optimized training samples for the Rubin Observatory
Legacy Survey of Space and Time (LSST), taking into account a realistic
description of the astronomical data environment. In this work, we test the
robustness of active learning techniques in a realistic simulated astronomical
data scenario. Our experiment takes into account the evolution of training and
pool samples, different costs per object, and two different sources of budget.
Results show that traditional active learning strategies significantly
outperform random sampling. Nevertheless, more complex batch strategies are not
able to significantly overcome simple uncertainty sampling techniques. Our
findings illustrate three important points: 1) active learning strategies are a
powerful tool to optimize the label-acquisition task in astronomy, 2) for
upcoming large surveys like LSST, such techniques allow us to tailor the
construction of the training sample for the first day of the survey, and 3) the
peculiar data environment related to the detection of astronomical transients
is a fertile ground that calls for the development of tailored machine learning
algorithms.
</p>
<a href="http://arxiv.org/abs/2010.05941" target="_blank">arXiv:2010.05941</a> [<a href="http://arxiv.org/pdf/2010.05941" target="_blank">pdf</a>]

<h2>Variable impedance control and learning -- A review. (arXiv:2010.06246v2 [cs.RO] UPDATED)</h2>
<h3>Fares J. Abu-Dakka, Matteo Saveriano</h3>
<p>Robots that physically interact with their surroundings, in order to
accomplish some tasks or assist humans in their activities, require to exploit
contact forces in a safe and proficient manner. Impedance control is considered
as a prominent approach in robotics to avoid large impact forces while
operating in unstructured environments. In such environments, the conditions
under which the interaction occurs may significantly vary during the task
execution. This demands robots to be endowed with on-line adaptation
capabilities to cope with sudden and unexpected changes in the environment. In
this context, variable impedance control arises as a powerful tool to modulate
the robot's behavior in response to variations in its surroundings. In this
survey, we present the state-of-the-art of approaches devoted to variable
impedance control from control and learning perspectives (separately and
jointly). Moreover, we propose a new taxonomy for mechanical impedance based on
variability, learning, and control. The objective of this survey is to put
together the concepts and efforts that have been done so far in this field, and
to describe advantages and disadvantages of each approach. The survey concludes
with open issues in the field and an envisioned framework that may potentially
solve them.
</p>
<a href="http://arxiv.org/abs/2010.06246" target="_blank">arXiv:2010.06246</a> [<a href="http://arxiv.org/pdf/2010.06246" target="_blank">pdf</a>]

<h2>On the Efficiency of K-Means Clustering: Evaluation, Optimization, and Algorithm Selection. (arXiv:2010.06654v2 [cs.DB] UPDATED)</h2>
<h3>Sheng Wang, Yuan Sun, Zhifeng Bao</h3>
<p>This paper presents a thorough evaluation of the existing methods that
accelerate Lloyd's algorithm for fast k-means clustering. To do so, we analyze
the pruning mechanisms of existing methods, and summarize their common pipeline
into a unified evaluation framework UniK. UniK embraces a class of well-known
methods and enables a fine-grained performance breakdown. Within UniK, we
thoroughly evaluate the pros and cons of existing methods using multiple
performance metrics on a number of datasets. Furthermore, we derive an
optimized algorithm over UniK, which effectively hybridizes multiple existing
methods for more aggressive pruning. To take this further, we investigate
whether the most efficient method for a given clustering task can be
automatically selected by machine learning, to benefit practitioners and
researchers.
</p>
<a href="http://arxiv.org/abs/2010.06654" target="_blank">arXiv:2010.06654</a> [<a href="http://arxiv.org/pdf/2010.06654" target="_blank">pdf</a>]

<h2>On Fair Division under Heterogeneous Matroid Constraints. (arXiv:2010.07280v2 [cs.GT] UPDATED)</h2>
<h3>Amitay Dror, Michal Feldman, Erel Segal-Halevi</h3>
<p>We study fair allocation of indivisible goods among additive agents with
feasibility constraints. In these settings, every agent is restricted to get a
bundle among a specified set of feasible bundles. Such scenarios have been of
great interest to the AI community due to their applicability to real-world
problems. Following some impossibility results, we restrict attention to
matroid feasibility constraints that capture natural scenarios, such as the
allocation of shifts to medical doctors or conference papers to referees.

We focus on the common fairness notion of envy-freeness up to one good (EF1).
Previous algorithms for finding EF1 allocations are either restricted to agents
with identical feasibility constraints, or allow free disposal of items. A
major open problem is the existence of EF1 complete allocations among
heterogeneous agents, where the heterogeneity is both in the agents'
feasibility constraints and in their valuations. In this work, we make progress
on this problem by providing positive and negative results for different
matroid and valuation types. Among other results, we devise polynomial-time
algorithms for finding EF1 allocations in the following settings: (i) n agents
with heterogeneous partition matroids and heterogeneous binary valuations, (ii)
2 agents with heterogeneous partition matroids and heterogeneous valuations,
and (iii) at most 3 agents with heterogeneous binary valuations and identical
base-orderable matroids.
</p>
<a href="http://arxiv.org/abs/2010.07280" target="_blank">arXiv:2010.07280</a> [<a href="http://arxiv.org/pdf/2010.07280" target="_blank">pdf</a>]

<h2>Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention. (arXiv:2010.07891v2 [cs.CL] UPDATED)</h2>
<h3>Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling</h3>
<p>A lack of corpora has so far limited advances in integrating human gaze data
as a supervisory signal in neural attention mechanisms for natural language
processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for
the first time, combines a cognitive model of reading with explicit human gaze
supervision in a single machine learning framework. On four different corpora
we demonstrate that our hybrid TSM duration predictions are highly correlated
with human gaze ground truth. We further propose a novel joint modeling
approach to integrate TSM predictions into the attention layer of a network
designed for a specific upstream NLP task without the need for any
task-specific human gaze data. We demonstrate that our joint model outperforms
the state of the art in paraphrase generation on the Quora Question Pairs
corpus by more than 10% in BLEU-4 and achieves state of the art performance for
sentence compression on the challenging Google Sentence Compression corpus. As
such, our work introduces a practical approach for bridging between data-driven
and cognitive models and demonstrates a new way to integrate human gaze-guided
neural attention into NLP tasks.
</p>
<a href="http://arxiv.org/abs/2010.07891" target="_blank">arXiv:2010.07891</a> [<a href="http://arxiv.org/pdf/2010.07891" target="_blank">pdf</a>]

<h2>A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age Estimation. (arXiv:2010.10368v2 [cs.CV] UPDATED)</h2>
<h3>Ali Akbari, Muhammad Awais, Zhen-Hua Feng, Ammarah Farooq, Josef Kittler</h3>
<p>The most existing studies in the facial age estimation assume training and
test images are captured under similar shooting conditions. However, this is
rarely valid in real-world applications, where training and test sets usually
have different characteristics. In this paper, we advocate a cross-dataset
protocol for age estimation benchmarking. In order to improve the cross-dataset
age estimation performance, we mitigate the inherent bias caused by the
learning algorithm itself. To this end, we propose a novel loss function that
is more effective for neural network training. The relative smoothness of the
proposed loss function is its advantage with regards to the optimisation
process performed by stochastic gradient descent (SGD). Compared with existing
loss functions, the lower gradient of the proposed loss function leads to the
convergence of SGD to a better optimum point, and consequently a better
generalisation. The cross-dataset experimental results demonstrate the
superiority of the proposed method over the state-of-the-art algorithms in
terms of accuracy and generalisation capability.
</p>
<a href="http://arxiv.org/abs/2010.10368" target="_blank">arXiv:2010.10368</a> [<a href="http://arxiv.org/pdf/2010.10368" target="_blank">pdf</a>]

<h2>A Level-wise Taxonomic Perspective on Automated Machine Learning to Date and Beyond: Challenges and Opportunities. (arXiv:2010.10777v2 [cs.LG] UPDATED)</h2>
<h3>Shubhra (Santu) Karmaker, Md. Mahadi Hassan, Micah J. Smith, Lei Xu, ChengXiang Zhai, Kalyan Veeramachaneni</h3>
<p>Automated machine learning (AutoML) is essentially automating the process of
applying machine learning to real-world problems. The primary goals of AutoML
tools are to provide methods and processes to make Machine Learning available
for non-Machine Learning experts (domain experts), to improve efficiency of
Machine Learning and to accelerate research on Machine Learning. Although
automation and efficiency are some of AutoML's main selling points, the process
still requires a surprising level of human involvement. A number of vital steps
of the machine learning pipeline, including understanding the attributes of
domain-specific data, defining prediction problems, creating a suitable
training data set etc. still tend to be done manually by a data scientist on an
ad-hoc basis. Often, this process requires a lot of back-and-forth between the
data scientist and domain experts, making the whole process more difficult and
inefficient. Altogether, AutoML systems are still far from a "real automatic
system". In this review article, we present a level-wise taxonomic perspective
on AutoML systems to-date and beyond, i.e., we introduce a new classification
system with seven levels to distinguish AutoML systems based on their level of
autonomy. We first start with a discussion on how an end-to-end Machine
learning pipeline actually looks like and which sub-tasks of Machine learning
Pipeline has indeed been automated so far. Next, we highlight the sub-tasks
which are still done manually by a data-scientist in most cases and how that
limits a domain expert's access to Machine learning. Then, we introduce the
novel level-based taxonomy of AutoML systems and define each level according to
their scope of automation support. Finally, we provide a road-map of future
research endeavor in the area of AutoML and discuss some important challenges
in achieving this ambitious goal.
</p>
<a href="http://arxiv.org/abs/2010.10777" target="_blank">arXiv:2010.10777</a> [<a href="http://arxiv.org/pdf/2010.10777" target="_blank">pdf</a>]

<h2>Gender Prediction Based on Vietnamese Names with Machine Learning Techniques. (arXiv:2010.10852v3 [cs.CL] UPDATED)</h2>
<h3>Huy Quoc To, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen, Anh Gia-Tuan Nguyen</h3>
<p>As biological gender is one of the aspects of presenting individual human,
much work has been done on gender classification based on people names. The
proposals for English and Chinese languages are tremendous; still, there have
been few works done for Vietnamese so far. We propose a new dataset for gender
prediction based on Vietnamese names. This dataset comprises over 26,000 full
names annotated with genders. This dataset is available on our website for
research purposes. In addition, this paper describes six machine learning
algorithms (Support Vector Machine, Multinomial Naive Bayes, Bernoulli Naive
Bayes, Decision Tree, Random Forrest and Logistic Regression) and a deep
learning model (LSTM) with fastText word embedding for gender prediction on
Vietnamese names. We create a dataset and investigate the impact of each name
component on detecting gender. As a result, the best F1-score that we have
achieved is up to 96\% on LSTM model and we generate a web API based on our
trained model.
</p>
<a href="http://arxiv.org/abs/2010.10852" target="_blank">arXiv:2010.10852</a> [<a href="http://arxiv.org/pdf/2010.10852" target="_blank">pdf</a>]

<h2>Quantum Superposition Spiking Neural Network. (arXiv:2010.12197v2 [cs.NE] UPDATED)</h2>
<h3>Yinqian Sun, Yi Zeng, Tielin Zhang</h3>
<p>Quantum brain as a novel hypothesis states that some non-trivial mechanisms
in quantum computation, such as superposition and entanglement, may have
important influence for the formation of brain functions. Inspired by this
idea, we propose Quantum Superposition Spiking Neural Network (QS-SNN), which
introduce quantum superposition to spiking neural network models to handel
challenges which are hard for other state-of-the-art machine learning models.
For human brain, grasping the main information no matter how the background
changes is necessary to interact efficiently with diverse environments. As an
example, it is easy for human to recognize the digits whether it is white
character with black background or inversely black character with white
background. While if the current machine learning models are trained with one
of the cases (e.g. white character with black background), it will be nearly
impossible for them to recognize the color inverted version. To handel this
challenge, we propose two-compartment spiking neural network with superposition
states encoding, which is inspired by quantum information theory and
spatial-temporal spiking property from neuron information encoding in the
brain. Typical network structures like fully-connected ANN, VGG, ResNet and
DenseNet are challenged with the same task. We train these networks on original
image dataset and then invert the background color to test their
generalization. Result shows that artificial neural network can not deal with
this condition while the quantum superposition spiking neural network(QS-SNN)
which we proposed in this paper recognizes the color-inverse image
successfully. Further the QS-SNN shows its robustness when noises are added on
inputs.
</p>
<a href="http://arxiv.org/abs/2010.12197" target="_blank">arXiv:2010.12197</a> [<a href="http://arxiv.org/pdf/2010.12197" target="_blank">pdf</a>]

<h2>Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning. (arXiv:2010.12238v2 [cs.CV] UPDATED)</h2>
<h3>Huan Fu, Shunming Li, Rongfei Jia, Mingming Gong, Binqiang Zhao, Dacheng Tao</h3>
<p>Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape
of a given 2D image from a large 3D shape database. The common routine is to
map 2D images and 3D shapes into an embedding space and define (or learn) a
shape similarity measure. While metric learning with some adaptation techniques
seems to be a natural solution to shape similarity learning, the performance is
often unsatisfactory for fine-grained shape retrieval. In the paper, we
identify the source of the poor performance and propose a practical solution to
this problem. We find that the shape difference between a negative pair is
entangled with the texture gap, making metric learning ineffective in pushing
away negative pairs. To tackle this issue, we develop a geometry-focused
multi-view metric learning framework empowered by texture synthesis. The
synthesis of textures for 3D shape models creates hard triplets, which suppress
the adverse effects of rich texture in 2D images, thereby push the network to
focus more on discovering geometric characteristics. Our approach shows
state-of-the-art performance on a recently released large-scale 3D-FUTURE[1]
repository, as well as three widely studied benchmarks, including Pix3D[2],
Stanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at:
https://github.com/3D-FRONT-FUTURE/IBSR-texture
</p>
<a href="http://arxiv.org/abs/2010.12238" target="_blank">arXiv:2010.12238</a> [<a href="http://arxiv.org/pdf/2010.12238" target="_blank">pdf</a>]

<h2>ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network Design in FPGA-based Systems. (arXiv:2010.12869v2 [cs.AR] UPDATED)</h2>
<h3>Suresh Nambi, Salim Ullah, Aditya Lohana, Siva Satyendra Sahoo, Farhad Merchant, Akash Kumar</h3>
<p>The recent advances in machine learning, in general, and Artificial Neural
Networks (ANN), in particular, has made smart embedded systems an attractive
option for a larger number of application areas. However, the high
computational complexity, memory footprints, and energy requirements of machine
learning models hinder their deployment on resource-constrained embedded
systems. Most state-of-the-art works have considered this problem by proposing
various low bit-width data representation schemes, optimized arithmetic
operators' implementations, and different complexity reduction techniques such
as network pruning. To further elevate the implementation gains offered by
these individual techniques, there is a need to cross-examine and combine these
techniques' unique features. This paper presents ExPAN(N)D, a framework to
analyze and ingather the efficacy of the Posit number representation scheme and
the efficiency of fixed-point arithmetic implementations for ANNs. The Posit
scheme offers a better dynamic range and higher precision for various
applications than IEEE $754$ single-precision floating-point format. However,
due to the dynamic nature of the various fields of the Posit scheme, the
corresponding arithmetic circuits have higher critical path delay and resource
requirements than the single-precision-based arithmetic units. Towards this
end, we propose a novel Posit to fixed-point converter for enabling
high-performance and energy-efficient hardware implementations for ANNs with
minimal drop in the output accuracy. We also propose a modified Posit-based
representation to store the trained parameters of a network. Compared to an
$8$-bit fixed-point-based inference accelerator, our proposed implementation
offers $\approx46\%$ and $\approx18\%$ reductions in the storage requirements
of the parameters and energy consumption of the MAC units, respectively.
</p>
<a href="http://arxiv.org/abs/2010.12869" target="_blank">arXiv:2010.12869</a> [<a href="http://arxiv.org/pdf/2010.12869" target="_blank">pdf</a>]

<h2>Multiscale Score Matching for Out-of-Distribution Detection. (arXiv:2010.13132v2 [cs.LG] UPDATED)</h2>
<h3>Ahsan Mahmood, Junier Oliva, Martin Styner</h3>
<p>We present a new methodology for detecting out-of-distribution (OOD) images
by utilizing norms of the score estimates at multiple noise scales. A score is
defined to be the gradient of the log density with respect to the input data.
Our methodology is completely unsupervised and follows a straight forward
training scheme. First, we train a deep network to estimate scores for levels
of noise. Once trained, we calculate the noisy score estimates for N
in-distribution samples and take the L2-norms across the input dimensions
(resulting in an NxL matrix). Then we train an auxiliary model (such as a
Gaussian Mixture Model) to learn the in-distribution spatial regions in this
L-dimensional space. This auxiliary model can now be used to identify points
that reside outside the learned space. Despite its simplicity, our experiments
show that this methodology significantly outperforms the state-of-the-art in
detecting out-of-distribution images. For example, our method can effectively
separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been
previously shown to be difficult for deep likelihood models.
</p>
<a href="http://arxiv.org/abs/2010.13132" target="_blank">arXiv:2010.13132</a> [<a href="http://arxiv.org/pdf/2010.13132" target="_blank">pdf</a>]

<h2>Deep Sequential Learning for Cervical Spine Fracture Detection on Computed Tomography Imaging. (arXiv:2010.13336v2 [eess.IV] UPDATED)</h2>
<h3>Hojjat Salehinejad, Edward Ho, Hui-Ming Lin, Priscila Crivellaro, Oleksandra Samorodova, Monica Tafur Arciniegas, Zamir Merali, Suradech Suthiphosuwan, Aditya Bharatha, Kristen Yeom, Muhammad Mamdani, Jefferson Wilson, Errol Colak</h3>
<p>Fractures of the cervical spine are a medical emergency and may lead to
permanent paralysis and even death. Accurate diagnosis in patients with
suspected fractures by computed tomography (CT) is critical to patient
management. In this paper, we propose a deep convolutional neural network
(DCNN) with a bidirectional long-short term memory (BLSTM) layer for the
automated detection of cervical spine fractures in CT axial images. We used an
annotated dataset of 3,666 CT scans (729 positive and 2,937 negative cases) to
train and validate the model. The validation results show a classification
accuracy of 70.92% and 79.18% on the balanced (104 positive and 104 negative
cases) and imbalanced (104 positive and 419 negative cases) test datasets,
respectively.
</p>
<a href="http://arxiv.org/abs/2010.13336" target="_blank">arXiv:2010.13336</a> [<a href="http://arxiv.org/pdf/2010.13336" target="_blank">pdf</a>]

<h2>Deep reinforced learning enables solving discrete-choice life cycle models to analyze social security reforms. (arXiv:2010.13471v2 [econ.GN] UPDATED)</h2>
<h3>Antti J. Tanskanen</h3>
<p>Discrete-choice life cycle models can be used to, e.g., estimate how social
security reforms change employment rate. Optimal employment choices during the
life course of an individual can be solved in the framework of life cycle
models. This enables estimating how a social security reform influences
employment rate. Mostly, life cycle models have been solved with dynamic
programming, which is not feasible when the state space is large, as often is
the case in a realistic life cycle model. Solving such life cycle models
requires the use of approximate methods, such as reinforced learning
algorithms. We compare how well a deep reinforced learning algorithm ACKTR and
dynamic programming solve a relatively simple life cycle model. We find that
the average utility is almost the same in both algorithms, however, the details
of the best policies found with different algorithms differ to a degree. In the
baseline model representing the current Finnish social security scheme, we find
that reinforced learning yields essentially as good results as dynamics
programming. We then analyze a straight-forward social security reform and find
that the employment changes due to the reform are almost the same. Our results
suggest that reinforced learning algorithms are of significant value in
analyzing complex life cycle models.
</p>
<a href="http://arxiv.org/abs/2010.13471" target="_blank">arXiv:2010.13471</a> [<a href="http://arxiv.org/pdf/2010.13471" target="_blank">pdf</a>]

<h2>OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning. (arXiv:2010.13611v2 [cs.LG] UPDATED)</h2>
<h3>Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, Ofir Nachum</h3>
<p>Reinforcement learning (RL) has achieved impressive performance in a variety
of online settings in which an agent's ability to query the environment for
transitions and rewards is effectively unlimited. However, in many practical
applications, the situation is reversed: an agent may have access to large
amounts of undirected offline experience data, while access to the online
environment is severely limited. In this work, we focus on this offline
setting. Our main insight is that, when presented with offline data composed of
a variety of behaviors, an effective way to leverage this data is to extract a
continuous space of recurring and temporally extended primitive behaviors
before using these primitives for downstream task learning. Primitives
extracted in this way serve two purposes: they delineate the behaviors that are
supported by the data from those that are not, making them useful for avoiding
distributional shift in offline RL; and they provide a degree of temporal
abstraction, which reduces the effective horizon yielding better learning in
theory, and improved offline RL in practice. In addition to benefiting offline
policy optimization, we show that performing offline primitive learning in this
way can also be leveraged for improving few-shot imitation learning as well as
exploration and transfer in online RL on a variety of benchmark domains.
Visualizations are available at https://sites.google.com/view/opal-iclr
</p>
<a href="http://arxiv.org/abs/2010.13611" target="_blank">arXiv:2010.13611</a> [<a href="http://arxiv.org/pdf/2010.13611" target="_blank">pdf</a>]

