---
title: Latest Deep Learning Papers
date: 2021-01-26 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (160 Articles)</h1>
<h2>DeepDT: Learning Geometry From Delaunay Triangulation for Surface Reconstruction. (arXiv:2101.10353v1 [cs.CV])</h2>
<h3>Yiming Luo, Zhenxing Mi, Wenbing Tao</h3>
<p>In this paper, a novel learning-based network, named DeepDT, is proposed to
reconstruct the surface from Delaunay triangulation of point cloud. DeepDT
learns to predict inside/outside labels of Delaunay tetrahedrons directly from
a point cloud and corresponding Delaunay triangulation. The local geometry
features are first extracted from the input point cloud and aggregated into a
graph deriving from the Delaunay triangulation. Then a graph filtering is
applied on the aggregated features in order to add structural regularization to
the label prediction of tetrahedrons. Due to the complicated spatial relations
between tetrahedrons and the triangles, it is impossible to directly generate
ground truth labels of tetrahedrons from ground truth surface. Therefore, we
propose a multilabel supervision strategy which votes for the label of a
tetrahedron with labels of sampling locations inside it. The proposed DeepDT
can maintain abundant geometry details without generating overly complex
surfaces , especially for inner surfaces of open scenes. Meanwhile, the
generalization ability and time consumption of the proposed method is
acceptable and competitive compared with the state-of-the-art methods.
Experiments demonstrate the superior performance of the proposed DeepDT.
</p>
<a href="http://arxiv.org/abs/2101.10353" target="_blank">arXiv:2101.10353</a> [<a href="http://arxiv.org/pdf/2101.10353" target="_blank">pdf</a>]

<h2>The GRIFFIN Perception Dataset: Bridging the Gap Between Flapping-Wing Flight and Robotic Perception. (arXiv:2101.10371v1 [cs.RO])</h2>
<h3>J.P. Rodr&#xed;guez-G&#xf3;mez, R. Tapia, J. L. Paneque, P. Grau, A. G&#xf3;mez Egu&#xed;luz, J.R. Mart&#xed;nez-de Dios, A. Ollero</h3>
<p>The development of automatic perception systems and techniques for
bio-inspired flapping-wing robots is severely hampered by the high technical
complexity of these platforms and the installation of onboard sensors and
electronics. Besides, flapping-wing robot perception suffers from high
vibration levels and abrupt movements during flight, which cause motion blur
and strong changes in lighting conditions. This paper presents a perception
dataset for bird-scale flapping-wing robots as a tool to help alleviate the
aforementioned problems. The presented data include measurements from onboard
sensors widely used in aerial robotics and suitable to deal with the perception
challenges of flapping-wing robots, such as an event camera, a conventional
camera, and two Inertial Measurement Units (IMUs), as well as ground truth
measurements from a laser tracker or a motion capture system. A total of 21
datasets of different types of flights were collected in three different
scenarios (one indoor and two outdoor). To the best of the authors' knowledge
this is the first dataset for flapping-wing robot perception.
</p>
<a href="http://arxiv.org/abs/2101.10371" target="_blank">arXiv:2101.10371</a> [<a href="http://arxiv.org/pdf/2101.10371" target="_blank">pdf</a>]

<h2>Learning to falsify automated driving vehicles with prior knowledge. (arXiv:2101.10377v1 [cs.RO])</h2>
<h3>Andrea Favrin, Vladislav Nenchev, Angelo Cenedese</h3>
<p>While automated driving technology has achieved a tremendous progress, the
scalable and rigorous testing and verification of safe automated and autonomous
driving vehicles remain challenging. This paper proposes a learning-based
falsification framework for testing the implementation of an automated or
self-driving function in simulation. We assume that the function specification
is associated with a violation metric on possible scenarios. Prior knowledge is
incorporated to limit the scenario parameter variance and in a model-based
falsifier to guide and improve the learning process. For an exemplary adaptive
cruise controller, the presented framework yields non-trivial falsifying
scenarios with higher reward, compared to scenarios obtained by purely
learning-based or purely model-based falsification approaches.
</p>
<a href="http://arxiv.org/abs/2101.10377" target="_blank">arXiv:2101.10377</a> [<a href="http://arxiv.org/pdf/2101.10377" target="_blank">pdf</a>]

<h2>Curriculum Learning: A Survey. (arXiv:2101.10382v1 [cs.LG])</h2>
<h3>Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe</h3>
<p>Training machine learning models in a meaningful order, from the easy samples
to the hard ones, using curriculum learning can provide performance
improvements over the standard training approach based on random data
shuffling, without any additional computational costs. Curriculum learning
strategies have been successfully employed in all areas of machine learning, in
a wide range of tasks. However, the necessity of finding a way to rank the
samples from easy to hard, as well as the right pacing function for introducing
more difficult data can limit the usage of the curriculum approaches. In this
survey, we show how these limits have been tackled in the literature, and we
present different curriculum learning instantiations for various tasks in
machine learning. We construct a multi-perspective taxonomy of curriculum
learning approaches by hand, considering various classification criteria. We
further build a hierarchical tree of curriculum learning methods using an
agglomerative clustering algorithm, linking the discovered clusters with our
taxonomy. At the end, we provide some interesting directions for future work.
</p>
<a href="http://arxiv.org/abs/2101.10382" target="_blank">arXiv:2101.10382</a> [<a href="http://arxiv.org/pdf/2101.10382" target="_blank">pdf</a>]

<h2>droidlet: modular, heterogenous, multi-modal agents. (arXiv:2101.10384v1 [cs.RO])</h2>
<h3>Anurag Pratik, Soumith Chintala, Kavya Srinet, Dhiraj Gandhi, Rebecca Qian, Yuxuan Sun, Ryan Drew, Sara Elkafrawy, Anoushka Tiwari, Tucker Hart, Mary Williamson, Abhinav Gupta, Arthur Szlam</h3>
<p>In recent years, there have been significant advances in building end-to-end
Machine Learning (ML) systems that learn at scale. But most of these systems
are: (a) isolated (perception, speech, or language only); (b) trained on static
datasets. On the other hand, in the field of robotics, large-scale learning has
always been difficult. Supervision is hard to gather and real world physical
interactions are expensive. In this work we introduce and open-source droidlet,
a modular, heterogeneous agent architecture and platform. It allows us to
exploit both large-scale static datasets in perception and language and
sophisticated heuristics often used in robotics; and provides tools for
interactive annotation. Furthermore, it brings together perception, language
and action onto one platform, providing a path towards agents that learn from
the richness of real world interactions.
</p>
<a href="http://arxiv.org/abs/2101.10384" target="_blank">arXiv:2101.10384</a> [<a href="http://arxiv.org/pdf/2101.10384" target="_blank">pdf</a>]

<h2>Online and Scalable Model Selection with Multi-Armed Bandits. (arXiv:2101.10385v1 [cs.LG])</h2>
<h3>Jiayi Xie, Michael Tashman, John Hoffman, Lee Winikor, Rouzbeh Gerami</h3>
<p>Many online applications running on live traffic are powered by machine
learning models, for which training, validation, and hyper-parameter tuning are
conducted on historical data. However, it is common for models demonstrating
strong performance in offline analysis to yield poorer performance when
deployed online. This problem is a consequence of the difficulty of training on
historical data in non-stationary environments. Moreover, the machine learning
metrics used for model selection may not sufficiently correlate with real-world
business metrics used to determine the success of the applications being
tested. These problems are particularly prominent in the Real-Time Bidding
(RTB) domain, in which ML models power bidding strategies, and a change in
models will likely affect performance of the advertising campaigns. In this
work, we present Automatic Model Selector (AMS), a system for scalable online
selection of RTB bidding strategies based on real-world performance metrics.
AMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate
multiple models against live traffic, allocating the most traffic to the
best-performing models while decreasing traffic to those with poorer online
performance, thereby minimizing the impact of inferior models on overall
campaign performance. The reliance on offline data is avoided, instead making
model selections on a case-by-case basis according to actionable business
goals. AMS allows new models to be safely introduced into live campaigns as
soon as they are developed, minimizing the risk to overall performance. In
live-traffic tests on multiple ad campaigns, the AMS system proved highly
effective at improving ad campaign performance.
</p>
<a href="http://arxiv.org/abs/2101.10385" target="_blank">arXiv:2101.10385</a> [<a href="http://arxiv.org/pdf/2101.10385" target="_blank">pdf</a>]

<h2>Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification. (arXiv:2101.10390v1 [cs.LG])</h2>
<h3>Joeri A. Zwerts, Jelle Treep, Casper S. Kaandorp, Floor Meewis, Amparo C. Koot, Heysem Kaya</h3>
<p>Automated classification of animal vocalisations is a potentially powerful
wildlife monitoring tool. Training robust classifiers requires sizable
annotated datasets, which are not easily recorded in the wild. To circumvent
this problem, we recorded four primate species under semi-natural conditions in
a wildlife sanctuary in Cameroon with the objective to train a classifier
capable of detecting species in the wild. Here, we introduce the collected
dataset, describe our approach and initial results of classifier development.
To increase the efficiency of the annotation process, we condensed the
recordings with an energy/change based automatic vocalisation detection.
Segmenting the annotated chunks into training, validation and test sets,
initial results reveal up to 82% unweighted average recall (UAR) test set
performance in four-class primate species classification.
</p>
<a href="http://arxiv.org/abs/2101.10390" target="_blank">arXiv:2101.10390</a> [<a href="http://arxiv.org/pdf/2101.10390" target="_blank">pdf</a>]

<h2>A Missing Data Imputation Method for 3D Object Reconstruction using Multi-modal Variational Autoencoder. (arXiv:2101.10391v1 [cs.CV])</h2>
<h3>Hyeonwoo Yu, Jean Oh</h3>
<p>For effective human-robot teaming, it is importantfor the robots to be able
to share their visual perceptionwith the human operators. In a harsh remote
collaborationsetting, however, it is especially challenging to transfer a
largeamount of sensory data over a low-bandwidth network in real-time, e.g.,
for the task of 3D shape reconstruction given 2Dcamera images. To reduce the
burden of data transferring, datacompression techniques such as autoencoder can
be utilized toobtain and transmit the data in terms of latent variables in
acompact form. However, due to the low-bandwidth limitation orcommunication
delay, some of the dimensions of latent variablescan be lost in transit,
degenerating the reconstruction results.Moreover, in order to achieve faster
transmission, an intentionalover compression can be used where only partial
elements ofthe latent variables are used. To handle these incomplete datacases,
we propose a method for imputation of latent variableswhose elements are
partially lost or manually excluded. Toperform imputation with only some
dimensions of variables,exploiting prior information of the category- or
instance-levelis essential. In general, a prior distribution used in
variationalautoencoders is achieved from all of the training
datapointsregardless of their labels. This type of flattened prior makes
itdifficult to perform imputation from the category- or instance-level
distributions.
</p>
<a href="http://arxiv.org/abs/2101.10391" target="_blank">arXiv:2101.10391</a> [<a href="http://arxiv.org/pdf/2101.10391" target="_blank">pdf</a>]

<h2>Anchor Distance for 3D Multi-Object Distance Estimation from 2D Single Shot. (arXiv:2101.10399v1 [cs.CV])</h2>
<h3>Hyeonwoo Yu, Jean Oh</h3>
<p>Visual perception of the objects in a 3D environment is a key to successful
performance in autonomous driving and simultaneous localization and mapping
(SLAM). In this paper, we present a real time approach for estimating the
distances to multiple objects in a scene using only a single-shot image. Given
a 2D Bounding Box (BBox) and object parameters, a 3D distance to the object can
be calculated directly using 3D reprojection; however, such methods are prone
to significant errors because an error from the 2D detection can be amplified
in 3D. In addition, it is also challenging to apply such methods to a real-time
system due to the computational burden. In the case of the traditional
multi-object detection methods, %they mostly pay attention to existing works
have been developed for specific tasks such as object segmentation or 2D BBox
regression. These methods introduce the concept of anchor BBox for elaborate 2D
BBox estimation, and predictors are specialized and trained for specific 2D
BBoxes. In order to estimate the distances to the 3D objects from a single 2D
image, we introduce the notion of \textit{anchor distance} based on an object's
location and propose a method that applies the anchor distance to the
multi-object detector structure. We let the predictors catch the distance prior
using anchor distance and train the network based on the distance. The
predictors can be characterized to the objects located in a specific distance
range. By propagating the distance prior using a distance anchor to the
predictors, it is feasible to perform the precise distance estimation and
real-time execution simultaneously. The proposed method achieves about 30 FPS
speed, and shows the lowest RMSE compared to the existing methods.
</p>
<a href="http://arxiv.org/abs/2101.10399" target="_blank">arXiv:2101.10399</a> [<a href="http://arxiv.org/pdf/2101.10399" target="_blank">pdf</a>]

<h2>A metric for evaluating 3D reconstruction and mapping performance with no ground truthing. (arXiv:2101.10402v1 [cs.CV])</h2>
<h3>Guoxiang Zhang, YangQuan Chen</h3>
<p>It is not easy when evaluating 3D mapping performance because existing
metrics require ground truth data that can only be collected with special
instruments. In this paper, we propose a metric, dense map posterior (DMP), for
this evaluation. It can work without any ground truth data. Instead, it
calculates a comparable value, reflecting a map posterior probability, from
dense point cloud observations. In our experiments, the proposed DMP is
benchmarked against ground truth-based metrics. Results show that DMP can
provide a similar evaluation capability. The proposed metric makes evaluating
different methods more flexible and opens many new possibilities, such as
self-supervised methods and more available datasets.
</p>
<a href="http://arxiv.org/abs/2101.10402" target="_blank">arXiv:2101.10402</a> [<a href="http://arxiv.org/pdf/2101.10402" target="_blank">pdf</a>]

<h2>Spectrum Attention Mechanism for Time Series Classification. (arXiv:2101.10420v1 [cs.LG])</h2>
<h3>Shibo Zhou, Yu Pan</h3>
<p>Time series classification(TSC) has always been an important and challenging
research task. With the wide application of deep learning, more and more
researchers use deep learning models to solve TSC problems. Since time series
always contains a lot of noise, which has a negative impact on network
training, people usually filter the original data before training the network.
The existing schemes are to treat the filtering and training as two stages, and
the design of the filter requires expert experience, which increases the design
difficulty of the algorithm and is not universal. We note that the essence of
filtering is to filter out the insignificant frequency components and highlight
the important ones, which is similar to the attention mechanism. In this paper,
we propose an attention mechanism that acts on spectrum (SAM). The network can
assign appropriate weights to each frequency component to achieve adaptive
filtering. We use L1 regularization to further enhance the frequency screening
capability of SAM. We also propose a segmented-SAM (SSAM) to avoid the loss of
time domain information caused by using the spectrum of the whole sequence. In
which, a tumbling window is introduced to segment the original data. Then SAM
is applied to each segment to generate new features. We propose a heuristic
strategy to search for the appropriate number of segments. Experimental results
show that SSAM can produce better feature representations, make the network
converge faster, and improve the robustness and classification accuracy.
</p>
<a href="http://arxiv.org/abs/2101.10420" target="_blank">arXiv:2101.10420</a> [<a href="http://arxiv.org/pdf/2101.10420" target="_blank">pdf</a>]

<h2>Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v1 [cs.LG])</h2>
<h3>Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, Scott Sanner</h3>
<p>Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as "review" trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe earlier proposed iCaRL remains competitive when the memory buffer is
small; GDumb outperforms many recently proposed methods in medium-size datasets
and MIR performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the "review" trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.
</p>
<a href="http://arxiv.org/abs/2101.10423" target="_blank">arXiv:2101.10423</a> [<a href="http://arxiv.org/pdf/2101.10423" target="_blank">pdf</a>]

<h2>Finding hidden-feature depending laws inside a data set and classifying it using Neural Network. (arXiv:2101.10427v1 [cs.LG])</h2>
<h3>Thilo Moshagen, Nihal Acharya Adde, Ajay Navilarekal Rajgopal</h3>
<p>The logcosh loss function for neural networks has been developed to combine
the advantage of the absolute error loss function of not overweighting outliers
with the advantage of the mean square error of continuous derivative near the
mean, which makes the last phase of learning easier. It is clear, and one
experiences it soon, that in the case of clustered data, an artificial neural
network with logcosh loss learns the bigger cluster rather than the mean of the
two. Even more so, the ANN, when used for regression of a set-valued function,
will learn a value close to one of the choices, in other words, one branch of
the set-valued function, while a mean-square-error NN will learn the value in
between. This work suggests a method that uses artificial neural networks with
logcosh loss to find the branches of set-valued mappings in parameter-outcome
sample sets and classifies the samples according to those branches.
</p>
<a href="http://arxiv.org/abs/2101.10427" target="_blank">arXiv:2101.10427</a> [<a href="http://arxiv.org/pdf/2101.10427" target="_blank">pdf</a>]

<h2>Deep Learning-Based Autoencoder for Data-Driven Modeling of an RF Photoinjector. (arXiv:2101.10437v1 [cs.LG])</h2>
<h3>Jun Zhu, Ye Chen, Frank Brinker, Winfried Decking, Sergey Tomin, Holger Schlarb</h3>
<p>We adopt a data-driven approach to model the longitudinal phase-space
diagnostic beamline at the European XFEL photoinjector. A deep convolutional
neural network (decoder) is used to build a 2D distribution from a small
feature space learned by another neural network (encoder). We demonstrate that
the autoencoder model trained on experimental data can make fast and very
high-quality predictions of megapixel images for the longitudinal phase-space
measurement. The prediction significantly outperforms existing models. We also
show that the knowledge learned by the model can be transfered to speed up
training of another model with a different injector setup. This opens the door
to a new way of precisely modeling a photoinjector and can be possibly extended
to the whole accelerator.
</p>
<a href="http://arxiv.org/abs/2101.10437" target="_blank">arXiv:2101.10437</a> [<a href="http://arxiv.org/pdf/2101.10437" target="_blank">pdf</a>]

<h2>Persistent Covering of a Graph under Latency and Energy Constraints. (arXiv:2101.10438v1 [cs.RO])</h2>
<h3>Jyh-Ming Lien, Sam Rodriguez, Marco Morales</h3>
<p>Most consumer-level low-cost unmanned aerial vehicles (UAVs) have limited
battery power and long charging time. Due to these energy constraints, they
cannot accomplish many practical tasks, such as monitoring a sport or political
event for hours. The problem of providing the service to cover an area for an
extended time is known as persistent covering in the literature. In the past,
researchers have proposed various hardware platforms, such as battery-swapping
mechanisms, to provide persistent covering. However, algorithmic approaches are
limited mostly due to the computational complexity and intractability of the
problem. Approximation algorithms have been considered to segment a large area
into smaller cells that require periodic visits under the latency constraints.
However, these methods assume unlimited energy. In this paper, we explore
geometric and topological properties that allow us to significantly reduce the
size of the optimization problem. Consequently, the proposed method can
efficiently determine the minimum number of UAVs needed and schedule their
routes to cover an area persistently. We demonstrated experimentally that the
proposed algorithm has better performance than the baseline methods.
</p>
<a href="http://arxiv.org/abs/2101.10438" target="_blank">arXiv:2101.10438</a> [<a href="http://arxiv.org/pdf/2101.10438" target="_blank">pdf</a>]

<h2>Towards glass-box CNNs. (arXiv:2101.10443v1 [cs.CV])</h2>
<h3>Piduguralla Manaswini, Jignesh S. Bhatt</h3>
<p>Convolution neural networks (CNNs) are brain-inspired architectures popular
for their ability to train and relearn visually complex tasks. It is
incremental and scalable; however, CNN is mostly treated as black-box and
involves multiple trial &amp; error runs. We observe that CNN constructs powerful
internal representations that help achieve state-of-the-art performance. Here
we propose three layer glass-box (analytical) CNN for two-class image
classifcation problems. First is a representation layer that encompasses both
the class information (group invariant) and symmetric transformations (group
equivariant) of input images. It is then passed through dimension reduction
layer (PCA). Finally the compact yet complete representation is provided to a
classifer. Analytical machine learning classifers and multilayer perceptrons
are used to assess sensitivity. Proposed glass-box CNN is compared with
equivariance of AlexNet (CNN) internal representation for better understanding
and dissemination of results. In future, we would like to construct glass-box
CNN for multiclass visually complex tasks.
</p>
<a href="http://arxiv.org/abs/2101.10443" target="_blank">arXiv:2101.10443</a> [<a href="http://arxiv.org/pdf/2101.10443" target="_blank">pdf</a>]

<h2>GnetSeg: Semantic Segmentation Model Optimized on a 224mW CNN Accelerator Chip at the Speed of 318FPS. (arXiv:2101.10444v1 [cs.CV])</h2>
<h3>Baohua Sun, Weixiong Lin, Hao Sha, Jiapeng Su</h3>
<p>Semantic segmentation is the task to cluster pixels on an image belonging to
the same class. It is widely used in the real-world applications including
autonomous driving, medical imaging analysis, industrial inspection, smartphone
camera for person segmentation and so on. Accelerating the semantic
segmentation models on the mobile and edge devices are practical needs for the
industry. Recent years have witnessed the wide availability of CNN
(Convolutional Neural Networks) accelerators. They have the advantages on power
efficiency, inference speed, which are ideal for accelerating the semantic
segmentation models on the edge devices. However, the CNN accelerator chips
also have the limitations on flexibility and memory. In addition, the CPU load
is very critical because the CNN accelerator chip works as a co-processor with
a host CPU. In this paper, we optimize the semantic segmentation model in order
to fully utilize the limited memory and the supported operators on the CNN
accelerator chips, and at the same time reduce the CPU load of the CNN model to
zero. The resulting model is called GnetSeg. Furthermore, we propose the
integer encoding for the mask of the GnetSeg model, which minimizes the latency
of data transfer between the CNN accelerator and the host CPU. The experimental
result shows that the model running on the 224mW chip achieves the speed of
318FPS with excellent accuracy for applications such as person segmentation.
</p>
<a href="http://arxiv.org/abs/2101.10444" target="_blank">arXiv:2101.10444</a> [<a href="http://arxiv.org/pdf/2101.10444" target="_blank">pdf</a>]

<h2>Dairy Cow rumination detection: A deep learning approach. (arXiv:2101.10445v1 [cs.CV])</h2>
<h3>Safa Ayadi, Ahmed ben said, Rateb Jabbar, Chafik Aloulou, Achraf Chabbouh, Ahmed Ben Achballah</h3>
<p>Cattle activity is an essential index for monitoring health and welfare of
the ruminants. Thus, changes in the livestock behavior are a critical indicator
for early detection and prevention of several diseases. Rumination behavior is
a significant variable for tracking the development and yield of animal
husbandry. Therefore, various monitoring methods and measurement equipment have
been used to assess cattle behavior. However, these modern attached devices are
invasive, stressful and uncomfortable for the cattle and can influence
negatively the welfare and diurnal behavior of the animal. Multiple research
efforts addressed the problem of rumination detection by adopting new methods
by relying on visual features. However, they only use few postures of the dairy
cow to recognize the rumination or feeding behavior. In this study, we
introduce an innovative monitoring method using Convolution Neural Network
(CNN)-based deep learning models. The classification process is conducted under
two main labels: ruminating and other, using all cow postures captured by the
monitoring camera. Our proposed system is simple and easy-to-use which is able
to capture long-term dynamics using a compacted representation of a video in a
single 2D image. This method proved efficiency in recognizing the rumination
behavior with 95%, 98% and 98% of average accuracy, recall and precision,
respectively.
</p>
<a href="http://arxiv.org/abs/2101.10445" target="_blank">arXiv:2101.10445</a> [<a href="http://arxiv.org/pdf/2101.10445" target="_blank">pdf</a>]

<h2>Towards Domain Invariant Single Image Dehazing. (arXiv:2101.10449v1 [cs.CV])</h2>
<h3>Pranjay Shyam, Kuk-Jin Yoon, Kyung-Soo Kim</h3>
<p>Presence of haze in images obscures underlying information, which is
undesirable in applications requiring accurate environment information. To
recover such an image, a dehazing algorithm should localize and recover
affected regions while ensuring consistency between recovered and its
neighboring regions. However owing to fixed receptive field of convolutional
kernels and non uniform haze distribution, assuring consistency between regions
is difficult. In this paper, we utilize an encoder-decoder based network
architecture to perform the task of dehazing and integrate an spatially aware
channel attention mechanism to enhance features of interest beyond the
receptive field of traditional conventional kernels. To ensure performance
consistency across diverse range of haze densities, we utilize greedy localized
data augmentation mechanism. Synthetic datasets are typically used to ensure a
large amount of paired training samples, however the methodology to generate
such samples introduces a gap between them and real images while accounting for
only uniform haze distribution and overlooking more realistic scenario of
non-uniform haze distribution resulting in inferior dehazing performance when
evaluated on real datasets. Despite this, the abundance of paired samples
within synthetic datasets cannot be ignored. Thus to ensure performance
consistency across diverse datasets, we train the proposed network within an
adversarial prior-guided framework that relies on a generated image along with
its low and high frequency components to determine if properties of dehazed
images matches those of ground truth. We preform extensive experiments to
validate the dehazing and domain invariance performance of proposed framework
across diverse domains and report state-of-the-art (SoTA) results.
</p>
<a href="http://arxiv.org/abs/2101.10449" target="_blank">arXiv:2101.10449</a> [<a href="http://arxiv.org/pdf/2101.10449" target="_blank">pdf</a>]

<h2>LAIF: AI, Deep Learning for Germany Suetterlin Letter Recognition and Generation. (arXiv:2101.10450v1 [cs.CV])</h2>
<h3>Enkhtogtokh Togootogtokh, Christian Klasen</h3>
<p>One of the successful early implementation of deep learning AI technology was
on letter recognition. With the recent breakthrough of artificial intelligence
(AI) brings more solid technology for complex problems like handwritten letter
recognition and even automatic generation of them. In this research, we
proposed deep learning framework called Ludwig AI Framework(LAIF) for Germany
Suetterlin letter recognition and generation. To recognize Suetterlin letter,
we proposed deep convolutional neural network. Since lack of big amount of data
to train for the deep models and huge cost to label existing hard copy of
handwritten letters, we also introduce the methodology with deep generative
adversarial network to generate handwritten letters as synthetic data. Main
source code is in https://github.com/enkhtogtokh/LAIF repository.
</p>
<a href="http://arxiv.org/abs/2101.10450" target="_blank">arXiv:2101.10450</a> [<a href="http://arxiv.org/pdf/2101.10450" target="_blank">pdf</a>]

<h2>Black-box Adversarial Attacks on Monocular Depth Estimation Using Evolutionary Multi-objective Optimization. (arXiv:2101.10452v1 [cs.CV])</h2>
<h3>Renya Daimo (1), Satoshi Ono (1), Takahiro Suzuki (1) ((1) Department of Information Science and Biomedical Engineering, Graduate School of Science and Engineering, Kagoshima University)</h3>
<p>This paper proposes an adversarial attack method to deep neural networks
(DNNs) for monocular depth estimation, i.e., estimating the depth from a single
image. Single image depth estimation has improved drastically in recent years
due to the development of DNNs. However, vulnerabilities of DNNs for image
classification have been revealed by adversarial attacks, and DNNs for
monocular depth estimation could contain similar vulnerabilities. Therefore,
research on vulnerabilities of DNNs for monocular depth estimation has spread
rapidly, but many of them assume white-box conditions where inside information
of DNNs is available, or are transferability-based black-box attacks that
require a substitute DNN model and a training dataset. Utilizing Evolutionary
Multi-objective Optimization, the proposed method in this paper analyzes DNNs
under the black-box condition where only output depth maps are available. In
addition, the proposed method does not require a substitute DNN that has a
similar architecture to the target DNN nor any knowledge about training data
used to train the target model. Experimental results showed that the proposed
method succeeded in attacking two DNN-based methods that were trained with
indoor and outdoor scenes respectively.
</p>
<a href="http://arxiv.org/abs/2101.10452" target="_blank">arXiv:2101.10452</a> [<a href="http://arxiv.org/pdf/2101.10452" target="_blank">pdf</a>]

<h2>Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate Time Series Forecasting. (arXiv:2101.10460v1 [cs.LG])</h2>
<h3>Nam Nguyen, Brian Quanz</h3>
<p>Probabilistic forecasting of high dimensional multivariate time series is a
notoriously challenging task, both in terms of computational burden and
distribution modeling. Most previous work either makes simple distribution
assumptions or abandons modeling cross-series correlations. A promising line of
work exploits scalable matrix factorization for latent-space forecasting, but
is limited to linear embeddings, unable to model distributions, and not
trainable end-to-end when using deep learning forecasting. We introduce a novel
temporal latent auto-encoder method which enables nonlinear factorization of
multivariate time series, learned end-to-end with a temporal deep learning
latent space forecast model. By imposing a probabilistic latent space model,
complex distributions of the input series are modeled via the decoder.
Extensive experiments demonstrate that our model achieves state-of-the-art
performance on many popular multivariate datasets, with gains sometimes as high
as $50\%$ for several standard metrics.
</p>
<a href="http://arxiv.org/abs/2101.10460" target="_blank">arXiv:2101.10460</a> [<a href="http://arxiv.org/pdf/2101.10460" target="_blank">pdf</a>]

<h2>How do some Bayesian Network machine learned graphs compare to causal knowledge?. (arXiv:2101.10461v1 [cs.AI])</h2>
<h3>Anthony C. Constantinou, Norman Fenton, Martin Neil</h3>
<p>The graph of a BN can be machine learned, determined by causal knowledge, or
a combination of both. In disciplines like bioinformatics, applying BN
structure learning algorithms can reveal new insights that would otherwise
remain unknown. However, these algorithms are less effective when the input
data are limited in terms of sample size, which is often the case when working
with real data. This paper focuses on purely machine learned and purely
knowledge-based BNs and investigates their differences in terms of graphical
structure and how well the implied statistical models explain the data. The
tests are based on four previous case studies that had their BN structure
determined by domain knowledge. Using various metrics, we compare the
knowledge-based graphs to the machine learned graphs generated from various
algorithms implemented in TETRAD spanning all three classes of learning. The
results show that while the algorithms are much better at arriving at a graph
with a high model selection score, the parameterised models obtained from those
graphs tend to be poor predictors of variables of interest, relative to the
corresponding inferences obtained from the knowledge-based graphs. Amongst our
conclusions is that structure learning is ineffective in the presence of
limited sample size relative to model dimensionality, which can be explained by
model fitting becoming increasingly distorted under these conditions;
essentially rendering ground truth graphs inaccurate by guiding algorithms
towards graphical patterns that may share higher evaluation scores and yet
deviate further from the ground truth graph. This highlights the value of
causal knowledge in these cases, as well as the need for more appropriate model
selection scores. Lastly, the experiments also provide new evidence that
support the notion that results from simulated data tell us little about actual
real-world performance.
</p>
<a href="http://arxiv.org/abs/2101.10461" target="_blank">arXiv:2101.10461</a> [<a href="http://arxiv.org/pdf/2101.10461" target="_blank">pdf</a>]

<h2>Appliance Operation Modes Identification Using Cycles Clustering. (arXiv:2101.10472v1 [cs.LG])</h2>
<h3>Abdelkareem Jaradat, Hanan Lutfiyya, Anwar Haque</h3>
<p>The increasing cost, energy demand, and environmental issues has led many
researchers to find approaches for energy monitoring, and hence energy
conservation. The emerging technologies of Internet of Things (IoT) and Machine
Learning (ML) deliver techniques that have the potential to efficiently
conserve energy and improve the utilization of energy consumption. Smart Home
Energy Management Systems (SHEMSs) have the potential to contribute in energy
conservation through the application of Demand Response (DR) in the residential
sector. In this paper, we propose appliances Operation Modes Identification
using Cycles Clustering (OMICC) which is SHEMS fundamental approach that
utilizes the sensed residential disaggregated power consumption in supporting
DR by providing consumers the opportunity to select lighter appliance operation
modes. The cycles of the Single Usage Profile (SUP) of an appliance are
extracted and reformed into features in terms of clusters of cycles. These
features are then used to identify the operation mode used in every occurrence
using K-Nearest Neighbors (KNN). Operation modes identification is considered a
basis for many potential smart DR applications within SHEMS towards the
consumers or the suppliers
</p>
<a href="http://arxiv.org/abs/2101.10472" target="_blank">arXiv:2101.10472</a> [<a href="http://arxiv.org/pdf/2101.10472" target="_blank">pdf</a>]

<h2>A Compositional Sheaf-Theoretic Framework for Event-Based Systems. (arXiv:2101.10485v1 [cs.RO])</h2>
<h3>Gioele Zardini (ETH Z&#xfc;rich), David I. Spivak (MIT), Andrea Censi (ETH Z&#xfc;rich), Emilio Frazzoli (ETH Z&#xfc;rich)</h3>
<p>A compositional sheaf-theoretic framework for the modeling of complex
event-based systems is presented. We show that event-based systems are
machines, with inputs and outputs, and that they can be composed with machines
of different types, all within a unified, sheaf-theoretic formalism. We take
robotic systems as an exemplar of complex systems and rigorously describe
actuators, sensors, and algorithms using this framework.
</p>
<a href="http://arxiv.org/abs/2101.10485" target="_blank">arXiv:2101.10485</a> [<a href="http://arxiv.org/pdf/2101.10485" target="_blank">pdf</a>]

<h2>Real-time Non-line-of-sight Imaging with Two-step Deep Remapping. (arXiv:2101.10492v1 [cs.CV])</h2>
<h3>Dayu Zhu, Wenshan Cai</h3>
<p>Conventional imaging only records the photons directly sent from the object
to the detector, while non-line-of-sight (NLOS) imaging takes the indirect
light into account. To explore the NLOS surroundings, most NLOS solutions
employ a transient scanning process, followed by a back-projection based
algorithm to reconstruct the NLOS scenes. However, the transient detection
requires sophisticated apparatus, with long scanning time and low robustness to
ambient environment, and the reconstruction algorithms typically cost tens of
minutes with high demand on memory and computational resources. Here we propose
a new NLOS solution to address the above defects, with innovations on both
detection equipment and reconstruction algorithm. We apply inexpensive
commercial Lidar for detection, with much higher scanning speed and better
compatibility to real-world imaging tasks. Our reconstruction framework is deep
learning based, consisting of a variational autoencoder and a compression
neural network. The generative feature and the two-step reconstruction strategy
of the framework guarantee high fidelity of NLOS imaging. The overall detection
and reconstruction process allows for real-time responses, with
state-of-the-art reconstruction performance. We have experimentally tested the
proposed solution on both a synthetic dataset and real objects, and further
demonstrated our method to be applicable for full-color NLOS imaging.
</p>
<a href="http://arxiv.org/abs/2101.10492" target="_blank">arXiv:2101.10492</a> [<a href="http://arxiv.org/pdf/2101.10492" target="_blank">pdf</a>]

<h2>Transparency in Multi-Human Multi-Robot Interaction. (arXiv:2101.10495v1 [cs.RO])</h2>
<h3>Jayam Patel, Tyagaraja Ramaswamy, Zhi Li, Carlo Pinciroli</h3>
<p>Transparency is a key factor in improving the performance of human-robot
interaction. A transparent interface allows humans to be aware of the state of
a robot and to assess the progress of the tasks at hand. When multi-robot
systems are involved, transparency is an even greater challenge, due to the
larger number of variables affecting the behavior of the robots as a whole.
Significant effort has been devoted to studying transparency when single
operators interact with multiple robots. However, studies on transparency that
focus on multiple human operators interacting with a multi-robot systems are
limited. This paper aims to fill this gap by presenting a human-swarm
interaction interface with graphical elements that can be enabled and disabled.
Through this interface, we study which graphical elements are contribute to
transparency by comparing four "transparency modes": (i) no transparency (no
operator receives information from the robots), (ii) central transparency (the
operators receive information only relevant to their personal task), (iii)
peripheral transparency (the operators share information on each others'
tasks), and (iv) mixed transparency (both central and peripheral). We report
the results in terms of awareness, trust, and workload of a user study
involving 18 participants engaged in a complex multi-robot task.
</p>
<a href="http://arxiv.org/abs/2101.10495" target="_blank">arXiv:2101.10495</a> [<a href="http://arxiv.org/pdf/2101.10495" target="_blank">pdf</a>]

<h2>ADMM-based Adaptive Sampling Strategy for Nonholonomic Mobile Robotic Sensor Networks. (arXiv:2101.10500v1 [cs.RO])</h2>
<h3>Viet-Anh Le, Linh Nguyen, Truong X. Nghiem</h3>
<p>This paper discusses the adaptive sampling problem in a nonholonomic mobile
robotic sensor network for efficiently monitoring a spatial field. It is
proposed to employ Gaussian process to model a spatial phenomenon and predict
it at unmeasured positions, which enables the sampling optimization problem to
be formulated by the use of the log determinant of a predicted covariance
matrix at next sampling locations. The control, movement and nonholonomic
dynamics constraints of the mobile sensors are also considered in the adaptive
sampling optimization problem. In order to tackle the nonlinearity and
nonconvexity of the objective function in the optimization problem we first
exploit the linearized alternating direction method of multipliers (L-ADMM)
method that can effectively simplify the objective function, though it is
computationally expensive since a nonconvex problem needs to be solved exactly
in each iteration. We then propose a novel approach called the successive
convexified ADMM (SC-ADMM) that sequentially convexify the nonlinear dynamic
constraints so that the original optimization problem can be split into convex
subproblems. It is noted that both the L-ADMM algorithm and our SC-ADMM
approach can solve the sampling optimization problem in either a centralized or
a distributed manner. We validated the proposed approaches in 1000 experiments
in a synthetic environment with a real-world dataset, where the obtained
results suggest that both the L-ADMM and SC- ADMM techniques can provide good
accuracy for the monitoring purpose. However, our proposed SC-ADMM approach
computationally outperforms the L-ADMM counterpart, demonstrating its better
practicality.
</p>
<a href="http://arxiv.org/abs/2101.10500" target="_blank">arXiv:2101.10500</a> [<a href="http://arxiv.org/pdf/2101.10500" target="_blank">pdf</a>]

<h2>Model-agnostic interpretation by visualization of feature perturbations. (arXiv:2101.10502v1 [cs.LG])</h2>
<h3>Wilson E. Marc&#xed;lio-Jr, Danilo M. Eler, Fabr&#xed;cio Breve</h3>
<p>Interpretation of machine learning models has become one of the most
important topics of research due to the necessity of maintaining control and
avoid bias in these algorithms. Since many machine learning algorithms are
published every day, there is a need for novel model-agnostic interpretation
approaches that could be used to interpret a great variety of algorithms. One
particularly useful way to interpret machine learning models is to feed
different input data to understand the changes in the prediction. Using such an
approach, practitioners can define relations among patterns of data and a
model's decision. In this work, we propose a model-agnostic interpretation
approach that uses visualization of feature perturbations induced by the
particle swarm optimization algorithm. We validate our approach both
qualitatively and quantitatively on publicly available datasets, showing the
capability to enhance the interpretation of different classifiers while
yielding very stable results if compared with the state of the art algorithms.
</p>
<a href="http://arxiv.org/abs/2101.10502" target="_blank">arXiv:2101.10502</a> [<a href="http://arxiv.org/pdf/2101.10502" target="_blank">pdf</a>]

<h2>On the Evaluation of Vision-and-Language Navigation Instructions. (arXiv:2101.10504v1 [cs.AI])</h2>
<h3>Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander Ku, Jason Baldridge, Eugene Ie</h3>
<p>Vision-and-Language Navigation wayfinding agents can be enhanced by
exploiting automatically generated navigation instructions. However, existing
instruction generators have not been comprehensively evaluated, and the
automatic evaluation metrics used to develop them have not been validated.
Using human wayfinders, we show that these generators perform on par with or
only slightly better than a template-based generator and far worse than human
instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are
ineffective for evaluating grounded navigation instructions. To improve
instruction evaluation, we propose an instruction-trajectory compatibility
model that operates without reference instructions. Our model shows the highest
correlation with human wayfinding outcomes when scoring individual
instructions. For ranking instruction generation systems, if reference
instructions are available we recommend using SPICE.
</p>
<a href="http://arxiv.org/abs/2101.10504" target="_blank">arXiv:2101.10504</a> [<a href="http://arxiv.org/pdf/2101.10504" target="_blank">pdf</a>]

<h2>Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm. (arXiv:2101.10506v1 [cs.LG])</h2>
<h3>Sajad Khodadadian, Thinh T. Doan, Siva Theja Maguluri, Justin Romberg</h3>
<p>Actor-critic style two-time-scale algorithms are very popular in
reinforcement learning, and have seen great empirical success. However, their
performance is not completely understood theoretically. In this paper, we
characterize the global convergence of an online natural actor-critic algorithm
in the tabular setting using a single trajectory. Our analysis applies to very
general settings, as we only assume that the underlying Markov chain is ergodic
under all policies (the so-called Recurrence assumption). We employ
$\epsilon$-greedy sampling in order to ensure enough exploration.

For a fixed exploration parameter $\epsilon$, we show that the natural actor
critic algorithm is $\mathcal{O}(\frac{1}{\epsilon T^{1/4}}+\epsilon)$ close to
the global optimum after $T$ iterations of the algorithm.

By carefully diminishing the exploration parameter $\epsilon$ as the
iterations proceed, we also show convergence to the global optimum at a rate of
$\mathcal{O}(1/T^{1/6})$.
</p>
<a href="http://arxiv.org/abs/2101.10506" target="_blank">arXiv:2101.10506</a> [<a href="http://arxiv.org/pdf/2101.10506" target="_blank">pdf</a>]

<h2>Continual Learning of Visual Concepts for Robots through Limited Supervision. (arXiv:2101.10509v1 [cs.RO])</h2>
<h3>Ali Ayub, Alan R. Wagner</h3>
<p>For many real-world robotics applications, robots need to continually adapt
and learn new concepts. Further, robots need to learn through limited data
because of scarcity of labeled data in the real-world environments. To this
end, my research focuses on developing robots that continually learn in dynamic
unseen environments/scenarios, learn from limited human supervision, remember
previously learned knowledge and use that knowledge to learn new concepts. I
develop machine learning models that not only produce State-of-the-results on
benchmark datasets but also allow robots to learn new objects and scenes in
unconstrained environments which lead to a variety of novel robotics
applications.
</p>
<a href="http://arxiv.org/abs/2101.10509" target="_blank">arXiv:2101.10509</a> [<a href="http://arxiv.org/pdf/2101.10509" target="_blank">pdf</a>]

<h2>Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v1 [cs.CV])</h2>
<h3>Mike Zheng Shou, Deepti Ghadiyaram, Weiyao Wang, Matt Feiszli</h3>
<p>This paper presents a novel task together with a new benchmark for detecting
generic, taxonomy-free event boundaries that segment a whole video into chunks.
Conventional work in temporal video segmentation and action detection focuses
on localizing pre-defined action categories and thus does not scale to generic
videos. Cognitive Science has known since last century that humans consistently
segment videos into meaningful temporal chunks. This segmentation happens
naturally, with no pre-defined event categories and without being explicitly
asked to do so. Here, we repeat these cognitive experiments on mainstream CV
datasets; with our novel annotation guideline which addresses the complexities
of taxonomy-free event boundary annotation, we introduce the task of Generic
Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Through
experiment and human study we demonstrate the value of the annotations. We view
this as an important stepping stone towards understanding the video as a whole,
and believe it has been previously neglected due to a lack of proper task
definition and annotations. Further, inspired by the cognitive finding that
humans mark boundaries at points where they are unable to predict the future
accurately, we explore un-supervised approaches based on temporal
predictability. We identify and extensively explore important design factors
for GEBD models on the TAPOS dataset and our Kinetics-GEBD while achieving
competitive performance and suggesting future work. We will release our
annotations and code at CVPR'21 LOVEU Challenge:
https://sites.google.com/view/loveucvpr21
</p>
<a href="http://arxiv.org/abs/2101.10511" target="_blank">arXiv:2101.10511</a> [<a href="http://arxiv.org/pdf/2101.10511" target="_blank">pdf</a>]

<h2>How Good is a Video Summary? A New Benchmarking Dataset and Evaluation Framework Towards Realistic Video Summarization. (arXiv:2101.10514v1 [cs.CV])</h2>
<h3>Vishal Kaushal, Suraj Kothawade, Anshul Tomar, Rishabh Iyer, Ganesh Ramakrishnan</h3>
<p>Automatic video summarization is still an unsolved problem due to several
challenges. The currently available datasets either have very short videos or
have few long videos of only a particular type. We introduce a new benchmarking
video dataset called VISIOCITY (VIdeo SummarIzatiOn based on Continuity, Intent
and DiversiTY) which comprises of longer videos across six different categories
with dense concept annotations capable of supporting different flavors of video
summarization and other vision problems. For long videos, human reference
summaries necessary for supervised video summarization techniques are difficult
to obtain. We explore strategies to automatically generate multiple reference
summaries from indirect ground truth present in VISIOCITY. We show that these
summaries are at par with human summaries. We also present a study of different
desired characteristics of a good summary and demonstrate how it is normal to
have two good summaries with different characteristics. Thus we argue that
evaluating a summary against one or more human summaries and using a single
measure has its shortcomings. We propose an evaluation framework for better
quantitative assessment of summary quality which is closer to human judgment.
Lastly, we present insights into how a model can be enhanced to yield better
summaries. Sepcifically, when multiple diverse ground truth summaries can
exist, learning from them individually and using a combination of loss
functions measuring different characteristics is better than learning from a
single combined (oracle) ground truth summary using a single loss function. We
demonstrate the effectiveness of doing so as compared to some of the
representative state of the art techniques tested on VISIOCITY. We release
VISIOCITY as a benchmarking dataset and invite researchers to test the
effectiveness of their video summarization algorithms on VISIOCITY.
</p>
<a href="http://arxiv.org/abs/2101.10514" target="_blank">arXiv:2101.10514</a> [<a href="http://arxiv.org/pdf/2101.10514" target="_blank">pdf</a>]

<h2>Deep Learning for Scene Classification: A Survey. (arXiv:2101.10531v1 [cs.CV])</h2>
<h3>Delu Zeng (1), Minyu Liao (1), Mohammad Tavakolian (2), Yulan Guo (3, 4), Bolei Zhou (5), Dewen Hu (3), Matti Pietik&#xe4;inen (2), Li Liu (2, 3) ((1) South China University of Technology, (2) University of Oulu, (3) National University of Defense Technology, (4) Sun Yatsen University, (5) Chinese University of Hong Kong)</h3>
<p>Scene classification, aiming at classifying a scene image to one of the
predefined scene categories by comprehending the entire image, is a
longstanding, fundamental and challenging problem in computer vision. The rise
of large-scale datasets, which constitute a dense sampling of diverse
real-world scenes, and the renaissance of deep learning techniques, which learn
powerful feature representations directly from big raw data, have been bringing
remarkable progress in the field of scene representation and classification. To
help researchers master needed advances in this field, the goal of this paper
is to provide a comprehensive survey of recent achievements in scene
classification using deep learning. More than 260 major publications are
included in this survey covering different aspects of scene classification,
including challenges, benchmark datasets, taxonomy, and quantitative
performance comparisons of the reviewed methods. In retrospect of what has been
achieved so far, this paper is concluded with a list of promising research
opportunities.
</p>
<a href="http://arxiv.org/abs/2101.10531" target="_blank">arXiv:2101.10531</a> [<a href="http://arxiv.org/pdf/2101.10531" target="_blank">pdf</a>]

<h2>Hyperspectral Image Classification: Artifacts of Dimension Reduction on Hybrid CNN. (arXiv:2101.10532v1 [cs.CV])</h2>
<h3>Muhammad Ahmad, Sidrah Shabbir, Rana Aamir Raza, Manuel Mazzara, Salvatore Distefano, Adil Mehmood Khan</h3>
<p>Convolutional Neural Networks (CNN) has been extensively studied for
Hyperspectral Image Classification (HSIC) more specifically, 2D and 3D CNN
models have proved highly efficient in exploiting the spatial and spectral
information of Hyperspectral Images. However, 2D CNN only considers the spatial
information and ignores the spectral information whereas 3D CNN jointly
exploits spatial-spectral information at a high computational cost. Therefore,
this work proposed a lightweight CNN (3D followed by 2D-CNN) model which
significantly reduces the computational cost by distributing spatial-spectral
feature extraction across a lighter model alongside a preprocessing that has
been carried out to improve the classification results. Five benchmark
Hyperspectral datasets (i.e., SalinasA, Salinas, Indian Pines, Pavia
University, Pavia Center, and Botswana) are used for experimental evaluation.
The experimental results show that the proposed pipeline outperformed in terms
of generalization performance, statistical significance, and computational
complexity, as compared to the state-of-the-art 2D/3D CNN models except
commonly used computationally expensive design choices.
</p>
<a href="http://arxiv.org/abs/2101.10532" target="_blank">arXiv:2101.10532</a> [<a href="http://arxiv.org/pdf/2101.10532" target="_blank">pdf</a>]

<h2>Modern Machine and Deep Learning Systems as a way to achieve Man-Computer Symbiosis. (arXiv:2101.10534v1 [cs.LG])</h2>
<h3>Chirag Gupta</h3>
<p>Man-Computer Symbiosis (MCS) was originally envisioned by the famous computer
pioneer J.C.R. Licklider in 1960, as a logical evolution of the then inchoate
relationship between computer and humans. In his paper, Licklider provided a
set of criteria by which to judge if a Man-Computer System is a symbiotic one,
and also provided some predictions about such systems in the near and far
future. Since then, innovations in computer networks and the invention of the
Internet were major developments towards that end. However, with most systems
based on conventional logical algorithms, many aspects of Licklider's MCS
remained unfulfilled. This paper explores the extent to which modern machine
learning systems in general, and deep learning ones in particular best
exemplify MCS systems, and why they are the prime contenders to achieve a true
Man-Computer Symbiosis as described by Licklider in his original paper in the
future. The case for deep learning is built by illustrating each point of the
original criteria as well as the criteria laid by subsequent research into MCS
systems, with specific examples and applications provided to strengthen the
arguments. The efficacy of deep neural networks in achieving Artificial General
Intelligence, which would be the perfect version of an MCS system is also
explored.
</p>
<a href="http://arxiv.org/abs/2101.10534" target="_blank">arXiv:2101.10534</a> [<a href="http://arxiv.org/pdf/2101.10534" target="_blank">pdf</a>]

<h2>Towards Entity Alignment in the Open World: An Unsupervised Approach. (arXiv:2101.10535v1 [cs.AI])</h2>
<h3>Weixin Zeng, Xiang Zhao, Jiuyang Tang, Xinyi Li, Minnan Luo, Qinghua Zheng</h3>
<p>Entity alignment (EA) aims to discover the equivalent entities in different
knowledge graphs (KGs). It is a pivotal step for integrating KGs to increase
knowledge coverage and quality. Recent years have witnessed a rapid increase of
EA frameworks. However, state-of-the-art solutions tend to rely on labeled data
for model training. Additionally, they work under the closed-domain setting and
cannot deal with entities that are unmatchable. To address these deficiencies,
we offer an unsupervised framework that performs entity alignment in the open
world. Specifically, we first mine useful features from the side information of
KGs. Then, we devise an unmatchable entity prediction module to filter out
unmatchable entities and produce preliminary alignment results. These
preliminary results are regarded as the pseudo-labeled data and forwarded to
the progressive learning framework to generate structural representations,
which are integrated with the side information to provide a more comprehensive
view for alignment. Finally, the progressive learning framework gradually
improves the quality of structural embeddings and enhances the alignment
performance by enriching the pseudo-labeled data with alignment results from
the previous round. Our solution does not require labeled data and can
effectively filter out unmatchable entities. Comprehensive experimental
evaluations validate its superiority.
</p>
<a href="http://arxiv.org/abs/2101.10535" target="_blank">arXiv:2101.10535</a> [<a href="http://arxiv.org/pdf/2101.10535" target="_blank">pdf</a>]

<h2>Ear Recognition. (arXiv:2101.10540v1 [cs.CV])</h2>
<h3>Nikolaos Athanasios Anagnostopoulos</h3>
<p>Ear recognition can be described as a revived scientific field. Ear
biometrics were long believed to not be accurate enough and held a secondary
place in scientific research, being seen as only complementary to other types
of biometrics, due to difficulties in measuring correctly the ear
characteristics and the potential occlusion of the ear by hair, clothes and ear
jewellery. However, recent research has reinstated them as a vivid research
field, after having addressed these problems and proven that ear biometrics can
provide really accurate identification and verification results. Several 2D and
3D imaging techniques, as well as acoustical techniques using sound emission
and reflection, have been developed and studied for ear recognition, while
there have also been significant advances towards a fully automated recognition
of the ear. Furthermore, ear biometrics have been proven to be mostly
non-invasive, adequately permanent and accurate, and hard to spoof and
counterfeit. Moreover, different ear recognition techniques have proven to be
as effective as face recognition ones, thus providing the opportunity for ear
recognition to be used in identification and verification applications.
Finally, even though some issues still remain open and require further
research, the scientific field of ear biometrics has proven to be not only
viable, but really thriving.
</p>
<a href="http://arxiv.org/abs/2101.10540" target="_blank">arXiv:2101.10540</a> [<a href="http://arxiv.org/pdf/2101.10540" target="_blank">pdf</a>]

<h2>Iterative Weak Learnability and Multi-Class AdaBoost. (arXiv:2101.10542v1 [stat.ML])</h2>
<h3>In-Koo Cho, Jonathan Libgober</h3>
<p>We construct an efficient recursive ensemble algorithm for the multi-class
classification problem, inspired by SAMME (Zhu, Zou, Rosset, and Hastie
(2009)). We strengthen the weak learnability condition in Zhu, Zou, Rosset, and
Hastie (2009) by requiring that the weak learnability condition holds for any
subset of labels with at least two elements. This condition is simpler to check
than many proposed alternatives (e.g., Mukherjee and Schapire (2013)). As
SAMME, our algorithm is reduced to the Adaptive Boosting algorithm (Schapire
and Freund (2012)) if the number of labels is two, and can be motivated as a
functional version of the steepest descending method to find an optimal
solution. In contrast to SAMME, our algorithm's final hypothesis converges to
the correct label with probability 1. For any number of labels, the probability
of misclassification vanishes exponentially as the training period increases.
The sum of the training error and an additional term, that depends only on the
sample size, bounds the generalization error of our algorithm as the Adaptive
Boosting algorithm.
</p>
<a href="http://arxiv.org/abs/2101.10542" target="_blank">arXiv:2101.10542</a> [<a href="http://arxiv.org/pdf/2101.10542" target="_blank">pdf</a>]

<h2>A Unified Paths Perspective for Pruning at Initialization. (arXiv:2101.10552v1 [cs.LG])</h2>
<h3>Thomas Gebhart, Udit Saxena, Paul Schrater</h3>
<p>A number of recent approaches have been proposed for pruning neural network
parameters at initialization with the goal of reducing the size and
computational burden of models while minimally affecting their training
dynamics and generalization performance. While each of these approaches have
some amount of well-founded motivation, a rigorous analysis of the effect of
these pruning methods on network training dynamics and their formal
relationship to each other has thus far received little attention. Leveraging
recent theoretical approximations provided by the Neural Tangent Kernel, we
unify a number of popular approaches for pruning at initialization under a
single path-centric framework. We introduce the Path Kernel as the
data-independent factor in a decomposition of the Neural Tangent Kernel and
show the global structure of the Path Kernel can be computed efficiently. This
Path Kernel decomposition separates the architectural effects from the
data-dependent effects within the Neural Tangent Kernel, providing a means to
predict the convergence dynamics of a network from its architecture alone. We
analyze the use of this structure in approximating training and generalization
performance of networks in the absence of data across a number of
initialization pruning approaches. Observing the relationship between input
data and paths and the relationship between the Path Kernel and its natural
norm, we additionally propose two augmentations of the SynFlow algorithm for
pruning at initialization.
</p>
<a href="http://arxiv.org/abs/2101.10552" target="_blank">arXiv:2101.10552</a> [<a href="http://arxiv.org/pdf/2101.10552" target="_blank">pdf</a>]

<h2>A General Framework Combining Generative Adversarial Networks and Mixture Density Networks for Inverse Modeling in Microstructural Materials Design. (arXiv:2101.10553v1 [cs.LG])</h2>
<h3>Zijiang Yang, Dipendra Jha, Arindam Paul, Wei-keng Liao, Alok Choudhary, Ankit Agrawal</h3>
<p>Microstructural materials design is one of the most important applications of
inverse modeling in materials science. Generally speaking, there are two broad
modeling paradigms in scientific applications: forward and inverse. While the
forward modeling estimates the observations based on known parameters, the
inverse modeling attempts to infer the parameters given the observations.
Inverse problems are usually more critical as well as difficult in scientific
applications as they seek to explore the parameters that cannot be directly
observed. Inverse problems are used extensively in various scientific fields,
such as geophysics, healthcare and materials science. However, it is
challenging to solve inverse problems, because they usually need to learn a
one-to-many non-linear mapping, and also require significant computing time,
especially for high-dimensional parameter space. Further, inverse problems
become even more difficult to solve when the dimension of input (i.e.
observation) is much lower than that of output (i.e. parameters). In this work,
we propose a framework consisting of generative adversarial networks and
mixture density networks for inverse modeling, and it is evaluated on a
materials science dataset for microstructural materials design. Compared with
baseline methods, the results demonstrate that the proposed framework can
overcome the above-mentioned challenges and produce multiple promising
solutions in an efficient manner.
</p>
<a href="http://arxiv.org/abs/2101.10553" target="_blank">arXiv:2101.10553</a> [<a href="http://arxiv.org/pdf/2101.10553" target="_blank">pdf</a>]

<h2>Self Sparse Generative Adversarial Networks. (arXiv:2101.10556v1 [cs.CV])</h2>
<h3>Wenliang Qian, Yang Xu, Wangmeng Zuo, Hui Li</h3>
<p>Generative Adversarial Networks (GANs) are an unsupervised generative model
that learns data distribution through adversarial training. However, recent
experiments indicated that GANs are difficult to train due to the requirement
of optimization in the high dimensional parameter space and the zero gradient
problem. In this work, we propose a Self Sparse Generative Adversarial Network
(Self-Sparse GAN) that reduces the parameter space and alleviates the zero
gradient problem. In the Self-Sparse GAN, we design a Self-Adaptive Sparse
Transform Module (SASTM) comprising the sparsity decomposition and feature-map
recombination, which can be applied on multi-channel feature maps to obtain
sparse feature maps. The key idea of Self-Sparse GAN is to add the SASTM
following every deconvolution layer in the generator, which can adaptively
reduce the parameter space by utilizing the sparsity in multi-channel feature
maps. We theoretically prove that the SASTM can not only reduce the search
space of the convolution kernel weight of the generator but also alleviate the
zero gradient problem by maintaining meaningful features in the Batch
Normalization layer and driving the weight of deconvolution layers away from
being negative. The experimental results show that our method achieves the best
FID scores for image generation compared with WGAN-GP on MNIST, Fashion-MNIST,
CIFAR-10, STL-10, mini-ImageNet, CELEBA-HQ, and LSUN bedrooms, and the relative
decrease of FID is 4.76% ~ 21.84%.
</p>
<a href="http://arxiv.org/abs/2101.10556" target="_blank">arXiv:2101.10556</a> [<a href="http://arxiv.org/pdf/2101.10556" target="_blank">pdf</a>]

<h2>Investigating the significance of adversarial attacks and their relation to interpretability for radar-based human activity recognition systems. (arXiv:2101.10562v1 [cs.CV])</h2>
<h3>Utku Ozbulak, Baptist Vandersmissen, Azarakhsh Jalalvand, Ivo Couckuyt, Arnout Van Messem, Wesley De Neve</h3>
<p>Given their substantial success in addressing a wide range of computer vision
challenges, Convolutional Neural Networks (CNNs) are increasingly being used in
smart home applications, with many of these applications relying on the
automatic recognition of human activities. In this context, low-power radar
devices have recently gained in popularity as recording sensors, given that the
usage of these devices allows mitigating a number of privacy concerns, a key
issue when making use of conventional video cameras. Another concern that is
often cited when designing smart home applications is the resilience of these
applications against cyberattacks. It is, for instance, well-known that the
combination of images and CNNs is vulnerable against adversarial examples,
mischievous data points that force machine learning models to generate wrong
classifications during testing time. In this paper, we investigate the
vulnerability of radar-based CNNs to adversarial attacks, and where these
radar-based CNNs have been designed to recognize human gestures. Through
experiments with four unique threat models, we show that radar-based CNNs are
susceptible to both white- and black-box adversarial attacks. We also expose
the existence of an extreme adversarial attack case, where it is possible to
change the prediction made by the radar-based CNNs by only perturbing the
padding of the inputs, without touching the frames where the action itself
occurs. Moreover, we observe that gradient-based attacks exercise perturbation
not randomly, but on important features of the input data. We highlight these
important features by making use of Grad-CAM, a popular neural network
interpretability method, hereby showing the connection between adversarial
perturbation and prediction interpretability.
</p>
<a href="http://arxiv.org/abs/2101.10562" target="_blank">arXiv:2101.10562</a> [<a href="http://arxiv.org/pdf/2101.10562" target="_blank">pdf</a>]

<h2>Transparent Contribution Evaluation for Secure Federated Learning on Blockchain. (arXiv:2101.10572v1 [cs.LG])</h2>
<h3>Shuaicheng Ma, Yang Cao, Li Xiong</h3>
<p>Federated Learning is a promising machine learning paradigm when multiple
parties collaborate to build a high-quality machine learning model.
Nonetheless, these parties are only willing to participate when given enough
incentives, such as a fair reward based on their contributions. Many studies
explored Shapley value based methods to evaluate each party's contribution to
the learned model. However, they commonly assume a trusted server to train the
model and evaluate the data owners' model contributions, which lacks
transparency and may hinder the success of federated learning in practice. In
this work, we propose a blockchain-based federated learning framework and a
protocol to transparently evaluate each participants' contribution. Our
framework protects all parties' privacy in the model building phrase and
transparently evaluates contributions based on the model updates. The
experiment with the handwritten digits dataset demonstrates that the proposed
method can effectively evaluate the contributions.
</p>
<a href="http://arxiv.org/abs/2101.10572" target="_blank">arXiv:2101.10572</a> [<a href="http://arxiv.org/pdf/2101.10572" target="_blank">pdf</a>]

<h2>Toward Personalized Affect-Aware Socially Assistive Robot Tutors in Long-Term Interventions for Children with Autism. (arXiv:2101.10580v1 [cs.RO])</h2>
<h3>Zhonghao Shi, Thomas R Groechel, Shomik Jain, Kourtney Chima, Ognjen (Oggi) Rudovic, Maja J Matari&#x107;</h3>
<p>Affect-aware socially assistive robotics (SAR) has shown great potential for
augmenting interventions for children with autism spectrum disorders (ASD).
However, current SAR cannot yet perceive the unique and diverse set of atypical
cognitive-affective behaviors from children with ASD in an automatic and
personalized fashion in long-term (multi-session) real-world interactions. To
bridge this gap, this work designed and validated personalized models of
arousal and valence for children with ASD using a multi-session in-home dataset
of SAR interventions. By training machine learning (ML) algorithms with
supervised domain adaptation (s-DA), the personalized models were able to trade
off between the limited individual data and the more abundant less personal
data pooled from other study participants. We evaluated the effects of
personalization on a long-term multimodal dataset consisting of 4 children with
ASD with a total of 19 sessions, and derived inter-rater reliability (IR)
scores for binary arousal (IR = 83%) and valence (IR = 81%) labels between
human annotators. Our results show that personalized Gradient Boosted Decision
Trees (XGBoost) models with s-DA outperformed two non-personalized
individualized and generic model baselines not only on the weighted average of
all sessions, but also statistically (p &lt; .05) across individual sessions. This
work paves the way for the development of personalized autonomous SAR systems
tailored toward individuals with atypical cognitive-affective and
socio-emotional needs.
</p>
<a href="http://arxiv.org/abs/2101.10580" target="_blank">arXiv:2101.10580</a> [<a href="http://arxiv.org/pdf/2101.10580" target="_blank">pdf</a>]

<h2>SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models. (arXiv:2101.10586v1 [cs.CV])</h2>
<h3>Haekyu Park, Zijie J. Wang, Nilaksh Das, Anindya S. Paul, Pruthvi Perumalla, Zhiyan Zhou, Duen Horng Chau</h3>
<p>Skeleton-based human action recognition technologies are increasingly used in
video based applications, such as home robotics, healthcare on aging
population, and surveillance. However, such models are vulnerable to
adversarial attacks, raising serious concerns for their use in safety-critical
applications. To develop an effective defense against attacks, it is essential
to understand how such attacks mislead the pose detection models into making
incorrect predictions. We present SkeletonVis, the first interactive system
that visualizes how the attacks work on the models to enhance human
understanding of attacks.
</p>
<a href="http://arxiv.org/abs/2101.10586" target="_blank">arXiv:2101.10586</a> [<a href="http://arxiv.org/pdf/2101.10586" target="_blank">pdf</a>]

<h2>Design, analysis and control of the series-parallel hybrid RH5 humanoid robot. (arXiv:2101.10591v1 [cs.RO])</h2>
<h3>Julian Esser, Shivesh Kumar, Heiner Peters, Vinzenz Bargsten, Jose de Gea Fernandez, Carlos Mastalli, Olivier Stasse, Frank Kirchner</h3>
<p>Last decades of humanoid research has shown that humanoids developed for high
dynamic performance require a stiff structure and optimal distribution of
mass--inertial properties. Humanoid robots built with a purely tree type
architecture tend to be bulky and usually suffer from velocity and force/torque
limitations. This paper presents a novel series-parallel hybrid humanoid called
RH5 which is 2 m tall and weighs only 62.5 kg capable of performing heavy-duty
dynamic tasks with 5 kg payloads in each hand. The analysis and control of this
humanoid is performed with whole-body trajectory optimization technique based
on differential dynamic programming (DDP). Additionally, we present an improved
contact stability soft-constrained DDP algorithm which is able to generate
physically consistent walking trajectories for the humanoid that can be tracked
via a simple PD position control in a physics simulator. Finally, we showcase
preliminary experimental results on the RH5 humanoid robot.
</p>
<a href="http://arxiv.org/abs/2101.10591" target="_blank">arXiv:2101.10591</a> [<a href="http://arxiv.org/pdf/2101.10591" target="_blank">pdf</a>]

<h2>Probability Trajectory: One New Movement Description for Trajectory Prediction. (arXiv:2101.10595v1 [cs.CV])</h2>
<h3>Pei Lv, Hui Wei, Tianxin Gu, Yuzhen Zhang, Xiaoheng Jiang, Bing Zhou, Mingliang Xu</h3>
<p>Trajectory prediction is a fundamental and challenging task for numerous
applications, such as autonomous driving and intelligent robots. Currently,
most of existing work treat the pedestrian trajectory as a series of fixed
two-dimensional coordinates. However, in real scenarios, the trajectory often
exhibits randomness, and has its own probability distribution. Inspired by this
observed fact, also considering other movement characteristics of pedestrians,
we propose one simple and intuitive movement description, probability
trajectory, which maps the coordinate points of pedestrian trajectory into
two-dimensional Gaussian distribution in images. Based on this unique
description, we develop one novel trajectory prediction method, called social
probability. The method combines the new probability trajectory and powerful
convolution recurrent neural networks together. Both the input and output of
our method are probability trajectories, which provide the recurrent neural
network with sufficient spatial and random information of moving pedestrians.
And the social probability extracts spatio-temporal features directly on the
new movement description to generate robust and accurate predicted results. The
experiments on public benchmark datasets show the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2101.10595" target="_blank">arXiv:2101.10595</a> [<a href="http://arxiv.org/pdf/2101.10595" target="_blank">pdf</a>]

<h2>Graphonomy: Universal Image Parsing via Graph Reasoning and Transfer. (arXiv:2101.10620v1 [cs.CV])</h2>
<h3>Liang Lin, Yiming Gao, Ke Gong, Meng Wang, Xiaodan Liang</h3>
<p>Prior highly-tuned image parsing models are usually studied in a certain
domain with a specific set of semantic labels and can hardly be adapted into
other scenarios (e.g., sharing discrepant label granularity) without extensive
re-training. Learning a single universal parsing model by unifying label
annotations from different domains or at various levels of granularity is a
crucial but rarely addressed topic. This poses many fundamental learning
challenges, e.g., discovering underlying semantic structures among different
label granularity or mining label correlation across relevant tasks. To address
these challenges, we propose a graph reasoning and transfer learning framework,
named "Graphonomy", which incorporates human knowledge and label taxonomy into
the intermediate graph representation learning beyond local convolutions. In
particular, Graphonomy learns the global and structured semantic coherency in
multiple domains via semantic-aware graph reasoning and transfer, enforcing the
mutual benefits of the parsing across domains (e.g., different datasets or
co-related tasks). The Graphonomy includes two iterated modules: Intra-Graph
Reasoning and Inter-Graph Transfer modules. The former extracts the semantic
graph in each domain to improve the feature representation learning by
propagating information with the graph; the latter exploits the dependencies
among the graphs from different domains for bidirectional knowledge transfer.
We apply Graphonomy to two relevant but different image understanding research
topics: human parsing and panoptic segmentation, and show Graphonomy can handle
both of them well via a standard pipeline against current state-of-the-art
approaches. Moreover, some extra benefit of our framework is demonstrated,
e.g., generating the human parsing at various levels of granularity by unifying
annotations across different datasets.
</p>
<a href="http://arxiv.org/abs/2101.10620" target="_blank">arXiv:2101.10620</a> [<a href="http://arxiv.org/pdf/2101.10620" target="_blank">pdf</a>]

<h2>Hyper-optimization with Gaussian Process and Differential Evolution Algorithm. (arXiv:2101.10625v1 [cs.LG])</h2>
<h3>Jakub Klus, Pavel Grunt, Martin Dobrovoln&#xfd;</h3>
<p>Optimization of problems with high computational power demands is a
challenging task. A probabilistic approach to such optimization called Bayesian
optimization lowers performance demands by solving mathematically simpler model
of the problem. Selected approach, Gaussian Process, models problem using a
mixture of Gaussian functions. This paper presents specific modifications of
Gaussian Process optimization components from available scientific libraries.
Presented modifications were submitted to BlackBox 2020 challenge, where it
outperformed some conventionally available optimization libraries.
</p>
<a href="http://arxiv.org/abs/2101.10625" target="_blank">arXiv:2101.10625</a> [<a href="http://arxiv.org/pdf/2101.10625" target="_blank">pdf</a>]

<h2>Ensembling complex network 'perspectives' for mild cognitive impairment detection with artificial neural networks. (arXiv:2101.10629v1 [cs.CV])</h2>
<h3>Eufemia Lella, Gennaro Vessio</h3>
<p>In this paper, we propose a novel method for mild cognitive impairment
detection based on jointly exploiting the complex network and the neural
network paradigm. In particular, the method is based on ensembling different
brain structural "perspectives" with artificial neural networks. On one hand,
these perspectives are obtained with complex network measures tailored to
describe the altered brain connectivity. In turn, the brain reconstruction is
obtained by combining diffusion-weighted imaging (DWI) data to tractography
algorithms. On the other hand, artificial neural networks provide a means to
learn a mapping from topological properties of the brain to the presence or
absence of cognitive decline. The effectiveness of the method is studied on a
well-known benchmark data set in order to evaluate if it can provide an
automatic tool to support the early disease diagnosis. Also, the effects of
balancing issues are investigated to further assess the reliability of the
complex network approach to DWI data.
</p>
<a href="http://arxiv.org/abs/2101.10629" target="_blank">arXiv:2101.10629</a> [<a href="http://arxiv.org/pdf/2101.10629" target="_blank">pdf</a>]

<h2>ResLT: Residual Learning for Long-tailed Recognition. (arXiv:2101.10633v1 [cs.CV])</h2>
<h3>Jiequan Cui, Shu Liu, Zhuotao Tian, Jiaya Jia</h3>
<p>Deep learning algorithms face great challenges with long-tailed data
distribution which, however, is quite a common case in real-world scenarios.
Previous methods tackle the problem from either the aspect of input space
(re-sampling classes with different frequencies) or loss space (re-weighting
classes with different weights), suffering from heavy over-fitting to tail
classes or hard optimization during training. To alleviate these issues, we
propose a more fundamental perspective for long-tailed recognition, {\it i.e.},
from the aspect of parameter space, and aims to preserve specific capacity for
classes with low frequencies. From this perspective, the trivial solution
utilizes different branches for the head, medium, tail classes respectively,
and then sums their outputs as the final results is not feasible. Instead, we
design the effective residual fusion mechanism -- with one main branch
optimized to recognize images from all classes, another two residual branches
are gradually fused and optimized to enhance images from medium+tail classes
and tail classes respectively. Then the branches are aggregated into final
results by additive shortcuts. We test our method on several benchmarks, {\it
i.e.}, long-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and
iNaturalist 2018. Experimental results manifest that our method achieves new
state-of-the-art for long-tailed recognition. Code will be available at
\url{https://github.com/FPNAS/ResLT}.
</p>
<a href="http://arxiv.org/abs/2101.10633" target="_blank">arXiv:2101.10633</a> [<a href="http://arxiv.org/pdf/2101.10633" target="_blank">pdf</a>]

<h2>CDSM -- Casual Inference using Deep Bayesian Dynamic Survival Models. (arXiv:2101.10643v1 [stat.ML])</h2>
<h3>Jie Zhu, Blanca Gallego</h3>
<p>A smart healthcare system that supports clinicians for risk-calibrated
treatment assessment typically requires the accurate modeling of time-to-event
outcomes. To tackle this sequential treatment effect estimation problem, we
developed causal dynamic survival model (CDSM) for causal inference with
survival outcomes using longitudinal electronic health record (EHR). CDSM has
impressive explanatory performance while maintaining the prediction capability
of conventional binary neural network predictors. It borrows the strength from
explanatory framework including the survival analysis and counterfactual
framework and integrates them with the prediction power from a deep Bayesian
recurrent neural network to extract implicit knowledge from EHR data. In two
large clinical cohort studies, our model identified the conditional average
treatment effect in accordance with previous literature yet detected individual
effect heterogeneity over time and patient subgroups. The model provides
individualized and clinically interpretable treatment effect estimations to
improve patient outcomes.
</p>
<a href="http://arxiv.org/abs/2101.10643" target="_blank">arXiv:2101.10643</a> [<a href="http://arxiv.org/pdf/2101.10643" target="_blank">pdf</a>]

<h2>Variational Information Bottleneck Model for Accurate Indoor Position Recognition. (arXiv:2101.10655v1 [cs.LG])</h2>
<h3>Weizhu Qian, Franck Gechter</h3>
<p>Recognizing user location with WiFi fingerprints is a popular approach for
accurate indoor positioning problems. In this work, our goal is to interpret
WiFi fingerprints into actual user locations. However, WiFi fingerprint data
can be very high dimensional in some cases, we need to find a good
representation of the input data for the learning task first. Otherwise, using
neural networks will suffer from severe overfitting. In this work, we solve
this issue by combining the Information Bottleneck method and Variational
Inference. Based on these two approaches, we propose a Variational Information
Bottleneck model for accurate indoor positioning. The proposed model consists
of an encoder structure and a predictor structure. The encoder is to find a
good representation in the input data for the learning task. The predictor is
to use the latent representation to predict the final output. To enhance the
generalization of our model, we also adopt the Dropout technique for each
hidden layer of the decoder. We conduct the validation experiments on a
real-world dataset. We also compare the proposed model to other existing
methods so as to quantify the performances of our method.
</p>
<a href="http://arxiv.org/abs/2101.10655" target="_blank">arXiv:2101.10655</a> [<a href="http://arxiv.org/pdf/2101.10655" target="_blank">pdf</a>]

<h2>Ordinal Monte Carlo Tree Search. (arXiv:2101.10670v1 [cs.AI])</h2>
<h3>Tobias Joppen, Johannes F&#xfc;rnkranz</h3>
<p>In many problem settings, most notably in game playing, an agent receives a
possibly delayed reward for its actions. Often, those rewards are handcrafted
and not naturally given. Even simple terminal-only rewards, like winning equals
one and losing equals minus one, can not be seen as an unbiased statement,
since these values are chosen arbitrarily, and the behavior of the learner may
change with different encodings. It is hard to argue about good rewards and the
performance of an agent often depends on the design of the reward signal. In
particular, in domains where states by nature only have an ordinal ranking and
where meaningful distance information between game state values is not
available, a numerical reward signal is necessarily biased. In this paper we
take a look at MCTS, a popular algorithm to solve MDPs, highlight a reoccurring
problem concerning its use of rewards, and show that an ordinal treatment of
the rewards overcomes this problem. Using the General Video Game Playing
framework we show dominance of our newly proposed ordinal MCTS algorithm over
other MCTS variants, based on a novel bandit algorithm that we also introduce
and test versus UCB.
</p>
<a href="http://arxiv.org/abs/2101.10670" target="_blank">arXiv:2101.10670</a> [<a href="http://arxiv.org/pdf/2101.10670" target="_blank">pdf</a>]

<h2>AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v1 [cs.CV])</h2>
<h3>Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang</h3>
<p>Recently, some approaches are proposed to harness deep convolutional networks
to facilitate superpixel segmentation. The common practice is to first evenly
divide the image into a pre-defined number of grids and then learn to associate
each pixel with its surrounding grids. However, simply applying a series of
convolution operations with limited receptive fields can only implicitly
perceive the relations between the pixel and its surrounding grids.
Consequently, existing methods often fail to provide an effective context when
inferring the association map. To remedy this issue, we propose a novel
\textbf{A}ssociation \textbf{I}mplantation (AI) module to enable the network to
explicitly capture the relations between the pixel and its surrounding grids.
The proposed AI module directly implants the features of grid cells to the
surrounding of its corresponding central pixel, and conducts convolution on the
padded window to adaptively transfer knowledge between them. With such an
implantation operation, the network could explicitly harvest the pixel-grid
level context, which is more in line with the target of superpixel segmentation
comparing to the pixel-wise relation. Furthermore, to pursue better boundary
precision, we design a boundary-perceiving loss to help the network
discriminate the pixels around boundaries in hidden feature level, which could
benefit the subsequent inferring modules to accurately identify more boundary
pixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our
method could not only achieve state-of-the-art performance but maintain
satisfactory inference efficiency.
</p>
<a href="http://arxiv.org/abs/2101.10696" target="_blank">arXiv:2101.10696</a> [<a href="http://arxiv.org/pdf/2101.10696" target="_blank">pdf</a>]

<h2>A fast algorithm for complex discord searches in time series: HOT SAX Time. (arXiv:2101.10698v1 [cs.LG])</h2>
<h3>Paolo Avogadro, Matteo Alessandro Dominoni</h3>
<p>Time series analysis is quickly proceeding towards long and complex tasks. In
recent years, fast approximate algorithms for discord search have been proposed
in order to compensate for the increasing size of the time series. It is more
interesting, however, to find quick exact solutions. In this research, we
improved HOT SAX by exploiting two main ideas: the warm-up process, and the
similarity between sequences close in time. The resulting algorithm, called HOT
SAX Time (HST), has been validated with real and synthetic time series, and
successfully compared with HOT SAX, RRA, SCAMP, and DADD. The complexity of a
discord search has been evaluated with a new indicator, the cost per sequence
(cps), which allows one to compare searches on time series of different
lengths. Numerical evidence suggests that two conditions are involved in
determining the complexity of a discord search in a non-trivial way: the length
of the discords, and the noise/signal ratio. In the case of complex searches,
HST can be more than 100 times faster than HOT SAX, thus being at the forefront
of the exact discord search.
</p>
<a href="http://arxiv.org/abs/2101.10698" target="_blank">arXiv:2101.10698</a> [<a href="http://arxiv.org/pdf/2101.10698" target="_blank">pdf</a>]

<h2>Introducing and assessing the explainable AI (XAI)method: SIDU. (arXiv:2101.10710v1 [cs.CV])</h2>
<h3>Satya M. Muddamsetty, Mohammad N. S. Jahromi, Andreea E. Ciontos, Laura M. Fenoy, Thomas B. Moeslund</h3>
<p>Explainable Artificial Intelligence (XAI) has in recent years become a
well-suited framework to generate human understandable explanations of black
box models. In this paper, we present a novel XAI visual explanation algorithm
denoted SIDU that can effectively localize entire object regions responsible
for prediction in a full extend. We analyze its robustness and effectiveness
through various computational and human subject experiments. In particular, we
assess the SIDU algorithm using three different types of evaluations
(Application, Human and Functionally-Grounded) to demonstrate its superior
performance. The robustness of SIDU is further studied in presence of
adversarial attack on black box models to better understand its performance.
</p>
<a href="http://arxiv.org/abs/2101.10710" target="_blank">arXiv:2101.10710</a> [<a href="http://arxiv.org/pdf/2101.10710" target="_blank">pdf</a>]

<h2>Short-term prediction of Time Series based on bounding techniques. (arXiv:2101.10719v1 [stat.ML])</h2>
<h3>Pedro Cadah&#xed;a, Jose Manuel Bravo Caro</h3>
<p>In this paper it is reconsidered the prediction problem in time series
framework by using a new non-parametric approach. Through this reconsideration,
the prediction is obtained by a weighted sum of past observed data. These
weights are obtained by solving a constrained linear optimization problem that
minimizes an outer bound of the prediction error. The innovation is to consider
both deterministic and stochastic assumptions in order to obtain the upper
bound of the prediction error, a tuning parameter is used to balance these
deterministic-stochastic assumptions in order to improve the predictor
performance. A benchmark is included to illustrate that the proposed predictor
can obtain suitable results in a prediction scheme, and can be an interesting
alternative method to the classical non-parametric methods. Besides, it is
shown how this model can outperform the preexisting ones in a short term
forecast.
</p>
<a href="http://arxiv.org/abs/2101.10719" target="_blank">arXiv:2101.10719</a> [<a href="http://arxiv.org/pdf/2101.10719" target="_blank">pdf</a>]

<h2>Consistent Mesh Colors for Multi-View Reconstructed 3D Scenes. (arXiv:2101.10734v1 [cs.CV])</h2>
<h3>Mohamed Dahy Elkhouly, Alessio Del Bue, Stuart James</h3>
<p>We address the issue of creating consistent mesh texture maps captured from
scenes without color calibration. We find that the method for aggregation of
the multiple views is crucial for creating spatially consistent meshes without
the need to explicitly optimize for spatial consistency. We compute a color
prior from the cross-correlation of observable view faces and the faces per
view to identify an optimal per-face color. We then use this color in a
re-weighting ratio for the best-view texture, which is identified by prior mesh
texturing work, to create a spatial consistent texture map. Despite our method
not explicitly handling spatial consistency, our results show qualitatively
more consistent results than other state-of-the-art techniques while being
computationally more efficient. We evaluate on prior datasets and additionally
Matterport3D showing qualitative improvements.
</p>
<a href="http://arxiv.org/abs/2101.10734" target="_blank">arXiv:2101.10734</a> [<a href="http://arxiv.org/pdf/2101.10734" target="_blank">pdf</a>]

<h2>Dynamic prediction of time to event with survival curves. (arXiv:2101.10739v1 [stat.ML])</h2>
<h3>Jie Zhu, Blanca Gallego</h3>
<p>With the ever-growing complexity of primary health care system, proactive
patient failure management is an effective way to enhancing the availability of
health care resource. One key enabler is the dynamic prediction of
time-to-event outcomes. Conventional explanatory statistical approach lacks the
capability of making precise individual level prediction, while the data
adaptive binary predictors does not provide nominal survival curves for
biologically plausible survival analysis. The purpose of this article is to
elucidate that the knowledge of explanatory survival analysis can significantly
enhance the current black-box data adaptive prediction models. We apply our
recently developed counterfactual dynamic survival model (CDSM) to static and
longitudinal observational data and testify that the inflection point of its
estimated individual survival curves provides reliable prediction of the
patient failure time.
</p>
<a href="http://arxiv.org/abs/2101.10739" target="_blank">arXiv:2101.10739</a> [<a href="http://arxiv.org/pdf/2101.10739" target="_blank">pdf</a>]

<h2>Towards Universal Physical Attacks On Cascaded Camera-Lidar 3D Object Detection Models. (arXiv:2101.10747v1 [cs.CV])</h2>
<h3>Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, Rabab Ward</h3>
<p>We propose a universal and physically realizable adversarial attack on a
cascaded multi-modal deep learning network (DNN), in the context of
self-driving cars. DNNs have achieved high performance in 3D object detection,
but they are known to be vulnerable to adversarial attacks. These attacks have
been heavily investigated in the RGB image domain and more recently in the
point cloud domain, but rarely in both domains simultaneously - a gap to be
filled in this paper. We use a single 3D mesh and differentiable rendering to
explore how perturbing the mesh's geometry and texture can reduce the
robustness of DNNs to adversarial attacks. We attack a prominent cascaded
multi-modal DNN, the Frustum-Pointnet model. Using the popular KITTI benchmark,
we showed that the proposed universal multi-modal attack was successful in
reducing the model's ability to detect a car by nearly 73%. This work can aid
in the understanding of what the cascaded RGB-point cloud DNN learns and its
vulnerability to adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2101.10747" target="_blank">arXiv:2101.10747</a> [<a href="http://arxiv.org/pdf/2101.10747" target="_blank">pdf</a>]

<h2>Learning Spatial and Spatio-Temporal Pixel Aggregations for Image and Video Denoising. (arXiv:2101.10760v1 [cs.CV])</h2>
<h3>Xiangyu Xu, Muchen Li, Wenxiu Sun, Ming-Hsuan Yang</h3>
<p>Existing denoising methods typically restore clear results by aggregating
pixels from the noisy input. Instead of relying on hand-crafted aggregation
schemes, we propose to explicitly learn this process with deep neural networks.
We present a spatial pixel aggregation network and learn the pixel sampling and
averaging strategies for image denoising. The proposed model naturally adapts
to image structures and can effectively improve the denoised results.
Furthermore, we develop a spatio-temporal pixel aggregation network for video
denoising to efficiently sample pixels across the spatio-temporal space. Our
method is able to solve the misalignment issues caused by large motion in
dynamic scenes. In addition, we introduce a new regularization term for
effectively training the proposed video denoising model. We present extensive
analysis of the proposed method and demonstrate that our model performs
favorably against the state-of-the-art image and video denoising approaches on
both synthetic and real-world data.
</p>
<a href="http://arxiv.org/abs/2101.10760" target="_blank">arXiv:2101.10760</a> [<a href="http://arxiv.org/pdf/2101.10760" target="_blank">pdf</a>]

<h2>An Efficient Statistical-based Gradient Compression Technique for Distributed Training Systems. (arXiv:2101.10761v1 [cs.LG])</h2>
<h3>Ahmed M. Abdelmoniem, Ahmed Elzanaty, Mohamed-Slim Alouini, Marco Canini</h3>
<p>The recent many-fold increase in the size of deep neural networks makes
efficient distributed training challenging. Many proposals exploit the
compressibility of the gradients and propose lossy compression techniques to
speed up the communication stage of distributed training. Nevertheless,
compression comes at the cost of reduced model quality and extra computation
overhead. In this work, we design an efficient compressor with minimal
overhead. Noting the sparsity of the gradients, we propose to model the
gradients as random variables distributed according to some sparsity-inducing
distributions (SIDs). We empirically validate our assumption by studying the
statistical characteristics of the evolution of gradient vectors over the
training process. We then propose Sparsity-Inducing Distribution-based
Compression (SIDCo), a threshold-based sparsification scheme that enjoys
similar threshold estimation quality to deep gradient compression (DGC) while
being faster by imposing lower compression overhead. Our extensive evaluation
of popular machine learning benchmarks involving both recurrent neural network
(RNN) and convolution neural network (CNN) models shows that SIDCo speeds up
training by up to 41:7%, 7:6%, and 1:9% compared to the no-compression
baseline, Topk, and DGC compressors, respectively.
</p>
<a href="http://arxiv.org/abs/2101.10761" target="_blank">arXiv:2101.10761</a> [<a href="http://arxiv.org/pdf/2101.10761" target="_blank">pdf</a>]

<h2>Benchmarking Invertible Architectures on Inverse Problems. (arXiv:2101.10763v1 [cs.LG])</h2>
<h3>Jakob Kruse, Lynton Ardizzone, Carsten Rother, Ullrich K&#xf6;the</h3>
<p>Recent work demonstrated that flow-based invertible neural networks are
promising tools for solving ambiguous inverse problems. Following up on this,
we investigate how ten invertible architectures and related models fare on two
intuitive, low-dimensional benchmark problems, obtaining the best results with
coupling layers and simple autoencoders. We hope that our initial efforts
inspire other researchers to evaluate their invertible architectures in the
same setting and put forth additional benchmarks, so our evaluation may
eventually grow into an official community challenge.
</p>
<a href="http://arxiv.org/abs/2101.10763" target="_blank">arXiv:2101.10763</a> [<a href="http://arxiv.org/pdf/2101.10763" target="_blank">pdf</a>]

<h2>LIGHTS: LIGHT Specularity Dataset for specular detection in Multi-view. (arXiv:2101.10772v1 [cs.CV])</h2>
<h3>Mohamed Dahy Elkhouly, Theodore Tsesmelis, Alessio Del Bue, Stuart James</h3>
<p>Specular highlights are commonplace in images, however, methods for detecting
them and in turn removing the phenomenon are particularly challenging. A reason
for this, is due to the difficulty of creating a dataset for training or
evaluation, as in the real-world we lack the necessary control over the
environment. Therefore, we propose a novel physically-based rendered LIGHT
Specularity (LIGHTS) Dataset for the evaluation of the specular highlight
detection task. Our dataset consists of 18 high quality architectural scenes,
where each scene is rendered with multiple views. In total we have 2,603 views
with an average of 145 views per scene. Additionally we propose a simple
aggregation based method for specular highlight detection that outperforms
prior work by 3.6% in two orders of magnitude less time on our dataset.
</p>
<a href="http://arxiv.org/abs/2101.10772" target="_blank">arXiv:2101.10772</a> [<a href="http://arxiv.org/pdf/2101.10772" target="_blank">pdf</a>]

<h2>Lightweight Multi-Branch Network for Person Re-Identification. (arXiv:2101.10774v1 [cs.CV])</h2>
<h3>Fabian Herzog, Xunbo Ji, Torben Teepe, Stefan H&#xf6;rmann, Johannes Gilg, Gerhard Rigoll</h3>
<p>Person Re-Identification aims to retrieve person identities from images
captured by multiple cameras or the same cameras in different time instances
and locations. Because of its importance in many vision applications from
surveillance to human-machine interaction, person re-identification methods
need to be reliable and fast. While more and more deep architectures are
proposed for increasing performance, those methods also increase overall model
complexity. This paper proposes a lightweight network that combines global,
part-based, and channel features in a unified multi-branch architecture that
builds on the resource-efficient OSNet backbone. Using a well-founded
combination of training techniques and design choices, our final model achieves
state-of-the-art results on CUHK03 labeled, CUHK03 detected, and Market-1501
with 85.1% mAP / 87.2% rank1, 82.4% mAP / 84.9% rank1, and 91.5% mAP / 96.3%
rank1, respectively.
</p>
<a href="http://arxiv.org/abs/2101.10774" target="_blank">arXiv:2101.10774</a> [<a href="http://arxiv.org/pdf/2101.10774" target="_blank">pdf</a>]

<h2>CoMo: A novel co-moving 3D camera system. (arXiv:2101.10775v1 [cs.CV])</h2>
<h3>Andrea Cavagna, Xiao Feng, Stefania Melillo, Leonardo Parisi, Lorena Postiglione, Pablo Villegas</h3>
<p>Motivated by the theoretical interest in reconstructing long 3D trajectories
of individual birds in large flocks, we developed CoMo, a co-moving camera
system of two synchronized high speed cameras coupled with rotational stages,
which allow us to dynamically follow the motion of a target flock. With the
rotation of the cameras we overcome the limitations of standard static systems
that restrict the duration of the collected data to the short interval of time
in which targets are in the cameras common field of view, but at the same time
we change in time the external parameters of the system, which have then to be
calibrated frame-by-frame. We address the calibration of the external
parameters measuring the position of the cameras and their three angles of yaw,
pitch and roll in the system "home" configuration (rotational stage at an angle
equal to 0deg and combining this static information with the time dependent
rotation due to the stages. We evaluate the robustness and accuracy of the
system by comparing reconstructed and measured 3D distances in what we call 3D
tests, which show a relative error of the order of 1%. The novelty of the work
presented in this paper is not only on the system itself, but also on the
approach we use in the tests, which we show to be a very powerful tool in
detecting and fixing calibration inaccuracies and that, for this reason, may be
relevant for a broad audience.
</p>
<a href="http://arxiv.org/abs/2101.10775" target="_blank">arXiv:2101.10775</a> [<a href="http://arxiv.org/pdf/2101.10775" target="_blank">pdf</a>]

<h2>Joint Forecasting of Features and Feature Motion for Dense Semantic Future Prediction. (arXiv:2101.10777v1 [cs.CV])</h2>
<h3>Josip &#x160;ari&#x107;, Sacha Vra&#x17e;i&#x107;, Sini&#x161;a &#x160;egvi&#x107;</h3>
<p>We present a novel dense semantic forecasting approach which is applicable to
a variety of architectures and tasks. The approach consists of two modules.
Feature-to-motion (F2M) module forecasts a dense deformation field which warps
past features into their future positions. Feature-to-feature (F2F) module
regresses the future features directly and is therefore able to account for
emergent scenery. The compound F2MF approach decouples effects of motion from
the effects of novelty in a task-agnostic manner. We aim to apply F2MF
forecasting to the most subsampled and the most abstract representation of a
desired single-frame model. Our implementations take advantage of deformable
convolutions and pairwise correlation coefficients across neighbouring time
instants. We perform experiments on three dense prediction tasks: semantic
segmentation, instance-level segmentation, and panoptic segmentation. The
results reveal state-of-the-art forecasting accuracy across all three
modalities on the Cityscapes dataset.
</p>
<a href="http://arxiv.org/abs/2101.10777" target="_blank">arXiv:2101.10777</a> [<a href="http://arxiv.org/pdf/2101.10777" target="_blank">pdf</a>]

<h2>Developing emotion recognition for video conference software to support people with autism. (arXiv:2101.10785v1 [cs.CV])</h2>
<h3>Marc Franzen, Michael Stephan Gresser, Tobias M&#xfc;ller, Prof. Dr. Sebastian Mauser</h3>
<p>We develop an emotion recognition software for the use with a video
conference software for autistic individuals which are unable to recognize
emotions properly. It can get an image out of the video stream, detect the
emotion in it with the help of a neural network and display the prediction to
the user. The network is trained on facial landmark features. The software is
fully modular to support adaption to different video conference software,
programming languages and implementations.
</p>
<a href="http://arxiv.org/abs/2101.10785" target="_blank">arXiv:2101.10785</a> [<a href="http://arxiv.org/pdf/2101.10785" target="_blank">pdf</a>]

<h2>The Consequences of the Framing of Machine Learning Risk Prediction Models: Evaluation of Sepsis in General Wards. (arXiv:2101.10790v1 [cs.LG])</h2>
<h3>Simon Meyer Lauritsen, Bo Thiesson, Marianne Johansson J&#xf8;rgensen, Anders Hammerich Riis, Ulrick Skipper Espelund, Jesper Bo Weile, Jeppe Lange</h3>
<p>Objectives: To evaluate the consequences of the framing of machine learning
risk prediction models. We evaluate how framing affects model performance and
model learning in four different approaches previously applied in published
artificial-intelligence (AI) models.

Setting and participants: We analysed structured secondary healthcare data
from 221,283 citizens from four Danish municipalities who were 18 years of age
or older.

Results: The four models had similar population level performance (a mean
area under the receiver operating characteristic curve of 0.73 to 0.82), in
contrast to the mean average precision, which varied greatly from 0.007 to
0.385. Correspondingly, the percentage of missing values also varied between
framing approaches. The on-clinical-demand framing, which involved samples for
each time the clinicians made an early warning score assessment, showed the
lowest percentage of missing values among the vital sign parameters, and this
model was also able to learn more temporal dependencies than the others. The
Shapley additive explanations demonstrated opposing interpretations of SpO2 in
the prediction of sepsis as a consequence of differentially framed models.

Conclusions: The profound consequences of framing mandate attention from
clinicians and AI developers, as the understanding and reporting of framing are
pivotal to the successful development and clinical implementation of future AI
technology. Model framing must reflect the expected clinical environment. The
importance of proper problem framing is by no means exclusive to sepsis
prediction and applies to most clinical risk prediction models.
</p>
<a href="http://arxiv.org/abs/2101.10790" target="_blank">arXiv:2101.10790</a> [<a href="http://arxiv.org/pdf/2101.10790" target="_blank">pdf</a>]

<h2>Adversarial Vulnerability of Active Transfer Learning. (arXiv:2101.10792v1 [cs.LG])</h2>
<h3>Nicolas M. M&#xfc;ller, Konstantin B&#xf6;ttinger</h3>
<p>Two widely used techniques for training supervised machine learning models on
small datasets are Active Learning and Transfer Learning. The former helps to
optimally use a limited budget to label new data. The latter uses large
pre-trained models as feature extractors and enables the design of complex,
non-linear models even on tiny datasets. Combining these two approaches is an
effective, state-of-the-art method when dealing with small datasets.

In this paper, we share an intriguing observation: Namely, that the
combination of these techniques is particularly susceptible to a new kind of
data poisoning attack: By adding small adversarial noise on the input, it is
possible to create a collision in the output space of the transfer learner. As
a result, Active Learning algorithms no longer select the optimal instances,
but almost exclusively the ones injected by the attacker. This allows an
attacker to manipulate the active learner to select and include arbitrary
images into the data set, even against an overwhelming majority of unpoisoned
samples. We show that a model trained on such a poisoned dataset has a
significantly deteriorated performance, dropping from 86\% to 34\% test
accuracy. We evaluate this attack on both audio and image datasets and support
our findings empirically. To the best of our knowledge, this weakness has not
been described before in literature.
</p>
<a href="http://arxiv.org/abs/2101.10792" target="_blank">arXiv:2101.10792</a> [<a href="http://arxiv.org/pdf/2101.10792" target="_blank">pdf</a>]

<h2>Global-Local Propagation Network for RGB-D Semantic Segmentation. (arXiv:2101.10801v1 [cs.CV])</h2>
<h3>Sihan Chen, Xinxin Zhu, Wei Liu, Xingjian He, Jing Liu</h3>
<p>Depth information matters in RGB-D semantic segmentation task for providing
additional geometric information to color images. Most existing methods exploit
a multi-stage fusion strategy to propagate depth feature to the RGB branch.
However, at the very deep stage, the propagation in a simple element-wise
addition manner can not fully utilize the depth information. We propose
Global-Local propagation network (GLPNet) to solve this problem. Specifically,
a local context fusion module(L-CFM) is introduced to dynamically align both
modalities before element-wise fusion, and a global context fusion
module(G-CFM) is introduced to propagate the depth information to the RGB
branch by jointly modeling the multi-modal global context features. Extensive
experiments demonstrate the effectiveness and complementarity of the proposed
fusion modules. Embedding two fusion modules into a two-stream encoder-decoder
structure, our GLPNet achieves new state-of-the-art performance on two
challenging indoor scene segmentation datasets, i.e., NYU-Depth v2 and SUN-RGBD
dataset.
</p>
<a href="http://arxiv.org/abs/2101.10801" target="_blank">arXiv:2101.10801</a> [<a href="http://arxiv.org/pdf/2101.10801" target="_blank">pdf</a>]

<h2>Automatic Curation of Large-Scale Datasets for Audio-Visual Representation Learning. (arXiv:2101.10803v1 [cs.CV])</h2>
<h3>Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, Yale Song</h3>
<p>Large-scale datasets are the cornerstone of self-supervised representation
learning. Existing algorithms extract learning signals by making certain
assumptions about the data, e.g., spatio-temporal continuity and multimodal
correspondence. Unfortunately, finding a large amount of data that satisfies
such assumptions is sometimes not straightforward. This restricts the community
to rely on datasets that require laborious annotation and/or manual filtering
processes. In this paper, we describe a subset optimization approach for
automatic dataset curation. Focusing on the scenario of audio-visual
representation learning, we pose the problem as finding a subset that maximizes
the mutual information between audio and visual channels in videos. We
demonstrate that our approach finds videos with high audio-visual
correspondence and show that self-supervised models trained on our data,
despite being automatically constructed, achieve similar downstream
performances to existing video datasets with similar scales. The most
significant benefit of our approach is scalability. We release the largest
video dataset for audio-visual research collected automatically using our
approach.
</p>
<a href="http://arxiv.org/abs/2101.10803" target="_blank">arXiv:2101.10803</a> [<a href="http://arxiv.org/pdf/2101.10803" target="_blank">pdf</a>]

<h2>CPTR: Full Transformer Network for Image Captioning. (arXiv:2101.10804v1 [cs.CV])</h2>
<h3>Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, Jing Liu</h3>
<p>In this paper, we consider the image captioning task from a new
sequence-to-sequence prediction perspective and propose Caption TransformeR
(CPTR) which takes the sequentialized raw images as the input to Transformer.
Compared to the "CNN+Transformer" design paradigm, our model can model global
context at every encoder layer from the beginning and is totally
convolution-free. Extensive experiments demonstrate the effectiveness of the
proposed model and we surpass the conventional "CNN+Transformer" methods on the
MSCOCO dataset. Besides, we provide detailed visualizations of the
self-attention between patches in the encoder and the "words-to-patches"
attention in the decoder thanks to the full Transformer architecture.
</p>
<a href="http://arxiv.org/abs/2101.10804" target="_blank">arXiv:2101.10804</a> [<a href="http://arxiv.org/pdf/2101.10804" target="_blank">pdf</a>]

<h2>Recovery of underdrawings and ghost-paintings via style transfer by deep convolutional neural networks: A digital tool for art scholars. (arXiv:2101.10807v1 [cs.CV])</h2>
<h3>Anthony Bourached, George Cann, Ryan-Rhys Griffiths, David G. Stork</h3>
<p>We describe the application of convolutional neural network style transfer to
the problem of improved visualization of underdrawings and ghost-paintings in
fine art oil paintings. Such underdrawings and hidden paintings are typically
revealed by x-ray or infrared techniques which yield images that are grayscale,
and thus devoid of color and full style information. Past methods for inferring
color in underdrawings have been based on physical x-ray fluorescence spectral
imaging of pigments in ghost-paintings and are thus expensive, time consuming,
and require equipment not available in most conservation studios. Our
algorithmic methods do not need such expensive physical imaging devices. Our
proof-of-concept system, applied to works by Pablo Picasso and Leonardo, reveal
colors and designs that respect the natural segmentation in the ghost-painting.
We believe the computed images provide insight into the artist and associated
oeuvre not available by other means. Our results strongly suggest that future
applications based on larger corpora of paintings for training will display
color schemes and designs that even more closely resemble works of the artist.
For these reasons refinements to our methods should find wide use in art
conservation, connoisseurship, and art analysis.
</p>
<a href="http://arxiv.org/abs/2101.10807" target="_blank">arXiv:2101.10807</a> [<a href="http://arxiv.org/pdf/2101.10807" target="_blank">pdf</a>]

<h2>Fast Facial Landmark Detection and Applications: A Survey. (arXiv:2101.10808v1 [cs.CV])</h2>
<h3>Kostiantyn Khabarlak, Larysa Koriashkina</h3>
<p>In this paper we survey and analyze modern neural-network-based facial
landmark detection algorithms. We focus on approaches that have led to a
significant increase in quality over the past few years on datasets with large
pose and emotion variability, high levels of face occlusions - all of which are
typical in real-world scenarios. We summarize the improvements into categories,
provide quality comparison on difficult and modern in-the-wild datasets: 300-W,
AFLW, WFLW, COFW. Additionally, we compare algorithm speed on CPU, GPU and
Mobile devices. For completeness, we also briefly touch on established methods
with open implementations available. Besides, we cover applications and
vulnerabilities of the landmark detection algorithms. Based on which, we raise
problems that as we hope will lead to further algorithm improvements in future.
</p>
<a href="http://arxiv.org/abs/2101.10808" target="_blank">arXiv:2101.10808</a> [<a href="http://arxiv.org/pdf/2101.10808" target="_blank">pdf</a>]

<h2>Exploring Transfer Learning on Face Recognition of Dark Skinned, Low Quality and Low Resource Face Data. (arXiv:2101.10809v1 [cs.CV])</h2>
<h3>Nuredin Ali</h3>
<p>There is a big difference in the tone of color of skin between dark and light
skinned people. Despite this fact, most face recognition tasks almost all
classical state-of-the-art models are trained on datasets containing an
overwhelming majority of light skinned face images. It is tedious to collect a
huge amount of data for dark skinned faces and train a model from scratch. In
this paper, we apply transfer learning on VGGFace to check how it works on
recognising dark skinned mainly Ethiopian faces. The dataset is of low quality
and low resource. Our experimental results show above 95\% accuracy which
indicates that transfer learning in such settings works.
</p>
<a href="http://arxiv.org/abs/2101.10809" target="_blank">arXiv:2101.10809</a> [<a href="http://arxiv.org/pdf/2101.10809" target="_blank">pdf</a>]

<h2>Semi-synthesis: A fast way to produce effective datasets for stereo matching. (arXiv:2101.10811v1 [cs.CV])</h2>
<h3>Ju He, Enyu Zhou, Liusheng Sun, Fei Lei, Chenyang Liu, Wenxiu Sun</h3>
<p>Stereo matching is an important problem in computer vision which has drawn
tremendous research attention for decades. Recent years, data-driven methods
with convolutional neural networks (CNNs) are continuously pushing stereo
matching to new heights. However, data-driven methods require large amount of
training data, which is not an easy task for real stereo data due to the
annotation difficulties of per-pixel ground-truth disparity. Though synthetic
dataset is proposed to fill the gaps of large data demand, the fine-tuning on
real dataset is still needed due to the domain variances between synthetic data
and real data. In this paper, we found that in synthetic datasets,
close-to-real-scene texture rendering is a key factor to boost up stereo
matching performance, while close-to-real-scene 3D modeling is less important.
We then propose semi-synthetic, an effective and fast way to synthesize large
amount of data with close-to-real-scene texture to minimize the gap between
synthetic data and real data. Extensive experiments demonstrate that models
trained with our proposed semi-synthetic datasets achieve significantly better
performance than with general synthetic datasets, especially on real data
benchmarks with limited training data. With further fine-tuning on the real
dataset, we also achieve SOTA performance on Middlebury and competitive results
on KITTI and ETH3D datasets.
</p>
<a href="http://arxiv.org/abs/2101.10811" target="_blank">arXiv:2101.10811</a> [<a href="http://arxiv.org/pdf/2101.10811" target="_blank">pdf</a>]

<h2>Impact of Explanation on Trust of a Novel Mobile Robot. (arXiv:2101.10813v1 [cs.RO])</h2>
<h3>Stephanie Rosenthal, Elizabeth J. Carter</h3>
<p>One challenge with introducing robots into novel environments is misalignment
between supervisor expectations and reality, which can greatly affect a user's
trust and continued use of the robot. We performed an experiment to test
whether the presence of an explanation of expected robot behavior affected a
supervisor's trust in an autonomous robot. We measured trust both subjectively
through surveys and objectively through a dual-task experiment design to
capture supervisors' neglect tolerance (i.e., their willingness to perform
their own task while the robot is acting autonomously). Our objective results
show that explanations can help counteract the novelty effect of seeing a new
robot perform in an unknown environment. Participants who received an
explanation of the robot's behavior were more likely to focus on their own task
at the risk of neglecting their robot supervision task during the first trials
of the robot's behavior compared to those who did not receive an explanation.
However, this effect diminished after seeing multiple trials, and participants
who received explanations were equally trusting of the robot's behavior as
those who did not receive explanations. Interestingly, participants were not
able to identify their own changes in trust through their survey responses,
demonstrating that the dual-task design measured subtler changes in a
supervisor's trust.
</p>
<a href="http://arxiv.org/abs/2101.10813" target="_blank">arXiv:2101.10813</a> [<a href="http://arxiv.org/pdf/2101.10813" target="_blank">pdf</a>]

<h2>When Would You Trust a Robot? A Study on Trust and Theory of Mind in Human-Robot Interactions. (arXiv:2101.10819v1 [cs.RO])</h2>
<h3>Wenxuan Mou, Martina Ruocco, Debora Zanatto, Angelo Cangelosi</h3>
<p>Trust is a critical issue in Human Robot Interactions as it is the core of
human desire to accept and use a non human agent. Theory of Mind has been
defined as the ability to understand the beliefs and intentions of others that
may differ from one's own. Evidences in psychology and HRI suggest that trust
and Theory of Mind are interconnected and interdependent concepts, as the
decision to trust another agent must depend on our own representation of this
entity's actions, beliefs and intentions. However, very few works take Theory
of Mind of the robot into consideration while studying trust in HRI. In this
paper, we investigated whether the exposure to the Theory of Mind abilities of
a robot could affect humans' trust towards the robot. To this end, participants
played a Price Game with a humanoid robot that was presented having either low
level Theory of Mind or high level Theory of Mind. Specifically, the
participants were asked to accept the price evaluations on common objects
presented by the robot. The willingness of the participants to change their own
price judgement of the objects (i.e., accept the price the robot suggested) was
used as the main measurement of the trust towards the robot. Our experimental
results showed that robots possessing a high level of Theory of Mind abilities
were trusted more than the robots presented with low level Theory of Mind
skills.
</p>
<a href="http://arxiv.org/abs/2101.10819" target="_blank">arXiv:2101.10819</a> [<a href="http://arxiv.org/pdf/2101.10819" target="_blank">pdf</a>]

<h2>Revisiting Locally Supervised Learning: an Alternative to End-to-end Training. (arXiv:2101.10832v1 [cs.CV])</h2>
<h3>Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, Gao Huang</h3>
<p>Due to the need to store the intermediate activations for back-propagation,
end-to-end (E2E) training of deep networks usually suffers from high GPUs
memory footprint. This paper aims to address this problem by revisiting the
locally supervised learning, where a network is split into gradient-isolated
modules and trained with local supervision. We experimentally show that simply
training local modules with E2E loss tends to collapse task-relevant
information at early layers, and hence hurts the performance of the full model.
To avoid this issue, we propose an information propagation (InfoPro) loss,
which encourages local modules to preserve as much useful information as
possible, while progressively discard task-irrelevant information. As InfoPro
loss is difficult to compute in its original form, we derive a feasible upper
bound as a surrogate optimization objective, yielding a simple but effective
algorithm. In fact, we show that the proposed method boils down to minimizing
the combination of a reconstruction loss and a normal cross-entropy/contrastive
term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10,
ImageNet and Cityscapes) validate that InfoPro is capable of achieving
competitive performance with less than 40% memory footprint compared to E2E
training, while allowing using training data with higher-resolution or larger
batch sizes under the same GPU memory constraint. Our method also enables
training local modules asynchronously for potential training acceleration. Code
is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.
</p>
<a href="http://arxiv.org/abs/2101.10832" target="_blank">arXiv:2101.10832</a> [<a href="http://arxiv.org/pdf/2101.10832" target="_blank">pdf</a>]

<h2>DataLoc+: A Data Augmentation Technique for Machine Learning in Room-Level Indoor Localization. (arXiv:2101.10833v1 [cs.CV])</h2>
<h3>Amr E Hilal, Ismail Arai, Samy El-Tawab</h3>
<p>Indoor localization has been a hot area of research over the past two
decades. Since its advent, it has been steadily utilizing the emerging
technologies to improve accuracy, and machine learning has been at the heart of
that. Machine learning has been increasingly used in fingerprint-based indoor
localization to replace or emulate the radio map that is used to predict
locations given a location signature. The prediction quality of a machine
learning model primarily depends on how well the model was trained, which
relies on the amount and quality of data used to train it. Data augmentation
has been used to improve quality of the trained models by synthetically
producing more training data, and several approaches were used in the
literature that tackles the problem of lack of training data from different
angles. In this paper, we propose DataLoc+, a data augmentation technique for
room-level indoor localization that combines different approaches in a simple
algorithm. We evaluate the technique by comparing it to the typical direct
snapshot approach using data collected from a field experiment conducted in a
hospital. Our evaluation shows that the model trained using the proposed
technique achieves higher accuracy. We also show that the technique adapts to
larger problems using a limited dataset while maintaining high accuracy.
</p>
<a href="http://arxiv.org/abs/2101.10833" target="_blank">arXiv:2101.10833</a> [<a href="http://arxiv.org/pdf/2101.10833" target="_blank">pdf</a>]

<h2>Ikshana: A Theory of Human Scene Understanding Mechanism. (arXiv:2101.10837v1 [cs.CV])</h2>
<h3>Venkata Satya Sai Ajay Daliparthi</h3>
<p>In recent years, deep neural networks achieved state-of-the-art performance
on many computer vision tasks. The two most commonly observed drawbacks of
these deep neural networks are: the requirement of a massive amount of labeled
data and a vast number of parameters. In this work, we propose a theory named
Ikshana, to explain the functioning of the human brain, while humans understand
a natural scene/image. We have designed an architecture named IkshanaNet and
evaluated on the Cityscapes pixel-level semantic segmentation benchmark, to
show how to implement our theory in practice. The results showed that the
Ikshana theory could perform with less training data. Also, through some
experiments evaluated on the validation set, we showed that the Ikshana theory
can significantly reduce the number of parameters of the network. In
conclusion, a deep neural network designed by following the Ikshana theory will
learn better vector representations of the image, useful for any computer
vision task.
</p>
<a href="http://arxiv.org/abs/2101.10837" target="_blank">arXiv:2101.10837</a> [<a href="http://arxiv.org/pdf/2101.10837" target="_blank">pdf</a>]

<h2>Three-Dimensional Investigation of the Metric Properties of Parabolic Double Projection Involving Catadioptric Camera. (arXiv:2101.10840v1 [cs.CV])</h2>
<h3>Ahmed Hamdy, Ahmed Elsherif, Saiid Shebl</h3>
<p>This paper presents an analytical study for the metric properties of the
paraboloidal double projection, i.e. central and orthogonal projections used in
the catadioptric camera system. Metric properties have not sufficiently studied
in previous treatments of such system. These properties incorporate the
determination of the true lengths of projected lines and areas bounded by
projected lines. The advantageous main gain of determining metric elements of
the paraboloidal double projection is studying distortion analysis and camera
calibration, which is considered an essential tool in testing camera accuracy.
Also, this may be considered as a significant utility in studying comparison
analysis between different cameras projection systems.
</p>
<a href="http://arxiv.org/abs/2101.10840" target="_blank">arXiv:2101.10840</a> [<a href="http://arxiv.org/pdf/2101.10840" target="_blank">pdf</a>]

<h2>Generative Adversarial Network using Perturbed-Convolutions. (arXiv:2101.10841v1 [cs.CV])</h2>
<h3>Seung Park, Yoon-Jae Yeo, Yong-Goo Shin</h3>
<p>Despite growing insights into the GAN training, it still suffers from
instability during the training procedure. To alleviate this problem, this
paper presents a novel convolutional layer, called perturbed-convolution
(PConv), which focuses on achieving two goals simultaneously: penalize the
discriminator for training GAN stably and prevent the overfitting problem in
the discriminator. PConv generates perturbed features by randomly disturbing an
input tensor before performing the convolution operation. This approach is
simple but surprisingly effective. First, to reliably classify real and
generated samples using the disturbed input tensor, the intermediate layers in
the discriminator should learn features having a small local Lipschitz value.
Second, due to the perturbed features in PConv, the discriminator is difficult
to memorize the real images; this makes the discriminator avoid the overfitting
problem. To show the generalization ability of the proposed method, we
conducted extensive experiments with various loss functions and datasets
including CIFAR-10, CelebA-HQ, LSUN, and tiny-ImageNet. Quantitative
evaluations demonstrate that WCL significantly improves the performance of GAN
and conditional GAN in terms of Frechet inception distance (FID). For instance,
the proposed method improves FID scores on the tiny-ImageNet dataset from 58.59
to 50.42.
</p>
<a href="http://arxiv.org/abs/2101.10841" target="_blank">arXiv:2101.10841</a> [<a href="http://arxiv.org/pdf/2101.10841" target="_blank">pdf</a>]

<h2>Source-free Domain Adaptation via Distributional Alignment by Matching Batch Normalization Statistics. (arXiv:2101.10842v1 [cs.CV])</h2>
<h3>Masato Ishii, Masashi Sugiyama</h3>
<p>In this paper, we propose a novel domain adaptation method for the
source-free setting. In this setting, we cannot access source data during
adaptation, while unlabeled target data and a model pretrained with source data
are given. Due to lack of source data, we cannot directly match the data
distributions between domains unlike typical domain adaptation algorithms. To
cope with this problem, we propose utilizing batch normalization statistics
stored in the pretrained model to approximate the distribution of unobserved
source data. Specifically, we fix the classifier part of the model during
adaptation and only fine-tune the remaining feature encoder part so that batch
normalization statistics of the features extracted by the encoder match those
stored in the fixed classifier. Additionally, we also maximize the mutual
information between the features and the classifier's outputs to further boost
the classification performance. Experimental results with several benchmark
datasets show that our method achieves competitive performance with
state-of-the-art domain adaptation methods even though it does not require
access to source data.
</p>
<a href="http://arxiv.org/abs/2101.10842" target="_blank">arXiv:2101.10842</a> [<a href="http://arxiv.org/pdf/2101.10842" target="_blank">pdf</a>]

<h2>Deep View Synthesis via Self-Consistent Generative Network. (arXiv:2101.10844v1 [cs.CV])</h2>
<h3>Zhuoman Liu, Wei Jia, Ming Yang, Peiyao Luo, Yong Guo, Mingkui Tan</h3>
<p>View synthesis aims to produce unseen views from a set of views captured by
two or more cameras at different positions. This task is non-trivial since it
is hard to conduct pixel-level matching among different views. To address this
issue, most existing methods seek to exploit the geometric information to match
pixels. However, when the distinct cameras have a large baseline (i.e., far
away from each other), severe geometry distortion issues would occur and the
geometric information may fail to provide useful guidance, resulting in very
blurry synthesized images. To address the above issues, in this paper, we
propose a novel deep generative model, called Self-Consistent Generative
Network (SCGN), which synthesizes novel views from the given input views
without explicitly exploiting the geometric information. The proposed SCGN
model consists of two main components, i.e., a View Synthesis Network (VSN) and
a View Decomposition Network (VDN), both employing an Encoder-Decoder
structure. Here, the VDN seeks to reconstruct input views from the synthesized
novel view to preserve the consistency of view synthesis. Thanks to VDN, SCGN
is able to synthesize novel views without using any geometric rectification
before encoding, making it easier for both training and applications. Finally,
adversarial loss is introduced to improve the photo-realism of novel views.
Both qualitative and quantitative comparisons against several state-of-the-art
methods on two benchmark tasks demonstrated the superiority of our approach.
</p>
<a href="http://arxiv.org/abs/2101.10844" target="_blank">arXiv:2101.10844</a> [<a href="http://arxiv.org/pdf/2101.10844" target="_blank">pdf</a>]

<h2>Analysis and evaluation of Deep Learning based Super-Resolution algorithms to improve performance in Low-Resolution Face Recognition. (arXiv:2101.10845v1 [cs.CV])</h2>
<h3>Angelo G. Menezes</h3>
<p>Surveillance scenarios are prone to several problems since they usually
involve low-resolution footage, and there is no control of how far the subjects
may be from the camera in the first place. This situation is suitable for the
application of upsampling (super-resolution) algorithms since they may be able
to recover the discriminant properties of the subjects involved. While general
super-resolution approaches were proposed to enhance image quality for
human-level perception, biometrics super-resolution methods seek the best
"computer perception" version of the image since their focus is on improving
automatic recognition performance. Convolutional neural networks and deep
learning algorithms, in general, have been applied to computer vision tasks and
are now state-of-the-art for several sub-domains, including image
classification, restoration, and super-resolution. However, no work has
evaluated the effects that the latest proposed super-resolution methods may
have upon the accuracy and face verification performance in low-resolution
"in-the-wild" data. This project aimed at evaluating and adapting different
deep neural network architectures for the task of face super-resolution driven
by face recognition performance in real-world low-resolution images. The
experimental results in a real-world surveillance and attendance datasets
showed that general super-resolution architectures might enhance face
verification performance of deep neural networks trained on high-resolution
faces. Also, since neural networks are function approximators and can be
trained based on specific objective functions, the use of a customized loss
function optimized for feature extraction showed promising results for
recovering discriminant features in low-resolution face images.
</p>
<a href="http://arxiv.org/abs/2101.10845" target="_blank">arXiv:2101.10845</a> [<a href="http://arxiv.org/pdf/2101.10845" target="_blank">pdf</a>]

<h2>Indoor Group Activity Recognition using Multi-Layered HMMs. (arXiv:2101.10857v1 [cs.CV])</h2>
<h3>Vinayak Elangovan</h3>
<p>Discovery and recognition of Group Activities (GA) based on imagery data
processing have significant applications in persistent surveillance systems,
which play an important role in some Internet services. The process is involved
with analysis of sequential imagery data with spatiotemporal associations.
Discretion of video imagery requires a proper inference system capable of
discriminating and differentiating cohesive observations and interlinking them
to known ontologies. We propose an Ontology based GAR with a proper inference
model that is capable of identifying and classifying a sequence of events in
group activities. A multi-layered Hidden Markov Model (HMM) is proposed to
recognize different levels of abstract GA. The multi-layered HMM consists of N
layers of HMMs where each layer comprises of M number of HMMs running in
parallel. The number of layers depends on the order of information to be
extracted. At each layer, by matching and correlating attributes of detected
group events, the model attempts to associate sensory observations to known
ontology perceptions. This paper demonstrates and compares performance of three
different implementation of HMM, namely, concatenated N-HMM, cascaded C-HMM and
hybrid H-HMM for building effective multi-layered HMM.
</p>
<a href="http://arxiv.org/abs/2101.10857" target="_blank">arXiv:2101.10857</a> [<a href="http://arxiv.org/pdf/2101.10857" target="_blank">pdf</a>]

<h2>A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v1 [cs.CV])</h2>
<h3>Lucas Prado Osco, Jos&#xe9; Marcato Junior, Ana Paula Marques Ramos, L&#xfa;cio Andr&#xe9; de Castro Jorge, Sarah Narges Fatholahi, Jonathan de Andrade Silva, Edson Takashi Matsubara, Hemerson Pistori, Wesley Nunes Gon&#xe7;alves, Jonathan Li</h3>
<p>Deep Neural Networks (DNNs) learn representation from data with an impressive
capability, and brought important breakthroughs for processing images,
time-series, natural language, audio, video, and many others. In the remote
sensing field, surveys and literature revisions specifically involving DNNs
algorithms' applications have been conducted in an attempt to summarize the
amount of information produced in its subfields. Recently, Unmanned Aerial
Vehicles (UAV) based applications have dominated aerial sensing research.
However, a literature revision that combines both "deep learning" and "UAV
remote sensing" thematics has not yet been conducted. The motivation for our
work was to present a comprehensive review of the fundamentals of Deep Learning
(DL) applied in UAV-based imagery. We focused mainly on describing
classification and regression techniques used in recent applications with
UAV-acquired data. For that, a total of 232 papers published in international
scientific journal databases was examined. We gathered the published material
and evaluated their characteristics regarding application, sensor, and
technique used. We relate how DL presents promising results and has the
potential for processing tasks associated with UAV-based image data. Lastly, we
project future perspectives, commentating on prominent DL paths to be explored
in the UAV remote sensing field. Our revision consists of a friendly-approach
to introduce, commentate, and summarize the state-of-the-art in UAV-based image
applications with DNNs algorithms in diverse subfields of remote sensing,
grouping it in the environmental, urban, and agricultural contexts.
</p>
<a href="http://arxiv.org/abs/2101.10861" target="_blank">arXiv:2101.10861</a> [<a href="http://arxiv.org/pdf/2101.10861" target="_blank">pdf</a>]

<h2>HANA: A HAndwritten NAme Database for Offline Handwritten Text Recognition. (arXiv:2101.10862v1 [cs.CV])</h2>
<h3>Christian M. Dahl, Torben Johansen, Emil N. S&#xf8;rensen, Simon Wittrock</h3>
<p>Methods for linking individuals across historical data sets, typically in
combination with AI based transcription models, are developing rapidly.
Probably the single most important identifier for linking is personal names.
However, personal names are prone to enumeration and transcription errors and
although modern linking methods are designed to handle such challenges these
sources of errors are critical and should be minimized. For this purpose,
improved transcription methods and large-scale databases are crucial
components. This paper describes and provides documentation for HANA, a newly
constructed large-scale database which consists of more than 1.1 million images
of handwritten word-groups. The database is a collection of personal names,
containing more than 105 thousand unique names with a total of more than 3.3
million examples. In addition, we present benchmark results for deep learning
models that automatically can transcribe the personal names from the scanned
documents. Focusing mainly on personal names, due to its vital role in linking,
we hope to foster more sophisticated, accurate, and robust models for
handwritten text recognition through making more challenging large-scale
databases publicly available. This paper describes the data source, the
collection process, and the image-processing procedures and methods that are
involved in extracting the handwritten personal names and handwritten text in
general from the forms.
</p>
<a href="http://arxiv.org/abs/2101.10862" target="_blank">arXiv:2101.10862</a> [<a href="http://arxiv.org/pdf/2101.10862" target="_blank">pdf</a>]

<h2>Online Body Schema Adaptation through Cost-Sensitive Active Learning. (arXiv:2101.10892v1 [cs.RO])</h2>
<h3>Gon&#xe7;alo Cunha, Pedro Vicente, Alexandre Bernardino, Ricardo Ribeiro, Pl&#xed;nio Moreno</h3>
<p>Humanoid robots have complex bodies and kinematic chains with several
Degrees-of-Freedom (DoF) which are difficult to model. Learning the parameters
of a kinematic model can be achieved by observing the position of the robot
links during prospective motions and minimising the prediction errors. This
work proposes a movement efficient approach for estimating online the
body-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)
parameters. A cost-sensitive active learning approach based on the A-Optimality
criterion is used to select optimal joint configurations. The chosen joint
configurations simultaneously minimise the error in the estimation of the body
schema and minimise the movement between samples. This reduces energy
consumption, along with mechanical fatigue and wear, while not compromising the
learning accuracy. The work was implemented in a simulation environment, using
the 7DoF arm of the iCub robot simulator. The hand pose is measured with a
single camera via markers placed in the palm and back of the robot's hand. A
non-parametric occlusion model is proposed to avoid choosing joint
configurations where the markers are not visible, thus preventing worthless
attempts. The results show cost-sensitive active learning has similar accuracy
to the standard active learning approach, while reducing in about half the
executed movement.
</p>
<a href="http://arxiv.org/abs/2101.10892" target="_blank">arXiv:2101.10892</a> [<a href="http://arxiv.org/pdf/2101.10892" target="_blank">pdf</a>]

<h2>White Paper: Challenges and Considerations for the Creation of a Large Labelled Repository of Online Videos with Questionable Content. (arXiv:2101.10894v1 [cs.CV])</h2>
<h3>Thamar Solorio, Mahsa Shafaei, Christos Smailis, Mona Diab, Theodore Giannakopoulos, Heng Ji, Yang Liu, Rada Mihalcea, Smaranda Muresan, Ioannis Kakadiaris</h3>
<p>This white paper presents a summary of the discussions regarding critical
considerations to develop an extensive repository of online videos annotated
with labels indicating questionable content. The main discussion points
include: 1) the type of appropriate labels that will result in a valuable
repository for the larger AI community; 2) how to design the collection and
annotation process, as well as the distribution of the corpus to maximize its
potential impact; and, 3) what actions we can take to reduce risk of trauma to
annotators.
</p>
<a href="http://arxiv.org/abs/2101.10894" target="_blank">arXiv:2101.10894</a> [<a href="http://arxiv.org/pdf/2101.10894" target="_blank">pdf</a>]

<h2>HexCNN: A Framework for Native Hexagonal Convolutional Neural Networks. (arXiv:2101.10897v1 [cs.CV])</h2>
<h3>Yunxiang Zhao, Qiuhong Ke, Flip Korn, Jianzhong Qi, Rui Zhang</h3>
<p>Hexagonal CNN models have shown superior performance in applications such as
IACT data analysis and aerial scene classification due to their better rotation
symmetry and reduced anisotropy. In order to realize hexagonal processing,
existing studies mainly use the ZeroOut method to imitate hexagonal processing,
which causes substantial memory and computation overheads. We address this
deficiency with a novel native hexagonal CNN framework named HexCNN. HexCNN
takes hexagon-shaped input and performs forward and backward propagation on the
original form of the input based on hexagon-shaped filters, hence avoiding
computation and memory overheads caused by imitation. For applications with
rectangle-shaped input but require hexagonal processing, HexCNN can be applied
by padding the input into hexagon-shape as preprocessing. In this case, we show
that the time and space efficiency of HexCNN still outperforms existing
hexagonal CNN methods substantially. Experimental results show that compared
with the state-of-the-art models, which imitate hexagonal processing but using
rectangle-shaped filters, HexCNN reduces the training time by up to 42.2%.
Meanwhile, HexCNN saves the memory space cost by up to 25% and 41.7% for
loading the input and performing convolution, respectively.
</p>
<a href="http://arxiv.org/abs/2101.10897" target="_blank">arXiv:2101.10897</a> [<a href="http://arxiv.org/pdf/2101.10897" target="_blank">pdf</a>]

<h2>Nondiscriminatory Treatment: a straightforward framework for multi-human parsing. (arXiv:2101.10913v1 [cs.CV])</h2>
<h3>Min Yan, Guoshan Zhang, Tong Zhang, Yueming Zhang</h3>
<p>Multi-human parsing aims to segment every body part of every human instance.
Nearly all state-of-the-art methods follow the "detection first" or
"segmentation first" pipelines. Different from them, we present an end-to-end
and box-free pipeline from a new and more human-intuitive perspective. In
training time, we directly do instance segmentation on humans and parts. More
specifically, we introduce a notion of "indiscriminate objects with categorie"
which treats humans and parts without distinction and regards them both as
instances with categories. In the mask prediction, each binary mask is obtained
by a combination of prototypes shared among all human and part categories. In
inference time, we design a brand-new grouping post-processing method that
relates each part instance with one single human instance and groups them
together to obtain the final human-level parsing result. We name our method as
Nondiscriminatory Treatment between Humans and Parts for Human Parsing (NTHP).
Experiments show that our network performs superiorly against state-of-the-art
methods by a large margin on the MHP v2.0 and PASCAL-Person-Part datasets.
</p>
<a href="http://arxiv.org/abs/2101.10913" target="_blank">arXiv:2101.10913</a> [<a href="http://arxiv.org/pdf/2101.10913" target="_blank">pdf</a>]

<h2>Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms. (arXiv:2101.10943v1 [stat.ML])</h2>
<h3>Alicia Curth, Mihaela van der Schaar</h3>
<p>The need to evaluate treatment effectiveness is ubiquitous in most of
empirical science, and interest in flexibly investigating effect heterogeneity
is growing rapidly. To do so, a multitude of model-agnostic, nonparametric
meta-learners have been proposed in recent years. Such learners decompose the
treatment effect estimation problem into separate sub-problems, each solvable
using standard supervised learning methods. Choosing between different
meta-learners in a data-driven manner is difficult, as it requires access to
counterfactual information. Therefore, with the ultimate goal of building
better understanding of the conditions under which some learners can be
expected to perform better than others a priori, we theoretically analyze four
broad meta-learning strategies which rely on plug-in estimation and
pseudo-outcome regression. We highlight how this theoretical reasoning can be
used to guide principled algorithm design and translate our analyses into
practice by considering a variety of neural network architectures as
base-learners for the discussed meta-learning strategies. In a simulation
study, we showcase the relative strengths of the learners under different
data-generating processes.
</p>
<a href="http://arxiv.org/abs/2101.10943" target="_blank">arXiv:2101.10943</a> [<a href="http://arxiv.org/pdf/2101.10943" target="_blank">pdf</a>]

<h2>Asymptotic Supervised Predictive Classifiers under Partition Exchangeability. (arXiv:2101.10950v1 [stat.ML])</h2>
<h3>Ali Amiryousefi</h3>
<p>The convergence of simultaneous and marginal predictive classifiers under
partition exchangeability in supervised classification is obtained. The result
shows the asymptotic convergence of these classifiers under infinite amount of
training or test data, such that after observing umpteen amount of data, the
differences between these classifiers would be negligible. This is an important
result from the practical perspective as under the presence of sufficiently
large amount of data, one can replace the simpler marginal classifier with
computationally more expensive simultaneous one.
</p>
<a href="http://arxiv.org/abs/2101.10950" target="_blank">arXiv:2101.10950</a> [<a href="http://arxiv.org/pdf/2101.10950" target="_blank">pdf</a>]

<h2>Incremental Search Space Construction for Machine Learning Pipeline Synthesis. (arXiv:2101.10951v1 [cs.LG])</h2>
<h3>Marc-Andr&#xe9; Z&#xf6;ller, Tien-Dung Nguyen, Marco F. Huber</h3>
<p>Automated machine learning (AutoML) aims for constructing machine learning
(ML) pipelines automatically. Many studies have investigated efficient methods
for algorithm selection and hyperparameter optimization. However, methods for
ML pipeline synthesis and optimization considering the impact of complex
pipeline structures containing multiple preprocessing and classification
algorithms have not been studied thoroughly. In this paper, we propose a
data-centric approach based on meta-features for pipeline construction and
hyperparameter optimization inspired by human behavior. By expanding the
pipeline search space incrementally in combination with meta-features of
intermediate data sets, we are able to prune the pipeline structure search
space efficiently. Consequently, flexible and data set specific ML pipelines
can be constructed. We prove the effectiveness and competitiveness of our
approach on 28 data sets used in well-established AutoML benchmarks in
comparison with state-of-the-art AutoML frameworks.
</p>
<a href="http://arxiv.org/abs/2101.10951" target="_blank">arXiv:2101.10951</a> [<a href="http://arxiv.org/pdf/2101.10951" target="_blank">pdf</a>]

<h2>RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content. (arXiv:2101.10955v1 [cs.CV])</h2>
<h3>Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, Alan C. Bovik</h3>
<p>Blind or no-reference video quality assessment of user-generated content
(UGC) has become a trending, challenging, unsolved problem. Accurate and
efficient video quality predictors suitable for this content are thus in great
demand to achieve more intelligent analysis and processing of UGC videos.
Previous studies have shown that natural scene statistics and deep learning
features are both sufficient to capture spatial distortions, which contribute
to a significant aspect of UGC video quality issues. However, these models are
either incapable or inefficient for predicting the quality of complex and
diverse UGC videos in practical applications. Here we introduce an effective
and efficient video quality model for UGC content, which we dub the Rapid and
Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably
to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.
RAPIQUE combines and leverages the advantages of both quality-aware scene
statistics features and semantics-aware deep convolutional features, allowing
us to design the first general and efficient spatial and temporal (space-time)
bandpass statistics model for video quality modeling. Our experimental results
on recent large-scale UGC video quality databases show that RAPIQUE delivers
top performances on all the datasets at a considerably lower computational
expense. We hope this work promotes and inspires further efforts towards
practical modeling of video quality problems for potential real-time and
low-latency applications. To promote public usage, an implementation of RAPIQUE
has been made freely available online: \url{https://github.com/vztu/RAPIQUE}.
</p>
<a href="http://arxiv.org/abs/2101.10955" target="_blank">arXiv:2101.10955</a> [<a href="http://arxiv.org/pdf/2101.10955" target="_blank">pdf</a>]

<h2>Investment vs. reward in a competitive knapsack problem. (arXiv:2101.10964v1 [cs.AI])</h2>
<h3>Oren Neumann, Claudius Gros</h3>
<p>Natural selection drives species to develop brains, with sizes that increase
with the complexity of the tasks to be tackled. Our goal is to investigate the
balance between the metabolic costs of larger brains compared to the advantage
they provide in solving general and combinatorial problems. Defining advantage
as the performance relative to competitors, a two-player game based on the
knapsack problem is used. Within this framework, two opponents compete over
shared resources, with the goal of collecting more resources than the opponent.
Neural nets of varying sizes are trained using a variant of the AlphaGo Zero
algorithm. A surprisingly simple relation, $N_A/(N_A+N_B)$, is found for the
relative win rate of a net with $N_A$ neurons against one with $N_B$. Success
increases linearly with investments in additional resources when the networks
sizes are very different, i.e. when $N_A \ll N_B$, with returns diminishing
when both networks become comparable in size.
</p>
<a href="http://arxiv.org/abs/2101.10964" target="_blank">arXiv:2101.10964</a> [<a href="http://arxiv.org/pdf/2101.10964" target="_blank">pdf</a>]

<h2>Evaluating Input Perturbation Methods for Interpreting CNNs and Saliency Map Comparison. (arXiv:2101.10977v1 [cs.LG])</h2>
<h3>Lukas Brunke, Prateek Agrawal, Nikhil George</h3>
<p>Input perturbation methods occlude parts of an input to a function and
measure the change in the function's output. Recently, input perturbation
methods have been applied to generate and evaluate saliency maps from
convolutional neural networks. In practice, neutral baseline images are used
for the occlusion, such that the baseline image's impact on the classification
probability is minimal. However, in this paper we show that arguably neutral
baseline images still impact the generated saliency maps and their evaluation
with input perturbations. We also demonstrate that many choices of
hyperparameters lead to the divergence of saliency maps generated by input
perturbations. We experimentally reveal inconsistencies among a selection of
input perturbation methods and find that they lack robustness for generating
saliency maps and for evaluating saliency maps as saliency metrics.
</p>
<a href="http://arxiv.org/abs/2101.10977" target="_blank">arXiv:2101.10977</a> [<a href="http://arxiv.org/pdf/2101.10977" target="_blank">pdf</a>]

<h2>Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation. (arXiv:2101.10979v1 [cs.CV])</h2>
<h3>Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen</h3>
<p>Self-training is a competitive approach in domain adaptive segmentation,
which trains the network with the pseudo labels on the target domain. However
inevitably, the pseudo labels are noisy and the target features are dispersed
due to the discrepancy between source and target domains. In this paper, we
rely on representative prototypes, the feature centroids of classes, to address
the two issues for unsupervised domain adaptation. In particular, we take one
step further and exploit the feature distances from prototypes that provide
richer information than mere prototypes. Specifically, we use it to estimate
the likelihood of pseudo labels to facilitate online correction in the course
of training. Meanwhile, we align the prototypical assignments based on relative
feature distances for two different views of the same target, producing a more
compact target feature space. Moreover, we find that distilling the already
learned knowledge to a self-supervised pretrained model further boosts the
performance. Our method shows tremendous performance advantage over
state-of-the-art methods. We will make the code publicly available.
</p>
<a href="http://arxiv.org/abs/2101.10979" target="_blank">arXiv:2101.10979</a> [<a href="http://arxiv.org/pdf/2101.10979" target="_blank">pdf</a>]

<h2>Unsupervised clustering of series using dynamic programming and neural processes. (arXiv:2101.10983v1 [cs.LG])</h2>
<h3>Karthigan Sinnathamby, Chang-Yu Hou, Lalitha Venkataramanan, Vasileios-Marios Gkortsas, Fran&#xe7;ois Fleuret</h3>
<p>Following the work of arXiv:2101.09512, we are interested in clustering a
given multi-variate series in an unsupervised manner. We would like to segment
and cluster the series such that the resulting blocks present in each cluster
are coherent with respect to a predefined model structure (e.g. a physics model
with a functional form defined by a number of parameters). However, such
approach might have its limitation, partly because there may exist multiple
models that describe the same data, and partly because the exact model behind
the data may not immediately known. Hence, it is useful to establish a general
framework that enables the integration of plausible models and also
accommodates data-driven approach into one approximated model to assist the
clustering task. Hence, in this work, we investigate the use of neural
processes to build the approximated model while yielding the same assumptions
required by the algorithm presented in arXiv:2101.09512.
</p>
<a href="http://arxiv.org/abs/2101.10983" target="_blank">arXiv:2101.10983</a> [<a href="http://arxiv.org/pdf/2101.10983" target="_blank">pdf</a>]

<h2>Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. (arXiv:2101.10994v1 [cs.CV])</h2>
<h3>Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler</h3>
<p>Neural signed distance functions (SDFs) are emerging as an effective
representation for 3D shapes. State-of-the-art methods typically encode the SDF
with a large, fixed-size neural network to approximate complex shapes with
implicit surfaces. Rendering with these large networks is, however,
computationally expensive since it requires many forward passes through the
network for every pixel, making these representations impractical for real-time
graphics. We introduce an efficient neural representation that, for the first
time, enables real-time rendering of high-fidelity neural SDFs, while achieving
state-of-the-art geometry reconstruction quality. We represent implicit
surfaces using an octree-based feature volume which adaptively fits shapes with
multiple discrete levels of detail (LODs), and enables continuous LOD with SDF
interpolation. We further develop an efficient algorithm to directly render our
novel neural SDF representation in real-time by querying only the necessary
LODs with sparse octree traversal. We show that our representation is 2-3
orders of magnitude more efficient in terms of rendering speed compared to
previous works. Furthermore, it produces state-of-the-art reconstruction
quality for complex shapes under both 3D geometric and 2D image-space metrics.
</p>
<a href="http://arxiv.org/abs/2101.10994" target="_blank">arXiv:2101.10994</a> [<a href="http://arxiv.org/pdf/2101.10994" target="_blank">pdf</a>]

<h2>Deep Burst Super-Resolution. (arXiv:2101.10997v1 [cs.CV])</h2>
<h3>Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte</h3>
<p>While single-image super-resolution (SISR) has attracted substantial interest
in recent years, the proposed approaches are limited to learning image priors
in order to add high frequency details. In contrast, multi-frame
super-resolution (MFSR) offers the possibility of reconstructing rich details
by combining signal information from multiple shifted images. This key
advantage, along with the increasing popularity of burst photography, have made
MFSR an important problem for real-world applications.

We propose a novel architecture for the burst super-resolution task. Our
network takes multiple noisy RAW images as input, and generates a denoised,
super-resolved RGB image as output. This is achieved by explicitly aligning
deep embeddings of the input frames using pixel-wise optical flow. The
information from all frames are then adaptively merged using an attention-based
fusion module. In order to enable training and evaluation on real-world data,
we additionally introduce the BurstSR dataset, consisting of smartphone bursts
and high-resolution DSLR ground-truth. We perform comprehensive experimental
analysis, demonstrating the effectiveness of the proposed architecture.
</p>
<a href="http://arxiv.org/abs/2101.10997" target="_blank">arXiv:2101.10997</a> [<a href="http://arxiv.org/pdf/2101.10997" target="_blank">pdf</a>]

<h2>SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with Drug Combinations and Heterogeneous Patient Groups. (arXiv:2101.10998v1 [cs.LG])</h2>
<h3>Hyun-Suk Lee, Cong Shen, William Zame, Jang-Won Lee, Mihaela van der Schaar</h3>
<p>Phase I clinical trials are designed to test the safety (non-toxicity) of
drugs and find the maximum tolerated dose (MTD). This task becomes
significantly more challenging when multiple-drug dose-combinations (DC) are
involved, due to the inherent conflict between the exponentially increasing DC
candidates and the limited patient budget. This paper proposes a novel Bayesian
design, SDF-Bayes, for finding the MTD for drug combinations in the presence of
safety constraints. Rather than the conventional principle of escalating or
de-escalating the current dose of one drug (perhaps alternating between drugs),
SDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the
basis of current information, is most likely to be the MTD (optimism), subject
to the constraint that it only chooses DCs that have a high probability of
being safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts
for patient heterogeneity and enables heterogeneous patient recruitment.
Extensive experiments based on both synthetic and real-world datasets
demonstrate the advantages of SDF-Bayes over state of the art DC trial designs
in terms of accuracy and safety.
</p>
<a href="http://arxiv.org/abs/2101.10998" target="_blank">arXiv:2101.10998</a> [<a href="http://arxiv.org/pdf/2101.10998" target="_blank">pdf</a>]

<h2>Population-Guided Large Margin Classifier for High-Dimension Low -Sample-Size Problems. (arXiv:1901.01377v2 [cs.LG] UPDATED)</h2>
<h3>Qingbo Yin, Ehsan Adeli, Liran Shen, Dinggang Shen</h3>
<p>Various applications in different fields, such as gene expression analysis or
computer vision, suffer from data sets with high-dimensional low-sample-size
(HDLSS), which has posed significant challenges for standard statistical and
modern machine learning methods. In this paper, we propose a novel linear
binary classifier, denoted by population-guided large margin classifier
(PGLMC), which is applicable to any sorts of data, including HDLSS. PGLMC is
conceived with a projecting direction w given by the comprehensive
consideration of local structural information of the hyperplane and the
statistics of the training samples. Our proposed model has several advantages
compared to those widely used approaches. First, it is not sensitive to the
intercept term b. Second, it operates well with imbalanced data. Third, it is
relatively simple to be implemented based on Quadratic Programming. Fourth, it
is robust to the model specification for various real applications. The
theoretical properties of PGLMC are proven. We conduct a series of evaluations
on two simulated and six real-world benchmark data sets, including DNA
classification, digit recognition, medical image analysis, and face
recognition. PGLMC outperforms the state-of-the-art classification methods in
most cases, or at least obtains comparable results.
</p>
<a href="http://arxiv.org/abs/1901.01377" target="_blank">arXiv:1901.01377</a> [<a href="http://arxiv.org/pdf/1901.01377" target="_blank">pdf</a>]

<h2>Benchmark and Survey of Automated Machine Learning Frameworks. (arXiv:1904.12054v5 [cs.LG] UPDATED)</h2>
<h3>Marc-Andr&#xe9; Z&#xf6;ller, Marco F. Huber</h3>
<p>Machine learning (ML) has become a vital part in many aspects of our daily
life. However, building well performing machine learning applications requires
highly specialized data scientists and domain experts. Automated machine
learning (AutoML) aims to reduce the demand for data scientists by enabling
domain experts to build machine learning applications automatically without
extensive knowledge of statistics and machine learning. This paper is a
combination of a survey on current AutoML methods and a benchmark of popular
AutoML frameworks on real data sets. Driven by the selected frameworks for
evaluation, we summarize and review important AutoML techniques and methods
concerning every step in building an ML pipeline. The selected AutoML
frameworks are evaluated on 137 data sets from established AutoML benchmark
suits.
</p>
<a href="http://arxiv.org/abs/1904.12054" target="_blank">arXiv:1904.12054</a> [<a href="http://arxiv.org/pdf/1904.12054" target="_blank">pdf</a>]

<h2>Quantifying the Alignment of Graph and Features in Deep Learning. (arXiv:1905.12921v3 [cs.LG] UPDATED)</h2>
<h3>Yifan Qian, Paul Expert, Tom Rieu, Pietro Panzarasa, Mauricio Barahona</h3>
<p>We show that the classification performance of graph convolutional networks
(GCNs) is related to the alignment between features, graph, and ground truth,
which we quantify using a subspace alignment measure (SAM) corresponding to the
Frobenius norm of the matrix of pairwise chordal distances between three
subspaces associated with features, graph, and ground truth. The proposed
measure is based on the principal angles between subspaces and has both
spectral and geometrical interpretations. We showcase the relationship between
the SAM and the classification performance through the study of limiting cases
of GCNs and systematic randomizations of both features and graph structure
applied to a constructive example and several examples of citation networks of
different origins. The analysis also reveals the relative importance of the
graph and features for classification purposes.
</p>
<a href="http://arxiv.org/abs/1905.12921" target="_blank">arXiv:1905.12921</a> [<a href="http://arxiv.org/pdf/1905.12921" target="_blank">pdf</a>]

<h2>Support vector machines on the D-Wave quantum annealer. (arXiv:1906.06283v3 [cs.LG] UPDATED)</h2>
<h3>Dennis Willsch, Madita Willsch, Hans De Raedt, Kristel Michielsen</h3>
<p>Kernel-based support vector machines (SVMs) are supervised machine learning
algorithms for classification and regression problems. We introduce a method to
train SVMs on a D-Wave 2000Q quantum annealer and study its performance in
comparison to SVMs trained on conventional computers. The method is applied to
both synthetic data and real data obtained from biology experiments. We find
that the quantum annealer produces an ensemble of different solutions that
often generalizes better to unseen data than the single global minimum of an
SVM trained on a conventional computer, especially in cases where only limited
training data is available. For cases with more training data than currently
fits on the quantum annealer, we show that a combination of classifiers for
subsets of the data almost always produces stronger joint classifiers than the
conventional SVM for the same parameters.
</p>
<a href="http://arxiv.org/abs/1906.06283" target="_blank">arXiv:1906.06283</a> [<a href="http://arxiv.org/pdf/1906.06283" target="_blank">pdf</a>]

<h2>Identifiability of Hierarchical Latent Attribute Models. (arXiv:1906.07869v3 [stat.ML] UPDATED)</h2>
<h3>Yuqi Gu, Gongjun Xu</h3>
<p>Hierarchical Latent Attribute Models (HLAMs) are a family of discrete latent
variable models that are attracting increasing attention in educational,
psychological, and behavioral sciences. The key ingredients of an HLAM include
a binary structural matrix and a directed acyclic graph specifying hierarchical
constraints on the configurations of latent attributes. These components encode
practitioners' design information and carry important scientific meanings.
Despite the popularity of HLAMs, the fundamental identifiability issue remains
unaddressed. The existence of the attribute hierarchy graph leads to degenerate
parameter space, and the potentially unknown structural matrix further
complicates the identifiability problem. This paper addresses this issue of
identifying the latent structure and model parameters underlying an HLAM. We
develop sufficient and necessary identifiability conditions. These results
directly and sharply characterize the different impacts on identifiability cast
by different attribute types in the graph. The proposed conditions not only
provide insights into diagnostic test designs under the attribute hierarchy,
but also serve as tools to assess the validity of an estimated HLAM.
</p>
<a href="http://arxiv.org/abs/1906.07869" target="_blank">arXiv:1906.07869</a> [<a href="http://arxiv.org/pdf/1906.07869" target="_blank">pdf</a>]

<h2>Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems. (arXiv:1911.01916v4 [cs.LG] UPDATED)</h2>
<h3>Xuezhi Wang, Nithum Thain, Anu Sinha, Flavien Prost, Ed H. Chi, Jilin Chen, Alex Beutel</h3>
<p>How can we build recommender systems to take into account fairness?
Real-world recommender systems are often composed of multiple models, built by
multiple teams. However, most research on fairness focuses on improving
fairness in a single model. Further, recent research on classification fairness
has shown that combining multiple "fair" classifiers can still result in an
"unfair" classification system. This presents a significant challenge: how do
we understand and improve fairness in recommender systems composed of multiple
components?

In this paper, we study the compositionality of recommender fairness. We
consider two recently proposed fairness ranking metrics: equality of exposure
and pairwise ranking accuracy. While we show that fairness in recommendation is
not guaranteed to compose, we provide theory for a set of conditions under
which fairness of individual models does compose. We then present an analytical
framework for both understanding whether a real system's signals can achieve
compositional fairness, and improving which component would have the greatest
impact on the fairness of the overall system. In addition to the theoretical
results, we find on multiple datasets -- including a large-scale real-world
recommender system -- that the overall system's end-to-end fairness is largely
achievable by improving fairness in individual components.
</p>
<a href="http://arxiv.org/abs/1911.01916" target="_blank">arXiv:1911.01916</a> [<a href="http://arxiv.org/pdf/1911.01916" target="_blank">pdf</a>]

<h2>Deep Learning for Visual Tracking: A Comprehensive Survey. (arXiv:1912.00535v2 [cs.CV] UPDATED)</h2>
<h3>Seyed Mojtaba Marvasti-Zadeh, Li Cheng, Hossein Ghanei-Yakhdan, Shohreh Kasaei</h3>
<p>Visual target tracking is one of the most sought-after yet challenging
research topics in computer vision. Given the ill-posed nature of the problem
and its popularity in a broad range of real-world scenarios, a number of
large-scale benchmark datasets have been established, on which considerable
methods have been developed and demonstrated with significant progress in
recent years -- predominantly by recent deep learning (DL)-based methods. This
survey aims to systematically investigate the current DL-based visual tracking
methods, benchmark datasets, and evaluation metrics. It also extensively
evaluates and analyzes the leading visual tracking methods. First, the
fundamental characteristics, primary motivations, and contributions of DL-based
methods are summarized from nine key aspects of: network architecture, network
exploitation, network training for visual tracking, network objective, network
output, exploitation of correlation filter advantages, aerial-view tracking,
long-term tracking, and online tracking. Second, popular visual tracking
benchmarks and their respective properties are compared, and their evaluation
metrics are summarized. Third, the state-of-the-art DL-based methods are
comprehensively examined on a set of well-established benchmarks of OTB2013,
OTB2015, VOT2018, LaSOT, UAV123, UAVDT, and VisDrone2019. Finally, by
conducting critical analyses of these state-of-the-art trackers quantitatively
and qualitatively, their pros and cons under various common scenarios are
investigated. It may serve as a gentle use guide for practitioners to weigh
when and under what conditions to choose which method(s). It also facilitates a
discussion on ongoing issues and sheds light on promising research directions.
</p>
<a href="http://arxiv.org/abs/1912.00535" target="_blank">arXiv:1912.00535</a> [<a href="http://arxiv.org/pdf/1912.00535" target="_blank">pdf</a>]

<h2>Learning Individually Fair Classifier with Path-Specific Causal-Effect Constraint. (arXiv:2002.06746v3 [cs.LG] UPDATED)</h2>
<h3>Yoichi Chikahara, Shinsaku Sakaue, Akinori Fujino, Hisashi Kashima</h3>
<p>Machine learning is used to make decisions for individuals in various fields,
which require us to achieve good prediction accuracy while ensuring fairness
with respect to sensitive features (e.g., race and gender). This problem,
however, remains difficult in complex real-world scenarios. To quantify
unfairness under such situations, existing methods utilize {\it path-specific
causal effects}. However, none of them can ensure fairness for each individual
without making impractical functional assumptions on the data. In this paper,
we propose a far more practical framework for learning an individually fair
classifier. To avoid restrictive functional assumptions, we define the {\it
probability of individual unfairness} (PIU) and solve an optimization problem
where PIU's upper bound, which can be estimated from data, is controlled to be
close to zero. We elucidate why our method can guarantee fairness for each
individual. Experimental results show that our method can learn an individually
fair classifier at a slight cost of accuracy.
</p>
<a href="http://arxiv.org/abs/2002.06746" target="_blank">arXiv:2002.06746</a> [<a href="http://arxiv.org/pdf/2002.06746" target="_blank">pdf</a>]

<h2>Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor. (arXiv:2002.08958v3 [cs.LG] UPDATED)</h2>
<h3>Mher Safaryan, Egor Shulgin, Peter Richt&#xe1;rik</h3>
<p>In order to mitigate the high communication cost in distributed and federated
learning, various vector compression schemes, such as quantization,
sparsification and dithering, have become very popular. In designing a
compression method, one aims to communicate as few bits as possible, which
minimizes the cost per communication round, while at the same time attempting
to impart as little distortion (variance) to the communicated messages as
possible, which minimizes the adverse effect of the compression on the overall
number of communication rounds. However, intuitively, these two goals are
fundamentally in conflict: the more compression we allow, the more distorted
the messages become. We formalize this intuition and prove an {\em uncertainty
principle} for randomized compression operators, thus quantifying this
limitation mathematically, and {\em effectively providing asymptotically tight
lower bounds on what might be achievable with communication compression}.
Motivated by these developments, we call for the search for the optimal
compression operator. In an attempt to take a first step in this direction, we
consider an unbiased compression method inspired by the Kashin representation
of vectors, which we call {\em Kashin compression (KC)}. In contrast to all
previously proposed compression mechanisms, KC enjoys a {\em dimension
independent} variance bound for which we derive an explicit formula even in the
regime when only a few bits need to be communicate per each vector entry.
</p>
<a href="http://arxiv.org/abs/2002.08958" target="_blank">arXiv:2002.08958</a> [<a href="http://arxiv.org/pdf/2002.08958" target="_blank">pdf</a>]

<h2>Anatomy-aware 3D Human Pose Estimation with Bone-based Pose Decomposition. (arXiv:2002.10322v5 [cs.CV] UPDATED)</h2>
<h3>Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, Jiebo Luo</h3>
<p>In this work, we propose a new solution to 3D human pose estimation in
videos. Instead of directly regressing the 3D joint locations, we draw
inspiration from the human skeleton anatomy and decompose the task into bone
direction prediction and bone length prediction, from which the 3D joint
locations can be completely derived. Our motivation is the fact that the bone
lengths of a human skeleton remain consistent across time. This promotes us to
develop effective techniques to utilize global information across all the
frames in a video for high-accuracy bone length prediction. Moreover, for the
bone direction prediction network, we propose a fully-convolutional propagating
architecture with long skip connections. Essentially, it predicts the
directions of different bones hierarchically without using any time-consuming
memory units e.g. LSTM). A novel joint shift loss is further introduced to
bridge the training of the bone length and bone direction prediction networks.
Finally, we employ an implicit attention mechanism to feed the 2D keypoint
visibility scores into the model as extra guidance, which significantly
mitigates the depth ambiguity in many challenging poses. Our full model
outperforms the previous best results on Human3.6M and MPI-INF-3DHP datasets,
where comprehensive evaluation validates the effectiveness of our model.
</p>
<a href="http://arxiv.org/abs/2002.10322" target="_blank">arXiv:2002.10322</a> [<a href="http://arxiv.org/pdf/2002.10322" target="_blank">pdf</a>]

<h2>Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning. (arXiv:2004.06496v5 [cs.LG] UPDATED)</h2>
<h3>Michael Everett, Bjorn Lutjens, Jonathan P. How</h3>
<p>Deep Neural Network-based systems are now the state-of-the-art in many
robotics tasks, but their application in safety-critical domains remains
dangerous without formal guarantees on network robustness. Small perturbations
to sensor inputs (from noise or adversarial examples) are often enough to
change network-based decisions, which was recently shown to cause an autonomous
vehicle to swerve into another lane. In light of these dangers, numerous
algorithms have been developed as defensive mechanisms from these adversarial
inputs, some of which provide formal robustness guarantees or certificates.
This work leverages research on certified adversarial robustness to develop an
online certifiably robust for deep reinforcement learning algorithms. The
proposed defense computes guaranteed lower bounds on state-action values during
execution to identify and choose a robust action under a worst-case deviation
in input space due to possible adversaries or noise. Moreover, the resulting
policy comes with a certificate of solution quality, even though the true state
and optimal action are unknown to the certifier due to the perturbations. The
approach is demonstrated on a Deep Q-Network policy and is shown to increase
robustness to noise and adversaries in pedestrian collision avoidance scenarios
and a classic control task. This work extends one of our prior works with new
performance guarantees, extensions to other RL algorithms, expanded results
aggregated across more scenarios, an extension into scenarios with adversarial
behavior, comparisons with a more computationally expensive method, and
visualizations that provide intuition about the robustness algorithm.
</p>
<a href="http://arxiv.org/abs/2004.06496" target="_blank">arXiv:2004.06496</a> [<a href="http://arxiv.org/pdf/2004.06496" target="_blank">pdf</a>]

<h2>Transformer Reasoning Network for Image-Text Matching and Retrieval. (arXiv:2004.09144v3 [cs.CV] UPDATED)</h2>
<h3>Nicola Messina, Fabrizio Falchi, Andrea Esuli, Giuseppe Amato</h3>
<p>Image-text matching is an interesting and fascinating task in modern AI
research. Despite the evolution of deep-learning-based image and text
processing systems, multi-modal matching remains a challenging problem. In this
work, we consider the problem of accurate image-text matching for the task of
multi-modal large-scale information retrieval. State-of-the-art results in
image-text matching are achieved by inter-playing image and text features from
the two different processing pipelines, usually using mutual attention
mechanisms. However, this invalidates any chance to extract separate visual and
textual features needed for later indexing steps in large-scale retrieval
systems. In this regard, we introduce the Transformer Encoder Reasoning Network
(TERN), an architecture built upon one of the modern relationship-aware
self-attentive architectures, the Transformer Encoder (TE). This architecture
is able to separately reason on the two different modalities and to enforce a
final common abstract concept space by sharing the weights of the deeper
transformer layers. Thanks to this design, the implemented network is able to
produce compact and very rich visual and textual features available for the
successive indexing step. Experiments are conducted on the MS-COCO dataset, and
we evaluate the results using a discounted cumulative gain metric with
relevance computed exploiting caption similarities, in order to assess possibly
non-exact but relevant search results. We demonstrate that on this metric we
are able to achieve state-of-the-art results in the image retrieval task. Our
code is freely available at https://github.com/mesnico/TERN.
</p>
<a href="http://arxiv.org/abs/2004.09144" target="_blank">arXiv:2004.09144</a> [<a href="http://arxiv.org/pdf/2004.09144" target="_blank">pdf</a>]

<h2>Explainable Goal-Driven Agents and Robots -- A Comprehensive Review. (arXiv:2004.09705v2 [cs.RO] UPDATED)</h2>
<h3>Fatai Sado, Chu Kiong Loo, Wei Shiung Liew, Matthias Kerzel, Stefan Wermter</h3>
<p>Recent applications of autonomous agents and robots, such as self-driving
cars, scenario-based trainers, exploration robots, and service robots have
brought attention to crucial trust-related challenges associated with the
current generation of artificial intelligence (AI) systems. AI systems based on
the connectionist deep learning neural network approach lack capabilities of
explaining their decisions and actions to others, despite their great
successes. Without symbolic interpretation capabilities, they are black boxes,
which renders their decisions or actions opaque, making it difficult to trust
them in safety-critical applications. The recent stance on the explainability
of AI systems has witnessed several approaches on eXplainable Artificial
Intelligence (XAI); however, most of the studies have focused on data-driven
XAI systems applied in computational sciences. Studies addressing the
increasingly pervasive goal-driven agents and robots are still missing. This
paper reviews approaches on explainable goal-driven intelligent agents and
robots, focusing on techniques for explaining and communicating agents
perceptual functions (example, senses, and vision) and cognitive reasoning
(example, beliefs, desires, intention, plans, and goals) with humans in the
loop. The review highlights key strategies that emphasize transparency,
understandability, and continual learning for explainability. Finally, the
paper presents requirements for explainability and suggests a roadmap for the
possible realization of effective goal-driven explainable agents and robots.
</p>
<a href="http://arxiv.org/abs/2004.09705" target="_blank">arXiv:2004.09705</a> [<a href="http://arxiv.org/pdf/2004.09705" target="_blank">pdf</a>]

<h2>A survey on modern trainable activation functions. (arXiv:2005.00817v3 [cs.LG] UPDATED)</h2>
<h3>Andrea Apicella, Francesco Donnarumma, Francesco Isgr&#xf2;, Roberto Prevete</h3>
<p>In neural networks literature, there is a strong interest in identifying and
defining activation functions which can improve neural network performance. In
recent years there has been a renovated interest of the scientific community in
investigating activation functions which can be trained during the learning
process, usually referred to as "trainable", "learnable" or "adaptable"
activation functions. They appear to lead to better network performance.
Diverse and heterogeneous models of trainable activation function have been
proposed in the literature. In this paper, we present a survey of these models.
Starting from a discussion on the use of the term "activation function" in
literature, we propose a taxonomy of trainable activation functions, highlight
common and distinctive proprieties of recent and past models, and discuss main
advantages and limitations of this type of approach. We show that many of the
proposed approaches are equivalent to adding neuron layers which use fixed
(non-trainable) activation functions and some simple local rule that
constraints the corresponding weight layers.
</p>
<a href="http://arxiv.org/abs/2005.00817" target="_blank">arXiv:2005.00817</a> [<a href="http://arxiv.org/pdf/2005.00817" target="_blank">pdf</a>]

<h2>Doubly-Stochastic Normalization of the Gaussian Kernel is Robust to Heteroskedastic Noise. (arXiv:2006.00402v2 [stat.ML] UPDATED)</h2>
<h3>Boris Landa, Ronald R.Coifman, Yuval Kluger</h3>
<p>A fundamental step in many data-analysis techniques is the construction of an
affinity matrix describing similarities between data points. When the data
points reside in Euclidean space, a widespread approach is to from an affinity
matrix by the Gaussian kernel with pairwise distances, and to follow with a
certain normalization (e.g. the row-stochastic normalization or its symmetric
variant). We demonstrate that the doubly-stochastic normalization of the
Gaussian kernel with zero main diagonal (i.e., no self loops) is robust to
heteroskedastic noise. That is, the doubly-stochastic normalization is
advantageous in that it automatically accounts for observations with different
noise variances. Specifically, we prove that in a suitable high-dimensional
setting where heteroskedastic noise does not concentrate too much in any
particular direction in space, the resulting (doubly-stochastic) noisy affinity
matrix converges to its clean counterpart with rate $m^{-1/2}$, where $m$ is
the ambient dimension. We demonstrate this result numerically, and show that in
contrast, the popular row-stochastic and symmetric normalizations behave
unfavorably under heteroskedastic noise. Furthermore, we provide examples of
simulated and experimental single-cell RNA sequence data with intrinsic
heteroskedasticity, where the advantage of the doubly-stochastic normalization
for exploratory analysis is evident.
</p>
<a href="http://arxiv.org/abs/2006.00402" target="_blank">arXiv:2006.00402</a> [<a href="http://arxiv.org/pdf/2006.00402" target="_blank">pdf</a>]

<h2>Leveraging the Feature Distribution in Transfer-based Few-Shot Learning. (arXiv:2006.03806v3 [cs.LG] UPDATED)</h2>
<h3>Yuqing Hu, Vincent Gripon, St&#xe9;phane Pateux</h3>
<p>Few-shot classification is a challenging problem due to the uncertainty
caused by using few labelled samples. In the past few years, many methods have
been proposed to solve few-shot classification, among which transfer-based
methods have proved to achieve the best performance. Following this vein, in
this paper we propose a novel transfer-based method that builds on two
successive steps: 1) preprocessing the feature vectors so that they become
closer to Gaussian-like distributions, and 2) leveraging this preprocessing
using an optimal-transport inspired algorithm (in the case of transductive
settings). Using standardized vision benchmarks, we prove the ability of the
proposed methodology to achieve state-of-the-art accuracy with various
datasets, backbone architectures and few-shot settings.
</p>
<a href="http://arxiv.org/abs/2006.03806" target="_blank">arXiv:2006.03806</a> [<a href="http://arxiv.org/pdf/2006.03806" target="_blank">pdf</a>]

<h2>CubifAE-3D: Monocular Camera Space Cubification for Auto-Encoder based 3D Object Detection. (arXiv:2006.04080v2 [cs.CV] UPDATED)</h2>
<h3>Shubham Shrivastava, Punarjay Chakravarty</h3>
<p>We introduce a method for 3D object detection using a single monocular image.
Starting from a synthetic dataset, we pre-train an RGB-to-Depth Auto-Encoder
(AE). The embedding learnt from this AE is then used to train a 3D Object
Detector (3DOD) CNN which is used to regress the parameters of 3D object poses
after the encoder from the AE generates a latent embedding from the RGB image.
We show that we can pre-train the AE using paired RGB and depth images from
simulation data once and subsequently only train the 3DOD network using real
data, comprising of RGB images and 3D object pose labels (without the
requirement of dense depth). Our 3DOD network utilizes a particular
`cubification' of 3D space around the camera, where each cuboid is tasked with
predicting N object poses, along with their class and confidence values. The AE
pre-training and this method of dividing the 3D space around the camera into
cuboids give our method its name - CubifAE-3D. We demonstrate results for
monocular 3D object detection in the Autonomous Vehicle (AV) use-case with the
Virtual KITTI 2 and the KITTI datasets.
</p>
<a href="http://arxiv.org/abs/2006.04080" target="_blank">arXiv:2006.04080</a> [<a href="http://arxiv.org/pdf/2006.04080" target="_blank">pdf</a>]

<h2>Knowledge Distillation: A Survey. (arXiv:2006.05525v5 [cs.LG] UPDATED)</h2>
<h3>Jianping Gou, Baosheng Yu, Stephen John Maybank, Dacheng Tao</h3>
<p>In recent years, deep neural networks have been successful in both industry
and academia, especially for computer vision tasks. The great success of deep
learning is mainly due to its scalability to encode large-scale data and to
maneuver billions of model parameters. However, it is a challenge to deploy
these cumbersome deep models on devices with limited resources, e.g., mobile
phones and embedded devices, not only because of the high computational
complexity but also the large storage requirements. To this end, a variety of
model compression and acceleration techniques have been developed. As a
representative type of model compression and acceleration, knowledge
distillation effectively learns a small student model from a large teacher
model. It has received rapid increasing attention from the community. This
paper provides a comprehensive survey of knowledge distillation from the
perspectives of knowledge categories, training schemes, teacher-student
architecture, distillation algorithms, performance comparison and applications.
Furthermore, challenges in knowledge distillation are briefly reviewed and
comments on future research are discussed and forwarded.
</p>
<a href="http://arxiv.org/abs/2006.05525" target="_blank">arXiv:2006.05525</a> [<a href="http://arxiv.org/pdf/2006.05525" target="_blank">pdf</a>]

<h2>CoDeNet: Efficient Deployment of Input-Adaptive Object Detection on Embedded FPGAs. (arXiv:2006.08357v2 [cs.CV] UPDATED)</h2>
<h3>Zhen Dong, Dequan Wang, Qijing Huang, Yizhao Gao, Yaohui Cai, Tian Li, Bichen Wu, Kurt Keutzer, John Wawrzynek</h3>
<p>Deploying deep learning models on embedded systems has been challenging due
to limited computing resources. The majority of existing work focuses on
accelerating image classification, while other fundamental vision problems,
such as object detection, have not been adequately addressed. Compared with
image classification, detection problems are more sensitive to the spatial
variance of objects, and therefore, require specialized convolutions to
aggregate spatial information. To address this need, recent work introduces
dynamic deformable convolution to augment regular convolutions. However, this
will lead to inefficient memory accesses of inputs with existing hardware. In
this work, we harness the flexibility of FPGAs to develop a novel object
detection pipeline with deformable convolutions. We show the speed-accuracy
tradeoffs for a set of algorithm modifications including irregular-access
versus limited-range and fixed-shape. We then Co-Design a Network CoDeNet with
the modified deformable convolution and quantize it to 4-bit weights and 8-bit
activations. With our high-efficiency implementation, our solution reaches 26.9
frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50
on the standard object detection dataset, Pascal VOC. With our higher accuracy
implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of
parameters-20.9x smaller but 10% more accurate than Tiny-YOLO.
</p>
<a href="http://arxiv.org/abs/2006.08357" target="_blank">arXiv:2006.08357</a> [<a href="http://arxiv.org/pdf/2006.08357" target="_blank">pdf</a>]

<h2>Unified SVM algorithm based on LS-DC Loss. (arXiv:2006.09111v3 [cs.LG] UPDATED)</h2>
<h3>Zhou Shuisheng, Zhou Wendi</h3>
<p>Over the past two decades, Support Vector Machine (SVM) has been a popular
supervised machine learning model, and plenty of distinct algorithms are
designed separately based on different KKT conditions of the SVM model for
classification/regression with the distinct losses, including the convex loss
or non-convex loss. This paper proposes an algorithm that can train different
SVM models in a \emph{unified} scheme. Firstly, we introduce a definition of
the \emph{LS-DC} (least-squares type of difference of convex) loss and show
that the most commonly used losses in the SVM community are LS-DC loss or can
be approximated by LS-DC loss. Based on DCA (difference of convex algorithm),
we propose a unified algorithm, called \emph{UniSVM}, which can solve the SVM
model with any convex or non-convex LS-DC loss, in which only a vector will be
changed according to the specifically chosen loss. Notably, for training robust
SVM models with non-convex losses, UniSVM has a dominant advantage over all the
existing algorithms because it has a closed-form solution per iteration while
the existing ones always need to solve an L1SVM/L2SVM per iteration.
Furthermore, by the low-rank approximation of the kernel matrix, UniSVM can
solve the large-scale nonlinear problems efficiently. To verify the efficacy
and feasibility of the proposed algorithm, we perform many experiments on some
small artificial problems and some large benchmark tasks with/without outliers
for classification and regression to compare it with some state-of-the-art
algorithms. The experimental results support that UniSVM can obtain the
comparable performance within less training time. The highlight advantage of
UniSVM is that its core code in Matlab is less than ten lines, hence it can be
easily grasped by users or researchers.
</p>
<a href="http://arxiv.org/abs/2006.09111" target="_blank">arXiv:2006.09111</a> [<a href="http://arxiv.org/pdf/2006.09111" target="_blank">pdf</a>]

<h2>Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised Performance. (arXiv:2006.09363v2 [cs.LG] UPDATED)</h2>
<h3>Leslie N. Smith, Adam Conovaloff</h3>
<p>Reaching the performance of fully supervised learning with unlabeled data and
only labeling one sample per class might be ideal for deep learning
applications. We demonstrate for the first time the potential for building
one-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test
accuracies that are comparable to fully supervised learning. Our method
combines class prototype refining, class balancing, and self-training. A good
prototype choice is essential and we propose a technique for obtaining iconic
examples. In addition, we demonstrate that class balancing methods
substantially improve accuracy results in semi-supervised learning to levels
that allow self-training to reach the level of fully supervised learning
performance. Rigorous empirical evaluations provide evidence that labeling
large datasets is not necessary for training deep neural networks. We made our
code available at https://github.com/lnsmith54/BOSS to facilitate replication
and for use with future real-world applications.
</p>
<a href="http://arxiv.org/abs/2006.09363" target="_blank">arXiv:2006.09363</a> [<a href="http://arxiv.org/pdf/2006.09363" target="_blank">pdf</a>]

<h2>Sketch-Guided Scenery Image Outpainting. (arXiv:2006.09788v2 [cs.CV] UPDATED)</h2>
<h3>Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang</h3>
<p>The outpainting results produced by existing approaches are often too random
to meet users' requirement. In this work, we take the image outpainting one
step forward by allowing users to harvest personal custom outpainting results
using sketches as the guidance. To this end, we propose an encoder-decoder
based network to conduct sketch-guided outpainting, where two alignment modules
are adopted to impose the generated content to be realistic and consistent with
the provided sketches. First, we apply a holistic alignment module to make the
synthesized part be similar to the real one from the global view. Second, we
reversely produce the sketches from the synthesized part and encourage them be
consistent with the ground-truth ones using a sketch alignment module. In this
way, the learned generator will be imposed to pay more attention to fine
details and be sensitive to the guiding sketches. To our knowledge, this work
is the first attempt to explore the challenging yet meaningful conditional
scenery image outpainting. We conduct extensive experiments on two collected
benchmarks to qualitatively and quantitatively validate the effectiveness of
our approach compared with the other state-of-the-art generative models.
</p>
<a href="http://arxiv.org/abs/2006.09788" target="_blank">arXiv:2006.09788</a> [<a href="http://arxiv.org/pdf/2006.09788" target="_blank">pdf</a>]

<h2>Graph Neural Networks: Architectures, Stability and Transferability. (arXiv:2008.01767v2 [cs.LG] UPDATED)</h2>
<h3>Luana Ruiz, Fernando Gama, Alejandro Ribeiro</h3>
<p>Graph Neural Networks (GNNs) are information processing architectures for
signals supported on graphs. They are presented here as generalizations of
convolutional neural networks (CNNs) in which individual layers contain banks
of graph convolutional filters instead of banks of classical convolutional
filters. Otherwise, GNNs operate as CNNs. Filters are composed with pointwise
nonlinearities and stacked in layers. It is shown that GNN architectures
exhibit equivariance to permutation and stability to graph deformations. These
properties help explain the good performance of GNNs that can be observed
empirically. It is also shown that if graphs converge to a limit object, a
graphon, GNNs converge to a corresponding limit object, a graphon neural
network. This convergence justifies the transferability of GNNs across networks
with different number of nodes. Concepts are illustrated by the application of
GNNs to recommendation systems, decentralized collaborative control, and
wireless communication networks.
</p>
<a href="http://arxiv.org/abs/2008.01767" target="_blank">arXiv:2008.01767</a> [<a href="http://arxiv.org/pdf/2008.01767" target="_blank">pdf</a>]

<h2>Model-Based Offline Planning. (arXiv:2008.05556v2 [cs.LG] UPDATED)</h2>
<h3>Arthur Argenson, Gabriel Dulac-Arnold</h3>
<p>Offline learning is a key part of making reinforcement learning (RL) useable
in real systems. Offline RL looks at scenarios where there is data from a
system's operation, but no direct access to the system when learning a policy.
Recent work on training RL policies from offline data has shown results both
with model-free policies learned directly from the data, or with planning on
top of learnt models of the data. Model-free policies tend to be more
performant, but are more opaque, harder to command externally, and less easy to
integrate into larger systems. We propose an offline learner that generates a
model that can be used to control the system directly through planning. This
allows us to have easily controllable policies directly from data, without ever
interacting with the system. We show the performance of our algorithm,
Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and
demonstrate its ability leverage planning to respect environmental constraints.
We are able to find near-optimal polices for certain simulated systems from as
little as 50 seconds of real-time system interaction, and create zero-shot
goal-conditioned policies on a series of environments. An accompanying video
can be found here: https://youtu.be/nxGGHdZOFts
</p>
<a href="http://arxiv.org/abs/2008.05556" target="_blank">arXiv:2008.05556</a> [<a href="http://arxiv.org/pdf/2008.05556" target="_blank">pdf</a>]

<h2>NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size. (arXiv:2009.00437v6 [cs.LG] UPDATED)</h2>
<h3>Xuanyi Dong, Lu Liu, Katarzyna Musial, Bogdan Gabrys</h3>
<p>Neural architecture search (NAS) has attracted a lot of attention and has
been illustrated to bring tangible benefits in a large number of applications
in the past few years. Architecture topology and architecture size have been
regarded as two of the most important aspects for the performance of deep
learning models and the community has spawned lots of searching algorithms for
both aspects of the neural architectures. However, the performance gain from
these searching algorithms is achieved under different search spaces and
training setups. This makes the overall performance of the algorithms to some
extent incomparable and the improvement from a sub-module of the searching
model unclear. In this paper, we propose NATS-Bench, a unified benchmark on
searching for both topology and size, for (almost) any up-to-date NAS
algorithm. NATS-Bench includes the search space of 15,625 neural cell
candidates for architecture topology and 32,768 for architecture size on three
datasets. We analyze the validity of our benchmark in terms of various criteria
and performance comparison of all candidates in the search space. We also show
the versatility of NATS-Bench by benchmarking 13 recent state-of-the-art NAS
algorithms on it. All logs and diagnostic information trained using the same
setup for each candidate are provided. This facilitates a much larger community
of researchers to focus on developing better NAS algorithms in a more
comparable and computationally cost friendly environment. All codes are
publicly available at: https://xuanyidong.com/assets/projects/NATS-Bench.
</p>
<a href="http://arxiv.org/abs/2009.00437" target="_blank">arXiv:2009.00437</a> [<a href="http://arxiv.org/pdf/2009.00437" target="_blank">pdf</a>]

<h2>Open-Ended Fine-Grained 3D Object Categorization by Combining Shape and Texture Features in Multiple Colorspaces. (arXiv:2009.09235v2 [cs.CV] UPDATED)</h2>
<h3>Nils Keunecke, S. Hamidreza Kasaei</h3>
<p>As a consequence of an ever-increasing number of service robots, there is a
growing demand for highly accurate real-time 3D object recognition. Considering
the expansion of robot applications in more complex and dynamic environments,it
is evident that it is not possible to pre-program all object categories and
anticipate all exceptions in advance. Therefore, robots should have the
functionality to learn about new object categories in an open-ended fashion
while working in the environment.Towards this goal, we propose a deep transfer
learning approach to generate a scale- and pose-invariant object representation
by considering shape and texture information in multiple colorspaces. The
obtained global object representation is then fed to an instance-based object
category learning and recognition,where a non-expert human user exists in the
learning loop and can interactively guide the process of experience acquisition
by teaching new object categories, or by correcting insufficient or erroneous
categories. In this work, shape information encodes the common patterns of all
categories, while texture information is used to describes the appearance of
each instance in detail.Multiple color space combinations and network
architectures are evaluated to find the most descriptive system. Experimental
results showed that the proposed network architecture out-performed the
selected state-of-the-art approaches in terms of object classification accuracy
and scalability. Furthermore, we performed a real robot experiment in the
context of serve-a-beer scenario to show the real-time performance of the
proposed approach.
</p>
<a href="http://arxiv.org/abs/2009.09235" target="_blank">arXiv:2009.09235</a> [<a href="http://arxiv.org/pdf/2009.09235" target="_blank">pdf</a>]

<h2>Semi-Supervised Learning of Multi-Object 3D Scene Representations. (arXiv:2010.04030v2 [cs.CV] UPDATED)</h2>
<h3>Cathrin Elich, Martin R. Oswald, Marc Pollefeys, Joerg Stueckler</h3>
<p>Representing scenes at the granularity of objects is a prerequisite for scene
understanding and decision making. We propose a novel approach for learning
multi-object 3D scene representations from images. A recurrent encoder
regresses a latent representation of 3D shapes, poses and texture of each
object from an input RGB image. The 3D shapes are represented continuously in
function-space as signed distance functions (SDF) which we efficiently
pre-train from example shapes in a supervised way. By differentiable rendering
we then train our model to decompose scenes self-supervised from RGB-D images.
Our approach learns to decompose images into the constituent objects of the
scene and to infer their shape, pose and texture from a single view. We
evaluate the accuracy of our model in inferring the 3D scene layout and
demonstrate its generative capabilities.
</p>
<a href="http://arxiv.org/abs/2010.04030" target="_blank">arXiv:2010.04030</a> [<a href="http://arxiv.org/pdf/2010.04030" target="_blank">pdf</a>]

<h2>Leveraging Activity Recognition to Enable Protective Behavior Detection in Continuous Data. (arXiv:2011.01776v3 [cs.LG] UPDATED)</h2>
<h3>Chongyang Wang, Yuan Gao, Akhil Mathur, Amanda C. De C. Williams, Nicholas D. Lane, Nadia Bianchi-Berthouze</h3>
<p>Protective behavior exhibited by people with chronic pain (CP) during
physical activities is the key to understanding their physical and emotional
states. Existing automatic protective behavior detection (PBD) methods rely on
pre-segmentation of activity instances as they expect the activity types to be
predefined by users. However, during real-life self-directed management, people
perform activities casually. Therefore, technology-enabled support should be
delivered continuously and automatically adapted to the activity type and
presence of protective behavior. Hence, to facilitate ubiquitous CP management,
it becomes critical to enable accurate PBD over continuous data. In this paper,
we propose to integrate human activity recognition (HAR) with PBD via a novel
hierarchical HAR-PBD architecture comprising graph-convolution and long
short-term memory (GC-LSTM) networks, and alleviate class imbalances using a
class-balanced focal categorical-cross-entropy (CFCC) loss. Through in-depth
evaluation of the approach using a CP patients' dataset, we show that the
leveraging of HAR, GC-LSTM networks, and CFCC loss leads to clear increase in
PBD performance against the baseline (macro F1 score of 0.81 vs. 0.66 and
precision-recall area-under-the-curve (PR-AUC) of 0.60 vs. 0.44). We conclude
by discussing possible use cases of the hierarchical architecture in CP
management and beyond. We also discuss current limitations and ways forward.
</p>
<a href="http://arxiv.org/abs/2011.01776" target="_blank">arXiv:2011.01776</a> [<a href="http://arxiv.org/pdf/2011.01776" target="_blank">pdf</a>]

<h2>AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries. (arXiv:2011.08435v4 [cs.LG] UPDATED)</h2>
<h3>Qianjiang Hu, Xiao Wang, Wei Hu, Guo-Jun Qi</h3>
<p>Contrastive learning relies on constructing a collection of negative examples
that are sufficiently hard to discriminate against positive queries when their
representations are self-trained. Existing contrastive learning methods either
maintain a queue of negative samples over minibatches while only a small
portion of them are updated in an iteration, or only use the other examples
from the current minibatch as negatives. They could not closely track the
change of the learned representation over iterations by updating the entire
queue as a whole, or discard the useful information from the past minibatches.
Alternatively, we present to directly learn a set of negative adversaries
playing against the self-trained representation. Two players, the
representation network and negative adversaries, are alternately updated to
obtain the most challenging negative examples against which the representation
of positive queries will be trained to discriminate. We further show that the
negative adversaries are updated towards a weighted combination of positive
queries by maximizing the adversarial contrastive loss, thereby allowing them
to closely track the change of representations over time. Experiment results
demonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves
superior performances (a top-1 accuracy of 73.2\% over 200 epochs and 75.7\%
over 800 epochs with linear evaluation on ImageNet), but also can be
pre-trained more efficiently with fewer epochs.
</p>
<a href="http://arxiv.org/abs/2011.08435" target="_blank">arXiv:2011.08435</a> [<a href="http://arxiv.org/pdf/2011.08435" target="_blank">pdf</a>]

<h2>Elementary Effects Analysis of factors controlling COVID-19 infections in computational simulation reveals the importance of Social Distancing and Mask Usage. (arXiv:2011.11381v2 [cs.AI] UPDATED)</h2>
<h3>Kelvin K.F. Li, Stephen A. Jarvis, Fayyaz Minhas</h3>
<p>COVID-19 was declared a pandemic by the World Health Organization (WHO) on
March 11, 2020. With half of the world's countries in lockdown as of April due
to this pandemic, monitoring and understanding the spread of the virus and
infection rates and how these factors relate to behavioural and societal
parameters is crucial for effective policy making. This paper aims to
investigate the effectiveness of masks, social distancing, lockdown and
self-isolation for reducing the spread of SARS-CoV-2 infections. Our findings
based on agent-based simulation modelling show that whilst requiring a lockdown
is widely believed to be the most efficient method to quickly reduce infection
numbers, the practice of social distancing and the usage of surgical masks can
potentially be more effective than requiring a lockdown. Our multivariate
analysis of simulation results using the Morris Elementary Effects Method
suggests that if a sufficient proportion of the population wore surgical masks
and followed social distancing regulations, then SARS-CoV-2 infections can be
controlled without requiring a lockdown.
</p>
<a href="http://arxiv.org/abs/2011.11381" target="_blank">arXiv:2011.11381</a> [<a href="http://arxiv.org/pdf/2011.11381" target="_blank">pdf</a>]

<h2>C-Learning: Horizon-Aware Cumulative Accessibility Estimation. (arXiv:2011.12363v3 [cs.LG] UPDATED)</h2>
<h3>Panteha Naderian, Gabriel Loaiza-Ganem, Harry J. Braviner, Anthony L. Caterini, Jesse C. Cresswell, Tong Li, Animesh Garg</h3>
<p>Multi-goal reaching is an important problem in reinforcement learning needed
to achieve algorithmic generalization. Despite recent advances in this field,
current algorithms suffer from three major challenges: high sample complexity,
learning only a single way of reaching the goals, and difficulties in solving
complex motion planning tasks. In order to address these limitations, we
introduce the concept of cumulative accessibility functions, which measure the
reachability of a goal from a given state within a specified horizon. We show
that these functions obey a recurrence relation, which enables learning from
offline interactions. We also prove that optimal cumulative accessibility
functions are monotonic in the planning horizon. Additionally, our method can
trade off speed and reliability in goal-reaching by suggesting multiple paths
to a single goal depending on the provided horizon. We evaluate our approach on
a set of multi-goal discrete and continuous control tasks. We show that our
method outperforms state-of-the-art goal-reaching algorithms in success rate,
sample complexity, and path optimality. Our code is available at
https://github.com/layer6ai-labs/CAE, and additional visualizations can be
found at https://sites.google.com/view/learning-cae/.
</p>
<a href="http://arxiv.org/abs/2011.12363" target="_blank">arXiv:2011.12363</a> [<a href="http://arxiv.org/pdf/2011.12363" target="_blank">pdf</a>]

<h2>Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v5 [cs.CV] UPDATED)</h2>
<h3>Tony Lindeberg</h3>
<p>This paper presents a hybrid approach between scale-space theory and deep
learning, where a deep learning architecture is constructed by coupling
parameterized scale-space operations in cascade. By sharing the learnt
parameters between multiple scale channels, and by using the transformation
properties of the scale-space primitives under scaling transformations, the
resulting network becomes provably scale covariant. By in addition performing
max pooling over the multiple scale channels, a resulting network architecture
for image classification also becomes provably scale invariant. We investigate
the performance of such networks on the MNISTLargeScale dataset, which contains
rescaled images from original MNIST over a factor of 4 concerning training data
and over a factor of 16 concerning testing data. It is demonstrated that the
resulting approach allows for scale generalization, enabling good performance
for classifying patterns at scales not present in the training data.
</p>
<a href="http://arxiv.org/abs/2011.14759" target="_blank">arXiv:2011.14759</a> [<a href="http://arxiv.org/pdf/2011.14759" target="_blank">pdf</a>]

<h2>Obstacle Avoidance Using a Monocular Camera. (arXiv:2012.01608v2 [cs.RO] UPDATED)</h2>
<h3>Kyle Hatch, John Mern, Mykel Kochenderfer</h3>
<p>A collision avoidance system based on simple digital cameras would help
enable the safe integration of small UAVs into crowded, low-altitude
environments. In this work, we present an obstacle avoidance system for small
UAVs that uses a monocular camera with a hybrid neural network and path planner
controller. The system is comprised of a vision network for estimating depth
from camera images, a high-level control network, a collision prediction
network, and a contingency policy. This system is evaluated on a simulated UAV
navigating an obstacle course in a constrained flight pattern. Results show the
proposed system achieves low collision rates while maintaining operationally
relevant flight speeds.
</p>
<a href="http://arxiv.org/abs/2012.01608" target="_blank">arXiv:2012.01608</a> [<a href="http://arxiv.org/pdf/2012.01608" target="_blank">pdf</a>]

<h2>Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling. (arXiv:2012.06046v2 [cs.LG] UPDATED)</h2>
<h3>Benedikt Boecking, Willie Neiswanger, Eric Xing, Artur Dubrawski</h3>
<p>Obtaining large annotated datasets is critical for training successful
machine learning models and it is often a bottleneck in practice. Weak
supervision offers a promising alternative for producing labeled datasets
without ground truth annotations by generating probabilistic labels using
multiple noisy heuristics. This process can scale to large datasets and has
demonstrated state of the art performance in diverse domains such as healthcare
and e-commerce. One practical issue with learning from user-generated
heuristics is that their creation requires creativity, foresight, and domain
expertise from those who hand-craft them, a process which can be tedious and
subjective. We develop the first framework for interactive weak supervision in
which a method proposes heuristics and learns from user feedback given on each
proposed heuristic. Our experiments demonstrate that only a small number of
feedback iterations are needed to train models that achieve highly competitive
test set performance without access to ground truth training labels. We conduct
user studies, which show that users are able to effectively provide feedback on
heuristics and that test set results track the performance of simulated
oracles.
</p>
<a href="http://arxiv.org/abs/2012.06046" target="_blank">arXiv:2012.06046</a> [<a href="http://arxiv.org/pdf/2012.06046" target="_blank">pdf</a>]

<h2>Doubly Stochastic Generative Arrivals Modeling. (arXiv:2012.13940v2 [stat.ML] UPDATED)</h2>
<h3>Yufeng Zheng, Zeyu Zheng</h3>
<p>We propose a new framework named DS-WGAN that integrates the doubly
stochastic (DS) structure and the Wasserstein generative adversarial networks
(WGAN) to model, estimate, and simulate a wide class of arrival processes with
general non-stationary and random arrival rates. Regarding statistical
properties, we prove consistency and convergence rate for the estimator solved
by the DS-WGAN framework under a non-parametric smoothness condition. Regarding
computational efficiency and tractability, we address a challenge in gradient
evaluation and model estimation, arised from the discontinuity in the
simulator. We then show that the DS-WGAN framework can conveniently facilitate
what-if simulation and predictive simulation for future scenarios that are
different from the history. Numerical experiments with synthetic and real data
sets are implemented to demonstrate the performance of DS-WGAN. The performance
is measured from both a statistical perspective and an operational performance
evaluation perspective. Numerical experiments suggest that, in terms of
performance, the successful model estimation for DS-WGAN only requires a
moderate size of representative data, which can be appealing in many contexts
of operational management.
</p>
<a href="http://arxiv.org/abs/2012.13940" target="_blank">arXiv:2012.13940</a> [<a href="http://arxiv.org/pdf/2012.13940" target="_blank">pdf</a>]

<h2>Temporal Contrastive Graph for Self-supervised Video Representation Learning. (arXiv:2101.00820v3 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin</h3>
<p>Attempt to fully explore the fine-grained temporal structure and global-local
chronological characteristics for self-supervised video representation
learning, this work takes a closer look at exploiting the temporal structure of
videos and further proposes a novel self-supervised method named Temporal
Contrastive Graph (TCG). In contrast to the existing methods that randomly
shuffle the video frames or video snippets within a video, our proposed TCG
roots in a hybrid graph contrastive learning strategy to regard the
inter-snippet and intra-snippet temporal relationships as self-supervision
signals for temporal representation learning. Inspired by the neuroscience
studies that the human visual system is sensitive to both local and global
temporal changes, our proposed TCG integrates the prior knowledge about the
frame and snippet orders into temporal contrastive graph structures, i.e., the
intra-/inter- snippet temporal contrastive graph modules, to well preserve the
local and global temporal relationships among video frame-sets and snippets. By
randomly removing edges and masking node features of the intra-snippet graphs
or inter-snippet graphs, our TCG can generate different correlated graph views.
Then, specific contrastive losses are designed to maximize the agreement
between node embeddings in different views. To learn the global context
representation and recalibrate the channel-wise features adaptively, we
introduce an adaptive video snippet order prediction module, which leverages
the relational knowledge among video snippets to predict the actual snippet
orders. Extensive experimental results demonstrate the superiority of our TCG
over the state-of-the-art methods on large-scale action recognition and video
retrieval benchmarks.
</p>
<a href="http://arxiv.org/abs/2101.00820" target="_blank">arXiv:2101.00820</a> [<a href="http://arxiv.org/pdf/2101.00820" target="_blank">pdf</a>]

<h2>TrackMPNN: A Message Passing Graph Neural Architecture for Multi-Object Tracking. (arXiv:2101.04206v2 [cs.CV] UPDATED)</h2>
<h3>Akshay Rangesh, Pranav Maheshwari, Mez Gebre, Siddhesh Mhatre, Vahid Ramezani, Mohan M. Trivedi</h3>
<p>This study follows many previous approaches to multi-object tracking (MOT)
that model the problem using graph-based data structures, and adapts this
formulation to make it amenable to modern neural networks. Our main
contributions in this work are the creation of a framework based on dynamic
undirected graphs that represent the data association problem over multiple
timesteps, and a message passing graph neural network (GNN) that operates on
these graphs to produce the desired likelihood for every association therein.
We further provide solutions and propositions for the computational problems
that need to be addressed to create a memory-efficient, real-time, online
algorithm that can reason over multiple timesteps, correct previous mistakes,
update beliefs, possess long-term memory, and handle missed/false detections.
In addition to this, our framework provides flexibility in the choice of
temporal window sizes to operate on and the losses used for training. In
essence, this study provides a framework for any kind of graph based neural
network to be trained using conventional techniques from supervised learning,
and then use these trained models to infer on new sequences in an online,
real-time, computationally tractable manner. To demonstrate the efficacy and
robustness of our approach, we only use the 2D box location and object category
to construct the descriptor for each object instance. Despite this, our model
performs on par with state-of-the-art approaches that make use of multiple
hand-crafted and/or learned features. Experiments, qualitative examples and
competitive results on popular MOT benchmarks for autonomous driving
demonstrate the promise and uniqueness of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2101.04206" target="_blank">arXiv:2101.04206</a> [<a href="http://arxiv.org/pdf/2101.04206" target="_blank">pdf</a>]

<h2>Scaling the Convex Barrier with Sparse Dual Algorithms. (arXiv:2101.05844v2 [cs.LG] UPDATED)</h2>
<h3>Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H.S. Torr, M. Pawan Kumar</h3>
<p>Tight and efficient neural network bounding is crucial to the scaling of
neural network verification systems. Many efficient bounding algorithms have
been presented recently, but they are often too loose to verify more
challenging properties. This is due to the weakness of the employed relaxation,
which is usually a linear program of size linear in the number of neurons.
While a tighter linear relaxation for piecewise-linear activations exists, it
comes at the cost of exponentially many constraints and currently lacks an
efficient customized solver. We alleviate this deficiency by presenting two
novel dual algorithms: one operates a subgradient method on a small active set
of dual variables, the other exploits the sparsity of Frank-Wolfe type
optimizers to incur only a linear memory cost. Both methods recover the
strengths of the new relaxation: tightness and a linear separation oracle. At
the same time, they share the benefits of previous dual approaches for weaker
relaxations: massive parallelism, GPU implementation, low cost per iteration
and valid bounds at any time. As a consequence, we can obtain better bounds
than off-the-shelf solvers in only a fraction of their running time, attaining
significant formal verification speed-ups.
</p>
<a href="http://arxiv.org/abs/2101.05844" target="_blank">arXiv:2101.05844</a> [<a href="http://arxiv.org/pdf/2101.05844" target="_blank">pdf</a>]

<h2>Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving. (arXiv:2101.06784v2 [cs.CV] UPDATED)</h2>
<h3>James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang, Eilyan Bitar, Ersin Yumer, Raquel Urtasun</h3>
<p>Modern self-driving perception systems have been shown to improve upon
processing complementary inputs such as LiDAR with images. In isolation, 2D
images have been found to be extremely vulnerable to adversarial attacks. Yet,
there have been limited studies on the adversarial robustness of multi-modal
models that fuse LiDAR features with image features. Furthermore, existing
works do not consider physically realizable perturbations that are consistent
across the input modalities. In this paper, we showcase practical
susceptibilities of multi-sensor detection by placing an adversarial object on
top of a host vehicle. We focus on physically realizable and input-agnostic
attacks as they are feasible to execute in practice, and show that a single
universal adversary can hide different host vehicles from state-of-the-art
multi-modal detectors. Our experiments demonstrate that successful attacks are
primarily caused by easily corrupted image features. Furthermore, we find that
in modern sensor fusion methods which project image features into 3D,
adversarial attacks can exploit the projection process to generate false
positives across distant regions in 3D. Towards more robust multi-modal
perception systems, we show that adversarial training with feature denoising
can boost robustness to such attacks significantly. However, we find that
standard adversarial defenses still struggle to prevent false positives which
are also caused by inaccurate associations between 3D LiDAR points and 2D
pixels.
</p>
<a href="http://arxiv.org/abs/2101.06784" target="_blank">arXiv:2101.06784</a> [<a href="http://arxiv.org/pdf/2101.06784" target="_blank">pdf</a>]

<h2>HyperNTF: A Hypergraph Regularized Nonnegative Tensor Factorization for Dimensionality Reduction. (arXiv:2101.06827v2 [cs.LG] UPDATED)</h2>
<h3>Wanguang Yin, Zhengming Ma, Quanying Liu</h3>
<p>Most methods for dimensionality reduction are based on either tensor
representation or local geometry learning. However, the tensor-based methods
severely rely on the assumption of global and multilinear structures in
high-dimensional data; and the manifold learning methods suffer from the
out-of-sample problem. In this paper, bridging the tensor decomposition and
manifold learning, we propose a novel method, called Hypergraph Regularized
Nonnegative Tensor Factorization (HyperNTF). HyperNTF can preserve
nonnegativity in tensor factorization, and uncover the higher-order
relationship among the nearest neighborhoods. Clustering analysis with HyperNTF
has low computation and storage costs. The experiments on four synthetic data
show a desirable property of hypergraph in uncovering the high-order
correlation to unfold the curved manifolds. Moreover, the numerical experiments
on six real datasets suggest that HyperNTF robustly outperforms
state-of-the-art algorithms in clustering analysis.
</p>
<a href="http://arxiv.org/abs/2101.06827" target="_blank">arXiv:2101.06827</a> [<a href="http://arxiv.org/pdf/2101.06827" target="_blank">pdf</a>]

<h2>GLocalX -- From Local to Global Explanations of Black Box AI Models. (arXiv:2101.07685v2 [cs.LG] UPDATED)</h2>
<h3>Mattia Setzu, Riccardo Guidotti, Anna Monreale, Franco Turini, Dino Pedreschi, Fosca Giannotti</h3>
<p>Artificial Intelligence (AI) has come to prominence as one of the major
components of our society, with applications in most aspects of our lives. In
this field, complex and highly nonlinear machine learning models such as
ensemble models, deep neural networks, and Support Vector Machines have
consistently shown remarkable accuracy in solving complex tasks. Although
accurate, AI models often are "black boxes" which we are not able to
understand. Relying on these models has a multifaceted impact and raises
significant concerns about their transparency. Applications in sensitive and
critical domains are a strong motivational factor in trying to understand the
behavior of black boxes. We propose to address this issue by providing an
interpretable layer on top of black box models by aggregating "local"
explanations. We present GLocalX, a "local-first" model agnostic explanation
method. Starting from local explanations expressed in form of local decision
rules, GLocalX iteratively generalizes them into global explanations by
hierarchically aggregating them. Our goal is to learn accurate yet simple
interpretable models to emulate the given black box, and, if possible, replace
it entirely. We validate GLocalX in a set of experiments in standard and
constrained settings with limited or no access to either data or local
explanations. Experiments show that GLocalX is able to accurately emulate
several models with simple and small models, reaching state-of-the-art
performance against natively global solutions. Our findings show how it is
often possible to achieve a high level of both accuracy and comprehensibility
of classification models, even in complex domains with high-dimensional data,
without necessarily trading one property for the other. This is a key
requirement for a trustworthy AI, necessary for adoption in high-stakes
decision making applications.
</p>
<a href="http://arxiv.org/abs/2101.07685" target="_blank">arXiv:2101.07685</a> [<a href="http://arxiv.org/pdf/2101.07685" target="_blank">pdf</a>]

<h2>Learning Abstract Task Representations. (arXiv:2101.07852v2 [cs.LG] UPDATED)</h2>
<h3>Mikhail M. Meskhi, Adriano Rivolli, Rafael G. Mantovani, Ricardo Vilalta</h3>
<p>A proper form of data characterization can guide the process of
learning-algorithm selection and model-performance estimation. The field of
meta-learning has provided a rich body of work describing effective forms of
data characterization using different families of meta-features (statistical,
model-based, information-theoretic, topological, etc.). In this paper, we start
with the abundant set of existing meta-features and propose a method to induce
new abstract meta-features as latent variables in a deep neural network. We
discuss the pitfalls of using traditional meta-features directly and argue for
the importance of learning high-level task properties. We demonstrate our
methodology using a deep neural network as a feature extractor. We demonstrate
that 1) induced meta-models mapping abstract meta-features to generalization
performance outperform other methods by ~18% on average, and 2) abstract
meta-features attain high feature-relevance scores.
</p>
<a href="http://arxiv.org/abs/2101.07852" target="_blank">arXiv:2101.07852</a> [<a href="http://arxiv.org/pdf/2101.07852" target="_blank">pdf</a>]

<h2>Riemannian-based Discriminant Analysis for Feature Extraction and Classification. (arXiv:2101.08032v2 [cs.LG] UPDATED)</h2>
<h3>Wanguang Yin, Zhengming Ma, Quanying Liu</h3>
<p>Discriminant analysis, as a widely used approach in machine learning to
extract low-dimensional features from the high-dimensional data, applies the
Fisher discriminant criterion to find the orthogonal discriminant projection
subspace. But most of the Euclidean-based algorithms for discriminant analysis
are easily convergent to a spurious local minima and hardly obtain an unique
solution. To address such problem, in this study we propose a novel method
named Riemannian-based Discriminant Analysis (RDA), which transforms the
traditional Euclidean-based methods to the Riemannian manifold space. In RDA,
the second-order geometry of trust-region methods is utilized to learn the
discriminant bases. To validate the efficiency and effectiveness of RDA, we
conduct a variety of experiments on image classification tasks. The numerical
results suggest that RDA can extract statistically significant features and
robustly outperform state-of-the-art algorithms in classification tasks.
</p>
<a href="http://arxiv.org/abs/2101.08032" target="_blank">arXiv:2101.08032</a> [<a href="http://arxiv.org/pdf/2101.08032" target="_blank">pdf</a>]

<h2>A Novel Genetic Algorithm with Hierarchical Evaluation Strategy for Hyperparameter Optimisation of Graph Neural Networks. (arXiv:2101.09300v2 [cs.LG] UPDATED)</h2>
<h3>Yingfang Yuan, Wenjun Wang, George M. Coghill, Wei Pang</h3>
<p>Graph representation of structured data can facilitate the extraction of
stereoscopic features, and it has demonstrated excellent ability when working
with deep learning systems, the so-called Graph Neural Networks (GNNs).
Choosing a promising architecture for constructing GNNs can be transferred to a
hyperparameter optimisation problem, a very challenging task due to the size of
the underlying search space and high computational cost for evaluating
candidate GNNs. To address this issue, this research presents a novel genetic
algorithm with a hierarchical evaluation strategy (HESGA), which combines the
full evaluation of GNNs with a fast evaluation approach. By using full
evaluation, a GNN is represented by a set of hyperparameter values and trained
on a specified dataset, and root mean square error (RMSE) will be used to
measure the quality of the GNN represented by the set of hyperparameter values
(for regression problems). While in the proposed fast evaluation process, the
training will be interrupted at an early stage, the difference of RMSE values
between the starting and interrupted epochs will be used as a fast score, which
implies the potential of the GNN being considered. To coordinate both types of
evaluations, the proposed hierarchical strategy uses the fast evaluation in a
lower level for recommending candidates to a higher level, where the full
evaluation will act as a final assessor to maintain a group of elite
individuals. To validate the effectiveness of HESGA, we apply it to optimise
two types of deep graph neural networks. The experimental results on three
benchmark datasets demonstrate its advantages compared to Bayesian
hyperparameter optimization.
</p>
<a href="http://arxiv.org/abs/2101.09300" target="_blank">arXiv:2101.09300</a> [<a href="http://arxiv.org/pdf/2101.09300" target="_blank">pdf</a>]

<h2>A systematic literature review on state-of-the-art deep learning methods for process prediction. (arXiv:2101.09320v2 [cs.LG] UPDATED)</h2>
<h3>Dominic A. Neu, Johannes Lahann, Peter Fettke</h3>
<p>Process mining enables the reconstruction and evaluation of business
processes based on digital traces in IT systems. An increasingly important
technique in this context is process prediction. Given a sequence of events of
an ongoing trace, process prediction allows forecasting upcoming events or
performance measurements. In recent years, multiple process prediction
approaches have been proposed, applying different data processing schemes and
prediction algorithms. This study focuses on deep learning algorithms since
they seem to outperform their machine learning alternatives consistently.
Whilst having a common learning algorithm, they use different data
preprocessing techniques, implement a variety of network topologies and focus
on various goals such as outcome prediction, time prediction or control-flow
prediction. Additionally, the set of log-data, evaluation metrics and baselines
used by the authors diverge, making the results hard to compare. This paper
attempts to synthesise the advantages and disadvantages of the procedural
decisions in these approaches by conducting a systematic literature review.
</p>
<a href="http://arxiv.org/abs/2101.09320" target="_blank">arXiv:2101.09320</a> [<a href="http://arxiv.org/pdf/2101.09320" target="_blank">pdf</a>]

<h2>An Optimal Reduction of TV-Denoising to Adaptive Online Learning. (arXiv:2101.09438v2 [cs.LG] UPDATED)</h2>
<h3>Dheeraj Baby, Xuandong Zhao, Yu-Xiang Wang</h3>
<p>We consider the problem of estimating a function from $n$ noisy samples whose
discrete Total Variation (TV) is bounded by $C_n$. We reveal a deep connection
to the seemingly disparate problem of Strongly Adaptive online learning
(Daniely et al, 2015) and provide an $O(n \log n)$ time algorithm that attains
the near minimax optimal rate of $\tilde O (n^{1/3}C_n^{2/3})$ under squared
error loss. The resulting algorithm runs online and optimally adapts to the
unknown smoothness parameter $C_n$. This leads to a new and more versatile
alternative to wavelets-based methods for (1) adaptively estimating TV bounded
functions; (2) online forecasting of TV bounded trends in time series.
</p>
<a href="http://arxiv.org/abs/2101.09438" target="_blank">arXiv:2101.09438</a> [<a href="http://arxiv.org/pdf/2101.09438" target="_blank">pdf</a>]

<h2>VIO-Aided Structure from Motion Under Challenging Environments. (arXiv:2101.09657v2 [cs.CV] UPDATED)</h2>
<h3>Zijie Jiang, Hajime Taira, Naoyuki Miyashita, Masatoshi Okutomi</h3>
<p>In this paper, we present a robust and efficient Structure from Motion
pipeline for accurate 3D reconstruction under challenging environments by
leveraging the camera pose information from a visual-inertial odometry.
Specifically, we propose a geometric verification method to filter out
mismatches by considering the prior geometric configuration of candidate image
pairs. Furthermore, we introduce an efficient and scalable reconstruction
approach that relies on batched image registration and robust bundle
adjustment, both leveraging the reliable local odometry estimation. Extensive
experimental results show that our pipeline performs better than the
state-of-the-art SfM approaches in terms of reconstruction accuracy and
robustness for challenging sequential image collections.
</p>
<a href="http://arxiv.org/abs/2101.09657" target="_blank">arXiv:2101.09657</a> [<a href="http://arxiv.org/pdf/2101.09657" target="_blank">pdf</a>]

<h2>Learning Synthetic Environments for Reinforcement Learning with Evolution Strategies. (arXiv:2101.09721v2 [cs.LG] UPDATED)</h2>
<h3>Fabio Ferreira, Thomas Nierhoff, Frank Hutter</h3>
<p>This work explores learning agent-agnostic synthetic environments (SEs) for
Reinforcement Learning. SEs act as a proxy for target environments and allow
agents to be trained more efficiently than when directly trained on the target
environment. We formulate this as a bi-level optimization problem and represent
an SE as a neural network. By using Natural Evolution Strategies and a
population of SE parameter vectors, we train agents in the inner loop on
evolving SEs while in the outer loop we use the performance on the target task
as a score for meta-updating the SE population. We show empirically that our
method is capable of learning SEs for two discrete-action-space tasks
(CartPole-v0 and Acrobot-v1) that allow us to train agents more robustly and
with up to 60% fewer steps. Not only do we show in experiments with 4000
evaluations that the SEs are robust against hyperparameter changes such as the
learning rate, batch sizes and network sizes, we also show that SEs trained
with DDQN agents transfer in limited ways to a discrete-action-space version of
TD3 and very well to Dueling DDQN.
</p>
<a href="http://arxiv.org/abs/2101.09721" target="_blank">arXiv:2101.09721</a> [<a href="http://arxiv.org/pdf/2101.09721" target="_blank">pdf</a>]

<h2>Multi-view Integration Learning for Irregularly-sampled Clinical Time Series. (arXiv:2101.09986v2 [cs.LG] UPDATED)</h2>
<h3>Yurim Lee, Eunji Jun, Heung-Il Suk</h3>
<p>Electronic health record (EHR) data is sparse and irregular as it is recorded
at irregular time intervals, and different clinical variables are measured at
each observation point. In this work, we propose a multi-view features
integration learning from irregular multivariate time series data by
self-attention mechanism in an imputation-free manner. Specifically, we devise
a novel multi-integration attention module (MIAM) to extract complex
information inherent in irregular time series data. In particular, we
explicitly learn the relationships among the observed values, missing
indicators, and time interval between the consecutive observations,
simultaneously. The rationale behind our approach is the use of human knowledge
such as what to measure and when to measure in different situations, which are
indirectly represented in the data. In addition, we build an attention-based
decoder as a missing value imputer that helps empower the representation
learning of the inter-relations among multi-view observations for the
prediction task, which operates at the training phase only. We validated the
effectiveness of our method over the public MIMIC-III and PhysioNet challenge
2012 datasets by comparing with and outperforming the state-of-the-art methods
for in-hospital mortality prediction.
</p>
<a href="http://arxiv.org/abs/2101.09986" target="_blank">arXiv:2101.09986</a> [<a href="http://arxiv.org/pdf/2101.09986" target="_blank">pdf</a>]

<h2>Solving a Multi-resource Partial-ordering Flexible Variant of the Job-shop Scheduling Problem with Hybrid ASP. (arXiv:2101.10162v2 [cs.AI] UPDATED)</h2>
<h3>Giulia Francescutto, Konstantin Schekotihin, Mohammed M. S. El-Kholany</h3>
<p>Many complex activities of production cycles, such as quality control or
fault analysis, require highly experienced specialists to perform various
operations on (semi)finished products using different tools. In practical
scenarios, the selection of a next operation is complicated, since each expert
has only a local view on the total set of operations to be performed. As a
result, decisions made by the specialists are suboptimal and might cause
significant costs. In this paper, we consider a Multi-resource Partial-ordering
Flexible Job-shop Scheduling (MPF-JSS) problem where partially-ordered
sequences of operations must be scheduled on multiple required resources, such
as tools and specialists. The resources are flexible and can perform one or
more operations depending on their properties. The problem is modeled using
Answer Set Programming (ASP) in which the time assignments are efficiently done
using Difference Logic. Moreover, we suggest two multi-shot solving strategies
aiming at the identification of the time bounds allowing for a solution of the
schedule optimization problem. Experiments conducted on a set of instances
extracted from a medium-sized semiconductor fault analysis lab indicate that
our approach can find schedules for 87 out of 91 considered real-world
instances.
</p>
<a href="http://arxiv.org/abs/2101.10162" target="_blank">arXiv:2101.10162</a> [<a href="http://arxiv.org/pdf/2101.10162" target="_blank">pdf</a>]

<h2>Proba-V-ref: Repurposing the Proba-V challenge for reference-aware super resolution. (arXiv:2101.10200v2 [cs.CV] UPDATED)</h2>
<h3>Ngoc Long Nguyen, J&#xe9;r&#xe9;my Anger, Axel Davy, Pablo Arias, Gabriele Facciolo</h3>
<p>The PROBA-V Super-Resolution challenge distributes real low-resolution image
series and corresponding high-resolution targets to advance research on
Multi-Image Super Resolution (MISR) for satellite images. However, in the
PROBA-V dataset the low-resolution image corresponding to the high-resolution
target is not identified. We argue that in doing so, the challenge ranks the
proposed methods not only by their MISR performance, but mainly by the
heuristics used to guess which image in the series is the most similar to the
high-resolution target. We demonstrate this by improving the performance
obtained by the two winners of the challenge only by using a different
reference image, which we compute following a simple heuristic. Based on this,
we propose PROBA-V-REF a variant of the PROBA-V dataset, in which the reference
image in the low-resolution series is provided, and show that the ranking
between the methods changes in this setting. This is relevant to many practical
use cases of MISR where the goal is to super-resolve a specific image of the
series, i.e. the reference is known. The proposed PROBA-V-REF should better
reflect the performance of the different methods for this reference-aware MISR
problem.
</p>
<a href="http://arxiv.org/abs/2101.10200" target="_blank">arXiv:2101.10200</a> [<a href="http://arxiv.org/pdf/2101.10200" target="_blank">pdf</a>]

<h2>Learning Parametrised Graph Shift Operators. (arXiv:2101.10050v1 [cs.LG] CROSS LISTED)</h2>
<h3>George Dasoulas, Johannes Lutzeyer, Michalis Vazirgiannis</h3>
<p>In many domains data is currently represented as graphs and therefore, the
graph representation of this data becomes increasingly important in machine
learning. Network data is, implicitly or explicitly, always represented using a
graph shift operator (GSO) with the most common choices being the adjacency,
Laplacian matrices and their normalisations. In this paper, a novel
parametrised GSO (PGSO) is proposed, where specific parameter values result in
the most commonly used GSOs and message-passing operators in graph neural
network (GNN) frameworks. The PGSO is suggested as a replacement of the
standard GSOs that are used in state-of-the-art GNN architectures and the
optimisation of the PGSO parameters is seamlessly included in the model
training. It is proved that the PGSO has real eigenvalues and a set of real
eigenvectors independent of the parameter values and spectral bounds on the
PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the
graph structure in a study on stochastic blockmodel networks, where they are
found to automatically replicate the GSO regularisation found in the
literature. On several real-world datasets the accuracy of state-of-the-art GNN
architectures is improved by the inclusion of the PGSO in both node- and
graph-classification tasks.
</p>
<a href="http://arxiv.org/abs/2101.10050" target="_blank">arXiv:2101.10050</a> [<a href="http://arxiv.org/pdf/2101.10050" target="_blank">pdf</a>]

