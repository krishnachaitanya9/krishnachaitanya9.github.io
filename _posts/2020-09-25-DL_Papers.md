---
title: Latest Deep Learning Papers
date: 2020-11-10 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (200 Articles)</h1>
<h2>Impedance Optimization for Uncertain Contact Interactions Through Risk Sensitive Optimal Control. (arXiv:2011.04684v1 [cs.RO])</h2>
<h3>Bilal Hammoud, Majid Khadiv, Ludovic Righetti</h3>
<p>This paper addresses the problem of computing optimal impedance schedules for
legged locomotion tasks involving complex contact interactions. We formulate
the problem of impedance regulation as a trade-off between disturbance
rejection and measurement uncertainty. We extend a stochastic optimal control
algorithm known as Risk Sensitive Control to take into account measurement
uncertainty and propose a formal way to include such uncertainty for unknown
contact locations. The approach can efficiently generate optimal state and
control trajectories along with local feedback control gains, i.e. impedance
schedules. Extensive simulations demonstrate the capabilities of the approach
in generating meaningful stiffness and damping modulation patterns before and
after contact interaction. For example, contact forces are reduced during early
contacts, damping increases to anticipate a high impact event and tracking is
automatically traded-off for increased stability. In particular, we show a
significant improvement in performance during jumping and trotting tasks with a
simulated quadruped robot.
</p>
<a href="http://arxiv.org/abs/2011.04684" target="_blank">arXiv:2011.04684</a> [<a href="http://arxiv.org/pdf/2011.04684" target="_blank">pdf</a>]

<h2>DIPN: Deep Interaction Prediction Network with Application to Clutter Removal. (arXiv:2011.04692v1 [cs.RO])</h2>
<h3>Baichuan Huang, Shuai D. Han, Abdeslam Boularias, Jingjin Yu</h3>
<p>We propose a Deep Interaction Prediction Network (DIPN) for learning to
predict complex interactions that ensue as a robot end-effector pushes multiple
objects, whose physical properties, including size, shape, mass, and friction
coefficients may be unknown a priori. DIPN "imagines" the effect of a push
action and generates an accurate synthetic image of the predicted outcome. DIPN
is shown to be sample efficient when trained in simulation or with a real
robotic system. The high accuracy of DIPN allows direct integration with a
grasp network, yielding a robotic manipulation system capable of executing
challenging clutter removal tasks while being trained in a fully
self-supervised manner. The overall network demonstrates intelligent behavior
in selecting proper actions between push and grasp for completing clutter
removal tasks and significantly outperforms the previous state-of-the-art.
Remarkably, DIPN achieves even better performance on the real robotic hardware
system than in simulation. Selected evaluation video clips, code, and
experiments log are available at https://github.com/rutgers-arc-lab/dipn.
</p>
<a href="http://arxiv.org/abs/2011.04692" target="_blank">arXiv:2011.04692</a> [<a href="http://arxiv.org/pdf/2011.04692" target="_blank">pdf</a>]

<h2>Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning. (arXiv:2011.04697v1 [cs.RO])</h2>
<h3>Zhiqian Qiao, Jeff Schneider, John M. Dolan</h3>
<p>For autonomous vehicles, effective behavior planning is crucial to ensure
safety of the ego car. In many urban scenarios, it is hard to create
sufficiently general heuristic rules, especially for challenging scenarios that
some new human drivers find difficult. In this work, we propose a behavior
planning structure based on reinforcement learning (RL) which is capable of
performing autonomous vehicle behavior planning with a hierarchical structure
in simulated urban environments. Application of the hierarchical structure
allows the various layers of the behavior planning system to be satisfied. Our
algorithms can perform better than heuristic-rule-based methods for elective
decisions such as when to turn left between vehicles approaching from the
opposite direction or possible lane-change when approaching an intersection due
to lane blockage or delay in front of the ego car. Such behavior is hard to
evaluate as correct or incorrect, but for some aggressive expert human drivers
handle such scenarios effectively and quickly. On the other hand, compared to
traditional RL methods, our algorithm is more sample-efficient, due to the use
of a hybrid reward mechanism and heuristic exploration during the training
process. The results also show that the proposed method converges to an optimal
policy faster than traditional RL methods.
</p>
<a href="http://arxiv.org/abs/2011.04697" target="_blank">arXiv:2011.04697</a> [<a href="http://arxiv.org/pdf/2011.04697" target="_blank">pdf</a>]

<h2>AI Poincar\'e: Machine Learning Conservation Laws from Trajectories. (arXiv:2011.04698v1 [cs.LG])</h2>
<h3>Ziming Liu (MIT), Max Tegmark (MIT)</h3>
<p>We present AI Poincar\'e, a machine learning algorithm for auto-discovering
conserved quantities using trajectory data from unknown dynamical systems. We
test it on five Hamiltonian systems, including the gravitational 3-body
problem, and find that it discovers not only all exactly conserved quantities,
but also periodic orbits, phase transitions and breakdown timescales for
approximate conservation laws.
</p>
<a href="http://arxiv.org/abs/2011.04698" target="_blank">arXiv:2011.04698</a> [<a href="http://arxiv.org/pdf/2011.04698" target="_blank">pdf</a>]

<h2>Safe Trajectory Planning Using Reinforcement Learning for Self Driving. (arXiv:2011.04702v1 [cs.RO])</h2>
<h3>Josiah Coad, Zhiqian Qiao, John M. Dolan</h3>
<p>Self-driving vehicles must be able to act intelligently in diverse and
difficult environments, marked by high-dimensional state spaces, a myriad of
optimization objectives and complex behaviors. Traditionally, classical
optimization and search techniques have been applied to the problem of
self-driving; but they do not fully address operations in environments with
high-dimensional states and complex behaviors. Recently, imitation learning has
been proposed for the task of self-driving; but it is labor-intensive to obtain
enough training data. Reinforcement learning has been proposed as a way to
directly control the car, but this has safety and comfort concerns. We propose
using model-free reinforcement learning for the trajectory planning stage of
self-driving and show that this approach allows us to operate the car in a more
safe, general and comfortable manner, required for the task of self driving.
</p>
<a href="http://arxiv.org/abs/2011.04702" target="_blank">arXiv:2011.04702</a> [<a href="http://arxiv.org/pdf/2011.04702" target="_blank">pdf</a>]

<h2>f-IRL: Inverse Reinforcement Learning via State Marginal Matching. (arXiv:2011.04709v1 [cs.LG])</h2>
<h3>Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, Benjamin Eysenbach</h3>
<p>Imitation learning is well-suited for robotic tasks where it is difficult to
directly program the behavior or specify a cost for optimal control. In this
work, we propose a method for learning the reward function (and the
corresponding policy) to match the expert state density. Our main result is the
analytic gradient of any f-divergence between the agent and expert state
distribution w.r.t. reward parameters. Based on the derived gradient, we
present an algorithm, f-IRL, that recovers a stationary reward function from
the expert density by gradient descent. We show that f-IRL can learn behaviors
from a hand-designed target state density or implicitly through expert
observations. Our method outperforms adversarial imitation learning methods in
terms of sample efficiency and the required number of expert trajectories on
IRL benchmarks. Moreover, we show that the recovered reward function can be
used to quickly solve downstream tasks, and empirically demonstrate its utility
on hard-to-explore tasks and for behavior transfer across changes in dynamics.
</p>
<a href="http://arxiv.org/abs/2011.04709" target="_blank">arXiv:2011.04709</a> [<a href="http://arxiv.org/pdf/2011.04709" target="_blank">pdf</a>]

<h2>Ontology-driven Event Type Classification in Images. (arXiv:2011.04714v1 [cs.CV])</h2>
<h3>Eric M&#xfc;ller-Budack, Matthias Springstein, Sherzod Hakimov, Kevin Mrutzek, Ralph Ewerth</h3>
<p>Event classification can add valuable information for semantic search and the
increasingly important topic of fact validation in news. So far, only few
approaches address image classification for newsworthy event types such as
natural disasters, sports events, or elections. Previous work distinguishes
only between a limited number of event types and relies on rather small
datasets for training. In this paper, we present a novel ontology-driven
approach for the classification of event types in images. We leverage a large
number of real-world news events to pursue two objectives: First, we create an
ontology based on Wikidata comprising the majority of event types. Second, we
introduce a novel large-scale dataset that was acquired through Web crawling.
Several baselines are proposed including an ontology-driven learning approach
that aims to exploit structured information of a knowledge graph to learn
relevant event relations using deep neural networks. Experimental results on
existing as well as novel benchmark datasets demonstrate the superiority of the
proposed ontology-driven approach.
</p>
<a href="http://arxiv.org/abs/2011.04714" target="_blank">arXiv:2011.04714</a> [<a href="http://arxiv.org/pdf/2011.04714" target="_blank">pdf</a>]

<h2>Real-time Locational Marginal Price Forecasting Using Generative Adversarial Network. (arXiv:2011.04717v1 [cs.LG])</h2>
<h3>Zhongxia Zhang, Meng Wu</h3>
<p>In this paper, we propose a model-free unsupervised learning approach to
forecast real-time locational marginal prices (RTLMPs) in wholesale electricity
markets. By organizing system-wide hourly RTLMP data into a 3-dimensional (3D)
tensor consisting of a series of time-indexed matrices, we formulate the RTLMP
forecasting problem as a problem of generating the next matrix with forecasted
RTLMPs given the historical RTLMP tensor, and propose a generative adversarial
network (GAN) model to forecast RTLMPs. The proposed formulation preserves the
spatio-temporal correlations among system-wide RTLMPs in the format of
historical RTLMP tensor. The proposed GAN model learns the spatio-temporal
correlations using the historical RTLMP tensors and generate RTLMPs that are
statistically similar and temporally coherent to the historical RTLMP tensor.
The proposed approach forecasts system-wide RTLMPs using only publicly
available historical price data, without involving confidential information of
system model, such as system parameters, topology, or operating conditions. The
effectiveness of the proposed approach is verified through case studies using
historical RTLMP data in Southwest Power Pool (SPP).
</p>
<a href="http://arxiv.org/abs/2011.04717" target="_blank">arXiv:2011.04717</a> [<a href="http://arxiv.org/pdf/2011.04717" target="_blank">pdf</a>]

<h2>Improving Neural Network Training in Low Dimensional Random Bases. (arXiv:2011.04720v1 [cs.LG])</h2>
<h3>Frithjof Gressmann, Zach Eaton-Rosen, Carlo Luschi</h3>
<p>Stochastic Gradient Descent (SGD) has proven to be remarkably effective in
optimizing deep neural networks that employ ever-larger numbers of parameters.
Yet, improving the efficiency of large-scale optimization remains a vital and
highly active area of research. Recent work has shown that deep neural networks
can be optimized in randomly-projected subspaces of much smaller dimensionality
than their native parameter space. While such training is promising for more
efficient and scalable optimization schemes, its practical application is
limited by inferior optimization performance. Here, we improve on recent random
subspace approaches as follows: Firstly, we show that keeping the random
projection fixed throughout training is detrimental to optimization. We propose
re-drawing the random subspace at each step, which yields significantly better
performance. We realize further improvements by applying independent
projections to different parts of the network, making the approximation more
efficient as network dimensionality grows. To implement these experiments, we
leverage hardware-accelerated pseudo-random number generation to construct the
random projections on-demand at every optimization step, allowing us to
distribute the computation of independent random directions across multiple
workers with shared random seeds. This yields significant reductions in memory
and is up to 10 times faster for the workloads in question.
</p>
<a href="http://arxiv.org/abs/2011.04720" target="_blank">arXiv:2011.04720</a> [<a href="http://arxiv.org/pdf/2011.04720" target="_blank">pdf</a>]

<h2>TrimTuner: Efficient Optimization of Machine Learning Jobs in the Cloud via Sub-Sampling. (arXiv:2011.04726v1 [cs.LG])</h2>
<h3>Pedro Mendes, Maria Casimiro, Paolo Romano, David Garlan</h3>
<p>This work introduces TrimTuner, the first system for optimizing machine
learning jobs in the cloud to exploit sub-sampling techniques to reduce the
cost of the optimization process while keeping into account user-specified
constraints. TrimTuner jointly optimizes the cloud and application-specific
parameters and, unlike state of the art works for cloud optimization, eschews
the need to train the model with the full training set every time a new
configuration is sampled. Indeed, by leveraging sub-sampling techniques and
data-sets that are up to 60x smaller than the original one, we show that
TrimTuner can reduce the cost of the optimization process by up to 50x.
Further, TrimTuner speeds-up the recommendation process by 65x with respect to
state of the art techniques for hyper-parameter optimization that use
sub-sampling techniques. The reasons for this improvement are twofold: i) a
novel domain specific heuristic that reduces the number of configurations for
which the acquisition function has to be evaluated; ii) the adoption of an
ensemble of decision trees that enables boosting the speed of the
recommendation process by one additional order of magnitude.
</p>
<a href="http://arxiv.org/abs/2011.04726" target="_blank">arXiv:2011.04726</a> [<a href="http://arxiv.org/pdf/2011.04726" target="_blank">pdf</a>]

<h2>Similarity-Based Clustering for Enhancing Image Classification Architectures. (arXiv:2011.04728v1 [cs.CV])</h2>
<h3>Dishant Parikh, Shambhavi Aggarwal</h3>
<p>Convolutional networks are at the center of best in class computer vision
applications for a wide assortment of undertakings. Since 2014, profound amount
of work began to make better convolutional architectures, yielding generous
additions in different benchmarks. Albeit expanded model size and computational
cost will, in general, mean prompt quality increases for most undertakings but,
the architectures now need to have some additional information to increase the
performance. We show empirical evidence that with the amalgamation of
content-based image similarity and deep learning models, we can provide the
flow of information which can be used in making clustered learning possible. We
show how parallel training of sub-dataset clusters not only reduces the cost of
computation but also increases the benchmark accuracies by 5-11 percent.
</p>
<a href="http://arxiv.org/abs/2011.04728" target="_blank">arXiv:2011.04728</a> [<a href="http://arxiv.org/pdf/2011.04728" target="_blank">pdf</a>]

<h2>Learning Task Space Actions for Bipedal Locomotion. (arXiv:2011.04741v1 [cs.RO])</h2>
<h3>Helei Duan, Jeremy Dao, Kevin Green, Taylor Apgar, Alan Fern, Jonathan Hurst</h3>
<p>Recent work has demonstrated the success of reinforcement learning (RL) for
training bipedal locomotion policies for real robots. This prior work, however,
has focused on learning joint-coordination controllers based on an objective of
following joint trajectories produced by already available controllers. As
such, it is difficult to train these approaches to achieve higher-level goals
of legged locomotion, such as simply specifying the desired end-effector foot
movement or ground reaction forces. In this work, we propose an approach for
integrating knowledge of the robot system into RL to allow for learning at the
level of task space actions in terms of feet setpoints. In particular, we
integrate learning a task space policy with a model-based inverse dynamics
controller, which translates task space actions into joint-level controls. With
this natural action space for learning locomotion, the approach is more sample
efficient and produces desired task space dynamics compared to learning purely
joint space actions. We demonstrate the approach in simulation and also show
that the learned policies are able to transfer to the real bipedal robot
Cassie. This result encourages further research towards incorporating bipedal
control techniques into the structure of the learning process to enable dynamic
behaviors.
</p>
<a href="http://arxiv.org/abs/2011.04741" target="_blank">arXiv:2011.04741</a> [<a href="http://arxiv.org/pdf/2011.04741" target="_blank">pdf</a>]

<h2>Personalized Query Rewriting in Conversational AI Agents. (arXiv:2011.04748v1 [cs.AI])</h2>
<h3>Alireza Roshan-Ghias, Clint Solomon Mathialagan, Pragaash Ponnusamy, Lambert Mathias, Chenlei Guo</h3>
<p>Spoken language understanding (SLU) systems in conversational AI agents often
experience errors in the form of misrecognitions by automatic speech
recognition (ASR) or semantic gaps in natural language understanding (NLU).
These errors easily translate to user frustrations, particularly so in
recurrent events e.g. regularly toggling an appliance, calling a frequent
contact, etc. In this work, we propose a query rewriting approach by leveraging
users' historically successful interactions as a form of memory. We present a
neural retrieval model and a pointer-generator network with hierarchical
attention and show that they perform significantly better at the query
rewriting task with the aforementioned user memories than without. We also
highlight how our approach with the proposed models leverages the structural
and semantic diversity in ASR's output towards recovering users' intents.
</p>
<a href="http://arxiv.org/abs/2011.04748" target="_blank">arXiv:2011.04748</a> [<a href="http://arxiv.org/pdf/2011.04748" target="_blank">pdf</a>]

<h2>Longitudinal modeling of MS patient trajectories improves predictions of disability progression. (arXiv:2011.04749v1 [cs.LG])</h2>
<h3>Edward De Brouwer, Thijs Becker, Yves Moreau, Eva Kubala Havrdova, Maria Trojano, Sara Eichau, Serkan Ozakbas, Marco Onofrj, Pierre Grammond, Jens Kuhle, Ludwig Kappos, Patrizia Sola, Elisabetta Cartechini, Jeannette Lechner-Scott, Raed Alroughani, Oliver Gerlach, Tomas Kalincik, Franco Granella, Francois GrandMaison, Roberto Bergamaschi, Maria Jose Sa, Bart Van Wijmeersch, Aysun Soysal, Jose Luis Sanchez-Menoyo, Claudio Solaro, Cavit Boz, Gerardo Iuliano, Katherine Buzzard, Eduardo Aguera-Morales, Murat Terzi, Tamara Castillo Trivio, Daniele Spitaleri, Vincent Van Pesch, Vahid Shaygannej, Fraser Moore, Celia Oreja Guevara, Davide Maimone, Riadh Gouider, Tunde Csepany, Cristina Ramo-Tello, Liesbet Peeters</h3>
<p>Research in Multiple Sclerosis (MS) has recently focused on extracting
knowledge from real-world clinical data sources. This type of data is more
abundant than data produced during clinical trials and potentially more
informative about real-world clinical practice. However, this comes at the cost
of less curated and controlled data sets. In this work, we address the task of
optimally extracting information from longitudinal patient data in the
real-world setting with a special focus on the sporadic sampling problem. Using
the MSBase registry, we show that with machine learning methods suited for
patient trajectories modeling, such as recurrent neural networks and tensor
factorization, we can predict disability progression of patients in a two-year
horizon with an ROC-AUC of 0.86, which represents a 33% decrease in the ranking
pair error (1-AUC) compared to reference methods using static clinical
features. Compared to the models available in the literature, this work uses
the most complete patient history for MS disease progression prediction.
</p>
<a href="http://arxiv.org/abs/2011.04749" target="_blank">arXiv:2011.04749</a> [<a href="http://arxiv.org/pdf/2011.04749" target="_blank">pdf</a>]

<h2>Predicting the Future is like Completing a Painting!. (arXiv:2011.04750v1 [cs.AI])</h2>
<h3>Nadir Maaroufi, Mehdi Najib, Mohamed Bakhouya</h3>
<p>This article is an introductory work towards a larger research framework
relative to Scientific Prediction. It is a mixed between science and philosophy
of science, therefore we can talk about Experimental Philosophy of Science. As
a first result, we introduce a new forecasting method based on image
completion, named Forecasting Method by Image Inpainting (FM2I). In fact, time
series forecasting is transformed into fully images- and signal-based
processing procedures. After transforming a time series data into its
corresponding image, the problem of data forecasting becomes essentially a
problem of image inpainting problem, i.e., completing missing data in the
image. An extensive experimental evaluation is conducted using a large dataset
proposed by the well-known M3-competition. Results show that FM2I represents an
efficient and robust tool for time series forecasting. It has achieved
prominent results in terms of accuracy and outperforms the best M3 forecasting
methods.
</p>
<a href="http://arxiv.org/abs/2011.04750" target="_blank">arXiv:2011.04750</a> [<a href="http://arxiv.org/pdf/2011.04750" target="_blank">pdf</a>]

<h2>Trajectory Planning for Autonomous Vehicles Using Hierarchical Reinforcement Learning. (arXiv:2011.04752v1 [cs.RO])</h2>
<h3>Kaleb Ben Naveed, Zhiqian Qiao, John M. Dolan</h3>
<p>Planning safe trajectories under uncertain and dynamic conditions makes the
autonomous driving problem significantly complex. Current sampling-based
methods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this
problem because of the high computational cost. Supervised learning methods
such as Imitation Learning lack generalization and safety guarantees. To
address these problems and in order to ensure a robust framework, we propose a
Hierarchical Reinforcement Learning (HRL) structure combined with a
Proportional-Integral-Derivative (PID) controller for trajectory planning. HRL
helps divide the task of autonomous vehicle driving into sub-goals and supports
the network to learn policies for both high-level options and low-level
trajectory planner choices. The introduction of sub-goals decreases convergence
time and enables the policies learned to be reused for other scenarios. In
addition, the proposed planner is made robust by guaranteeing smooth
trajectories and by handling the noisy perception system of the ego-car. The
PID controller is used for tracking the waypoints, which ensures smooth
trajectories and reduces jerk. The problem of incomplete observations is
handled by using a Long-Short-Term-Memory (LSTM) layer in the network. Results
from the high-fidelity CARLA simulator indicate that the proposed method
reduces convergence time, generates smoother trajectories, and is able to
handle dynamic surroundings and noisy observations.
</p>
<a href="http://arxiv.org/abs/2011.04752" target="_blank">arXiv:2011.04752</a> [<a href="http://arxiv.org/pdf/2011.04752" target="_blank">pdf</a>]

<h2>Learning to Infer Semantic Parameters for 3D Shape Editing. (arXiv:2011.04755v1 [cs.CV])</h2>
<h3>Fangyin Wei, Elena Sizikova, Avneesh Sud, Szymon Rusinkiewicz, Thomas Funkhouser</h3>
<p>Many applications in 3D shape design and augmentation require the ability to
make specific edits to an object's semantic parameters (e.g., the pose of a
person's arm or the length of an airplane's wing) while preserving as much
existing details as possible. We propose to learn a deep network that infers
the semantic parameters of an input shape and then allows the user to
manipulate those parameters. The network is trained jointly on shapes from an
auxiliary synthetic template and unlabeled realistic models, ensuring
robustness to shape variability while relieving the need to label realistic
exemplars. At testing time, edits within the parameter space drive deformations
to be applied to the original shape, which provides semantically-meaningful
manipulation while preserving the details. This is in contrast to prior methods
that either use autoencoders with a limited latent-space dimensionality,
failing to preserve arbitrary detail, or drive deformations with
purely-geometric controls, such as cages, losing the ability to update local
part regions. Experiments with datasets of chairs, airplanes, and human bodies
demonstrate that our method produces more natural edits than prior work.
</p>
<a href="http://arxiv.org/abs/2011.04755" target="_blank">arXiv:2011.04755</a> [<a href="http://arxiv.org/pdf/2011.04755" target="_blank">pdf</a>]

<h2>MUSE: Illustrating Textual Attributes by Portrait Generation. (arXiv:2011.04761v1 [cs.CV])</h2>
<h3>Xiaodan Hu, Pengfei Yu, Kevin Knight, Heng Ji, Bo Li, Honghui Shi</h3>
<p>We propose a novel approach, MUSE, to illustrate textual attributes visually
via portrait generation. MUSE takes a set of attributes written in text, in
addition to facial features extracted from a photo of the subject as input. We
propose 11 attribute types to represent inspirations from a subject's profile,
emotion, story, and environment. We propose a novel stacked neural network
architecture by extending an image-to-image generative model to accept textual
attributes. Experiments show that our approach significantly outperforms
several state-of-the-art methods without using textual attributes, with
Inception Score score increased by 6% and Fr\'echet Inception Distance (FID)
score decreased by 11%, respectively. We also propose a new attribute
reconstruction metric to evaluate whether the generated portraits preserve the
subject's attributes. Experiments show that our approach can accurately
illustrate 78% textual attributes, which also help MUSE capture the subject in
a more creative and expressive way.
</p>
<a href="http://arxiv.org/abs/2011.04761" target="_blank">arXiv:2011.04761</a> [<a href="http://arxiv.org/pdf/2011.04761" target="_blank">pdf</a>]

<h2>Predicting Landsat Reflectance with Deep Generative Fusion. (arXiv:2011.04762v1 [cs.CV])</h2>
<h3>Shahine Bouabid, Maxim Chernetskiy, Maxime Rischard, Jevgenij Gamper</h3>
<p>Public satellite missions are commonly bound to a trade-off between spatial
and temporal resolution as no single sensor provides fine-grained acquisitions
with frequent coverage. This hinders their potential to assist vegetation
monitoring or humanitarian actions, which require detecting rapid and detailed
terrestrial surface changes. In this work, we probe the potential of deep
generative models to produce high-resolution optical imagery by fusing products
with different spatial and temporal characteristics. We introduce a dataset of
co-registered Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat
surface reflectance time series and demonstrate the ability of our generative
model to blend coarse daily reflectance information into low-paced finer
acquisitions. We benchmark our proposed model against state-of-the-art
reflectance fusion algorithms.
</p>
<a href="http://arxiv.org/abs/2011.04762" target="_blank">arXiv:2011.04762</a> [<a href="http://arxiv.org/pdf/2011.04762" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Navigation in AAA Video Games. (arXiv:2011.04764v1 [cs.LG])</h2>
<h3>Eloi Alonso, Maxim Peter, David Goumard, Joshua Romoff</h3>
<p>In video games, non-player characters (NPCs) are used to enhance the players'
experience in a variety of ways, e.g., as enemies, allies, or innocent
bystanders. A crucial component of NPCs is navigation, which allows them to
move from one point to another on the map. The most popular approach for NPC
navigation in the video game industry is to use a navigation mesh (NavMesh),
which is a graph representation of the map, with nodes and edges indicating
traversable areas. Unfortunately, complex navigation abilities that extend the
character's capacity for movement, e.g., grappling hooks, jetpacks,
teleportation, or double-jumps, increases the complexity of the NavMesh, making
it intractable in many practical scenarios. Game designers are thus constrained
to only add abilities that can be handled by a NavMesh if they want to have NPC
navigation. As an alternative, we propose to use Deep Reinforcement Learning
(Deep RL) to learn how to navigate 3D maps using any navigation ability. We
test our approach on complex 3D environments in the Unity game engine that are
notably an order of magnitude larger than maps typically used in the Deep RL
literature. One of these maps is directly modeled after a Ubisoft AAA game. We
find that our approach performs surprisingly well, achieving at least $90\%$
success rate on all tested scenarios. A video of our results is available at
https://youtu.be/WFIf9Wwlq8M.
</p>
<a href="http://arxiv.org/abs/2011.04764" target="_blank">arXiv:2011.04764</a> [<a href="http://arxiv.org/pdf/2011.04764" target="_blank">pdf</a>]

<h2>Singular Sturm-Liouville Problems with Zero Potential (q=0) and Singular Slow Feature Analysis. (arXiv:2011.04765v1 [cs.LG])</h2>
<h3>Stefan Richthofer, Laurenz Wiskott</h3>
<p>A Sturm-Liouville problem ($\lambda wy=(ry')'+qy$) is singular if its domain
is unbounded or if $r$ or $w$ vanish at the boundary. Then it is difficult to
tell whether profound results from regular Sturm-Liouville theory apply.
Existing criteria are often difficult to apply, e.g. because they are
formulated in terms of the solution function.

We study the special case that the potential $q$ is zero under Neumann
boundary conditions and give simple and explicit criteria, solely in terms of
the coefficient functions, to assess whether various properties of the regular
case apply. Specifically, these properties are discreteness of the spectrum
(BD), self-adjointness, oscillation ($i$th solution has $i$ zeros) and that the
$i$th eigenvalue equals the SFA delta value (the total energy) of the $i$th
solution. We further prove that stationary points of each solution strictly
interlace with its zeros (in singular or regular case, regardless of the
boundary condition, for zero potential or if $q &lt; \lambda w$ everywhere). If
$\frac{r}{w}$ is bounded and of bounded variation, the criterion simplifies to
requiring $\frac{|w'|}{w} \to \infty$ at singular boundary points.

This research is motivated by Slow Feature Analysis (SFA), a data processing
algorithm that extracts the slowest uncorrelated signals from a
high-dimensional input signal and has notable success in computer vision,
computational neuroscience and blind source separation. From [Sprekeler et al.,
2014] it is known that for an important class of scenarios (statistically
independent input), an analytic formulation of SFA reduces to a Sturm-Liouville
problem with zero potential and Neumann boundary conditions. So far, the
mathematical SFA theory has only considered the regular case, except for a
special case that is solved by Hermite Polynomials. This work generalizes SFA
theory to the singular case, i.e. open-space scenarios.
</p>
<a href="http://arxiv.org/abs/2011.04765" target="_blank">arXiv:2011.04765</a> [<a href="http://arxiv.org/pdf/2011.04765" target="_blank">pdf</a>]

<h2>Deep Bayesian Nonparametric Factor Analysis. (arXiv:2011.04770v1 [stat.ML])</h2>
<h3>Arunesh Mittal, Paul Sajda, John Paisley</h3>
<p>We propose a deep generative factor analysis model with beta process prior
that can approximate complex non-factorial distributions over the latent codes.
We outline a stochastic EM algorithm for scalable inference in a specific
instantiation of this model and present some preliminary results.
</p>
<a href="http://arxiv.org/abs/2011.04770" target="_blank">arXiv:2011.04770</a> [<a href="http://arxiv.org/pdf/2011.04770" target="_blank">pdf</a>]

<h2>Towards One-Dollar Robots: An Integrated Design and Fabrication Strategy for Electromechanical Systems. (arXiv:2011.04772v1 [cs.RO])</h2>
<h3>Wenzhong Yan, Ankur Mehta</h3>
<p>To improve the accessibility of robotics, we propose a design and fabrication
strategy to build low-cost electromechanical systems for robotic devices. Our
method, based on origami-inspired cut-and-fold and E-textiles techniques, aims
at minimizing the resources for robot creation. Specifically, we explore
techniques to create robots with the resources restricted to single-layer
sheets (e.g. polyester film) and conductive sewing threads. To demonstrate our
strategy's feasibility, these techniques are successfully integrated into an
electromechanical oscillator (about 0.40 USD), which can generate electrical
oscillation under constant-current power and potentially be used as a simple
robot controller in lieu of additional external electronics.
</p>
<a href="http://arxiv.org/abs/2011.04772" target="_blank">arXiv:2011.04772</a> [<a href="http://arxiv.org/pdf/2011.04772" target="_blank">pdf</a>]

<h2>After All, Only The Last Neuron Matters: Comparing Multi-modal Fusion Functions for Scene Graph Generation. (arXiv:2011.04779v1 [cs.CV])</h2>
<h3>Mohamed Karim Belaid</h3>
<p>From object segmentation to word vector representations, Scene Graph
Generation (SGG) became a complex task built upon numerous research results. In
this paper, we focus on the last module of this model: the fusion function. The
role of this latter is to combine three hidden states. We perform an ablation
test in order to compare different implementations. First, we reproduce the
state-of-the-art results using SUM, and GATE functions. Then we expand the
original solution by adding more model-agnostic functions: an adapted version
of DIST and a mixture between MFB and GATE. On the basis of the
state-of-the-art configuration, DIST performed the best Recall @ K, which makes
it now part of the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2011.04779" target="_blank">arXiv:2011.04779</a> [<a href="http://arxiv.org/pdf/2011.04779" target="_blank">pdf</a>]

<h2>Planning under Uncertainty to Goal Distributions. (arXiv:2011.04782v1 [cs.RO])</h2>
<h3>Adam Conkey, Tucker Hermans</h3>
<p>Goal spaces for planning problems are typically conceived of as subsets of
the state space. It is common to select a particular goal state to plan to, and
the agent monitors its progress to the goal with a distance function defined
over the state space. Due to numerical imprecision, state uncertainty, and
stochastic dynamics, the agent will be unable to arrive at a particular state
in a verifiable manner. It is therefore common to consider a goal achieved if
the agent reaches a state within a small distance threshold to the goal. This
approximation fails to explicitly account for the agent's state uncertainty.
Point-based goals further do not accommodate goal uncertainty that arises when
goals are estimated in a data-driven way. We argue that goal distributions are
a more appropriate goal representation and present a novel approach to planning
under uncertainty to goal distributions. We use the unscented transform to
propagate state uncertainty under stochastic dynamics and use cross-entropy
method to minimize the Kullback-Leibler divergence between the current state
distribution and the goal distribution. We derive reductions of our cost
function to commonly used goal-reaching costs such as weighted Euclidean
distance, goal set indicators, chance-constrained goal sets, and maximum
expectation of reaching a goal point. We explore different combinations of goal
distributions, planner distributions, and divergence to illustrate behaviors
achievable in our framework.
</p>
<a href="http://arxiv.org/abs/2011.04782" target="_blank">arXiv:2011.04782</a> [<a href="http://arxiv.org/pdf/2011.04782" target="_blank">pdf</a>]

<h2>Lifelong Learning Without a Task Oracle. (arXiv:2011.04783v1 [cs.LG])</h2>
<h3>Amanda Rios, Laurent Itti</h3>
<p>Supervised deep neural networks are known to undergo a sharp decline in the
accuracy of older tasks when new tasks are learned, termed "catastrophic
forgetting". Many state-of-the-art solutions to continual learning rely on
biasing and/or partitioning a model to accommodate successive tasks
incrementally. However, these methods largely depend on the availability of a
task-oracle to confer task identities to each test sample, without which the
models are entirely unable to perform. To address this shortcoming, we propose
and compare several candidate task-assigning mappers which require very little
memory overhead: (1) Incremental unsupervised prototype assignment using either
nearest means, Gaussian Mixture Models or fuzzy ART backbones; (2) Supervised
incremental prototype assignment with fast fuzzy ARTMAP; (3) Shallow perceptron
trained via a dynamic coreset. Our proposed model variants are trained either
from pre-trained feature extractors or task-dependent feature embeddings of the
main classifier network. We apply these pipeline variants to continual learning
benchmarks, comprised of either sequences of several datasets or within one
single dataset. Overall, these methods, despite their simplicity and
compactness, perform very close to a ground truth oracle, especially in
experiments of inter-dataset task assignment. Moreover, best-performing
variants only impose an average cost of 1.7% parameter memory increase.
</p>
<a href="http://arxiv.org/abs/2011.04783" target="_blank">arXiv:2011.04783</a> [<a href="http://arxiv.org/pdf/2011.04783" target="_blank">pdf</a>]

<h2>Modeling Trust in Human-Robot Interaction: A Survey. (arXiv:2011.04796v1 [cs.RO])</h2>
<h3>Zahra Rezaei Khavas, Reza Ahmadzadeh, Paul Robinette</h3>
<p>As the autonomy and capabilities of robotic systems increase, they are
expected to play the role of teammates rather than tools and interact with
human collaborators in a more realistic manner, creating a more human-like
relationship. Given the impact of trust observed in human-robot interaction
(HRI), appropriate trust in robotic collaborators is one of the leading factors
influencing the performance of human-robot interaction. Team performance can be
diminished if people do not trust robots appropriately by disusing or misusing
them based on limited experience. Therefore, trust in HRI needs to be
calibrated properly, rather than maximized, to let the formation of an
appropriate level of trust in human collaborators. For trust calibration in
HRI, trust needs to be modeled first. There are many reviews on factors
affecting trust in HRI, however, as there are no reviews concentrated on
different trust models, in this paper, we review different techniques and
methods for trust modeling in HRI. We also present a list of potential
directions for further research and some challenges that need to be addressed
in future work on human-robot trust modeling.
</p>
<a href="http://arxiv.org/abs/2011.04796" target="_blank">arXiv:2011.04796</a> [<a href="http://arxiv.org/pdf/2011.04796" target="_blank">pdf</a>]

<h2>Attentive Social Recommendation: Towards User And Item Diversities. (arXiv:2011.04797v1 [cs.AI])</h2>
<h3>Dongsheng Luo, Yuchen Bian, Xiang Zhang, Jun Huan</h3>
<p>Social recommendation system is to predict unobserved user-item rating values
by taking advantage of user-user social relation and user-item ratings.
However, user/item diversities in social recommendations are not well utilized
in the literature. Especially, inter-factor (social and rating factors)
relations and distinct rating values need taking into more consideration. In
this paper, we propose an attentive social recommendation system (ASR) to
address this issue from two aspects. First, in ASR, Rec-conv graph network
layers are proposed to extract the social factor, user-rating and item-rated
factors and then automatically assign contribution weights to aggregate these
factors into the user/item embedding vectors. Second, a disentangling strategy
is applied for diverse rating values. Extensive experiments on benchmarks
demonstrate the effectiveness and advantages of our ASR.
</p>
<a href="http://arxiv.org/abs/2011.04797" target="_blank">arXiv:2011.04797</a> [<a href="http://arxiv.org/pdf/2011.04797" target="_blank">pdf</a>]

<h2>Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. (arXiv:2011.04798v1 [stat.ML])</h2>
<h3>Ding Zhou, Xue-Xin Wei</h3>
<p>The ability to record activities from hundreds of neurons simultaneously in
the brain has placed an increasing demand for developing appropriate
statistical techniques to analyze such data. Recently, deep generative models
have been proposed to fit neural population responses. While these methods are
flexible and expressive, the downside is that they can be difficult to
interpret and identify. To address this problem, we propose a method that
integrates key ingredients from latent models and traditional neural encoding
models. Our method, pi-VAE, is inspired by recent progress on identifiable
variational auto-encoder, which we adapt to make appropriate for neuroscience
applications. Specifically, we propose to construct latent variable models of
neural activity while simultaneously modeling the relation between the latent
and task variables (non-neural variables, e.g. sensory, motor, and other
externally observable states). The incorporation of task variables results in
models that are not only more constrained, but also show qualitative
improvements in interpretability and identifiability. We validate pi-VAE using
synthetic data, and apply it to analyze neurophysiological datasets from rat
hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits
the data better, but also provides unexpected novel insights into the structure
of the neural codes.
</p>
<a href="http://arxiv.org/abs/2011.04798" target="_blank">arXiv:2011.04798</a> [<a href="http://arxiv.org/pdf/2011.04798" target="_blank">pdf</a>]

<h2>Sparse Longitudinal Representations of Electronic Health Record Data for the Early Detection of Chronic Kidney Disease in Diabetic Patients. (arXiv:2011.04802v1 [cs.LG])</h2>
<h3>Jinghe Zhang, Kamran Kowsari, Mehdi Boukhechba, James Harrison, Jennifer Lobo, Laura Barnes</h3>
<p>Chronic kidney disease (CKD) is a gradual loss of renal function over time,
and it increases the risk of mortality, decreased quality of life, as well as
serious complications. The prevalence of CKD has been increasing in the last
couple of decades, which is partly due to the increased prevalence of diabetes
and hypertension. To accurately detect CKD in diabetic patients, we propose a
novel framework to learn sparse longitudinal representations of patients'
medical records. The proposed method is also compared with widely used
baselines such as Aggregated Frequency Vector and Bag-of-Pattern in Sequences
on real EHR data, and the experimental results indicate that the proposed model
achieves higher predictive performance. Additionally, the learned
representations are interpreted and visualized to bring clinical insights.
</p>
<a href="http://arxiv.org/abs/2011.04802" target="_blank">arXiv:2011.04802</a> [<a href="http://arxiv.org/pdf/2011.04802" target="_blank">pdf</a>]

<h2>Self-Tuning Stochastic Optimization with Curvature-Aware Gradient Filtering. (arXiv:2011.04803v1 [cs.LG])</h2>
<h3>Ricky T. Q. Chen, Dami Choi, Lukas Balles, David Duvenaud, Philipp Hennig</h3>
<p>Standard first-order stochastic optimization algorithms base their updates
solely on the average mini-batch gradient, and it has been shown that tracking
additional quantities such as the curvature can help de-sensitize common
hyperparameters. Based on this intuition, we explore the use of exact
per-sample Hessian-vector products and gradients to construct optimizers that
are self-tuning and hyperparameter-free. Based on a dynamics model of the
gradient, we derive a process which leads to a curvature-corrected,
noise-adaptive online gradient estimate. The smoothness of our updates makes it
more amenable to simple step size selection schemes, which we also base off of
our estimates quantities. We prove that our model-based procedure converges in
the noisy quadratic setting. Though we do not see similar gains in deep
learning tasks, we can match the performance of well-tuned optimizers and
ultimately, this is an interesting step for constructing self-tuning
optimizers.
</p>
<a href="http://arxiv.org/abs/2011.04803" target="_blank">arXiv:2011.04803</a> [<a href="http://arxiv.org/pdf/2011.04803" target="_blank">pdf</a>]

<h2>ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes. (arXiv:2011.04812v1 [cs.RO])</h2>
<h3>Kejun Li, Maegan Tucker, Erdem B&#x131;y&#x131;k, Ellen Novoseller, Joel W. Burdick, Yanan Sui, Dorsa Sadigh, Yisong Yue, Aaron D. Ames</h3>
<p>Characterizing what types of exoskeleton gaits are comfortable for users, and
understanding the science of walking more generally, require recovering a
user's utility landscape. Learning these landscapes is challenging, as walking
trajectories are defined by numerous gait parameters, data collection from
human trials is expensive, and user safety and comfort must be ensured. This
work proposes the Region of Interest Active Learning (ROIAL) framework, which
actively learns each user's underlying utility function over a region of
interest that ensures safety and comfort. ROIAL learns from ordinal and
preference feedback, which are more reliable feedback mechanisms than absolute
numerical scores. The algorithm's performance is evaluated both in simulation
and experimentally for three able-bodied subjects walking inside of a
lower-body exoskeleton. ROIAL learns Bayesian posteriors that predict each
exoskeleton user's utility landscape across four exoskeleton gait parameters.
The algorithm discovers both commonalities and discrepancies across users' gait
preferences and identifies the gait parameters that most influenced user
feedback. These results demonstrate the feasibility of recovering gait utility
landscapes from limited human trials.
</p>
<a href="http://arxiv.org/abs/2011.04812" target="_blank">arXiv:2011.04812</a> [<a href="http://arxiv.org/pdf/2011.04812" target="_blank">pdf</a>]

<h2>Bimanual Regrasping for Suture Needles using Reinforcement Learning for Rapid Motion Planning. (arXiv:2011.04813v1 [cs.RO])</h2>
<h3>Zih-Yun Chiu, Florian Richter, Emily K. Funk, Ryan K. Orosco, Michael C. Yip</h3>
<p>Regrasping a suture needle is an important process in suturing, and previous
study has shown that it takes on average 7.4s before the needle is thrown
again. To bring efficiency into suturing, prior work either designs a
task-specific mechanism or guides the gripper toward some specific pick-up
point for proper grasping of a needle. Yet, these methods are usually not
deployable when the working space is changed. These prior efforts highlight the
need for more efficient regrasping and more generalizability of a proposed
method. Therefore, in this work, we present rapid trajectory generation for
bimanual needle regrasping via reinforcement learning (RL). Demonstrations from
a sampling-based motion planning algorithm is incorporated to speed up the
learning. In addition, we propose the ego-centric state and action spaces for
this bimanual planning problem, where the reference frames are on the
end-effectors instead of some fixed frame. Thus, the learned policy can be
directly applied to any robot configuration and even to different robot arms.
Our experiments in simulation show that the success rate of a single pass is
97%, and the planning time is 0.0212s on average, which outperforms other
widely used motion planning algorithms. For the real-world experiments, the
success rate is 73.3% if the needle pose is reconstructed from an RGB image,
with a planning time of 0.0846s and a run time of 5.1454s. If the needle pose
is known beforehand, the success rate becomes 90.5%, with a planning time of
0.0807s and a run time of 2.8801s.
</p>
<a href="http://arxiv.org/abs/2011.04813" target="_blank">arXiv:2011.04813</a> [<a href="http://arxiv.org/pdf/2011.04813" target="_blank">pdf</a>]

<h2>Encoding Defensive Driving as a Dynamic Nash Game. (arXiv:2011.04815v1 [cs.RO])</h2>
<h3>Chih-Yuan Chiu, David Fridovich-Keil, Claire J. Tomlin</h3>
<p>Robots deployed in real-world environments should operate safely in a robust
manner. In scenarios where an "ego" agent navigates in an environment with
multiple other "non-ego" agents, two modes of safety are commonly proposed:
adversarial robustness and probabilistic constraint satisfaction. However,
while the former is generally computationally-intractable and leads to
overconservative solutions, the latter typically relies on strong
distributional assumptions and ignores strategic coupling between agents. To
avoid these drawbacks, we present a novel formulation of robustness within the
framework of general sum dynamic game theory, modeled on defensive driving.
More precisely, we inject the ego's cost function with an adversarial phase, a
time interval during which other agents are assumed to be temporarily
distracted, to robustify the ego agent's trajectory against other agents'
potentially dangerous behavior during this time. We demonstrate the
effectiveness of our new formulation in encoding safety via multiple traffic
scenarios.
</p>
<a href="http://arxiv.org/abs/2011.04815" target="_blank">arXiv:2011.04815</a> [<a href="http://arxiv.org/pdf/2011.04815" target="_blank">pdf</a>]

<h2>StylePredict: Machine Theory of Mind for Human Driver Behavior From Trajectories. (arXiv:2011.04816v1 [cs.RO])</h2>
<h3>Rohan Chandra, Aniket Bera, Dinesh Manocha</h3>
<p>Studies have shown that autonomous vehicles (AVs) behave conservatively in a
traffic environment composed of human drivers and do not adapt to local
conditions and socio-cultural norms. It is known that socially aware AVs can be
designed if there exist a mechanism to understand the behaviors of human
drivers. We present a notion of Machine Theory of Mind (M-ToM) to infer the
behaviors of human drivers by observing the trajectory of their vehicles. Our
M-ToM approach, called StylePredict, is based on trajectory analysis of
vehicles, which has been investigated in robotics and computer vision.
StylePredict mimics human ToM to infer driver behaviors, or styles, using a
computational mapping between the extracted trajectory of a vehicle in traffic
and the driver behaviors using graph-theoretic techniques, including spectral
analysis and centrality functions. We use StylePredict to analyze driver
behavior in different cultures in the USA, China, India, and Singapore, based
on traffic density, heterogeneity, and conformity to traffic rules and observe
an inverse correlation between longitudinal (overspeeding) and lateral
(overtaking, lane-changes) driving styles.
</p>
<a href="http://arxiv.org/abs/2011.04816" target="_blank">arXiv:2011.04816</a> [<a href="http://arxiv.org/pdf/2011.04816" target="_blank">pdf</a>]

<h2>Decentralized Structural-RNN for Robot Crowd Navigation with Deep Reinforcement Learning. (arXiv:2011.04820v1 [cs.RO])</h2>
<h3>Shuijing Liu, Peixin Chang, Weihang Liang, Neeloy Chakraborty, Katherine Driggs-Campbell</h3>
<p>Safe and efficient navigation through human crowds is an essential capability
for mobile robots. Previous work on robot crowd navigation assumes that the
dynamics of all agents are known and well-defined. In addition, the performance
of previous methods deteriorates in partially observable environments and
environments with dense crowds. To tackle these problems, we propose
decentralized structural-Recurrent Neural Network (DS-RNN), a novel network
that reasons about spatial and temporal relationships for robot decision making
in crowd navigation. We train our network with model-free deep reinforcement
learning without any expert supervision. We demonstrate that our model
outperforms previous methods and successfully transfer the policy learned in
the simulator to a real-world TurtleBot 2i.
</p>
<a href="http://arxiv.org/abs/2011.04820" target="_blank">arXiv:2011.04820</a> [<a href="http://arxiv.org/pdf/2011.04820" target="_blank">pdf</a>]

<h2>Multi-Agent Active Search using Realistic Depth-Aware Noise Model. (arXiv:2011.04825v1 [cs.RO])</h2>
<h3>Ramina Ghods, William J. Durkin, Jeff Schneider</h3>
<p>The search for objects of interest in an unknown environment by making
data-collection decisions (i.e., active search or active sensing) has robotics
applications in many fields, including the search and rescue of human survivors
following disasters, detecting gas leaks or locating and preventing animal
poachers. Existing algorithms often prioritize the location accuracy of objects
of interest while other practical issues such as the reliability of object
detection as a function of distance and lines of sight remain largely ignored.
An additional challenge is that in many active search scenarios, communication
infrastructure may be damaged, unreliable, or unestablished, making centralized
control of multiple search agents impractical. We present an algorithm called
Noise-Aware Thompson Sampling (NATS) that addresses these issues for multiple
ground-based robot agents performing active search considering two sources of
sensory information from monocular optical imagery and sonar tracking. NATS
utilizes communications between robot agents in a decentralized manner that is
robust to intermittent loss of communication links. Additionally, it takes into
account object detection uncertainty from depth as well as environmental
occlusions. Using simulation results, we show that NATS significantly
outperforms existing methods such as information-greedy policies or exhaustive
search. We demonstrate the real-world viability of NATS using a photo-realistic
environment created in the Unreal Engine 4 game development platform with the
AirSim plugin.
</p>
<a href="http://arxiv.org/abs/2011.04825" target="_blank">arXiv:2011.04825</a> [<a href="http://arxiv.org/pdf/2011.04825" target="_blank">pdf</a>]

<h2>Learning Efficient Constraint Graph Sampling for Robotic Sequential Manipulation. (arXiv:2011.04828v1 [cs.RO])</h2>
<h3>Joaquim Ortiz de Haro, Valentin N. Hartmann, Ozgur S. Oguz, Marc Toussaint</h3>
<p>Efficient sampling from constraint manifolds, and thereby generating a
diverse set of solutions of feasibility problems, is a fundamental challenge.
We consider the case where a problem is factored, that is, the underlying
nonlinear mathematical program is decomposed into differentiable equality and
inequality constraints, each of which depends only on some variables. Such
problems are at the core of efficient and robust sequential robot manipulation
planning. Naive sequential conditional sampling of individual variables, as
well as fully joint sampling of all variables at once (e.g., leveraging
optimization methods), can be highly inefficient and non-robust. We propose a
novel framework to learn how to break the overall problem into smaller
sequential sampling problems. Specifically, we leverage Monte-Carlo Tree Search
to learn which variable subsets should be assigned in which sequential order,
in order to minimize the computation time to generate full samples. This
strategy allows us to efficiently compute a set of diverse valid robot
configurations for mode-switches within sequential manipulation tasks, which
are waypoints for subsequent trajectory optimization or sampling-based motion
planning algorithms. We show that the learning method quickly converges to the
best sampling strategy for a given problem, and outperforms user-defined
orderings and joint optimization, while also providing a higher sample
diversity. Video: https://youtu.be/xWAjBGACZhs
</p>
<a href="http://arxiv.org/abs/2011.04828" target="_blank">arXiv:2011.04828</a> [<a href="http://arxiv.org/pdf/2011.04828" target="_blank">pdf</a>]

<h2>Adaptive Learning of Rank-One Models for Efficient Pairwise Sequence Alignment. (arXiv:2011.04832v1 [cs.LG])</h2>
<h3>Govinda M. Kamath, Tavor Z. Baharav, Ilan Shomorony</h3>
<p>Pairwise alignment of DNA sequencing data is a ubiquitous task in
bioinformatics and typically represents a heavy computational burden.
State-of-the-art approaches to speed up this task use hashing to identify short
segments (k-mers) that are shared by pairs of reads, which can then be used to
estimate alignment scores. However, when the number of reads is large,
accurately estimating alignment scores for all pairs is still very costly.
Moreover, in practice, one is only interested in identifying pairs of reads
with large alignment scores. In this work, we propose a new approach to
pairwise alignment estimation based on two key new ingredients. The first
ingredient is to cast the problem of pairwise alignment estimation under a
general framework of rank-one crowdsourcing models, where the workers'
responses correspond to k-mer hash collisions. These models can be accurately
solved via a spectral decomposition of the response matrix. The second
ingredient is to utilise a multi-armed bandit algorithm to adaptively refine
this spectral estimator only for read pairs that are likely to have large
alignments. The resulting algorithm iteratively performs a spectral
decomposition of the response matrix for adaptively chosen subsets of the read
pairs.
</p>
<a href="http://arxiv.org/abs/2011.04832" target="_blank">arXiv:2011.04832</a> [<a href="http://arxiv.org/pdf/2011.04832" target="_blank">pdf</a>]

<h2>Kinematics-Guided Reinforcement Learning for Object-Aware 3D Ego-Pose Estimation. (arXiv:2011.04837v1 [cs.CV])</h2>
<h3>Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Shun Iwase, Kris M. Kitani</h3>
<p>We propose a method for incorporating object interaction and human body
dynamics into the task of 3D ego-pose estimation using a head-mounted camera.
We use a kinematics model of the human body to represent the entire range of
human motion, and a dynamics model of the body to interact with objects inside
a physics simulator. By bringing together object modeling, kinematics modeling,
and dynamics modeling in a reinforcement learning (RL) framework, we enable
object-aware 3D ego-pose estimation. We devise several representational
innovations through the design of the state and action space to incorporate 3D
scene context and improve pose estimation quality. We also construct a
fine-tuning step to correct the drift and refine the estimated human-object
interaction. This is the first work to estimate a physically valid 3D full-body
interaction sequence with objects (e.g., chairs, boxes, obstacles) from
egocentric videos. Experiments with both controlled and in-the-wild settings
show that our method can successfully extract an object-conditioned 3D ego-pose
sequence that is consistent with the laws of physics.
</p>
<a href="http://arxiv.org/abs/2011.04837" target="_blank">arXiv:2011.04837</a> [<a href="http://arxiv.org/pdf/2011.04837" target="_blank">pdf</a>]

<h2>Robots of the Lost Arc: Learning to Dynamically Manipulate Fixed-Endpoint Ropes and Cables. (arXiv:2011.04840v1 [cs.RO])</h2>
<h3>Harry Zhang, Jeffrey Ichnowski, Daniel Seita, Jonathan Wang, Ken Goldberg</h3>
<p>High-speed arm motions can dynamically manipulate ropes and cables to vault
over obstacles, knock objects from pedestals, and weave between obstacles. In
this paper, we propose a self-supervised learning pipeline that enables a UR5
robot to perform these three tasks. The pipeline trains a deep convolutional
neural network that takes as input an image of the scene with object and
target. It computes a 3D apex point for the robot arm, which, together with a
task-specific trajectory function, defines an arcing motion for a manipulator
arm to dynamically manipulate the cable to perform a task with varying obstacle
and target locations. The trajectory function computes high-speed minimum-jerk
arcing motions that are constrained to remain within joint limits and to travel
through the 3D apex point by repeatedly solving quadratic programs for shorter
time horizons to find the shortest and fastest feasible motion. We experiment
with the proposed pipeline on 5 physical cables with different thickness and
mass and compare performance with two baselines in which a human chooses the
apex point. Results suggest that the robot using the learned apex point can
achieve success rates of 81.7% in vaulting, 65.0% in knocking, and 60.0% in
weaving, while a baseline with a fixed apex across the three tasks achieves
respective success rates of 51.7%, 36.7%, and 15.0%, and a baseline with
human-specified task-specific apex points achieves 66.7%, 56.7%, and 15.0%
success rate respectively. Code, data, and supplementary materials are
available at https: //sites.google.com/berkeley.edu/dynrope/home
</p>
<a href="http://arxiv.org/abs/2011.04840" target="_blank">arXiv:2011.04840</a> [<a href="http://arxiv.org/pdf/2011.04840" target="_blank">pdf</a>]

<h2>CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection. (arXiv:2011.04841v1 [cs.CV])</h2>
<h3>Ramin Nabati, Hairong Qi</h3>
<p>The perception system in autonomous vehicles is responsible for detecting and
tracking the surrounding objects. This is usually done by taking advantage of
several sensing modalities to increase robustness and accuracy, which makes
sensor fusion a crucial part of the perception system. In this paper, we focus
on the problem of radar and camera sensor fusion and propose a middle-fusion
approach to exploit both radar and camera data for 3D object detection. Our
approach, called CenterFusion, first uses a center point detection network to
detect objects by identifying their center points on the image. It then solves
the key data association problem using a novel frustum-based method to
associate the radar detections to their corresponding object's center point.
The associated radar detections are used to generate radar-based feature maps
to complement the image features, and regress to object properties such as
depth, rotation and velocity. We evaluate CenterFusion on the challenging
nuScenes dataset, where it improves the overall nuScenes Detection Score (NDS)
of the state-of-the-art camera-based algorithm by more than 12%. We further
show that CenterFusion significantly improves the velocity estimation accuracy
without using any additional temporal information. The code is available at
https://github.com/mrnabati/CenterFusion .
</p>
<a href="http://arxiv.org/abs/2011.04841" target="_blank">arXiv:2011.04841</a> [<a href="http://arxiv.org/pdf/2011.04841" target="_blank">pdf</a>]

<h2>Ellipse Detection and Localization with Applications to Knots in Sawn Lumber Images. (arXiv:2011.04844v1 [cs.CV])</h2>
<h3>Shenyi Pan, Shuxian Fan, Samuel W.K. Wong, James V. Zidek, Helge Rhodin</h3>
<p>While general object detection has seen tremendous progress, localization of
elliptical objects has received little attention in the literature. Our
motivating application is the detection of knots in sawn timber images, which
is an important problem since the number and types of knots are visual
characteristics that adversely affect the quality of sawn timber. We
demonstrate how models can be tailored to the elliptical shape and thereby
improve on general purpose detectors; more generally, elliptical defects are
common in industrial production, such as enclosed air bubbles when casting
glass or plastic. In this paper, we adapt the Faster R-CNN with its Region
Proposal Network (RPN) to model elliptical objects with a Gaussian function,
and extend the existing Gaussian Proposal Network (GPN) architecture by adding
the region-of-interest pooling and regression branches, as well as using the
Wasserstein distance as the loss function to predict the precise locations of
elliptical objects. Our proposed method has promising results on the lumber
knot dataset: knots are detected with an average intersection over union of
73.05%, compared to 63.63% for general purpose detectors. Specific to the
lumber application, we also propose an algorithm to correct any misalignment in
the raw timber images during scanning, and contribute the first open-source
lumber knot dataset by labeling the elliptical knots in the preprocessed
images.
</p>
<a href="http://arxiv.org/abs/2011.04844" target="_blank">arXiv:2011.04844</a> [<a href="http://arxiv.org/pdf/2011.04844" target="_blank">pdf</a>]

<h2>AES: Autonomous Excavator System for Real-World and Hazardous Environments. (arXiv:2011.04848v1 [cs.RO])</h2>
<h3>Jinxin Zhao, Pinxin Long, Liyang Wang, Lingfeng Qian, Feixiang Lu, Xibin Song, Dinesh Manocha, Liangjun Zhang</h3>
<p>Excavators are widely used for material-handling applications in unstructured
environments, including mining and construction. The size of the global market
of excavators is 44.12 Billion USD in 2018 and is predicted to grow to 63.14
Billion USD by 2026. Operating excavators in a real-world environment can be
challenging due to extreme conditions and rock sliding, ground collapse, or
exceeding dust. Multiple fatalities and injuries occur each year during
excavations. An autonomous excavator that can substitute human operators in
these hazardous environments would substantially lower the number of injuries
and can improve the overall productivity.
</p>
<a href="http://arxiv.org/abs/2011.04848" target="_blank">arXiv:2011.04848</a> [<a href="http://arxiv.org/pdf/2011.04848" target="_blank">pdf</a>]

<h2>Inverse Kinematics as Low-Rank Euclidean Distance Matrix Completion. (arXiv:2011.04850v1 [cs.RO])</h2>
<h3>Filip Mari&#x107;, Matthew Giamou, Ivan Petrovi&#x107;, Jonathan Kelly</h3>
<p>The majority of inverse kinematics (IK) algorithms search for solutions in a
configuration space defined by joint angles. However, the kinematics of many
robots can also be described in terms of distances between rigidly-attached
points, which collectively form a Euclidean distance matrix. This alternative
geometric description of the kinematics reveals an elegant equivalence between
IK and the problem of low-rank matrix completion. We use this connection to
implement a novel Riemannian optimization-based solution to IK for various
articulated robots with symmetric joint angle constraints.
</p>
<a href="http://arxiv.org/abs/2011.04850" target="_blank">arXiv:2011.04850</a> [<a href="http://arxiv.org/pdf/2011.04850" target="_blank">pdf</a>]

<h2>Social-STAGE: Spatio-Temporal Multi-Modal Future Trajectory Forecast. (arXiv:2011.04853v1 [cs.CV])</h2>
<h3>Srikanth Malla, Chiho Choi, Behzad Dariush</h3>
<p>This paper considers the problem of multi-modal future trajectory forecast
with ranking. Here, multi-modality and ranking refer to the multiple plausible
path predictions and the confidence in those predictions, respectively. We
propose Social-STAGE, Social interaction-aware Spatio-Temporal multi-Attention
Graph convolution network with novel Evaluation for multi-modality. Our main
contributions include analysis and formulation of multi-modality with ranking
using interaction and multi-attention, and introduction of new metrics to
evaluate the diversity and associated confidence of multi-modal predictions. We
evaluate our approach on existing public datasets ETH and UCY and show that the
proposed algorithm outperforms the state of the arts on these datasets.
</p>
<a href="http://arxiv.org/abs/2011.04853" target="_blank">arXiv:2011.04853</a> [<a href="http://arxiv.org/pdf/2011.04853" target="_blank">pdf</a>]

<h2>Understanding the hand-gestures using Convolutional Neural Networks and Generative Adversial Networks. (arXiv:2011.04860v1 [cs.CV])</h2>
<h3>Arpita Vats</h3>
<p>In this paper, it is introduced a hand gesture recognition system to
recognize the characters in the real time. The system consists of three
modules: real time hand tracking, training gesture and gesture recognition
using Convolutional Neural Networks. Camshift algorithm and hand blobs analysis
for hand tracking are being used to obtain motion descriptors and hand region.
It is fairy robust to background cluster and uses skin color for hand gesture
tracking and recognition. Furthermore, the techniques have been proposed to
improve the performance of the recognition and the accuracy using the
approaches like selection of the training images and the adaptive threshold
gesture to remove non-gesture pattern that helps to qualify an input pattern as
a gesture. In the experiments, it has been tested to the vocabulary of 36
gestures including the alphabets and digits, and results effectiveness of the
approach.
</p>
<a href="http://arxiv.org/abs/2011.04860" target="_blank">arXiv:2011.04860</a> [<a href="http://arxiv.org/pdf/2011.04860" target="_blank">pdf</a>]

<h2>On Efficient and Robust Metrics for RANSAC Hypotheses and 3D Rigid Registration. (arXiv:2011.04862v1 [cs.CV])</h2>
<h3>Jiaqi Yang, Zhiqiang Huang, Siwen Quan, Qian Zhang, Yanning Zhang, Zhiguo Cao</h3>
<p>This paper focuses on developing efficient and robust evaluation metrics for
RANSAC hypotheses to achieve accurate 3D rigid registration. Estimating
six-degree-of-freedom (6-DoF) pose from feature correspondences remains a
popular approach to 3D rigid registration, where random sample consensus
(RANSAC) is a de-facto choice to this problem. However, existing metrics for
RANSAC hypotheses are either time-consuming or sensitive to common nuisances,
parameter variations, and different application scenarios, resulting in
performance deterioration in overall registration accuracy and speed. We
alleviate this problem by first analyzing the contributions of inliers and
outliers, and then proposing several efficient and robust metrics with
different designing motivations for RANSAC hypotheses. Comparative experiments
on four standard datasets with different nuisances and application scenarios
verify that the proposed metrics can significantly improve the registration
performance and are more robust than several state-of-the-art competitors,
making them good gifts to practical applications. This work also draws an
interesting conclusion, i.e., not all inliers are equal while all outliers
should be equal, which may shed new light on this research problem.
</p>
<a href="http://arxiv.org/abs/2011.04862" target="_blank">arXiv:2011.04862</a> [<a href="http://arxiv.org/pdf/2011.04862" target="_blank">pdf</a>]

<h2>STCNet: Spatio-Temporal Cross Network for Industrial Smoke Detection. (arXiv:2011.04863v1 [cs.CV])</h2>
<h3>Yichao Cao, Qingfei Tang, Xiaobo Lu, Fan Li, Jinde Cao</h3>
<p>Industrial smoke emissions present a serious threat to natural ecosystems and
human health. Prior works have shown that using computer vision techniques to
identify smoke is a low cost and convenient method. However, industrial smoke
detection is a challenging task because industrial emission particles are often
decay rapidly outside the stacks or facilities and steam is very similar to
smoke. To overcome these problems, a novel Spatio-Temporal Cross Network
(STCNet) is proposed to recognize industrial smoke emissions. The proposed
STCNet involves a spatial pathway to extract texture features and a temporal
pathway to capture smoke motion information. We assume that spatial and
temporal pathway could guide each other. For example, the spatial path can
easily recognize the obvious interference such as trees and buildings, and the
temporal path can highlight the obscure traces of smoke movement. If the two
pathways could guide each other, it will be helpful for the smoke detection
performance. In addition, we design an efficient and concise spatio-temporal
dual pyramid architecture to ensure better fusion of multi-scale spatiotemporal
information. Finally, extensive experiments on public dataset show that our
STCNet achieves clear improvements on the challenging RISE industrial smoke
detection dataset against the best competitors by 6.2%. The code will be
available at: https://github.com/Caoyichao/STCNet.
</p>
<a href="http://arxiv.org/abs/2011.04863" target="_blank">arXiv:2011.04863</a> [<a href="http://arxiv.org/pdf/2011.04863" target="_blank">pdf</a>]

<h2>Neural Network Compression Via Sparse Optimization. (arXiv:2011.04868v1 [cs.LG])</h2>
<h3>Tianyi Chen, Bo Ji, Yixin Shi, Biyi Fang, Sheng Yi, Tianyu Ding, Xiao Tu</h3>
<p>The compression of deep neural networks (DNNs) to reduce inference cost
becomes increasingly important to meet realistic deployment requirements of
various applications. There have been a significant amount of work regarding
network compression, while most of them are heuristic rule-based or typically
not friendly to be incorporated into varying scenarios. On the other hand,
sparse optimization yielding sparse solutions naturally fits the compression
requirement, but due to the limited study of sparse optimization in stochastic
learning, its extension and application onto model compression is rarely well
explored. In this work, we propose a model compression framework based on the
recent progress on sparse stochastic optimization. Compared to existing model
compression techniques, our method is effective and requires fewer extra
engineering efforts to incorporate with varying applications, and has been
numerically demonstrated on benchmark compression tasks. Particularly, we
achieve up to 7.2 and 2.9 times FLOPs reduction with the same level of
evaluation accuracy on VGG16 for CIFAR10 and ResNet50 for ImageNet compared to
the baseline heavy models, respectively.
</p>
<a href="http://arxiv.org/abs/2011.04868" target="_blank">arXiv:2011.04868</a> [<a href="http://arxiv.org/pdf/2011.04868" target="_blank">pdf</a>]

<h2>An Efficient Closed-Form Method for Optimal Hybrid Force-Velocity Control. (arXiv:2011.04872v1 [cs.RO])</h2>
<h3>Yifan Hou, Matthew T. Mason</h3>
<p>This paper derives a closed-form method for computing hybrid force-velocity
control. The key idea is to maximize the kinematic conditioning of the
mechanical system, which includes a robot, free objects, a rigid environment
and contact constraints. The method is complete, in that it always produces an
optimal/near optimal solution when a solution exists. It is efficient, since it
is in closed form, avoiding the iterative search of previous work. We test the
method on 78,000 randomly generated test cases. The method outperforms our
previous search-based technique by being from 7 to 40 times faster, while
consistently producing better solutions in the sense of robustness to kinematic
singularity. We also test the method in several representative manipulation
experiments.
</p>
<a href="http://arxiv.org/abs/2011.04872" target="_blank">arXiv:2011.04872</a> [<a href="http://arxiv.org/pdf/2011.04872" target="_blank">pdf</a>]

<h2>A low latency ASR-free end to end spoken language understanding system. (arXiv:2011.04884v1 [cs.CV])</h2>
<h3>Mohamed Mhiri, Samuel Myer, Vikrant Singh Tomar</h3>
<p>In recent years, developing a speech understanding system that classifies a
waveform to structured data, such as intents and slots, without first
transcribing the speech to text has emerged as an interesting research problem.
This work proposes such as system with an additional constraint of designing a
system that has a small enough footprint to run on small micro-controllers and
embedded systems with minimal latency. Given a streaming input speech signal,
the proposed system can process it segment-by-segment without the need to have
the entire stream at the moment of processing. The proposed system is evaluated
on the publicly available Fluent Speech Commands dataset. Experiments show that
the proposed system yields state-of-the-art performance with the advantage of
low latency and a much smaller model when compared to other published works on
the same task.
</p>
<a href="http://arxiv.org/abs/2011.04884" target="_blank">arXiv:2011.04884</a> [<a href="http://arxiv.org/pdf/2011.04884" target="_blank">pdf</a>]

<h2>CoADNet: Collaborative Aggregation-and-Distribution Networks for Co-Salient Object Detection. (arXiv:2011.04887v1 [cs.CV])</h2>
<h3>Qijian Zhang, Runmin Cong, Junhui Hou, Chongyi Li, Yao Zhao</h3>
<p>Co-Salient Object Detection (CoSOD) aims at discovering salient objects that
repeatedly appear in a given query group containing two or more relevant
images. One challenging issue is how to effectively capture co-saliency cues by
modeling and exploiting inter-image relationships. In this paper, we present an
end-to-end collaborative aggregation-and-distribution network (CoADNet) to
capture both salient and repetitive visual patterns from multiple images.
First, we integrate saliency priors into the backbone features to suppress the
redundant background information through an online intra-saliency guidance
structure. After that, we design a two-stage aggregate-and-distribute
architecture to explore group-wise semantic interactions and produce the
co-saliency features. In the first stage, we propose a group-attentional
semantic aggregation module that models inter-image relationships to generate
the group-wise semantic representations. In the second stage, we propose a
gated group distribution module that adaptively distributes the learned group
semantics to different individuals in a dynamic gating mechanism. Finally, we
develop a group consistency preserving decoder tailored for the CoSOD task,
which maintains group constraints during feature decoding to predict more
consistent full-resolution co-saliency maps. The proposed CoADNet is evaluated
on four prevailing CoSOD benchmark datasets, which demonstrates the remarkable
performance improvement over ten state-of-the-art competitors.
</p>
<a href="http://arxiv.org/abs/2011.04887" target="_blank">arXiv:2011.04887</a> [<a href="http://arxiv.org/pdf/2011.04887" target="_blank">pdf</a>]

<h2>Feasible Region-based Identification Using Duality (Extended Version). (arXiv:2011.04904v1 [cs.RO])</h2>
<h3>Jaskaran Grover, Changliu Liu, Katia Sycara</h3>
<p>We consider the problem of estimating bounds on parameters representing tasks
being performed by individual robots in a multirobot system. In our previous
work, we derived necessary conditions based on persistency of excitation
analysis for the exact identification of these parameters. We concluded that
depending on the robot's task, the dynamics of individual robots may fail to
satisfy these conditions, thereby preventing exact inference. As an extension
to that work, this paper focuses on estimating bounds on task parameters when
such conditions are not satisfied. Each robot in the team uses
optimization-based controllers for mediating between task satisfaction and
collision avoidance. We use KKT conditions of this optimization and SVD of
active collision avoidance constraints to derive explicit relations between
Lagrange multipliers, robot dynamics, and task parameters. Using these
relations, we are able to derive bounds on each robot's task parameters.
Through numerical simulations, we show how our proposed region based
identification approach generates feasible regions for parameters when a
conventional estimator such as a UKF fails. Additionally, empirical evidence
shows that this approach generates contracting sets which converge to the true
parameters much faster than the rate at which a UKF based estimate converges.
Videos of these results are available at https://bit.ly/2JDMgeJ
</p>
<a href="http://arxiv.org/abs/2011.04904" target="_blank">arXiv:2011.04904</a> [<a href="http://arxiv.org/pdf/2011.04904" target="_blank">pdf</a>]

<h2>Stage-wise Channel Pruning for Model Compression. (arXiv:2011.04908v1 [cs.CV])</h2>
<h3>Mingyang Zhang, Linlin Ou</h3>
<p>Auto-ML pruning methods aim at searching a pruning strategy automatically to
reduce the computational complexity of deep Convolutional Neural Networks(deep
CNNs). However, some previous works found that the results of many Auto-ML
pruning methods even cannot surpass the results of the uniformly pruning
method. In this paper, we first analyze the reason for the ineffectiveness of
Auto-ML pruning. Subsequently, a stage-wise pruning(SP) method is proposed to
solve the above problem. As with most of the previous Auto-ML pruning methods,
SP also trains a super-net that can provide proxy performance for sub-nets and
search the best sub-net who has the best proxy performance. Different from
previous works, we split a deep CNN into several stages and use a full-net
where all layers are not pruned to supervise the training and the searching of
sub-nets. Remarkably, the proxy performance of sub-nets trained with SP is
closer to the actual performance than most of the previous Auto-ML pruning
works. Therefore, SP achieves the state-of-the-art on both CIFAR-10 and
ImageNet under the mobile setting.
</p>
<a href="http://arxiv.org/abs/2011.04908" target="_blank">arXiv:2011.04908</a> [<a href="http://arxiv.org/pdf/2011.04908" target="_blank">pdf</a>]

<h2>Spring-Rod System Identification via Differentiable Physics Engine. (arXiv:2011.04910v1 [cs.RO])</h2>
<h3>Kun Wang, Mridul Aanjaneya, Kostas Bekris</h3>
<p>We propose a novel differentiable physics engine for system identification of
complex spring-rod assemblies. Unlike black-box data-driven methods for
learning the evolution of a dynamical system \emph{and} its parameters, we
modularize the design of our engine using a discrete form of the governing
equations of motion, similar to a traditional physics engine. We further reduce
the dimension from 3D to 1D for each module, which allows efficient learning of
system parameters using linear regression. The regression parameters correspond
to physical quantities, such as spring stiffness or the mass of the rod, making
the pipeline explainable. The approach significantly reduces the amount of
training data required, and also avoids iterative identification of data
sampling and model training. We compare the performance of the proposed engine
with previous solutions, and demonstrate its efficacy on tensegrity systems,
such as NASA's icosahedron.
</p>
<a href="http://arxiv.org/abs/2011.04910" target="_blank">arXiv:2011.04910</a> [<a href="http://arxiv.org/pdf/2011.04910" target="_blank">pdf</a>]

<h2>Towards Unifying Feature Attribution and Counterfactual Explanations: Different Means to the Same End. (arXiv:2011.04917v1 [cs.LG])</h2>
<h3>Ramaravind K. Mothilal, Divyat Mahajan, Chenhao Tan, Amit Sharma</h3>
<p>To explain a machine learning model, there are two main approaches: feature
attributions that assign an importance score to each input feature, and
counterfactual explanations that provide input examples with minimal changes to
alter the model's prediction. We provide two key results towards unifying these
approaches in terms of their interpretation and use. First, we present a method
to generate feature attribution explanations from a set of counterfactual
examples. These feature attributions convey how important a feature is to
changing the classification outcome of a model, especially on whether a subset
of features is necessary and/or sufficient for that change, which feature
attribution methods are unable to provide. Second, we show how counterfactual
examples can be used to evaluate the goodness of an attribution-based
explanation in terms of its necessity and sufficiency. As a result, we
highlight the complementarity of these two approaches and provide an
interpretation based on a causal inference framework. Our evaluation on three
benchmark datasets -- Adult Income, LendingClub, and GermanCredit -- confirm
the complementarity. Feature attribution methods like LIME and SHAP and
counterfactual explanation methods like DiCE often do not agree on feature
importance rankings. In addition, by restricting the features that can be
modified for generating counterfactual examples, we find that the top-k
features from LIME or SHAP are neither necessary nor sufficient explanations of
a model's prediction. Finally, we present a case study of different explanation
methods on a real-world hospital triage problem.
</p>
<a href="http://arxiv.org/abs/2011.04917" target="_blank">arXiv:2011.04917</a> [<a href="http://arxiv.org/pdf/2011.04917" target="_blank">pdf</a>]

<h2>Expressiveness of Neural Networks Having Width Equal or Below the Input Dimension. (arXiv:2011.04923v1 [cs.LG])</h2>
<h3>Hans-Peter Beise, Steve Dias Da Cruz</h3>
<p>The expressiveness of deep neural networks of bounded width has recently been
investigated in a series of articles. The understanding about the minimum width
needed to ensure universal approximation for different kind of activation
functions has progressively been extended (Park et al., 2020). In particular,
it turned out that, with respect to approximation on general compact sets in
the input space, a network width less than or equal to the input dimension
excludes universal approximation. In this work, we focus on network functions
of width less than or equal to the latter critical bound. We prove that in this
regime the exact fit of partially constant functions on disjoint compact sets
is still possible for ReLU network functions under some conditions on the
mutual location of these components. Conversely, we conclude from a maximum
principle that for all continuous and monotonic activation functions, universal
approximation of arbitrary continuous functions is impossible on sets that
coincide with the boundary of an open set plus an inner point of that set. We
also show that some network functions of maximum width two, respectively one,
allow universal approximation on finite sets.
</p>
<a href="http://arxiv.org/abs/2011.04923" target="_blank">arXiv:2011.04923</a> [<a href="http://arxiv.org/pdf/2011.04923" target="_blank">pdf</a>]

<h2>Towards a Better Global Loss Landscape of GANs. (arXiv:2011.04926v1 [cs.LG])</h2>
<h3>Ruoyu Sun, Tiantian Fang, Alex Schwing</h3>
<p>Understanding of GAN training is still very limited. One major challenge is
its non-convex-non-concave min-max objective, which may lead to sub-optimal
local minima. In this work, we perform a global landscape analysis of the
empirical loss of GANs. We prove that a class of separable-GAN, including the
original JS-GAN, has exponentially many bad basins which are perceived as
mode-collapse. We also study the relativistic pairing GAN (RpGAN) loss which
couples the generated samples and the true samples. We prove that RpGAN has no
bad basins. Experiments on synthetic data show that the predicted bad basin can
indeed appear in training. We also perform experiments to support our theory
that RpGAN has a better landscape than separable-GAN. For instance, we
empirically show that RpGAN performs better than separable-GAN with relatively
narrow neural nets. The code is available at https://github.com/AilsaF/RS-GAN.
</p>
<a href="http://arxiv.org/abs/2011.04926" target="_blank">arXiv:2011.04926</a> [<a href="http://arxiv.org/pdf/2011.04926" target="_blank">pdf</a>]

<h2>An End-to-End Differentiable but Explainable Physics Engine for Tensegrity Robots: Modeling and Control. (arXiv:2011.04929v1 [cs.RO])</h2>
<h3>Kun Wang, Mridul Aanjaneya, Kostas Bekris</h3>
<p>This work proposes an end-to-end differentiable physics engine for tensegrity
robots, which introduces a data-efficient linear contact model for accurately
predicting collision responses that arise due to contacting surfaces, and a
linear actuator model that can drive these robots by expanding and contracting
their flexible cables. To the best of the authors' knowledge, this is the
\emph{first} differentiable physics engine for tensegrity robots that supports
cable modeling, contact, and actuation. This engine can be used inside an
off-the-shelf, RL-based locomotion controller in order to provide training
examples. This paper proposes a progressive training pipeline for the
differentiable physics engine that helps avoid local optima during the training
phase and reduces data requirements. It demonstrates the data-efficiency
benefits of using the differentiable engine for learning locomotion policies
for NASA's icosahedron SUPERballBot. In particular, after the engine has been
trained with few trajectories to match a ground truth simulated model, then a
policy learned on the differentiable engine is shown to be transferable back to
the ground-truth model. Training the controller requires orders of magnitude
more data than training the differential engine.
</p>
<a href="http://arxiv.org/abs/2011.04929" target="_blank">arXiv:2011.04929</a> [<a href="http://arxiv.org/pdf/2011.04929" target="_blank">pdf</a>]

<h2>Simple means Faster: Real-Time Human Motion Forecasting in Monocular First Person Videos on CPU. (arXiv:2011.04943v1 [cs.CV])</h2>
<h3>Junaid Ahmed Ansari, Brojeshwar Bhowmick</h3>
<p>We present a simple, fast, and light-weight RNN based framework for
forecasting future locations of humans in first person monocular videos. The
primary motivation for this work was to design a network which could accurately
predict future trajectories at a very high rate on a CPU. Typical applications
of such a system would be a social robot or a visual assistance system for all,
as both cannot afford to have high compute power to avoid getting heavier, less
power efficient, and costlier. In contrast to many previous methods which rely
on multiple type of cues such as camera ego-motion or 2D pose of the human, we
show that a carefully designed network model which relies solely on bounding
boxes can not only perform better but also predicts trajectories at a very high
rate while being quite low in size of approximately 17 MB. Specifically, we
demonstrate that having an auto-encoder in the encoding phase of the past
information and a regularizing layer in the end boosts the accuracy of
predictions with negligible overhead. We experiment with three first person
video datasets: CityWalks, FPL and JAAD. Our simple method trained on CityWalks
surpasses the prediction accuracy of state-of-the-art method (STED) while being
9.6x faster on a CPU (STED runs on a GPU). We also demonstrate that our model
can transfer zero-shot or after just 15% fine-tuning to other similar datasets
and perform on par with the state-of-the-art methods on such datasets (FPL and
DTP). To the best of our knowledge, we are the first to accurately forecast
trajectories at a very high prediction rate of 78 trajectories per second on
CPU.
</p>
<a href="http://arxiv.org/abs/2011.04943" target="_blank">arXiv:2011.04943</a> [<a href="http://arxiv.org/pdf/2011.04943" target="_blank">pdf</a>]

<h2>Multi-modal Fusion for Single-Stage Continuous Gesture Recognition. (arXiv:2011.04945v1 [cs.CV])</h2>
<h3>Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>Gesture recognition is a much studied research area which has myriad
real-world applications including robotics and human-machine interaction.
Current gesture recognition methods have heavily focused on isolated gestures,
and existing continuous gesture recognition methods are limited by a two-stage
approach where independent models are required for detection and
classification, with the performance of the latter being constrained by
detection performance. In contrast, we introduce a single-stage continuous
gesture recognition model, that can detect and classify multiple gestures in a
single video via a single model. This approach learns the natural transitions
between gestures and non-gestures without the need for a pre-processing
segmentation stage to detect individual gestures. To enable this, we introduce
a multi-modal fusion mechanism to support the integration of important
information that flows from multi-modal inputs, and is scalable to any number
of modes. Additionally, we propose Unimodal Feature Mapping (UFM) and
Multi-modal Feature Mapping (MFM) models to map uni-modal features and the
fused multi-modal features respectively. To further enhance the performance we
propose a mid-point based loss function that encourages smooth alignment
between the ground truth and the prediction. We demonstrate the utility of our
proposed framework which can handle variable-length input videos, and
outperforms the state-of-the-art on two challenging datasets, EgoGesture, and
IPN hand. Furthermore, ablative experiments show the importance of different
components of the proposed framework.
</p>
<a href="http://arxiv.org/abs/2011.04945" target="_blank">arXiv:2011.04945</a> [<a href="http://arxiv.org/pdf/2011.04945" target="_blank">pdf</a>]

<h2>Model-based Reinforcement Learning from Signal Temporal Logic Specifications. (arXiv:2011.04950v1 [cs.RO])</h2>
<h3>Parv Kapoor, Anand Balakrishnan, Jyotirmoy V. Deshmukh</h3>
<p>Techniques based on Reinforcement Learning (RL) are increasingly being used
to design control policies for robotic systems. RL fundamentally relies on
state-based reward functions to encode desired behavior of the robot and bad
reward functions are prone to exploitation by the learning agent, leading to
behavior that is undesirable in the best case and critically dangerous in the
worst. On the other hand, designing good reward functions for complex tasks is
a challenging problem. In this paper, we propose expressing desired high-level
robot behavior using a formal specification language known as Signal Temporal
Logic (STL) as an alternative to reward/cost functions. We use STL
specifications in conjunction with model-based learning to design model
predictive controllers that try to optimize the satisfaction of the STL
specification over a finite time horizon. The proposed algorithm is empirically
evaluated on simulations of robotic system such as a pick-and-place robotic
arm, and adaptive cruise control for autonomous vehicles.
</p>
<a href="http://arxiv.org/abs/2011.04950" target="_blank">arXiv:2011.04950</a> [<a href="http://arxiv.org/pdf/2011.04950" target="_blank">pdf</a>]

<h2>Unsupervised Contrastive Photo-to-Caricature Translation based on Auto-distortion. (arXiv:2011.04965v1 [cs.CV])</h2>
<h3>Yuhe Ding, Xin Ma, Mandi Luo, Aihua Zheng, Ran He</h3>
<p>Photo-to-caricature translation aims to synthesize the caricature as a
rendered image exaggerating the features through sketching, pencil strokes, or
other artistic drawings. Style rendering and geometry deformation are the most
important aspects in photo-to-caricature translation task. To take both into
consideration, we propose an unsupervised contrastive photo-to-caricature
translation architecture. Considering the intuitive artifacts in the existing
methods, we propose a contrastive style loss for style rendering to enforce the
similarity between the style of rendered photo and the caricature, and
simultaneously enhance its discrepancy to the photos. To obtain an exaggerating
deformation in an unpaired/unsupervised fashion, we propose a Distortion
Prediction Module (DPM) to predict a set of displacements vectors for each
input image while fixing some controlling points, followed by the thin plate
spline interpolation for warping. The model is trained on unpaired photo and
caricature while can offer bidirectional synthesizing via inputting either a
photo or a caricature. Extensive experiments demonstrate that the proposed
model is effective to generate hand-drawn like caricatures compared with
existing competitors.
</p>
<a href="http://arxiv.org/abs/2011.04965" target="_blank">arXiv:2011.04965</a> [<a href="http://arxiv.org/pdf/2011.04965" target="_blank">pdf</a>]

<h2>Detecting Human-Object Interaction with Mixed Supervision. (arXiv:2011.04971v1 [cs.CV])</h2>
<h3>Suresh Kirthi Kumaraswamy (1), Miaojing Shi (2), Ewa Kijak (3) ((1) Univ Le Mans, CNRS, IRISA, (2) Kings College London, (3) Univ Rennes, Inria, CNRS, IRISA)</h3>
<p>Human object interaction (HOI) detection is an important task in image
understanding and reasoning. It is in a form of HOI triplet hhuman; verb;
objecti, requiring bounding boxes for human and object, and action between them
for the task completion. In other words, this task requires strong supervision
for training that is however hard to procure. A natural solution to overcome
this is to pursue weakly-supervised learning, where we only know the presence
of certain HOI triplets in images but their exact location is unknown. Most
weakly-supervised learning methods do not make provision for leveraging data
with strong supervision, when they are available; and indeed a naive
combination of this two paradigms in HOI detection fails to make contributions
to each other. In this regard we propose a mixed-supervised HOI detection
pipeline: thanks to a specific design of momentum-independent learning that
learns seamlessly across these two types of supervision. Moreover, in light of
the annotation insufficiency in mixed supervision, we introduce an HOI element
swapping technique to synthesize diverse and hard negatives across images and
improve the robustness of the model. Our method is evaluated on the challenging
HICO-DET dataset. It performs close to or even better than many
fully-supervised methods by using a mixed amount of strong and weak
annotations; furthermore, it outperforms representative state of the art
weaklyand fully-supervised methods under the same supervision.
</p>
<a href="http://arxiv.org/abs/2011.04971" target="_blank">arXiv:2011.04971</a> [<a href="http://arxiv.org/pdf/2011.04971" target="_blank">pdf</a>]

<h2>Conceptual Compression via Deep Structure and Texture Synthesis. (arXiv:2011.04976v1 [cs.CV])</h2>
<h3>Jianhui Chang, Zhenghui Zhao, Chuanmin Jia, Shiqi Wang, Lingbo Yang, Jian Zhang, Siwei Ma</h3>
<p>Existing compression methods typically focus on the removal of signal-level
redundancies, while the potential and versatility of decomposing visual data
into compact conceptual components still lack further study. To this end, we
propose a novel conceptual compression framework that encodes visual data into
compact structure and texture representations, then decodes in a deep synthesis
fashion, aiming to achieve better visual reconstruction quality, flexible
content manipulation, and potential support for various vision tasks. In
particular, we propose to compress images by a dual-layered model consisting of
two complementary visual features: 1) structure layer represented by structural
maps and 2) texture layer characterized by low-dimensional deep
representations. At the encoder side, the structural maps and texture
representations are individually extracted and compressed, generating the
compact, interpretable, inter-operable bitstreams. During the decoding stage, a
hierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm
where the textures are rendered into the decoded structural maps, leading to
high-quality reconstruction with remarkable visual realism. Extensive
experiments on diverse images have demonstrated the superiority of our
framework with lower bitrates, higher reconstruction quality, and increased
versatility towards visual analysis and content manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2011.04976" target="_blank">arXiv:2011.04976</a> [<a href="http://arxiv.org/pdf/2011.04976" target="_blank">pdf</a>]

<h2>SelfDeco: Self-Supervised Monocular Depth Completion in Challenging Indoor Environments. (arXiv:2011.04977v1 [cs.CV])</h2>
<h3>Jaehoon Choi, Dongki Jung, Yonghan Lee, Deokhwa Kim, Dinesh Manocha, Donghwan Lee</h3>
<p>We present a novel algorithm for self-supervised monocular depth completion.
Our approach is based on training a neural network that requires only sparse
depth measurements and corresponding monocular video sequences without dense
depth labels. Our self-supervised algorithm is designed for challenging indoor
environments with textureless regions, glossy and transparent surface,
non-Lambertian surfaces, moving people, longer and diverse depth ranges and
scenes captured by complex ego-motions. Our novel architecture leverages both
deep stacks of sparse convolution blocks to extract sparse depth features and
pixel-adaptive convolutions to fuse image and depth features. We compare with
existing approaches in NYUv2, KITTI and NAVERLABS indoor datasets, and observe
5\:-\:34 \% improvements in root-means-square error (RMSE) reduction.
</p>
<a href="http://arxiv.org/abs/2011.04977" target="_blank">arXiv:2011.04977</a> [<a href="http://arxiv.org/pdf/2011.04977" target="_blank">pdf</a>]

<h2>CircuitBot: Learning to Survive with Robotic Circuit Drawing. (arXiv:2011.04987v1 [cs.RO])</h2>
<h3>Xianglong Tan, Weijie Lyu, Andre Rosendo</h3>
<p>Robots with the ability to actively acquire power from surroundings will be
greatly beneficial for long-term autonomy, and to survive in dynamic, uncertain
environments. In this work, a scenario is presented where a robot has limited
energy, and the only way to survive is to access the energy from a power
source. With no cables or wires available, the robot learns to construct an
electrical path and avoid potential obstacles during the connection. We present
this robot, capable of drawing connected circuit patterns with graphene-based
conductive ink. A state-of-the-art Mix-Variable Bayesian Optimization is
adopted to optimize the placement of conductive shapes to maximize the power
this robot receives. Our results show that, within a small number of trials,
the robot learns to build parallel circuits to maximize the voltage received
and avoid obstacles which steal energy from the robot.
</p>
<a href="http://arxiv.org/abs/2011.04987" target="_blank">arXiv:2011.04987</a> [<a href="http://arxiv.org/pdf/2011.04987" target="_blank">pdf</a>]

<h2>AIM 2020 Challenge on Learned Image Signal Processing Pipeline. (arXiv:2011.04994v1 [cs.CV])</h2>
<h3>Andrey Ignatov, Radu Timofte, Zhilu Zhang, Ming Liu, Haolin Wang, Wangmeng Zuo, Jiawei Zhang, Ruimao Zhang, Zhanglin Peng, Sijie Ren, Linhui Dai, Xiaohong Liu, Chengqi Li, Jun Chen, Yuichi Ito, Bhavya Vasudeva, Puneesh Deora, Umapada Pal, Zhenyu Guo, Yu Zhu, Tian Liang, Chenghua Li, Cong Leng, Zhihong Pan, Baopu Li, Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, JaeHyun Baek, Magauiya Zhussip, Yeskendir Koishekenov, Hwechul Cho Ye, Xin Liu, Xueying Hu, Jun Jiang, Jinwei Gu, Kai Li, Pengliang Tan, Bingxin Hou</h3>
<p>This paper reviews the second AIM learned ISP challenge and provides the
description of the proposed solutions and results. The participating teams were
solving a real-world RAW-to-RGB mapping problem, where to goal was to map the
original low-quality RAW images captured by the Huawei P20 device to the same
photos obtained with the Canon 5D DSLR camera. The considered task embraced a
number of complex computer vision subtasks, such as image demosaicing,
denoising, white balancing, color and contrast correction, demoireing, etc. The
target metric used in this challenge combined fidelity scores (PSNR and SSIM)
with solutions' perceptual results measured in a user study. The proposed
solutions significantly improved the baseline results, defining the
state-of-the-art for practical image signal processing pipeline modeling.
</p>
<a href="http://arxiv.org/abs/2011.04994" target="_blank">arXiv:2011.04994</a> [<a href="http://arxiv.org/pdf/2011.04994" target="_blank">pdf</a>]

<h2>Margins are Insufficient for Explaining Gradient Boosting. (arXiv:2011.04998v1 [cs.LG])</h2>
<h3>Allan Gr&#xf8;nlund, Lior Kamma, Kasper Green Larsen</h3>
<p>Boosting is one of the most successful ideas in machine learning, achieving
great practical performance with little fine-tuning. The success of boosted
classifiers is most often attributed to improvements in margins. The focus on
margin explanations was pioneered in the seminal work by Schapire et al. (1998)
and has culminated in the $k$'th margin generalization bound by Gao and Zhou
(2013), which was recently proved to be near-tight for some data distributions
(Gronlund et al. 2019). In this work, we first demonstrate that the $k$'th
margin bound is inadequate in explaining the performance of state-of-the-art
gradient boosters. We then explain the short comings of the $k$'th margin bound
and prove a stronger and more refined margin-based generalization bound for
boosted classifiers that indeed succeeds in explaining the performance of
modern gradient boosters. Finally, we improve upon the recent generalization
lower bound by Gr{\o}nlund et al. (2019).
</p>
<a href="http://arxiv.org/abs/2011.04998" target="_blank">arXiv:2011.04998</a> [<a href="http://arxiv.org/pdf/2011.04998" target="_blank">pdf</a>]

<h2>Untangling Dense Knots by Learning Task-Relevant Keypoints. (arXiv:2011.04999v1 [cs.RO])</h2>
<h3>Jennifer Grannen, Priya Sundaresan, Brijen Thananjeyan, Jeffrey Ichnowski, Ashwin Balakrishna, Minho Hwang, Vainavi Viswanath, Michael Laskey, Joseph E. Gonzalez, Ken Goldberg</h3>
<p>Untangling ropes, wires, and cables is a challenging task for robots due to
the high-dimensional configuration space, visual homogeneity, self-occlusions,
and complex dynamics. We consider dense (tight) knots that lack space between
self-intersections and present an iterative approach that uses learned
geometric structure in configurations. We instantiate this into an algorithm,
HULK: Hierarchical Untangling from Learned Keypoints, which combines
learning-based perception with a geometric planner into a policy that guides a
bilateral robot to untangle knots. To evaluate the policy, we perform
experiments both in a novel simulation environment modelling cables with varied
knot types and textures and in a physical system using the da Vinci surgical
robot. We find that HULK is able to untangle cables with dense figure-eight and
overhand knots and generalize to varied textures and appearances. We compare
two variants of HULK to three baselines and observe that HULK achieves 43.3%
higher success rates on a physical system compared to the next best baseline.
HULK successfully untangles a cable from a dense initial configuration
containing up to two overhand and figure-eight knots in 97.9% of 378 simulation
experiments with an average of 12.1 actions per trial. In physical experiments,
HULK achieves 61.7% untangling success, averaging 8.48 actions per trial.
Supplementary material, code, and videos can be found at
https://tinyurl.com/y3a88ycu.
</p>
<a href="http://arxiv.org/abs/2011.04999" target="_blank">arXiv:2011.04999</a> [<a href="http://arxiv.org/pdf/2011.04999" target="_blank">pdf</a>]

<h2>Unbalanced Optimal Transport using Integral Probability Metric Regularization. (arXiv:2011.05001v1 [cs.LG])</h2>
<h3>J. Saketha Nath (IIT Hyderabad, INDIA)</h3>
<p>Unbalanced Optimal Transport (UOT) is the generalization of classical optimal
transport to un-normalized measures. Existing formulations employ KL (or, in
general, $\phi$-divergence) based regularization for handling the mismatch in
the total mass of the given measures. Motivated by the known advantages of
Integral Probability Metrics (IPMs) over $\phi$-divergences, this paper
proposes novel formulations for UOT that employ IPMs for regularization. Under
mild conditions, we show that the proposed formulations lift the ground metric
to metrics over measures. More interestingly, the induced metric turns out to
be another IPM whose generating set is the intersection of that of the IPM
employed and the set of 1-Lipschitz functions under the ground metric. We
further generalize these metrics to obtain analogues of p-Wasserstein metrics
for the unbalanced case. When the regularizing IPM is chosen as the MMD, the
proposed UOT as well as the corresponding Barycenter formulations, turn out to
be those of minimizing a convex quadratic subject to non-negativity constraints
and hence can be solved very efficiently. We also discuss connections with
formulations for robust optimal transport in the case of noisy marginals.
</p>
<a href="http://arxiv.org/abs/2011.05001" target="_blank">arXiv:2011.05001</a> [<a href="http://arxiv.org/pdf/2011.05001" target="_blank">pdf</a>]

<h2>Removing Brightness Bias in Rectified Gradients. (arXiv:2011.05002v1 [cs.CV])</h2>
<h3>Lennart Brocki, Neo Christopher Chung</h3>
<p>Interpretation and improvement of deep neural networks relies on better
understanding of their underlying mechanisms. In particular, gradients of
classes or concepts with respect to the input features (e.g., pixels in images)
are often used as importance scores, which are visualized in saliency maps.
Thus, a family of saliency methods provide an intuitive way to identify input
features with substantial influences on classifications or latent concepts.
Rectified Gradients \cite{Kim2019} is a new method which introduce layer-wise
thresholding in order to denoise the saliency maps. While visually coherent in
certain cases, we identify a brightness bias in Rectified Gradients. We
demonstrate that dark areas of an input image are not highlighted by a saliency
map using Rectified Gradients, even if it is relevant for the class or concept.
Even in the scaled images, the bias exists around an artificial point in color
spectrum. Our simple modification removes this bias and recovers input features
that were removed due to their colors.

"No Bias Rectified Gradient" is available at
\url{https://github.com/lenbrocki/NoBias-Rectified-Gradient}
</p>
<a href="http://arxiv.org/abs/2011.05002" target="_blank">arXiv:2011.05002</a> [<a href="http://arxiv.org/pdf/2011.05002" target="_blank">pdf</a>]

<h2>Joint Super-Resolution and Rectification for Solar Cell Inspection. (arXiv:2011.05003v1 [cs.CV])</h2>
<h3>Mathis Hoffmann, Thomas K&#xf6;hler, Bernd Doll, Frank Schebesch, Florian Talkenberg, Ian Marius Peters, Christoph J. Brabec, Andreas Maier, Vincent Christlein</h3>
<p>Visual inspection of solar modules is an important monitoring facility in
photovoltaic power plants. Since a single measurement of fast CMOS sensors is
limited in spatial resolution and often not sufficient to reliably detect small
defects, we apply multi-frame super-resolution (MFSR) to a sequence of low
resolution measurements. In addition, the rectification and removal of lens
distortion simplifies subsequent analysis. Therefore, we propose to fuse this
pre-processing with standard MFSR algorithms. This is advantageous, because we
omit a separate processing step, the motion estimation becomes more stable and
the spacing of high-resolution (HR) pixels on the rectified module image
becomes uniform w.r.t. the module plane, regardless of perspective distortion.
We present a comprehensive user study showing that MFSR is beneficial for
defect recognition by human experts and that the proposed method performs
better than the state of the art. Furthermore, we apply automated crack
segmentation and show that the proposed method performs 3x better than bicubic
upsampling and 2x better than the state of the art for automated inspection.
</p>
<a href="http://arxiv.org/abs/2011.05003" target="_blank">arXiv:2011.05003</a> [<a href="http://arxiv.org/pdf/2011.05003" target="_blank">pdf</a>]

<h2>Deep Multimodal Fusion by Channel Exchanging. (arXiv:2011.05005v1 [cs.CV])</h2>
<h3>Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, Junzhou Huang</h3>
<p>Deep multimodal fusion by using multiple sources of data for classification
or regression has exhibited a clear advantage over the unimodal counterpart on
various applications. Yet, current methods including aggregation-based and
alignment-based fusion are still inadequate in balancing the trade-off between
inter-modal fusion and intra-modal processing, incurring a bottleneck of
performance improvement. To this end, this paper proposes
Channel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework
that dynamically exchanges channels between sub-networks of different
modalities. Specifically, the channel exchanging process is self-guided by
individual channel importance that is measured by the magnitude of
Batch-Normalization (BN) scaling factor during training. The validity of such
exchanging process is also guaranteed by sharing convolutional filters yet
keeping separate BN layers across modalities, which, as an add-on benefit,
allows our multimodal architecture to be almost as compact as a unimodal
network. Extensive experiments on semantic segmentation via RGB-D data and
image translation through multi-domain input verify the effectiveness of our
CEN compared to current state-of-the-art methods. Detailed ablation studies
have also been carried out, which provably affirm the advantage of each
component we propose. Our code is available at https://github.com/yikaiw/CEN.
</p>
<a href="http://arxiv.org/abs/2011.05005" target="_blank">arXiv:2011.05005</a> [<a href="http://arxiv.org/pdf/2011.05005" target="_blank">pdf</a>]

<h2>Neural Latent Dependency Model for Sequence Labeling. (arXiv:2011.05009v1 [cs.LG])</h2>
<h3>Yang Zhou, Yong Jiang, Zechuan Hu, Kewei Tu</h3>
<p>Sequence labeling is a fundamental problem in machine learning, natural
language processing and many other fields. A classic approach to sequence
labeling is linear chain conditional random fields (CRFs). When combined with
neural network encoders, they achieve very good performance in many sequence
labeling tasks. One limitation of linear chain CRFs is their inability to model
long-range dependencies between labels. High order CRFs extend linear chain
CRFs by modeling dependencies no longer than their order, but the computational
complexity grows exponentially in the order. In this paper, we propose the
Neural Latent Dependency Model (NLDM) that models dependencies of arbitrary
length between labels with a latent tree structure. We develop an end-to-end
training algorithm and a polynomial-time inference algorithm of our model. We
evaluate our model on both synthetic and real datasets and show that our model
outperforms strong baselines.
</p>
<a href="http://arxiv.org/abs/2011.05009" target="_blank">arXiv:2011.05009</a> [<a href="http://arxiv.org/pdf/2011.05009" target="_blank">pdf</a>]

<h2>Residual Pose: A Decoupled Approach for Depth-based 3D Human Pose Estimation. (arXiv:2011.05010v1 [cs.CV])</h2>
<h3>Angel Mart&#xed;nez-Gonz&#xe1;lez, Michael Villamizar, Olivier Can&#xe9;vet, Jean-Marc Odobez</h3>
<p>We propose to leverage recent advances in reliable 2D pose estimation with
Convolutional Neural Networks (CNN) to estimate the 3D pose of people from
depth images in multi-person Human-Robot Interaction (HRI) scenarios. Our
method is based on the observation that using the depth information to obtain
3D lifted points from 2D body landmark detections provides a rough estimate of
the true 3D human pose, thus requiring only a refinement step. In that line our
contributions are threefold. (i) we propose to perform 3D pose estimation from
depth images by decoupling 2D pose estimation and 3D pose refinement; (ii) we
propose a deep-learning approach that regresses the residual pose between the
lifted 3D pose and the true 3D pose; (iii) we show that despite its simplicity,
our approach achieves very competitive results both in accuracy and speed on
two public datasets and is therefore appealing for multi-person HRI compared to
recent state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.05010" target="_blank">arXiv:2011.05010</a> [<a href="http://arxiv.org/pdf/2011.05010" target="_blank">pdf</a>]

<h2>A step towards neural genome assembly. (arXiv:2011.05013v1 [cs.LG])</h2>
<h3>Lovro Vr&#x10d;ek, Petar Veli&#x10d;kovi&#x107;, Mile &#x160;iki&#x107;</h3>
<p>De novo genome assembly focuses on finding connections between a vast amount
of short sequences in order to reconstruct the original genome. The central
problem of genome assembly could be described as finding a Hamiltonian path
through a large directed graph with a constraint that an unknown number of
nodes and edges should be avoided. However, due to local structures in the
graph and biological features, the problem can be reduced to graph
simplification, which includes removal of redundant information. Motivated by
recent advancements in graph representation learning and neural execution of
algorithms, in this work we train the MPNN model with max-aggregator to execute
several algorithms for graph simplification. We show that the algorithms were
learned successfully and can be scaled to graphs of sizes up to 20 times larger
than the ones used in training. We also test on graphs obtained from real-world
genomic data---that of a lambda phage and E. coli.
</p>
<a href="http://arxiv.org/abs/2011.05013" target="_blank">arXiv:2011.05013</a> [<a href="http://arxiv.org/pdf/2011.05013" target="_blank">pdf</a>]

<h2>Point Cloud Registration Based on Consistency Evaluation of Rigid Transformation in Parameter Space. (arXiv:2011.05014v1 [cs.CV])</h2>
<h3>Masaki Yoshii, Ikuko Shimizu</h3>
<p>We can use a method called registration to integrate some point clouds that
represent the shape of the real world. In this paper, we propose highly
accurate and stable registration method. Our method detects keypoints from
point clouds and generates triplets using multiple descriptors. Furthermore,
our method evaluates the consistency of rigid transformation parameters of each
triplet with histograms and obtains the rigid transformation between the point
clouds. In the experiment of this paper, our method had minimul errors and no
major failures. As a result, we obtained sufficiently accurate and stable
registration results compared to the comparative methods.
</p>
<a href="http://arxiv.org/abs/2011.05014" target="_blank">arXiv:2011.05014</a> [<a href="http://arxiv.org/pdf/2011.05014" target="_blank">pdf</a>]

<h2>Distributed Learning with Low Communication Cost via Gradient Boosting Untrained Neural Network. (arXiv:2011.05022v1 [cs.LG])</h2>
<h3>Xiatian Zhang, Xunshi He, Nan Wang, Rong Chen</h3>
<p>For high-dimensional data, there are huge communication costs for distributed
GBDT because the communication volume of GBDT is related to the number of
features. To overcome this problem, we propose a novel gradient boosting
algorithm, the Gradient Boosting Untrained Neural Network(GBUN). GBUN ensembles
the untrained randomly generated neural network that softly distributes data
samples to multiple neuron outputs and dramatically reduces the communication
costs for distributed learning. To avoid creating huge neural networks for
high-dimensional data, we extend Simhash algorithm to mimic forward calculation
of the neural network. Our experiments on multiple public datasets show that
GBUN is as good as conventional GBDT in terms of prediction accuracy and much
better than it in scaling property for distributed learning. Comparing to
conventional GBDT varieties, GBUN speeds up the training process up to 13 times
on the cluster with 64 machines, and up to 4614 times on the cluster with
100KB/s network bandwidth. Therefore, GBUN is not only an efficient distributed
learning algorithm but also has great potentials for federated learning.
</p>
<a href="http://arxiv.org/abs/2011.05022" target="_blank">arXiv:2011.05022</a> [<a href="http://arxiv.org/pdf/2011.05022" target="_blank">pdf</a>]

<h2>Sparse within Sparse Gaussian Processes using Neighbor Information. (arXiv:2011.05041v1 [stat.ML])</h2>
<h3>Gia-Lac Tran, Dimitrios Milios, Pietro Michiardi, Maurizio Filippone</h3>
<p>Approximations to Gaussian processes based on inducing variables, combined
with variational inference techniques, enable state-of-the-art sparse
approaches to infer GPs at scale through mini batch-based learning. In this
work, we address one limitation of sparse GPs, which is due to the challenge in
dealing with a large number of inducing variables without imposing a special
structure on the inducing inputs. In particular, we introduce a novel
hierarchical prior, which imposes sparsity on the set of inducing variables. We
treat our model variationally, and we experimentally show considerable
computational gains compared to standard sparse GPs when sparsity on the
inducing variables is realized considering the nearest inducing inputs of a
random mini-batch of the data. We perform an extensive experimental validation
that demonstrates the effectiveness of our approach compared to the
state-of-the-art. Our approach enables the possibility to use sparse GPs using
a large number of inducing points without incurring a prohibitive computational
cost.
</p>
<a href="http://arxiv.org/abs/2011.05041" target="_blank">arXiv:2011.05041</a> [<a href="http://arxiv.org/pdf/2011.05041" target="_blank">pdf</a>]

<h2>Building an Automated and Self-Aware Anomaly Detection System. (arXiv:2011.05047v1 [cs.LG])</h2>
<h3>Sayan Chakraborty, Smit Shah, Kiumars Soltani, Anna Swigart, Luyao Yang, Kyle Buckingham</h3>
<p>Organizations rely heavily on time series metrics to measure and model key
aspects of operational and business performance. The ability to reliably detect
issues with these metrics is imperative to identifying early indicators of
major problems before they become pervasive. It can be very challenging to
proactively monitor a large number of diverse and constantly changing time
series for anomalies, so there are often gaps in monitoring coverage, disabled
or ignored monitors due to false positive alarms, and teams resorting to manual
inspection of charts to catch problems. Traditionally, variations in the data
generation processes and patterns have required strong modeling expertise to
create models that accurately flag anomalies. In this paper, we describe an
anomaly detection system that overcomes this common challenge by keeping track
of its own performance and making changes as necessary to each model without
requiring manual intervention. We demonstrate that this novel approach
outperforms available alternatives on benchmark datasets in many scenarios.
</p>
<a href="http://arxiv.org/abs/2011.05047" target="_blank">arXiv:2011.05047</a> [<a href="http://arxiv.org/pdf/2011.05047" target="_blank">pdf</a>]

<h2>Human-centric Spatio-Temporal Video Grounding With Visual Transformers. (arXiv:2011.05049v1 [cs.CV])</h2>
<h3>Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, Dong Xu</h3>
<p>In this work, we introduce a novel task - Humancentric Spatio-Temporal Video
Grounding (HC-STVG). Unlike the existing referring expression tasks in images
or videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal
tube of the target person from an untrimmed video based on a given textural
description. This task is useful, especially for healthcare and
security-related applications, where the surveillance videos can be extremely
long but only a specific person during a specific period of time is concerned.
HC-STVG is a video grounding task that requires both spatial (where) and
temporal (when) localization. Unfortunately, the existing grounding methods
cannot handle this task well. We tackle this task by proposing an effective
baseline method named Spatio-Temporal Grounding with Visual Transformers
(STGVT), which utilizes Visual Transformers to extract cross-modal
representations for video-sentence matching and temporal localization. To
facilitate this task, we also contribute an HC-STVG dataset consisting of 5,660
video-sentence pairs on complex multi-person scenes. Specifically, each video
lasts for 20 seconds, pairing with a natural query sentence with an average of
17.25 words. Extensive experiments are conducted on this dataset, demonstrating
the newly-proposed method outperforms the existing baseline methods.
</p>
<a href="http://arxiv.org/abs/2011.05049" target="_blank">arXiv:2011.05049</a> [<a href="http://arxiv.org/pdf/2011.05049" target="_blank">pdf</a>]

<h2>Sample Complexity Bounds for Two Timescale Value-based Reinforcement Learning Algorithms. (arXiv:2011.05053v1 [cs.LG])</h2>
<h3>Tengyu Xu, Yingbin Liang</h3>
<p>Two timescale stochastic approximation (SA) has been widely used in
value-based reinforcement learning algorithms. In the policy evaluation
setting, it can model the linear and nonlinear temporal difference learning
with gradient correction (TDC) algorithms as linear SA and nonlinear SA,
respectively. In the policy optimization setting, two timescale nonlinear SA
can also model the greedy gradient-Q (Greedy-GQ) algorithm. In previous
studies, the non-asymptotic analysis of linear TDC and Greedy-GQ has been
studied in the Markovian setting, with diminishing or accuracy-dependent
stepsize. For the nonlinear TDC algorithm, only the asymptotic convergence has
been established. In this paper, we study the non-asymptotic convergence rate
of two timescale linear and nonlinear TDC and Greedy-GQ under Markovian
sampling and with accuracy-independent constant stepsize. For linear TDC, we
provide a novel non-asymptotic analysis and show that it attains an
$\epsilon$-accurate solution with the optimal sample complexity of
$\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$ under a constant stepsize. For
nonlinear TDC and Greedy-GQ, we show that both algorithms attain
$\epsilon$-accurate stationary solution with sample complexity
$\mathcal{O}(\epsilon^{-2})$. It is the first non-asymptotic convergence result
established for nonlinear TDC under Markovian sampling and our result for
Greedy-GQ outperforms the previous result orderwisely by a factor of
$\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$.
</p>
<a href="http://arxiv.org/abs/2011.05053" target="_blank">arXiv:2011.05053</a> [<a href="http://arxiv.org/pdf/2011.05053" target="_blank">pdf</a>]

<h2>Decoupled Appearance and Motion Learning for Efficient Anomaly Detection in Surveillance Video. (arXiv:2011.05054v1 [cs.CV])</h2>
<h3>Bo Li, Sam Leroux, Pieter Simoens</h3>
<p>Automating the analysis of surveillance video footage is of great interest
when urban environments or industrial sites are monitored by a large number of
cameras. As anomalies are often context-specific, it is hard to predefine
events of interest and collect labelled training data. A purely unsupervised
approach for automated anomaly detection is much more suitable. For every
camera, a separate algorithm could then be deployed that learns over time a
baseline model of appearance and motion related features of the objects within
the camera viewport. Anything that deviates from this baseline is flagged as an
anomaly for further analysis downstream. We propose a new neural network
architecture that learns the normal behavior in a purely unsupervised fashion.
In contrast to previous work, we use latent code predictions as our anomaly
metric. We show that this outperforms reconstruction-based and frame
prediction-based methods on different benchmark datasets both in terms of
accuracy and robustness against changing lighting and weather conditions. By
decoupling an appearance and a motion model, our model can also process 16 to
45 times more frames per second than related approaches which makes our model
suitable for deploying on the camera itself or on other edge devices.
</p>
<a href="http://arxiv.org/abs/2011.05054" target="_blank">arXiv:2011.05054</a> [<a href="http://arxiv.org/pdf/2011.05054" target="_blank">pdf</a>]

<h2>What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes. (arXiv:2011.05064v1 [cs.AI])</h2>
<h3>Herman Yau, Chris Russell, Simon Hadfield,</h3>
<p>We present a novel form of explanation for Reinforcement Learning, based
around the notion of intended outcome. These explanations describe the outcome
an agent is trying to achieve by its actions. We provide a simple proof that
general methods for post-hoc explanations of this nature are impossible in
traditional reinforcement learning. Rather, the information needed for the
explanations must be collected in conjunction with training the agent. We
derive approaches designed to extract local explanations based on intention for
several variants of Q-function approximation and prove consistency between the
explanations and the Q-values learned. We demonstrate our method on multiple
reinforcement learning problems, and provide code to help researchers
introspecting their RL environments and algorithms.
</p>
<a href="http://arxiv.org/abs/2011.05064" target="_blank">arXiv:2011.05064</a> [<a href="http://arxiv.org/pdf/2011.05064" target="_blank">pdf</a>]

<h2>Efficient Algorithms for Stochastic Repeated Second-price Auctions. (arXiv:2011.05072v1 [cs.LG])</h2>
<h3>Juliette Achddou (PSL, DI-ENS, VALDA), Olivier Capp&#xe9; (DI-ENS, VALDA), Aur&#xe9;lien Garivier (ENS Lyon, Inria)</h3>
<p>Developing efficient sequential bidding strategies for repeated auctions is
an important practical challenge in various marketing tasks. In this setting,
the bidding agent obtains information, on both the value of the item at sale
and the behavior of the other bidders, only when she wins the auction. Standard
bandit theory does not apply to this problem due to the presence of
action-dependent censoring. In this work, we consider second-price auctions and
propose novel, efficient UCB-like algorithms for this task. These algorithms
are analyzed in the stochastic setting, assuming regularity of the distribution
of the opponents' bids. We provide regret upper bounds that quantify the
improvement over the baseline algorithm proposed in the literature. The
improvement is particularly significant in cases when the value of the
auctioned item is low, yielding a spectacular reduction in the order of the
worst-case regret. We further provide the first parametric lower bound for this
problem that applies to generic UCB-like strategies. As an alternative, we
propose more explainable strategies which are reminiscent of the Explore Then
Commit bandit algorithm. We provide a critical analysis of this class of
strategies, showing both important advantages and limitations. In particular,
we provide a minimax lower bound and propose a nearly minimax-optimal instance
of this class.
</p>
<a href="http://arxiv.org/abs/2011.05072" target="_blank">arXiv:2011.05072</a> [<a href="http://arxiv.org/pdf/2011.05072" target="_blank">pdf</a>]

<h2>Efficient and Transferable Adversarial Examples from Bayesian Neural Networks. (arXiv:2011.05074v1 [cs.LG])</h2>
<h3>Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon</h3>
<p>Deep neural networks are vulnerable to evasion attacks, i.e., carefully
crafted examples designed to fool a model at test time. Attacks that
successfully evade an ensemble of models can transfer to other independently
trained models, which proves useful in black-box settings. Unfortunately, these
methods involve heavy computation costs to train the models forming the
ensemble. To overcome this, we propose a new method to generate transferable
adversarial examples efficiently. Inspired by Bayesian deep learning, our
method builds such ensembles by sampling from the posterior distribution of
neural network weights during a single training process. Experiments on
CIFAR-10 show that our approach improves the transfer rates significantly at
equal or even lower computation costs. Intra-architecture transfer rate is
increased by 23% compared to classical ensemble-based attacks, while requiring
4 times less training epochs. In the inter-architecture case, we show that we
can combine our method with ensemble-based attacks to increase their transfer
rate by up to 15% with constant training computational cost.
</p>
<a href="http://arxiv.org/abs/2011.05074" target="_blank">arXiv:2011.05074</a> [<a href="http://arxiv.org/pdf/2011.05074" target="_blank">pdf</a>]

<h2>Model Predictive Control for Human-Centred Lower Limb Robotic Assistance. (arXiv:2011.05079v1 [cs.RO])</h2>
<h3>Christopher Caulcrick, Weiguang Huo, Enrico Franco, Samer Mohammed, Will Hoult, Ravi Vaidyanathan</h3>
<p>Loss of mobility or balance resulting from neural trauma is a critical
consideration in public health. Robotic exoskeletons hold great potential for
rehabilitation and assisted movement, yet optimal assist-as-needed (AAN)
control remains unresolved given pathological variance among patients. We
introduce a model predictive control (MPC) architecture for lower limb
exoskeletons centred around a fuzzy logic algorithm (FLA) identifying modes of
assistance based on human involvement. Assistance modes are: 1) passive for
human relaxed and robot dominant, 2) active-assist for human cooperation with
the task, and 3) safety in the case of human resistance to the robot. Human
torque is estimated from electromyography (EMG) signals prior to joint motions,
enabling advanced prediction of torque by the MPC and selection of assistance
mode by the FLA. The controller is demonstrated in hardware with three subjects
on a 1-DOF knee exoskeleton tracking a sinusoidal trajectory with human relaxed
assistive, and resistive. Experimental results show quick and appropriate
transfers among the assistance modes and satisfied assistive performance in
each mode. Results illustrate an objective approach to lower limb robotic
assistance through on-the-fly transition between modes of movement, providing a
new level of human-robot synergy for mobility assist and rehabilitation.
</p>
<a href="http://arxiv.org/abs/2011.05079" target="_blank">arXiv:2011.05079</a> [<a href="http://arxiv.org/pdf/2011.05079" target="_blank">pdf</a>]

<h2>Higher-Order Spectral Clustering of Directed Graphs. (arXiv:2011.05080v1 [cs.LG])</h2>
<h3>Steinar Laenen, He Sun</h3>
<p>Clustering is an important topic in algorithms, and has a number of
applications in machine learning, computer vision, statistics, and several
other research disciplines. Traditional objectives of graph clustering are to
find clusters with low conductance. Not only are these objectives just
applicable for undirected graphs, they are also incapable to take the
relationships between clusters into account, which could be crucial for many
applications. To overcome these downsides, we study directed graphs (digraphs)
whose clusters exhibit further "structural" information amongst each other.
Based on the Hermitian matrix representation of digraphs, we present a
nearly-linear time algorithm for digraph clustering, and further show that our
proposed algorithm can be implemented in sublinear time under reasonable
assumptions. The significance of our theoretical work is demonstrated by
extensive experimental results on the UN Comtrade Dataset: the output
clustering of our algorithm exhibits not only how the clusters (sets of
countries) relate to each other with respect to their import and export
records, but also how these clusters evolve over time, in accordance with known
facts in international trade.
</p>
<a href="http://arxiv.org/abs/2011.05080" target="_blank">arXiv:2011.05080</a> [<a href="http://arxiv.org/pdf/2011.05080" target="_blank">pdf</a>]

<h2>MP-ResNet: Multi-path Residual Network for the Semantic segmentation of High-Resolution PolSAR Images. (arXiv:2011.05088v1 [cs.CV])</h2>
<h3>Lei Ding, Kai Zheng, Dong Lin, Yuxing Chen, Bing Liu, Jiansheng Li, Lorenzo Bruzzone</h3>
<p>There are limited studies on the semantic segmentation of high-resolution
Polarimetric Synthetic Aperture Radar (PolSAR) images due to the scarcity of
training data and the inference of speckle noises. The Gaofen contest has
provided open access of a high-quality PolSAR semantic segmentation dataset.
Taking this chance, we propose a Multi-path ResNet (MP-ResNet) architecture for
the semantic segmentation of high-resolution PolSAR images. Compared to
conventional U-shape encoder-decoder convolutional neural network (CNN)
architectures, the MP-ResNet learns semantic context with its parallel
multi-scale branches, which greatly enlarges its valid receptive fields and
improves the embedding of local discriminative features. In addition, MP-ResNet
adopts a multi-level feature fusion design in its decoder to make the best use
of the features learned from its different branches. Ablation studies show that
the MPResNet has significant advantages over its baseline method (FCN with
ResNet34). It also surpasses several classic state-of-the-art methods in terms
of overall accuracy (OA), mean F1 and fwIoU, whereas its computational costs
are not much increased. This CNN architecture can be used as a baseline method
for future studies on the semantic segmentation of PolSAR images. The code is
available at: https://github.com/ggsDing/SARSeg.
</p>
<a href="http://arxiv.org/abs/2011.05088" target="_blank">arXiv:2011.05088</a> [<a href="http://arxiv.org/pdf/2011.05088" target="_blank">pdf</a>]

<h2>Two-stage Training of Graph Neural Networks for Graph Classification. (arXiv:2011.05097v1 [cs.LG])</h2>
<h3>Manh Tuan Do, Noseong Park, Kijung Shin</h3>
<p>Graph Neural Networks (GNNs) have received massive attention in the field of
machine learning on graphs. Inspired by the success of neural networks, a line
of research has been conducted to train GNNs to deal with various tasks, such
as node classification, graph classification, and link prediction. In this
work, our task of interest is graph classification. Several GNN models have
been proposed and shown great performance in this task. However, the question
is whether the original setting has fully utilized the power of the
architecture.

In this work, we propose a two-stage training framework based on triplet
loss. After the first stage, graphs of the same class are close while those of
different classes are mapped far apart. Once graphs are well-separated based on
labels, they can be easier to classify. This framework is generic in the sense
that it is compatible to any GNN architecture. By adapting 5 GNNs to the
triplet framework, together with some additional fine tuning, we demonstrate
the consistent improvement in performance over the original setting of each
model up to 5.4% in 12 datasets.
</p>
<a href="http://arxiv.org/abs/2011.05097" target="_blank">arXiv:2011.05097</a> [<a href="http://arxiv.org/pdf/2011.05097" target="_blank">pdf</a>]

<h2>On-Device Language Identification of Text in Images using Diacritic Characters. (arXiv:2011.05108v1 [cs.CV])</h2>
<h3>Shubham Vatsal, Nikhil Arora, Gopi Ramena, Sukumar Moharana, Dhruval Jain, Naresh Purre, Rachit S Munjal</h3>
<p>Diacritic characters can be considered as a unique set of characters
providing us with adequate and significant clue in identifying a given language
with considerably high accuracy. Diacritics, though associated with phonetics
often serve as a distinguishing feature for many languages especially the ones
with a Latin script. In this proposed work, we aim to identify language of text
in images using the presence of diacritic characters in order to improve
Optical Character Recognition (OCR) performance in any given automated
environment. We showcase our work across 13 Latin languages encompassing 85
diacritic characters. We use an architecture similar to Squeezedet for object
detection of diacritic characters followed by a shallow network to finally
identify the language. OCR systems when accompanied with identified language
parameter tends to produce better results than sole deployment of OCR systems.
The discussed work apart from guaranteeing an improvement in OCR results also
takes on-device (mobile phone) constraints into consideration in terms of model
size and inference time.
</p>
<a href="http://arxiv.org/abs/2011.05108" target="_blank">arXiv:2011.05108</a> [<a href="http://arxiv.org/pdf/2011.05108" target="_blank">pdf</a>]

<h2>Feedback-Based Dynamic Feature Selection for Constrained Continuous Data Acquisition. (arXiv:2011.05112v1 [cs.LG])</h2>
<h3>Alp Sahin, Xiangrui Zeng</h3>
<p>Relevant and high-quality data are critical to successful development of
machine learning applications. For machine learning applications on dynamic
systems equipped with a large number of sensors, such as connected vehicles and
robots, how to find relevant and high-quality data features in an efficient way
is a challenging problem. In this work, we address the problem of feature
selection in constrained continuous data acquisition. We propose a
feedback-based dynamic feature selection algorithm that efficiently decides on
the feature set for data collection from a dynamic system in a step-wise
manner. We formulate the sequential feature selection procedure as a Markov
Decision Process. The machine learning model performance feedback with an
exploration component is used as the reward function in an $\epsilon$-greedy
action selection. Our evaluation shows that the proposed feedback-based feature
selection algorithm has superior performance over constrained baseline methods
and matching performance with unconstrained baseline methods.
</p>
<a href="http://arxiv.org/abs/2011.05112" target="_blank">arXiv:2011.05112</a> [<a href="http://arxiv.org/pdf/2011.05112" target="_blank">pdf</a>]

<h2>Self-supervised Graph Representation Learning via Bootstrapping. (arXiv:2011.05126v1 [cs.LG])</h2>
<h3>Feihu Che, Guohua Yang, Dawei Zhang, Jianhua Tao, Pengpeng Shao, Tong Liu</h3>
<p>Graph neural networks~(GNNs) apply deep learning techniques to
graph-structured data and have achieved promising performance in graph
representation learning. However, existing GNNs rely heavily on enough labels
or well-designed negative samples. To address these issues, we propose a new
self-supervised graph representation method: deep graph bootstrapping~(DGB).
DGB consists of two neural networks: online and target networks, and the input
of them are different augmented views of the initial graph. The online network
is trained to predict the target network while the target network is updated
with a slow-moving average of the online network, which means the online and
target networks can learn from each other. As a result, the proposed DGB can
learn graph representation without negative examples in an unsupervised manner.
In addition, we summarize three kinds of augmentation methods for
graph-structured data and apply them to the DGB. Experiments on the benchmark
datasets show the DGB performs better than the current state-of-the-art methods
and how the augmentation methods affect the performances.
</p>
<a href="http://arxiv.org/abs/2011.05126" target="_blank">arXiv:2011.05126</a> [<a href="http://arxiv.org/pdf/2011.05126" target="_blank">pdf</a>]

<h2>Relation-weighted link prediction for disease gene identification. (arXiv:2011.05138v1 [cs.LG])</h2>
<h3>Srivamshi Pittala, William Koehler, Jonathan Deans, Daniel Salinas, Martin Bringmann, Katharina Sophia Volz, Berk Kapicioglu</h3>
<p>Identification of disease genes, which are a set of genes associated with a
disease, plays an important role in understanding and curing diseases. In this
paper, we present a biomedical knowledge graph designed specifically for this
problem, propose a novel machine learning method that identifies disease genes
on such graphs by leveraging recent advances in network biology and graph
representation learning, study the effects of various relation types on
prediction performance, and empirically demonstrate that our algorithms
outperform its closest state-of-the-art competitor in disease gene
identification by 24.1%. We also show that we achieve higher precision than
Open Targets, the leading initiative for target identification, with respect to
predicting drug targets in clinical trials for Parkinson's disease.
</p>
<a href="http://arxiv.org/abs/2011.05138" target="_blank">arXiv:2011.05138</a> [<a href="http://arxiv.org/pdf/2011.05138" target="_blank">pdf</a>]

<h2>Multi-pooled Inception features for no-reference image quality assessment. (arXiv:2011.05139v1 [cs.CV])</h2>
<h3>Domonkos Varga</h3>
<p>Image quality assessment (IQA) is an important element of a broad spectrum of
applications ranging from automatic video streaming to display technology.
Furthermore, the measurement of image quality requires a balanced investigation
of image content and features. Our proposed approach extracts visual features
by attaching global average pooling (GAP) layers to multiple Inception modules
of on an ImageNet database pretrained convolutional neural network (CNN). In
contrast to previous methods, we do not take patches from the input image.
Instead, the input image is treated as a whole and is run through a pretrained
CNN body to extract resolution-independent, multi-level deep features. As a
consequence, our method can be easily generalized to any input image size and
pretrained CNNs. Thus, we present a detailed parameter study with respect to
the CNN base architectures and the effectiveness of different deep features. We
demonstrate that our best proposal - called MultiGAP-NRIQA - is able to provide
state-of-the-art results on three benchmark IQA databases. Furthermore, these
results were also confirmed in a cross database test using the LIVE In the Wild
Image Quality Challenge database.
</p>
<a href="http://arxiv.org/abs/2011.05139" target="_blank">arXiv:2011.05139</a> [<a href="http://arxiv.org/pdf/2011.05139" target="_blank">pdf</a>]

<h2>Multi-modal, multi-task, multi-attention (M3) deep learning detection of reticular pseudodrusen: 1 towards automated and accessible classification of age-related macular degeneration. (arXiv:2011.05142v1 [cs.CV])</h2>
<h3>Qingyu Chen, Tiarnan D. L. Keenan, Alexis Allot, Yifan Peng, Elvira Agr&#xf3;n, Amitha Domalpally, Caroline C. W. Klaver, Daniel T. Luttikhuizen, Marcus H. Colyer, Catherine A. Cukras, Henry E. Wiley, M. Teresa Magone, Chantal Cousineau-Krieger, Wai T. Wong, Yingying Zhu, Emily Y. Chew, Zhiyong Lu (for the AREDS2 Deep Learning Research Group)</h3>
<p>Objective Reticular pseudodrusen (RPD), a key feature of age-related macular
degeneration (AMD), are poorly detected by human experts on standard color
fundus photography (CFP) and typically require advanced imaging modalities such
as fundus autofluorescence (FAF). The objective was to develop and evaluate the
performance of a novel 'M3' deep learning framework on RPD detection. Materials
and Methods A deep learning framework M3 was developed to detect RPD presence
accurately using CFP alone, FAF alone, or both, employing &gt;8000 CFP-FAF image
pairs obtained prospectively (Age-Related Eye Disease Study 2). The M3
framework includes multi-modal (detection from single or multiple image
modalities), multi-task (training different tasks simultaneously to improve
generalizability), and multi-attention (improving ensembled feature
representation) operation. Performance on RPD detection was compared with
state-of-the-art deep learning models and 13 ophthalmologists; performance on
detection of two other AMD features (geographic atrophy and pigmentary
abnormalities) was also evaluated. Results For RPD detection, M3 achieved area
under receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP
alone, FAF alone, and both, respectively. M3 performance on CFP was very
substantially superior to human retinal specialists (median F1-score 0.644
versus 0.350). External validation (on Rotterdam Study, Netherlands)
demonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also
accurately detected geographic atrophy and pigmentary abnormalities (AUROC
0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion
This study demonstrates the successful development, robust evaluation, and
external validation of a novel deep learning framework that enables accessible,
accurate, and automated AMD diagnosis and prognosis.
</p>
<a href="http://arxiv.org/abs/2011.05142" target="_blank">arXiv:2011.05142</a> [<a href="http://arxiv.org/pdf/2011.05142" target="_blank">pdf</a>]

<h2>A Multi-Plant Disease Diagnosis Method using Convolutional Neural Network. (arXiv:2011.05151v1 [cs.CV])</h2>
<h3>Muhammad Mohsin Kabir, Abu Quwsar Ohi, M. F. Mridha</h3>
<p>A disease that limits a plant from its maximal capacity is defined as plant
disease. From the perspective of agriculture, diagnosing plant disease is
crucial, as diseases often limit plants' production capacity. However, manual
approaches to recognize plant diseases are often temporal, challenging, and
time-consuming. Therefore, computerized recognition of plant diseases is highly
desired in the field of agricultural automation. Due to the recent improvement
of computer vision, identifying diseases using leaf images of a particular
plant has already been introduced. Nevertheless, the most introduced model can
only diagnose diseases of a specific plant. Hence, in this chapter, we
investigate an optimal plant disease identification model combining the
diagnosis of multiple plants. Despite relying on multi-class classification,
the model inherits a multilabel classification method to identify the plant and
the type of disease in parallel. For the experiment and evaluation, we
collected data from various online sources that included leaf images of six
plants, including tomato, potato, rice, corn, grape, and apple. In our
investigation, we implement numerous popular convolutional neural network (CNN)
architectures. The experimental results validate that the Xception and DenseNet
architectures perform better in multi-label plant disease classification tasks.
Through architectural investigation, we imply that skip connections, spatial
convolutions, and shorter hidden layer connectivity cause better results in
plant disease classification.
</p>
<a href="http://arxiv.org/abs/2011.05151" target="_blank">arXiv:2011.05151</a> [<a href="http://arxiv.org/pdf/2011.05151" target="_blank">pdf</a>]

<h2>Safety Verification of Neural Network Controlled Systems. (arXiv:2011.05174v1 [cs.AI])</h2>
<h3>Arthur Clavi&#xe8;re, Eric Asselin, Christophe Garion (ISAE-SUPAERO), Claire Pagetti (ANITI)</h3>
<p>In this paper, we propose a system-level approach for verifying the safety of
neural network controlled systems, combining a continuous-time physical system
with a discrete-time neural network based controller. We assume a generic model
for the controller that can capture both simple and complex behaviours
involving neural networks. Based on this model, we perform a reachability
analysis that soundly approximates the reachable states of the overall system,
allowing to achieve a formal proof of safety. To this end, we leverage both
validated simulation to approximate the behaviour of the physical system and
abstract interpretation to approximate the behaviour of the controller. We
evaluate the applicability of our approach using a real-world use case.
Moreover, we show that our approach can provide valuable information when the
system cannot be proved totally safe.
</p>
<a href="http://arxiv.org/abs/2011.05174" target="_blank">arXiv:2011.05174</a> [<a href="http://arxiv.org/pdf/2011.05174" target="_blank">pdf</a>]

<h2>Generation of Human-aware Navigation Maps using Graph Neural Networks. (arXiv:2011.05180v1 [cs.RO])</h2>
<h3>Daniel Rodriguez-Criado, Pilar Bachiller, Luis J. Manso</h3>
<p>Minimising the discomfort caused by robots when navigating in social
situations is crucial for them to be accepted. The paper presents a machine
learning-based framework that bootstraps existing one-dimensional datasets to
generate a cost map dataset and a model combining Graph Neural Network and
Convolutional Neural Network layers to produce cost maps for human-aware
navigation in real-time. The proposed framework is evaluated against the
original one-dimensional dataset and in simulated navigation tasks. The results
outperform similar state-of-the-art-methods considering the accuracy on the
dataset and the navigation metrics used. The applications of the proposed
framework are not limited to human-aware navigation, it could be applied to
other fields where map generation is needed.
</p>
<a href="http://arxiv.org/abs/2011.05180" target="_blank">arXiv:2011.05180</a> [<a href="http://arxiv.org/pdf/2011.05180" target="_blank">pdf</a>]

<h2>Biomedical Information Extraction for Disease Gene Prioritization. (arXiv:2011.05188v1 [cs.LG])</h2>
<h3>Jupinder Parmar, William Koehler, Martin Bringmann, Katharina Sophia Volz, Berk Kapicioglu</h3>
<p>We introduce a biomedical information extraction (IE) pipeline that extracts
biological relationships from text and demonstrate that its components, such as
named entity recognition (NER) and relation extraction (RE), outperform
state-of-the-art in BioNLP. We apply it to tens of millions of PubMed abstracts
to extract protein-protein interactions (PPIs) and augment these extractions to
a biomedical knowledge graph that already contains PPIs extracted from STRING,
the leading structured PPI database. We show that, despite already containing
PPIs from an established structured source, augmenting our own IE-based
extractions to the graph allows us to predict novel disease-gene associations
with a 20% relative increase in hit@30, an important step towards developing
drug targets for uncured diseases.
</p>
<a href="http://arxiv.org/abs/2011.05188" target="_blank">arXiv:2011.05188</a> [<a href="http://arxiv.org/pdf/2011.05188" target="_blank">pdf</a>]

<h2>Multiplicity and Diversity: Analyzing the Optimal Solution Space of the Correlation Clustering Problem on Complete Signed Graphs. (arXiv:2011.05196v1 [cs.RO])</h2>
<h3>Nejat Arinik (LIA), Rosa Figueiredo, Vincent Labatut</h3>
<p>In order to study real-world systems, many applied works model them through
signed graphs, i.e. graphs whose edges are labeled as either positive or
negative. Such a graph is considered as structurally balanced when it can be
partitioned into a number of modules, such that positive (resp. negative) edges
are located inside (resp. in-between) the modules. When it is not the case,
authors look for the closest partition to such balance, a problem called
Correlation Clustering (CC). Due to the complexity of the CC problem, the
standard approach is to find a single optimal partition and stick to it, even
if other optimal or high scoring solutions possibly exist. In this work, we
study the space of optimal solutions of the CC problem, on a collection of
synthetic complete graphs. We show empirically that under certain conditions,
there can be many optimal partitions of a signed graph. Some of these are very
different and thus provide distinct perspectives on the system, as illustrated
on a small real-world graph. This is an important result, as it implies that
one may have to find several, if not all, optimal solutions of the CC problem,
in order to properly study the considered system.
</p>
<a href="http://arxiv.org/abs/2011.05196" target="_blank">arXiv:2011.05196</a> [<a href="http://arxiv.org/pdf/2011.05196" target="_blank">pdf</a>]

<h2>Dynamic Embeddings for Interaction Prediction. (arXiv:2011.05208v1 [cs.LG])</h2>
<h3>Zekarias T. Kefato, Sarunas Girdzijauskas, Nasrullah Sheikh, Alberto Montresor</h3>
<p>In recommender systems (RSs), predicting the next item that a user interacts
with is critical for user retention. While the last decade has seen an
explosion of RSs aimed at identifying relevant items that match user
preferences, there is still a range of aspects that could be considered to
further improve their performance. For example, often RSs are centered around
the user, who is modeled using her recent sequence of activities. Recent
studies, however, have shown the effectiveness of modeling the mutual
interactions between users and items using separate user and item embeddings.
Building on the success of these studies, we propose a novel method called
DeePRed that addresses some of their limitations. In particular, we avoid
recursive and costly interactions between consecutive short-term embeddings by
using long-term (stationary) embeddings as a proxy. This enable us to train
DeePRed using simple mini-batches without the overhead of specialized
mini-batches proposed in previous studies. Moreover, DeePRed's effectiveness
comes from the aforementioned design and a multi-way attention mechanism that
inspects user-item compatibility. Experiments show that DeePRed outperforms the
best state-of-the-art approach by at least 14% on next item prediction task,
while gaining more than an order of magnitude speedup over the best performing
baselines. Although this study is mainly concerned with temporal interaction
networks, we also show the power and flexibility of DeePRed by adapting it to
the case of static interaction networks, substituting the short- and long-term
aspects with local and global ones.
</p>
<a href="http://arxiv.org/abs/2011.05208" target="_blank">arXiv:2011.05208</a> [<a href="http://arxiv.org/pdf/2011.05208" target="_blank">pdf</a>]

<h2>Pixel precise unsupervised detection of viral particle proliferation in cellular imaging data. (arXiv:2011.05209v1 [cs.CV])</h2>
<h3>Birgitta Dresp-Langley, John M. Wandeto</h3>
<p>Cellular and molecular imaging techniques and models have been developed to
characterize single stages of viral proliferation after focal infection of
cells in vitro. The fast and automatic classification of cell imaging data may
prove helpful prior to any further comparison of representative experimental
data to mathematical models of viral propagation in host cells. Here, we use
computer generated images drawn from a reproduction of an imaging model from a
previously published study of experimentally obtained cell imaging data
representing progressive viral particle proliferation in host cell monolayers.
Inspired by experimental time-based imaging data, here in this study viral
particle increase in time is simulated by a one-by-one increase, across images,
in black or gray single pixels representing dead or partially infected cells,
and hypothetical remission by a one-by-one increase in white pixels coding for
living cells in the original image model. The image simulations are submitted
to unsupervised learning by a Self-Organizing Map (SOM) and the Quantization
Error in the SOM output (SOM-QE) is used for automatic classification of the
image simulations as a function of the represented extent of viral particle
proliferation or cell recovery. Unsupervised classification by SOM-QE of 160
model images, each with more than three million pixels, is shown to provide a
statistically reliable, pixel precise, and fast classification model that
outperforms human computer-assisted image classification by RGB image mean
computation. The automatic classification procedure proposed here provides a
powerful approach to understand finely tuned mechanisms in the infection and
proliferation of virus in cell lines in vitro or other cells.
</p>
<a href="http://arxiv.org/abs/2011.05209" target="_blank">arXiv:2011.05209</a> [<a href="http://arxiv.org/pdf/2011.05209" target="_blank">pdf</a>]

<h2>A Variational Infinite Mixture for Probabilistic Inverse Dynamics Learning. (arXiv:2011.05217v1 [cs.LG])</h2>
<h3>Hany Abdulsamad, Peter Nickl, Pascal Klink, Jan Peters</h3>
<p>Probabilistic regression techniques in control and robotics applications have
to fulfill different criteria of data-driven adaptability, computational
efficiency, scalability to high dimensions, and the capacity to deal with
different modalities in the data. Classical regressors usually fulfill only a
subset of these properties. In this work, we extend seminal work on Bayesian
nonparametric mixtures and derive an efficient variational Bayes inference
technique for infinite mixtures of probabilistic local polynomial models with
well-calibrated certainty quantification. We highlight the model's power in
combining data-driven complexity adaptation, fast prediction and the ability to
deal with discontinuous functions and heteroscedastic noise. We benchmark this
technique on a range of large real inverse dynamics datasets, showing that the
infinite mixture formulation is competitive with classical Local Learning
methods and regularizes model complexity by adapting the number of components
based on data and without relying on heuristics. Moreover, to showcase the
practicality of the approach, we use the learned models for online inverse
dynamics control of a Barret-WAM manipulator, significantly improving the
trajectory tracking performance.
</p>
<a href="http://arxiv.org/abs/2011.05217" target="_blank">arXiv:2011.05217</a> [<a href="http://arxiv.org/pdf/2011.05217" target="_blank">pdf</a>]

<h2>On-line force capability evaluation based on efficient polytope vertex search. (arXiv:2011.05226v1 [cs.RO])</h2>
<h3>Antun Skuric (AUCTUS), Vincent Padois (AUCTUS), David Daney (AUCTUS)</h3>
<p>Ellipsoid-based manipulability measures are often used to characterize the
force/velocity task-space capabilities of robots. While computationally simple,
this approach largely approximate and underestimate the true capabilities.
Force/velocity polytopes appear to be a more appropriate representation to
characterize robot's task-space capabilities. However, due to the computational
complexity of the associated vertex search problem, the polytope approach is
mostly restricted to offline use, e.g. as a tool aiding robot mechanical
design, robot placement in work-space and offline trajectory planning. In this
paper, a novel on-line polytope vertex search algorithm is proposed. It
exploits the parallelotop geometry of actuator constraints. The proposed
algorithm significantly reduces the complexity and computation time of the
vertex search problem in comparison to commonly used algorithms. In order to
highlight the on-line capability of the proposed algorithm and its potential
for robot control, a challenging experiment with two collaborating Franka Emika
Panda robots, carrying a load of 12 kilograms, is proposed. In this experiment,
the load distribution is adapted on-line, as a function of the configuration
dependant task-space force capability of each robot, in order to avoid, as much
as possible, the saturation of their capacity.
</p>
<a href="http://arxiv.org/abs/2011.05226" target="_blank">arXiv:2011.05226</a> [<a href="http://arxiv.org/pdf/2011.05226" target="_blank">pdf</a>]

<h2>Temporal Stochastic Softmax for 3D CNNs: An Application in Facial Expression Recognition. (arXiv:2011.05227v1 [cs.CV])</h2>
<h3>Th&#xe9;o Ayral, Marco Pedersoli, Simon Bacon, Eric Granger</h3>
<p>Training deep learning models for accurate spatiotemporal recognition of
facial expressions in videos requires significant computational resources. For
practical reasons, 3D Convolutional Neural Networks (3D CNNs) are usually
trained with relatively short clips randomly extracted from videos. However,
such uniform sampling is generally sub-optimal because equal importance is
assigned to each temporal clip. In this paper, we present a strategy for
efficient video-based training of 3D CNNs. It relies on softmax temporal
pooling and a weighted sampling mechanism to select the most relevant training
clips. The proposed softmax strategy provides several advantages: a reduced
computational complexity due to efficient clip sampling, and an improved
accuracy since temporal weighting focuses on more relevant clips during both
training and inference. Experimental results obtained with the proposed method
on several facial expression recognition benchmarks show the benefits of
focusing on more informative clips in training videos. In particular, our
approach improves performance and computational cost by reducing the impact of
inaccurate trimming and coarse annotation of videos, and heterogeneous
distribution of visual information across time.
</p>
<a href="http://arxiv.org/abs/2011.05227" target="_blank">arXiv:2011.05227</a> [<a href="http://arxiv.org/pdf/2011.05227" target="_blank">pdf</a>]

<h2>VFH+ based shared control for remotely operated mobile robots. (arXiv:2011.05228v1 [cs.RO])</h2>
<h3>Pantelis Pappas, Manolis Chiou, Georgios-Theofanis Epsimos, Grigoris Nikolaou, Rustam Stolkin</h3>
<p>This paper addresses the problem of safe and efficient navigation in remotely
controlled robots operating in hazardous and unstructured environments; or
conducting other remote robotic tasks. A shared control method is presented
which blends the commands from a VFH+ obstacle avoidance navigation module with
the teleoperation commands provided by an operator via a joypad. The presented
approach offers several advantages such as flexibility allowing for a
straightforward adaptation of the controller's behaviour and easy integration
with variable autonomy systems; as well as the ability to cope with dynamic
environments. The advantages of the presented controller are demonstrated by an
experimental evaluation in a disaster response scenario. More specifically,
presented evidence show a clear performance increase in terms of safety and
task completion time compared to a pure teleoperation approach, as well as an
ability to cope with previously unobserved obstacles.
</p>
<a href="http://arxiv.org/abs/2011.05228" target="_blank">arXiv:2011.05228</a> [<a href="http://arxiv.org/pdf/2011.05228" target="_blank">pdf</a>]

<h2>Uses and Abuses of the Cross-Entropy Loss: Case Studies in Modern Deep Learning. (arXiv:2011.05231v1 [stat.ML])</h2>
<h3>Elliott Gordon-Rodriguez, Gabriel Loaiza-Ganem, Geoff Pleiss, John P. Cunningham</h3>
<p>Modern deep learning is primarily an experimental science, in which empirical
advances occasionally come at the expense of probabilistic rigor. Here we focus
on one such example; namely the use of the categorical cross-entropy loss to
model data that is not strictly categorical, but rather takes values on the
simplex. This practice is standard in neural network architectures with label
smoothing and actor-mimic reinforcement learning, amongst others. Drawing on
the recently discovered continuous-categorical distribution, we propose
probabilistically-inspired alternatives to these models, providing an approach
that is more principled and theoretically appealing. Through careful
experimentation, including an ablation study, we identify the potential for
outperformance in these models, thereby highlighting the importance of a proper
probabilistic treatment, as well as illustrating some of the failure modes
thereof.
</p>
<a href="http://arxiv.org/abs/2011.05231" target="_blank">arXiv:2011.05231</a> [<a href="http://arxiv.org/pdf/2011.05231" target="_blank">pdf</a>]

<h2>Classification of Polarimetric SAR Images Using Compact Convolutional Neural Networks. (arXiv:2011.05243v1 [cs.CV])</h2>
<h3>Mete Ahishali, Serkan Kiranyaz, Turker Ince, Moncef Gabbouj</h3>
<p>Classification of polarimetric synthetic aperture radar (PolSAR) images is an
active research area with a major role in environmental applications. The
traditional Machine Learning (ML) methods proposed in this domain generally
focus on utilizing highly discriminative features to improve the classification
performance, but this task is complicated by the well-known "curse of
dimensionality" phenomena. Other approaches based on deep Convolutional Neural
Networks (CNNs) have certain limitations and drawbacks, such as high
computational complexity, an unfeasibly large training set with ground-truth
labels, and special hardware requirements. In this work, to address the
limitations of traditional ML and deep CNN based methods, a novel and
systematic classification framework is proposed for the classification of
PolSAR images, based on a compact and adaptive implementation of CNNs using a
sliding-window classification approach. The proposed approach has three
advantages. First, there is no requirement for an extensive feature extraction
process. Second, it is computationally efficient due to utilized compact
configurations. In particular, the proposed compact and adaptive CNN model is
designed to achieve the maximum classification accuracy with minimum training
and computational complexity. This is of considerable importance considering
the high costs involved in labelling in PolSAR classification. Finally, the
proposed approach can perform classification using smaller window sizes than
deep CNNs. Experimental evaluations have been performed over the most
commonly-used four benchmark PolSAR images: AIRSAR L-Band and RADARSAT-2 C-Band
data of San Francisco Bay and Flevoland areas. Accordingly, the best obtained
overall accuracies range between 92.33 - 99.39% for these benchmark study
sites.
</p>
<a href="http://arxiv.org/abs/2011.05243" target="_blank">arXiv:2011.05243</a> [<a href="http://arxiv.org/pdf/2011.05243" target="_blank">pdf</a>]

<h2>Perception Improvement for Free: Exploring Imperceptible Black-box Adversarial Attacks on Image Classification. (arXiv:2011.05254v1 [cs.CV])</h2>
<h3>Yongwei Wang, Mingquan Feng, Rabab Ward, Z. Jane Wang, Lanjun Wang</h3>
<p>Deep neural networks are vulnerable to adversarial attacks. White-box
adversarial attacks can fool neural networks with small adversarial
perturbations, especially for large size images. However, keeping successful
adversarial perturbations imperceptible is especially challenging for
transfer-based black-box adversarial attacks. Often such adversarial examples
can be easily spotted due to their unpleasantly poor visual qualities, which
compromises the threat of adversarial attacks in practice. In this study, to
improve the image quality of black-box adversarial examples perceptually, we
propose structure-aware adversarial attacks by generating adversarial images
based on psychological perceptual models. Specifically, we allow higher
perturbations on perceptually insignificant regions, while assigning lower or
no perturbation on visually sensitive regions. In addition to the proposed
spatial-constrained adversarial perturbations, we also propose a novel
structure-aware frequency adversarial attack method in the discrete cosine
transform (DCT) domain. Since the proposed attacks are independent of the
gradient estimation, they can be directly incorporated with existing
gradient-based attacks. Experimental results show that, with the comparable
attack success rate (ASR), the proposed methods can produce adversarial
examples with considerably improved visual quality for free. With the
comparable perceptual quality, the proposed approaches achieve higher attack
success rates: particularly for the frequency structure-aware attacks, the
average ASR improves more than 10% over the baseline attacks.
</p>
<a href="http://arxiv.org/abs/2011.05254" target="_blank">arXiv:2011.05254</a> [<a href="http://arxiv.org/pdf/2011.05254" target="_blank">pdf</a>]

<h2>ATCN: Agile Temporal Convolutional Networks for Processing of Time Series on Edge. (arXiv:2011.05260v1 [cs.LG])</h2>
<h3>Mohammadreza Baharani, Steven Furgurson, Hamed Tabkhi</h3>
<p>This paper presents a scalable deep learning model called Agile Temporal
Convolutional Network (ATCN) for high-accurate fast classification and time
series prediction in resource-constrained embedded systems. ATCN is primarily
designed for mobile embedded systems with performance and memory constraints
such as wearable biomedical devices and real-time reliability monitoring
systems. It makes fundamental improvements over the mainstream temporal
convolutional neural networks, including the incorporation of separable
depth-wise convolution to reduce the computational complexity of the model and
residual connections as time attention machines, increase the network depth and
accuracy. The result of this configurability makes the ATCN a family of compact
networks with formalized hyper-parameters that allow the model architecture to
be configurable and adjusted based on the application requirements. We
demonstrate the capabilities of our proposed ATCN on accuracy and performance
trade-off on three embedded applications, including transistor reliability
monitoring, heartbeat classification of ECG signals, and digit classification.
Our comparison results against state-of-the-art approaches demonstrate much
lower computation and memory demand for faster processing with better
prediction and classification accuracy. The source code of the ATCN model is
publicly available at https://github.com/TeCSAR-UNCC/ATCN.
</p>
<a href="http://arxiv.org/abs/2011.05260" target="_blank">arXiv:2011.05260</a> [<a href="http://arxiv.org/pdf/2011.05260" target="_blank">pdf</a>]

<h2>Coordinated Aerial-Ground Robot Exploration via Monte-Carlo View Quality Rendering. (arXiv:2011.05275v1 [cs.RO])</h2>
<h3>Di Deng, Zhefan Xu, Wenbo Zhao, Kenji Shimada</h3>
<p>We present a framework for a ground-aerial robotic team to explore large,
unstructured, and unknown environments. In such exploration problems, the
effectiveness of existing exploration-boosting heuristics often scales poorly
with the environments' size and complexity. This work proposes a novel
framework combining incremental frontier distribution, goal selection with
Monte-Carlo view quality rendering, and an automatic-differentiable information
gain measure to improve exploration efficiency. Simulated with multiple complex
environments, we demonstrate that the proposed method effectively utilizes
collaborative aerial and ground robots, consistently guides agents to
informative viewpoints, improves exploration paths' information gain, and
reduces planning time.
</p>
<a href="http://arxiv.org/abs/2011.05275" target="_blank">arXiv:2011.05275</a> [<a href="http://arxiv.org/pdf/2011.05275" target="_blank">pdf</a>]

<h2>Explainable Knowledge Tracing Models for Big Data: Is Ensembling an Answer?. (arXiv:2011.05285v1 [cs.LG])</h2>
<h3>Tirth Shah, Lukas Olson, Aditya Sharma, Nirmal Patel</h3>
<p>In this paper, we describe our Knowledge Tracing model for the 2020 NeurIPS
Education Challenge. We used a combination of 22 models to predict whether the
students will answer a given question correctly or not. Our combination of
different approaches allowed us to get an accuracy higher than any of the
individual models, and the variation of our model types gave our solution
better explainability, more alignment with learning science theories, and high
predictive power.
</p>
<a href="http://arxiv.org/abs/2011.05285" target="_blank">arXiv:2011.05285</a> [<a href="http://arxiv.org/pdf/2011.05285" target="_blank">pdf</a>]

<h2>Continual Learning of Control Primitives: Skill Discovery via Reset-Games. (arXiv:2011.05286v1 [cs.LG])</h2>
<h3>Kelvin Xu, Siddharth Verma, Chelsea Finn, Sergey Levine</h3>
<p>Reinforcement learning has the potential to automate the acquisition of
behavior in complex settings, but in order for it to be successfully deployed,
a number of practical challenges must be addressed. First, in real world
settings, when an agent attempts a task and fails, the environment must somehow
"reset" so that the agent can attempt the task again. While easy in simulation,
this could require considerable human effort in the real world, especially if
the number of trials is very large. Second, real world learning often involves
complex, temporally extended behavior that is often difficult to acquire with
random exploration. While these two problems may at first appear unrelated, in
this work, we show how a single method can allow an agent to acquire skills
with minimal supervision while removing the need for resets. We do this by
exploiting the insight that the need to "reset" an agent to a broad set of
initial states for a learning task provides a natural setting to learn a
diverse set of "reset-skills". We propose a general-sum game formulation that
balances the objectives of resetting and learning skills, and demonstrate that
this approach improves performance on reset-free tasks, and additionally show
that the skills we obtain can be used to significantly accelerate downstream
learning.
</p>
<a href="http://arxiv.org/abs/2011.05286" target="_blank">arXiv:2011.05286</a> [<a href="http://arxiv.org/pdf/2011.05286" target="_blank">pdf</a>]

<h2>Two-Sided Fairness in Non-Personalised Recommendations. (arXiv:2011.05287v1 [cs.AI])</h2>
<h3>Aadi Swadipto Mondal, Rakesh Bal, Sayan Sinha, Gourab K Patro</h3>
<p>Recommender systems are one of the most widely used services on several
online platforms to suggest potential items to the end-users. These services
often use different machine learning techniques for which fairness is a
concerning factor, especially when the downstream services have the ability to
cause social ramifications. Thus, focusing on the non-personalised (global)
recommendations in news media platforms (e.g., top-k trending topics on
Twitter, top-k news on a news platform, etc.), we discuss on two specific
fairness concerns together (traditionally studied separately)---user fairness
and organisational fairness. While user fairness captures the idea of
representing the choices of all the individual users in the case of global
recommendations, organisational fairness tries to ensure
politically/ideologically balanced recommendation sets. This makes user
fairness a user-side requirement and organisational fairness a platform-side
requirement. For user fairness, we test with methods from social choice theory,
i.e., various voting rules known to better represent user choices in their
results. Even in our application of voting rules to the recommendation setup,
we observe high user satisfaction scores. Now for organisational fairness, we
propose a bias metric which measures the aggregate ideological bias of a
recommended set of items (articles). Analysing the results obtained from voting
rule-based recommendation, we find that while the well-known voting rules are
better from the user side, they show high bias values and clearly not suitable
for organisational requirements of the platforms. Thus, there is a need to
build an encompassing mechanism by cohesively bridging ideas of user fairness
and organisational fairness. In this abstract paper, we intend to frame the
elementary ideas along with the clear motivation behind the requirement of such
a mechanism.
</p>
<a href="http://arxiv.org/abs/2011.05287" target="_blank">arXiv:2011.05287</a> [<a href="http://arxiv.org/pdf/2011.05287" target="_blank">pdf</a>]

<h2>Frontier-based Automatic-differentiable Information Gain Measure for Robotic Exploration of Unknown 3D Environments. (arXiv:2011.05288v1 [cs.RO])</h2>
<h3>Di Deng, Zhefan Xu, Wenbo Zhao, Kenji Shimada</h3>
<p>The path planning problem for autonomous exploration of an unknown region by
a robotic agent typically employs frontier-based or information-theoretic
heuristics. Frontier-based heuristics typically evaluate the information gain
of a viewpoint by the number of visible frontier voxels, which is a discrete
measure that can only be optimized by sampling. On the other hand,
information-theoretic heuristics compute information gain as the mutual
information between the map and the sensor's measurement. Although the gradient
of such measures can be computed, the computation involves costly numerical
differentiation. In this work, we add a novel fuzzy logic filter in the
counting of visible frontier voxels surrounding a viewpoint, which allows the
gradient of the information gain with respect to the viewpoint to be
efficiently computed using automatic differentiation. This enables us to
simultaneously optimize information gain with other differentiable quality
measures such as path length. Using multiple simulation environments, we
demonstrate that the proposed gradient-based optimization method consistently
improves the information gain and other quality measures of exploration paths.
</p>
<a href="http://arxiv.org/abs/2011.05288" target="_blank">arXiv:2011.05288</a> [<a href="http://arxiv.org/pdf/2011.05288" target="_blank">pdf</a>]

<h2>Learning to Communicate and Correct Pose Errors. (arXiv:2011.05289v1 [cs.CV])</h2>
<h3>Nicholas Vadivelu, Mengye Ren, James Tu, Jingkang Wang, Raquel Urtasun</h3>
<p>Learned communication makes multi-agent systems more effective by aggregating
distributed information. However, it also exposes individual agents to the
threat of erroneous messages they might receive. In this paper, we study the
setting proposed in V2VNet, where nearby self-driving vehicles jointly perform
object detection and motion forecasting in a cooperative manner. Despite a huge
performance boost when the agents solve the task together, the gain is quickly
diminished in the presence of pose noise since the communication relies on
spatial transformations. Hence, we propose a novel neural reasoning framework
that learns to communicate, to estimate potential errors, and finally, to reach
a consensus about those errors. Experiments confirm that our proposed framework
significantly improves the robustness of multi-agent self-driving perception
and motion forecasting systems under realistic and severe localization noise.
</p>
<a href="http://arxiv.org/abs/2011.05289" target="_blank">arXiv:2011.05289</a> [<a href="http://arxiv.org/pdf/2011.05289" target="_blank">pdf</a>]

<h2>Topological Regularization via Persistence-Sensitive Optimization. (arXiv:2011.05290v1 [cs.LG])</h2>
<h3>Arnur Nigmetov, Aditi S. Krishnapriyan, Nicole Sanderson, Dmitriy Morozov</h3>
<p>Optimization, a key tool in machine learning and statistics, relies on
regularization to reduce overfitting. Traditional regularization methods
control a norm of the solution to ensure its smoothness. Recently, topological
methods have emerged as a way to provide a more precise and expressive control
over the solution, relying on persistent homology to quantify and reduce its
roughness. All such existing techniques back-propagate gradients through the
persistence diagram, which is a summary of the topological features of a
function. Their downside is that they provide information only at the critical
points of the function. We propose a method that instead builds on
persistence-sensitive simplification and translates the required changes to the
persistence diagram into changes on large subsets of the domain, including both
critical and regular points. This approach enables a faster and more precise
topological regularization, the benefits of which we illustrate with
experimental evidence.
</p>
<a href="http://arxiv.org/abs/2011.05290" target="_blank">arXiv:2011.05290</a> [<a href="http://arxiv.org/pdf/2011.05290" target="_blank">pdf</a>]

<h2>Computational Design and Fabrication of Corrugated Mechanisms from Behavioral Specifications. (arXiv:2011.05298v1 [cs.RO])</h2>
<h3>Chang Liu, Wenzhong Yan, Ankur Mehta</h3>
<p>Orthogonally assembled double-layered corrugated (OADLC) mechanisms are a
class of foldable structures that harness origami-inspired methods to enhance
the structural stiffness of resulting devices; these mechanisms have extensive
applications due to their lightweight, compact nature as well as their high
strength-to-weight ratio. However, the design of these mechanisms remains
challenging. Here, we propose an efficient method to rapidly design OADLC
mechanisms from desired behavioral specifications, i.e. in-plane stiffness and
out-of-plane stiffness. Based on an equivalent plate model, we develop and
validate analytical formulas for the behavioral specifications of OADLC
mechanisms; the analytical formulas can be described as expressions of design
parameters. On the basis of the analytical expressions, we formulate the design
of OADLC mechanisms from behavioral specifications into an optimization problem
that minimizes the weight with given design constraints. The 2D folding
patterns of the optimized OADLC mechanisms can be generated automatically and
directly delivered for fabrication. Our rapid design method is demonstrated by
developing stiffness-enhanced mechanisms with a desired out-of-plane stiffness
for a foldable gripper that enables a blimp to perch steadily under air
disturbance and weight limit.
</p>
<a href="http://arxiv.org/abs/2011.05298" target="_blank">arXiv:2011.05298</a> [<a href="http://arxiv.org/pdf/2011.05298" target="_blank">pdf</a>]

<h2>Node Attribute Completion in Knowledge Graphs with Multi-Relational Propagation. (arXiv:2011.05301v1 [cs.LG])</h2>
<h3>Eda Bayram, Alberto Garcia-Duran, Robert West</h3>
<p>The existing literature on knowledge graph completion mostly focuses on the
link prediction task. However, knowledge graphs have an additional
incompleteness problem: their nodes possess numerical attributes, whose values
are often missing. Our approach, denoted as MrAP, imputes the values of missing
attributes by propagating information across the multi-relational structure of
a knowledge graph. It employs regression functions for predicting one node
attribute from another depending on the relationship between the nodes and the
type of the attributes. The propagation mechanism operates iteratively in a
message passing scheme that collects the predictions at every iteration and
updates the value of the node attributes. Experiments over two benchmark
datasets show the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2011.05301" target="_blank">arXiv:2011.05301</a> [<a href="http://arxiv.org/pdf/2011.05301" target="_blank">pdf</a>]

<h2>On Estimating the Training Cost of Conversational Recommendation Systems. (arXiv:2011.05302v1 [cs.LG])</h2>
<h3>Stefanos Antaris, Dimitrios Rafailidis, Mohammad Aliannejadi</h3>
<p>Conversational recommendation systems have recently gain a lot of attention,
as users can continuously interact with the system over multiple conversational
turns. However, conversational recommendation systems are based on complex
neural architectures, thus the training cost of such models is high. To shed
light on the high computational training time of state-of-the art
conversational models, we examine five representative strategies and
demonstrate this issue. Furthermore, we discuss possible ways to cope with the
high training cost following knowledge distillation strategies, where we detail
the key challenges to reduce the online inference time of the high number of
model parameters in conversational recommendation systems
</p>
<a href="http://arxiv.org/abs/2011.05302" target="_blank">arXiv:2011.05302</a> [<a href="http://arxiv.org/pdf/2011.05302" target="_blank">pdf</a>]

<h2>Supervised PCA: A Multiobjective Approach. (arXiv:2011.05309v1 [stat.ML])</h2>
<h3>Alexander Ritchie, Laura Balzano, Clayton Scott</h3>
<p>Methods for supervised principal component analysis (SPCA) aim to incorporate
label information into principal component analysis (PCA), so that the
extracted features are more useful for a prediction task of interest. Prior
work on SPCA has focused primarily on optimizing prediction error, and has
neglected the value of maximizing variance explained by the extracted features.
We propose a new method for SPCA that addresses both of these objectives
jointly, and demonstrate empirically that our approach dominates existing
approaches, i.e., outperforms them with respect to both prediction error and
variation explained. Our approach accommodates arbitrary supervised learning
losses and, through a statistical reformulation, provides a novel low-rank
extension of generalized linear models.
</p>
<a href="http://arxiv.org/abs/2011.05309" target="_blank">arXiv:2011.05309</a> [<a href="http://arxiv.org/pdf/2011.05309" target="_blank">pdf</a>]

<h2>Grounding Implicit Goal Description for Robot Indoor Navigation Via Recursive Belief Update. (arXiv:2011.05319v1 [cs.RO])</h2>
<h3>Rui Chen, Jinxin Zhao, Liangjun Zhang</h3>
<p>Natural language-based robotic navigation remains a challenging problem due
to the human knowledge of navigation constraints, and destination is not
directly compatible with the robot knowledge base. In this paper, we aim to
translate natural destination commands into high-level robot navigation plans
given a map of interest. We identify grammatically associated segments of
destination description and recursively apply each of them to update a belief
distribution of an area over the given map.We train a destination grounding
model using a dataset of single-step belief update for precise, proximity, and
directional modifier types. We demonstrate our method on real-world navigation
task in an office consisting of 80 areas. Offline experimental results show
that our method can directly extract goal destination from unheard, long, and
composite text commands asked by humans. This enables users to specify their
destination goals for the robot in general and natural form. Hardware
experiment results also show that the designed model brings much convenience
for specifying a navigation goal to a service robot.
</p>
<a href="http://arxiv.org/abs/2011.05319" target="_blank">arXiv:2011.05319</a> [<a href="http://arxiv.org/pdf/2011.05319" target="_blank">pdf</a>]

<h2>Robotic Exploration of Unknown 2D Environment Using a Frontier-based Automatic-Differentiable Information Gain Measure. (arXiv:2011.05323v1 [cs.RO])</h2>
<h3>Di Deng, Runlin Duan, Jiahong Liu, Kuangjie Sheng, Kenji Shimada</h3>
<p>At the heart of path-planning methods for autonomous robotic exploration is a
heuristic which encourages exploring unknown regions of the environment. Such
heuristics are typically computed using frontier-based or information-theoretic
methods. Frontier-based methods define the information gain of an exploration
path as the number of boundary cells, or frontiers, which are visible from the
path. However, the discrete and non-differentiable nature of this measure of
information gain makes it difficult to optimize using gradient-based methods.
In contrast, information-theoretic methods define information gain as the
mutual information between the sensor's measurements and the explored map.
However, computation of the gradient of mutual information involves finite
differencing and is thus computationally expensive. This work proposes an
exploration planning framework that combines sampling-based path planning and
gradient-based path optimization. The main contribution of this framework is a
novel reformulation of information gain as a differentiable function. This
allows us to simultaneously optimize information gain with other differentiable
quality measures, such as smoothness. The proposed planning framework's
effectiveness is verified both in simulation and in hardware experiments using
a Turtlebot3 Burger robot.
</p>
<a href="http://arxiv.org/abs/2011.05323" target="_blank">arXiv:2011.05323</a> [<a href="http://arxiv.org/pdf/2011.05323" target="_blank">pdf</a>]

<h2>Sparse Stochastic Zeroth-Order Optimization with an Application to Bandit Structured Prediction. (arXiv:1806.04458v3 [stat.ML] UPDATED)</h2>
<h3>Artem Sokolov, Julian Hitschler, Mayumi Ohta, Stefan Riezler</h3>
<p>Stochastic zeroth-order (SZO), or gradient-free, optimization allows to
optimize arbitrary functions by relying only on function evaluations under
parameter perturbations, however, the iteration complexity of SZO methods
suffers a factor proportional to the dimensionality of the perturbed function.
We show that in scenarios with natural sparsity patterns as in structured
prediction applications, this factor can be reduced to the expected number of
active features over input-output pairs. We give a general proof that applies
sparse SZO optimization to Lipschitz-continuous, nonconvex, stochastic
objectives, and present an experimental evaluation on linear bandit structured
prediction tasks with sparse word-based feature representations that confirm
our theoretical results.
</p>
<a href="http://arxiv.org/abs/1806.04458" target="_blank">arXiv:1806.04458</a> [<a href="http://arxiv.org/pdf/1806.04458" target="_blank">pdf</a>]

<h2>PointConv: Deep Convolutional Networks on 3D Point Clouds. (arXiv:1811.07246v3 [cs.CV] UPDATED)</h2>
<h3>Wenxuan Wu, Zhongang Qi, Li Fuxin</h3>
<p>Unlike images which are represented in regular dense grids, 3D point clouds
are irregular and unordered, hence applying convolution on them can be
difficult. In this paper, we extend the dynamic filter to a new convolution
operation, named PointConv. PointConv can be applied on point clouds to build
deep convolutional networks. We treat convolution kernels as nonlinear
functions of the local coordinates of 3D points comprised of weight and density
functions. With respect to a given point, the weight functions are learned with
multi-layer perceptron networks and density functions through kernel density
estimation. The most important contribution of this work is a novel
reformulation proposed for efficiently computing the weight functions, which
allowed us to dramatically scale up the network and significantly improve its
performance. The learned convolution kernel can be used to compute
translation-invariant and permutation-invariant convolution on any point set in
the 3D space. Besides, PointConv can also be used as deconvolution operators to
propagate features from a subsampled point cloud back to its original
resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep
convolutional neural networks built on PointConv are able to achieve
state-of-the-art on challenging semantic segmentation benchmarks on 3D point
clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed
that networks built on PointConv can match the performance of convolutional
networks in 2D images of a similar structure.
</p>
<a href="http://arxiv.org/abs/1811.07246" target="_blank">arXiv:1811.07246</a> [<a href="http://arxiv.org/pdf/1811.07246" target="_blank">pdf</a>]

<h2>A Primal-dual Learning Algorithm for Personalized Dynamic Pricing with an Inventory Constraint. (arXiv:1812.09234v2 [cs.LG] UPDATED)</h2>
<h3>Ningyuan Chen, Guillermo Gallego</h3>
<p>We consider the problem of a firm seeking to use personalized pricing to sell
an exogenously given stock of a product over a finite selling horizon to
different consumer types. We assume that the type of an arriving consumer can
be observed but the demand function associated with each type is initially
unknown. The firm sets personalized prices dynamically for each type and
attempts to maximize the revenue over the season. We provide a learning
algorithm that is near-optimal when the demand and capacity scale in
proportion. The algorithm utilizes the primal-dual formulation of the problem
and learns the dual optimal solution explicitly. It allows the algorithm to
overcome the curse of dimensionality (the rate of regret is independent of the
number of types) and sheds light on novel algorithmic designs for learning
problems with resource constraints.
</p>
<a href="http://arxiv.org/abs/1812.09234" target="_blank">arXiv:1812.09234</a> [<a href="http://arxiv.org/pdf/1812.09234" target="_blank">pdf</a>]

<h2>CoCoNet: A Collaborative Convolutional Network. (arXiv:1901.09886v4 [cs.CV] UPDATED)</h2>
<h3>Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal</h3>
<p>We present an end-to-end deep network for fine-grained visual categorization
called Collaborative Convolutional Network (CoCoNet). The network uses a
collaborative layer after the convolutional layers to represent an image as an
optimal weighted collaboration of features learned from training samples as a
whole rather than one at a time. This gives CoCoNet more power to encode the
fine-grained nature of the data with limited samples. We perform a detailed
study of the performance with 1-stage and 2-stage transfer learning. The
ablation study shows that the proposed method outperforms its constituent parts
consistently. CoCoNet also outperforms few state-of-the-art competing methods.
Experiments have been performed on the fine-grained bird species classification
problem as a representative example, but the method may be applied to other
similar tasks. We also introduce a new public dataset for fine-grained species
recognition, that of Indian endemic birds and have reported initial results on
it.
</p>
<a href="http://arxiv.org/abs/1901.09886" target="_blank">arXiv:1901.09886</a> [<a href="http://arxiv.org/pdf/1901.09886" target="_blank">pdf</a>]

<h2>The Cost of Privacy: Optimal Rates of Convergence for Parameter Estimation with Differential Privacy. (arXiv:1902.04495v5 [stat.ML] UPDATED)</h2>
<h3>T. Tony Cai, Yichen Wang, Linjun Zhang</h3>
<p>Privacy-preserving data analysis is a rising challenge in contemporary
statistics, as the privacy guarantees of statistical methods are often achieved
at the expense of accuracy. In this paper, we investigate the tradeoff between
statistical accuracy and privacy in mean estimation and linear regression,
under both the classical low-dimensional and modern high-dimensional settings.
A primary focus is to establish minimax optimality for statistical estimation
with the $(\varepsilon,\delta)$-differential privacy constraint. To this end,
we find that classical lower bound arguments fail to yield sharp results, and
new technical tools are called for.

By refining the "tracing adversary" technique for lower bounds in the
theoretical computer science literature, we formulate a general lower bound
argument for minimax risks with differential privacy constraints, and apply
this argument to high-dimensional mean estimation and linear regression
problems. We also design computationally efficient algorithms that attain the
minimax lower bounds up to a logarithmic factor. In particular, for the
high-dimensional linear regression, a novel private iterative hard thresholding
pursuit algorithm is proposed, based on a privately truncated version of
stochastic gradient descent. The numerical performance of these algorithms is
demonstrated by simulation studies and applications to real data containing
sensitive information, for which privacy-preserving statistical methods are
necessary.
</p>
<a href="http://arxiv.org/abs/1902.04495" target="_blank">arXiv:1902.04495</a> [<a href="http://arxiv.org/pdf/1902.04495" target="_blank">pdf</a>]

<h2>PProCRC: Probabilistic Collaboration of Image Patches. (arXiv:1903.09123v3 [cs.CV] UPDATED)</h2>
<h3>Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal</h3>
<p>We present a conditional probabilistic framework for collaborative
representation of image patches. It incorporates background compensation and
outlier patch suppression into the main formulation itself, thus doing away
with the need for pre-processing steps to handle the same. A closed form
non-iterative solution of the cost function is derived. The proposed method
(PProCRC) outperforms earlier CRC formulations: patch based (PCRC, GP-CRC) as
well as the state-of-the-art probabilistic (ProCRC and EProCRC) on three
fine-grained species recognition datasets (Oxford Flowers, Oxford-IIIT Pets and
CUB Birds) using two CNN backbones (Vgg-19 and ResNet-50).
</p>
<a href="http://arxiv.org/abs/1903.09123" target="_blank">arXiv:1903.09123</a> [<a href="http://arxiv.org/pdf/1903.09123" target="_blank">pdf</a>]

<h2>Classified Regression for Bayesian Optimization: Robot Learning with Unknown Penalties. (arXiv:1907.10383v2 [cs.LG] UPDATED)</h2>
<h3>Alonso Marco, Dominik Baumann, Philipp Hennig, Sebastian Trimpe</h3>
<p>Learning robot controllers by minimizing a black-box objective cost using
Bayesian optimization (BO) can be time-consuming and challenging. It is very
often the case that some roll-outs result in failure behaviors, causing
premature experiment detention. In such cases, the designer is forced to decide
on heuristic cost penalties because the acquired data is often scarce, or not
comparable with that of the stable policies. To overcome this, we propose a
Bayesian model that captures exactly what we know about the cost of unstable
controllers prior to data collection: Nothing, except that it should be a
somewhat large number. The resulting Bayesian model, approximated with a
Gaussian process, predicts high cost values in regions where failures are
likely to occur. In this way, the model guides the BO exploration toward
regions of stability. We demonstrate the benefits of the proposed model in
several illustrative and statistical synthetic benchmarks, and also in
experiments on a real robotic platform. In addition, we propose and
experimentally validate a new BO method to account for unknown constraints.
Such method is an extension of Max-Value Entropy Search, a recent
information-theoretic method, to solve unconstrained global optimization
problems.
</p>
<a href="http://arxiv.org/abs/1907.10383" target="_blank">arXiv:1907.10383</a> [<a href="http://arxiv.org/pdf/1907.10383" target="_blank">pdf</a>]

<h2>Task-Assisted Domain Adaptation with Anchor Tasks. (arXiv:1908.06079v3 [cs.CV] UPDATED)</h2>
<h3>Zhizhong Li, Linjie Luo, Sergey Tulyakov, Qieyun Dai, Derek Hoiem</h3>
<p>Some tasks, such as surface normals or single-view depth estimation, require
per-pixel ground truth that is difficult to obtain on real images but easy to
obtain on synthetic. However, models learned on synthetic images often do not
generalize well to real images due to the domain shift. Our key idea to improve
domain adaptation is to introduce a separate anchor task (such as facial
landmarks) whose annotations can be obtained at no cost or are already
available on both synthetic and real datasets. To further leverage the implicit
relationship between the anchor and main tasks, we apply our \freeze technique
that learns the cross-task guidance on the source domain with the final network
layers, and use it on the target domain. We evaluate our methods on surface
normal estimation on two pairs of datasets (indoor scenes and faces) with two
kinds of anchor tasks (semantic segmentation and facial landmarks). We show
that blindly applying domain adaptation or training the auxiliary task on only
one domain may hurt performance, while using anchor tasks on both domains is
better behaved. Our \freeze technique outperforms competing approaches,
reaching performance in facial images on par with a recently popular surface
normal estimation method using shape from shading domain knowledge.
</p>
<a href="http://arxiv.org/abs/1908.06079" target="_blank">arXiv:1908.06079</a> [<a href="http://arxiv.org/pdf/1908.06079" target="_blank">pdf</a>]

<h2>On Posterior Collapse and Encoder Feature Dispersion in Sequence VAEs. (arXiv:1911.03976v2 [cs.LG] UPDATED)</h2>
<h3>Teng Long, Yanshuai Cao, Jackie Chi Kit Cheung</h3>
<p>Variational autoencoders (VAEs) hold great potential for modelling text, as
they could in theory separate high-level semantic and syntactic properties from
local regularities of natural language. Practically, however, VAEs with
autoregressive decoders often suffer from posterior collapse, a phenomenon
where the model learns to ignore the latent variables, causing the sequence VAE
to degenerate into a language model. In this paper, we argue that posterior
collapse is in part caused by the lack of dispersion in encoder features. We
provide empirical evidence to verify this hypothesis, and propose a
straightforward fix using pooling. This simple technique effectively prevents
posterior collapse, allowing model to achieve significantly better data
log-likelihood than standard sequence VAEs. Comparing to existing work, our
proposed method is able to achieve comparable or superior performances while
being more computationally efficient.
</p>
<a href="http://arxiv.org/abs/1911.03976" target="_blank">arXiv:1911.03976</a> [<a href="http://arxiv.org/pdf/1911.03976" target="_blank">pdf</a>]

<h2>Disentangle, align and fuse for multimodal and semi-supervised image segmentation. (arXiv:1911.04417v5 [cs.CV] UPDATED)</h2>
<h3>Agisilaos Chartsias, Giorgos Papanastasiou, Chengjia Wang, Scott Semple, David E. Newby, Rohan Dharmakumar, Sotirios A. Tsaftaris</h3>
<p>Magnetic resonance (MR) protocols rely on several sequences to assess
pathology and organ status properly. Despite advances in image analysis, we
tend to treat each sequence, here termed modality, in isolation. Taking
advantage of the common information shared between modalities (an organ's
anatomy) is beneficial for multi-modality processing and learning. However, we
must overcome inherent anatomical misregistrations and disparities in signal
intensity across the modalities to obtain this benefit. We present a method
that offers improved segmentation accuracy of the modality of interest (over a
single input model), by learning to leverage information present in other
modalities, even if few (semi-supervised) or no (unsupervised) annotations are
available for this specific modality. Core to our method is learning a
disentangled decomposition into anatomical and imaging factors. Shared
anatomical factors from the different inputs are jointly processed and fused to
extract more accurate segmentation masks. Image misregistrations are corrected
with a Spatial Transformer Network, which non-linearly aligns the anatomical
factors. The imaging factor captures signal intensity characteristics across
different modality data and is used for image reconstruction, enabling
semi-supervised learning. Temporal and slice pairing between inputs are learned
dynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) and
Blood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2
abdominal segmentation. Code is available at
https://github.com/vios-s/multimodal_segmentation.
</p>
<a href="http://arxiv.org/abs/1911.04417" target="_blank">arXiv:1911.04417</a> [<a href="http://arxiv.org/pdf/1911.04417" target="_blank">pdf</a>]

<h2>Curriculum Self-Paced Learning for Cross-Domain Object Detection. (arXiv:1911.06849v2 [cs.CV] UPDATED)</h2>
<h3>Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe</h3>
<p>Training (source) domain bias affects state-of-the-art object detectors, such
as Faster R-CNN, when applied to new (target) domains. To alleviate this
problem, researchers proposed various domain adaptation methods to improve
object detection results in the cross-domain setting, e.g. by translating
images with ground-truth labels from the source domain to the target domain
using Cycle-GAN. On top of combining Cycle-GAN transformations and self-paced
learning in a smart and efficient way, in this paper, we propose a novel
self-paced algorithm that learns from easy to hard. Our method is simple and
effective, without any overhead during inference. It uses only pseudo-labels
for samples taken from the target domain, i.e. the domain adaptation is
unsupervised. We conduct experiments on four cross-domain benchmarks, showing
better results than the state of the art. We also perform an ablation study
demonstrating the utility of each component in our framework. Additionally, we
study the applicability of our framework to other object detectors.
Furthermore, we compare our difficulty measure with other measures from the
related literature, proving that it yields superior results and that it
correlates well with the performance metric.
</p>
<a href="http://arxiv.org/abs/1911.06849" target="_blank">arXiv:1911.06849</a> [<a href="http://arxiv.org/pdf/1911.06849" target="_blank">pdf</a>]

<h2>STConvS2S: Spatiotemporal Convolutional Sequence to Sequence Network for Weather Forecasting. (arXiv:1912.00134v4 [cs.LG] UPDATED)</h2>
<h3>Rafaela Castro, Yania M. Souto, Eduardo Ogasawara, Fabio Porto, Eduardo Bezerra</h3>
<p>Applying machine learning models to meteorological data brings many
opportunities to the Geosciences field, such as predicting future weather
conditions more accurately. In recent years, modeling meteorological data with
deep neural networks has become a relevant area of investigation. These works
apply either recurrent neural networks (RNN) or some hybrid approach mixing RNN
and convolutional neural networks (CNN). In this work, we propose STConvS2S
(Spatiotemporal Convolutional Sequence to Sequence Network), a deep learning
architecture built for learning both spatial and temporal data dependencies
using only convolutional layers. Our proposed architecture resolves two
limitations of convolutional networks to predict sequences using historical
data: (1) they violate the temporal order during the learning process and (2)
they require the lengths of the input and output sequences to be equal.
Computational experiments using air temperature and rainfall data from South
America show that our architecture captures spatiotemporal context and that it
outperforms or matches the results of state-of-the-art architectures for
forecasting tasks. In particular, one of the variants of our proposed
architecture is 23% better at predicting future sequences and five times faster
at training than the RNN-based model used as a baseline.
</p>
<a href="http://arxiv.org/abs/1912.00134" target="_blank">arXiv:1912.00134</a> [<a href="http://arxiv.org/pdf/1912.00134" target="_blank">pdf</a>]

<h2>Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving. (arXiv:1912.02249v2 [cs.CV] UPDATED)</h2>
<h3>Michal Uricar, Ganesh Sistu, Hazem Rashed, Antonin Vobecky, Varun Ravi Kumar, Pavel Krizek, Fabian Burger, Senthil Yogamani</h3>
<p>Wide-angle fisheye cameras are commonly used in automated driving for parking
and low-speed navigation tasks. Four of such cameras form a surround-view
system that provides a complete and detailed view of the vehicle. These cameras
are directly exposed to harsh environmental settings and can get soiled very
easily by mud, dust, water, frost. Soiling on the camera lens can severely
degrade the visual perception algorithms, and a camera cleaning system
triggered by a soiling detection algorithm is increasingly being deployed.
While adverse weather conditions, such as rain, are getting attention recently,
there is only limited work on general soiling. The main reason is the
difficulty in collecting a diverse dataset as it is a relatively rare event. We
propose a novel GAN based algorithm for generating unseen patterns of soiled
images. Additionally, the proposed method automatically provides the
corresponding soiling masks eliminating the manual annotation cost.
Augmentation of the generated soiled images for training improves the accuracy
of soiling detection tasks significantly by 18% demonstrating its usefulness.
The manually annotated soiling dataset and the generated augmentation dataset
will be made public. We demonstrate the generalization of our fisheye trained
GAN model on the Cityscapes dataset. We provide an empirical evaluation of the
degradation of the semantic segmentation algorithm with the soiled data.
</p>
<a href="http://arxiv.org/abs/1912.02249" target="_blank">arXiv:1912.02249</a> [<a href="http://arxiv.org/pdf/1912.02249" target="_blank">pdf</a>]

<h2>Training Deep Neural Networks for Interpretability and Adversarial Robustness. (arXiv:1912.03430v5 [cs.LG] UPDATED)</h2>
<h3>Adam Noack, Isaac Ahern, Dejing Dou, Boyang Li</h3>
<p>Deep neural networks (DNNs) have had many successes, but they suffer from two
major issues: (1) a vulnerability to adversarial examples and (2) a tendency to
elude human interpretation. Interestingly, recent empirical and theoretical
evidence suggests these two seemingly disparate issues are actually connected.
In particular, robust models tend to provide more interpretable gradients than
non-robust models. However, whether this relationship works in the opposite
direction remains obscure. With this paper, we seek empirical answers to the
following question: can models acquire adversarial robustness when they are
trained to have interpretable gradients? We introduce a theoretically inspired
technique called Interpretation Regularization (IR), which encourages a model's
gradients to (1) match the direction of interpretable target salience maps and
(2) have small magnitude. To assess model performance and tease apart factors
that contribute to adversarial robustness, we conduct extensive experiments on
MNIST and CIFAR-10 with both $\ell_2$ and $\ell_\infty$ attacks. We demonstrate
that training the networks to have interpretable gradients improves their
robustness to adversarial perturbations. Applying the network interpretation
technique SmoothGrad yields additional performance gains, especially in
cross-norm attacks and under heavy perturbations. The results indicate that the
interpretability of the model gradients is a crucial factor for adversarial
robustness. Code for the experiments can be found at
https://github.com/a1noack/interp_regularization.
</p>
<a href="http://arxiv.org/abs/1912.03430" target="_blank">arXiv:1912.03430</a> [<a href="http://arxiv.org/pdf/1912.03430" target="_blank">pdf</a>]

<h2>HyperCon: Image-To-Video Model Transfer for Video-To-Video Translation Tasks. (arXiv:1912.04950v2 [cs.CV] UPDATED)</h2>
<h3>Ryan Szeto, Mostafa El-Khamy, Jungwon Lee, Jason J. Corso</h3>
<p>Video-to-video translation is more difficult than image-to-image translation
due to the temporal consistency problem that, if unaddressed, leads to
distracting flickering effects. Although video models designed from scratch
produce temporally consistent results, training them to match the vast visual
knowledge captured by image models requires an intractable number of videos. To
combine the benefits of image and video models, we propose an image-to-video
model transfer method called Hyperconsistency (HyperCon) that transforms any
well-trained image model into a temporally consistent video model without
fine-tuning. HyperCon works by translating a temporally interpolated video
frame-wise and then aggregating over temporally localized windows on the
interpolated video. It handles both masked and unmasked inputs, enabling
support for even more video-to-video translation tasks than prior
image-to-video model transfer techniques. We demonstrate HyperCon on video
style transfer and inpainting, where it performs favorably compared to prior
state-of-the-art methods without training on a single stylized or incomplete
video. Our project website is available at
https://ryanszeto.com/projects/hypercon .
</p>
<a href="http://arxiv.org/abs/1912.04950" target="_blank">arXiv:1912.04950</a> [<a href="http://arxiv.org/pdf/1912.04950" target="_blank">pdf</a>]

<h2>FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for Semi-supervised Optical Flow Estimation. (arXiv:2001.06171v2 [cs.CV] UPDATED)</h2>
<h3>Xiaolin Song, Jingyu Yang, Cuiling Lan, Wenjun Zeng</h3>
<p>Optical flow estimation is an important yet challenging problem in the field
of video analytics. The features of different semantics levels/layers of a
convolutional neural network can provide information of different granularity.
To exploit such flexible and comprehensive information, we propose a
semi-supervised Feature Pyramidal Correlation and Residual Reconstruction
Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of
two main modules: pyramid correlation mapping and residual reconstruction. The
pyramid correlation mapping module takes advantage of the multi-scale
correlations of global/local patches by aggregating features of different
scales to form a multi-level cost volume. The residual reconstruction module
aims to reconstruct the sub-band high-frequency residuals of finer optical flow
in each stage. Based on the pyramid correlation mapping, we further propose a
correlation-warping-normalization (CWN) module to efficiently exploit the
correlation dependency. Experiment results show that the proposed scheme
achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and
0.10 in terms of average end-point error (AEE) against competing baseline
methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel
dataset, respectively.
</p>
<a href="http://arxiv.org/abs/2001.06171" target="_blank">arXiv:2001.06171</a> [<a href="http://arxiv.org/pdf/2001.06171" target="_blank">pdf</a>]

<h2>Augmenting GAIL with BC for sample efficient imitation learning. (arXiv:2001.07798v4 [cs.LG] UPDATED)</h2>
<h3>Rohit Jena, Changliu Liu, Katia Sycara</h3>
<p>Imitation learning is the problem of recovering an expert policy without
access to a reward signal. Behavior cloning and GAIL are two widely used
methods for performing imitation learning. Behavior cloning converges in a few
iterations but doesn't achieve peak performance due to its inherent iid
assumption about the state-action distribution. GAIL addresses the issue by
accounting for the temporal dependencies when performing a state distribution
matching between the agent and the expert. Although GAIL is sample efficient in
the number of expert trajectories required, it is still not very sample
efficient in terms of the environment interactions needed for convergence of
the policy. Given the complementary benefits of both methods, we present a
simple and elegant method to combine both methods to enable stable and sample
efficient learning. Our algorithm is very simple to implement and integrates
with different policy gradient algorithms. We demonstrate the effectiveness of
the algorithm in low dimensional control tasks, gridworlds and in high
dimensional image-based tasks.
</p>
<a href="http://arxiv.org/abs/2001.07798" target="_blank">arXiv:2001.07798</a> [<a href="http://arxiv.org/pdf/2001.07798" target="_blank">pdf</a>]

<h2>Learning Deep Analysis Dictionaries for Image Super-Resolution. (arXiv:2001.12010v2 [stat.ML] UPDATED)</h2>
<h3>Jun-Jie Huang, Pier Luigi Dragotti</h3>
<p>Inspired by the recent success of deep neural networks and the recent efforts
to develop multi-layer dictionary models, we propose a Deep Analysis dictionary
Model (DeepAM) which is optimized to address a specific regression task known
as single image super-resolution. Contrary to other multi-layer dictionary
models, our architecture contains L layers of analysis dictionary and
soft-thresholding operators to gradually extract high-level features and a
layer of synthesis dictionary which is designed to optimize the regression task
at hand. In our approach, each analysis dictionary is partitioned into two
sub-dictionaries: an Information Preserving Analysis Dictionary (IPAD) and a
Clustering Analysis Dictionary (CAD). The IPAD together with the corresponding
soft-thresholds is designed to pass the key information from the previous layer
to the next layer, while the CAD together with the corresponding
soft-thresholding operator is designed to produce a sparse feature
representation of its input data that facilitates discrimination of key
features. DeepAM uses both supervised and unsupervised setup. Simulation
results show that the proposed deep analysis dictionary model achieves better
performance compared to a deep neural network that has the same structure and
is optimized using back-propagation when training datasets are small.
</p>
<a href="http://arxiv.org/abs/2001.12010" target="_blank">arXiv:2001.12010</a> [<a href="http://arxiv.org/pdf/2001.12010" target="_blank">pdf</a>]

<h2>Statistical Optimal Transport posed as Learning Kernel Embedding. (arXiv:2002.03179v6 [cs.LG] UPDATED)</h2>
<h3>J. Saketha Nath (IIT Hyderabad, INDIA), Pratik Jawanpuria (Microsoft IDC, INDIA)</h3>
<p>The objective in statistical Optimal Transport (OT) is to consistently
estimate the optimal transport plan/map solely using samples from the given
source and target marginal distributions. This work takes the novel approach of
posing statistical OT as that of learning the transport plan's kernel mean
embedding from sample based estimates of marginal embeddings. The proposed
estimator controls overfitting by employing maximum mean discrepancy based
regularization, which is complementary to $\phi$-divergence (entropy) based
regularization popularly employed in existing estimators. A key result is that,
under very mild conditions, $\epsilon$-optimal recovery of the transport plan
as well as the Barycentric-projection based transport map is possible with a
sample complexity that is completely dimension-free. Moreover, the implicit
smoothing in the kernel mean embeddings enables out-of-sample estimation. An
appropriate representer theorem is proved leading to a kernelized convex
formulation for the estimator, which can then be potentially used to perform OT
even in non-standard domains. Empirical results illustrate the efficacy of the
proposed approach.
</p>
<a href="http://arxiv.org/abs/2002.03179" target="_blank">arXiv:2002.03179</a> [<a href="http://arxiv.org/pdf/2002.03179" target="_blank">pdf</a>]

<h2>Robust Optimization for Fairness with Noisy Protected Groups. (arXiv:2002.09343v3 [cs.LG] UPDATED)</h2>
<h3>Serena Wang, Wenshuo Guo, Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, Michael I. Jordan</h3>
<p>Many existing fairness criteria for machine learning involve equalizing some
metric across protected groups such as race or gender. However, practitioners
trying to audit or enforce such group-based criteria can easily face the
problem of noisy or biased protected group information. First, we study the
consequences of naively relying on noisy protected group labels: we provide an
upper bound on the fairness violations on the true groups G when the fairness
criteria are satisfied on noisy groups $\hat{G}$. Second, we introduce two new
approaches using robust optimization that, unlike the naive approach of only
relying on $\hat{G}$, are guaranteed to satisfy fairness criteria on the true
protected groups G while minimizing a training objective. We provide
theoretical guarantees that one such approach converges to an optimal feasible
solution. Using two case studies, we show empirically that the robust
approaches achieve better true group fairness guarantees than the naive
approach.
</p>
<a href="http://arxiv.org/abs/2002.09343" target="_blank">arXiv:2002.09343</a> [<a href="http://arxiv.org/pdf/2002.09343" target="_blank">pdf</a>]

<h2>HYDRA: Pruning Adversarially Robust Neural Networks. (arXiv:2002.10509v3 [cs.CV] UPDATED)</h2>
<h3>Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana</h3>
<p>In safety-critical but computationally resource-constrained applications,
deep learning faces two key challenges: lack of robustness against adversarial
attacks and large neural network size (often millions of parameters). While the
research community has extensively explored the use of robust training and
network pruning independently to address one of these challenges, only a few
recent works have studied them jointly. However, these works inherit a
heuristic pruning strategy that was developed for benign training, which
performs poorly when integrated with robust training techniques, including
adversarial training and verifiable robust training. To overcome this
challenge, we propose to make pruning techniques aware of the robust training
objective and let the training objective guide the search for which connections
to prune. We realize this insight by formulating the pruning objective as an
empirical risk minimization problem which is solved efficiently using SGD. We
demonstrate that our approach, titled HYDRA, achieves compressed networks with
state-of-the-art benign and robust accuracy, simultaneously. We demonstrate the
success of our approach across CIFAR-10, SVHN, and ImageNet dataset with four
robust training techniques: iterative adversarial training, randomized
smoothing, MixTrain, and CROWN-IBP. We also demonstrate the existence of highly
robust sub-networks within non-robust networks. Our code and compressed
networks are publicly available at
\url{https://github.com/inspire-group/compactness-robustness}.
</p>
<a href="http://arxiv.org/abs/2002.10509" target="_blank">arXiv:2002.10509</a> [<a href="http://arxiv.org/pdf/2002.10509" target="_blank">pdf</a>]

<h2>Probably Approximately Correct Vision-Based Planning using Motion Primitives. (arXiv:2002.12852v2 [cs.RO] UPDATED)</h2>
<h3>Sushant Veer, Anirudha Majumdar</h3>
<p>This paper presents an approach for learning vision-based planners that
provably generalize to novel environments (i.e., environments unseen during
training). We leverage the Probably Approximately Correct (PAC)-Bayes framework
to obtain an upper bound on the expected cost of policies across all
environments. Minimizing the PAC-Bayes upper bound thus trains policies that
are accompanied by a certificate of performance on novel environments. The
training pipeline we propose provides strong generalization guarantees for deep
neural network policies by (a) obtaining a good prior distribution on the space
of policies using Evolutionary Strategies (ES) followed by (b) formulating the
PAC-Bayes optimization as an efficiently-solvable parametric convex
optimization problem. We demonstrate the efficacy of our approach for producing
strong generalization guarantees for learned vision-based motion planners
through two simulated examples: (1) an Unmanned Aerial Vehicle (UAV) navigating
obstacle fields with an onboard vision sensor, and (2) a dynamic quadrupedal
robot traversing rough terrains with proprioceptive and exteroceptive sensors.
</p>
<a href="http://arxiv.org/abs/2002.12852" target="_blank">arXiv:2002.12852</a> [<a href="http://arxiv.org/pdf/2002.12852" target="_blank">pdf</a>]

<h2>Decentralized MCTS via Learned Teammate Models. (arXiv:2003.08727v3 [cs.AI] UPDATED)</h2>
<h3>Aleksander Czechowski, Frans A. Oliehoek</h3>
<p>Decentralized online planning can be an attractive paradigm for cooperative
multi-agent systems, due to improved scalability and robustness. A key
difficulty of such approach lies in making accurate predictions about the
decisions of other agents. In this paper, we present a trainable online
decentralized planning algorithm based on decentralized Monte Carlo Tree
Search, combined with models of teammates learned from previous episodic runs.
By only allowing one agent to adapt its models at a time, under the assumption
of ideal policy approximation, successive iterations of our method are
guaranteed to improve joint policies, and eventually lead to convergence to a
Nash equilibrium. We test the efficiency of the algorithm by performing
experiments in several scenarios of the spatial task allocation environment
introduced in [Claes et al., 2015]. We show that deep learning and
convolutional neural networks can be employed to produce accurate policy
approximators which exploit the spatial features of the problem, and that the
proposed algorithm improves over the baseline planning performance for
particularly challenging domain configurations.
</p>
<a href="http://arxiv.org/abs/2003.08727" target="_blank">arXiv:2003.08727</a> [<a href="http://arxiv.org/pdf/2003.08727" target="_blank">pdf</a>]

<h2>Do We Need Depth in State-Of-The-Art Face Authentication?. (arXiv:2003.10895v2 [cs.CV] UPDATED)</h2>
<h3>Amir Livne, Alex Bronstein, Ron Kimmel, Ziv Aviv, Shahaf Grofit</h3>
<p>Some face recognition methods are designed to utilize geometric information
extracted from depth sensors to overcome the weaknesses of single-image based
recognition technologies. However, the accurate acquisition of the depth
profile is an expensive and challenging process. Here, we introduce a novel
method that learns to recognize faces from stereo camera systems without the
need to explicitly compute the facial surface or depth map. The raw face stereo
images along with the location in the image from which the face is extracted
allow the proposed CNN to improve the recognition task while avoiding the need
to explicitly handle the geometric structure of the face. This way, we keep the
simplicity and cost efficiency of identity authentication from a single image,
while enjoying the benefits of geometric data without explicitly reconstructing
it. We demonstrate that the suggested method outperforms both existing
single-image and explicit depth based methods on large-scale benchmarks, and
even capable of recognize spoofing attacks. We also provide an ablation study
that shows that the suggested method uses the face locations in the left and
right images to encode informative features that improve the overall
performance.
</p>
<a href="http://arxiv.org/abs/2003.10895" target="_blank">arXiv:2003.10895</a> [<a href="http://arxiv.org/pdf/2003.10895" target="_blank">pdf</a>]

<h2>Estimate of the Neural Network Dimension Using Algebraic Topology and Lie Theory. (arXiv:2004.02881v8 [stat.ML] UPDATED)</h2>
<h3>Luciano Melodia, Richard Lenz</h3>
<p>In this paper we present an approach to determine the smallest possible
number of neurons in a layer of a neural network in such a way that the
topology of the input space can be learned sufficiently well. We introduce a
general procedure based on persistent homology to investigate topological
invariants of the manifold on which we suspect the data set. We specify the
required dimensions precisely, assuming that there is a smooth manifold on or
near which the data are located. Furthermore, we require that this space is
connected and has a commutative group structure in the mathematical sense.
These assumptions allow us to derive a decomposition of the underlying space
whose topology is well known. We use the representatives of the $k$-dimensional
homology groups from the persistence landscape to determine an integer
dimension for this decomposition. This number is the dimension of the embedding
that is capable of capturing the topology of the data manifold. We derive the
theory and validate it experimentally on toy data sets.
</p>
<a href="http://arxiv.org/abs/2004.02881" target="_blank">arXiv:2004.02881</a> [<a href="http://arxiv.org/pdf/2004.02881" target="_blank">pdf</a>]

<h2>Line Art Correlation Matching Feature Transfer Network for Automatic Animation Colorization. (arXiv:2004.06718v3 [cs.CV] UPDATED)</h2>
<h3>Zhang Qian, Wang Bo, Wen Wei, Li Hai, Liu Jun Hui</h3>
<p>Automatic animation line art colorization is a challenging computer vision
problem, since the information of the line art is highly sparse and abstracted
and there exists a strict requirement for the color and style consistency
between frames. Recently, a lot of Generative Adversarial Network (GAN) based
image-to-image translation methods for single line art colorization have
emerged. They can generate perceptually appealing results conditioned on line
art images. However, these methods can not be adopted for the purpose of
animation colorization because there is a lack of consideration of the
in-between frame consistency. Existing methods simply input the previous
colored frame as a reference to color the next line art, which will mislead the
colorization due to the spatial misalignment of the previous colored frame and
the next line art especially at positions where apparent changes happen. To
address these challenges, we design a kind of correlation matching feature
transfer model (called CMFT) to align the colored reference feature in a
learnable way and integrate the model into an U-Net based generator in a
coarse-to-fine manner. This enables the generator to transfer the layer-wise
synchronized features from the deep semantic code to the content progressively.
Extension evaluation shows that CMFT model can effectively improve the
in-between consistency and the quality of colored frames especially when the
motion is intense and diverse.
</p>
<a href="http://arxiv.org/abs/2004.06718" target="_blank">arXiv:2004.06718</a> [<a href="http://arxiv.org/pdf/2004.06718" target="_blank">pdf</a>]

<h2>ActionSpotter: Deep Reinforcement Learning Framework for Temporal Action Spotting in Videos. (arXiv:2004.06971v2 [cs.LG] UPDATED)</h2>
<h3>Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard (ISIR, PIROS, SU)</h3>
<p>Summarizing video content is an important task in many applications. This
task can be defined as the computation of the ordered list of actions present
in a video. Such a list could be extracted using action detection algorithms.
However, it is not necessary to determine the temporal boundaries of actions to
know their existence. Moreover, localizing precise boundaries usually requires
dense video analysis to be effective. In this work, we propose to directly
compute this ordered list by sparsely browsing the video and selecting one
frame per action instance, task known as action spotting in literature. To do
this, we propose ActionSpotter, a spotting algorithm that takes advantage of
Deep Reinforcement Learning to efficiently spot actions while adapting its
video browsing speed, without additional supervision. Experiments performed on
datasets THUMOS14 and ActivityNet show that our framework outperforms state of
the art detection methods. In particular, the spotting mean Average Precision
on THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of
video.
</p>
<a href="http://arxiv.org/abs/2004.06971" target="_blank">arXiv:2004.06971</a> [<a href="http://arxiv.org/pdf/2004.06971" target="_blank">pdf</a>]

<h2>Frequency-Weighted Robust Tensor Principal Component Analysis. (arXiv:2004.10068v2 [cs.CV] UPDATED)</h2>
<h3>Shenghan Wang, Yipeng Liu, Lanlan Feng, Ce Zhu</h3>
<p>Robust tensor principal component analysis (RTPCA) can separate the low-rank
component and sparse component from multidimensional data, which has been used
successfully in several image applications. Its performance varies with
different kinds of tensor decompositions, and the tensor singular value
decomposition (t-SVD) is a popularly selected one. The standard t-SVD takes the
discrete Fourier transform to exploit the residual in the 3rd mode in the
decomposition. When minimizing the tensor nuclear norm related to t-SVD, all
the frontal slices in frequency domain are optimized equally. In this paper, we
incorporate frequency component analysis into t-SVD to enhance the RTPCA
performance. Specially, different frequency bands are unequally weighted with
respect to the corresponding physical meanings, and the frequency-weighted
tensor nuclear norm can be obtained. Accordingly we rigorously deduce the
frequency-weighted tensor singular value threshold operator, and apply it for
low rank approximation subproblem in RTPCA. The newly obtained
frequency-weighted RTPCA can be solved by alternating direction method of
multipliers, and it is the first time that frequency analysis is taken in
tensor principal component analysis. Numerical experiments on synthetic 3D
data, color image denoising and background modeling verify that the proposed
method outperforms the state-of-the-art algorithms both in accuracy and
computational complexity.
</p>
<a href="http://arxiv.org/abs/2004.10068" target="_blank">arXiv:2004.10068</a> [<a href="http://arxiv.org/pdf/2004.10068" target="_blank">pdf</a>]

<h2>Spanning Attack: Reinforce Black-box Attacks with Unlabeled Data. (arXiv:2005.04871v2 [cs.LG] UPDATED)</h2>
<h3>Lu Wang, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Yuan Jiang</h3>
<p>Adversarial black-box attacks aim to craft adversarial perturbations by
querying input-output pairs of machine learning models. They are widely used to
evaluate the robustness of pre-trained models. However, black-box attacks often
suffer from the issue of query inefficiency due to the high dimensionality of
the input space, and therefore incur a false sense of model robustness. In this
paper, we relax the conditions of the black-box threat model, and propose a
novel technique called the spanning attack. By constraining adversarial
perturbations in a low-dimensional subspace via spanning an auxiliary unlabeled
dataset, the spanning attack significantly improves the query efficiency of a
wide variety of existing black-box attacks. Extensive experiments show that the
proposed method works favorably in both soft-label and hard-label black-box
attacks. Our code is available at https://github.com/wangwllu/spanning_attack.
</p>
<a href="http://arxiv.org/abs/2005.04871" target="_blank">arXiv:2005.04871</a> [<a href="http://arxiv.org/pdf/2005.04871" target="_blank">pdf</a>]

<h2>Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. (arXiv:2005.10242v9 [cs.LG] UPDATED)</h2>
<h3>Tongzhou Wang, Phillip Isola</h3>
<p>Contrastive representation learning has been outstandingly successful in
practice. In this work, we identify two key properties related to the
contrastive loss: (1) alignment (closeness) of features from positive pairs,
and (2) uniformity of the induced distribution of the (normalized) features on
the hypersphere. We prove that, asymptotically, the contrastive loss optimizes
these properties, and analyze their positive effects on downstream tasks.
Empirically, we introduce an optimizable metric to quantify each property.
Extensive experiments on standard vision and language datasets confirm the
strong agreement between both metrics and downstream task performance.
Remarkably, directly optimizing for these two metrics leads to representations
with comparable or better performance at downstream tasks than contrastive
learning.

Project Page: https://ssnl.github.io/hypersphere

Code: https://github.com/SsnL/align_uniform ,
https://github.com/SsnL/moco_align_uniform
</p>
<a href="http://arxiv.org/abs/2005.10242" target="_blank">arXiv:2005.10242</a> [<a href="http://arxiv.org/pdf/2005.10242" target="_blank">pdf</a>]

<h2>Network-to-Network Translation with Conditional Invertible Neural Networks. (arXiv:2005.13580v2 [cs.CV] UPDATED)</h2>
<h3>Robin Rombach, Patrick Esser, Bj&#xf6;rn Ommer</h3>
<p>Given the ever-increasing computational costs of modern machine learning
models, we need to find new ways to reuse such expert models and thus tap into
the resources that have been invested in their creation. Recent work suggests
that the power of these massive models is captured by the representations they
learn. Therefore, we seek a model that can relate between different existing
representations and propose to solve this task with a conditionally invertible
network. This network demonstrates its capability by (i) providing generic
transfer between diverse domains, (ii) enabling controlled content synthesis by
allowing modification in other domains, and (iii) facilitating diagnosis of
existing representations by translating them into interpretable domains such as
images. Our domain transfer network can translate between fixed representations
without having to learn or finetune them. This allows users to utilize various
existing domain-specific expert models from the literature that had been
trained with extensive computational resources. Experiments on diverse
conditional image synthesis tasks, competitive image modification results and
experiments on image-to-image and text-to-image generation demonstrate the
generic applicability of our approach. For example, we translate between BERT
and BigGAN, state-of-the-art text and image models to provide text-to-image
generation, which neither of both experts can perform on their own.
</p>
<a href="http://arxiv.org/abs/2005.13580" target="_blank">arXiv:2005.13580</a> [<a href="http://arxiv.org/pdf/2005.13580" target="_blank">pdf</a>]

<h2>DeepMark++: Real-time Clothing Detection at the Edge. (arXiv:2006.00710v3 [cs.CV] UPDATED)</h2>
<h3>Alexey Sidnev, Alexander Krapivin, Alexey Trushkov, Ekaterina Krasikova, Maxim Kazakov, Mikhail Viryasov</h3>
<p>Clothing recognition is the most fundamental AI application challenge within
the fashion domain. While existing solutions offer decent recognition accuracy,
they are generally slow and require significant computational resources. In
this paper we propose a single-stage approach to overcome this obstacle and
deliver rapid clothing detection and keypoint estimation. Our solution is based
on a multi-target network CenterNet, and we introduce several powerful
post-processing techniques to enhance performance. Our most accurate model
achieves results comparable to state-of-the-art solutions on the DeepFashion2
dataset, and our light and fast model runs at 17 FPS on the Huawei P40 Pro
smartphone. In addition, we achieved second place in the DeepFashion2 Landmark
Estimation Challenge 2020 with 0.582 mAP on the test dataset.
</p>
<a href="http://arxiv.org/abs/2006.00710" target="_blank">arXiv:2006.00710</a> [<a href="http://arxiv.org/pdf/2006.00710" target="_blank">pdf</a>]

<h2>RarePlanes: Synthetic Data Takes Flight. (arXiv:2006.02963v2 [cs.CV] UPDATED)</h2>
<h3>Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan Lewis, Daeil Kim</h3>
<p>RarePlanes is a unique open-source machine learning dataset that incorporates
both real and synthetically generated satellite imagery. The RarePlanes dataset
specifically focuses on the value of synthetic data to aid computer vision
algorithms in their ability to automatically detect aircraft and their
attributes in satellite imagery. Although other synthetic/real combination
datasets exist, RarePlanes is the largest openly-available very-high resolution
dataset built to test the value of synthetic data from an overhead perspective.
Previous research has shown that synthetic data can reduce the amount of real
training data needed and potentially improve performance for many tasks in the
computer vision domain. The real portion of the dataset consists of 253 Maxar
WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700
hand-annotated aircraft. The accompanying synthetic dataset is generated via
AI.Reverie's simulation platform and features 50,000 synthetic satellite images
simulating a total area of 9331.2 km^2 with ~630,000 aircraft annotations. Both
the real and synthetically generated aircraft feature 10 fine grain attributes
including: aircraft length, wingspan, wing-shape, wing-position, wingspan
class, propulsion, number of engines, number of vertical-stabilizers, presence
of canards, and aircraft role. Finally, we conduct extensive experiments to
evaluate the real and synthetic datasets and compare performances. By doing so,
we show the value of synthetic data for the task of detecting and classifying
aircraft from an overhead perspective.
</p>
<a href="http://arxiv.org/abs/2006.02963" target="_blank">arXiv:2006.02963</a> [<a href="http://arxiv.org/pdf/2006.02963" target="_blank">pdf</a>]

<h2>Efficient MCMC Sampling for Bayesian Matrix Factorization by Breaking Posterior Symmetries. (arXiv:2006.04295v3 [stat.ML] UPDATED)</h2>
<h3>Saibal De, Hadi Salehi, Alex Gorodetsky</h3>
<p>Bayesian low-rank matrix factorization techniques have become an essential
tool for relational data analysis and matrix completion. A standard approach is
to assign zero-mean Gaussian priors on the columns or rows of factor matrices
to create a conjugate system. This choice of prior leads to simple
implementations; however it also causes symmetries in the posterior
distribution that can severely reduce the efficiency of Markov-chain
Monte-Carlo (MCMC) sampling approaches. In this paper, we propose a simple
modification to the prior choice that provably breaks these symmetries and
maintains/improves accuracy. Specifically, we provide conditions that the
Gaussian prior mean and covariance must satisfy so the posterior does not
exhibit invariances that yield sampling difficulties. For example, we show that
using non-zero linearly independent prior means significantly lowers the
autocorrelation of MCMC samples, and can also lead to lower reconstruction
errors.
</p>
<a href="http://arxiv.org/abs/2006.04295" target="_blank">arXiv:2006.04295</a> [<a href="http://arxiv.org/pdf/2006.04295" target="_blank">pdf</a>]

<h2>Double Descent Risk and Volume Saturation Effects: A Geometric Perspective. (arXiv:2006.04366v2 [stat.ML] UPDATED)</h2>
<h3>Prasad Cheema, Mahito Sugiyama</h3>
<p>The appearance of the double-descent risk phenomenon has received growing
interest in the machine learning and statistics community, as it challenges
well-understood notions behind the U-shaped train-test curves. Motivated
through Rissanen's minimum description length (MDL), Balasubramanian's Occam's
Razor, and Amari's information geometry, we investigate how the logarithm of
the model volume: $\log V$, works to extend intuition behind the AIC and BIC
model selection criteria. We find that for the particular model classes of
isotropic linear regression and statistical lattices, the $\log V$ term may be
decomposed into a sum of distinct components, each of which assist in their
explanations of the appearance of this phenomenon. In particular they suggest
why generalization error does not necessarily continue to grow with increasing
model dimensionality.
</p>
<a href="http://arxiv.org/abs/2006.04366" target="_blank">arXiv:2006.04366</a> [<a href="http://arxiv.org/pdf/2006.04366" target="_blank">pdf</a>]

<h2>Self-Supervised Relational Reasoning for Representation Learning. (arXiv:2006.05849v3 [cs.LG] UPDATED)</h2>
<h3>Massimiliano Patacchiola, Amos Storkey</h3>
<p>In self-supervised learning, a system is tasked with achieving a surrogate
objective by defining alternative targets on a set of unlabeled data. The aim
is to build useful representations that can be used in downstream tasks,
without costly manual annotation. In this work, we propose a novel
self-supervised formulation of relational reasoning that allows a learner to
bootstrap a signal from information implicit in unlabeled data. Training a
relation head to discriminate how entities relate to themselves
(intra-reasoning) and other entities (inter-reasoning), results in rich and
descriptive representations in the underlying neural network backbone, which
can be used in downstream tasks such as classification and image retrieval. We
evaluate the proposed method following a rigorous experimental procedure, using
standard datasets, protocols, and backbones. Self-supervised relational
reasoning outperforms the best competitor in all conditions by an average 14%
in accuracy, and the most recent state-of-the-art model by 3%. We link the
effectiveness of the method to the maximization of a Bernoulli log-likelihood,
which can be considered as a proxy for maximizing the mutual information,
resulting in a more efficient objective with respect to the commonly used
contrastive losses.
</p>
<a href="http://arxiv.org/abs/2006.05849" target="_blank">arXiv:2006.05849</a> [<a href="http://arxiv.org/pdf/2006.05849" target="_blank">pdf</a>]

<h2>Fast and Flexible Temporal Point Processes with Triangular Maps. (arXiv:2006.12631v2 [cs.LG] UPDATED)</h2>
<h3>Oleksandr Shchur, Nicholas Gao, Marin Bilo&#x161;, Stephan G&#xfc;nnemann</h3>
<p>Temporal point process (TPP) models combined with recurrent neural networks
provide a powerful framework for modeling continuous-time event data. While
such models are flexible, they are inherently sequential and therefore cannot
benefit from the parallelism of modern hardware. By exploiting the recent
developments in the field of normalizing flows, we design TriTPP -- a new class
of non-recurrent TPP models, where both sampling and likelihood computation can
be done in parallel. TriTPP matches the flexibility of RNN-based methods but
permits orders of magnitude faster sampling. This enables us to use the new
model for variational inference in continuous-time discrete-state systems. We
demonstrate the advantages of the proposed framework on synthetic and
real-world datasets.
</p>
<a href="http://arxiv.org/abs/2006.12631" target="_blank">arXiv:2006.12631</a> [<a href="http://arxiv.org/pdf/2006.12631" target="_blank">pdf</a>]

<h2>Learning Data Augmentation with Online Bilevel Optimization for Image Classification. (arXiv:2006.14699v2 [cs.CV] UPDATED)</h2>
<h3>Saypraseuth Mounsaveng, Issam Laradji, Ismail Ben Ayed, David Vazquez, Marco Pedersoli</h3>
<p>Data augmentation is a key practice in machine learning for improving
generalization performance. However, finding the best data augmentation
hyperparameters requires domain knowledge or a computationally demanding
search. We address this issue by proposing an efficient approach to
automatically train a network that learns an effective distribution of
transformations to improve its generalization. Using bilevel optimization, we
directly optimize the data augmentation parameters using a validation set. This
framework can be used as a general solution to learn the optimal data
augmentation jointly with an end task model like a classifier. Results show
that our joint training method produces an image classification accuracy that
is comparable to or better than carefully hand-crafted data augmentation. Yet,
it does not need an expensive external validation loop on the data augmentation
hyperparameters.
</p>
<a href="http://arxiv.org/abs/2006.14699" target="_blank">arXiv:2006.14699</a> [<a href="http://arxiv.org/pdf/2006.14699" target="_blank">pdf</a>]

<h2>Bandit Overfitting in Offline Policy Learning. (arXiv:2006.15368v2 [cs.LG] UPDATED)</h2>
<h3>David Brandfonbrener, William F. Whitney, Rajesh Ranganath, Joan Bruna</h3>
<p>We study the offline policy learning problem in a contextual bandit
framework. Specifically, we focus on the issue of overfitting which is
especially important in a modern context where we often use overparameterized
models that can interpolate the data. Our first contribution is to introduce a
regret decomposition into approximation, estimation, and bandit errors that
emphasizes the distinction between the policy learning and supervised learning
problems. The bandit error measures the error from overfitting to the single
action observed at each context, which we call "bandit overfitting". Our second
contribution is to show both in theory and experiments how bandit overfitting
is different for policy-based versus value-based algorithms when we use
overparameterized models. We find that bandit overfitting can become a severe
problem for policy-based algorithms, but value-based algorithms effectively
reduce the policy learning problem to regression and thus avoid the worst
problems of bandit overfitting.
</p>
<a href="http://arxiv.org/abs/2006.15368" target="_blank">arXiv:2006.15368</a> [<a href="http://arxiv.org/pdf/2006.15368" target="_blank">pdf</a>]

<h2>Simplifying Models with Unlabeled Output Data. (arXiv:2006.16205v2 [cs.LG] UPDATED)</h2>
<h3>Sang Michael Xie, Tengyu Ma, Percy Liang</h3>
<p>We focus on prediction problems with high-dimensional outputs that are
subject to output validity constraints, e.g. a pseudocode-to-code translation
task where the code must compile. For these problems, labeled input-output
pairs are expensive to obtain, but "unlabeled" outputs, i.e. outputs without
corresponding inputs, are freely available and provide information about output
validity (e.g. code on GitHub). In this paper, we present predict-and-denoise,
a framework that can leverage unlabeled outputs. Specifically, we first train a
denoiser to map possibly invalid outputs to valid outputs using synthetic
perturbations of the unlabeled outputs. Second, we train a predictor composed
with this fixed denoiser. We show theoretically that for a family of functions
with a high-dimensional discrete valid output space, composing with a denoiser
reduces the complexity of a 2-layer ReLU network needed to represent the
function and that this complexity gap can be arbitrarily large. We evaluate the
framework empirically on several datasets, including image generation from
attributes and pseudocode-to-code translation. On the SPoC pseudocode-to-code
dataset, our framework improves the proportion of code outputs that pass all
test cases by 3-5% over a baseline Transformer.
</p>
<a href="http://arxiv.org/abs/2006.16205" target="_blank">arXiv:2006.16205</a> [<a href="http://arxiv.org/pdf/2006.16205" target="_blank">pdf</a>]

<h2>RGBT Salient Object Detection: A Large-scale Dataset and Benchmark. (arXiv:2007.03262v4 [cs.CV] UPDATED)</h2>
<h3>Zhengzheng Tu, Yan Ma, Zhun Li, Chenglong Li, Jieming Xu, Yongtao Liu</h3>
<p>Salient object detection in complex scenes and environments is a challenging
research topic. Most works focus on RGB-based salient object detection, which
limits its performance of real-life applications when confronted with adverse
conditions such as dark environments and complex backgrounds. Taking advantage
of RGB and thermal infrared images becomes a new research direction for
detecting salient object in complex scenes recently, as thermal infrared
spectrum imaging provides the complementary information and has been applied to
many computer vision tasks. However, current research for RGBT salient object
detection is limited by the lack of a large-scale dataset and comprehensive
benchmark. This work contributes such a RGBT image dataset named VT5000,
including 5000 spatially aligned RGBT image pairs with ground truth
annotations. VT5000 has 11 challenges collected in different scenes and
environments for exploring the robustness of algorithms. With this dataset, we
propose a powerful baseline approach, which extracts multi-level features
within each modality and aggregates these features of all modalities with the
attention mechanism, for accurate RGBT salient object detection. Extensive
experiments show that the proposed baseline approach outperforms the
state-of-the-art methods on VT5000 dataset and other two public datasets. In
addition, we carry out a comprehensive analysis of different algorithms of RGBT
salient object detection on VT5000 dataset, and then make several valuable
conclusions and provide some potential research directions for RGBT salient
object detection.
</p>
<a href="http://arxiv.org/abs/2007.03262" target="_blank">arXiv:2007.03262</a> [<a href="http://arxiv.org/pdf/2007.03262" target="_blank">pdf</a>]

<h2>InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning. (arXiv:2007.04589v5 [cs.LG] UPDATED)</h2>
<h3>Kwot Sin Lee, Ngoc-Trung Tran, Ngai-Man Cheung</h3>
<p>While Generative Adversarial Networks (GANs) are fundamental to many
generative modelling applications, they suffer from numerous issues. In this
work, we propose a principled framework to simultaneously mitigate two
fundamental issues in GANs: catastrophic forgetting of the discriminator and
mode collapse of the generator. We achieve this by employing for GANs a
contrastive learning and mutual information maximization approach, and perform
extensive analyses to understand sources of improvements. Our approach
significantly stabilizes GAN training and improves GAN performance for image
synthesis across five datasets under the same training and evaluation
conditions against state-of-the-art works. Our approach is simple to implement
and practical: it involves only one auxiliary objective, has a low
computational cost, and performs robustly across a wide range of training
settings and datasets without any hyperparameter tuning. For reproducibility,
our code is available in Mimicry: https://github.com/kwotsin/mimicry.
</p>
<a href="http://arxiv.org/abs/2007.04589" target="_blank">arXiv:2007.04589</a> [<a href="http://arxiv.org/pdf/2007.04589" target="_blank">pdf</a>]

<h2>Unsupervised Multi-Target Domain Adaptation Through Knowledge Distillation. (arXiv:2007.07077v3 [cs.CV] UPDATED)</h2>
<h3>Le Thanh Nguyen-Meidine, Atif Bela, Madhu Kiran, Jose Dolz, Louis-Antoine Blais-Morin, Eric Granger</h3>
<p>Unsupervised domain adaptation (UDA) seeks to alleviate the problem of domain
shift between the distribution of unlabeled data from the target domain w.r.t.
labeled data from the source domain. While the single-target UDA scenario is
well studied in the literature, Multi-Target Domain Adaptation (MTDA) remains
largely unexplored despite its practical importance, e.g., in multi-camera
video-surveillance applications. The MTDA problem can be addressed by adapting
one specialized model per target domain, although this solution is too costly
in many real-world applications. Blending multiple targets for MTDA has been
proposed, yet this solution may lead to a reduction in model specificity and
accuracy. In this paper, we propose a novel unsupervised MTDA approach to train
a CNN that can generalize well across multiple target domains. Our
Multi-Teacher MTDA (MT-MTDA) method relies on multi-teacher knowledge
distillation (KD) to iteratively distill target domain knowledge from multiple
teachers to a common student. The KD process is performed in a progressive
manner, where the student is trained by each teacher on how to perform UDA for
a specific target, instead of directly learning domain adapted features.
Finally, instead of combining the knowledge from each teacher, MT-MTDA
alternates between teachers that distill knowledge, thereby preserving the
specificity of each target (teacher) when learning to adapt to the student.
MT-MTDA is compared against state-of-the-art methods on several challenging UDA
benchmarks, and empirical results show that our proposed model can provide a
considerably higher level of accuracy across multiple target domains. Our code
is available at: https://github.com/LIVIAETS/MT-MTDA
</p>
<a href="http://arxiv.org/abs/2007.07077" target="_blank">arXiv:2007.07077</a> [<a href="http://arxiv.org/pdf/2007.07077" target="_blank">pdf</a>]

<h2>Neural Networks with Recurrent Generative Feedback. (arXiv:2007.09200v2 [cs.LG] UPDATED)</h2>
<h3>Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y. Tsao, Anima Anandkumar</h3>
<p>Neural networks are vulnerable to input perturbations such as additive noise
and adversarial attacks. In contrast, human perception is much more robust to
such perturbations. The Bayesian brain hypothesis states that human brains use
an internal generative model to update the posterior beliefs of the sensory
input. This mechanism can be interpreted as a form of self-consistency between
the maximum a posteriori (MAP) estimation of an internal generative model and
the external environment. Inspired by such hypothesis, we enforce
self-consistency in neural networks by incorporating generative recurrent
feedback. We instantiate this design on convolutional neural networks (CNNs).
The proposed framework, termed Convolutional Neural Networks with Feedback
(CNN-F), introduces a generative feedback with latent variables to existing CNN
architectures, where consistent predictions are made through alternating MAP
inference under a Bayesian framework. In the experiments, CNN-F shows
considerably improved adversarial robustness over conventional feedforward CNNs
on standard benchmarks.
</p>
<a href="http://arxiv.org/abs/2007.09200" target="_blank">arXiv:2007.09200</a> [<a href="http://arxiv.org/pdf/2007.09200" target="_blank">pdf</a>]

<h2>SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation Synergized with Semantic Segmentation for Autonomous Driving. (arXiv:2008.04017v2 [cs.CV] UPDATED)</h2>
<h3>Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim Fingscheidt, Patrick Maeder</h3>
<p>State-of-the-art self-supervised learning approaches for monocular depth
estimation usually suffer from scale ambiguity. They do not generalize well
when applied on distance estimation for complex projection models such as in
fisheye and omnidirectional cameras. This paper introduces a novel multi-task
learning strategy to improve self-supervised monocular distance estimation on
fisheye and pinhole camera images. Our contribution to this work is threefold:
Firstly, we introduce a novel distance estimation network architecture using a
self-attention based encoder coupled with robust semantic feature guidance to
the decoder that can be trained in a one-stage fashion. Secondly, we integrate
a generalized robust loss function, which improves performance significantly
while removing the need for hyperparameter tuning with the reprojection loss.
Finally, we reduce the artifacts caused by dynamic objects violating static
world assumptions using a semantic masking strategy. We significantly improve
upon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is
little work on fisheye cameras, we evaluated the proposed method on KITTI using
a pinhole model. We achieved state-of-the-art performance among self-supervised
methods without requiring an external scale estimation.
</p>
<a href="http://arxiv.org/abs/2008.04017" target="_blank">arXiv:2008.04017</a> [<a href="http://arxiv.org/pdf/2008.04017" target="_blank">pdf</a>]

<h2>Beyond Pointwise Submodularity: Non-Monotone Adaptive Submodular Maximization in Linear Time. (arXiv:2008.05004v3 [cs.LG] UPDATED)</h2>
<h3>Shaojie Tang</h3>
<p>In this paper, we study the non-monotone adaptive submodular maximization
problem subject to a cardinality constraint. We first revisit the adaptive
random greedy algorithm proposed in \citep{gotovos2015non}, where they show
that this algorithm achieves a $1/e$ approximation ratio if the objective
function is adaptive submodular and pointwise submodular. It is not clear
whether the same guarantee holds under adaptive submodularity (without
resorting to pointwise submodularity) or not. Our first contribution is to show
that the adaptive random greedy algorithm achieves a $1/e$ approximation ratio
under adaptive submodularity. One limitation of the adaptive random greedy
algorithm is that it requires $O(n\times k)$ value oracle queries, where $n$ is
the size of the ground set and $k$ is the cardinality constraint. Our second
contribution is to develop the first linear-time algorithm for the non-monotone
adaptive submodular maximization problem. Our algorithm achieves a
$1/e-\epsilon$ approximation ratio (this bound is improved to $1-1/e-\epsilon$
for monotone case), using only $O(n\epsilon^{-2}\log \epsilon^{-1})$ value
oracle queries. Notably, $O(n\epsilon^{-2}\log \epsilon^{-1})$ is independent
of the cardinality constraint. For the monotone case, we propose a faster
algorithm that achieves a $1-1/e-\epsilon$ approximation ratio in expectation
with $O(n \log \frac{1}{\epsilon})$ value oracle queries. We also generalize
our study by considering a partition matroid constraint, and develop a
linear-time algorithm for monotone and fully adaptive submodular functions.
</p>
<a href="http://arxiv.org/abs/2008.05004" target="_blank">arXiv:2008.05004</a> [<a href="http://arxiv.org/pdf/2008.05004" target="_blank">pdf</a>]

<h2>DNN2LR: Automatic Feature Crossing for Real-world Tabular Data. (arXiv:2008.09775v3 [cs.LG] UPDATED)</h2>
<h3>Zhaocheng Liu, Qiang Liu, Haoli Zhang, Yuntian Chen, Jun Zhu</h3>
<p>For sake of reliability, it is necessary for models in real-world
applications to be both powerful and globally interpretable. Simple
classifiers, e.g., Logistic Regression (LR), are globally interpretable, but
not powerful enough to model complex nonlinear interactions among features in
tabular data. Meanwhile, Deep Neural Networks (DNNs) have shown great
effectiveness for modeling tabular data, but is not globally interpretable. In
this work, we find local piece-wise interpretations in DNN of a specific
feature are usually inconsistent in different samples, which is caused by
feature interactions in the hidden layers. Accordingly, we can design an
automatic feature crossing method to find feature interactions in DNN, and use
them as cross features in LR. We give definition of the interpretation
inconsistency in DNN, based on which a novel feature crossing method called
DNN2LR is proposed. Extensive experiments have been conducted on four public
datasets and two real-world datasets. The final model, i.e., a LR model
empowered with cross features, generated by DNN2LR can outperform the complex
DNN model, as well as several state-of-the-art feature crossing methods. The
experimental results strongly verify the effectiveness and efficiency of
DNN2LR, especially on real-world datasets with large numbers of feature fields.
</p>
<a href="http://arxiv.org/abs/2008.09775" target="_blank">arXiv:2008.09775</a> [<a href="http://arxiv.org/pdf/2008.09775" target="_blank">pdf</a>]

<h2>Neural Bridge Sampling for Evaluating Safety-Critical Autonomous Systems. (arXiv:2008.10581v2 [cs.LG] UPDATED)</h2>
<h3>Aman Sinha, Matthew O&#x27;Kelly, Russ Tedrake, John Duchi</h3>
<p>Learning-based methodologies increasingly find applications in
safety-critical domains like autonomous driving and medical robotics. Due to
the rare nature of dangerous events, real-world testing is prohibitively
expensive and unscalable. In this work, we employ a probabilistic approach to
safety evaluation in simulation, where we are concerned with computing the
probability of dangerous events. We develop a novel rare-event simulation
method that combines exploration, exploitation, and optimization techniques to
find failure modes and estimate their rate of occurrence. We provide rigorous
guarantees for the performance of our method in terms of both statistical and
computational efficiency. Finally, we demonstrate the efficacy of our approach
on a variety of scenarios, illustrating its usefulness as a tool for rapid
sensitivity analysis and model comparison that are essential to developing and
testing safety-critical autonomous systems.
</p>
<a href="http://arxiv.org/abs/2008.10581" target="_blank">arXiv:2008.10581</a> [<a href="http://arxiv.org/pdf/2008.10581" target="_blank">pdf</a>]

<h2>Multi-Loss Weighting with Coefficient of Variations. (arXiv:2009.01717v2 [cs.CV] UPDATED)</h2>
<h3>Rick Groenendijk, Sezer Karaoglu, Theo Gevers, Thomas Mensink</h3>
<p>Many interesting tasks in machine learning and computer vision are learned by
optimising an objective function defined as a weighted linear combination of
multiple losses. The final performance is sensitive to choosing the correct
(relative) weights for these losses. Finding a good set of weights is often
done by adopting them into the set of hyper-parameters, which are set using an
extensive grid search. This is computationally expensive. In this paper, we
propose a weighting scheme based on the coefficient of variations and set the
weights based on properties observed while training the model. The proposed
method incorporates a measure of uncertainty to balance the losses, and as a
result the loss weights evolve during training without requiring another
(learning based) optimisation. In contrast to many loss weighting methods in
literature, we focus on single-task multi-loss problems, such as monocular
depth estimation and semantic segmentation, and show that multi-task approaches
for loss weighting do not work on those single-tasks. The validity of the
approach is shown empirically for depth estimation and semantic segmentation on
multiple datasets.
</p>
<a href="http://arxiv.org/abs/2009.01717" target="_blank">arXiv:2009.01717</a> [<a href="http://arxiv.org/pdf/2009.01717" target="_blank">pdf</a>]

<h2>Adjusting Bias in Long Range Stereo Matching: A semantics guided approach. (arXiv:2009.04629v2 [cs.CV] UPDATED)</h2>
<h3>WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, Alireza Bab-Hadiashar, David Suter</h3>
<p>Stereo vision generally involves the computation of pixel correspondences and
estimation of disparities between rectified image pairs. In many applications,
including simultaneous localization and mapping (SLAM) and 3D object detection,
the disparities are primarily needed to calculate depth values and the accuracy
of depth estimation is often more compelling than disparity estimation. The
accuracy of disparity estimation, however, does not directly translate to the
accuracy of depth estimation, especially for faraway objects. In the context of
learning-based stereo systems, this is largely due to biases imposed by the
choices of the disparity-based loss function and the training data.
Consequently, the learning algorithms often produce unreliable depth estimates
of foreground objects, particularly at large distances~($&gt;50$m). To resolve
this issue, we first analyze the effect of those biases and then propose a pair
of novel depth-based loss functions for foreground and background, separately.
These loss functions are tunable and can balance the inherent bias of the
stereo learning algorithms. The efficacy of our solution is demonstrated by an
extensive set of experiments, which are benchmarked against state of the art.
We show on KITTI~2015 benchmark that our proposed solution yields substantial
improvements in disparity and depth estimation, particularly for objects
located at distances beyond 50 meters, outperforming the previous state of the
art by $10\%$.
</p>
<a href="http://arxiv.org/abs/2009.04629" target="_blank">arXiv:2009.04629</a> [<a href="http://arxiv.org/pdf/2009.04629" target="_blank">pdf</a>]

<h2>Understanding Self-supervised Learning with Dual Deep Networks. (arXiv:2010.00578v4 [cs.LG] UPDATED)</h2>
<h3>Yuandong Tian, Lantao Yu, Xinlei Chen, Surya Ganguli</h3>
<p>We propose a novel theoretical framework to understand self-supervised
learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR,
BYOL). First, we prove that in each SGD update of SimCLR, the weights at each
layer are updated by a \emph{covariance operator} that specifically amplifies
initial random selectivities that vary across data samples but survive averages
over data augmentations, which we show leads to the emergence of hierarchical
features, if the input data are generated from a hierarchical latent tree
model. With the same framework, we also show analytically that in BYOL, the
usage of BatchNorm and a predictor creates an implicit contrastive term, acting
as an approximate covariance operator. The term is formed by the inter-play
between the zero-mean operation of BatchNorm and the extra predictor in the
online network. Extensive ablation studies justify our theoretical findings.
</p>
<a href="http://arxiv.org/abs/2010.00578" target="_blank">arXiv:2010.00578</a> [<a href="http://arxiv.org/pdf/2010.00578" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video with Applications for Virtual Reality. (arXiv:2010.07704v2 [cs.CV] UPDATED)</h2>
<h3>Alisha Sharma, Ryan Nett, Jonathan Ventura</h3>
<p>We introduce a convolutional neural network model for unsupervised learning
of depth and ego-motion from cylindrical panoramic video. Panoramic depth
estimation is an important technology for applications such as virtual reality,
3D modeling, and autonomous robotic navigation. In contrast to previous
approaches for applying convolutional neural networks to panoramic imagery, we
use the cylindrical panoramic projection which allows for the use of the
traditional CNN layers such as convolutional filters and max pooling without
modification. Our evaluation of synthetic and real data shows that unsupervised
learning of depth and ego-motion on cylindrical panoramic images can produce
high-quality depth maps and that an increased field-of-view improves ego-motion
estimation accuracy. We create two new datasets to evaluate our approach: a
synthetic dataset created using the CARLA simulator, and Headcam, a novel
dataset of panoramic video collected from a helmet-mounted camera while biking
in an urban setting. We also apply our network to the problem of converting
monocular panoramas to stereo panoramas.
</p>
<a href="http://arxiv.org/abs/2010.07704" target="_blank">arXiv:2010.07704</a> [<a href="http://arxiv.org/pdf/2010.07704" target="_blank">pdf</a>]

<h2>Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free. (arXiv:2010.11828v2 [cs.CV] UPDATED)</h2>
<h3>Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, Zhangyang Wang</h3>
<p>Adversarial training and its many variants substantially improve deep network
robustness, yet at the cost of compromising standard accuracy. Moreover, the
training process is heavy and hence it becomes impractical to thoroughly
explore the trade-off between accuracy and robustness. This paper asks this new
question: how to quickly calibrate a trained model in-situ, to examine the
achievable trade-offs between its standard and robust accuracies, without
(re-)training it many times? Our proposed framework, Once-for-all Adversarial
Training (OAT), is built on an innovative model-conditional training framework,
with a controlling hyper-parameter as the input. The trained model could be
adjusted among different standard and robust accuracies "for free" at testing
time. As an important knob, we exploit dual batch normalization to separate
standard and adversarial feature statistics, so that they can be learned in one
model without degrading performance. We further extend OAT to a Once-for-all
Adversarial Training and Slimming (OATS) framework, that allows for the joint
trade-off among accuracy, robustness and runtime efficiency. Experiments show
that, without any re-training nor ensembling, OAT/OATS achieve similar or even
superior performance compared to dedicatedly trained models at various
configurations. Our codes and pretrained models are available at:
https://github.com/VITA-Group/Once-for-All-Adversarial-Training.
</p>
<a href="http://arxiv.org/abs/2010.11828" target="_blank">arXiv:2010.11828</a> [<a href="http://arxiv.org/pdf/2010.11828" target="_blank">pdf</a>]

<h2>Graph and graphon neural network stability. (arXiv:2010.12529v2 [cs.LG] UPDATED)</h2>
<h3>Luana Ruiz, Zhiyang Wang, Alejandro Ribeiro</h3>
<p>Graph neural networks (GNNs) are learning architectures that rely on
knowledge of the graph structure to generate meaningful representations of
large-scale network data. GNN stability is thus important as in real-world
scenarios there are typically uncertainties associated with the graph. We
analyze GNN stability using kernel objects called graphons. Graphons are both
limits of convergent graph sequences and generating models for deterministic
and stochastic graphs. Building upon the theory of graphon signal processing,
we define graphon neural networks and analyze their stability to graphon
perturbations. We then extend this analysis by interpreting the graphon neural
network as a generating model for GNNs on deterministic and stochastic graphs
instantiated from the original and perturbed graphons. We observe that GNNs are
stable to graphon perturbations with a stability bound that decreases
asymptotically with the size of the graph. This asymptotic behavior is further
demonstrated in an experiment of movie recommendation.
</p>
<a href="http://arxiv.org/abs/2010.12529" target="_blank">arXiv:2010.12529</a> [<a href="http://arxiv.org/pdf/2010.12529" target="_blank">pdf</a>]

<h2>How to Make Deep RL Work in Practice. (arXiv:2010.13083v2 [cs.LG] UPDATED)</h2>
<h3>Nirnai Rao, Elie Aljalbout, Axel Sauer, Sami Haddadin</h3>
<p>In recent years, challenging control problems became solvable with deep
reinforcement learning (RL). To be able to use RL for large-scale real-world
applications, a certain degree of reliability in their performance is
necessary. Reported results of state-of-the-art algorithms are often difficult
to reproduce. One reason for this is that certain implementation details
influence the performance significantly. Commonly, these details are not
highlighted as important techniques to achieve state-of-the-art performance.
Additionally, techniques from supervised learning are often used by default but
influence the algorithms in a reinforcement learning setting in different and
not well-understood ways. In this paper, we investigate the influence of
certain initialization, input normalization, and adaptive learning techniques
on the performance of state-of-the-art RL algorithms. We make suggestions which
of those techniques to use by default and highlight areas that could benefit
from a solution specifically tailored to RL.
</p>
<a href="http://arxiv.org/abs/2010.13083" target="_blank">arXiv:2010.13083</a> [<a href="http://arxiv.org/pdf/2010.13083" target="_blank">pdf</a>]

<h2>Proceedings of the AI-HRI Symposium at AAAI-FSS 2020. (arXiv:2010.13830v2 [cs.RO] UPDATED)</h2>
<h3>Shelly Bagchi, Jason R. Wilson, Muneeb I. Ahmad, Christian Dondrup, Zhao Han, Justin W. Hart, Matteo Leonetti, Katrin Lohan, Ross Mead, Emmanuel Senft, Jivko Sinapov, Megan L. Zimmerman</h3>
<p>The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. In that
time, the related topic of trust in robotics has been rapidly growing, with
major research efforts at universities and laboratories across the world.
Indeed, many of the past participants in AI-HRI have been or are now involved
with research into trust in HRI. While trust has no consensus definition, it is
regularly associated with predictability, reliability, inciting confidence, and
meeting expectations. Furthermore, it is generally believed that trust is
crucial for adoption of both AI and robotics, particularly when transitioning
technologies from the lab to industrial, social, and consumer applications.
However, how does trust apply to the specific situations we encounter in the
AI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a
growing need for research that lives directly at the intersection of AI and HRI
that is serviced by this symposium. Over the course of the two-day meeting, we
propose to create a collaborative forum for discussion of current efforts in
trust for AI-HRI, with a sub-session focused on the related topic of
explainable AI (XAI) for HRI.
</p>
<a href="http://arxiv.org/abs/2010.13830" target="_blank">arXiv:2010.13830</a> [<a href="http://arxiv.org/pdf/2010.13830" target="_blank">pdf</a>]

<h2>Socially-Compatible Behavior Design of Autonomous Vehicles with Verification on Real Human Data. (arXiv:2010.14712v2 [cs.RO] UPDATED)</h2>
<h3>Letian Wang, Liting Sun, Masayoshi Tomizuka, Wei Zhan</h3>
<p>As more and more autonomous vehicles (AVs) are being deployed on public
roads, designing socially compatible behaviors for them is of critical
importance. Based on observations, AVs need to predict the future behaviors of
other traffic participants, and be aware of the uncertainties associated with
such prediction so that safe, efficient, and human-like motions can be
generated. In this paper, we propose an integrated prediction and planning
framework that allows the AVs to online infer the characteristics of other road
users and generate behaviors optimizing not only their own rewards, but also
their courtesy to others, as well as their confidence on the consequences in
the presence of uncertainties. Based on the definitions of courtesy and
confidence, we explore the influences of such factors on the behaviors of AVs
in interactive driving scenarios. Moreover, we evaluate the proposed algorithm
on naturalistic human driving data by comparing the generated behavior with the
ground truth. Results show that the online inference can significantly improve
the human-likeness of the generated behaviors. Furthermore, we find that human
drivers show great courtesy to others, even for those without right-of-way.
</p>
<a href="http://arxiv.org/abs/2010.14712" target="_blank">arXiv:2010.14712</a> [<a href="http://arxiv.org/pdf/2010.14712" target="_blank">pdf</a>]

<h2>Robust Quadrupedal Locomotion on Sloped Terrains: A Linear Policy Approach. (arXiv:2010.16342v2 [cs.RO] UPDATED)</h2>
<h3>Kartik Paigwar, Lokesh Krishna, Sashank Tirumala, Naman Khetan, Aditya Sagi, Ashish Joglekar, Shalabh Bhatnagar, Ashitava Ghosal, Bharadwaj Amrutur, Shishir Kolathaya</h3>
<p>In this paper, with a view toward fast deployment of locomotion gaits in
low-cost hardware, we use a linear policy for realizing end-foot trajectories
in the quadruped robot, Stoch $2$. In particular, the parameters of the
end-foot trajectories are shaped via a linear feedback policy that takes the
torso orientation and the terrain slope as inputs. The corresponding desired
joint angles are obtained via an inverse kinematics solver and tracked via a
PID control law. Augmented Random Search, a model-free and a gradient-free
learning algorithm is used to train this linear policy. Simulation results show
that the resulting walking is robust to terrain slope variations and external
pushes. This methodology is not only computationally light-weight but also uses
minimal sensing and actuation capabilities in the robot, thereby justifying the
approach.
</p>
<a href="http://arxiv.org/abs/2010.16342" target="_blank">arXiv:2010.16342</a> [<a href="http://arxiv.org/pdf/2010.16342" target="_blank">pdf</a>]

<h2>Data-Driven Adaptive Task Allocation for Heterogeneous Multi-Robot Teams Using Robust Control Barrier Functions. (arXiv:2011.01164v2 [cs.RO] UPDATED)</h2>
<h3>Yousef Emam, Gennaro Notomista, Paul Glotfelter, Magnus Egerstedt</h3>
<p>Multi-robot task allocation is a ubiquitous problem in robotics due to its
applicability in a variety of scenarios. Adaptive task-allocation algorithms
account for unknown disturbances and unpredicted phenomena in the environment
where robots are deployed to execute tasks. However, this adaptivity typically
comes at the cost of requiring precise knowledge of robot models in order to
evaluate the allocation effectiveness and to adjust the task assignment online.
As such, environmental disturbances can significantly degrade the accuracy of
the models which in turn negatively affects the quality of the task allocation.
In this paper, we leverage Gaussian processes, differential inclusions, and
robust control barrier functions to learn environmental disturbances in order
to guarantee robust task execution. We show the implementation and the
effectiveness of the proposed framework on a real multi-robot system.
</p>
<a href="http://arxiv.org/abs/2011.01164" target="_blank">arXiv:2011.01164</a> [<a href="http://arxiv.org/pdf/2011.01164" target="_blank">pdf</a>]

<h2>Learning to Rank with Missing Data via Generative Adversarial Networks. (arXiv:2011.02089v2 [stat.ML] UPDATED)</h2>
<h3>Grace Deng, Cuize Han, David S. Matteson</h3>
<p>We explore the role of Conditional Generative Adversarial Networks (GAN) in
imputing missing data and apply GAN imputation on a novel use case in
e-commerce: a learning-to-rank problem with incomplete training data.
Conventional imputation methods often make assumptions regarding the underlying
distribution of the missing data, while GANs offer an alternative framework to
sidestep approximating intractable distributions. First, we prove that GAN
imputation offers theoretical guarantees beyond the naive Missing Completely At
Random (MCAR) scenario. Next, we show that empirically, the Conditional GAN
structure is well suited for data with heterogeneous distributions and across
unbalanced classes, improving performance metrics such as RMSE. Using an Amazon
Search ranking dataset, we produce standard ranking models trained on
GAN-imputed data that are comparable to training on ground-truth data based on
standard ranking quality metrics NDCG and MRR. We also highlight how different
neural net features such as convolution and dropout layers can improve
performance given different missing value settings.
</p>
<a href="http://arxiv.org/abs/2011.02089" target="_blank">arXiv:2011.02089</a> [<a href="http://arxiv.org/pdf/2011.02089" target="_blank">pdf</a>]

<h2>Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v3 [cs.CV] UPDATED)</h2>
<h3>Mike Roberts, Nathan Paczan</h3>
<p>For many fundamental scene understanding tasks, it is difficult or impossible
to obtain per-pixel ground truth labels from real images. We address this
challenge by introducing Hypersim, a photorealistic synthetic dataset for
holistic indoor scene understanding. To create our dataset, we leverage a large
repository of synthetic scenes created by professional artists, and we generate
77,400 images of 461 indoor scenes with detailed per-pixel labels and
corresponding ground truth geometry. Our dataset: (1) relies exclusively on
publicly available 3D assets; (2) includes complete scene geometry, material
information, and lighting information for every scene; (3) includes dense
per-pixel semantic instance segmentations for every image; and (4) factors
every image into diffuse reflectance, diffuse illumination, and a non-diffuse
residual term that captures view-dependent lighting effects. Together, these
features make our dataset well-suited for geometric learning problems that
require direct 3D supervision, multi-task learning problems that require
reasoning jointly over multiple input and output modalities, and inverse
rendering problems. We analyze our dataset at the level of scenes, objects, and
pixels, and we analyze costs in terms of money, annotation effort, and
computation time. Remarkably, we find that it is possible to generate our
entire dataset from scratch, for roughly half the cost of training a
state-of-the-art natural language processing model. All the code we used to
generate our dataset will be made available online.
</p>
<a href="http://arxiv.org/abs/2011.02523" target="_blank">arXiv:2011.02523</a> [<a href="http://arxiv.org/pdf/2011.02523" target="_blank">pdf</a>]

<h2>Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery. (arXiv:2011.03247v2 [cs.CV] UPDATED)</h2>
<h3>Shiqi Tian, Ailong Ma, Zhuo Zheng, Yanfei Zhong</h3>
<p>With the acceleration of the urban expansion, urban change detection (UCD),
as a significant and effective approach, can provide the change information
with respect to geospatial objects for dynamical urban analysis. However,
existing datasets suffer from three bottlenecks: (1) lack of high spatial
resolution images; (2) lack of semantic annotation; (3) lack of long-range
multi-temporal images. In this paper, we propose a large scale benchmark
dataset, termed Hi-UCD. This dataset uses aerial images with a spatial
resolution of 0.1 m provided by the Estonia Land Board, including three-time
phases, and semantically annotated with nine classes of land cover to obtain
the direction of ground objects change. It can be used for detecting and
analyzing refined urban changes. We benchmark our dataset using some classic
methods in binary and multi-class change detection. Experimental results show
that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong
benchmark accelerating future research.
</p>
<a href="http://arxiv.org/abs/2011.03247" target="_blank">arXiv:2011.03247</a> [<a href="http://arxiv.org/pdf/2011.03247" target="_blank">pdf</a>]

<h2>Kernel Dependence Network. (arXiv:2011.03320v2 [cs.LG] UPDATED)</h2>
<h3>Chieh Wu, Aria Masoomi, Arthur Gretton, Jennifer Dy</h3>
<p>We propose a greedy strategy to spectrally train a deep network for
multi-class classification. Each layer is defined as a composition of linear
weights with the feature map of a Gaussian kernel acting as the activation
function. At each layer, the linear weights are learned by maximizing the
dependence between the layer output and the labels using the Hilbert Schmidt
Independence Criterion (HSIC). By constraining the solution space on the
Stiefel Manifold, we demonstrate how our network construct (Kernel Dependence
Network or KNet) can be solved spectrally while leveraging the eigenvalues to
automatically find the width and the depth of the network. We theoretically
guarantee the existence of a solution for the global optimum while providing
insight into our network's ability to generalize.
</p>
<a href="http://arxiv.org/abs/2011.03320" target="_blank">arXiv:2011.03320</a> [<a href="http://arxiv.org/pdf/2011.03320" target="_blank">pdf</a>]

<h2>A Multi-stream Convolutional Neural Network for Micro-expression Recognition Using Optical Flow and EVM. (arXiv:2011.03756v2 [cs.CV] UPDATED)</h2>
<h3>Jinming Liu, Ke Li, Baolin Song, Li Zhao</h3>
<p>Micro-expression (ME) recognition plays a crucial role in a wide range of
applications, particularly in public security and psychotherapy. Recently,
traditional methods rely excessively on machine learning design and the
recognition rate is not high enough for its practical application because of
its short duration and low intensity. On the other hand, some methods based on
deep learning also cannot get high accuracy due to problems such as the
imbalance of databases. To address these problems, we design a multi-stream
convolutional neural network (MSCNN) for ME recognition in this paper.
Specifically, we employ EVM and optical flow to magnify and visualize subtle
movement changes in MEs and extract the masks from the optical flow images. And
then, we add the masks, optical flow images, and grayscale images into the
MSCNN. After that, in order to overcome the imbalance of databases, we added a
random over-sampler after the Dense Layer of the neural network. Finally,
extensive experiments are conducted on two public ME databases: CASME II and
SAMM. Compared with many recent state-of-the-art approaches, our method
achieves more promising recognition results.
</p>
<a href="http://arxiv.org/abs/2011.03756" target="_blank">arXiv:2011.03756</a> [<a href="http://arxiv.org/pdf/2011.03756" target="_blank">pdf</a>]

<h2>Deep traffic light detection by overlaying synthetic context on arbitrary natural images. (arXiv:2011.03841v2 [cs.CV] UPDATED)</h2>
<h3>Jean Pablo Vieira de Mello, Lucas Tabelini, Rodrigo F. Berriel, Thiago M. Paix&#xe3;o, Alberto F. de Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos</h3>
<p>Deep neural networks come as an effective solution to many problems
associated with autonomous driving. By providing real image samples with
traffic context to the network, the model learns to detect and classify
elements of interest, such as pedestrians, traffic signs, and traffic lights.
However, acquiring and annotating real data can be extremely costly in terms of
time and effort. In this context, we propose a method to generate artificial
traffic-related training data for deep traffic light detectors. This data is
generated using basic non-realistic computer graphics to blend fake traffic
scenes on top of arbitrary image backgrounds that are not related to the
traffic domain. Thus, a large amount of training data can be generated without
annotation efforts. Furthermore, it also tackles the intrinsic data imbalance
problem in traffic light datasets, caused mainly by the low amount of samples
of the yellow state. Experiments show that it is possible to achieve results
comparable to those obtained with real training data from the problem domain,
yielding an average mAP and an average F1-score which are each nearly 4 p.p.
higher than the respective metrics obtained with a real-world reference model.
</p>
<a href="http://arxiv.org/abs/2011.03841" target="_blank">arXiv:2011.03841</a> [<a href="http://arxiv.org/pdf/2011.03841" target="_blank">pdf</a>]

<h2>Graph Kernels: State-of-the-Art and Future Challenges. (arXiv:2011.03854v2 [cs.LG] UPDATED)</h2>
<h3>Karsten Borgwardt, Elisabetta Ghisu, Felipe Llinares-L&#xf3;pez, Leslie O&#x27;Bray, Bastian Rieck</h3>
<p>Graph-structured data are an integral part of many application domains,
including chemoinformatics, computational biology, neuroimaging, and social
network analysis. Over the last two decades, numerous graph kernels, i.e.
kernel functions between graphs, have been proposed to solve the problem of
assessing the similarity between graphs, thereby making it possible to perform
predictions in both classification and regression settings. This manuscript
provides a review of existing graph kernels, their applications, software plus
data resources, and an empirical comparison of state-of-the-art graph kernels.
</p>
<a href="http://arxiv.org/abs/2011.03854" target="_blank">arXiv:2011.03854</a> [<a href="http://arxiv.org/pdf/2011.03854" target="_blank">pdf</a>]

<h2>Adversarial Black-Box Attacks On Text Classifiers Using Multi-Objective Genetic Optimization Guided By Deep Networks. (arXiv:2011.03901v2 [cs.AI] UPDATED)</h2>
<h3>Alex Mathai, Shreya Khare, Srikanth Tamilselvam, Senthil Mani</h3>
<p>We propose a novel genetic-algorithm technique that generates black-box
adversarial examples which successfully fool neural network based text
classifiers. We perform a genetic search with multi-objective optimization
guided by deep learning based inferences and Seq2Seq mutation to generate
semantically similar but imperceptible adversaries. We compare our approach
with DeepWordBug (DWB) on SST and IMDB sentiment datasets by attacking three
trained models viz. char-LSTM, word-LSTM and elmo-LSTM. On an average, we
achieve an attack success rate of 65.67% for SST and 36.45% for IMDB across the
three models showing an improvement of 49.48% and 101% respectively.
Furthermore, our qualitative study indicates that 94% of the time, the users
were not able to distinguish between an original and adversarial sample.
</p>
<a href="http://arxiv.org/abs/2011.03901" target="_blank">arXiv:2011.03901</a> [<a href="http://arxiv.org/pdf/2011.03901" target="_blank">pdf</a>]

<h2>Distance-Based Anomaly Detection for Industrial Surfaces Using Triplet Networks. (arXiv:2011.04121v2 [cs.CV] UPDATED)</h2>
<h3>Tareq Tayeh, Sulaiman Aburakhia, Ryan Myers, Abdallah Shami</h3>
<p>Surface anomaly detection plays an important quality control role in many
manufacturing industries to reduce scrap production. Machine-based visual
inspections have been utilized in recent years to conduct this task instead of
human experts. In particular, deep learning Convolutional Neural Networks
(CNNs) have been at the forefront of these image processing-based solutions due
to their predictive accuracy and efficiency. Training a CNN on a classification
objective requires a sufficiently large amount of defective data, which is
often not available. In this paper, we address that challenge by training the
CNN on surface texture patches with a distance-based anomaly detection
objective instead. A deep residual-based triplet network model is utilized, and
defective training samples are synthesized exclusively from non-defective
samples via random erasing techniques to directly learn a similarity metric
between the same-class samples and out-of-class samples. Evaluation results
demonstrate the approach's strength in detecting different types of anomalies,
such as bent, broken, or cracked surfaces, for known surfaces that are part of
the training data and unseen novel surfaces.
</p>
<a href="http://arxiv.org/abs/2011.04121" target="_blank">arXiv:2011.04121</a> [<a href="http://arxiv.org/pdf/2011.04121" target="_blank">pdf</a>]

<h2>Community Detection by Principal Components Clustering Methods. (arXiv:2011.04377v2 [stat.ML] UPDATED)</h2>
<h3>Huan Qing, Jingli Wang</h3>
<p>Based on the classical Degree Corrected Stochastic Blockmodel (DCSBM) model
for network community detection problem, we propose two novel approaches:
principal component clustering (PCC) and normalized principal component
clustering (NPCC). Without any parameters to be estimated, the PCC method is
simple to be implemented. Under mild conditions, we show that PCC yields
consistent community detection. NPCC is designed based on the combination of
the PCC and the RSC method (Qin &amp; Rohe 2013). Population analysis for NPCC
shows that NPCC returns perfect clustering for the ideal case under DCSBM. PCC
and NPCC is illustrated through synthetic and real-world datasets. Numerical
results show that NPCC provides a significant improvement compare with PCC and
RSC. Moreover, NPCC inherits nice properties of PCC and RSC such that NPCC is
insensitive to the number of eigenvectors to be clustered and the choosing of
the tuning parameter. When dealing with two weak signal networks Simmons and
Caltech, by considering one more eigenvectors for clustering, we provide two
refinements PCC+ and NPCC+ of PCC and NPCC, respectively. Both two refinements
algorithms provide improvement performances compared with their original
algorithms. Especially, NPCC+ provides satisfactory performances on Simmons and
Caltech, with error rates of 121/1137 and 96/590, respectively.
</p>
<a href="http://arxiv.org/abs/2011.04377" target="_blank">arXiv:2011.04377</a> [<a href="http://arxiv.org/pdf/2011.04377" target="_blank">pdf</a>]

<h2>Combining Propositional Logic Based Decision Diagrams with Decision Making in Urban Systems. (arXiv:2011.04405v2 [cs.AI] UPDATED)</h2>
<h3>Jiajing Ling, Kushagra Chandak, Akshat Kumar</h3>
<p>Solving multiagent problems can be an uphill task due to uncertainty in the
environment, partial observability, and scalability of the problem at hand.
Especially in an urban setting, there are more challenges since we also need to
maintain safety for all users while minimizing congestion of the agents as well
as their travel times. To this end, we tackle the problem of multiagent
pathfinding under uncertainty and partial observability where the agents are
tasked to move from their starting points to ending points while also
satisfying some constraints, e.g., low congestion, and model it as a multiagent
reinforcement learning problem. We compile the domain constraints using
propositional logic and integrate them with the RL algorithms to enable fast
simulation for RL.
</p>
<a href="http://arxiv.org/abs/2011.04405" target="_blank">arXiv:2011.04405</a> [<a href="http://arxiv.org/pdf/2011.04405" target="_blank">pdf</a>]

<h2>Playing optical tweezers with deep reinforcement learning: in virtual, physical and augmented environments. (arXiv:2011.04424v2 [cs.LG] UPDATED)</h2>
<h3>Matthew Praeger, Yunhui Xie, James A. Grant-Jacob, Robert W. Eason, Ben Mills</h3>
<p>Reinforcement learning was carried out in a simulated environment to learn
continuous velocity control over multiple motor axes. This was then applied to
a real-world optical tweezers experiment with the objective of moving a
laser-trapped microsphere to a target location whilst avoiding collisions with
other free-moving microspheres. The concept of training a neural network in a
virtual environment has significant potential in the application of machine
learning for experimental optimization and control, as the neural network can
discover optimal methods for problem solving without the risk of damage to
equipment, and at a speed not limited by movement in the physical environment.
As the neural network treats both virtual and physical environments
equivalently, we show that the network can also be applied to an augmented
environment, where a virtual environment is combined with the physical
environment. This technique may have the potential to unlock capabilities
associated with mixed and augmented reality, such as enforcing safety limits
for machine motion or as a method of inputting observations from additional
sensors.
</p>
<a href="http://arxiv.org/abs/2011.04424" target="_blank">arXiv:2011.04424</a> [<a href="http://arxiv.org/pdf/2011.04424" target="_blank">pdf</a>]

<h2>Deep Learning for Flight Demand and Delays Forecasting. (arXiv:2011.04476v2 [cs.LG] UPDATED)</h2>
<h3>Liya Wang, Amy Mykityshyn, Craig Johnson, Benjamin D. Marple</h3>
<p>The last few years have seen an increased interest in deep learning (DL) due
to its success in applications such as computer vision, natural language
processing (NLP), and self-driving cars. Inspired by this success, this paper
applied DL to predict flight demand and delays, which have been a concern for
airlines and the other stakeholders in the National Airspace System (NAS).
Demand and delay prediction can be formulated as a supervised learning problem,
where, given an understanding of past historical demand and delays, a deep
learning network can examine sequences of historic data to predict current and
future sequences. With that in mind, we applied a well-known DL method,
sequence to sequence (seq2seq), to solve the problem. Our results show that the
seq2seq method can reduce demand prediction mean squared error (MSE) by 50%,
compared to two classical baseline algorithms.
</p>
<a href="http://arxiv.org/abs/2011.04476" target="_blank">arXiv:2011.04476</a> [<a href="http://arxiv.org/pdf/2011.04476" target="_blank">pdf</a>]

<h2>Multi-Agent Decentralized Belief Propagation on Graphs. (arXiv:2011.04501v2 [cs.AI] UPDATED)</h2>
<h3>Yitao Chen, Deepanshu Vasal</h3>
<p>We consider the problem of interactive partially observable Markov decision
processes (I-POMDPs), where the agents are located at the nodes of a
communication network. Specifically, we assume a certain message type for all
messages. Moreover, each agent makes individual decisions based on the
interactive belief states, the information observed locally and the messages
received from its neighbors over the network. Within this setting, the
collective goal of the agents is to maximize the globally averaged return over
the network through exchanging information with their neighbors. We propose a
decentralized belief propagation algorithm for the problem, and prove the
convergence of our algorithm. Finally we show multiple applications of our
framework. Our work appears to be the first study of decentralized belief
propagation algorithm for networked multi-agent I-POMDPs.
</p>
<a href="http://arxiv.org/abs/2011.04501" target="_blank">arXiv:2011.04501</a> [<a href="http://arxiv.org/pdf/2011.04501" target="_blank">pdf</a>]

<h2>Covariance-free Partial Least Squares: An Incremental Dimensionality Reduction Method. (arXiv:1910.02319v2 [cs.CV] CROSS LISTED)</h2>
<h3>Artur Jordao, Maiko Lie, Victor Hugo Cunha de Melo, William Robson Schwartz</h3>
<p>Dimensionality reduction plays an important role in computer vision problems
since it reduces computational cost and is often capable of yielding more
discriminative data representation. In this context, Partial Least Squares
(PLS) has presented notable results in tasks such as image classification and
neural network optimization. However, PLS is infeasible on large datasets, such
as ImageNet, because it requires all the data to be in memory in advance, which
is often impractical due to hardware limitations. Additionally, this
requirement prevents us from employing PLS on streaming applications where the
data are being continuously generated. Motivated by this, we propose a novel
incremental PLS, named Covariance-free Incremental Partial Least Squares
(CIPLS), which learns a low-dimensional representation of the data using a
single sample at a time. In contrast to other state-of-the-art approaches,
instead of adopting a partially-discriminative or SGD-based model, we extend
Nonlinear Iterative Partial Least Squares (NIPALS) -- the standard algorithm
used to compute PLS -- for incremental processing. Among the advantages of this
approach are the preservation of discriminative information across all
components, the possibility of employing its score matrices for feature
selection, and its computational efficiency. We validate CIPLS on face
verification and image classification tasks, where it outperforms several other
incremental dimensionality reduction techniques. In the context of feature
selection, CIPLS achieves comparable results when compared to state-of-the-art
techniques.
</p>
<a href="http://arxiv.org/abs/1910.02319" target="_blank">arXiv:1910.02319</a> [<a href="http://arxiv.org/pdf/1910.02319" target="_blank">pdf</a>]

