---
title: Latest Deep Learning Papers
date: 2020-12-23 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (149 Articles)</h1>
<h2>Generative Interventions for Causal Learning. (arXiv:2012.12265v1 [cs.CV])</h2>
<h3>Chengzhi Mao, Amogh Gupta, Augustine Cha, Hao Wang, Junfeng Yang, Carl Vondrick</h3>
<p>We introduce a framework for learning robust visual representations that
generalize to new viewpoints, backgrounds, and scene contexts. Discriminative
models often learn naturally occurring spurious correlations, which cause them
to fail on images outside of the training distribution. In this paper, we show
that we can steer generative models to manufacture interventions on features
caused by confounding factors. Experiments, visualizations, and theoretical
results show this method learns robust representations more consistent with the
underlying causal relationships. Our approach improves performance on multiple
datasets demanding out-of-distribution generalization, and we demonstrate
state-of-the-art performance generalizing from ImageNet to ObjectNet dataset.
</p>
<a href="http://arxiv.org/abs/2012.12265" target="_blank">arXiv:2012.12265</a> [<a href="http://arxiv.org/pdf/2012.12265" target="_blank">pdf</a>]

<h2>Group-Aware Robot Navigation in Crowded Environments. (arXiv:2012.12291v1 [cs.RO])</h2>
<h3>Kapil Katyal, Yuxiang Gao, Jared Markowitz, I-Jeng Wang, Chien-Ming Huang</h3>
<p>Human-aware robot navigation promises a range of applications in which mobile
robots bring versatile assistance to people in common human environments. While
prior research has mostly focused on modeling pedestrians as independent,
intentional individuals, people move in groups; consequently, it is imperative
for mobile robots to respect human groups when navigating around people. This
paper explores learning group-aware navigation policies based on dynamic group
formation using deep reinforcement learning. Through simulation experiments, we
show that group-aware policies, compared to baseline policies that neglect
human groups, achieve greater robot navigation performance (e.g., fewer
collisions), minimize violation of social norms and discomfort, and reduce the
robot's movement impact on pedestrians. Our results contribute to the
development of social navigation and the integration of mobile robots into
human environments.
</p>
<a href="http://arxiv.org/abs/2012.12291" target="_blank">arXiv:2012.12291</a> [<a href="http://arxiv.org/pdf/2012.12291" target="_blank">pdf</a>]

<h2>Evolutionary Variational Optimization of Generative Models. (arXiv:2012.12294v1 [stat.ML])</h2>
<h3>Jakob Drefs, Enrico Guiraud, J&#xf6;rg L&#xfc;cke</h3>
<p>We combine two popular optimization approaches to derive learning algorithms
for generative models: variational optimization and evolutionary algorithms.
The combination is realized for generative models with discrete latents by
using truncated posteriors as the family of variational distributions. The
variational parameters of truncated posteriors are sets of latent states. By
interpreting these states as genomes of individuals and by using the
variational lower bound to define a fitness, we can apply evolutionary
algorithms to realize the variational loop. The used variational distributions
are very flexible and we show that evolutionary algorithms can effectively and
efficiently optimize the variational bound. Furthermore, the variational loop
is generally applicable ("black box") with no analytical derivations required.
To show general applicability, we apply the approach to three generative models
(we use noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse
Coding). To demonstrate effectiveness and efficiency of the novel variational
approach, we use the standard competitive benchmarks of image denoising and
inpainting. The benchmarks allow quantitative comparisons to a wide range of
methods including probabilistic approaches, deep deterministic and generative
networks, and non-local image processing methods. In the category of
"zero-shot" learning (when only the corrupted image is used for training), we
observed the evolutionary variational algorithm to significantly improve the
state-of-the-art in many benchmark settings. For one well-known inpainting
benchmark, we also observed state-of-the-art performance across all categories
of algorithms although we only train on the corrupted image. In general, our
investigations highlight the importance of research on optimization methods for
generative models to achieve performance improvements.
</p>
<a href="http://arxiv.org/abs/2012.12294" target="_blank">arXiv:2012.12294</a> [<a href="http://arxiv.org/pdf/2012.12294" target="_blank">pdf</a>]

<h2>Flexible deep transfer learning by separate feature embeddings and manifold alignment. (arXiv:2012.12302v1 [cs.CV])</h2>
<h3>Samuel Rivera, Joel Klipfel, Deborah Weeks</h3>
<p>Object recognition is a key enabler across industry and defense. As
technology changes, algorithms must keep pace with new requirements and data.
New modalities and higher resolution sensors should allow for increased
algorithm robustness. Unfortunately, algorithms trained on existing labeled
datasets do not directly generalize to new data because the data distributions
do not match. Transfer learning (TL) or domain adaptation (DA) methods have
established the groundwork for transferring knowledge from existing labeled
source data to new unlabeled target datasets. However, current DA approaches
assume similar source and target feature spaces and suffer in the case of
massive domain shifts or changes in the feature space. Existing methods assume
the data are either the same modality, or can be aligned to a common feature
space. Therefore, most methods are not designed to support a fundamental domain
change such as visual to auditory data. We propose a novel deep learning
framework that overcomes this limitation by learning separate feature
extractions for each domain while minimizing the distance between the domains
in a latent lower-dimensional space. The alignment is achieved by considering
the data manifold along with an adversarial training procedure. We demonstrate
the effectiveness of the approach versus traditional methods with several
ablation experiments on synthetic, measured, and satellite image datasets. We
also provide practical guidelines for training the network while overcoming
vanishing gradients which inhibit learning in some adversarial training
settings.
</p>
<a href="http://arxiv.org/abs/2012.12302" target="_blank">arXiv:2012.12302</a> [<a href="http://arxiv.org/pdf/2012.12302" target="_blank">pdf</a>]

<h2>Pattern Recognition Scheme for Large-Scale Cloud Detection over Landmarks. (arXiv:2012.12306v1 [cs.CV])</h2>
<h3>Adri&#xe1;n P&#xe9;rez-Suay, Julia Amor&#xf3;s-L&#xf3;pez, Luis G&#xf3;mez-Chova, Jordi Mu&#xf1;oz-Mar&#xed;, Dieter Just, Gustau Camps-Valls</h3>
<p>Landmark recognition and matching is a critical step in many Image Navigation
and Registration (INR) models for geostationary satellite services, as well as
to maintain the geometric quality assessment (GQA) in the instrument data
processing chain of Earth observation satellites. Matching the landmark
accurately is of paramount relevance, and the process can be strongly impacted
by the cloud contamination of a given landmark. This paper introduces a
complete pattern recognition methodology able to detect the presence of clouds
over landmarks using Meteosat Second Generation (MSG) data. The methodology is
based on the ensemble combination of dedicated support vector machines (SVMs)
dependent on the particular landmark and illumination conditions. This
divide-and-conquer strategy is motivated by the data complexity and follows a
physically-based strategy that considers variability both in seasonality and
illumination conditions along the day to split observations. In addition, it
allows training the classification scheme with millions of samples at an
affordable computational costs. The image archive was composed of 200 landmark
test sites with near 7 million multispectral images that correspond to MSG
acquisitions during 2010. Results are analyzed in terms of cloud detection
accuracy and computational cost. We provide illustrative source code and a
portion of the huge training data to the community.
</p>
<a href="http://arxiv.org/abs/2012.12306" target="_blank">arXiv:2012.12306</a> [<a href="http://arxiv.org/pdf/2012.12306" target="_blank">pdf</a>]

<h2>Nonlinear Cook distance for Anomalous Change Detection. (arXiv:2012.12307v1 [cs.CV])</h2>
<h3>Jos&#xe9; A. Padr&#xf3;n Hidalgo, Adri&#xe1;n P&#xe9;rez-Suay, Fatih Nar, Gustau Camps-Valls</h3>
<p>In this work we propose a method to find anomalous changes in remote sensing
images based on the chronochrome approach. A regressor between images is used
to discover the most {\em influential points} in the observed data. Typically,
the pixels with largest residuals are decided to be anomalous changes. In order
to find the anomalous pixels we consider the Cook distance and propose its
nonlinear extension using random Fourier features as an efficient nonlinear
measure of impact. Good empirical performance is shown over different
multispectral images both visually and quantitatively evaluated with ROC
curves.
</p>
<a href="http://arxiv.org/abs/2012.12307" target="_blank">arXiv:2012.12307</a> [<a href="http://arxiv.org/pdf/2012.12307" target="_blank">pdf</a>]

<h2>Randomized RX for target detection. (arXiv:2012.12308v1 [cs.CV])</h2>
<h3>Fatih Nar, Adri&#xe1;n P&#xe9;rez-Suay, Jos&#xe9; Antonio Padr&#xf3;n, Gustau Camps-Valls</h3>
<p>This work tackles the target detection problem through the well-known global
RX method. The RX method models the clutter as a multivariate Gaussian
distribution, and has been extended to nonlinear distributions using kernel
methods. While the kernel RX can cope with complex clutters, it requires a
considerable amount of computational resources as the number of clutter pixels
gets larger. Here we propose random Fourier features to approximate the
Gaussian kernel in kernel RX and consequently our development keep the accuracy
of the nonlinearity while reducing the computational cost which is now
controlled by an hyperparameter. Results over both synthetic and real-world
image target detection problems show space and time efficiency of the proposed
method while providing high detection performance.
</p>
<a href="http://arxiv.org/abs/2012.12308" target="_blank">arXiv:2012.12308</a> [<a href="http://arxiv.org/pdf/2012.12308" target="_blank">pdf</a>]

<h2>Mixture Model Framework for Traumatic Brain Injury Prognosis Using Heterogeneous Clinical and Outcome Data. (arXiv:2012.12310v1 [cs.LG])</h2>
<h3>Alan D. Kaplan, Qi Cheng, K. Aditya Mohan, Lindsay D. Nelson, Sonia Jain, Harvey Levin, Abel Torres-Espin, Austin Chou, J. Russell Huie, Adam R. Ferguson, Michael McCrea, Joseph Giacino, Shivshankar Sundaram, Amy J. Markowitz, Geoffrey T. Manley</h3>
<p>Prognoses of Traumatic Brain Injury (TBI) outcomes are neither easily nor
accurately determined from clinical indicators. This is due in part to the
heterogeneity of damage inflicted to the brain, ultimately resulting in diverse
and complex outcomes. Using a data-driven approach on many distinct data
elements may be necessary to describe this large set of outcomes and thereby
robustly depict the nuanced differences among TBI patients' recovery. In this
work, we develop a method for modeling large heterogeneous data types relevant
to TBI. Our approach is geared toward the probabilistic representation of mixed
continuous and discrete variables with missing values. The model is trained on
a dataset encompassing a variety of data types, including demographics,
blood-based biomarkers, and imaging findings. In addition, it includes a set of
clinical outcome assessments at 3, 6, and 12 months post-injury. The model is
used to stratify patients into distinct groups in an unsupervised learning
setting. We use the model to infer outcomes using input data, and show that the
collection of input data reduces uncertainty of outcomes over a baseline
approach. In addition, we quantify the performance of a likelihood scoring
technique that can be used to self-evaluate confidence in model fit and
prediction.
</p>
<a href="http://arxiv.org/abs/2012.12310" target="_blank">arXiv:2012.12310</a> [<a href="http://arxiv.org/pdf/2012.12310" target="_blank">pdf</a>]

<h2>Video Influencers: Unboxing the Mystique. (arXiv:2012.12311v1 [cs.LG])</h2>
<h3>Prashant Rajaram, Puneet Manchanda</h3>
<p>Influencer marketing is being used increasingly as a tool to reach customers
because of the growing popularity of social media stars who primarily reach
their audience(s) via custom videos. Despite the rapid growth in influencer
marketing, there has been little research on the design and effectiveness of
influencer videos. Using publicly available data on YouTube influencer videos,
we implement novel interpretable deep learning architectures, supported by
transfer learning, to identify significant relationships between advertising
content in videos (across text, audio, and images) and video views, interaction
rates and sentiment. By avoiding ex-ante feature engineering and instead using
ex-post interpretation, our approach avoids making a trade-off between
interpretability and predictive ability. We filter out relationships that are
affected by confounding factors unassociated with an increase in attention to
video elements, thus facilitating the generation of plausible causal
relationships between video elements and marketing outcomes which can be tested
in the field. A key finding is that brand mentions in the first 30 seconds of a
video are on average associated with a significant increase in attention to the
brand but a significant decrease in sentiment expressed towards the video. We
illustrate the learnings from our approach for both influencers and brands.
</p>
<a href="http://arxiv.org/abs/2012.12311" target="_blank">arXiv:2012.12311</a> [<a href="http://arxiv.org/pdf/2012.12311" target="_blank">pdf</a>]

<h2>Hierarchical Recurrent Attention Networks for Structured Online Maps. (arXiv:2012.12314v1 [cs.CV])</h2>
<h3>Namdar Homayounfar, Wei-Chiu Ma, Shrinidhi Kowshika Lakshmikanth, Raquel Urtasun</h3>
<p>In this paper, we tackle the problem of online road network extraction from
sparse 3D point clouds. Our method is inspired by how an annotator builds a
lane graph, by first identifying how many lanes there are and then drawing each
one in turn. We develop a hierarchical recurrent network that attends to
initial regions of a lane boundary and traces them out completely by outputting
a structured polyline. We also propose a novel differentiable loss function
that measures the deviation of the edges of the ground truth polylines and
their predictions. This is more suitable than distances on vertices, as there
exists many ways to draw equivalent polylines. We demonstrate the effectiveness
of our method on a 90 km stretch of highway, and show that we can recover the
right topology 92\% of the time.
</p>
<a href="http://arxiv.org/abs/2012.12314" target="_blank">arXiv:2012.12314</a> [<a href="http://arxiv.org/pdf/2012.12314" target="_blank">pdf</a>]

<h2>Drug-Target Interaction Prediction via an Ensemble of Weighted Nearest Neighbors with Interaction Recovery. (arXiv:2012.12325v1 [cs.LG])</h2>
<h3>Bin Liu, Konstantinos Pliakos, Celine Vens, Grigorios Tsoumakas</h3>
<p>Predicting drug-target interactions (DTI) via reliable computational methods
is an effective and efficient way to mitigate the enormous costs and time of
the drug discovery process. Structure-based drug similarities and
sequence-based target protein similarities are the commonly used information
for DTI prediction. Among numerous computational methods, neighborhood-based
chemogenomic approaches that leverage drug and target similarities to perform
predictions directly are simple but promising ones. However, most existing
similarity-based methods follow the transductive setting. These methods cannot
directly generalize to unseen data because they should be re-built to predict
the interactions for new arriving drugs, targets, or drug-target pairs.
Besides, many similarity-based methods, especially neighborhood-based ones,
cannot handle directly all three types of interaction prediction. Furthermore,
a large amount of missing interactions in current DTI datasets hinders most DTI
prediction methods. To address these issues, we propose a new method denoted as
Weighted k Nearest Neighbor with Interaction Recovery (WkNNIR). Not only can
WkNNIR estimate interactions of any new drugs and/or new targets without any
need of re-training, but it can also recover missing interactions. In addition,
WkNNIR exploits local imbalance to promote the influence of more reliable
similarities on the DTI prediction process. We also propose a series of
ensemble methods that employ diverse sampling strategies and could be coupled
with WkNNIR as well as any other DTI prediction method to improve performance.
Experimental results over four benchmark datasets demonstrate the effectiveness
of our approaches in predicting drug-target interactions. Lastly, we confirm
the practical prediction ability of proposed methods to discover reliable
interactions that not reported in the original benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.12325" target="_blank">arXiv:2012.12325</a> [<a href="http://arxiv.org/pdf/2012.12325" target="_blank">pdf</a>]

<h2>Deep Unsupervised Image Hashing by Maximizing Bit Entropy. (arXiv:2012.12334v1 [cs.CV])</h2>
<h3>Yunqiang Li, Jan van Gemert</h3>
<p>Unsupervised hashing is important for indexing huge image or video
collections without having expensive annotations available. Hashing aims to
learn short binary codes for compact storage and efficient semantic retrieval.
We propose an unsupervised deep hashing layer called Bi-half Net that maximizes
entropy of the binary codes. Entropy is maximal when both possible values of
the bit are uniformly (half-half) distributed. To maximize bit entropy, we do
not add a term to the loss function as this is difficult to optimize and tune.
Instead, we design a new parameter-free network layer to explicitly force
continuous image features to approximate the optimal half-half bit
distribution. This layer is shown to minimize a penalized term of the
Wasserstein distance between the learned continuous image features and the
optimal half-half bit distribution. Experimental results on the image datasets
Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and
Hmdb-51 show that our approach leads to compact codes and compares favorably to
the current state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2012.12334" target="_blank">arXiv:2012.12334</a> [<a href="http://arxiv.org/pdf/2012.12334" target="_blank">pdf</a>]

<h2>Goal Reasoning by Selecting Subgoals with Deep Q-Learning. (arXiv:2012.12335v1 [cs.AI])</h2>
<h3>Carlos N&#xfa;&#xf1;ez-Molina, Vladislav Nikolov, Ignacio Vellido, Juan Fern&#xe1;ndez-Olivares</h3>
<p>In this work we propose a goal reasoning method which learns to select
subgoals with Deep Q-Learning in order to decrease the load of a planner when
faced with scenarios with tight time restrictions, such as online execution
systems. We have designed a CNN-based goal selection module and trained it on a
standard video game environment, testing it on different games (planning
domains) and levels (planning problems) to measure its generalization
abilities. When comparing its performance with a satisfying planner, the
results obtained show both approaches are able to find plans of good quality,
but our method greatly decreases planning time. We conclude our approach can be
successfully applied to different types of domains (games), and shows good
generalization properties when evaluated on new levels (problems) of the same
game (domain).
</p>
<a href="http://arxiv.org/abs/2012.12335" target="_blank">arXiv:2012.12335</a> [<a href="http://arxiv.org/pdf/2012.12335" target="_blank">pdf</a>]

<h2>SEAN-EP: A Platform for Collecting Human Feedback for Social Robot Navigation at Scale. (arXiv:2012.12336v1 [cs.RO])</h2>
<h3>Nathan Tsoi, Mohamed Hussein, Olivia Fugikawa, JD Zhao, Marynel V&#xe1;zquez</h3>
<p>We introduce the SEAN Experimental Platform (SEAN-EP), an open-source system
that allows roboticists to gather human feedback for social robot navigation at
scale using online interactive simulations. Through SEAN-EP, remote users can
control the motion of a human avatar via their web browser and interact with a
virtual robot controlled through the Robot Operating System. Heavy computation
in SEAN-EP is delegated to cloud servers such that users do not need
specialized hardware to take part in the simulations. We validated SEAN-EP and
its usability through an online survey, and compared the data collected from
this survey with a similar video survey. Our results suggest that human
perceptions of robots may differ based on whether they interact with the robots
in simulation or observe them in videos. Also, our study suggests that people
may perceive the surveys with interactive simulations as less mentally
demanding than video surveys.
</p>
<a href="http://arxiv.org/abs/2012.12336" target="_blank">arXiv:2012.12336</a> [<a href="http://arxiv.org/pdf/2012.12336" target="_blank">pdf</a>]

<h2>Seeing past words: Testing the cross-modal capabilities of pretrained V&L models. (arXiv:2012.12352v1 [cs.CV])</h2>
<h3>Letitia Parcalabescu, Albert Gatt, Anette Frank, Iacer Calixto</h3>
<p>We investigate the ability of general-purpose pretrained vision and language
V&amp;L models to perform reasoning in two tasks that require multimodal
integration: (1) discriminating a correct image-sentence pair from an incorrect
one, and (2) counting entities in an image. We evaluate three pretrained V&amp;L
models on these tasks: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and
finetuned settings. Our results show that models solve task (1) very well, as
expected, since all models use task (1) for pretraining. However, none of the
pretrained V&amp;L models are able to adequately solve task (2), our counting
probe, and they cannot generalise to out-of-distribution quantities. Our
investigations suggest that pretrained V&amp;L representations are less successful
than expected at integrating the two modalities. We propose a number of
explanations for these findings: LXMERT's results on the image-sentence
alignment task (and to a lesser extent those obtained by ViLBERT 12-in-1)
indicate that the model may exhibit catastrophic forgetting. As for our results
on the counting probe, we find evidence that all models are impacted by dataset
bias, and also fail to individuate entities in the visual input.
</p>
<a href="http://arxiv.org/abs/2012.12352" target="_blank">arXiv:2012.12352</a> [<a href="http://arxiv.org/pdf/2012.12352" target="_blank">pdf</a>]

<h2>Unbiased Subdata Selection for Fair Classification: A Unified Framework and Scalable Algorithms. (arXiv:2012.12356v1 [stat.ML])</h2>
<h3>Qing Ye, Weijun Xie</h3>
<p>As an important problem in modern data analytics, classification has
witnessed varieties of applications from different domains. Different from
conventional classification approaches, fair classification concerns the issues
of unintentional biases against the sensitive features (e.g., gender, race).
Due to high nonconvexity of fairness measures, existing methods are often
unable to model exact fairness, which can cause inferior fair classification
outcomes. This paper fills the gap by developing a novel unified framework to
jointly optimize accuracy and fairness. The proposed framework is versatile and
can incorporate different fairness measures studied in literature precisely as
well as can be applicable to many classifiers including deep classification
models. Specifically, in this paper, we first prove Fisher consistency of the
proposed framework. We then show that many classification models within this
framework can be recast as mixed-integer convex programs, which can be solved
effectively by off-the-shelf solvers when the instance sizes are moderate and
can be used as benchmarks to compare the efficiency of approximation
algorithms. We prove that in the proposed framework, when the classification
outcomes are known, the resulting problem, termed "unbiased subdata selection,"
is strongly polynomial-solvable and can be used to enhance the classification
fairness by selecting more representative data points. This motivates us to
develop iterative refining strategy (IRS) to solve the large-scale instances,
where we improve the classification accuracy and conduct the unbiased subdata
selection in an alternating fashion. We study the convergence property of IRS
and derive its approximation bound. More broadly, this framework can be
leveraged to improve classification models with unbalanced data by taking F1
score into consideration.
</p>
<a href="http://arxiv.org/abs/2012.12356" target="_blank">arXiv:2012.12356</a> [<a href="http://arxiv.org/pdf/2012.12356" target="_blank">pdf</a>]

<h2>A Structure-Aware Method for Direct Pose Estimation. (arXiv:2012.12360v1 [cs.CV])</h2>
<h3>Hunter Blanton, Scott Workman, Nathan Jacobs</h3>
<p>Estimating camera pose from a single image is a fundamental problem in
computer vision. Existing methods for solving this task fall into two distinct
categories, which we refer to as direct and indirect. Direct methods, such as
PoseNet, regress pose from the image as a fixed function, for example using a
feed-forward convolutional network. Such methods are desirable because they are
deterministic and run in constant time. Indirect methods for pose regression
are often non-deterministic, with various external dependencies such as image
retrieval and hypothesis sampling. We propose a direct method that takes
inspiration from structure-based approaches to incorporate explicit 3D
constraints into the network. Our approach maintains the desirable qualities of
other direct methods while achieving much lower error in general.
</p>
<a href="http://arxiv.org/abs/2012.12360" target="_blank">arXiv:2012.12360</a> [<a href="http://arxiv.org/pdf/2012.12360" target="_blank">pdf</a>]

<h2>Unbiased Gradient Estimation for Distributionally Robust Learning. (arXiv:2012.12367v1 [stat.ML])</h2>
<h3>Soumyadip Ghosh, Mark Squillante</h3>
<p>Seeking to improve model generalization, we consider a new approach based on
distributionally robust learning (DRL) that applies stochastic gradient descent
to the outer minimization problem. Our algorithm efficiently estimates the
gradient of the inner maximization problem through multi-level Monte Carlo
randomization. Leveraging theoretical results that shed light on why standard
gradient estimators fail, we establish the optimal parameterization of the
gradient estimators of our approach that balances a fundamental tradeoff
between computation time and statistical variance. Numerical experiments
demonstrate that our DRL approach yields significant benefits over previous
work.
</p>
<a href="http://arxiv.org/abs/2012.12367" target="_blank">arXiv:2012.12367</a> [<a href="http://arxiv.org/pdf/2012.12367" target="_blank">pdf</a>]

<h2>On Frank-Wolfe Optimization for Adversarial Robustness and Interpretability. (arXiv:2012.12368v1 [cs.LG])</h2>
<h3>Theodoros Tsiligkaridis, Jay Roberts</h3>
<p>Deep neural networks are easily fooled by small perturbations known as
adversarial attacks. Adversarial Training (AT) is a technique that
approximately solves a robust optimization problem to minimize the worst-case
loss and is widely regarded as the most effective defense against such attacks.
While projected gradient descent (PGD) has received most attention for
approximately solving the inner maximization of AT, Frank-Wolfe (FW)
optimization is projection-free and can be adapted to any $L^p$ norm. A
Frank-Wolfe adversarial training approach is presented and is shown to provide
as competitive level of robustness as PGD-AT without much tuning for a variety
of architectures. We empirically show that robustness is strongly connected to
the $L^2$ magnitude of the adversarial perturbation and that more locally
linear loss landscapes tend to have larger $L^2$ distortions despite having the
same $L^\infty$ distortion. We provide theoretical guarantees on the magnitude
of the distortion for FW that depend on local geometry which FW-AT exploits. It
is empirically shown that FW-AT achieves strong robustness to white-box attacks
and black-box attacks and offers improved resistance to gradient masking.
Further, FW-AT allows networks to learn high-quality human-interpretable
features which are then used to generate counterfactual explanations to model
predictions by using dense and sparse adversarial perturbations.
</p>
<a href="http://arxiv.org/abs/2012.12368" target="_blank">arXiv:2012.12368</a> [<a href="http://arxiv.org/pdf/2012.12368" target="_blank">pdf</a>]

<h2>Out-distribution aware Self-training in an Open World Setting. (arXiv:2012.12372v1 [cs.LG])</h2>
<h3>Maximilian Augustin, Matthias Hein</h3>
<p>Deep Learning heavily depends on large labeled datasets which limits further
improvements. While unlabeled data is available in large amounts, in particular
in image recognition, it does not fulfill the closed world assumption of
semi-supervised learning that all unlabeled data are task-related. The goal of
this paper is to leverage unlabeled data in an open world setting to further
improve prediction performance. For this purpose, we introduce out-distribution
aware self-training, which includes a careful sample selection strategy based
on the confidence of the classifier. While normal self-training deteriorates
prediction performance, our iterative scheme improves using up to 15 times the
amount of originally labeled data. Moreover, our classifiers are by design
out-distribution aware and can thus distinguish task-related inputs from
unrelated ones.
</p>
<a href="http://arxiv.org/abs/2012.12372" target="_blank">arXiv:2012.12372</a> [<a href="http://arxiv.org/pdf/2012.12372" target="_blank">pdf</a>]

<h2>The Life and Death of SSDs and HDDs: Similarities, Differences, and Prediction Models. (arXiv:2012.12373v1 [cs.LG])</h2>
<h3>Riccardo Pinciroli, Lishan Yang, Jacob Alter, Evgenia Smirni</h3>
<p>Data center downtime typically centers around IT equipment failure. Storage
devices are the most frequently failing components in data centers. We present
a comparative study of hard disk drives (HDDs) and solid state drives (SSDs)
that constitute the typical storage in data centers. Using a six-year field
data of 100,000 HDDs of different models from the same manufacturer from the
BackBlaze dataset and a six-year field data of 30,000 SSDs of three models from
a Google data center, we characterize the workload conditions that lead to
failures and illustrate that their root causes differ from common expectation
but remain difficult to discern. For the case of HDDs we observe that young and
old drives do not present many differences in their failures. Instead, failures
may be distinguished by discriminating drives based on the time spent for head
positioning. For SSDs, we observe high levels of infant mortality and
characterize the differences between infant and non-infant failures. We develop
several machine learning failure prediction models that are shown to be
surprisingly accurate, achieving high recall and low false positive rates.
These models are used beyond simple prediction as they aid us to untangle the
complex interaction of workload characteristics that lead to failures and
identify failure root causes from monitored symptoms.
</p>
<a href="http://arxiv.org/abs/2012.12373" target="_blank">arXiv:2012.12373</a> [<a href="http://arxiv.org/pdf/2012.12373" target="_blank">pdf</a>]

<h2>DAGMapper: Learning to Map by Discovering Lane Topology. (arXiv:2012.12377v1 [cs.CV])</h2>
<h3>Namdar Homayounfar, Wei-Chiu Ma, Justin Liang, Xinyu Wu, Jack Fan, Raquel Urtasun</h3>
<p>One of the fundamental challenges to scale self-driving is being able to
create accurate high definition maps (HD maps) with low cost. Current attempts
to automate this process typically focus on simple scenarios, estimate
independent maps per frame or do not have the level of precision required by
modern self driving vehicles. In contrast, in this paper we focus on drawing
the lane boundaries of complex highways with many lanes that contain topology
changes due to forks and merges. Towards this goal, we formulate the problem as
inference in a directed acyclic graphical model (DAG), where the nodes of the
graph encode geometric and topological properties of the local regions of the
lane boundaries. Since we do not know a priori the topology of the lanes, we
also infer the DAG topology (i.e., nodes and edges) for each region. We
demonstrate the effectiveness of our approach on two major North American
Highways in two different states and show high precision and recall as well as
89% correct topology.
</p>
<a href="http://arxiv.org/abs/2012.12377" target="_blank">arXiv:2012.12377</a> [<a href="http://arxiv.org/pdf/2012.12377" target="_blank">pdf</a>]

<h2>Fractal Dimension Generalization Measure. (arXiv:2012.12384v1 [cs.LG])</h2>
<h3>Valeri Alexiev</h3>
<p>Developing a robust generalization measure for the performance of machine
learning models is an important and challenging task. A lot of recent research
in the area focuses on the model decision boundary when predicting
generalization. In this paper, as part of the "Predicting Generalization in
Deep Learning" competition, we analyse the complexity of decision boundaries
using the concept of fractal dimension and develop a generalization measure
based on that technique.
</p>
<a href="http://arxiv.org/abs/2012.12384" target="_blank">arXiv:2012.12384</a> [<a href="http://arxiv.org/pdf/2012.12384" target="_blank">pdf</a>]

<h2>Workspace Analysis and Optimal Design of Cable-Driven Parallel Robots via Auxiliary Counterbalances. (arXiv:2012.12387v1 [cs.RO])</h2>
<h3>Ronghuai Qi, Hamed Jamshidifar, Amir Khajepour</h3>
<p>Cable-driven parallel robots (CDPRs) are widely investigated and applied in
the worldwide; however, traditional configurations make them to be limited in
reaching their maximum workspace duo to constraints such as the maximum
allowable tensions of cables. In this paper, we introduce auxiliary
counterbalances to tackle this problem and focus on workspace analysis and
optimal design of CDPRs with such systems. Besides, kinematics, dynamics, and
parameters optimization formulas and algorithm are provided to maximize the
reachable workspace of CDPRs. Case studies for different configurations are
presented and discussed. Numerical results suggest the effectiveness of the
aforementioned approaches, and the obtained parameters can also be applied for
actual CDPRs design.
</p>
<a href="http://arxiv.org/abs/2012.12387" target="_blank">arXiv:2012.12387</a> [<a href="http://arxiv.org/pdf/2012.12387" target="_blank">pdf</a>]

<h2>Probabilistic Outlier Detection and Generation. (arXiv:2012.12394v1 [cs.LG])</h2>
<h3>Stefano Giovanni Rizzo, Linsey Pang, Yixian Chen, Sanjay Chawla</h3>
<p>A new method for outlier detection and generation is introduced by lifting
data into the space of probability distributions which are not analytically
expressible, but from which samples can be drawn using a neural generator.
Given a mixture of unknown latent inlier and outlier distributions, a
Wasserstein double autoencoder is used to both detect and generate inliers and
outliers. The proposed method, named WALDO (Wasserstein Autoencoder for
Learning the Distribution of Outliers), is evaluated on classical data sets
including MNIST, CIFAR10 and KDD99 for detection accuracy and robustness. We
give an example of outlier detection on a real retail sales data set and an
example of outlier generation for simulating intrusion attacks. However we
foresee many application scenarios where WALDO can be used. To the best of our
knowledge this is the first work that studies both outlier detection and
generation together.
</p>
<a href="http://arxiv.org/abs/2012.12394" target="_blank">arXiv:2012.12394</a> [<a href="http://arxiv.org/pdf/2012.12394" target="_blank">pdf</a>]

<h2>Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net. (arXiv:2012.12395v1 [cs.CV])</h2>
<h3>Wenjie Luo, Bin Yang, Raquel Urtasun</h3>
<p>In this paper we propose a novel deep neural network that is able to jointly
reason about 3D detection, tracking and motion forecasting given data captured
by a 3D sensor. By jointly reasoning about these tasks, our holistic approach
is more robust to occlusion as well as sparse data at range. Our approach
performs 3D convolutions across space and time over a bird's eye view
representation of the 3D world, which is very efficient in terms of both memory
and computation. Our experiments on a new very large scale dataset captured in
several north american cities, show that we can outperform the state-of-the-art
by a large margin. Importantly, by sharing computation we can perform all tasks
in as little as 30 ms.
</p>
<a href="http://arxiv.org/abs/2012.12395" target="_blank">arXiv:2012.12395</a> [<a href="http://arxiv.org/pdf/2012.12395" target="_blank">pdf</a>]

<h2>Multi-Task Multi-Sensor Fusion for 3D Object Detection. (arXiv:2012.12397v1 [cs.CV])</h2>
<h3>Ming Liang, Bin Yang, Yun Chen, Rui Hu, Raquel Urtasun</h3>
<p>In this paper we propose to exploit multiple related tasks for accurate
multi-sensor 3D object detection. Towards this goal we present an end-to-end
learnable architecture that reasons about 2D and 3D object detection as well as
ground estimation and depth completion. Our experiments show that all these
tasks are complementary and help the network learn better representations by
fusing information at various levels. Importantly, our approach leads the KITTI
benchmark on 2D, 3D and BEV object detection, while being real time.
</p>
<a href="http://arxiv.org/abs/2012.12397" target="_blank">arXiv:2012.12397</a> [<a href="http://arxiv.org/pdf/2012.12397" target="_blank">pdf</a>]

<h2>Turn Signal Prediction: A Federated Learning Case Study. (arXiv:2012.12401v1 [cs.AI])</h2>
<h3>Sonal Doomra, Naman Kohli, Shounak Athavale</h3>
<p>Driving etiquette takes a different flavor for each locality as drivers not
only comply with rules/laws but also abide by local unspoken convention. When
to have the turn signal (indicator) on/off is one such etiquette which does not
have a definitive right or wrong answer. Learning this behavior from the
abundance of data generated from various sensor modalities integrated in the
vehicle is a suitable candidate for deep learning. But what makes it a prime
candidate for Federated Learning are privacy concerns and bandwidth limitations
for any data aggregation. This paper presents a long short-term memory (LSTM)
based Turn Signal Prediction (on or off) model using vehicle control area
network (CAN) signal data. The model is trained using two approaches, one by
centrally aggregating the data and the other in a federated manner. Centrally
trained models and federated models are compared under similar hyperparameter
settings. This research demonstrates the efficacy of federated learning, paving
the way for in-vehicle learning of driving etiquette.
</p>
<a href="http://arxiv.org/abs/2012.12401" target="_blank">arXiv:2012.12401</a> [<a href="http://arxiv.org/pdf/2012.12401" target="_blank">pdf</a>]

<h2>Learning Joint 2D-3D Representations for Depth Completion. (arXiv:2012.12402v1 [cs.CV])</h2>
<h3>Yun Chen, Bin Yang, Ming Liang, Raquel Urtasun</h3>
<p>In this paper, we tackle the problem of depth completion from RGBD data.
Towards this goal, we design a simple yet effective neural network block that
learns to extract joint 2D and 3D features. Specifically, the block consists of
two domain-specific sub-networks that apply 2D convolution on image pixels and
continuous convolution on 3D points, with their output features fused in image
space. We build the depth completion network simply by stacking the proposed
block, which has the advantage of learning hierarchical representations that
are fully fused between 2D and 3D spaces at multiple levels. We demonstrate the
effectiveness of our approach on the challenging KITTI depth completion
benchmark and show that our approach outperforms the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2012.12402" target="_blank">arXiv:2012.12402</a> [<a href="http://arxiv.org/pdf/2012.12402" target="_blank">pdf</a>]

<h2>Open source software for automatic subregional assessment of knee cartilage degradation using quantitative T2 relaxometry and deep learning. (arXiv:2012.12406v1 [cs.CV])</h2>
<h3>Kevin A. Thomas (1), Dominik Krzemi&#x144;ski (2), &#x141;ukasz Kidzi&#x144;ski (3), Rohan Paul (1), Elka B. Rubin (4), Eni Halilaj (5), Marianne S. Black (4) Akshay Chaudhari (1,4), Garry E. Gold (3,4,6), Scott L. Delp (3,6,7) ((1) Department of Biomedical Data Science, Stanford University, California, USA (2) Cardiff University Brain Research Imaging Centre, Cardiff University, United Kingdom (3) Department of Biomedical Engineering, Stanford University, California, USA (4) Department of Radiology, Stanford University, California, USA (5) Department of Mechanical Engineering, Carnegie Mellon University, Pennsylvania, USA (6) Department of Orthopaedic Surgery, Stanford University, California, USA (7) Department of Mechanical Engineering, Stanford University, California, USA)</h3>
<p>Objective: We evaluate a fully-automated femoral cartilage segmentation model
for measuring T2 relaxation values and longitudinal changes using multi-echo
spin echo (MESE) MRI. We have open sourced this model and corresponding
segmentations. Methods: We trained a neural network to segment femoral
cartilage from MESE MRIs. Cartilage was divided into 12 subregions along
medial-lateral, superficial-deep, and anterior-central-posterior boundaries.
Subregional T2 values and four-year changes were calculated using a
musculoskeletal radiologist's segmentations (Reader 1) and the model's
segmentations. These were compared using 28 held out images. A subset of 14
images were also evaluated by a second expert (Reader 2) for comparison.
Results: Model segmentations agreed with Reader 1 segmentations with a Dice
score of 0.85 +/- 0.03. The model's estimated T2 values for individual
subregions agreed with those of Reader 1 with an average Spearman correlation
of 0.89 and average mean absolute error (MAE) of 1.34 ms. The model's estimated
four-year change in T2 for individual regions agreed with Reader 1 with an
average correlation of 0.80 and average MAE of 1.72 ms. The model agreed with
Reader 1 at least as closely as Reader 2 agreed with Reader 1 in terms of Dice
score (0.85 vs 0.75) and subregional T2 values. Conclusions: We present a fast,
fully-automated model for segmentation of MESE MRIs. Assessments of cartilage
health using its segmentations agree with those of an expert as closely as
experts agree with one another. This has the potential to accelerate
osteoarthritis research.
</p>
<a href="http://arxiv.org/abs/2012.12406" target="_blank">arXiv:2012.12406</a> [<a href="http://arxiv.org/pdf/2012.12406" target="_blank">pdf</a>]

<h2>Sensing and Reconstruction of 3D Deformation on Pneumatic Soft Robots. (arXiv:2012.12411v1 [cs.RO])</h2>
<h3>Rob B.N. Scharff, Guoxin Fang, Yingjun Tian, Jun Wu, Jo M.P. Geraedts, Charlie C.L. Wang</h3>
<p>Real-time proprioception is a challenging problem for soft robots, which have
almost infinite degrees-of-freedom in body deformation. When multiple actuators
are used, it becomes more difficult as deformation can also occur on actuators
caused by interaction between each other. To tackle this problem, we present a
method in this paper to sense and reconstruct 3D deformation on pneumatic soft
robots by first integrating multiple low-cost sensors inside the chambers of
pneumatic actuators and then using machine learning to convert the captured
signals into shape parameters of soft robots. An exterior motion capture system
is employed to generate the datasets for both training and testing. With the
help of good shape parameterization, the 3D shape of a soft robot can be
accurately reconstructed from signals obtained from multiple sensors. We
demonstrate the effectiveness of this approach on two designs of soft robots --
a robotic joint and a deformable membrane. After parameterizing the deformation
of these soft robots into compact shape parameters, we can effectively train
the neural networks to reconstruct the 3D deformation from the sensor signals.
The sensing and shape prediction pipeline can run at 50Hz in real-time on a
consumer-level device.
</p>
<a href="http://arxiv.org/abs/2012.12411" target="_blank">arXiv:2012.12411</a> [<a href="http://arxiv.org/pdf/2012.12411" target="_blank">pdf</a>]

<h2>Optical Braille Recognition Using Object Detection CNN. (arXiv:2012.12412v1 [cs.CV])</h2>
<h3>Ilya G. Ovodov</h3>
<p>This paper proposes an optical Braille recognition method that uses an object
detection convolutional neural network to detect whole Braille characters at
once. The proposed algorithm is robust to the deformation of the page shown in
the image and perspective distortions. It makes it usable for recognition of
Braille texts being shoot on a smartphone camera, including bowed pages and
perspective distorted images. The proposed algorithm shows high performance and
accuracy compared to existing methods. We also introduce a new "Angelina
Braille Images Dataset" containing 240 annotated photos of Braille texts. The
proposed algorithm and dataset are available at GitHub.
</p>
<a href="http://arxiv.org/abs/2012.12412" target="_blank">arXiv:2012.12412</a> [<a href="http://arxiv.org/pdf/2012.12412" target="_blank">pdf</a>]

<h2>Stochastic Gradient Variance Reduction by Solving a Filtering Problem. (arXiv:2012.12418v1 [cs.LG])</h2>
<h3>Xingyi Yang</h3>
<p>Deep neural networks (DNN) are typically optimized using stochastic gradient
descent (SGD). However, the estimation of the gradient using stochastic samples
tends to be noisy and unreliable, resulting in large gradient variance and bad
convergence. In this paper, we propose \textbf{Filter Gradient Decent}~(FGD),
an efficient stochastic optimization algorithm that makes the consistent
estimation of the local gradient by solving an adaptive filtering problem with
different design of filters. Our method reduces variance in stochastic gradient
descent by incorporating the historical states to enhance the current
estimation. It is able to correct noisy gradient direction as well as to
accelerate the convergence of learning. We demonstrate the effectiveness of the
proposed Filter Gradient Descent on numerical optimization and training neural
networks, where it achieves superior and robust performance compared with
traditional momentum-based methods. To the best of our knowledge, we are the
first to provide a practical solution that integrates filtering into gradient
estimation by making the analogy between gradient estimation and filtering
problems in signal processing. (The code is provided in
https://github.com/Adamdad/Filter-Gradient-Decent)
</p>
<a href="http://arxiv.org/abs/2012.12418" target="_blank">arXiv:2012.12418</a> [<a href="http://arxiv.org/pdf/2012.12418" target="_blank">pdf</a>]

<h2>Hybrid FL: Algorithms and Implementation. (arXiv:2012.12420v1 [cs.LG])</h2>
<h3>Xinwei Zhang, Wotao Yin, Mingyi Hong, Tianyi Chen</h3>
<p>Federated learning (FL) is a recently proposed distributed machine learning
paradigm dealing with distributed and private data sets. Based on the data
partition pattern, FL is often categorized into horizontal, vertical, and
hybrid settings. Despite the fact that many works have been developed for the
first two approaches, the hybrid FL setting (which deals with partially
overlapped feature space and sample space) remains less explored, though this
setting is extremely important in practice. In this paper, we first set up a
new model-matching-based problem formulation for hybrid FL, then propose an
efficient algorithm that can collaboratively train the global and local models
to deal with full and partial featured data. We conduct numerical experiments
on the multi-view ModelNet40 data set to validate the performance of the
proposed algorithm. To the best of our knowledge, this is the first formulation
and algorithm developed for the hybrid FL.
</p>
<a href="http://arxiv.org/abs/2012.12420" target="_blank">arXiv:2012.12420</a> [<a href="http://arxiv.org/pdf/2012.12420" target="_blank">pdf</a>]

<h2>SmartShuttle: Model Based Design and Evaluation of Automated On-Demand Shuttles for Solving the First-Mile and Last-Mile Problem in a Smart City. (arXiv:2012.12431v1 [cs.RO])</h2>
<h3>Sukru Yaren Gelbal, Bilin Aksun-Guvenc, Levent Guvenc</h3>
<p>The final project report for the SmartShuttle sub-project of the Ohio State
University is presented in this report. This has been a two year project where
the unified, scalable and replicable automated driving architecture introduced
by the Automated Driving Lab of the Ohio State University has been further
developed, replicated in different vehicles and scaled between different
vehicle sizes. A limited scale demonstration was also conducted during the
first year of the project. The architecture used was further developed in the
second project year including parameter space based low level controller
design, perception methods and data collection. Perception sensor and other
relevant vehicle data were collected in the second project year. Our approach
changed to using soft AVs in a hardware-in-the-loop simulation environment for
proof-of-concept testing. Our second year work also had a change of
localization from GPS and lidar based SLAM to GPS and map matching using a
previously constructed lidar map in a geo-fenced area. An example lidar map was
also created. Perception sensor and other collected data and an example lidar
map are shared as datasets as further outcomes of the project.
</p>
<a href="http://arxiv.org/abs/2012.12431" target="_blank">arXiv:2012.12431</a> [<a href="http://arxiv.org/pdf/2012.12431" target="_blank">pdf</a>]

<h2>Pit30M: A Benchmark for Global Localization in the Age of Self-Driving Cars. (arXiv:2012.12437v1 [cs.CV])</h2>
<h3>Julieta Martinez, Sasha Doubov, Jack Fan, Ioan Andrei B&#xe2;rsan, Shenlong Wang, Gell&#xe9;rt M&#xe1;ttyus, Raquel Urtasun</h3>
<p>We are interested in understanding whether retrieval-based localization
approaches are good enough in the context of self-driving vehicles. Towards
this goal, we introduce Pit30M, a new image and LiDAR dataset with over 30
million frames, which is 10 to 100 times larger than those used in previous
work. Pit30M is captured under diverse conditions (i.e., season, weather, time
of the day, traffic), and provides accurate localization ground truth. We also
automatically annotate our dataset with historical weather and astronomical
data, as well as with image and LiDAR semantic segmentation as a proxy measure
for occlusion. We benchmark multiple existing methods for image and LiDAR
retrieval and, in the process, introduce a simple, yet effective convolutional
network-based LiDAR retrieval method that is competitive with the state of the
art. Our work provides, for the first time, a benchmark for sub-metre
retrieval-based localization at city scale. The dataset, additional
experimental results, as well as more information about the sensors,
calibration, and metadata, are available on the project website:
https://uber.com/atg/datasets/pit30m
</p>
<a href="http://arxiv.org/abs/2012.12437" target="_blank">arXiv:2012.12437</a> [<a href="http://arxiv.org/pdf/2012.12437" target="_blank">pdf</a>]

<h2>State of the Art of Adaptive Cruise Control and Stop and Go Systems. (arXiv:2012.12438v1 [cs.RO])</h2>
<h3>Emre Kural, Tahsin Hacibekir, Bilin Aksun-Guvenc</h3>
<p>This paper presents the state of the art of Adaptive Cruise Control (ACC) and
Stop and Go systems as well as Intelligent Transportation Systems enhanced with
inter vehicle communication. The sensors used in these systems and the level of
their current technology are introduced. Simulators related to ACC and Stop and
Go (S&amp;G) systems are also surveyed and the MEKAR simulator is presented.
Finally, future trends of ACC and Stop and Go systems and their advantages are
emphasized.
</p>
<a href="http://arxiv.org/abs/2012.12438" target="_blank">arXiv:2012.12438</a> [<a href="http://arxiv.org/pdf/2012.12438" target="_blank">pdf</a>]

<h2>Correspondence Learning for Controllable Person Image Generation. (arXiv:2012.12440v1 [cs.CV])</h2>
<h3>Shilong Shen</h3>
<p>We present a generative model for controllable person image synthesis,as
shown in Figure , which can be applied to pose-guided person image synthesis,
$i.e.$, converting the pose of a source person image to the target pose while
preserving the texture of that source person image, and clothing-guided person
image synthesis, $i.e.$, changing the clothing texture of a source person image
to the desired clothing texture. By explicitly establishing the dense
correspondence between the target pose and the source image, we can effectively
address the misalignment introduced by pose tranfer and generate high-quality
images. Specifically, we first generate the target semantic map under the
guidence of the target pose, which can provide more accurate pose
representation and structural constraints during the generation process. Then,
decomposed attribute encoder is used to extract the component features, which
not only helps to establish a more accurate dense correspondence, but also
realizes the clothing-guided person generation. After that, we will establish a
dense correspondence between the target pose and the source image within the
sharded domain. The source image feature is warped according to the dense
correspondence to flexibly account for deformations. Finally, the network
renders image based on the warped source image feature and the target pose.
Experimental results show that our method is superior to state-of-the-art
methods in pose-guided person generation and its effectiveness in
clothing-guided person generation.
</p>
<a href="http://arxiv.org/abs/2012.12440" target="_blank">arXiv:2012.12440</a> [<a href="http://arxiv.org/pdf/2012.12440" target="_blank">pdf</a>]

<h2>A Survey of Recent Developments in Collision Avoidance, Collision Warning and Inter-Vehicle Communication Systems. (arXiv:2012.12441v1 [cs.RO])</h2>
<h3>Oncu Ararat, Bilin Aksun-Guvenc</h3>
<p>This paper presents the state-of-the-art on Collision Avoidance and Collision
Warning (CA,CW) systems. Traffic accidents result from driver errors or
situations that are unpredictable for the driver. CA,CW systems are developed
for reducing traffic accidents and saving peoples lives by warning drivers or
taking compensatory action when drivers can not react fast enough. Besides
these initiatives, this paper explains the importance of CA,CW driving
assistance systems by considering economic issues as well. Technological
developments are investigated in the context of finished and ongoing projects
all over the world. CA,CW system algorithms are discussed regarding performance
criteria such as reliability and strictness. This paper also presents the
information on Inter-Vehicle Communication Systems (IVC) which will be a key
ingredient of future CA,CW systems.
</p>
<a href="http://arxiv.org/abs/2012.12441" target="_blank">arXiv:2012.12441</a> [<a href="http://arxiv.org/pdf/2012.12441" target="_blank">pdf</a>]

<h2>MG-SAGC: A multiscale graph and its self-adaptive graph convolution network for 3D point clouds. (arXiv:2012.12445v1 [cs.CV])</h2>
<h3>Bo Wu, Bo Lang</h3>
<p>To enhance the ability of neural networks to extract local point cloud
features and improve their quality, in this paper, we propose a multiscale
graph generation method and a self-adaptive graph convolution method. First, we
propose a multiscale graph generation method for point clouds. This approach
transforms point clouds into a structured multiscale graph form that supports
multiscale analysis of point clouds in the scale space and can obtain the
dimensional features of point cloud data at different scales, thus making it
easier to obtain the best point cloud features. Because traditional
convolutional neural networks are not applicable to graph data with irregular
vertex neighborhoods, this paper presents an sef-adaptive graph convolution
kernel that uses the Chebyshev polynomial to fit an irregular convolution
filter based on the theory of optimal approximation. In this paper, we adopt
max pooling to synthesize the features of different scale maps and generate the
point cloud features. In experiments conducted on three widely used public
datasets, the proposed method significantly outperforms other state-of-the-art
models, demonstrating its effectiveness and generalizability.
</p>
<a href="http://arxiv.org/abs/2012.12445" target="_blank">arXiv:2012.12445</a> [<a href="http://arxiv.org/pdf/2012.12445" target="_blank">pdf</a>]

<h2>Skeleton-based Approaches based on Machine Vision: A Survey. (arXiv:2012.12447v1 [cs.CV])</h2>
<h3>Jie Li, Binglin Li, Min Gao</h3>
<p>Recently, skeleton-based approaches have achieved rapid progress on the basis
of great success in skeleton representation. Plenty of researches focus on
solving specific problems according to skeleton features. Some skeleton-based
approaches have been mentioned in several overviews on object detection as a
non-essential part. Nevertheless, there has not been any thorough analysis of
skeleton-based approaches attentively. Instead of describing these techniques
in terms of theoretical constructs, we devote to summarizing skeleton-based
approaches with regard to application fields and given tasks as comprehensively
as possible. This paper is conducive to further understanding of skeleton-based
application and dealing with particular issues.
</p>
<a href="http://arxiv.org/abs/2012.12447" target="_blank">arXiv:2012.12447</a> [<a href="http://arxiv.org/pdf/2012.12447" target="_blank">pdf</a>]

<h2>Partial Identifiability in Discrete Data With Measurement Error. (arXiv:2012.12449v1 [stat.ML])</h2>
<h3>Noam Finkelstein, Roy Adams, Suchi Saria, Ilya Shpitser</h3>
<p>When data contains measurement errors, it is necessary to make assumptions
relating the observed, erroneous data to the unobserved true phenomena of
interest. These assumptions should be justifiable on substantive grounds, but
are often motivated by mathematical convenience, for the sake of exactly
identifying the target of inference. We adopt the view that it is preferable to
present bounds under justifiable assumptions than to pursue exact
identification under dubious ones. To that end, we demonstrate how a broad
class of modeling assumptions involving discrete variables, including common
measurement error and conditional independence assumptions, can be expressed as
linear constraints on the parameters of the model. We then use linear
programming techniques to produce sharp bounds for factual and counterfactual
distributions under measurement error in such models. We additionally propose a
procedure for obtaining outer bounds on non-linear models. Our method yields
sharp bounds in a number of important settings -- such as the instrumental
variable scenario with measurement error -- for which no bounds were previously
known.
</p>
<a href="http://arxiv.org/abs/2012.12449" target="_blank">arXiv:2012.12449</a> [<a href="http://arxiv.org/pdf/2012.12449" target="_blank">pdf</a>]

<h2>Towards Automated Satellite Conjunction Management with Bayesian Deep Learning. (arXiv:2012.12450v1 [cs.LG])</h2>
<h3>Francesco Pinto, Giacomo Acciarini, Sascha Metz, Sarah Boufelja, Sylvester Kaczmarek, Klaus Merz, Jos&#xe9; A. Martinez-Heras, Francesca Letizia, Christopher Bridges, At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</h3>
<p>After decades of space travel, low Earth orbit is a junkyard of discarded
rocket bodies, dead satellites, and millions of pieces of debris from
collisions and explosions. Objects in high enough altitudes do not re-enter and
burn up in the atmosphere, but stay in orbit around Earth for a long time. With
a speed of 28,000 km/h, collisions in these orbits can generate fragments and
potentially trigger a cascade of more collisions known as the Kessler syndrome.
This could pose a planetary challenge, because the phenomenon could escalate to
the point of hindering future space operations and damaging satellite
infrastructure critical for space and Earth science applications. As commercial
entities place mega-constellations of satellites in orbit, the burden on
operators conducting collision avoidance manoeuvres will increase. For this
reason, development of automated tools that predict potential collision events
(conjunctions) is critical. We introduce a Bayesian deep learning approach to
this problem, and develop recurrent neural network architectures (LSTMs) that
work with time series of conjunction data messages (CDMs), a standard data
format used by the space community. We show that our method can be used to
model all CDM features simultaneously, including the time of arrival of future
CDMs, providing predictions of conjunction event evolution with associated
uncertainties.
</p>
<a href="http://arxiv.org/abs/2012.12450" target="_blank">arXiv:2012.12450</a> [<a href="http://arxiv.org/pdf/2012.12450" target="_blank">pdf</a>]

<h2>CholecSeg8k: A Semantic Segmentation Dataset for Laparoscopic Cholecystectomy Based on Cholec80. (arXiv:2012.12453v1 [cs.CV])</h2>
<h3>W.-Y. Hong, C.-L. Kao, Y.-H. Kuo, J.-R. Wang, W.-L. Chang, C.-S. Shih</h3>
<p>Computer-assisted surgery has been developed to enhance surgery correctness
and safety. However, researchers and engineers suffer from limited annotated
data to develop and train better algorithms. Consequently, the development of
fundamental algorithms such as Simultaneous Localization and Mapping (SLAM) is
limited. This article elaborates on the efforts of preparing the dataset for
semantic segmentation, which is the foundation of many computer-assisted
surgery mechanisms. Based on the Cholec80 dataset [3], we extracted 8,080
laparoscopic cholecystectomy image frames from 17 video clips in Cholec80 and
annotated the images. The dataset is named CholecSeg8K and its total size is
3GB. Each of these images is annotated at pixel-level for thirteen classes,
which are commonly founded in laparoscopic cholecystectomy surgery. CholecSeg8k
is released under the license CC BY- NC-SA 4.0.
</p>
<a href="http://arxiv.org/abs/2012.12453" target="_blank">arXiv:2012.12453</a> [<a href="http://arxiv.org/pdf/2012.12453" target="_blank">pdf</a>]

<h2>Augmenting Policy Learning with Routines Discovered from a Demonstration. (arXiv:2012.12469v1 [cs.LG])</h2>
<h3>Zelin Zhao, Chuang Gan, Jiajun Wu, Xiaoxiao Guo, Joshua Tenenbaum</h3>
<p>Humans can abstract prior knowledge from very little data and use it to boost
skill learning. In this paper, we propose routine-augmented policy learning
(RAPL), which discovers routines composed of primitive actions from a single
demonstration and uses discovered routines to augment policy learning. To
discover routines from the demonstration, we first abstract routine candidates
by identifying grammar over the demonstrated action trajectory. Then, the best
routines measured by length and frequency are selected to form a routine
library. We propose to learn policy simultaneously at primitive-level and
routine-level with discovered routines, leveraging the temporal structure of
routines. Our approach enables imitating expert behavior at multiple temporal
scales for imitation learning and promotes reinforcement learning exploration.
Extensive experiments on Atari games demonstrate that RAPL improves the
state-of-the-art imitation learning method SQIL and reinforcement learning
method A2C. Further, we show that discovered routines can generalize to unseen
levels and difficulties on the CoinRun benchmark.
</p>
<a href="http://arxiv.org/abs/2012.12469" target="_blank">arXiv:2012.12469</a> [<a href="http://arxiv.org/pdf/2012.12469" target="_blank">pdf</a>]

<h2>Comparison of Classification Algorithms Towards Subject-Specific and Subject-Independent BCI. (arXiv:2012.12473v1 [cs.LG])</h2>
<h3>Parisa Ghane, Ulisses Braga-Neto</h3>
<p>Motor imagery brain computer interface designs are considered difficult due
to limitations in subject-specific data collection and calibration, as well as
demanding system adaptation requirements. Recently, subject-independent (SI)
designs received attention because of their possible applicability to multiple
users without prior calibration and rigorous system adaptation. SI designs are
challenging and have shown low accuracy in the literature. Two major factors in
system performance are the classification algorithm and the quality of
available data. This paper presents a comparative study of classification
performance for both SS and SI paradigms. Our results show that classification
algorithms for SS models display large variance in performance. Therefore,
distinct classification algorithms per subject may be required. SI models
display lower variance in performance but should only be used if a relatively
large sample size is available. For SI models, LDA and CART had the highest
accuracy for small and moderate sample size, respectively, whereas we
hypothesize that SVM would be superior to the other classifiers if large
training sample-size was available. Additionally, one should choose the design
approach considering the users. While the SS design sound more promising for a
specific subject, an SI approach can be more convenient for mentally or
physically challenged users.
</p>
<a href="http://arxiv.org/abs/2012.12473" target="_blank">arXiv:2012.12473</a> [<a href="http://arxiv.org/pdf/2012.12473" target="_blank">pdf</a>]

<h2>Self-supervised self-supervision by combining deep learning and probabilistic logic. (arXiv:2012.12474v1 [cs.LG])</h2>
<h3>Hunter Lang, Hoifung Poon</h3>
<p>Labeling training examples at scale is a perennial challenge in machine
learning. Self-supervision methods compensate for the lack of direct
supervision by leveraging prior knowledge to automatically generate noisy
labeled examples. Deep probabilistic logic (DPL) is a unifying framework for
self-supervised learning that represents unknown labels as latent variables and
incorporates diverse self-supervision using probabilistic logic to train a deep
neural network end-to-end using variational EM. While DPL is successful at
combining pre-specified self-supervision, manually crafting self-supervision to
attain high accuracy may still be tedious and challenging. In this paper, we
propose Self-Supervised Self-Supervision (S4), which adds to DPL the capability
to learn new self-supervision automatically. Starting from an initial "seed,"
S4 iteratively uses the deep neural network to propose new self supervision.
These are either added directly (a form of structured self-training) or
verified by a human expert (as in feature-based active learning). Experiments
show that S4 is able to automatically propose accurate self-supervision and can
often nearly match the accuracy of supervised methods with a tiny fraction of
the human effort.
</p>
<a href="http://arxiv.org/abs/2012.12474" target="_blank">arXiv:2012.12474</a> [<a href="http://arxiv.org/pdf/2012.12474" target="_blank">pdf</a>]

<h2>IIRC: Incremental Implicitly-Refined Classification. (arXiv:2012.12477v1 [cs.CV])</h2>
<h3>Mohamed Abdelsalam, Mojtaba Faramarzi, Shagun Sodhani, Sarath Chandar</h3>
<p>We introduce the "Incremental Implicitly-Refined Classi-fication (IIRC)"
setup, an extension to the class incremental learning setup where the incoming
batches of classes have two granularity levels. i.e., each sample could have a
high-level (coarse) label like "bear" and a low-level (fine) label like "polar
bear". Only one label is provided at a time, and the model has to figure out
the other label if it has already learnfed it. This setup is more aligned with
real-life scenarios, where a learner usually interacts with the same family of
entities multiple times, discovers more granularity about them, while still
trying not to forget previous knowledge. Moreover, this setup enables
evaluating models for some important lifelong learning challenges that cannot
be easily addressed under the existing setups. These challenges can be
motivated by the example "if a model was trained on the class bear in one task
and on polar bear in another task, will it forget the concept of bear, will it
rightfully infer that a polar bear is still a bear? and will it wrongfully
associate the label of polar bear to other breeds of bear?". We develop a
standardized benchmark that enables evaluating models on the IIRC setup. We
evaluate several state-of-the-art lifelong learning algorithms and highlight
their strengths and limitations. For example, distillation-based methods
perform relatively well but are prone to incorrectly predicting too many labels
per image. We hope that the proposed setup, along with the benchmark, would
provide a meaningful problem setting to the practitioners
</p>
<a href="http://arxiv.org/abs/2012.12477" target="_blank">arXiv:2012.12477</a> [<a href="http://arxiv.org/pdf/2012.12477" target="_blank">pdf</a>]

<h2>Localization in the Crowd with Topological Constraints. (arXiv:2012.12482v1 [cs.CV])</h2>
<h3>Shahira Abousamra, Minh Hoai, Dimitris Samaras, Chao Chen</h3>
<p>We address the problem of crowd localization, i.e., the prediction of dots
corresponding to people in a crowded scene. Due to various challenges, a
localization method is prone to spatial semantic errors, i.e., predicting
multiple dots within a same person or collapsing multiple dots in a cluttered
region. We propose a topological approach targeting these semantic errors. We
introduce a topological constraint that teaches the model to reason about the
spatial arrangement of dots. To enforce this constraint, we define a
persistence loss based on the theory of persistent homology. The loss compares
the topographic landscape of the likelihood map and the topology of the ground
truth. Topological reasoning improves the quality of the localization algorithm
especially near cluttered regions. On multiple public benchmarks, our method
outperforms previous localization methods. Additionally, we demonstrate the
potential of our method in improving the performance in the crowd counting
task.
</p>
<a href="http://arxiv.org/abs/2012.12482" target="_blank">arXiv:2012.12482</a> [<a href="http://arxiv.org/pdf/2012.12482" target="_blank">pdf</a>]

<h2>Global Models for Time Series Forecasting: A Simulation Study. (arXiv:2012.12485v1 [cs.LG])</h2>
<h3>Hansika Hewamalage, Christoph Bergmeir, Kasun Bandara</h3>
<p>In the current context of Big Data, the nature of many forecasting problems
has changed from predicting isolated time series to predicting many time series
from similar sources. This has opened up the opportunity to develop competitive
global forecasting models that simultaneously learn from many time series. But,
it still remains unclear when global forecasting models can outperform the
univariate benchmarks, especially along the dimensions of the
homogeneity/heterogeneity of series, the complexity of patterns in the series,
the complexity of forecasting models, and the lengths/number of series. Our
study attempts to address this problem through investigating the effect from
these factors, by simulating a number of datasets that have controllable time
series characteristics. Specifically, we simulate time series from simple data
generating processes (DGP), such as Auto Regressive (AR) and Seasonal AR, to
complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold
Auto-Regressive, and Mackey-Glass Equations. The data heterogeneity is
introduced by mixing time series generated from several DGPs into a single
dataset. The lengths and the number of series in the dataset are varied in
different scenarios. We perform experiments on these datasets using global
forecasting models including Recurrent Neural Networks (RNN), Feed-Forward
Neural Networks, Pooled Regression (PR) models and Light Gradient Boosting
Models (LGBM), and compare their performance against standard statistical
univariate forecasting techniques. Our experiments demonstrate that when
trained as global forecasting models, techniques such as RNNs and LGBMs, which
have complex non-linear modelling capabilities, are competitive methods in
general under challenging forecasting scenarios such as series having short
lengths, datasets with heterogeneous series and having minimal prior knowledge
of the patterns of the series.
</p>
<a href="http://arxiv.org/abs/2012.12485" target="_blank">arXiv:2012.12485</a> [<a href="http://arxiv.org/pdf/2012.12485" target="_blank">pdf</a>]

<h2>Active Sampling for Accelerated MRI with Low-Rank Tensors. (arXiv:2012.12496v1 [cs.CV])</h2>
<h3>Zichang He, Bo Zhao, Zheng Zhang</h3>
<p>Magnetic resonance imaging (MRI) is a powerful imaging modality that
revolutionizes medicine and biology. The imaging speed of high-dimensional MRI
is often limited, which constrains its practical utility. Recently, low-rank
tensor models have been exploited to enable fast MR imaging with sparse
sampling. Most existing methods use some pre-defined sampling design, and
active sensing has not been explored for low-rank tensor imaging. In this
paper, we introduce an active low-rank tensor model for fast MR imaging.We
propose an active sampling method based on a Query-by-Committee model, making
use of the benefits of low-rank tensor structure. Numerical experiments on a
3-D MRI data set demonstrate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.12496" target="_blank">arXiv:2012.12496</a> [<a href="http://arxiv.org/pdf/2012.12496" target="_blank">pdf</a>]

<h2>Small-Group Learning, with Application to Neural Architecture Search. (arXiv:2012.12502v1 [cs.LG])</h2>
<h3>Xuefeng Du, Pengtao Xie</h3>
<p>Small-group learning is a broadly used methodology in human learning and
shows great effectiveness in improving learning outcomes: a small group of
students work together towards the same learning objective, where they express
their understanding of a topic to their peers, compare their ideas, and help
each other to trouble-shoot problems. We are interested in investigating
whether this powerful learning technique can be borrowed from humans to improve
the learning abilities of machines. We propose a novel learning approach called
small-group learning (SGL). In our approach, each learner uses its
intermediately trained model to generate a pseudo-labeled dataset and re-trains
its model using pseudo-labeled datasets generated by other learners. We propose
a multi-level optimization framework to formulate SGL which involves three
learning stages: learners train their network weights independently; learners
train their network weights collaboratively via mutual pseudo-labeling;
learners improve their architectures by minimizing validation losses. We
develop an efficient algorithm to solve the SGL problem. We apply our approach
to neural architecture search and achieve significant improvement on CIFAR-100,
CIFAR-10, and ImageNet.
</p>
<a href="http://arxiv.org/abs/2012.12502" target="_blank">arXiv:2012.12502</a> [<a href="http://arxiv.org/pdf/2012.12502" target="_blank">pdf</a>]

<h2>Blur More To Deblur Better: Multi-Blur2Deblur For Efficient Video Deblurring. (arXiv:2012.12507v1 [cs.CV])</h2>
<h3>Dongwon Park, Dong Un Kang, Se Young Chun</h3>
<p>One of the key components for video deblurring is how to exploit neighboring
frames. Recent state-of-the-art methods either used aligned adjacent frames to
the center frame or propagated the information on past frames to the current
frame recurrently. Here we propose multi-blur-to-deblur (MB2D), a novel concept
to exploit neighboring frames for efficient video deblurring. Firstly, inspired
by unsharp masking, we argue that using more blurred images with long exposures
as additional inputs significantly improves performance. Secondly, we propose
multi-blurring recurrent neural network (MBRNN) that can synthesize more
blurred images from neighboring frames, yielding substantially improved
performance with existing video deblurring methods. Lastly, we propose
multi-scale deblurring with connecting recurrent feature map from MBRNN (MSDR)
to achieve state-of-the-art performance on the popular GoPro and Su datasets in
fast and memory efficient ways.
</p>
<a href="http://arxiv.org/abs/2012.12507" target="_blank">arXiv:2012.12507</a> [<a href="http://arxiv.org/pdf/2012.12507" target="_blank">pdf</a>]

<h2>Deep Semantic Dictionary Learning for Multi-label Image Classification. (arXiv:2012.12509v1 [cs.CV])</h2>
<h3>Fengtao Zhou, Sheng Huang, Yun Xing</h3>
<p>Compared with single-label image classification, multi-label image
classification is more practical and challenging. Some recent studies attempted
to leverage the semantic information of categories for improving multi-label
image classification performance. However, these semantic-based methods only
take semantic information as type of complements for visual representation
without further exploitation. In this paper, we present a innovative path
towards the solution of the multi-label image classification which considers it
as a dictionary learning task. A novel end-to-end model named Deep Semantic
Dictionary Learning (DSDL) is designed. In DSDL, an auto-encoder is applied to
generate the semantic dictionary from class-level semantics and then such
dictionary is utilized for representing the visual features extracted by
Convolutional Neural Network (CNN) with label embeddings. The DSDL provides a
simple but elegant way to exploit and reconcile the label, semantic and visual
spaces simultaneously via conducting the dictionary learning among them.
Moreover, inspired by iterative optimization of traditional dictionary
learning, we further devise a novel training strategy named Alternately
Parameters Update Strategy (APUS) for optimizing DSDL, which alteratively
optimizes the representation coefficients and the semantic dictionary in
forward and backward propagation. Extensive experimental results on three
popular benchmarks demonstrate that our method achieves promising performances
in comparison with the state-of-the-arts. Our codes and models are available at
https://github.com/ZFT-CQU/DSDL.
</p>
<a href="http://arxiv.org/abs/2012.12509" target="_blank">arXiv:2012.12509</a> [<a href="http://arxiv.org/pdf/2012.12509" target="_blank">pdf</a>]

<h2>Towards Overcoming False Positives in Visual Relationship Detection. (arXiv:2012.12510v1 [cs.CV])</h2>
<h3>Daisheng Jin, Xiao Ma, Chongzhi Zhang, Yizhuo Zhou, Jiashu Tao, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Zhoujun Li, Xianglong Li, Hongsheng Li</h3>
<p>In this paper, we investigate the cause of the high false positive rate in
Visual Relationship Detection (VRD). We observe that during training, the
relationship proposal distribution is highly imbalanced: most of the negative
relationship proposals are easy to identify, e.g., the inaccurate object
detection, which leads to the under-fitting of low-frequency difficult
proposals. This paper presents Spatially-Aware Balanced negative pRoposal
sAmpling (SABRA), a robust VRD framework that alleviates the influence of false
positives. To effectively optimize the model under imbalanced distribution,
SABRA adopts Balanced Negative Proposal Sampling (BNPS) strategy for mini-batch
sampling. BNPS divides proposals into 5 well defined sub-classes and generates
a balanced training distribution according to the inverse frequency. BNPS gives
an easier optimization landscape and significantly reduces the number of false
positives. To further resolve the low-frequency challenging false positive
proposals with high spatial ambiguity, we improve the spatial modeling ability
of SABRA on two aspects: a simple and efficient multi-head heterogeneous graph
attention network (MH-GAT) that models the global spatial interactions of
objects, and a spatial mask decoder that learns the local spatial
configuration. SABRA outperforms SOTA methods by a large margin on two
human-object interaction (HOI) datasets and one general VRD dataset.
</p>
<a href="http://arxiv.org/abs/2012.12510" target="_blank">arXiv:2012.12510</a> [<a href="http://arxiv.org/pdf/2012.12510" target="_blank">pdf</a>]

<h2>Analyzing Representations inside Convolutional Neural Networks. (arXiv:2012.12516v1 [cs.LG])</h2>
<h3>Uday Singh Saini, Evangelos E. Papalexakis</h3>
<p>How can we discover and succinctly summarize the concepts that a neural
network has learned? Such a task is of great importance in applications of
networks in areas of inference that involve classification, like medical
diagnosis based on fMRI/x-ray etc. In this work, we propose a framework to
categorize the concepts a network learns based on the way it clusters a set of
input examples, clusters neurons based on the examples they activate for, and
input features all in the same latent space. This framework is unsupervised and
can work without any labels for input features, it only needs access to
internal activations of the network for each input example, thereby making it
widely applicable. We extensively evaluate the proposed method and demonstrate
that it produces human-understandable and coherent concepts that a ResNet-18
has learned on the CIFAR-100 dataset.
</p>
<a href="http://arxiv.org/abs/2012.12516" target="_blank">arXiv:2012.12516</a> [<a href="http://arxiv.org/pdf/2012.12516" target="_blank">pdf</a>]

<h2>Vehicle Re-identification Based on Dual Distance Center Loss. (arXiv:2012.12519v1 [cs.CV])</h2>
<h3>Zhijun Hu, Yong Xu, Jie Wen, Lilei Sun, Raja S P</h3>
<p>Recently, deep learning has been widely used in the field of vehicle
re-identification. When training a deep model, softmax loss is usually used as
a supervision tool. However, the softmax loss performs well for closed-set
tasks, but not very well for open-set tasks. In this paper, we sum up five
shortcomings of center loss and solved all of them by proposing a dual distance
center loss (DDCL). Especially we solve the shortcoming that center loss must
combine with the softmax loss to supervise training the model, which provides
us with a new perspective to examine the center loss. In addition, we verify
the inconsistency between the proposed DDCL and softmax loss in the feature
space, which makes the center loss no longer be limited by the softmax loss in
the feature space after removing the softmax loss. To be specifically, we add
the Pearson distance on the basis of the Euclidean distance to the same center,
which makes all features of the same class be confined to the intersection of a
hypersphere and a hypercube in the feature space. The proposed Pearson distance
strengthens the intra-class compactness of the center loss and enhances the
generalization ability of center loss. Moreover, by designing a Euclidean
distance threshold between all center pairs, which not only strengthens the
inter-class separability of center loss, but also makes the center loss (or
DDCL) works well without the combination of softmax loss. We apply DDCL in the
field of vehicle re-identification named VeRi-776 dataset and VehicleID
dataset. And in order to verify its good generalization ability, we also verify
it in two datasets commonly used in the field of person re-identification named
MSMT17 dataset and Market1501 dataset.
</p>
<a href="http://arxiv.org/abs/2012.12519" target="_blank">arXiv:2012.12519</a> [<a href="http://arxiv.org/pdf/2012.12519" target="_blank">pdf</a>]

<h2>Robot Operating System Compatible Mobile Robots for Education and Research. (arXiv:2012.12527v1 [cs.RO])</h2>
<h3>Alim Kerem Erdogmu&#x15f;, Didem Ozupek Tas, Mustafa Karaca, Ugur Yayan</h3>
<p>The use of mobile robots has inevitably increased in recent years. The
increase in the companies that produce products in this field, the popularity
of the studies in the robotic field and the technological competence to serve
many different areas have revealed this increase in usage. The importance of
mobile robots used as health, education, pro-duction, logistics, defense
industry and space equipment is now more important than before. The fact that
robotic education can be reduced to a very young age, the production and coding
of simple robots with easily accessible parts is also an important factor in
this field. At this point, the effect of educational robots on the spread of
robotic technol-ogy cannot be denied.
</p>
<a href="http://arxiv.org/abs/2012.12527" target="_blank">arXiv:2012.12527</a> [<a href="http://arxiv.org/pdf/2012.12527" target="_blank">pdf</a>]

<h2>The Translucent Patch: A Physical and Universal Attack on Object Detectors. (arXiv:2012.12528v1 [cs.CV])</h2>
<h3>Alon Zolfi, Moshe Kravchik, Yuval Elovici, Asaf Shabtai</h3>
<p>Physical adversarial attacks against object detectors have seen increasing
success in recent years. However, these attacks require direct access to the
object of interest in order to apply a physical patch. Furthermore, to hide
multiple objects, an adversarial patch must be applied to each object. In this
paper, we propose a contactless translucent physical patch containing a
carefully constructed pattern, which is placed on the camera's lens, to fool
state-of-the-art object detectors. The primary goal of our patch is to hide all
instances of a selected target class. In addition, the optimization method used
to construct the patch aims to ensure that the detection of other (untargeted)
classes remains unharmed. Therefore, in our experiments, which are conducted on
state-of-the-art object detection models used in autonomous driving, we study
the effect of the patch on the detection of both the selected target class and
the other classes. We show that our patch was able to prevent the detection of
42.27% of all stop sign instances while maintaining high (nearly 80%) detection
of the other classes.
</p>
<a href="http://arxiv.org/abs/2012.12528" target="_blank">arXiv:2012.12528</a> [<a href="http://arxiv.org/pdf/2012.12528" target="_blank">pdf</a>]

<h2>Motif-Driven Contrastive Learning of Graph Representations. (arXiv:2012.12533v1 [cs.LG])</h2>
<h3>Shichang Zhang, Ziniu Hu, Arjun Subramonian, Yizhou Sun</h3>
<p>Graph motifs are significant subgraph patterns occurring frequently in
graphs, and they play important roles in representing the whole graph
characteristics. For example, in chemical domain, functional groups are motifs
that can determine molecule properties. Mining and utilizing motifs, however,
is a non-trivial task for large graph datasets. Traditional motif discovery
approaches rely on exact counting or statistical estimation, which are hard to
scale for large datasets with continuous and high-dimension features. In light
of the significance and challenges of motif mining, we propose MICRO-Graph: a
framework for MotIf-driven Contrastive leaRning Of Graph representations to: 1)
pre-train Graph Neural Net-works (GNNs) in a self-supervised manner to
automatically extract motifs from large graph datasets; 2) leverage learned
motifs to guide the contrastive learning of graph representations, which
further benefit various downstream tasks. Specifically, given a graph dataset,
a motif learner cluster similar and significant subgraphs into corresponding
motif slots. Based on the learned motifs, a motif-guided subgraph segmenter is
trained to generate more informative subgraphs, which are used to conduct
graph-to-subgraph contrastive learning of GNNs. By pre-training on ogbg-molhiv
molecule dataset with our proposed MICRO-Graph, the pre-trained GNN model can
enhance various chemical property prediction down-stream tasks with scarce
label by 2.0%, which is significantly higher than other state-of-the-art
self-supervised learning baselines.
</p>
<a href="http://arxiv.org/abs/2012.12533" target="_blank">arXiv:2012.12533</a> [<a href="http://arxiv.org/pdf/2012.12533" target="_blank">pdf</a>]

<h2>BENN: Bias Estimation Using Deep Neural Network. (arXiv:2012.12537v1 [cs.LG])</h2>
<h3>Amit Giloni, Edita Grolman, Tanja Hagemann, Ronald Fromm, Sebastian Fischer, Yuval Elovici, Asaf Shabtai</h3>
<p>The need to detect bias in machine learning (ML) models has led to the
development of multiple bias detection methods, yet utilizing them is
challenging since each method: i) explores a different ethical aspect of bias,
which may result in contradictory output among the different methods, ii)
provides an output of a different range/scale and therefore, can't be compared
with other methods, and iii) requires different input, and therefore a human
expert needs to be involved to adjust each method according to the examined
model. In this paper, we present BENN -- a novel bias estimation method that
uses a pretrained unsupervised deep neural network. Given a ML model and data
samples, BENN provides a bias estimation for every feature based on the model's
predictions. We evaluated BENN using three benchmark datasets and one
proprietary churn prediction model used by a European Telco and compared it
with an ensemble of 21 existing bias estimation methods. Evaluation results
highlight the significant advantages of BENN over the ensemble, as it is
generic (i.e., can be applied to any ML model) and there is no need for a
domain expert, yet it provides bias estimations that are aligned with those of
the ensemble.
</p>
<a href="http://arxiv.org/abs/2012.12537" target="_blank">arXiv:2012.12537</a> [<a href="http://arxiv.org/pdf/2012.12537" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation for Semantic Segmentation by Content Transfer. (arXiv:2012.12545v1 [cs.CV])</h2>
<h3>Suhyeon Lee, Junhyuk Hyun, Hongje Seong, Euntai Kim</h3>
<p>In this paper, we tackle the unsupervised domain adaptation (UDA) for
semantic segmentation, which aims to segment the unlabeled real data using
labeled synthetic data. The main problem of UDA for semantic segmentation
relies on reducing the domain gap between the real image and synthetic image.
To solve this problem, we focused on separating information in an image into
content and style. Here, only the content has cues for semantic segmentation,
and the style makes the domain gap. Thus, precise separation of content and
style in an image leads to effect as supervision of real data even when
learning with synthetic data. To make the best of this effect, we propose a
zero-style loss. Even though we perfectly extract content for semantic
segmentation in the real domain, another main challenge, the class imbalance
problem, still exists in UDA for semantic segmentation. We address this problem
by transferring the contents of tail classes from synthetic to real domain.
Experimental results show that the proposed method achieves the
state-of-the-art performance in semantic segmentation on the major two UDA
settings.
</p>
<a href="http://arxiv.org/abs/2012.12545" target="_blank">arXiv:2012.12545</a> [<a href="http://arxiv.org/pdf/2012.12545" target="_blank">pdf</a>]

<h2>Efficient video annotation with visual interpolation and frame selection guidance. (arXiv:2012.12554v1 [cs.CV])</h2>
<h3>A. Kuznetsova, A. Talati, Y. Luo, K. Simmons, V. Ferrari</h3>
<p>We introduce a unified framework for generic video annotation with bounding
boxes. Video annotation is a longstanding problem, as it is a tedious and
time-consuming process. We tackle two important challenges of video annotation:
(1) automatic temporal interpolation and extrapolation of bounding boxes
provided by a human annotator on a subset of all frames, and (2) automatic
selection of frames to annotate manually. Our contribution is two-fold: first,
we propose a model that has both interpolating and extrapolating capabilities;
second, we propose a guiding mechanism that sequentially generates suggestions
for what frame to annotate next, based on the annotations made previously. We
extensively evaluate our approach on several challenging datasets in simulation
and demonstrate a reduction in terms of the number of manual bounding boxes
drawn by 60% over linear interpolation and by 35% over an off-the-shelf
tracker. Moreover, we also show 10% annotation time improvement over a
state-of-the-art method for video annotation with bounding boxes [25]. Finally,
we run human annotation experiments and provide extensive analysis of the
results, showing that our approach reduces actual measured annotation time by
50% compared to commonly used linear interpolation.
</p>
<a href="http://arxiv.org/abs/2012.12554" target="_blank">arXiv:2012.12554</a> [<a href="http://arxiv.org/pdf/2012.12554" target="_blank">pdf</a>]

<h2>A Survey on Visual Transformer. (arXiv:2012.12556v1 [cs.CV])</h2>
<h3>Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao</h3>
<p>Transformer is a type of deep neural network mainly based on self-attention
mechanism which is originally applied in natural language processing field.
Inspired by the strong representation ability of transformer, researchers
propose to extend transformer for computer vision tasks. Transformer-based
models show competitive and even better performance on various visual
benchmarks compared to other network types such as convolutional networks and
recurrent networks. In this paper we provide a literature review of these
visual transformer models by categorizing them in different tasks and analyze
the advantages and disadvantages of these methods. In particular, the main
categories include the basic image classification, high-level vision, low-level
vision and video processing. Self-attention in computer vision is also briefly
revisited as self-attention is the base component in transformer. Efficient
transformer methods are included for pushing transformer into real
applications. Finally, we give a discussion about the further research
directions for visual transformer.
</p>
<a href="http://arxiv.org/abs/2012.12556" target="_blank">arXiv:2012.12556</a> [<a href="http://arxiv.org/pdf/2012.12556" target="_blank">pdf</a>]

<h2>Multi-grained Trajectory Graph Convolutional Networks for Habit-unrelated Human Motion Prediction. (arXiv:2012.12558v1 [cs.CV])</h2>
<h3>Jin Liu, Jianqin Yin</h3>
<p>Human motion prediction is an essential part for human-robot collaboration.
Unlike most of the existing methods mainly focusing on improving the
effectiveness of spatiotemporal modeling for accurate prediction, we take
effectiveness and efficiency into consideration, aiming at the prediction
quality, computational efficiency and the lightweight of the model. A
multi-grained trajectory graph convolutional networks based and lightweight
framework is proposed for habit-unrelated human motion prediction.
Specifically, we represent human motion as multi-grained trajectories,
including joint trajectory and sub-joint trajectory. Based on the advanced
representation, multi-grained trajectory graph convolutional networks are
proposed to explore the spatiotemporal dependencies at the multiple
granularities. Moreover, considering the right-handedness habit of the vast
majority of people, a new motion generation method is proposed to generate the
motion with left-handedness, to better model the motion with less bias to the
human habit. Experimental results on challenging datasets, including Human3.6M
and CMU Mocap, show that the proposed model outperforms state-of-the-art with
less than 0.12 times parameters, which demonstrates the effectiveness and
efficiency of our proposed method.
</p>
<a href="http://arxiv.org/abs/2012.12558" target="_blank">arXiv:2012.12558</a> [<a href="http://arxiv.org/pdf/2012.12558" target="_blank">pdf</a>]

<h2>ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation. (arXiv:2012.12570v1 [cs.CV])</h2>
<h3>Guodong Zeng, Till D. Lerch, Florian Schmaranzer, Guoyan Zheng, Juergen Burger, Kate Gerber, Moritz Tannast, Klaus Siebenrock, Nicolas Gerber</h3>
<p>Unsupervised domain adaptation (UDA) for cross-modality medical image
segmentation has shown great progress by domain-invariant feature learning or
image appearance translation. Adapted feature learning usually cannot detect
domain shifts at the pixel level and is not able to achieve good results in
dense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN,
translates images into different styles with good appearance, despite its
population, its semantic consistency is hardly to maintain and results in poor
cross-modality segmentation. In this paper, we propose intra- and
cross-modality semantic consistency (ICMSC) for UDA and our key insight is that
the segmentation of synthesised images in different styles should be
consistent. Specifically, our model consists of an image translation module and
a domain-specific segmentation module. The image translation module is a
standard CycleGAN, while the segmentation module contains two domain-specific
segmentation networks. The intra-modality semantic consistency (IMSC) forces
the reconstructed image after a cycle to be segmented in the same way as the
original input image, while the cross-modality semantic consistency (CMSC)
encourages the synthesized images after translation to be segmented exactly the
same as before translation. Comprehensive experimental results on
cross-modality hip joint bone segmentation show the effectiveness of our
proposed method, which achieves an average DICE of 81.61% on the acetabulum and
88.16% on the proximal femur, outperforming other state-of-the-art methods. It
is worth to note that without UDA, a model trained on CT for hip joint bone
segmentation is non-transferable to MRI and has almost zero-DICE segmentation.
</p>
<a href="http://arxiv.org/abs/2012.12570" target="_blank">arXiv:2012.12570</a> [<a href="http://arxiv.org/pdf/2012.12570" target="_blank">pdf</a>]

<h2>IFGAN: Missing Value Imputation using Feature-specific Generative Adversarial Networks. (arXiv:2012.12581v1 [cs.LG])</h2>
<h3>Wei Qiu, Yangsibo Huang, Quanzheng Li</h3>
<p>Missing value imputation is a challenging and well-researched topic in data
mining. In this paper, we propose IFGAN, a missing value imputation algorithm
based on Feature-specific Generative Adversarial Networks (GAN). Our idea is
intuitive yet effective: a feature-specific generator is trained to impute
missing values, while a discriminator is expected to distinguish the imputed
values from observed ones. The proposed architecture is capable of handling
different data types, data distributions, missing mechanisms, and missing
rates. It also improves post-imputation analysis by preserving inter-feature
correlations. We empirically show on several real-life datasets that IFGAN
outperforms current state-of-the-art algorithm under various missing
conditions.
</p>
<a href="http://arxiv.org/abs/2012.12581" target="_blank">arXiv:2012.12581</a> [<a href="http://arxiv.org/pdf/2012.12581" target="_blank">pdf</a>]

<h2>Stability in Abstract Argumentation. (arXiv:2012.12588v1 [cs.AI])</h2>
<h3>Jean-Guy Mailly, Julien Rossit</h3>
<p>The notion of stability in a structured argumentation setup characterizes
situations where the acceptance status associated with a given literal will not
be impacted by any future evolution of this setup. In this paper, we abstract
away from the logical structure of arguments, and we transpose this notion of
stability to the context of Dungean argumentation frameworks. In particular, we
show how this problem can be translated into reasoning with Argument-Incomplete
AFs. Then we provide preliminary complexity results for stability under four
prominent semantics, in the case of both credulous and skeptical reasoning.
Finally, we illustrate to what extent this notion can be useful with an
application to argument-based negotiation.
</p>
<a href="http://arxiv.org/abs/2012.12588" target="_blank">arXiv:2012.12588</a> [<a href="http://arxiv.org/pdf/2012.12588" target="_blank">pdf</a>]

<h2>Comparison of Privacy-Preserving Distributed Deep Learning Methods in Healthcare. (arXiv:2012.12591v1 [cs.LG])</h2>
<h3>Manish Gawali, Arvind C S, Shriya Suryavanshi, Harshit Madaan, Ashrika Gaikwad, Bhanu Prakash KN, Viraj Kulkarni, Aniruddha Pant</h3>
<p>In this paper, we compare three privacy-preserving distributed learning
techniques: federated learning, split learning, and SplitFed. We use these
techniques to develop binary classification models for detecting tuberculosis
from chest X-rays and compare them in terms of classification performance,
communication and computational costs, and training time. We propose a novel
distributed learning architecture called SplitFedv3, which performs better than
split learning and SplitFedv2 in our experiments. We also propose alternate
mini-batch training, a new training technique for split learning, that performs
better than alternate client training, where clients take turns to train a
model.
</p>
<a href="http://arxiv.org/abs/2012.12591" target="_blank">arXiv:2012.12591</a> [<a href="http://arxiv.org/pdf/2012.12591" target="_blank">pdf</a>]

<h2>AutonoML: Towards an Integrated Framework for Autonomous Machine Learning. (arXiv:2012.12600v1 [cs.LG])</h2>
<h3>David Jacob Kedziora, Katarzyna Musial, Bogdan Gabrys</h3>
<p>Over the last decade, the long-running endeavour to automate high-level
processes in machine learning (ML) has risen to mainstream prominence,
stimulated by advances in optimisation techniques and their impact on selecting
ML models/algorithms. Central to this drive is the appeal of engineering a
computational system that both discovers and deploys high-performance solutions
to arbitrary ML problems with minimal human interaction. Beyond this, an even
loftier goal is the pursuit of autonomy, which describes the capability of the
system to independently adjust an ML solution over a lifetime of changing
contexts. However, these ambitions are unlikely to be achieved in a robust
manner without the broader synthesis of various mechanisms and theoretical
frameworks, which, at the present time, remain scattered across numerous
research threads. Accordingly, this review seeks to motivate a more expansive
perspective on what constitutes an automated/autonomous ML system, alongside
consideration of how best to consolidate those elements. In doing so, we survey
developments in the following research areas: hyperparameter optimisation,
multi-component models, neural architecture search, automated feature
engineering, meta-learning, multi-level ensembling, dynamic adaptation,
multi-objective evaluation, resource constraints, flexible user involvement,
and the principles of generalisation. We also develop a conceptual framework
throughout the review, augmented by each topic, to illustrate one possible way
of fusing high-level mechanisms into an autonomous ML system. Ultimately, we
conclude that the notion of architectural integration deserves more discussion,
without which the field of automated ML risks stifling both its technical
advantages and general uptake.
</p>
<a href="http://arxiv.org/abs/2012.12600" target="_blank">arXiv:2012.12600</a> [<a href="http://arxiv.org/pdf/2012.12600" target="_blank">pdf</a>]

<h2>ConvMath: A Convolutional Sequence Network for Mathematical Expression Recognition. (arXiv:2012.12619v1 [cs.CV])</h2>
<h3>Zuoyu Yan, Xiaode Zhang, Liangcai Gao, Ke Yuan, Zhi Tang</h3>
<p>Despite the recent advances in optical character recognition (OCR),
mathematical expressions still face a great challenge to recognize due to their
two-dimensional graphical layout. In this paper, we propose a convolutional
sequence modeling network, ConvMath, which converts the mathematical expression
description in an image into a LaTeX sequence in an end-to-end way. The network
combines an image encoder for feature extraction and a convolutional decoder
for sequence generation. Compared with other Long Short Term Memory(LSTM) based
encoder-decoder models, ConvMath is entirely based on convolution, thus it is
easy to perform parallel computation. Besides, the network adopts multi-layer
attention mechanism in the decoder, which allows the model to align output
symbols with source feature vectors automatically, and alleviates the problem
of lacking coverage while training the model. The performance of ConvMath is
evaluated on an open dataset named IM2LATEX-100K, including 103556 samples. The
experimental results demonstrate that the proposed network achieves
state-of-the-art accuracy and much better efficiency than previous methods.
</p>
<a href="http://arxiv.org/abs/2012.12619" target="_blank">arXiv:2012.12619</a> [<a href="http://arxiv.org/pdf/2012.12619" target="_blank">pdf</a>]

<h2>Commission Fee is not Enough: A Hierarchical Reinforced Framework for Portfolio Management. (arXiv:2012.12620v1 [cs.AI])</h2>
<h3>Rundong Wang, Hongxin Wei, Bo An, Zhouyan Feng, Jun Yao</h3>
<p>Portfolio management via reinforcement learning is at the forefront of
fintech research, which explores how to optimally reallocate a fund into
different financial assets over the long term by trial-and-error. Existing
methods are impractical since they usually assume each reallocation can be
finished immediately and thus ignoring the price slippage as part of the
trading cost. To address these issues, we propose a hierarchical reinforced
stock trading system for portfolio management (HRPM). Concretely, we decompose
the trading process into a hierarchy of portfolio management over trade
execution and train the corresponding policies. The high-level policy gives
portfolio weights at a lower frequency to maximize the long term profit and
invokes the low-level policy to sell or buy the corresponding shares within a
short time window at a higher frequency to minimize the trading cost. We train
two levels of policies via pre-training scheme and iterative training scheme
for data efficiency. Extensive experimental results in the U.S. market and the
China market demonstrate that HRPM achieves significant improvement against
many state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.12620" target="_blank">arXiv:2012.12620</a> [<a href="http://arxiv.org/pdf/2012.12620" target="_blank">pdf</a>]

<h2>Direct Estimation of Spinal Cobb Angles by Structured Multi-Output Regression. (arXiv:2012.12626v1 [cs.CV])</h2>
<h3>Haoliang Sun, Xiantong Zhen, Chris Bailey, Parham Rasoulinejad, Yilong Yin, Shuo Li</h3>
<p>The Cobb angle that quantitatively evaluates the spinal curvature plays an
important role in the scoliosis diagnosis and treatment. Conventional
measurement of these angles suffers from huge variability and low reliability
due to intensive manual intervention. However, since there exist high ambiguity
and variability around boundaries of vertebrae, it is challenging to obtain
Cobb angles automatically. In this paper, we formulate the estimation of the
Cobb angles from spinal X-rays as a multi-output regression task. We propose
structured support vector regression (S^2VR) to jointly estimate Cobb angles
and landmarks of the spine in X-rays in one single framework. The proposed
S^2VR can faithfully handle the nonlinear relationship between input images and
quantitative outputs, while explicitly capturing the intrinsic correlation of
outputs. We introduce the manifold regularization to exploit the geometry of
the output space. We propose learning the kernel in S2VR by kernel target
alignment to enhance its discriminative ability. The proposed method is
evaluated on the spinal X-rays dataset of 439 scoliosis subjects, which
achieves the inspiring correlation coefficient of 92.76% with ground truth
obtained manually by human experts and outperforms two baseline methods. Our
method achieves the direct estimation of Cobb angles with high accuracy, which
indicates its great potential in clinical use.
</p>
<a href="http://arxiv.org/abs/2012.12626" target="_blank">arXiv:2012.12626</a> [<a href="http://arxiv.org/pdf/2012.12626" target="_blank">pdf</a>]

<h2>Efficient Continual Learning with Modular Networks and Task-Driven Priors. (arXiv:2012.12631v1 [cs.LG])</h2>
<h3>Tom Veniat, Ludovic Denoyer, Marc&#x27;Aurelio Ranzato</h3>
<p>Existing literature in Continual Learning (CL) has focused on overcoming
catastrophic forgetting, the inability of the learner to recall how to perform
tasks observed in the past. There are however other desirable properties of a
CL system, such as the ability to transfer knowledge from previous tasks and to
scale memory and compute sub-linearly with the number of tasks. Since most
current benchmarks focus only on forgetting using short streams of tasks, we
first propose a new suite of benchmarks to probe CL algorithms across these new
axes. Finally, we introduce a new modular architecture, whose modules represent
atomic skills that can be composed to perform a certain task. Learning a task
reduces to figuring out which past modules to re-use, and which new modules to
instantiate to solve the current task. Our learning algorithm leverages a
task-driven prior over the exponential search space of all possible ways to
combine modules, enabling efficient learning on long streams of tasks. Our
experiments show that this modular architecture and learning algorithm perform
competitively on widely used CL benchmarks while yielding superior performance
on the more challenging benchmarks we introduce in this work.
</p>
<a href="http://arxiv.org/abs/2012.12631" target="_blank">arXiv:2012.12631</a> [<a href="http://arxiv.org/pdf/2012.12631" target="_blank">pdf</a>]

<h2>Overview of FPGA deep learning acceleration based on convolutional neural network. (arXiv:2012.12634v1 [cs.AI])</h2>
<h3>Simin Liu</h3>
<p>In recent years, deep learning has become more and more mature, and as a
commonly used algorithm in deep learning, convolutional neural networks have
been widely used in various visual tasks. In the past, research based on deep
learning algorithms mainly relied on hardware such as GPUs and CPUs. However,
with the increasing development of FPGAs, both field programmable logic gate
arrays, it has become the main implementation hardware platform that combines
various neural network deep learning algorithms This article is a review
article, which mainly introduces the related theories and algorithms of
convolution. It summarizes the application scenarios of several existing FPGA
technologies based on convolutional neural networks, and mainly introduces the
application of accelerators. At the same time, it summarizes some accelerators'
under-utilization of logic resources or under-utilization of memory bandwidth,
so that they can't get the best performance.
</p>
<a href="http://arxiv.org/abs/2012.12634" target="_blank">arXiv:2012.12634</a> [<a href="http://arxiv.org/pdf/2012.12634" target="_blank">pdf</a>]

<h2>Gradient-Free Adversarial Attacks for Bayesian Neural Networks. (arXiv:2012.12640v1 [cs.LG])</h2>
<h3>Matthew Yuan, Matthew Wicker, Luca Laurenti</h3>
<p>The existence of adversarial examples underscores the importance of
understanding the robustness of machine learning models. Bayesian neural
networks (BNNs), due to their calibrated uncertainty, have been shown to posses
favorable adversarial robustness properties. However, when approximate Bayesian
inference methods are employed, the adversarial robustness of BNNs is still not
well understood. In this work, we employ gradient-free optimization methods in
order to find adversarial examples for BNNs. In particular, we consider genetic
algorithms, surrogate models, as well as zeroth order optimization methods and
adapt them to the goal of finding adversarial examples for BNNs. In an
empirical evaluation on the MNIST and Fashion MNIST datasets, we show that for
various approximate Bayesian inference methods the usage of gradient-free
algorithms can greatly improve the rate of finding adversarial examples
compared to state-of-the-art gradient-based methods.
</p>
<a href="http://arxiv.org/abs/2012.12640" target="_blank">arXiv:2012.12640</a> [<a href="http://arxiv.org/pdf/2012.12640" target="_blank">pdf</a>]

<h2>Autonomous Outdoor Scanning via Online Topological and Geometric Path Optimization. (arXiv:2012.12642v1 [cs.RO])</h2>
<h3>Pengdi Huang, Liqiang Lin, Kai Xu, Hui Huang</h3>
<p>Autonomous 3D acquisition of outdoor environments poses special challenges.
Different from indoor scenes, where the room space is delineated by clear
boundaries and separations (e.g., walls and furniture), an outdoor environment
is spacious and unbounded (thinking of a campus). Therefore, unlike for indoor
scenes where the scanning effort is mainly devoted to the discovery of boundary
surfaces, scanning an open and unbounded area requires actively delimiting the
extent of scanning region and dynamically planning a traverse path within that
region. Thus, for outdoor scenes, we formulate the planning of an
energy-efficient autonomous scanning through a discrete-continuous optimization
of robot scanning paths. The discrete optimization computes a topological map,
through solving an online traveling sales problem (Online TSP), which
determines the scanning goals and paths on-the-fly. The dynamic goals are
determined as a collection of visit sites with high reward of
visibility-to-unknown. A visit graph is constructed via connecting the visit
sites with edges weighted by traversing cost. This topological map evolves as
the robot scans via deleting outdated sites that are either visited or become
rewardless and inserting newly discovered ones. The continuous part optimizes
the traverse paths geometrically between two neighboring visit sites via
maximizing the information gain of scanning along the paths. The discrete and
continuous processes alternate until the traverse cost of the current graph
exceeds the remaining energy capacity of the robot. Our approach is evaluated
with both synthetic and field tests, demonstrating its effectiveness and
advantages over alternatives. The project is at
this http URL, and the codes are available at
https://github.com/alualu628628/Autonomous-Outdoor-Scanning-via-Online-Topological-and-Geometric-Path-Optimization.
</p>
<a href="http://arxiv.org/abs/2012.12642" target="_blank">arXiv:2012.12642</a> [<a href="http://arxiv.org/pdf/2012.12642" target="_blank">pdf</a>]

<h2>On Calibration of Scene-Text Recognition Models. (arXiv:2012.12643v1 [cs.CV])</h2>
<h3>Ron Slossberg, Oron Anschel, Amir Markovitz, Ron Litman, Aviad Aberdam, Shahar Tsiper, Shai Mazor, Jon Wu, R. Manmatha</h3>
<p>In this work, we study the problem of word-level confidence calibration for
scene-text recognition (STR). Although the topic of confidence calibration has
been an active research area for the last several decades, the case of
structured and sequence prediction calibration has been scarcely explored. We
analyze several recent STR methods and show that they are consistently
overconfident. We then focus on the calibration of STR models on the word
rather than the character level. In particular, we demonstrate that for
attention based decoders, calibration of individual character predictions
increases word-level calibration error compared to an uncalibrated model. In
addition, we apply existing calibration methodologies as well as new
sequence-based extensions to numerous STR models, demonstrating reduced
calibration error by up to a factor of nearly 7. Finally, we show consistently
improved accuracy results by applying our proposed sequence calibration method
as a preprocessing step to beam-search.
</p>
<a href="http://arxiv.org/abs/2012.12643" target="_blank">arXiv:2012.12643</a> [<a href="http://arxiv.org/pdf/2012.12643" target="_blank">pdf</a>]

<h2>SWA Object Detection. (arXiv:2012.12645v1 [cs.CV])</h2>
<h3>Haoyang Zhang, Ying Wang, Feras Dayoub, Niko S&#xfc;nderhauf</h3>
<p>Do you want to improve 1.0 AP for your object detector without any inference
cost and any change to your detector? Let us tell you such a recipe. It is
surprisingly simple: train your detector for an extra 12 epochs using cyclical
learning rates and then average these 12 checkpoints as your final detection
model. This potent recipe is inspired by Stochastic Weights Averaging (SWA),
which is proposed in arXiv:1803.0540 for improving generalization in deep
neural networks. We found it also very effective in object detection. In this
technique report, we systematically investigate the effects of applying SWA to
object detection as well as instance segmentation. Through extensive
experiments, we discover a good policy of performing SWA in object detection,
and we consistently achieve $\sim$1.0 AP improvement over various popular
detectors on the challenging COCO benchmark. We hope this work will make more
researchers in object detection know this technique and help them train better
object detectors. Code is available at:
https://github.com/hyz-xmaster/swa_object_detection .
</p>
<a href="http://arxiv.org/abs/2012.12645" target="_blank">arXiv:2012.12645</a> [<a href="http://arxiv.org/pdf/2012.12645" target="_blank">pdf</a>]

<h2>Second-Moment Loss: A Novel Regression Objective for Improved Uncertainties. (arXiv:2012.12687v1 [cs.LG])</h2>
<h3>Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja Fischer, Stefan Wrobel</h3>
<p>Quantification of uncertainty is one of the most promising approaches to
establish safe machine learning. Despite its importance, it is far from being
generally solved, especially for neural networks. One of the most commonly used
approaches so far is Monte Carlo dropout, which is computationally cheap and
easy to apply in practice. However, it can underestimate the uncertainty. We
propose a new objective, referred to as second-moment loss (SML), to address
this issue. While the full network is encouraged to model the mean, the dropout
networks are explicitly used to optimize the model variance. We analyze the
performance of the new objective on various toy and UCI regression datasets.
Comparing to the state-of-the-art of deep ensembles, SML leads to comparable
prediction accuracies and uncertainty estimates while only requiring a single
model. Under distribution shift, we observe moderate improvements. From a
safety perspective also the study of worst-case uncertainties is crucial. In
this regard we improve considerably. Finally, we show that SML can be
successfully applied to SqueezeDet, a modern object detection network. We
improve on its uncertainty-related scores while not deteriorating regression
quality. As a side result, we introduce an intuitive Wasserstein distance-based
uncertainty measure that is non-saturating and thus allows to resolve quality
differences between any two uncertainty estimates.
</p>
<a href="http://arxiv.org/abs/2012.12687" target="_blank">arXiv:2012.12687</a> [<a href="http://arxiv.org/pdf/2012.12687" target="_blank">pdf</a>]

<h2>Compliance Generation for Privacy Documents under GDPR: A Roadmap for Implementing Automation and Machine Learning. (arXiv:2012.12718v1 [cs.AI])</h2>
<h3>David Restrepo Amariles, Aurore Cl&#xe9;ment Troussel, Rajaa El Hamdani</h3>
<p>Most prominent research today addresses compliance with data protection laws
through consumer-centric and public-regulatory approaches. We shift this
perspective with the Privatech project to focus on corporations and law firms
as agents of compliance. To comply with data protection laws, data processors
must implement accountability measures to assess and document compliance in
relation to both privacy documents and privacy practices. In this paper, we
survey, on the one hand, current research on GDPR automation, and on the other
hand, the operational challenges corporations face to comply with GDPR, and
that may benefit from new forms of automation. We attempt to bridge the gap. We
provide a roadmap for compliance assessment and generation by identifying
compliance issues, breaking them down into tasks that can be addressed through
machine learning and automation, and providing notes about related developments
in the Privatech project.
</p>
<a href="http://arxiv.org/abs/2012.12718" target="_blank">arXiv:2012.12718</a> [<a href="http://arxiv.org/pdf/2012.12718" target="_blank">pdf</a>]

<h2>Identification of Unexpected Decisions in Partially Observable Monte-Carlo Planning: a Rule-Based Approach. (arXiv:2012.12732v1 [cs.AI])</h2>
<h3>Giulio Mazzi, Alberto Castellini, Alessandro Farinelli</h3>
<p>Partially Observable Monte-Carlo Planning (POMCP) is a powerful online
algorithm able to generate approximate policies for large Partially Observable
Markov Decision Processes. The online nature of this method supports
scalability by avoiding complete policy representation. The lack of an explicit
representation however hinders interpretability. In this work, we propose a
methodology based on Satisfiability Modulo Theory (SMT) for analyzing POMCP
policies by inspecting their traces, namely sequences of
belief-action-observation triplets generated by the algorithm. The proposed
method explores local properties of policy behavior to identify unexpected
decisions. We propose an iterative process of trace analysis consisting of
three main steps, i) the definition of a question by means of a parametric
logical formula describing (probabilistic) relationships between beliefs and
actions, ii) the generation of an answer by computing the parameters of the
logical formula that maximize the number of satisfied clauses (solving a
MAX-SMT problem), iii) the analysis of the generated logical formula and the
related decision boundaries for identifying unexpected decisions made by POMCP
with respect to the original question. We evaluate our approach on Tiger, a
standard benchmark for POMDPs, and a real-world problem related to mobile robot
navigation. Results show that the approach can exploit human knowledge on the
domain, outperforming state-of-the-art anomaly detection methods in identifying
unexpected decisions. An improvement of the Area Under Curve up to 47\% has
been achieved in our tests.
</p>
<a href="http://arxiv.org/abs/2012.12732" target="_blank">arXiv:2012.12732</a> [<a href="http://arxiv.org/pdf/2012.12732" target="_blank">pdf</a>]

<h2>Multi-Modality Cut and Paste for 3D Object Detection. (arXiv:2012.12741v1 [cs.CV])</h2>
<h3>Wenwei Zhang, Zhe Wang, Chen Change Loy</h3>
<p>Three-dimensional (3D) object detection is essential in autonomous driving.
There are observations that multi-modality methods based on both point cloud
and imagery features perform only marginally better or sometimes worse than
approaches that solely use single-modality point cloud. This paper investigates
the reason behind this counter-intuitive phenomenon through a careful
comparison between augmentation techniques used by single modality and
multi-modality methods. We found that existing augmentations practiced in
single-modality detection are equally useful for multi-modality detection. Then
we further present a new multi-modality augmentation approach, Multi-mOdality
Cut and pAste (MoCa). MoCa boosts detection performance by cutting point cloud
and imagery patches of ground-truth objects and pasting them into different
scenes in a consistent manner while avoiding collision between objects. We also
explore beneficial architecture design and optimization practices in
implementing a good multi-modality detector. Without using ensemble of
detectors, our multi-modality detector achieves new state-of-the-art
performance on nuScenes dataset and competitive performance on KITTI 3D
benchmark. Our method also wins the best PKL award in the 3rd nuScenes
detection challenge. Code and models will be released at
https://github.com/open-mmlab/mmdetection3d.
</p>
<a href="http://arxiv.org/abs/2012.12741" target="_blank">arXiv:2012.12741</a> [<a href="http://arxiv.org/pdf/2012.12741" target="_blank">pdf</a>]

<h2>Estimation of Driver's Gaze Region from Head Position and Orientation using Probabilistic Confidence Regions. (arXiv:2012.12754v1 [cs.CV])</h2>
<h3>Sumit Jha, Carlos Busso</h3>
<p>A smart vehicle should be able to understand human behavior and predict their
actions to avoid hazardous situations. Specific traits in human behavior can be
automatically predicted, which can help the vehicle make decisions, increasing
safety. One of the most important aspects pertaining to the driving task is the
driver's visual attention. Predicting the driver's visual attention can help a
vehicle understand the awareness state of the driver, providing important
contextual information. While estimating the exact gaze direction is difficult
in the car environment, a coarse estimation of the visual attention can be
obtained by tracking the position and orientation of the head. Since the
relation between head pose and gaze direction is not one-to-one, this paper
proposes a formulation based on probabilistic models to create salient regions
describing the visual attention of the driver. The area of the predicted region
is small when the model has high confidence on the prediction, which is
directly learned from the data. We use Gaussian process regression (GPR) to
implement the framework, comparing the performance with different regression
formulations such as linear regression and neural network based methods. We
evaluate these frameworks by studying the tradeoff between spatial resolution
and accuracy of the probability map using naturalistic recordings collected
with the UTDrive platform. We observe that the GPR method produces the best
result creating accurate predictions with localized salient regions. For
example, the 95% confidence region is defined by an area that covers 3.77%
region of a sphere surrounding the driver.
</p>
<a href="http://arxiv.org/abs/2012.12754" target="_blank">arXiv:2012.12754</a> [<a href="http://arxiv.org/pdf/2012.12754" target="_blank">pdf</a>]

<h2>Principled network extraction from images. (arXiv:2012.12758v1 [cs.CV])</h2>
<h3>Diego Baptista, Caterina De Bacco</h3>
<p>Images of natural systems may represent patterns of network-like structure,
which could reveal important information about the topological properties of
the underlying subject. However, the image itself does not automatically
provide a formal definition of a network in terms of sets of nodes and edges.
Instead, this information should be suitably extracted from the raw image data.
Motivated by this, we present a principled model to extract network topologies
from images that is scalable and efficient. We map this goal into solving a
routing optimization problem where the solution is a network that minimizes an
energy function which can be interpreted in terms of an operational and
infrastructural cost. Our method relies on recent results from optimal
transport theory and is a principled alternative to standard image-processing
techniques that are based on heuristics. We test our model on real images of
the retinal vascular system, slime mold and river networks and compare with
routines combining image-processing techniques. Results are tested in terms of
a similarity measure related to the amount of information preserved in the
extraction. We find that our model finds networks from retina vascular network
images that are more similar to hand-labeled ones, while also giving high
performance in extracting networks from images of rivers and slime mold for
which there is no ground truth available. While there is no unique method that
fits all the images the best, our approach performs consistently across
datasets, its algorithmic implementation is efficient and can be fully
automatized to be run on several datasets with little supervision.
</p>
<a href="http://arxiv.org/abs/2012.12758" target="_blank">arXiv:2012.12758</a> [<a href="http://arxiv.org/pdf/2012.12758" target="_blank">pdf</a>]

<h2>Matrix optimization based Euclidean embedding with outliers. (arXiv:2012.12772v1 [stat.ML])</h2>
<h3>Qian Zhang, Xinyuan Zhao, Chao Ding</h3>
<p>Euclidean embedding from noisy observations containing outlier errors is an
important and challenging problem in statistics and machine learning. Many
existing methods would struggle with outliers due to a lack of detection
ability. In this paper, we propose a matrix optimization based embedding model
that can produce reliable embeddings and identify the outliers jointly. We show
that the estimators obtained by the proposed method satisfy a non-asymptotic
risk bound, implying that the model provides a high accuracy estimator with
high probability when the order of the sample size is roughly the degree of
freedom up to a logarithmic factor. Moreover, we show that under some mild
conditions, the proposed model also can identify the outliers without any prior
information with high probability. Finally, numerical experiments demonstrate
that the matrix optimization-based model can produce configurations of high
quality and successfully identify outliers even for large networks.
</p>
<a href="http://arxiv.org/abs/2012.12772" target="_blank">arXiv:2012.12772</a> [<a href="http://arxiv.org/pdf/2012.12772" target="_blank">pdf</a>]

<h2>Adaptive Precision Training for Resource Constrained Devices. (arXiv:2012.12775v1 [cs.LG])</h2>
<h3>Tian Huang, Tao Luo, Joey Tianyi Zhou</h3>
<p>Learn in-situ is a growing trend for Edge AI. Training deep neural network
(DNN) on edge devices is challenging because both energy and memory are
constrained. Low precision training helps to reduce the energy cost of a single
training iteration, but that does not necessarily translate to energy savings
for the whole training process, because low precision could slows down the
convergence rate. One evidence is that most works for low precision training
keep an fp32 copy of the model during training, which in turn imposes memory
requirements on edge devices. In this work we propose Adaptive Precision
Training. It is able to save both total training energy cost and memory usage
at the same time. We use model of the same precision for both forward and
backward pass in order to reduce memory usage for training. Through evaluating
the progress of training, APT allocates layer-wise precision dynamically so
that the model learns quicker for longer time. APT provides an application
specific hyper-parameter for users to play trade-off between training energy
cost, memory usage and accuracy. Experiment shows that APT achieves more than
50% saving on training energy and memory usage with limited accuracy loss. 20%
more savings of training energy and memory usage can be achieved in return for
a 1% sacrifice in accuracy loss.
</p>
<a href="http://arxiv.org/abs/2012.12775" target="_blank">arXiv:2012.12775</a> [<a href="http://arxiv.org/pdf/2012.12775" target="_blank">pdf</a>]

<h2>Coarse-to-Fine Object Tracking Using Deep Features and Correlation Filters. (arXiv:2012.12784v1 [cs.CV])</h2>
<h3>Ahmed Zgaren, Wassim Bouachir, Riadh Ksantini</h3>
<p>During the last years, deep learning trackers achieved stimulating results
while bringing interesting ideas to solve the tracking problem. This progress
is mainly due to the use of learned deep features obtained by training deep
convolutional neural networks (CNNs) on large image databases. But since CNNs
were originally developed for image classification, appearance modeling
provided by their deep layers might be not enough discriminative for the
tracking task. In fact,such features represent high-level information, that is
more related to object category than to a specific instance of the object.
Motivated by this observation, and by the fact that discriminative correlation
filters(DCFs) may provide a complimentary low-level information, we presenta
novel tracking algorithm taking advantage of both approaches. We formulate the
tracking task as a two-stage procedure. First, we exploit the generalization
ability of deep features to coarsely estimate target translation, while
ensuring invariance to appearance change. Then, we capitalize on the
discriminative power of correlation filters to precisely localize the tracked
object. Furthermore, we designed an update control mechanism to learn
appearance change while avoiding model drift. We evaluated the proposed tracker
on object tracking benchmarks. Experimental results show the robustness of our
algorithm, which performs favorably against CNN and DCF-based trackers. Code is
available at: https://github.com/AhmedZgaren/Coarse-to-fine-Tracker
</p>
<a href="http://arxiv.org/abs/2012.12784" target="_blank">arXiv:2012.12784</a> [<a href="http://arxiv.org/pdf/2012.12784" target="_blank">pdf</a>]

<h2>Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. (arXiv:2012.12803v1 [cs.LG])</h2>
<h3>Vitaly Feldman, Audra McMillan, Kunal Talwar</h3>
<p>Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and
Thakurta [EFMRTT19] demonstrates that random shuffling of input data amplifies
differential privacy guarantees. Such amplification leads to substantially
stronger privacy guarantees for systems in which data is contributed
anonymously [BEMMRLRKTS17] and for the analysis of noisy stochastic gradient
descent. We show that an $\varepsilon_0$-locally differentially private
algorithm, under shuffling with $n$ users, amplifies to a
$(\Theta((1-e^{-\varepsilon_0})\sqrt{\frac{e^{\varepsilon_0}\log(1/\delta)}{n}}),
\delta)$-central differential privacy guarantee. This significantly improves
over previous work and achieves the asymptotically optimal dependence on
$\varepsilon_0$.

Our result is based on a new approach that is simpler than previous work and
extends to approximate differential privacy with nearly the same guarantees.
Our work also yields an empirical method to derive tighter bounds on the
central $\varepsilon$ and we show that it gets to within a small constant
factor of the correct bound. As a direct corollary of our analysis we derive a
simple and asymptotically optimal algorithm for discrete distribution
estimation in the shuffle model of privacy [CSUZZ19].
</p>
<a href="http://arxiv.org/abs/2012.12803" target="_blank">arXiv:2012.12803</a> [<a href="http://arxiv.org/pdf/2012.12803" target="_blank">pdf</a>]

<h2>Warping of Radar Data into Camera Image for Cross-Modal Supervision in Automotive Applications. (arXiv:2012.12809v1 [cs.CV])</h2>
<h3>Christopher Grimm, Tai Fei, Ernst Warsitz, Ridha Farhoud, Tobias Breddermann, Reinhold Haeb-Umbach</h3>
<p>In this paper, we present a novel framework to project automotive radar
range-Doppler (RD) spectrum into camera image. The utilized warping operation
is designed to be fully differentiable, which allows error backpropagation
through the operation. This enables the training of neural networks (NN)
operating exclusively on RD spectrum by utilizing labels provided from camera
vision models. As the warping operation relies on accurate scene flow,
additionally, we present a novel scene flow estimation algorithm fed from
camera, lidar and radar, enabling us to improve the accuracy of the warping
operation. We demonstrate the framework in multiple applications like
direction-of-arrival (DoA) estimation, target detection, semantic segmentation
and estimation of radar power from camera data. Extensive evaluations have been
carried out for the DoA application and suggest superior quality for NN based
estimators compared to classical estimators. The novel scene flow estimation
approach is benchmarked against state-of-the-art scene flow algorithms and
outperforms them by roughly a third.
</p>
<a href="http://arxiv.org/abs/2012.12809" target="_blank">arXiv:2012.12809</a> [<a href="http://arxiv.org/pdf/2012.12809" target="_blank">pdf</a>]

<h2>Focal Frequency Loss for Generative Models. (arXiv:2012.12821v1 [cs.CV])</h2>
<h3>Liming Jiang, Bo Dai, Wayne Wu, Chen Change Loy</h3>
<p>Despite the remarkable success of generative models in creating
photorealistic images using deep neural networks, gaps could still exist
between the real and generated images, especially in the frequency domain. In
this study, we find that narrowing the frequency domain gap can ameliorate the
image synthesis quality further. To this end, we propose the focal frequency
loss, a novel objective function that brings optimization of generative models
into the frequency domain. The proposed loss allows the model to dynamically
focus on the frequency components that are hard to synthesize by down-weighting
the easy frequencies. This objective function is complementary to existing
spatial losses, offering great impedance against the loss of important
frequency information due to the inherent crux of neural networks. We
demonstrate the versatility and effectiveness of focal frequency loss to
improve various baselines in both perceptual quality and quantitative
performance.
</p>
<a href="http://arxiv.org/abs/2012.12821" target="_blank">arXiv:2012.12821</a> [<a href="http://arxiv.org/pdf/2012.12821" target="_blank">pdf</a>]

<h2>EQ-Net: A Unified Deep Learning Framework for Log-Likelihood Ratio Estimation and Quantization. (arXiv:2012.12843v1 [cs.LG])</h2>
<h3>Marius Arvinte, Ahmed H. Tewfik, Sriram Vishwanath</h3>
<p>In this work, we introduce EQ-Net: the first holistic framework that solves
both the tasks of log-likelihood ratio (LLR) estimation and quantization using
a data-driven method. We motivate our approach with theoretical insights on two
practical estimation algorithms at the ends of the complexity spectrum and
reveal a connection between the complexity of an algorithm and the information
bottleneck method: simpler algorithms admit smaller bottlenecks when
representing their solution. This motivates us to propose a two-stage algorithm
that uses LLR compression as a pretext task for estimation and is focused on
low-latency, high-performance implementations via deep neural networks. We
carry out extensive experimental evaluation and demonstrate that our single
architecture achieves state-of-the-art results on both tasks when compared to
previous methods, with gains in quantization efficiency as high as $20\%$ and
reduced estimation latency by up to $60\%$ when measured on general purpose and
graphical processing units (GPU). In particular, our approach reduces the GPU
inference latency by more than two times in several multiple-input
multiple-output (MIMO) configurations. Finally, we demonstrate that our scheme
is robust to distributional shifts and retains a significant part of its
performance when evaluated on 5G channel models, as well as channel estimation
errors.
</p>
<a href="http://arxiv.org/abs/2012.12843" target="_blank">arXiv:2012.12843</a> [<a href="http://arxiv.org/pdf/2012.12843" target="_blank">pdf</a>]

<h2>Superhuman Surgical Peg Transfer Using Depth-Sensing and Deep Recurrent Neural Networks. (arXiv:2012.12844v1 [cs.RO])</h2>
<h3>Minho Hwang, Brijen Thananjeyan, Daniel Seita, Jeffrey Ichnowski, Samuel Paradis, Danyal Fer, Thomas Low, Ken Goldberg</h3>
<p>We consider the automation of the well-known peg-transfer task from the
Fundamentals of Laparoscopic Surgery (FLS). While human surgeons teleoperate
robots to perform this task with great dexterity, it remains challenging to
automate. We present an approach that leverages emerging innovations in depth
sensing, deep learning, and Peiper's method for computing inverse kinematics
with time-minimized joint motion. We use the da Vinci Research Kit (dVRK)
surgical robot with a Zivid depth sensor, and automate three variants of the
peg-transfer task: unilateral, bilateral without handovers, and bilateral with
handovers. We use 3D-printed fiducial markers with depth sensing and a deep
recurrent neural network to improve the precision of the dVRK to less than 1
mm. We report experimental results for 1800 block transfer trials. Results
suggest that the fully automated system can outperform an experienced human
surgical resident, who performs far better than untrained humans, in terms of
both speed and success rate. For the most difficult variant of peg transfer
(with handovers) we compare the performance of the surgical resident with
performance of the automated system over 120 trials for each. The experienced
surgical resident achieves success rate 93.2 % with mean transfer time of 8.6
seconds. The automated system achieves success rate 94.1 % with mean transfer
time of 8.1 seconds. To our knowledge this is the first fully automated system
to achieve "superhuman" performance in both speed and success on peg transfer.
Supplementary material is available at
https://sites.google.com/view/surgicalpegtransfer.
</p>
<a href="http://arxiv.org/abs/2012.12844" target="_blank">arXiv:2012.12844</a> [<a href="http://arxiv.org/pdf/2012.12844" target="_blank">pdf</a>]

<h2>Training data-efficient image transformers & distillation through attention. (arXiv:2012.12877v1 [cs.CV])</h2>
<h3>Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv&#xe9; J&#xe9;gou</h3>
<p>Recently, neural networks purely based on attention were shown to address
image understanding tasks such as image classification. However, these visual
transformers are pre-trained with hundreds of millions of images using an
expensive infrastructure, thereby limiting their adoption by the larger
community.

In this work, with an adequate training scheme, we produce a competitive
convolution-free transformer by training on Imagenet only. We train it on a
single computer in less than 3 days. Our reference vision transformer (86M
parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on
ImageNet with no external data. We share our code and models to accelerate
community advances on this line of research.

Additionally, we introduce a teacher-student strategy specific to
transformers. It relies on a distillation token ensuring that the student
learns from the teacher through attention. We show the interest of this
token-based distillation, especially when using a convnet as a teacher. This
leads us to report results competitive with convnets for both Imagenet (where
we obtain up to 84.4% accuracy) and when transferring to other tasks.
</p>
<a href="http://arxiv.org/abs/2012.12877" target="_blank">arXiv:2012.12877</a> [<a href="http://arxiv.org/pdf/2012.12877" target="_blank">pdf</a>]

<h2>Exploring Instance-Level Uncertainty for Medical Detection. (arXiv:2012.12880v1 [cs.CV])</h2>
<h3>Jiawei Yang, Yuan Liang, Yao Zhang, Weinan Song, Kun Wang, Lei He</h3>
<p>The ability of deep learning to predict with uncertainty is recognized as key
for its adoption in clinical routines. Moreover, performance gain has been
enabled by modelling uncertainty according to empirical evidence. While
previous work has widely discussed the uncertainty estimation in segmentation
and classification tasks, its application on bounding-box-based detection has
been limited, mainly due to the challenge of bounding box aligning. In this
work, we explore to augment a 2.5D detection CNN with two different
bounding-box-level (or instance-level) uncertainty estimates, i.e., predictive
variance and Monte Carlo (MC) sample variance. Experiments are conducted for
lung nodule detection on LUNA16 dataset, a task where significant semantic
ambiguities can exist between nodules and non-nodules. Results show that our
method improves the evaluating score from 84.57% to 88.86% by utilizing a
combination of both types of variances. Moreover, we show the generated
uncertainty enables superior operating points compared to using the probability
threshold only, and can further boost the performance to 89.52%. Example nodule
detections are visualized to further illustrate the advantages of our method.
</p>
<a href="http://arxiv.org/abs/2012.12880" target="_blank">arXiv:2012.12880</a> [<a href="http://arxiv.org/pdf/2012.12880" target="_blank">pdf</a>]

<h2>Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the Wild. (arXiv:2012.12884v1 [cs.CV])</h2>
<h3>Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman</h3>
<p>Given an "in-the-wild" video of a person, we reconstruct an animatable model
of the person in the video. The output model can be rendered in any body pose
to any camera view, via the learned controls, without explicit 3D mesh
reconstruction. At the core of our method is a volumetric 3D human
representation reconstructed with a deep network trained on input video,
enabling novel pose/view synthesis. Our method is an advance over GAN-based
image-to-image translation since it allows image synthesis for any pose and
camera via the internal 3D representation, while at the same time it does not
require a pre-rigged model or ground truth meshes for training, as in
mesh-based learning. Experiments validate the design choices and yield results
on synthetic data and on real videos of diverse people performing unconstrained
activities (e.g. dancing or playing tennis). Finally, we demonstrate motion
re-targeting and bullet-time rendering with the learned models.
</p>
<a href="http://arxiv.org/abs/2012.12884" target="_blank">arXiv:2012.12884</a> [<a href="http://arxiv.org/pdf/2012.12884" target="_blank">pdf</a>]

<h2>ANR: Articulated Neural Rendering for Virtual Avatars. (arXiv:2012.12890v1 [cs.CV])</h2>
<h3>Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, Christoph Lassner</h3>
<p>The combination of traditional rendering with neural networks in Deferred
Neural Rendering (DNR) provides a compelling balance between computational
complexity and realism of the resulting images. Using skinned meshes for
rendering articulating objects is a natural extension for the DNR framework and
would open it up to a plethora of applications. However, in this case the
neural shading step must account for deformations that are possibly not
captured in the mesh, as well as alignment inaccuracies and dynamics -- which
can confound the DNR pipeline. We present Articulated Neural Rendering (ANR), a
novel framework based on DNR which explicitly addresses its limitations for
virtual human avatars. We show the superiority of ANR not only with respect to
DNR but also with methods specialized for avatar creation and animation. In two
user studies, we observe a clear preference for our avatar model and we
demonstrate state-of-the-art performance on quantitative evaluation metrics.
Perceptually, we observe better temporal stability, level of detail and
plausibility.
</p>
<a href="http://arxiv.org/abs/2012.12890" target="_blank">arXiv:2012.12890</a> [<a href="http://arxiv.org/pdf/2012.12890" target="_blank">pdf</a>]

<h2>Noisy Labels Can Induce Good Representations. (arXiv:2012.12896v1 [cs.LG])</h2>
<h3>Jingling Li, Mozhi Zhang, Keyulu Xu, John P. Dickerson, Jimmy Ba</h3>
<p>The current success of deep learning depends on large-scale labeled datasets.
In practice, high-quality annotations are expensive to collect, but noisy
annotations are more affordable. Previous works report mixed empirical results
when training with noisy labels: neural networks can easily memorize random
labels, but they can also generalize from noisy labels. To explain this puzzle,
we study how architecture affects learning with noisy labels. We observe that
if an architecture "suits" the task, training with noisy labels can induce
useful hidden representations, even when the model generalizes poorly; i.e.,
the last few layers of the model are more negatively affected by noisy labels.
This finding leads to a simple method to improve models trained on noisy
labels: replacing the final dense layers with a linear model, whose weights are
learned from a small set of clean data. We empirically validate our findings
across three architectures (Convolutional Neural Networks, Graph Neural
Networks, and Multi-Layer Perceptrons) and two domains (graph algorithmic tasks
and image classification). Furthermore, we achieve state-of-the-art results on
image classification benchmarks by combining our method with existing
approaches on noisy label training.
</p>
<a href="http://arxiv.org/abs/2012.12896" target="_blank">arXiv:2012.12896</a> [<a href="http://arxiv.org/pdf/2012.12896" target="_blank">pdf</a>]

<h2>Inferring Restaurant Styles by Mining Crowd Sourced Photos from User-Review Websites. (arXiv:1611.06301v2 [cs.CV] UPDATED)</h2>
<h3>Haofu Liao, Yuncheng Li, Tianran Hu, Jiebo Luo</h3>
<p>When looking for a restaurant online, user uploaded photos often give people
an immediate and tangible impression about a restaurant. Due to their
informativeness, such user contributed photos are leveraged by restaurant
review websites to provide their users an intuitive and effective search
experience. In this paper, we present a novel approach to inferring restaurant
types or styles (ambiance, dish styles, suitability for different occasions)
from user uploaded photos on user-review websites. To that end, we first
collect a novel restaurant photo dataset associating the user contributed
photos with the restaurant styles from TripAdvior. We then propose a deep
multi-instance multi-label learning (MIML) framework to deal with the unique
problem setting of the restaurant style classification task. We employ a
two-step bootstrap strategy to train a multi-label convolutional neural network
(CNN). The multi-label CNN is then used to compute the confidence scores of
restaurant styles for all the images associated with a restaurant. The computed
confidence scores are further used to train a final binary classifier for each
restaurant style tag. Upon training, the styles of a restaurant can be profiled
by analyzing restaurant photos with the trained multi-label CNN and SVM models.
Experimental evaluation has demonstrated that our crowd sourcing-based approach
can effectively infer the restaurant style when there are a sufficient number
of user uploaded photos for a given restaurant.
</p>
<a href="http://arxiv.org/abs/1611.06301" target="_blank">arXiv:1611.06301</a> [<a href="http://arxiv.org/pdf/1611.06301" target="_blank">pdf</a>]

<h2>Lipschitz Constrained GANs via Boundedness and Continuity. (arXiv:1803.06107v3 [cs.CV] UPDATED)</h2>
<h3>Kanglin Liu, Guoping Qiu</h3>
<p>One of the challenges in the study of Generative Adversarial Networks (GANs)
is the difficulty of its performance control. Lipschitz constraint is essential
in guaranteeing training stability for GANs. Although heuristic methods such as
weight clipping, gradient penalty and spectral normalization have been proposed
to enforce Lipschitz constraint, it is still difficult to achieve a solution
that is both practically effective and theoretically provably satisfying a
Lipschitz constraint. In this paper, we introduce the boundedness and
continuity ($BC$) conditions to enforce the Lipschitz constraint on the
discriminator functions of GANs. We prove theoretically that GANs with
discriminators meeting the BC conditions satisfy the Lipschitz constraint. We
present a practically very effective implementation of a GAN based on a
convolutional neural network (CNN) by forcing the CNN to satisfy the $BC$
conditions (BC-GAN). We show that as compared to recent techniques including
gradient penalty and spectral normalization, BC-GANs not only have better
performances but also lower computational complexity.
</p>
<a href="http://arxiv.org/abs/1803.06107" target="_blank">arXiv:1803.06107</a> [<a href="http://arxiv.org/pdf/1803.06107" target="_blank">pdf</a>]

<h2>Bandit-Based Monte Carlo Optimization for Nearest Neighbors. (arXiv:1805.08321v3 [cs.LG] UPDATED)</h2>
<h3>Vivek Bagaria, Tavor Z. Baharav, Govinda M. Kamath, David N. Tse</h3>
<p>The celebrated Monte Carlo method estimates an expensive-to-compute quantity
by random sampling. Bandit-based Monte Carlo optimization is a general
technique for computing the minimum of many such expensive-to-compute
quantities by adaptive random sampling. The technique converts an optimization
problem into a statistical estimation problem which is then solved via
multi-armed bandits. We apply this technique to solve the problem of
high-dimensional k-nearest neighbors, developing an algorithm which we prove is
able to identify exact nearest neighbors with high probability. We show that
under regularity assumptions on a dataset of n points in d-dimensional space,
the complexity of our algorithm scales logarithmically with the dimension of
the data as $O((n+d)\log^2 (\frac{nd}{\delta}))$ for error probability
$\delta$, rather than linearly as in exact computation requiring O(nd). We
corroborate our theoretical results with numerical simulations, showing that
our algorithm outperforms both exact computation and state-of-the-art
algorithms such as kGraph, NGT, and LSH on real datasets.
</p>
<a href="http://arxiv.org/abs/1805.08321" target="_blank">arXiv:1805.08321</a> [<a href="http://arxiv.org/pdf/1805.08321" target="_blank">pdf</a>]

<h2>Multiview 2D/3D Rigid Registration via a Point-Of-Interest Network for Tracking and Triangulation ($\text{POINT}^2$). (arXiv:1903.03896v4 [cs.CV] UPDATED)</h2>
<h3>Haofu Liao, Wei-An Lin, Jiarui Zhang, Jingdan Zhang, Jiebo Luo, S. Kevin Zhou</h3>
<p>We propose to tackle the problem of multiview 2D/3D rigid registration for
intervention via a Point-Of-Interest Network for Tracking and Triangulation
($\text{POINT}^2$). $\text{POINT}^2$ learns to establish 2D point-to-point
correspondences between the pre- and intra-intervention images by tracking a
set of random POIs. The 3D pose of the pre-intervention volume is then
estimated through a triangulation layer. In $\text{POINT}^2$, the unified
framework of the POI tracker and the triangulation layer enables learning
informative 2D features and estimating 3D pose jointly. In contrast to existing
approaches, $\text{POINT}^2$ only requires a single forward-pass to achieve a
reliable 2D/3D registration. As the POI tracker is shift-invariant,
$\text{POINT}^2$ is more robust to the initial pose of the 3D pre-intervention
image. Extensive experiments on a large-scale clinical cone-beam CT (CBCT)
dataset show that the proposed $\text{POINT}^2$ method outperforms the existing
learning-based method in terms of accuracy, robustness and running time.
Furthermore, when used as an initial pose estimator, our method also improves
the robustness and speed of the state-of-the-art optimization-based approaches
by ten folds.
</p>
<a href="http://arxiv.org/abs/1903.03896" target="_blank">arXiv:1903.03896</a> [<a href="http://arxiv.org/pdf/1903.03896" target="_blank">pdf</a>]

<h2>Distance Preserving Grid Layouts. (arXiv:1903.06262v2 [cs.CV] UPDATED)</h2>
<h3>Gladys Hilasaca, Fernando V. Paulovich</h3>
<p>Distance preserving visualization techniques have emerged as one of the
fundamental tools for data analysis. One example are the techniques that
arrange data instances into two-dimensional grids so that the pairwise
distances among the instances are preserved into the produced layouts.
Currently, the state-of-the-art approaches produce such grids by solving
assignment problems or using permutations to optimize cost functions. Although
precise, such strategies are computationally expensive, limited to small
datasets or being dependent on specialized hardware to speed up the process. In
this paper, we present a new technique, called Distance-preserving Grid
(DGrid), that employs a binary space partitioning process in combination with
multidimensional projections to create orthogonal regular grid layouts. Our
results show that DGrid is as precise as the existing state-of-the-art
techniques whereas requiring only a fraction of the running time and
computational resources.
</p>
<a href="http://arxiv.org/abs/1903.06262" target="_blank">arXiv:1903.06262</a> [<a href="http://arxiv.org/pdf/1903.06262" target="_blank">pdf</a>]

<h2>Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks. (arXiv:1905.13654v8 [stat.ML] UPDATED)</h2>
<h3>Soufiane Hayou, Arnaud Doucet, Judith Rousseau</h3>
<p>Recent work by Jacot et al. (2018) has shown that training a neural network
of any kind with gradient descent in parameter space is strongly related to
kernel gradient descent in function space with respect to the Neural Tangent
Kernel (NTK). Lee et al. (2019) built on this result by establishing that the
output of a neural network trained using gradient descent can be approximated
by a linear model for wide networks. In parallel, a recent line of studies
(Schoenholz et al. 2017; Hayou et al. 2019) has suggested that a special
initialization, known as the Edge of Chaos, improves training. In this paper,
we bridge the gap between these two concepts by quantifying the impact of the
initialization and the activation function on the NTK when the network depth
becomes large. In particular, we show that the performance of wide deep neural
networks cannot be explained by the NTK regime and we provide experiments
illustrating our theoretical results.
</p>
<a href="http://arxiv.org/abs/1905.13654" target="_blank">arXiv:1905.13654</a> [<a href="http://arxiv.org/pdf/1905.13654" target="_blank">pdf</a>]

<h2>Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos. (arXiv:1907.02055v2 [cs.CV] UPDATED)</h2>
<h3>Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi</h3>
<p>We propose KeypointGAN, a new method for recognizing the pose of objects from
a single image that for learning uses only unlabelled videos and a weak
empirical prior on the object poses. Video frames differ primarily in the pose
of the objects they contain, so our method distils the pose information by
analyzing the differences between frames. The distillation uses a new dual
representation of the geometry of objects as a set of 2D keypoints, and as a
pictorial representation, i.e. a skeleton image. This has three benefits: (1)
it provides a tight `geometric bottleneck' which disentangles pose from
appearance, (2) it can leverage powerful image-to-image translation networks to
map between photometry and geometry, and (3) it allows to incorporate empirical
pose priors in the learning process. The pose priors are obtained from unpaired
data, such as from a different dataset or modality such as mocap, such that no
annotated image is ever used in learning the pose recognition network. In
standard benchmarks for pose recognition for humans and faces, our method
achieves state-of-the-art performance among methods that do not require any
labelled images for training.
</p>
<a href="http://arxiv.org/abs/1907.02055" target="_blank">arXiv:1907.02055</a> [<a href="http://arxiv.org/pdf/1907.02055" target="_blank">pdf</a>]

<h2>Hard-Mining Loss based Convolutional Neural Network for Face Recognition. (arXiv:1908.09747v2 [cs.CV] UPDATED)</h2>
<h3>Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey</h3>
<p>Face Recognition is one of the prominent problems in the computer vision
domain. Witnessing advances in deep learning, significant work has been
observed in face recognition, which touched upon various parts of the
recognition framework like Convolutional Neural Network (CNN), Layers, Loss
functions, etc. Various loss functions such as Cross-Entropy, Angular-Softmax
and ArcFace have been introduced to learn the weights of network for face
recognition. However, these loss functions do not give high priority to the
hard samples as compared to the easy samples. Moreover, their learning process
is biased due to a number of easy examples compared to hard examples. In this
paper, we address this issue by considering hard examples with more priority.
In order to do so, We propose a Hard-Mining loss by increasing the loss for
harder examples and decreasing the loss for easy examples. The proposed concept
is generic and can be used with any existing loss function. We test the
Hard-Mining loss with different losses such as Cross-Entropy, Angular-Softmax
and ArcFace. The proposed Hard-Mining loss is tested over widely used Labeled
Faces in the Wild (LFW) and YouTube Faces (YTF) datasets. The training is
performed over CASIA-WebFace and MS-Celeb-1M datasets. We use the residual
network (i.e., ResNet18) for the experimental analysis. The experimental
results suggest that the performance of existing loss functions is boosted when
used in the framework of the proposed Hard-Mining loss.
</p>
<a href="http://arxiv.org/abs/1908.09747" target="_blank">arXiv:1908.09747</a> [<a href="http://arxiv.org/pdf/1908.09747" target="_blank">pdf</a>]

<h2>Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions. (arXiv:1909.00590v5 [cs.LG] UPDATED)</h2>
<h3>Hansika Hewamalage, Christoph Bergmeir, Kasun Bandara</h3>
<p>Recurrent Neural Networks (RNN) have become competitive forecasting methods,
as most notably shown in the winning method of the recent M4 competition.
However, established statistical models such as ETS and ARIMA gain their
popularity not only from their high accuracy, but they are also suitable for
non-expert users as they are robust, efficient, and automatic. In these areas,
RNNs have still a long way to go. We present an extensive empirical study and
an open-source software framework of existing RNN architectures for
forecasting, that allow us to develop guidelines and best practices for their
use. For example, we conclude that RNNs are capable of modelling seasonality
directly if the series in the dataset possess homogeneous seasonal patterns,
otherwise we recommend a deseasonalization step. Comparisons against ETS and
ARIMA demonstrate that the implemented (semi-)automatic RNN models are no
silver bullets, but they are competitive alternatives in many situations.
</p>
<a href="http://arxiv.org/abs/1909.00590" target="_blank">arXiv:1909.00590</a> [<a href="http://arxiv.org/pdf/1909.00590" target="_blank">pdf</a>]

<h2>Visual Question Answering using Deep Learning: A Survey and Performance Analysis. (arXiv:1909.01860v2 [cs.CV] UPDATED)</h2>
<h3>Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, Snehasis Mukherjee</h3>
<p>The Visual Question Answering (VQA) task combines challenges for processing
data with both Visual and Linguistic processing, to answer basic `common sense'
questions about given images. Given an image and a question in natural
language, the VQA system tries to find the correct answer to it using visual
elements of the image and inference gathered from textual questions. In this
survey, we cover and discuss the recent datasets released in the VQA domain
dealing with various types of question-formats and robustness of the
machine-learning models. Next, we discuss about new deep learning models that
have shown promising results over the VQA datasets. At the end, we present and
discuss some of the results computed by us over the vanilla VQA model, Stacked
Attention Network and the VQA Challenge 2017 winner model. We also provide the
detailed analysis along with the challenges and future research directions.
</p>
<a href="http://arxiv.org/abs/1909.01860" target="_blank">arXiv:1909.01860</a> [<a href="http://arxiv.org/pdf/1909.01860" target="_blank">pdf</a>]

<h2>Training Robust Deep Neural Networks via Adversarial Noise Propagation. (arXiv:1909.09034v2 [cs.LG] UPDATED)</h2>
<h3>Aishan Liu, Xianglong Liu, Chongzhi Zhang, Hang Yu, Qiang Liu, Dacheng Tao</h3>
<p>In practice, deep neural networks have been found to be vulnerable to various
types of noise, such as adversarial examples and corruption. Various
adversarial defense methods have accordingly been developed to improve
adversarial robustness for deep models. However, simply training on data mixed
with adversarial examples, most of these models still fail to defend against
the generalized types of noise. Motivated by the fact that hidden layers play a
highly important role in maintaining a robust model, this paper proposes a
simple yet powerful training algorithm, named \emph{Adversarial Noise
Propagation} (ANP), which injects noise into the hidden layers in a layer-wise
manner. ANP can be implemented efficiently by exploiting the nature of the
backward-forward training style. Through thorough investigations, we determine
that different hidden layers make different contributions to model robustness
and clean accuracy, while shallow layers are comparatively more critical than
deep layers. Moreover, our framework can be easily combined with other
adversarial training methods to further improve model robustness by exploiting
the potential of hidden layers. Extensive experiments on MNIST, CIFAR-10,
CIFAR-10-C, CIFAR-10-P, and ImageNet demonstrate that ANP enables the strong
robustness for deep models against both adversarial and corrupted ones, and
also significantly outperforms various adversarial defense methods.
</p>
<a href="http://arxiv.org/abs/1909.09034" target="_blank">arXiv:1909.09034</a> [<a href="http://arxiv.org/pdf/1909.09034" target="_blank">pdf</a>]

<h2>mfEGRA: Multifidelity Efficient Global Reliability Analysis through Active Learning for Failure Boundary Location. (arXiv:1910.02497v4 [stat.ML] UPDATED)</h2>
<h3>Anirban Chaudhuri, Alexandre N. Marques, Karen E. Willcox</h3>
<p>This paper develops mfEGRA, a multifidelity active learning method using
data-driven adaptively refined surrogates for failure boundary location in
reliability analysis. This work addresses the issue of prohibitive cost of
reliability analysis using Monte Carlo sampling for expensive-to-evaluate
high-fidelity models by using cheaper-to-evaluate approximations of the
high-fidelity model. The method builds on the Efficient Global Reliability
Analysis (EGRA) method, which is a surrogate-based method that uses adaptive
sampling for refining Gaussian process surrogates for failure boundary location
using a single-fidelity model. Our method introduces a two-stage adaptive
sampling criterion that uses a multifidelity Gaussian process surrogate to
leverage multiple information sources with different fidelities. The method
combines expected feasibility criterion from EGRA with one-step lookahead
information gain to refine the surrogate around the failure boundary. The
computational savings from mfEGRA depends on the discrepancy between the
different models, and the relative cost of evaluating the different models as
compared to the high-fidelity model. We show that accurate estimation of
reliability using mfEGRA leads to computational savings of $\sim$46% for an
analytic multimodal test problem and 24% for a three-dimensional acoustic horn
problem, when compared to single-fidelity EGRA. We also show the effect of
using a priori drawn Monte Carlo samples in the implementation for the acoustic
horn problem, where mfEGRA leads to computational savings of 45% for the
three-dimensional case and 48% for a rarer event four-dimensional case as
compared to single-fidelity EGRA.
</p>
<a href="http://arxiv.org/abs/1910.02497" target="_blank">arXiv:1910.02497</a> [<a href="http://arxiv.org/pdf/1910.02497" target="_blank">pdf</a>]

<h2>Carpe Diem, Seize the Samples Uncertain "At the Moment" for Adaptive Batch Selection. (arXiv:1911.08050v2 [cs.LG] UPDATED)</h2>
<h3>Hwanjun Song, Minseok Kim, Sundong Kim, Jae-Gil Lee</h3>
<p>The accuracy of deep neural networks is significantly affected by how well
mini-batches are constructed during the training step. In this paper, we
propose a novel adaptive batch selection algorithm called Recency Bias that
exploits the uncertain samples predicted inconsistently in recent iterations.
The historical label predictions of each training sample are used to evaluate
its predictive uncertainty within a sliding window. Then, the sampling
probability for the next mini-batch is assigned to each training sample in
proportion to its predictive uncertainty. By taking advantage of this design,
Recency Bias not only accelerates the training step but also achieves a more
accurate network. We demonstrate the superiority of Recency Bias by extensive
evaluation on two independent tasks. Compared with existing batch selection
methods, the results showed that Recency Bias reduced the test error by up to
20.97% in a fixed wall-clock training time. At the same time, it improved the
training time by up to 59.32% to reach the same test error
</p>
<a href="http://arxiv.org/abs/1911.08050" target="_blank">arXiv:1911.08050</a> [<a href="http://arxiv.org/pdf/1911.08050" target="_blank">pdf</a>]

<h2>Adversarial Risk via Optimal Transport and Optimal Couplings. (arXiv:1912.02794v2 [cs.LG] UPDATED)</h2>
<h3>Muni Sreenivas Pydi, Varun Jog</h3>
<p>Modern machine learning algorithms perform poorly on adversarially
manipulated data. Adversarial risk quantifies the error of classifiers in
adversarial settings; adversarial classifiers minimize adversarial risk. In
this paper, we analyze adversarial risk and adversarial classifiers from an
optimal transport perspective. We show that the optimal adversarial risk for
binary classification with 0-1 loss is determined by an optimal transport cost
between the probability distributions of the two classes. We develop optimal
transport plans (probabilistic couplings) for univariate distributions such as
the normal, the uniform, and the triangular distribution. We also derive
optimal adversarial classifiers in these settings. Our analysis leads to
algorithm-independent fundamental limits on adversarial risk, which we
calculate for several real-world datasets. We extend our results to general
loss functions under convexity and smoothness assumptions.
</p>
<a href="http://arxiv.org/abs/1912.02794" target="_blank">arXiv:1912.02794</a> [<a href="http://arxiv.org/pdf/1912.02794" target="_blank">pdf</a>]

<h2>Identifying Mislabeled Data using the Area Under the Margin Ranking. (arXiv:2001.10528v4 [cs.LG] UPDATED)</h2>
<h3>Geoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, Kilian Q. Weinberger</h3>
<p>Not all data in a typical training set help with generalization; some samples
can be overly ambiguous or outrightly mislabeled. This paper introduces a new
method to identify such samples and mitigate their impact when training neural
networks. At the heart of our algorithm is the Area Under the Margin (AUM)
statistic, which exploits differences in the training dynamics of clean and
mislabeled samples. A simple procedure - adding an extra class populated with
purposefully mislabeled threshold samples - learns a AUM upper bound that
isolates mislabeled data. This approach consistently improves upon prior work
on synthetic and real-world datasets. On the WebVision50 classification task
our method removes 17% of training data, yielding a 1.6% (absolute) improvement
in test error. On CIFAR100 removing 13% of the data leads to a 1.2% drop in
error.
</p>
<a href="http://arxiv.org/abs/2001.10528" target="_blank">arXiv:2001.10528</a> [<a href="http://arxiv.org/pdf/2001.10528" target="_blank">pdf</a>]

<h2>Uncovering differential equations from data with hidden variables. (arXiv:2002.02250v2 [stat.ML] UPDATED)</h2>
<h3>Agust&#xed;n Somacal, Yamila Barrera, Leonardo Boechi, Matthieu Jonckheere, Vincent Lefieux, Dominique Picard, Ezequiel Smucler</h3>
<p>SINDy is a method for learning system of differential equations from data by
solving a sparse linear regression optimization problem [Brunton et al., 2016].
In this article, we propose an extension of the SINDy method that learns
systems of differential equations in cases where some of the variables are not
observed. Our extension is based on regressing a higher order time derivative
of a target variable onto a dictionary of functions that includes lower order
time derivatives of the target variable. We evaluate our method by measuring
the prediction accuracy of the learned dynamical systems on synthetic data and
on a real data-set of temperature time series provided by the R\'eseau de
Transport d'\'Electricit\'e (RTE). Our method provides high quality short-term
forecasts and it is orders of magnitude faster than competing methods for
learning differential equations with latent variables.
</p>
<a href="http://arxiv.org/abs/2002.02250" target="_blank">arXiv:2002.02250</a> [<a href="http://arxiv.org/pdf/2002.02250" target="_blank">pdf</a>]

<h2>Robustness Verification for Transformers. (arXiv:2002.06622v2 [cs.LG] UPDATED)</h2>
<h3>Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, Cho-Jui Hsieh</h3>
<p>Robustness verification that aims to formally certify the prediction behavior
of neural networks has become an important tool for understanding model
behavior and obtaining safety guarantees. However, previous methods can usually
only handle neural networks with relatively simple architectures. In this
paper, we consider the robustness verification problem for Transformers.
Transformers have complex self-attention layers that pose many challenges for
verification, including cross-nonlinearity and cross-position dependency, which
have not been discussed in previous works. We resolve these challenges and
develop the first robustness verification algorithm for Transformers. The
certified robustness bounds computed by our method are significantly tighter
than those by naive Interval Bound Propagation. These bounds also shed light on
interpreting Transformers as they consistently reflect the importance of
different words in sentiment analysis.
</p>
<a href="http://arxiv.org/abs/2002.06622" target="_blank">arXiv:2002.06622</a> [<a href="http://arxiv.org/pdf/2002.06622" target="_blank">pdf</a>]

<h2>Robot Playing Kendama with Model-Based and Model-Free Reinforcement Learning. (arXiv:2003.06751v2 [cs.RO] UPDATED)</h2>
<h3>Shidi Li</h3>
<p>Several model-based and model-free methods have been proposed for the robot
trajectory learning task. Both approaches have their benefits and drawbacks.
They can usually complement each other. Many research works are trying to
integrate some model-based and model-free methods into one algorithm and
perform well in simulators or quasi-static robot tasks. Difficulties still
exist when algorithms are used in particular trajectory learning tasks. In this
paper, we propose a robot trajectory learning framework for precise tasks with
discontinuous dynamics and high speed. The trajectories learned from the human
demonstration are optimized by DDP and PoWER successively. The framework is
tested on the Kendama manipulation task, which can also be difficult for humans
to achieve. The results show that our approach can plan the trajectories to
successfully complete the task.
</p>
<a href="http://arxiv.org/abs/2003.06751" target="_blank">arXiv:2003.06751</a> [<a href="http://arxiv.org/pdf/2003.06751" target="_blank">pdf</a>]

<h2>Understanding when spatial transformer networks do not support invariance, and what to do about it. (arXiv:2004.11678v4 [cs.CV] UPDATED)</h2>
<h3>Lukas Finnveden, Ylva Jansson, Tony Lindeberg</h3>
<p>Spatial transformer networks (STNs) were designed to enable convolutional
neural networks (CNNs) to learn invariance to image transformations. STNs were
originally proposed to transform CNN feature maps as well as input images. This
enables the use of more complex features when predicting transformation
parameters. However, since STNs perform a purely spatial transformation, they
do not, in the general case, have the ability to align the feature maps of a
transformed image with those of its original. STNs are therefore unable to
support invariance when transforming CNN feature maps. We present a simple
proof for this and study the practical implications, showing that this
inability is coupled with decreased classification accuracy. We therefore
investigate alternative STN architectures that make use of complex features. We
find that while deeper localization networks are difficult to train,
localization networks that share parameters with the classification network
remain stable as they grow deeper, which allows for higher classification
accuracy on difficult datasets. Finally, we explore the interaction between
localization network complexity and iterative image alignment.
</p>
<a href="http://arxiv.org/abs/2004.11678" target="_blank">arXiv:2004.11678</a> [<a href="http://arxiv.org/pdf/2004.11678" target="_blank">pdf</a>]

<h2>A Deep Attentive Convolutional Neural Network for Automatic Cortical Plate Segmentation in Fetal MRI. (arXiv:2004.12847v2 [cs.CV] UPDATED)</h2>
<h3>Haoran Dou, Davood Karimi, Caitlin K. Rollins, Cynthia M. Ortinau, Lana Vasung, Clemente Velasco-Annis, Abdelhakim Ouaalam, Xin Yang, Dong Ni, Ali Gholipour</h3>
<p>Fetal cortical plate segmentation is essential in quantitative analysis of
fetal brain maturation and cortical folding. Manual segmentation of the
cortical plate, or manual refinement of automatic segmentations is tedious and
time-consuming. Automatic segmentation of the cortical plate, on the other
hand, is challenged by the relatively low resolution of the reconstructed fetal
brain MRI scans compared to the thin structure of the cortical plate, partial
voluming, and the wide range of variations in the morphology of the cortical
plate as the brain matures during gestation. To reduce the burden of manual
refinement of segmentations, we have developed a new and powerful deep learning
segmentation method. Our method exploits new deep attentive modules with mixed
kernel convolutions within a fully convolutional neural network architecture
that utilizes deep supervision and residual connections. We evaluated our
method quantitatively based on several performance measures and expert
evaluations. Results show that our method outperforms several state-of-the-art
deep models for segmentation, as well as a state-of-the-art multi-atlas
segmentation technique. We achieved average Dice similarity coefficient of
0.87, average Hausdorff distance of 0.96 mm, and average symmetric surface
difference of 0.28 mm on reconstructed fetal brain MRI scans of fetuses scanned
in the gestational age range of 16 to 39 weeks. With a computation time of less
than 1 minute per fetal brain, our method can facilitate and accelerate
large-scale studies on normal and altered fetal brain cortical maturation and
folding.
</p>
<a href="http://arxiv.org/abs/2004.12847" target="_blank">arXiv:2004.12847</a> [<a href="http://arxiv.org/pdf/2004.12847" target="_blank">pdf</a>]

<h2>Frugal Optimization for Cost-related Hyperparameters. (arXiv:2005.01571v3 [cs.LG] UPDATED)</h2>
<h3>Qingyun Wu, Chi Wang, Silu Huang</h3>
<p>The increasing demand for democratizing machine learning algorithms calls for
hyperparameter optimization (HPO) solutions at low cost. Many machine learning
algorithms have hyperparameters which can cause a large variation in the
training cost. But this effect is largely ignored in existing HPO methods,
which are incapable to properly control cost during the optimization process.
To address this problem, we develop a new cost-frugal HPO solution. The core of
our solution is a simple but new randomized direct-search method, for which we
prove a convergence rate of $O(\frac{\sqrt{d}}{\sqrt{K}})$ and an
$O(d\epsilon^{-2})$-approximation guarantee on the total cost. We provide
strong empirical results in comparison with state-of-the-art HPO methods on
large AutoML benchmarks.
</p>
<a href="http://arxiv.org/abs/2005.01571" target="_blank">arXiv:2005.01571</a> [<a href="http://arxiv.org/pdf/2005.01571" target="_blank">pdf</a>]

<h2>TIME: Text and Image Mutual-Translation Adversarial Networks. (arXiv:2005.13192v2 [cs.CV] UPDATED)</h2>
<h3>Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard de Melo, Ahmed Elgammal</h3>
<p>Focusing on text-to-image (T2I) generation, we propose Text and Image
Mutual-Translation Adversarial Networks (TIME), a lightweight but effective
model that jointly learns a T2I generator G and an image captioning
discriminator D under the Generative Adversarial Network framework. While
previous methods tackle the T2I problem as a uni-directional task and use
pre-trained language models to enforce the image--text consistency, TIME
requires neither extra modules nor pre-training. We show that the performance
of G can be boosted substantially by training it jointly with D as a language
model. Specifically, we adopt Transformers to model the cross-modal connections
between the image features and word embeddings, and design an annealing
conditional hinge loss that dynamically balances the adversarial learning. In
our experiments, TIME achieves state-of-the-art (SOTA) performance on the CUB
and MS-COCO dataset (Inception Score of 4.91 and Fr\'echet Inception Distance
of 14.3 on CUB), and shows promising performance on MS-COCO on image captioning
and downstream vision-language tasks.
</p>
<a href="http://arxiv.org/abs/2005.13192" target="_blank">arXiv:2005.13192</a> [<a href="http://arxiv.org/pdf/2005.13192" target="_blank">pdf</a>]

<h2>Attribute-Efficient Learning of Halfspaces with Malicious Noise: Near-Optimal Label Complexity and Noise Tolerance. (arXiv:2006.03781v4 [stat.ML] UPDATED)</h2>
<h3>Jie Shen, Chicheng Zhang</h3>
<p>This paper is concerned with computationally efficient learning of
homogeneous sparse halfspaces in $\mathbb{R}^d$ under noise. Though recent
works have established attribute-efficient learning algorithms under various
types of label noise (e.g. bounded noise), it remains an open question when and
how $s$-sparse halfspaces can be efficiently learned under the challenging
malicious noise model, where an adversary may corrupt both the unlabeled
examples and the labels. We answer this question in the affirmative by
designing a computationally efficient active learning algorithm with
near-optimal label complexity of $\tilde{O}\big({s \log^4 \frac d \epsilon}
\big)$ and noise tolerance $\eta = \Omega(\epsilon)$, where $\epsilon \in (0,
1)$ is the target error rate, under the assumption that the distribution over
(uncorrupted) unlabeled examples is isotropic log-concave. Our algorithm can be
straightforwardly tailored to the passive learning setting, and we show that
the sample complexity is $\tilde{O}\big({\frac 1 \epsilon s^2 \log^5 d} \big)$
which also enjoys the attribute efficiency. Our main techniques include
attribute-efficient paradigms for instance reweighting and for empirical risk
minimization, and a new analysis of uniform concentration for unbounded data --
all of them crucially take the structure of the underlying halfspace into
account.
</p>
<a href="http://arxiv.org/abs/2006.03781" target="_blank">arXiv:2006.03781</a> [<a href="http://arxiv.org/pdf/2006.03781" target="_blank">pdf</a>]

<h2>Generative 3D Part Assembly via Dynamic Graph Learning. (arXiv:2006.07793v3 [cs.CV] UPDATED)</h2>
<h3>Jialei Huang, Guanqi Zhan, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas Guibas, Hao Dong</h3>
<p>Autonomous part assembly is a challenging yet crucial task in 3D computer
vision and robotics. Analogous to buying an IKEA furniture, given a set of 3D
parts that can assemble a single shape, an intelligent agent needs to perceive
the 3D part geometry, reason to propose pose estimations for the input parts,
and finally call robotic planning and control routines for actuation. In this
paper, we focus on the pose estimation subproblem from the vision side
involving geometric and relational reasoning over the input part geometry.
Essentially, the task of generative 3D part assembly is to predict a 6-DoF part
pose, including a rigid rotation and translation, for each input part that
assembles a single 3D shape as the final output. To tackle this problem, we
propose an assembly-oriented dynamic graph learning framework that leverages an
iterative graph neural network as a backbone. It explicitly conducts sequential
part assembly refinements in a coarse-to-fine manner, exploits a pair of part
relation reasoning module and part aggregation module for dynamically adjusting
both part features and their relations in the part graph. We conduct extensive
experiments and quantitative comparisons to three strong baseline methods,
demonstrating the effectiveness of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2006.07793" target="_blank">arXiv:2006.07793</a> [<a href="http://arxiv.org/pdf/2006.07793" target="_blank">pdf</a>]

<h2>A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges. (arXiv:2006.09319v2 [cs.LG] UPDATED)</h2>
<h3>Laura Swiler, Mamikon Gulian, Ari Frankel, Cosmin Safta, John Jakeman</h3>
<p>Gaussian process regression is a popular Bayesian framework for surrogate
modeling of expensive data sources. As part of a broader effort in scientific
machine learning, many recent works have incorporated physical constraints or
other a priori information within Gaussian process regression to supplement
limited data and regularize the behavior of the model. We provide an overview
and survey of several classes of Gaussian process constraints, including
positivity or bound constraints, monotonicity and convexity constraints,
differential equation constraints provided by linear PDEs, and boundary
condition constraints. We compare the strategies behind each approach as well
as the differences in implementation, concluding with a discussion of the
computational challenges introduced by constraints.
</p>
<a href="http://arxiv.org/abs/2006.09319" target="_blank">arXiv:2006.09319</a> [<a href="http://arxiv.org/pdf/2006.09319" target="_blank">pdf</a>]

<h2>Learning of Discrete Graphical Models with Neural Networks. (arXiv:2006.11937v2 [cs.LG] UPDATED)</h2>
<h3>Abhijith J., Andrey Y. Lokhov, Sidhant Misra, Marc Vuffray</h3>
<p>Graphical models are widely used in science to represent joint probability
distributions with an underlying conditional dependence structure. The inverse
problem of learning a discrete graphical model given i.i.d samples from its
joint distribution can be solved with near-optimal sample complexity using a
convex optimization method known as Generalized Regularized Interaction
Screening Estimator (GRISE). But the computational cost of GRISE becomes
prohibitive when the energy function of the true graphical model has
higher-order terms. We introduce NeurISE, a neural net based algorithm for
graphical model learning, to tackle this limitation of GRISE. We use neural
nets as function approximators in an Interaction Screening objective function.
The optimization of this objective then produces a neural-net representation
for the conditionals of the graphical model. NeurISE algorithm is seen to be a
better alternative to GRISE when the energy function of the true model has a
high order with a high degree of symmetry. In these cases NeurISE is able to
find the correct parsimonious representation for the conditionals without being
fed any prior information about the true model. NeurISE can also be used to
learn the underlying structure of the true model with some simple modifications
to its training procedure. In addition, we also show a variant of NeurISE that
can be used to learn a neural net representation for the full energy function
of the true model.
</p>
<a href="http://arxiv.org/abs/2006.11937" target="_blank">arXiv:2006.11937</a> [<a href="http://arxiv.org/pdf/2006.11937" target="_blank">pdf</a>]

<h2>Submodular Combinatorial Information Measures with Applications in Machine Learning. (arXiv:2006.15412v3 [cs.LG] UPDATED)</h2>
<h3>Rishabh Iyer, Ninad Khargonkar, Jeff Bilmes, Himanshu Asnani</h3>
<p>Information-theoretic quantities like entropy and mutual information have
found numerous uses in machine learning. It is well known that there is a
strong connection between these entropic quantities and submodularity since
entropy over a set of random variables is submodular. In this paper, we study
combinatorial information measures that generalize independence, (conditional)
entropy, (conditional) mutual information, and total correlation defined over
sets of (not necessarily random) variables. These measures strictly generalize
the corresponding entropic measures since they are all parameterized via
submodular functions that themselves strictly generalize entropy. Critically,
we show that, unlike entropic mutual information in general, the submodular
mutual information is actually submodular in one argument, holding the other
fixed, for a large class of submodular functions whose third-order partial
derivatives satisfy a non-negativity property. This turns out to include a
number of practically useful cases such as the facility location and set-cover
functions. We study specific instantiations of the submodular information
measures on these, as well as the probabilistic coverage, graph-cut, and
saturated coverage functions, and see that they all have mathematically
intuitive and practically useful expressions. Regarding applications, we
connect the maximization of submodular (conditional) mutual information to
problems such as mutual-information-based, query-based, and privacy-preserving
summarization -- and we connect optimizing the multi-set submodular mutual
information to clustering and robust partitioning.
</p>
<a href="http://arxiv.org/abs/2006.15412" target="_blank">arXiv:2006.15412</a> [<a href="http://arxiv.org/pdf/2006.15412" target="_blank">pdf</a>]

<h2>Relationship between manifold smoothness and adversarial vulnerability in deep learning with local errors. (arXiv:2007.02047v2 [cs.LG] UPDATED)</h2>
<h3>Zijian Jiang, Jianwen Zhou, Haiping Huang</h3>
<p>Artificial neural networks can achieve impressive performances, and even
outperform humans in some specific tasks. Nevertheless, unlike biological
brains, the artificial neural networks suffer from tiny perturbations in
sensory input, under various kinds of adversarial attacks. It is therefore
necessary to study the origin of the adversarial vulnerability. Here, we
establish a fundamental relationship between geometry of hidden representations
(manifold perspective) and the generalization capability of the deep networks.
For this purpose, we choose a deep neural network trained by local errors, and
then analyze emergent properties of trained networks through the manifold
dimensionality, manifold smoothness, and the generalization capability. To
explore effects of adversarial examples, we consider independent Gaussian noise
attacks and fast-gradient-sign-method (FGSM) attacks. Our study reveals that a
high generalization accuracy requires a relatively fast power-law decay of the
eigen-spectrum of hidden representations. Under Gaussian attacks, the
relationship between generalization accuracy and power-law exponent is
monotonic, while a non-monotonic behavior is observed for FGSM attacks. Our
empirical study provides a route towards a final mechanistic interpretation of
adversarial vulnerability under adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2007.02047" target="_blank">arXiv:2007.02047</a> [<a href="http://arxiv.org/pdf/2007.02047" target="_blank">pdf</a>]

<h2>SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations. (arXiv:2007.04137v2 [cs.CV] UPDATED)</h2>
<h3>Giulio Lovisotto, Henry Turner, Ivo Sluganovic, Martin Strohmeier, Ivan Martinovic</h3>
<p>Research into adversarial examples (AE) has developed rapidly, yet static
adversarial patches are still the main technique for conducting attacks in the
real world, despite being obvious, semi-permanent and unmodifiable once
deployed.

In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a
novel technique that allows adversaries to realize physically robust real-world
AE by using a light projector. Attackers can project a specifically crafted
adversarial perturbation onto a real-world object, transforming it into an AE.
This allows the adversary greater control over the attack compared to
adversarial patches: (i) projections can be dynamically turned on and off or
modified at will, (ii) projections do not suffer from the locality constraint
imposed by patches, making them harder to detect.

We study the feasibility of SLAP in the self-driving scenario, targeting both
object detector and traffic sign recognition tasks, focusing on the detection
of stop signs. We conduct experiments in a variety of ambient light conditions,
including outdoors, showing how in non-bright settings the proposed method
generates AE that are extremely robust, causing misclassifications on
state-of-the-art networks with up to 99% success rate for a variety of angles
and distances. We also demostrate that SLAP-generated AE do not present
detectable behaviours seen in adversarial patches and therefore bypass
SentiNet, a physical AE detection method. We evaluate other defences including
an adaptive defender using adversarial learning which is able to thwart the
attack effectiveness up to 80% even in favourable attacker conditions.
</p>
<a href="http://arxiv.org/abs/2007.04137" target="_blank">arXiv:2007.04137</a> [<a href="http://arxiv.org/pdf/2007.04137" target="_blank">pdf</a>]

<h2>Spatially Aware Multimodal Transformers for TextVQA. (arXiv:2007.12146v2 [cs.CV] UPDATED)</h2>
<h3>Yash Kant, Dhruv Batra, Peter Anderson, Alex Schwing, Devi Parikh, Jiasen Lu, Harsh Agrawal</h3>
<p>Textual cues are essential for everyday tasks like buying groceries and using
public transport. To develop this assistive technology, we study the TextVQA
task, i.e., reasoning about text in images to answer a question. Existing
approaches are limited in their use of spatial relations and rely on
fully-connected transformer-like architectures to implicitly learn the spatial
structure of a scene. In contrast, we propose a novel spatially aware
self-attention layer such that each visual entity only looks at neighboring
entities defined by a spatial graph. Further, each head in our multi-head
self-attention layer focuses on a different subset of relations. Our approach
has two advantages: (1) each head considers local context instead of dispersing
the attention amongst all visual entities; (2) we avoid learning redundant
features. We show that our model improves the absolute accuracy of current
state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline,
and 4.62% on questions that involve spatial reasoning and can be answered
correctly using OCR tokens. Similarly on ST-VQA, we improve the absolute
accuracy by 4.2%. We further show that spatially aware self-attention improves
visual grounding.
</p>
<a href="http://arxiv.org/abs/2007.12146" target="_blank">arXiv:2007.12146</a> [<a href="http://arxiv.org/pdf/2007.12146" target="_blank">pdf</a>]

<h2>FedEmail: Federated learning in phishing email detection to enabling multiple organizational collaborations without sharing email data. (arXiv:2007.13300v2 [cs.LG] UPDATED)</h2>
<h3>Chandra Thapa, Jun Wen Tang, Sharif Abuadbba, Yansong Gao, Yifeng Zheng, Seyit A. Camtepe, Surya Nepal, Mahathir Almashor</h3>
<p>Artificial intelligence (AI) in phishing email detection typically focuses on
centralized data training that eventually accesses sensitive raw email data
from the collected data repository. Due to privacy, organizations are reluctant
to share their email data, and it is uncommon to find enough data in one
organization to forming a global AI model. Thus, a privacy-friendly AI
technique such as federated learning (FL) is a desideratum. FL enables machine
learning over multi-organizational email datasets to preserve their privacy
without the requirement of accessing them and sharing with other organizations
during the learning in a distributed computing framework. To the best of our
knowledge, this work is the first to investigate FL in email anti-phishing.
Building upon a Recurrent Convolutional Neural Network for phishing email
detection, we analyze and evaluate FL-entangled learning performance under
various settings, including (i) balanced and imbalanced data distribution among
organizations, (ii) scalability, and (iii) communication overhead. Our results
positively corroborate comparable performance statistics of FL in phishing
email detection to centralized learning. As a trade-off to privacy and
distributed learning, FL has a communication overhead of 0.179 GB per global
epoch per its organizations, and a gradual degradation in performance if we
increase the number of organizations but keep the same total email dataset.
However, if we allow to increase the total email dataset with the introduction
of new organizations in the FL framework, the organization-level performance is
improved. For example, a newly added organization in FL makes an improvement in
testing accuracy by 1.87% and fast convergence compared to centralized
learning. Besides, our empirical results find that FL has a good performance
over imbalanced email dataset.
</p>
<a href="http://arxiv.org/abs/2007.13300" target="_blank">arXiv:2007.13300</a> [<a href="http://arxiv.org/pdf/2007.13300" target="_blank">pdf</a>]

<h2>Counterfactual-based minority oversampling for imbalanced classification. (arXiv:2008.09488v2 [cs.LG] UPDATED)</h2>
<h3>Hao Luo, Li Liu</h3>
<p>A key challenge of oversampling in imbalanced classification is that the
generation of new minority samples often neglects the usage of majority
classes, resulting in most new minority sampling spreading the whole minority
space. In view of this, we present a new oversampling framework based on the
counterfactual theory. Our framework introduces a counterfactual objective by
leveraging the rich inherent information of majority classes and explicitly
perturbing majority samples to generate new samples in the territory of
minority space. It can be analytically shown that the new minority samples
satisfy the minimum inversion, and therefore most of them locate near the
decision boundary. Empirical evaluations on benchmark datasets suggest that our
approach significantly outperforms the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2008.09488" target="_blank">arXiv:2008.09488</a> [<a href="http://arxiv.org/pdf/2008.09488" target="_blank">pdf</a>]

<h2>A Survey on Reinforcement Learning for Combinatorial Optimization. (arXiv:2008.12248v2 [cs.LG] UPDATED)</h2>
<h3>Yunhao Yang, Andrew Whinston</h3>
<p>This paper gives a detailed review of reinforcement learning in combinatorial
optimization, introduces the history of combinatorial optimization starting in
the 1960s, and compares it with the reinforcement learning algorithms in recent
years. We explicitly look at a famous combinatorial problem known as the
Traveling Salesperson Problem (TSP). We compare the approach of the modern
reinforcement learning algorithms on TSP with an approach published in 1970.
Then, we discuss the similarities between these algorithms and how the approach
of reinforcement learning changes due to the evolution of machine learning
techniques and computing power. We also mention the deep learning approach on
the TSP, which is named Deep Reinforcement Learning. We argue that deep
learning is a generic approach that can be integrated with traditional
reinforcement learning algorithms and optimize the outcomes of the TSP.
</p>
<a href="http://arxiv.org/abs/2008.12248" target="_blank">arXiv:2008.12248</a> [<a href="http://arxiv.org/pdf/2008.12248" target="_blank">pdf</a>]

<h2>Information Bottleneck Constrained Latent Bidirectional Embedding for Zero-Shot Learning. (arXiv:2009.07451v2 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Lei Zhou, Xiao Bai, Lin Gu, Tatsuya Harada, Jun Zhou</h3>
<p>Zero-shot learning (ZSL) aims to recognize novel classes by transferring
semantic knowledge from seen classes to unseen classes. Though many ZSL methods
rely on a direct mapping between the visual and the semantic space, the
calibration deviation and hubness problem limit the generalization capability
to unseen classes. Recently emerged generative ZSL methods generate unseen
image features to transform ZSL into a supervised classification problem.
However, most generative models still suffer from the seen-unseen bias problem
as only seen data is used for training. To address these issues, we propose a
novel bidirectional embedding based generative model with a tight
visual-semantic coupling constraint. We learn a unified latent space that
calibrates the embedded parametric distributions of both visual and semantic
spaces. Since the embedding from high-dimensional visual features comprise much
non-semantic information, the alignment of visual and semantic in latent space
would inevitably been deviated. Therefore, we introduce information bottleneck
(IB) constraint to ZSL for the first time to preserve essential attribute
information during the mapping. Specifically, we utilize the uncertainty
estimation and the wake-sleep procedure to alleviate the feature noises and
improve model abstraction capability. In addition, our method can be easily
extended to transductive ZSL setting by generating labels for unseen images. We
then introduce a robust loss to solve this label noise problem. Extensive
experimental results show that our method outperforms the state-of-the-art
methods in different ZSL settings on most benchmark datasets. The code will be
available at https://github.com/osierboy/IBZSL.
</p>
<a href="http://arxiv.org/abs/2009.07451" target="_blank">arXiv:2009.07451</a> [<a href="http://arxiv.org/pdf/2009.07451" target="_blank">pdf</a>]

<h2>MARA-Net: Single Image Deraining Network with Multi-level connections and Adaptive Regional Attentions. (arXiv:2009.13990v3 [cs.CV] UPDATED)</h2>
<h3>Yeachan Park, Myeongho Jeon, Junho Lee, Myungjoo Kang</h3>
<p>Removing rain streaks from single images is an important problem in various
computer vision tasks because rain streaks can degrade outdoor images and
reduce their visibility. While recent convolutional neural network-based
deraining models have succeeded in capturing rain streaks effectively,
difficulties in recovering the details in rain-free images still remain. In
this paper, we present a multi-level connection and adaptive regional attention
network (MARA-Net) to properly restore the original background textures in
rainy images. The first main idea is a multi-level connection design that
repeatedly connects multi-level features of the encoder network to the decoder
network. Multi-level connections encourage the decoding process to use the
feature information of all levels. Channel attention is considered in
multi-level connections to learn which level of features is important in the
decoding process of the current level. The second main idea is a wide regional
non-local block (WRNL). As rain streaks primarily exhibit a vertical
distribution, we divide the grid of the image into horizontally-wide patches
and apply a non-local operation to each region to explore the rich rain-free
background information. Experimental results on both synthetic and real-world
rainy datasets demonstrate that the proposed model significantly outperforms
existing state-of-the-art models. Furthermore, the results of the joint
deraining and segmentation experiment prove that our model contributes
effectively to other vision tasks.
</p>
<a href="http://arxiv.org/abs/2009.13990" target="_blank">arXiv:2009.13990</a> [<a href="http://arxiv.org/pdf/2009.13990" target="_blank">pdf</a>]

<h2>Uncertainty Sets for Image Classifiers using Conformal Prediction. (arXiv:2009.14193v2 [cs.CV] UPDATED)</h2>
<h3>Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, Michael I. Jordan</h3>
<p>Convolutional image classifiers can achieve high predictive accuracy, but
quantifying their uncertainty remains an unresolved challenge, hindering their
deployment in consequential settings. Existing uncertainty quantification
techniques, such as Platt scaling, attempt to calibrate the network's
probability estimates, but they do not have formal guarantees. We present an
algorithm that modifies any classifier to output a predictive set containing
the true label with a user-specified probability, such as 90%. The algorithm is
simple and fast like Platt scaling, but provides a formal finite-sample
coverage guarantee for every model and dataset. Our method modifies an existing
conformal prediction algorithm to give more stable predictive sets by
regularizing the small scores of unlikely classes after Platt scaling. In
experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other
classifiers, our scheme outperforms existing approaches, achieving coverage
with sets that are often factors of 5 to 10 smaller.
</p>
<a href="http://arxiv.org/abs/2009.14193" target="_blank">arXiv:2009.14193</a> [<a href="http://arxiv.org/pdf/2009.14193" target="_blank">pdf</a>]

<h2>Riemannian Langevin Algorithm for Solving Semidefinite Programs. (arXiv:2010.11176v3 [stat.ML] UPDATED)</h2>
<h3>Mufan Bill Li, Murat A. Erdogdu</h3>
<p>We propose a Langevin diffusion-based algorithm for non-convex optimization
and sampling on a product manifold of spheres. Under a logarithmic Sobolev
inequality, we establish a guarantee for finite iteration convergence to the
Gibbs distribution in terms of Kullback--Leibler divergence. We show that with
an appropriate temperature choice, the suboptimality gap to the global minimum
is guaranteed to be arbitrarily small with high probability.

As an application, we consider the Burer--Monteiro approach for solving a
semidefinite program (SDP) with diagonal constraints, and analyze the proposed
Langevin algorithm for optimizing the non-convex objective. In particular, we
establish a logarithmic Sobolev inequality for the Burer--Monteiro problem when
there are no spurious local minima, but under the presence saddle points.
Combining the results, we then provide a global optimality guarantee for the
SDP and the Max-Cut problem. More precisely, we show that the Langevin
algorithm achieves $\epsilon$ accuracy with high probability in
$\widetilde{\Omega}( \epsilon^{-5} )$ iterations.
</p>
<a href="http://arxiv.org/abs/2010.11176" target="_blank">arXiv:2010.11176</a> [<a href="http://arxiv.org/pdf/2010.11176" target="_blank">pdf</a>]

<h2>Multimodal Sensor Fusion with Differentiable Filters. (arXiv:2010.13021v2 [cs.RO] UPDATED)</h2>
<h3>Michelle A. Lee, Brent Yi, Roberto Mart&#xed;n-Mart&#xed;n, Silvio Savarese, Jeannette Bohg</h3>
<p>Leveraging multimodal information with recursive Bayesian filters improves
performance and robustness of state estimation, as recursive filters can
combine different modalities according to their uncertainties. Prior work has
studied how to optimally fuse different sensor modalities with analytical state
estimation algorithms. However, deriving the dynamics and measurement models
along with their noise profile can be difficult or lead to intractable models.
Differentiable filters provide a way to learn these models end-to-end while
retaining the algorithmic structure of recursive filters. This can be
especially helpful when working with sensor modalities that are high
dimensional and have very different characteristics. In contact-rich
manipulation, we want to combine visual sensing (which gives us global
information) with tactile sensing (which gives us local information). In this
paper, we study new differentiable filtering architectures to fuse
heterogeneous sensor information. As case studies, we evaluate three tasks: two
in planar pushing (simulated and real) and one in manipulating a kinematically
constrained door (simulated). In extensive evaluations, we find that
differentiable filters that leverage crossmodal sensor information reach
comparable accuracies to unstructured LSTM models, while presenting
interpretability benefits that may be important for safety-critical systems. We
also release an open-source library for creating and training differentiable
Bayesian filters in PyTorch, which can be found on our project website:
https://sites.google.com/view/multimodalfilter
</p>
<a href="http://arxiv.org/abs/2010.13021" target="_blank">arXiv:2010.13021</a> [<a href="http://arxiv.org/pdf/2010.13021" target="_blank">pdf</a>]

<h2>Analytic Bipedal Walking with Fused Angles and Corrective Actions in the Tilt Phase Space. (arXiv:2011.10339v2 [cs.RO] UPDATED)</h2>
<h3>Philipp Allgeuer</h3>
<p>This work presents algorithms for the feedback-stabilised walking of bipedal
humanoid robotic platforms, along with the underlying theoretical and
sensorimotor frameworks required to achieve it. Bipedal walking is inherently
complex and difficult to control due to the high level of nonlinearity and
significant number of degrees of freedom of the concerned robots, the limited
observability and controllability of the corresponding states, and the
combination of imperfect actuation with less-than-ideal sensing. The presented
methods deal with these issues in a multitude of ways, ranging from the
development of an actuator control and feed-forward compensation scheme, to the
inclusion of filtering in almost all of the gait stabilisation feedback
pipelines. Two gaits are developed and investigated, the direct fused angle
feedback gait, and the tilt phase controller. Both gaits follow the design
philosophy of leveraging a semi-stable open-loop gait generator, and extending
it through stabilising feedback via the means of so-called corrective actions.
The idea of using corrective actions is to modify the generation of the
open-loop joint waveforms in such a way that the balance of the robot is
influenced and thereby ameliorated. Examples of such corrective actions include
modifications of the arm swing and leg swing trajectories, the application of
dynamic positional and rotational offsets to the hips and feet, and adjustments
of the commanded step size and timing. Underpinning both feedback gaits and
their corresponding gait generators are significant advances in the field of 3D
rotation theory. These advances include the development of three novel rotation
representations, the tilt angles, fused angles, and tilt phase space
representations. All three of these representations are founded on a new
innovative way of splitting 3D rotations into their respective yaw and tilt
components.
</p>
<a href="http://arxiv.org/abs/2011.10339" target="_blank">arXiv:2011.10339</a> [<a href="http://arxiv.org/pdf/2011.10339" target="_blank">pdf</a>]

<h2>Learning from Lexical Perturbations for Consistent Visual Question Answering. (arXiv:2011.13406v2 [cs.CV] UPDATED)</h2>
<h3>Spencer Whitehead, Hui Wu, Yi Ren Fung, Heng Ji, Rogerio Feris, Kate Saenko</h3>
<p>Existing Visual Question Answering (VQA) models are often fragile and
sensitive to input variations. In this paper, we propose a novel approach to
address this issue based on modular networks, which creates two questions
related by linguistic perturbations and regularizes the visual reasoning
process between them to be consistent during training. We show that our
framework markedly improves consistency and generalization ability,
demonstrating the value of controlled linguistic perturbations as a useful and
currently underutilized training and regularization tool for VQA models. We
also present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and
augmentation pipeline to create controllable linguistic variations of VQA
questions. Our benchmark uniquely draws from large-scale linguistic resources,
avoiding human annotation effort while maintaining data quality compared to
generative approaches. We benchmark existing VQA models using VQA P2 and
provide robustness analysis on each type of linguistic variation.
</p>
<a href="http://arxiv.org/abs/2011.13406" target="_blank">arXiv:2011.13406</a> [<a href="http://arxiv.org/pdf/2011.13406" target="_blank">pdf</a>]

<h2>Deep Dose Plugin Towards Real-time Monte Carlo Dose Calculation Through a Deep Learning based Denoising Algorithm. (arXiv:2011.14959v2 [cs.CV] UPDATED)</h2>
<h3>Ti Bai, Biling Wang, Dan Nguyen, Steve Jiang</h3>
<p>Monte Carlo (MC) simulation is considered the gold standard method for
radiotherapy dose calculation. However, achieving high precision requires a
large number of simulation histories, which is time consuming. The use of
computer graphics processing units (GPUs) has greatly accelerated MC simulation
and allows dose calculation within a few minutes for a typical radiotherapy
treatment plan. However, some clinical applications demand real time efficiency
for MC dose calculation. To tackle this problem, we have developed a real time,
deep learning based dose denoiser that can be plugged into a current GPU based
MC dose engine to enable real time MC dose calculation. We used two different
acceleration strategies to achieve this goal: 1) we applied voxel unshuffle and
voxel shuffle operators to decrease the input and output sizes without any
information loss, and 2) we decoupled the 3D volumetric convolution into a 2D
axial convolution and a 1D slice convolution. In addition, we used a weakly
supervised learning framework to train the network, which greatly reduces the
size of the required training dataset and thus enables fast fine tuning based
adaptation of the trained model to different radiation beams. Experimental
results show that the proposed denoiser can run in as little as 39 ms, which is
around 11.6 times faster than the baseline model. As a result, the whole MC
dose calculation pipeline can be finished within 0.15 seconds, including both
GPU MC dose calculation and deep learning based denoising, achieving the real
time efficiency needed for some radiotherapy applications, such as online
adaptive radiotherapy.
</p>
<a href="http://arxiv.org/abs/2011.14959" target="_blank">arXiv:2011.14959</a> [<a href="http://arxiv.org/pdf/2011.14959" target="_blank">pdf</a>]

<h2>CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. (arXiv:2012.00924v2 [cs.CV] UPDATED)</h2>
<h3>Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, Cewu Lu</h3>
<p>Estimating hand-object (HO) pose during interaction has been brought
remarkable growth in virtue of deep learning methods. Modeling the contact
between the hand and object properly is the key to construct a plausible grasp.
Yet, previous works usually focus on jointly estimating HO pose but not fully
explore the physical contact preserved in grasping. In this paper, we present
an explicit contact representation, Contact Potential Field (CPF) that models
each hand-object contact as a spring-mass system. Then we can refine a natural
grasp by minimizing the elastic energy w.r.t those systems. To recover CPF, we
also propose a learning-fitting hybrid framework named MIHO. Extensive
experiments on two public benchmarks have shown that our method can achieve
state-of-the-art in several reconstruction metrics, and allow us to produce
more physically plausible HO pose even when the ground-truth exhibits severe
interpenetration or disjointedness. Our code is available at
https://github.com/lixiny/CPF.
</p>
<a href="http://arxiv.org/abs/2012.00924" target="_blank">arXiv:2012.00924</a> [<a href="http://arxiv.org/pdf/2012.00924" target="_blank">pdf</a>]

<h2>Online Stochastic Optimization with Wasserstein Based Non-stationarity. (arXiv:2012.06961v2 [cs.LG] UPDATED)</h2>
<h3>Jiashuo Jiang, Xiaocheng Li, Jiawei Zhang</h3>
<p>We consider a general online stochastic optimization problem with multiple
budget constraints over a horizon of finite time periods. In each time period,
a reward function and multiple cost functions are revealed, and the decision
maker needs to specify an action from a convex and compact action set to
collect the reward and consume the budget. Each cost function corresponds to
the consumption of one budget. In each period, the reward and cost functions
are drawn from an unknown distribution, which is non-stationary across time.
The objective of the decision maker is to maximize the cumulative reward
subject to the budget constraints. This formulation captures a wide range of
applications including online linear programming and network revenue
management, among others. In this paper, we consider two settings: (i) a
data-driven setting where the true distribution is unknown but a prior estimate
(possibly inaccurate) is available; (ii) an uninformative setting where the
true distribution is completely unknown. We propose a unified
Wasserstein-distance based measure to quantify the inaccuracy of the prior
estimate in setting (i) and the non-stationarity of the system in setting (ii).
We show that the proposed measure leads to a necessary and sufficient condition
for the attainability of a sublinear regret in both settings. For setting (i),
we propose a new algorithm, which takes a primal-dual perspective and
integrates the prior information of the underlying distributions into an online
gradient descent procedure in the dual space. The algorithm also naturally
extends to the uninformative setting (ii). Under both settings, we show the
corresponding algorithm achieves a regret of optimal order. In numerical
experiments, we demonstrate how the proposed algorithms can be naturally
integrated with the re-solving technique to further boost the empirical
performance.
</p>
<a href="http://arxiv.org/abs/2012.06961" target="_blank">arXiv:2012.06961</a> [<a href="http://arxiv.org/pdf/2012.06961" target="_blank">pdf</a>]

<h2>Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v2 [cs.CV] UPDATED)</h2>
<h3>Xiangyun Zhao, Raviteja Vemulapalli, Philip Mansfield, Boqing Gong, Bradley Green, Lior Shapira, Ying Wu</h3>
<p>Collecting labeled data for the task of semantic segmentation is expensive
and time-consuming, as it requires dense pixel-level annotations. While recent
Convolutional Neural Network (CNN) based semantic segmentation approaches have
achieved impressive results by using large amounts of labeled training data,
their performance drops significantly as the amount of labeled data decreases.
This happens because deep CNNs trained with the de facto cross-entropy loss can
easily overfit to small amounts of labeled data. To address this issue, we
propose a simple and effective contrastive learning-based training strategy in
which we first pretrain the network using a pixel-wise class label-based
contrastive loss, and then fine-tune it using the cross-entropy loss. This
approach increases intra-class compactness and inter-class separability thereby
resulting in a better pixel classifier. We demonstrate the effectiveness of the
proposed training strategy in both fully-supervised and semi-supervised
settings using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our
results show that pretraining with label-based contrastive loss results in
large performance gains (more than 20% absolute improvement in some settings)
when the amount of labeled data is limited.
</p>
<a href="http://arxiv.org/abs/2012.06985" target="_blank">arXiv:2012.06985</a> [<a href="http://arxiv.org/pdf/2012.06985" target="_blank">pdf</a>]

<h2>FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction. (arXiv:2012.07999v3 [cs.CV] UPDATED)</h2>
<h3>ShahRukh Athar, Albert Pumarola, Francesc Moreno-Noguer, Dimitris Samaras</h3>
<p>Facial Expressions induce a variety of high-level details on the 3D face
geometry. For example, a smile causes the wrinkling of cheeks or the formation
of dimples, while being angry often causes wrinkling of the forehead. Morphable
Models (3DMMs) of the human face fail to capture such fine details in their
PCA-based representations and consequently cannot generate such details when
used to edit expressions. In this work, we introduce FaceDet3D, a
first-of-its-kind method that generates - from a single image - geometric
facial details that are consistent with any desired target expression. The
facial details are represented as a vertex displacement map and used then by a
Neural Renderer to photo-realistically render novel images of any single image
in any desired expression and view. The project website is:
this http URL
</p>
<a href="http://arxiv.org/abs/2012.07999" target="_blank">arXiv:2012.07999</a> [<a href="http://arxiv.org/pdf/2012.07999" target="_blank">pdf</a>]

<h2>General Policies, Serializations, and Planning Width. (arXiv:2012.08033v2 [cs.AI] UPDATED)</h2>
<h3>Blai Bonet, Hector Geffner</h3>
<p>It has been observed that in many of the benchmark planning domains, atomic
goals can be reached with a simple polynomial exploration procedure, called IW,
that runs in time exponential in the problem width. Such problems have indeed a
bounded width: a width that does not grow with the number of problem variables
and is often no greater than two. Yet, while the notion of width has become
part of the state-of-the-art planning algorithms like BFWS, there is still no
good explanation for why so many benchmark domains have bounded width. In this
work, we address this question by relating bounded width and serialized width
to ideas of generalized planning, where general policies aim to solve multiple
instances of a planning problem all at once. We show that bounded width is a
property of planning domains that admit optimal general policies in terms of
features that are explicitly or implicitly represented in the domain encoding.
The results are extended to much larger class of domains with bounded
serialized width where the general policies do not have to be optimal. The
study leads also to a new simple, meaningful, and expressive language for
specifying domain serializations in the form of policy sketches which can be
used for encoding domain control knowledge by hand or for learning it from
traces. The use of sketches and the meaning of the theoretical results are all
illustrated through a number of examples.
</p>
<a href="http://arxiv.org/abs/2012.08033" target="_blank">arXiv:2012.08033</a> [<a href="http://arxiv.org/pdf/2012.08033" target="_blank">pdf</a>]

<h2>Self-Supervised Sketch-to-Image Synthesis. (arXiv:2012.09290v2 [cs.CV] UPDATED)</h2>
<h3>Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal</h3>
<p>Imagining a colored realistic image from an arbitrarily drawn sketch is one
of the human capabilities that we eager machines to mimic. Unlike previous
methods that either requires the sketch-image pairs or utilize low-quantity
detected edges as sketches, we study the exemplar-based sketch-to-image (s2i)
synthesis task in a self-supervised learning manner, eliminating the necessity
of the paired sketch data. To this end, we first propose an unsupervised method
to efficiently synthesize line-sketches for general RGB-only datasets. With the
synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to
decouple the content/style features from sketches and RGB-images, and
synthesize images that are both content-faithful to the sketches and
style-consistent to the RGB-images. While prior works employ either the
cycle-consistence loss or dedicated attentional modules to enforce the
content/style fidelity, we show AE's superior performance with pure
self-supervisions. To further improve the synthesis quality in high resolution,
we also leverage an adversarial network to refine the details of synthetic
images. Extensive experiments on 1024*1024 resolution demonstrate a new
state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art
datasets. Moreover, with the proposed sketch generator, the model shows a
promising performance on style mixing and style transfer, which require
synthesized images to be both style-consistent and semantically meaningful. Our
code is available on
https://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch,
and please visit https://create.playform.io/my-projects?mode=sketch for an
online demo of our model.
</p>
<a href="http://arxiv.org/abs/2012.09290" target="_blank">arXiv:2012.09290</a> [<a href="http://arxiv.org/pdf/2012.09290" target="_blank">pdf</a>]

<h2>Predicting Events in MOBA Games: Dataset, Attribution, and Evaluation. (arXiv:2012.09424v2 [cs.AI] UPDATED)</h2>
<h3>Zelong Yang, Yan Wang, Piji Li, Shaobin Lin, Shuming Shi, Shao-Lun Huang</h3>
<p>The multiplayer online battle arena (MOBA) games have become increasingly
popular in recent years. Consequently, many efforts have been devoted to
providing pre-game or in-game predictions for them. However, these works are
limited in the following two aspects: 1) the lack of sufficient in-game
features; 2) the absence of interpretability in the prediction results. These
two limitations greatly restrict the practical performance and industrial
application of the current works. In this work, we collect and release a
large-scale dataset containing rich in-game features for the popular MOBA game
Honor of Kings. We then propose to predict four types of important events in an
interpretable way by attributing the predictions to the input features using
two gradient-based attribution methods: Integrated Gradients and SmoothGrad. To
evaluate the explanatory power of different models and attribution methods, a
fidelity-based evaluation metric is further proposed. Finally, we evaluate the
accuracy and Fidelity of several competitive methods on the collected dataset
to assess how well machines predict events in MOBA games.
</p>
<a href="http://arxiv.org/abs/2012.09424" target="_blank">arXiv:2012.09424</a> [<a href="http://arxiv.org/pdf/2012.09424" target="_blank">pdf</a>]

<h2>3D Object Classification on Partial Point Clouds: A Practical Perspective. (arXiv:2012.10042v2 [cs.CV] UPDATED)</h2>
<h3>Zelin Xu, Ke Chen, Tong Zhang, C. L. Philip Chen, Kui Jia</h3>
<p>A point cloud is a popular shape representation adopted in 3D object
classification, which covers the whole surface of an object and is usually well
aligned. However, such an assumption can be invalid in practice, as point
clouds collected in real-world scenarios are typically scanned from visible
object parts observed under arbitrary SO(3) viewpoint, which are thus
incomplete due to self and inter-object occlusion. In light of this, this paper
introduces a practical setting to classify partial point clouds of object
instances under any poses. Compared to the classification of complete object
point clouds, such a problem is made more challenging in view of geometric
similarities of local shape across object classes and intra-class
dissimilarities of geometries restricted by their observation view. We consider
that specifying the location of partial point clouds on their object surface is
essential to alleviate suffering from the aforementioned challenges, which can
be solved via an auxiliary task of 6D object pose estimation. To this end, a
novel algorithm in an alignment-classification manner is proposed in this
paper, which consists of an alignment module predicting object pose for the
rigid transformation of visible point clouds to their canonical pose and a
typical point classifier such as PointNet++ and DGCNN. Experiment results on
the popular ModelNet40 and ScanNet datasets, which are adapted to a single-view
partial setting, demonstrate the proposed method can outperform three
alternative schemes extended from representative point cloud classifiers for
complete point clouds.
</p>
<a href="http://arxiv.org/abs/2012.10042" target="_blank">arXiv:2012.10042</a> [<a href="http://arxiv.org/pdf/2012.10042" target="_blank">pdf</a>]

<h2>FcaNet: Frequency Channel Attention Networks. (arXiv:2012.11879v2 [cs.CV] UPDATED)</h2>
<h3>Zequn Qin, Pengyi Zhang, Fei Wu, Xi Li</h3>
<p>Attention mechanism, especially channel attention, has gained great success
in the computer vision field. Many works focus on how to design efficient
channel attention mechanisms while ignoring a fundamental problem, i.e., using
global average pooling (GAP) as the unquestionable pre-processing method. In
this work, we start from a different view and rethink channel attention using
frequency analysis. Based on the frequency analysis, we mathematically prove
that the conventional GAP is a special case of the feature decomposition in the
frequency domain. With the proof, we naturally generalize the pre-processing of
channel attention mechanism in the frequency domain and propose FcaNet with
novel multi-spectral channel attention. The proposed method is simple but
effective. We can change only one line of code in the calculation to implement
our method within existing channel attention methods. Moreover, the proposed
method achieves state-of-the-art results compared with other channel attention
methods on image classification, object detection, and instance segmentation
tasks. Our method could improve by 1.8% in terms of Top-1 accuracy on ImageNet
compared with the baseline SENet-50, with the same number of parameters and the
same computational cost. Our code and models will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.11879" target="_blank">arXiv:2012.11879</a> [<a href="http://arxiv.org/pdf/2012.11879" target="_blank">pdf</a>]

<h2>Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Deforming Scene from Monocular Video. (arXiv:2012.12247v2 [cs.CV] UPDATED)</h2>
<h3>Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh&#xf6;fer, Christoph Lassner, Christian Theobalt</h3>
<p>In this tech report, we present the current state of our ongoing work on
reconstructing Neural Radiance Fields (NERF) of general non-rigid scenes via
ray bending. Non-rigid NeRF (NR-NeRF) takes RGB images of a deforming object
(e.g., from a monocular video) as input and then learns a geometry and
appearance representation that not only allows to reconstruct the input
sequence but also to re-render any time step into novel camera views with high
fidelity. In particular, we show that a consumer-grade camera is sufficient to
synthesize convincing bullet-time videos of short and simple scenes. In
addition, the resulting representation enables correspondence estimation across
views and time, and provides rigidity scores for each point in the scene. We
urge the reader to watch the supplemental videos for qualitative results. We
will release our code.
</p>
<a href="http://arxiv.org/abs/2012.12247" target="_blank">arXiv:2012.12247</a> [<a href="http://arxiv.org/pdf/2012.12247" target="_blank">pdf</a>]

