---
title: Latest Deep Learning Papers
date: 2020-11-22 21:00:02 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (142 Articles)</h1>
<h2>Dual Contradistinctive Generative Autoencoder. (arXiv:2011.10063v1 [cs.CV])</h2>
<h3>Gaurav Parmar, Dacheng Li, Kwonjoon Lee, Zhuowen Tu</h3>
<p>We present a new generative autoencoder model with dual contradistinctive
losses to improve generative autoencoder that performs simultaneous inference
(reconstruction) and synthesis (sampling). Our model, named dual
contradistinctive generative autoencoder (DC-VAE), integrates an instance-level
discriminative loss (maintaining the instance-level fidelity for the
reconstruction/synthesis) with a set-level adversarial loss (encouraging the
set-level fidelity for there construction/synthesis), both being
contradistinctive. Extensive experimental results by DC-VAE across different
resolutions including 32x32, 64x64, 128x128, and 512x512 are reported. The two
contradistinctive losses in VAE work harmoniously in DC-VAE leading to a
significant qualitative and quantitative performance enhancement over the
baseline VAEs without architectural changes. State-of-the-art or competitive
results among generative autoencoders for image reconstruction, image
synthesis, image interpolation, and representation learning are observed.
DC-VAE is a general-purpose VAE model, applicable to a wide variety of
downstream tasks in computer vision and machine learning.
</p>
<a href="http://arxiv.org/abs/2011.10063" target="_blank">arXiv:2011.10063</a> [<a href="http://arxiv.org/pdf/2011.10063" target="_blank">pdf</a>]

<h2>Anderson acceleration of coordinate descent. (arXiv:2011.10065v1 [stat.ML])</h2>
<h3>Quentin Bertrand, Mathurin Massias</h3>
<p>Acceleration of first order methods is mainly obtained via inertial
techniques \`a la Nesterov, or via nonlinear extrapolation. The latter has
known a recent surge of interest, with successful applications to gradient and
proximal gradient techniques. On multiple Machine Learning problems, coordinate
descent achieves performance significantly superior to full-gradient methods.
Speeding up coordinate descent in practice is not easy: inertially accelerated
versions of coordinate descent are theoretically accelerated, but might not
always lead to practical speed-ups. We propose an accelerated version of
coordinate descent using extrapolation, showing considerable speed up in
practice, compared to inertial accelerated coordinate descent and extrapolated
(proximal) gradient descent. Experiments on least squares, Lasso, elastic net
and logistic regression validate the approach.
</p>
<a href="http://arxiv.org/abs/2011.10065" target="_blank">arXiv:2011.10065</a> [<a href="http://arxiv.org/pdf/2011.10065" target="_blank">pdf</a>]

<h2>Lidar-based exploration and discretization for mobile robot planning. (arXiv:2011.10066v1 [cs.RO])</h2>
<h3>Yuxiao Chen, Andrew Singletary, Aaron D. Ames</h3>
<p>In robotic applications, the control, and actuation deal with a continuous
description of the system and environment, while high-level planning usually
works with a discrete description. This paper considers the problem of bridging
the low-level control and high-level planning for robotic systems via sensor
data. In particular, we propose a discretization algorithm that identifies free
polytopes via lidar point cloud data. A transition graph is then constructed
where each node corresponds to a free polytope and two nodes are connected with
an edge if the two corresponding free polytopes intersect. Furthermore, a
distance measure is associated with each edge, which allows for the assessment
of quality (or cost) of the transition for high-level planning. For the
low-level control, the free polytopes act as a convenient encoding of the
environment and allow for the planning of collision-free trajectories that
realizes the high-level plan. The results are demonstrated in high-fidelity ROS
simulations and experiments with a drone and a Segway.
</p>
<a href="http://arxiv.org/abs/2011.10066" target="_blank">arXiv:2011.10066</a> [<a href="http://arxiv.org/pdf/2011.10066" target="_blank">pdf</a>]

<h2>Error-Bounded Correction of Noisy Labels. (arXiv:2011.10077v1 [cs.CV])</h2>
<h3>Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, Chao Chen</h3>
<p>To collect large scale annotated data, it is inevitable to introduce label
noise, i.e., incorrect class labels. To be robust against label noise, many
successful methods rely on the noisy classifiers (i.e., models trained on the
noisy training data) to determine whether a label is trustworthy. However, it
remains unknown why this heuristic works well in practice. In this paper, we
provide the first theoretical explanation for these methods. We prove that the
prediction of a noisy classifier can indeed be a good indicator of whether the
label of a training data is clean. Based on the theoretical result, we propose
a novel algorithm that corrects the labels based on the noisy classifier
prediction. The corrected labels are consistent with the true Bayesian optimal
classifier with high probability. We incorporate our label correction algorithm
into the training of deep neural networks and train models that achieve
superior testing performance on multiple public datasets.
</p>
<a href="http://arxiv.org/abs/2011.10077" target="_blank">arXiv:2011.10077</a> [<a href="http://arxiv.org/pdf/2011.10077" target="_blank">pdf</a>]

<h2>Hybrid Consistency Training with Prototype Adaptation for Few-Shot Learning. (arXiv:2011.10082v1 [cs.CV])</h2>
<h3>Meng Ye, Xiao Lin, Giedrius Burachas, Ajay Divakaran, Yi Yao</h3>
<p>Few-Shot Learning (FSL) aims to improve a model's generalization capability
in low data regimes. Recent FSL works have made steady progress via metric
learning, meta learning, representation learning, etc. However, FSL remains
challenging due to the following longstanding difficulties. 1) The seen and
unseen classes are disjoint, resulting in a distribution shift between training
and testing. 2) During testing, labeled data of previously unseen classes is
sparse, making it difficult to reliably extrapolate from labeled support
examples to unlabeled query examples. To tackle the first challenge, we
introduce Hybrid Consistency Training to jointly leverage interpolation
consistency, including interpolating hidden features, that imposes linear
behavior locally and data augmentation consistency that learns robust
embeddings against sample variations. As for the second challenge, we use
unlabeled examples to iteratively normalize features and adapt prototypes, as
opposed to commonly used one-time update, for more reliable prototype-based
transductive inference. We show that our method generates a 2% to 5%
improvement over the state-of-the-art methods with similar backbones on five
FSL datasets and, more notably, a 7% to 8% improvement for more challenging
cross-domain FSL.
</p>
<a href="http://arxiv.org/abs/2011.10082" target="_blank">arXiv:2011.10082</a> [<a href="http://arxiv.org/pdf/2011.10082" target="_blank">pdf</a>]

<h2>Classification by Attention: Scene Graph Classification with Prior Knowledge. (arXiv:2011.10084v1 [cs.CV])</h2>
<h3>Sahand Sharifzadeh, Sina Moayed Baharlou, Volker Tresp</h3>
<p>A main challenge in scene graph classification is that the appearance of
objects and relations can be significantly different from one image to another.
Previous works have addressed this by relational reasoning over all objects in
an image, or incorporating prior knowledge into classification. Unlike previous
works, we do not consider separate models for the perception and prior
knowledge. Instead, we take a multi-task learning approach, where the
classification is implemented as an attention layer. This allows for the prior
knowledge to emerge and propagate within the perception model. By enforcing the
model to also represent the prior, we achieve a strong inductive bias. We show
that our model can accurately generate commonsense knowledge and that the
iterative injection of this knowledge to scene representations leads to a
significantly higher classification performance. Additionally, our model can be
fine-tuned on external knowledge given as triples. When combined with
self-supervised learning, this leads to accurate predictions with 1% of
annotated images only.
</p>
<a href="http://arxiv.org/abs/2011.10084" target="_blank">arXiv:2011.10084</a> [<a href="http://arxiv.org/pdf/2011.10084" target="_blank">pdf</a>]

<h2>Logically Consistent Loss for Visual Question Answering. (arXiv:2011.10094v1 [cs.CV])</h2>
<h3>Anh-Cat Le-Ngo, Truyen Tran, Santu Rana, Sunil Gupta, Svetha Venkatesh</h3>
<p>Given an image, a back-ground knowledge, and a set of questions about an
object, human learners answer the questions very consistently regardless of
question forms and semantic tasks. The current advancement in neural-network
based Visual Question Answering (VQA), despite their impressive performance,
cannot ensure such consistency due to identically distribution (i.i.d.)
assumption. We propose a new model-agnostic logic constraint to tackle this
issue by formulating a logically consistent loss in the multi-task learning
framework as well as a data organisation called family-batch and hybrid-batch.
To demonstrate usefulness of this proposal, we train and evaluate MAC-net based
VQA machines with and without the proposed logically consistent loss and the
proposed data organization. The experiments confirm that the proposed loss
formulae and introduction of hybrid-batch leads to more consistency as well as
better performance. Though the proposed approach is tested with MAC-net, it can
be utilised in any other QA methods whenever the logical consistency between
answers exist.
</p>
<a href="http://arxiv.org/abs/2011.10094" target="_blank">arXiv:2011.10094</a> [<a href="http://arxiv.org/pdf/2011.10094" target="_blank">pdf</a>]

<h2>The Fundamental Principles of Reproducibility. (arXiv:2011.10098v1 [cs.LG])</h2>
<h3>Odd Erik Gundersen</h3>
<p>Reproducibility is a confused terminology. In this paper, I take a
fundamental view on reproducibility rooted in the scientific method. The
scientific method is analysed and characterised in order to develop the
terminology required to define reproducibility. Further, the literature on
reproducibility and replication is surveyed, and experiments are modeled as
tasks and problem solving methods. Machine learning is used to exemplify the
described approach. Based on the analysis, reproducibility is defined and three
different types of reproducibility as well as four degrees of reproducibility
are specified.
</p>
<a href="http://arxiv.org/abs/2011.10098" target="_blank">arXiv:2011.10098</a> [<a href="http://arxiv.org/pdf/2011.10098" target="_blank">pdf</a>]

<h2>Efficient Consensus Model based on Proximal Gradient Method applied to Convolutional Sparse Problems. (arXiv:2011.10100v1 [cs.LG])</h2>
<h3>Gustavo Silva, Paul Rodriguez</h3>
<p>Convolutional sparse representation (CSR), shift-invariant model for inverse
problems, has gained much attention in the fields of signal/image processing,
machine learning and computer vision. The most challenging problems in CSR
implies the minimization of a composite function of the form $min_x \sum_i
f_i(x) + g(x)$, where a direct and low-cost solution can be difficult to
achieve. However, it has been reported that semi-distributed formulations such
as ADMM consensus can provide important computational benefits. In the present
work, we derive and detail a thorough theoretical analysis of an efficient
consensus algorithm based on proximal gradient (PG) approach. The effectiveness
of the proposed algorithm with respect to its ADMM counterpart is primarily
assessed in the classic convolutional dictionary learning problem. Furthermore,
our consensus method, which is generically structured, can be used to solve
other optimization problems, where a sum of convex functions with a
regularization term share a single global variable. As an example, the proposed
algorithm is also applied to another particular convolutional problem for the
anomaly detection task.
</p>
<a href="http://arxiv.org/abs/2011.10100" target="_blank">arXiv:2011.10100</a> [<a href="http://arxiv.org/pdf/2011.10100" target="_blank">pdf</a>]

<h2>Online Multi-Object Tracking with delta-GLMB Filter based on Occlusion and Identity Switch Handling. (arXiv:2011.10111v1 [cs.CV])</h2>
<h3>Mohammadjavad Abbaspour, Mohammad Ali Masnadi-Shirazi</h3>
<p>In this paper, we propose an online multi-object tracking (MOT) method in a
delta Generalized Labeled Multi-Bernoulli (delta-GLMB) filter framework to
address occlusion and miss-detection issues, reduce false alarms, and recover
identity switch (ID switch). To handle occlusion and miss-detection issues, we
propose a measurement-to-disappeared track association method based on one-step
delta-GLMB filter, so it is possible to manage these difficulties by jointly
processing occluded or miss-detected objects. This part of proposed method is
based on a proposed similarity metric which is responsible for defining the
weight of hypothesized reappeared tracks. We also extend the delta-GLMB filter
to efficiently recover switched IDs using the cardinality density, size and
color features of the hypothesized tracks. We also propose a novel birth model
to achieve more effective clutter removal performance. In both
occlusion/miss-detection handler and newly-birthed object detector sections of
the proposed method, unassigned measurements play a significant role, since
they are used as the candidates for reappeared or birth objects. In addition,
we perform an ablation study which confirms the effectiveness of our
contributions in comparison with the baseline method. We evaluate the proposed
method on well-known and publicly available MOT15 and MOT17 test datasets which
are focused on pedestrian tracking. Experimental results show that the proposed
tracker performs better or at least at the same level of the state-of-the-art
online and offline MOT methods. It effectively handles the occlusion and ID
switch issues and reduces false alarms as well.
</p>
<a href="http://arxiv.org/abs/2011.10111" target="_blank">arXiv:2011.10111</a> [<a href="http://arxiv.org/pdf/2011.10111" target="_blank">pdf</a>]

<h2>Deep Learning with a Single Neuron: Folding a Deep Neural Network in Time using Feedback-Modulated Delay Loops. (arXiv:2011.10115v1 [cs.LG])</h2>
<h3>Florian Stelzer (1 and 2), Andr&#xe9; R&#xf6;hm (3), Raul Vicente (4), Ingo Fischer (3), Serhiy Yanchuk (1) ((1) Institute of Mathematics, Technische Universit&#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&#xe4;t zu Berlin, Germany, (3) Instituto de F&#xed;sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Spain, (4) Institute of Computer Science, University of Tartu, Tartu, Estonia)</h3>
<p>Deep neural networks are among the most widely applied machine learning tools
showing outstanding performance in a broad range of tasks. We present a method
for folding a deep neural network of arbitrary size into a single neuron with
multiple time-delayed feedback loops. This single-neuron deep neural network
comprises only a single nonlinearity and appropriately adjusted modulations of
the feedback signals. The network states emerge in time as a temporal unfolding
of the neuron's dynamics. By adjusting the feedback-modulation within the
loops, we adapt the network's connection weights. These connection weights are
determined via a modified back-propagation algorithm that we designed for such
types of networks. Our approach fully recovers standard Deep Neural Networks
(DNN), encompasses sparse DNNs, and extends the DNN concept toward dynamical
systems implementations. The new method, which we call Folded-in-time DNN
(Fit-DNN), exhibits promising performance in a set of benchmark tasks.
</p>
<a href="http://arxiv.org/abs/2011.10115" target="_blank">arXiv:2011.10115</a> [<a href="http://arxiv.org/pdf/2011.10115" target="_blank">pdf</a>]

<h2>Batteries, camera, action! Learning a semantic control space for expressive robot cinematography. (arXiv:2011.10118v1 [cs.CV])</h2>
<h3>Rogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam, Jessica Hodgins</h3>
<p>Aerial vehicles are revolutionizing the way film-makers can capture shots of
actors by composing novel aerial and dynamic viewpoints. However, despite great
advancements in autonomous flight technology, generating expressive camera
behaviors is still a challenge and requires non-technical users to edit a large
number of unintuitive control parameters. In this work we develop a data-driven
framework that enables editing of these complex camera positioning parameters
in a semantic space (e.g. calm, enjoyable, establishing). First, we generate a
database of video clips with a diverse range of shots in a photo-realistic
simulator, and use hundreds of participants in a crowd-sourcing framework to
obtain scores for a set of semantic descriptors for each clip. Next, we analyze
correlations between descriptors and build a semantic control space based on
cinematography guidelines and human perception studies. Finally, we learn a
generative model that can map a set of desired semantic video descriptors into
low-level camera trajectory parameters. We evaluate our system by demonstrating
that our model successfully generates shots that are rated by participants as
having the expected degrees of expression for each descriptor. We also show
that our models generalize to different scenes in both simulation and
real-world experiments. Supplementary video: https://youtu.be/6WX2yEUE9_k
</p>
<a href="http://arxiv.org/abs/2011.10118" target="_blank">arXiv:2011.10118</a> [<a href="http://arxiv.org/pdf/2011.10118" target="_blank">pdf</a>]

<h2>VLG-Net: Video-Language Graph Matching Network for Video Grounding. (arXiv:2011.10132v1 [cs.CV])</h2>
<h3>Sisi Qu, Mattia Soldan, Mengmeng Xu, Jesper Tegner, Bernard Ghanem</h3>
<p>Grounding language queries in videos aims at identifying the time interval
(or moment) semantically relevant to a language query. The solution to this
challenging task demands the understanding of videos' and queries' semantic
content and the fine-grained reasoning about their multi-modal interactions.
Our key idea is to recast this challenge into an algorithmic graph matching
problem. Fueled by recent advances in Graph Neural Networks, we propose to
leverage Graph Convolutional Networks to model video and textual information as
well as their semantic alignment. To enable the mutual exchange of information
across the domains, we design a novel Video-Language Graph Matching Network
(VLG-Net) to match video and query graphs. Core ingredients include
representation graphs, built on top of video snippets and query tokens
separately, which are used for modeling the intra-modality relationships. A
Graph Matching layer is adopted for cross-modal context modeling and
multi-modal fusion. Finally, moment candidates are created using masked moment
attention pooling by fusing the moment's enriched snippet features. We
demonstrate superior performance over state-of-the-art grounding methods on
three widely used datasets for temporal localization of moments in videos with
natural language queries: ActivityNet-Captions, TACoS, and DiDeMo.
</p>
<a href="http://arxiv.org/abs/2011.10132" target="_blank">arXiv:2011.10132</a> [<a href="http://arxiv.org/pdf/2011.10132" target="_blank">pdf</a>]

<h2>Provable Multi-Objective Reinforcement Learning with Generative Models. (arXiv:2011.10134v1 [cs.LG])</h2>
<h3>Dongruo Zhou, Jiahao Chen, Quanquan Gu</h3>
<p>Multi-objective reinforcement learning (MORL) is an extension of ordinary,
single-objective reinforcement learning (RL) that is applicable to many real
world tasks where multiple objectives exist without known relative costs. We
study the problem of single policy MORL, which learns an optimal policy given
the preference of objectives. Existing methods require strong assumptions such
as exact knowledge of the multi-objective Markov decision process, and are
analyzed in the limit of infinite data and time. We propose a new algorithm
called model-based envelop value iteration (EVI), which generalizes the
enveloped multi-objective $Q$-learning algorithm in Yang, 2019. Our method can
learn a near-optimal value function with polynomial sample complexity and
linear convergence speed. To the best of our knowledge, this is the first
finite-sample analysis of MORL algorithms.
</p>
<a href="http://arxiv.org/abs/2011.10134" target="_blank">arXiv:2011.10134</a> [<a href="http://arxiv.org/pdf/2011.10134" target="_blank">pdf</a>]

<h2>Cooperating RPN's Improve Few-Shot Object Detection. (arXiv:2011.10142v1 [cs.CV])</h2>
<h3>Weilin Zhang, Yu-Xiong Wang, David A. Forsyth</h3>
<p>Learning to detect an object in an image from very few training examples -
few-shot object detection - is challenging, because the classifier that sees
proposal boxes has very little training data. A particularly challenging
training regime occurs when there are one or two training examples. In this
case, if the region proposal network (RPN) misses even one high
intersection-over-union (IOU) training box, the classifier's model of how
object appearance varies can be severely impacted. We use multiple distinct yet
cooperating RPN's. Our RPN's are trained to be different, but not too
different; doing so yields significant performance improvements over state of
the art for COCO and PASCAL VOC in the very few-shot setting. This effect
appears to be independent of the choice of classifier or dataset.
</p>
<a href="http://arxiv.org/abs/2011.10142" target="_blank">arXiv:2011.10142</a> [<a href="http://arxiv.org/pdf/2011.10142" target="_blank">pdf</a>]

<h2>Interpretable and Transferable Models to Understand the Impact of Lockdown Measures on Local Air Quality. (arXiv:2011.10144v1 [cs.LG])</h2>
<h3>Johanna Einsiedler, Yun Cheng, Franz Papst, Olga Saukh</h3>
<p>The COVID-19 related lockdown measures offer a unique opportunity to
understand how changes in economic activity and traffic affect ambient air
quality and how much pollution reduction potential can the society offer
through digitalization and mobilitylimiting policies. In this work, we estimate
pollution reduction over the lockdown period by using the measurements from
ground air pollution monitoring stations, training a long-term prediction model
and comparing its predictions to measured values over the lockdown month.We
show that our models achieve state-of-the-art performance on the data from air
pollution measurement stations in Switzerland and in China: evaluate up to
-15.8% / +34.4% change in NO2 / PM10 in Zurich; -35.3 % / -3.5 % and -42.4 % /
-34.7 % in NO2 / PM2.5 in Beijing and Wuhan respectively. Our reduction
estimates are consistent with recent publications, yet in contrast to prior
works, our method takes local weather into account. What can we learn from
pollution emissions during lockdown? The lockdown period was too short to train
meaningful models from scratch. To tackle this problem, we use transfer
learning to newly fit only traffic-dependent variables. We show that the
resulting models are accurate, suitable for an analysis of the post-lockdown
period and capable of estimating the future air pollution reduction potential.
</p>
<a href="http://arxiv.org/abs/2011.10144" target="_blank">arXiv:2011.10144</a> [<a href="http://arxiv.org/pdf/2011.10144" target="_blank">pdf</a>]

<h2>FlowStep3D: Model Unrolling for Self-Supervised Scene Flow Estimation. (arXiv:2011.10147v1 [cs.CV])</h2>
<h3>Yair Kittenplon, Yonina C. Eldar, Dan Raviv</h3>
<p>Estimating the 3D motion of points in a scene, known as scene flow, is a core
problem in computer vision. Traditional learning-based methods designed to
learn end-to-end 3D flow often suffer from poor generalization. Here we present
a recurrent architecture that learns a single step of an unrolled iterative
alignment procedure for refining scene flow predictions. Inspired by classical
algorithms, we demonstrate iterative convergence toward the solution using
strong regularization. The proposed method can handle sizeable temporal
deformations and suggests a slimmer architecture than competitive all-to-all
correlation approaches. Trained on FlyingThings3D synthetic data only, our
network successfully generalizes to real scans, outperforming all existing
methods by a large margin on the KITTI self-supervised benchmark.
</p>
<a href="http://arxiv.org/abs/2011.10147" target="_blank">arXiv:2011.10147</a> [<a href="http://arxiv.org/pdf/2011.10147" target="_blank">pdf</a>]

<h2>Robot Gaining Accurate Pouring Skills through Self-Supervised Learning and Generalization. (arXiv:2011.10150v1 [cs.RO])</h2>
<h3>Yongqiang Huang, Juan Wilches, Yu Sun</h3>
<p>Pouring is one of the most commonly executed tasks in humans' daily lives,
whose accuracy is affected by multiple factors, including the type of material
to be poured and the geometry of the source and receiving containers. In this
work, we propose a self-supervised learning approach that learns the pouring
dynamics, pouring motion, and outcomes from unsupervised demonstrations for
accurate pouring. The learned pouring model is then generalized by
self-supervised practicing to different conditions such as using unaccustomed
pouring cups. We have evaluated the proposed approach first with one container
from the training set and four new but similar containers. The proposed
approach achieved better pouring accuracy than a regular human with a similar
pouring speed for all five cups. Both the accuracy and pouring speed outperform
state-of-the-art works. We have also evaluated the proposed self-supervised
generalization approach using unaccustomed containers that are far different
from the ones in the training set. The self-supervised generalization reduces
the pouring error of the unaccustomed containers to the desired accuracy level.
</p>
<a href="http://arxiv.org/abs/2011.10150" target="_blank">arXiv:2011.10150</a> [<a href="http://arxiv.org/pdf/2011.10150" target="_blank">pdf</a>]

<h2>An Efficient End-to-End Deep Learning Training Framework via Fine-Grained Pattern-Based Pruning. (arXiv:2011.10170v1 [cs.CV])</h2>
<h3>Chengming Zhang, Geng Yuan, Wei Niu, Jiannan Tian, Sian Jin, Donglin Zhuang, Zhe Jiang, Yanzhi Wang, Bin Ren, Shuaiwen Leon Song, Dingwen Tao</h3>
<p>Convolutional neural networks (CNNs) are becoming increasingly deeper, wider,
and non-linear because of the growing demand on prediction accuracy and
analysis quality. The wide and deep CNNs, however, require a large amount of
computing resources and processing time. Many previous works have studied model
pruning to improve inference performance, but little work has been done for
effectively reducing training cost. In this paper, we propose ClickTrain: an
efficient and accurate end-to-end training and pruning framework for CNNs.
Different from the existing pruning-during-training work, ClickTrain provides
higher model accuracy and compression ratio via fine-grained
architecture-preserving pruning. By leveraging pattern-based pruning with our
proposed novel accurate weight importance estimation, dynamic pattern
generation and selection, and compiler-assisted computation optimizations,
ClickTrain generates highly accurate and fast pruned CNN models for direct
deployment without any time overhead, compared with the baseline training.
ClickTrain also reduces the end-to-end time cost of the state-of-the-art
pruning-after-training methods by up to about 67% with comparable accuracy and
compression ratio. Moreover, compared with the state-of-the-art
pruning-during-training approach, ClickTrain reduces the accuracy drop by up to
2.1% and improves the compression ratio by up to 2.2X on the tested datasets,
under similar limited training time.
</p>
<a href="http://arxiv.org/abs/2011.10170" target="_blank">arXiv:2011.10170</a> [<a href="http://arxiv.org/pdf/2011.10170" target="_blank">pdf</a>]

<h2>FLAVA: Find, Localize, Adjust and Verify to Annotate LiDAR-Based Point Clouds. (arXiv:2011.10174v1 [cs.RO])</h2>
<h3>Tai Wang, Conghui He, Zhe Wang, Jianping Shi, Dahua Lin</h3>
<p>Recent years have witnessed the rapid progress of perception algorithms on
top of LiDAR, a widely adopted sensor for autonomous driving systems. These
LiDAR-based solutions are typically data hungry, requiring a large amount of
data to be labeled for training and evaluation. However, annotating this kind
of data is very challenging due to the sparsity and irregularity of point
clouds and more complex interaction involved in this procedure. To tackle this
problem, we propose FLAVA, a systematic approach to minimizing human
interaction in the annotation process. Specifically, we divide the annotation
pipeline into four parts: find, localize, adjust and verify. In addition, we
carefully design the UI for different stages of the annotation procedure, thus
keeping the annotators to focus on the aspects that are most important to each
stage. Furthermore, our system also greatly reduces the amount of interaction
by introducing a light-weight yet effective mechanism to propagate the
annotation results. Experimental results show that our method can remarkably
accelerate the procedure and improve the annotation quality.
</p>
<a href="http://arxiv.org/abs/2011.10174" target="_blank">arXiv:2011.10174</a> [<a href="http://arxiv.org/pdf/2011.10174" target="_blank">pdf</a>]

<h2>ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis. (arXiv:2011.10185v1 [cs.CV])</h2>
<h3>Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Chunguo Li, Luxi Yang</h3>
<p>Deep Convolutional Neural Networks (CNNs) are powerful models that have
achieved excellent performance on difficult computer vision tasks. Although
CNNS perform well whenever large labeled training samples are available, they
work badly on video frame synthesis due to objects deforming and moving, scene
lighting changes, and cameras moving in video sequence. In this paper, we
present a novel and general end-to-end architecture, called convolutional
Transformer or ConvTransformer, for video frame sequence learning and video
frame synthesis. The core ingredient of ConvTransformer is the proposed
attention layer, i.e., multi-head convolutional self-attention, that learns the
sequential dependence of video sequence. Our method ConvTransformer uses an
encoder, built upon multi-head convolutional self-attention layers, to map the
input sequence to a feature map sequence, and then another deep networks,
incorporating multi-head convolutional self-attention layers, decode the target
synthesized frames from the feature maps sequence. Experiments on video future
frame extrapolation task show ConvTransformer to be superior in quality while
being more parallelizable to recent approaches built upon convoltuional LSTM
(ConvLSTM). To the best of our knowledge, this is the first time that
ConvTransformer architecture is proposed and applied to video frame synthesis.
</p>
<a href="http://arxiv.org/abs/2011.10185" target="_blank">arXiv:2011.10185</a> [<a href="http://arxiv.org/pdf/2011.10185" target="_blank">pdf</a>]

<h2>MobileDepth: Efficient Monocular Depth Prediction on Mobile Devices. (arXiv:2011.10189v1 [cs.CV])</h2>
<h3>Yekai Wang</h3>
<p>Depth prediction is fundamental for many useful applications on computer
vision and robotic systems. On mobile phones, the performance of some useful
applications such as augmented reality, autofocus and so on could be enhanced
by accurate depth prediction. In this work, an efficient fully convolutional
network architecture for depth prediction has been proposed, which uses RegNetY
06 as the encoder and split-concatenate shuffle blocks as decoder. At the same
time, an appropriate combination of data augmentation, hyper-parameters and
loss functions to efficiently train the lightweight network has been provided.
Also, an Android application has been developed which can load CNN models to
predict depth map by the monocular images captured from the mobile camera and
evaluate the average latency and frame per second of the models. As a result,
the network achieves 82.7% {\delta}1 accuracy on NYU Depth v2 dataset and at
the same time, have only 62ms latency on ARM A76 CPUs so that it can predict
the depth map from the mobile camera in real-time.
</p>
<a href="http://arxiv.org/abs/2011.10189" target="_blank">arXiv:2011.10189</a> [<a href="http://arxiv.org/pdf/2011.10189" target="_blank">pdf</a>]

<h2>Action Duration Prediction for Segment-Level Alignment of Weakly-Labeled Videos. (arXiv:2011.10190v1 [cs.CV])</h2>
<h3>Reza Ghoddoosian, Saif Sayed, Vassilis Athitsos</h3>
<p>This paper focuses on weakly-supervised action alignment, where only the
ordered sequence of video-level actions is available for training. We propose a
novel Duration Network, which captures a short temporal window of the video and
learns to predict the remaining duration of a given action at any point in time
with a level of granularity based on the type of that action. Further, we
introduce a Segment-Level Beam Search to obtain the best alignment, that
maximizes our posterior probability. Segment-Level Beam Search efficiently
aligns actions by considering only a selected set of frames that have more
confident predictions. The experimental results show that our alignments for
long videos are more robust than existing models. Moreover, the proposed method
achieves state of the art results in certain cases on the popular Breakfast and
Hollywood Extended datasets.
</p>
<a href="http://arxiv.org/abs/2011.10190" target="_blank">arXiv:2011.10190</a> [<a href="http://arxiv.org/pdf/2011.10190" target="_blank">pdf</a>]

<h2>CLIPPER: A Graph-Theoretic Framework for Robust Data Association. (arXiv:2011.10202v1 [cs.RO])</h2>
<h3>Parker C. Lusk, Kaveh Fathian, Jonathan P. How</h3>
<p>We present CLIPPER (Consistent LInking, Pruning, and Pairwise Error
Rectification), a framework for robust data association in the presence of
noise and outliers. We formulate the problem in a graph-theoretic framework
using the notion of geometric consistency. State-of-the-art techniques that use
this framework utilize either combinatorial optimization techniques that do not
scale well to large-sized problems, or use heuristic approximations that yield
low accuracy in high-noise, high-outlier regimes. In contrast, CLIPPER uses a
relaxation of the combinatorial problem and returns solutions that are
guaranteed to correspond to the optima of the original problem. Low time
complexity is achieved with an efficient projected gradient ascent approach.
Experiments indicate that CLIPPER maintains a consistently low runtime of 15 ms
where exact methods can require up to 24 s at their peak, even on small-sized
problems with 200 associations. When evaluated on noisy point cloud
registration problems, CLIPPER achieves 100% precision and 98% recall in 90%
outlier regimes while competing algorithms begin degrading by 70% outliers. In
an instance of associating noisy points of the Stanford Bunny with 990 outlier
associations and only 10 inlier associations, CLIPPER successfully returns 8
inlier associations with 100% precision in 138 ms.
</p>
<a href="http://arxiv.org/abs/2011.10202" target="_blank">arXiv:2011.10202</a> [<a href="http://arxiv.org/pdf/2011.10202" target="_blank">pdf</a>]

<h2>Sequential Targeting: an incremental learning approach for data imbalance in text classification. (arXiv:2011.10216v1 [cs.LG])</h2>
<h3>Joel Jang, Yoonjeon Kim, Kyoungho Choi, Sungho Suh</h3>
<p>Classification tasks require a balanced distribution of data to ensure the
learner to be trained to generalize over all classes. In real-world datasets,
however, the number of instances vary substantially among classes. This
typically leads to a learner that promotes bias towards the majority group due
to its dominating property. Therefore, methods to handle imbalanced datasets
are crucial for alleviating distributional skews and fully utilizing the
under-represented data, especially in text classification. While addressing the
imbalance in text data, most methods utilize sampling methods on the numerical
representation of the data, which limits its efficiency on how effective the
representation is. We propose a novel training method, Sequential
Targeting(ST), independent of the effectiveness of the representation method,
which enforces an incremental learning setting by splitting the data into
mutually exclusive subsets and training the learner adaptively. To address
problems that arise within incremental learning, we apply elastic weight
consolidation. We demonstrate the effectiveness of our method through
experiments on simulated benchmark datasets (IMDB) and data collected from
NAVER.
</p>
<a href="http://arxiv.org/abs/2011.10216" target="_blank">arXiv:2011.10216</a> [<a href="http://arxiv.org/pdf/2011.10216" target="_blank">pdf</a>]

<h2>DoDNet: Learning to segment multi-organ and tumors from multiple partially labeled datasets. (arXiv:2011.10217v1 [cs.CV])</h2>
<h3>Jianpeng Zhang, Yutong Xie, Yong Xia, Chunhua Shen</h3>
<p>Due to the intensive cost of labor and expertise in annotating 3D medical
images at a voxel level, most benchmark datasets are equipped with the
annotations of only one type of organs and/or tumors, resulting in the
so-called partially labeling issue. To address this, we propose a dynamic
on-demand network (DoDNet) that learns to segment multiple organs and tumors on
partially labeled datasets. DoDNet consists of a shared encoder-decoder
architecture, a task encoding module, a controller for generating dynamic
convolution filters, and a single but dynamic segmentation head. The
information of the current segmentation task is encoded as a task-aware prior
to tell the model what the task is expected to solve. Different from existing
approaches which fix kernels after training, the kernels in dynamic head are
generated adaptively by the controller, conditioned on both input image and
assigned task. Thus, DoDNet is able to segment multiple organs and tumors, as
done by multiple networks or a multi-head network, in a much efficient and
flexible manner. We have created a large-scale partially labeled dataset,
termed MOTS, and demonstrated the superior performance of our DoDNet over other
competitors on seven organ and tumor segmentation tasks. We also transferred
the weights pre-trained on MOTS to a downstream multi-organ segmentation task
and achieved state-of-the-art performance. This study provides a general 3D
medical image segmentation model that has been pre-trained on a large-scale
partially labelled dataset and can be extended (after fine-tuning) to
downstream volumetric medical data segmentation tasks. The dataset and code
areavailableat: https://git.io/DoDNet
</p>
<a href="http://arxiv.org/abs/2011.10217" target="_blank">arXiv:2011.10217</a> [<a href="http://arxiv.org/pdf/2011.10217" target="_blank">pdf</a>]

<h2>Optimizing Approximate Leave-one-out Cross-validation to Tune Hyperparameters. (arXiv:2011.10218v1 [stat.ML])</h2>
<h3>Ryan Burn</h3>
<p>For a large class of regularized models, leave-one-out cross-validation can
be efficiently estimated with an approximate leave-one-out formula (ALO). We
consider the problem of adjusting hyperparameters so as to optimize ALO. We
derive efficient formulas to compute the gradient and hessian of ALO and show
how to apply a second-order optimizer to find hyperparameters. We demonstrate
the usefulness of the proposed approach by finding hyperparameters for
regularized logistic regression and ridge regression on various real-world data
sets.
</p>
<a href="http://arxiv.org/abs/2011.10218" target="_blank">arXiv:2011.10218</a> [<a href="http://arxiv.org/pdf/2011.10218" target="_blank">pdf</a>]

<h2>Certified Monotonic Neural Networks. (arXiv:2011.10219v1 [cs.LG])</h2>
<h3>Xingchao Liu, Xing Han, Na Zhang, Qiang Liu</h3>
<p>Learning monotonic models with respect to a subset of the inputs is a
desirable feature to effectively address the fairness, interpretability, and
generalization issues in practice. Existing methods for learning monotonic
neural networks either require specifically designed model structures to ensure
monotonicity, which can be too restrictive/complicated, or enforce monotonicity
by adjusting the learning process, which cannot provably guarantee the learned
model is monotonic on selected features. In this work, we propose to certify
the monotonicity of the general piece-wise linear neural networks by solving a
mixed integer linear programming problem.This provides a new general approach
for learning monotonic neural networks with arbitrary model structures. Our
method allows us to train neural networks with heuristic monotonicity
regularizations, and we can gradually increase the regularization magnitude
until the learned network is certified monotonic. Compared to prior works, our
approach does not require human-designed constraints on the weight space and
also yields more accurate approximation. Empirical studies on various datasets
demonstrate the efficiency of our approach over the state-of-the-art methods,
such as Deep Lattice Networks.
</p>
<a href="http://arxiv.org/abs/2011.10219" target="_blank">arXiv:2011.10219</a> [<a href="http://arxiv.org/pdf/2011.10219" target="_blank">pdf</a>]

<h2>Complexity Controlled Generative Adversarial Networks. (arXiv:2011.10223v1 [cs.LG])</h2>
<h3>Himanshu Pant, Jayadeva, Sumit Soman</h3>
<p>One of the issues faced in training Generative Adversarial Nets (GANs) and
their variants is the problem of mode collapse, wherein the training stability
in terms of the generative loss increases as more training data is used. In
this paper, we propose an alternative architecture via the Low-Complexity
Neural Network (LCNN), which attempts to learn models with low complexity. The
motivation is that controlling model complexity leads to models that do not
overfit the training data. We incorporate the LCNN loss function for GANs, Deep
Convolutional GANs (DCGANs) and Spectral Normalized GANs (SNGANs), in order to
develop hybrid architectures called the LCNN-GAN, LCNN-DCGAN and LCNN-SNGAN
respectively. On various large benchmark image datasets, we show that the use
of our proposed models results in stable training while avoiding the problem of
mode collapse, resulting in better training stability. We also show how the
learning behavior can be controlled by a hyperparameter in the LCNN functional,
which also provides an improved inception score.
</p>
<a href="http://arxiv.org/abs/2011.10223" target="_blank">arXiv:2011.10223</a> [<a href="http://arxiv.org/pdf/2011.10223" target="_blank">pdf</a>]

<h2>A global universality of two-layer neural networks with ReLU activations. (arXiv:2011.10225v1 [cs.LG])</h2>
<h3>Naoya Hatano, Masahiro Ikeda, Isao Ishikawa, Yoshihiro Sawano</h3>
<p>In the present study, we investigate a universality of neural networks, which
concerns a density of the set of two-layer neural networks in a function
spaces. There are many works that handle the convergence over compact sets. In
the present paper, we consider a global convergence by introducing a norm
suitably, so that our results will be uniform over any compact set.
</p>
<a href="http://arxiv.org/abs/2011.10225" target="_blank">arXiv:2011.10225</a> [<a href="http://arxiv.org/pdf/2011.10225" target="_blank">pdf</a>]

<h2>StressNet: Deep Learning to Predict Stress With Fracture Propagation in Brittle Materials. (arXiv:2011.10227v1 [cs.LG])</h2>
<h3>Yinan Wang, Diane Oyen, Weihong (Grace)Guo, Anishi Mehta, Cory Braker Scott, Nishant Panda, M. Giselle Fern&#xe1;ndez-Godino, Gowri Srinivasan, Xiaowei Yue</h3>
<p>Catastrophic failure in brittle materials is often due to the rapid growth
and coalescence of cracks aided by high internal stresses. Hence, accurate
prediction of maximum internal stress is critical to predicting time to failure
and improving the fracture resistance and reliability of materials. Existing
high-fidelity methods, such as the Finite-Discrete Element Model (FDEM), are
limited by their high computational cost. Therefore, to reduce computational
cost while preserving accuracy, a novel deep learning model, "StressNet," is
proposed to predict the entire sequence of maximum internal stress based on
fracture propagation and the initial stress data. More specifically, the
Temporal Independent Convolutional Neural Network (TI-CNN) is designed to
capture the spatial features of fractures like fracture path and spall regions,
and the Bidirectional Long Short-term Memory (Bi-LSTM) Network is adapted to
capture the temporal features. By fusing these features, the evolution in time
of the maximum internal stress can be accurately predicted. Moreover, an
adaptive loss function is designed by dynamically integrating the Mean Squared
Error (MSE) and the Mean Absolute Percentage Error (MAPE), to reflect the
fluctuations in maximum internal stress. After training, the proposed model is
able to compute accurate multi-step predictions of maximum internal stress in
approximately 20 seconds, as compared to the FDEM run time of 4 hours, with an
average MAPE of 2% relative to test data.
</p>
<a href="http://arxiv.org/abs/2011.10227" target="_blank">arXiv:2011.10227</a> [<a href="http://arxiv.org/pdf/2011.10227" target="_blank">pdf</a>]

<h2>Efficient Conditional Pre-training for Transfer Learning. (arXiv:2011.10231v1 [cs.CV])</h2>
<h3>Shuvam Chakraborty, Burak Uzkent, Kumar Ayush, Kumar Tanmay, Evan Sheehan, Stefano Ermon</h3>
<p>Almost all the state-of-the-art neural networks for computer vision tasks are
trained by (1) Pre-training on a large scale dataset and (2) finetuning on the
target dataset. This strategy helps reduce the dependency on the target dataset
and improves convergence rate and generalization on the target task. Although
pre-training on large scale datasets is very useful, its foremost disadvantage
is high training cost. To address this, we propose efficient target dataset
conditioned filtering methods to remove less relevant samples from the
pre-training dataset. Unlike prior work, we focus on efficiency, adaptability,
and flexibility in addition to performance. Additionally, we discover that
lowering image resolutions in the pre-training step offers a great trade-off
between cost and performance. We validate our techniques by pre-training on
ImageNet in both the unsupervised and supervised settings and finetuning on a
diverse collection of target datasets and tasks. Our proposed methods
drastically reduce pre-training cost and provide strong performance boosts.
</p>
<a href="http://arxiv.org/abs/2011.10231" target="_blank">arXiv:2011.10231</a> [<a href="http://arxiv.org/pdf/2011.10231" target="_blank">pdf</a>]

<h2>Deep Snapshot HDR Imaging Using Multi-Exposure Color Filter Array. (arXiv:2011.10232v1 [cs.CV])</h2>
<h3>Takeru Suda, Masayuki Tanaka, Yusuke Monno, Masatoshi Okutomi</h3>
<p>In this paper, we propose a deep snapshot high dynamic range (HDR) imaging
framework that can effectively reconstruct an HDR image from the RAW data
captured using a multi-exposure color filter array (ME-CFA), which consists of
a mosaic pattern of RGB filters with different exposure levels. To effectively
learn the HDR image reconstruction network, we introduce the idea of luminance
normalization that simultaneously enables effective loss computation and input
data normalization by considering relative local contrasts in the
"normalized-by-luminance" HDR domain. This idea makes it possible to equally
handle the errors in both bright and dark areas regardless of absolute
luminance levels, which significantly improves the visual image quality in a
tone-mapped domain. Experimental results using two public HDR image datasets
demonstrate that our framework outperforms other snapshot methods and produces
high-quality HDR images with fewer visual artifacts.
</p>
<a href="http://arxiv.org/abs/2011.10232" target="_blank">arXiv:2011.10232</a> [<a href="http://arxiv.org/pdf/2011.10232" target="_blank">pdf</a>]

<h2>GAN based ball screw drive picture database enlargement for failure classification. (arXiv:2011.10235v1 [cs.LG])</h2>
<h3>Tobias Schlagenhauf, Chenwei Sun, J&#xfc;rgen Fleischer</h3>
<p>The lack of reliable large datasets is one of the biggest difficulties of
using modern machine learning methods in the field of failure detection in the
manufacturing industry. In order to develop the function of failure
classification for ball screw surface, sufficient image data of surface
failures is necessary. When training a neural network model based on a small
dataset, the trained model may lack the generalization ability and may perform
poorly in practice. The main goal of this paper is to generate synthetic images
based on the generative adversarial network (GAN) to enlarge the image dataset
of ball screw surface failures. Pitting failure and rust failure are two
possible failure types on ball screw surface chosen in this paper to represent
the surface failure classes. The quality and diversity of generated images are
evaluated afterwards using qualitative methods including expert observation,
t-SNE visualization and the quantitative method of FID score. To verify whether
the GAN based generated images can increase failure classification performance,
the real image dataset was augmented and replaced by GAN based generated images
to do the classification task. The authors successfully created GAN based
images of ball screw surface failures which showed positive effect on
classification test performance.
</p>
<a href="http://arxiv.org/abs/2011.10235" target="_blank">arXiv:2011.10235</a> [<a href="http://arxiv.org/pdf/2011.10235" target="_blank">pdf</a>]

<h2>Shuffle and Learn: Minimizing Mutual Information for Unsupervised Hashing. (arXiv:2011.10239v1 [cs.CV])</h2>
<h3>Fangrui Liu, Zheng Liu</h3>
<p>Unsupervised binary representation allows fast data retrieval without any
annotations, enabling practical application like fast person re-identification
and multimedia retrieval. It is argued that conflicts in binary space are one
of the major barriers to high-performance unsupervised hashing as current
methods failed to capture the precise code conflicts in the full domain. A
novel relaxation method called Shuffle and Learn is proposed to tackle code
conflicts in the unsupervised hash. Approximated derivatives for joint
probability and the gradients for the binary layer are introduced to bridge the
update from the hash to the input. Proof on $\epsilon$-Convergence of joint
probability with approximated derivatives is provided to guarantee the
preciseness on update applied on the mutual information. The proposed algorithm
is carried out with iterative global updates to minimize mutual information,
diverging the code before regular unsupervised optimization. Experiments
suggest that the proposed method can relax the code optimization from local
optimum and help to generate binary representations that are more
discriminative and informative without any annotations. Performance benchmarks
on image retrieval with the unsupervised binary code are conducted on three
open datasets, and the model achieves state-of-the-art accuracy on image
retrieval task for all those datasets. Datasets and reproducible code are
provided.
</p>
<a href="http://arxiv.org/abs/2011.10239" target="_blank">arXiv:2011.10239</a> [<a href="http://arxiv.org/pdf/2011.10239" target="_blank">pdf</a>]

<h2>LAGNet: Logic-Aware Graph Network for Human Interaction Understanding. (arXiv:2011.10250v1 [cs.CV])</h2>
<h3>Zhenhua Wang, Jiajun Meng, Jin Zhou, Dongyan Guo, Guosheng Lin, Jianhua Zhang, Javen Qinfeng Shi, Shengyong Chen</h3>
<p>Compared with the progress made on human activity classification, much less
success has been achieved on human interaction understanding (HIU). Apart from
the latter task is much more challenging, the main cause is that recent
approaches learn human interactive relations via shallow graphical
representations, which is inadequate to model complicated human interactions.
In this paper, we propose a deep logic-aware graph network, which combines the
representative ability of graph attention and the rigorousness of logical
reasoning to facilitate human interaction understanding. Our network consists
of three components, a backbone CNN to extract image features, a graph network
to learn interactive relations among participants, and a logic-aware reasoning
module. Our key observation is that the first-order logic for HIU can be
embedded into higher-order energy functions, minimizing which delivers
logic-aware predictions. An efficient mean-field inference algorithm is
proposed, such that all modules of our network could be trained jointly in an
end-to-end way. Experimental results show that our approach achieves leading
performance on three existing benchmarks and a new challenging dataset crafted
by ourselves. Code will be publicly available.
</p>
<a href="http://arxiv.org/abs/2011.10250" target="_blank">arXiv:2011.10250</a> [<a href="http://arxiv.org/pdf/2011.10250" target="_blank">pdf</a>]

<h2>On-Device Text Image Super Resolution. (arXiv:2011.10251v1 [cs.CV])</h2>
<h3>Dhruval Jain, Arun D Prabhu, Gopi Ramena, Manoj Goyal, Debi Prasanna Mohanty, Sukumar Moharana, Naresh Purre</h3>
<p>Recent research on super-resolution (SR) has witnessed major developments
with the advancements of deep convolutional neural networks. There is a need
for information extraction from scenic text images or even document images on
device, most of which are low-resolution (LR) images. Therefore, SR becomes an
essential pre-processing step as Bicubic Upsampling, which is conventionally
present in smartphones, performs poorly on LR images. To give the user more
control over his privacy, and to reduce the carbon footprint by reducing the
overhead of cloud computing and hours of GPU usage, executing SR models on the
edge is a necessity in the recent times. There are various challenges in
running and optimizing a model on resource-constrained platforms like
smartphones. In this paper, we present a novel deep neural network that
reconstructs sharper character edges and thus boosts OCR confidence. The
proposed architecture not only achieves significant improvement in PSNR over
bicubic upsampling on various benchmark datasets but also runs with an average
inference time of 11.7 ms per image. We have outperformed state-of-the-art on
the Text330 dataset. We also achieve an OCR accuracy of 75.89% on the ICDAR
2015 TextSR dataset, where ground truth has an accuracy of 78.10%.
</p>
<a href="http://arxiv.org/abs/2011.10251" target="_blank">arXiv:2011.10251</a> [<a href="http://arxiv.org/pdf/2011.10251" target="_blank">pdf</a>]

<h2>Unbalanced Incomplete Multi-view Clustering via the Scheme of View Evolution: Weak Views are Meat; Strong Views do Eat. (arXiv:2011.10254v1 [cs.LG])</h2>
<h3>Xiang Fang, Yuchong Hu, Pan Zhou, Xiao-Yang Liu, Dapeng Oliver Wu</h3>
<p>Incomplete multi-view clustering is an important technique to deal with
real-world incomplete multi-view data. Previous works assume that all views
have the same incompleteness, i.e., balanced incompleteness. However, different
views often have distinct incompleteness, i.e., unbalanced incompleteness,
which results in strong views (low-incompleteness views) and weak views
(high-incompleteness views). The unbalanced incompleteness prevents us from
directly using the previous methods for clustering. In this paper, inspired by
the effective biological evolution theory, we design the novel scheme of view
evolution to cluster strong and weak views. Moreover, we propose an Unbalanced
Incomplete Multi-view Clustering method (UIMC), which is the first effective
method based on view evolution for unbalanced incomplete multi-view clustering.
Compared with previous methods, UIMC has two unique advantages: 1) it proposes
weighted multi-view subspace clustering to integrate these unbalanced
incomplete views, which effectively solves the unbalanced incomplete multi-view
problem; 2) it designs the low-rank and robust representation to recover the
data, which diminishes the impact of the incompleteness and noises. Extensive
experimental results demonstrate that UIMC improves the clustering performance
by up to 40% on three evaluation metrics over other state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.10254" target="_blank">arXiv:2011.10254</a> [<a href="http://arxiv.org/pdf/2011.10254" target="_blank">pdf</a>]

<h2>Cascade Attentive Dropout for Weakly Supervised Object Detection. (arXiv:2011.10258v1 [cs.CV])</h2>
<h3>Wenlong Gao, Ying Chen, Yong Peng</h3>
<p>Weakly supervised object detection (WSOD) aims to classify and locate objects
with only image-level supervision. Many WSOD approaches adopt multiple instance
learning as the initial model, which is prone to converge to the most
discriminative object regions while ignoring the whole object, and therefore
reduce the model detection performance. In this paper, a novel cascade
attentive dropout strategy is proposed to alleviate the part domination
problem, together with an improved global context module. We purposely discard
attentive elements in both channel and space dimensions, and capture the
inter-pixel and inter-channel dependencies to induce the model to better
understand the global context. Extensive experiments have been conducted on the
challenging PASCAL VOC 2007 benchmarks, which achieve 49.8% mAP and 66.0%
CorLoc, outperforming state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2011.10258" target="_blank">arXiv:2011.10258</a> [<a href="http://arxiv.org/pdf/2011.10258" target="_blank">pdf</a>]

<h2>SLADE: A Self-Training Framework For Distance Metric Learning. (arXiv:2011.10269v1 [cs.CV])</h2>
<h3>Jiali Duan, Yen-Liang Lin, Son Tran, Larry Davis, C.-C. Jay Kuo</h3>
<p>Most existing distance metric learning approaches use fully labeled data to
learn the sample similarities in an embedding space. We present a self-training
framework, SLADE, to improve retrieval performance by leveraging additional
unlabeled data. We first train a teacher model on the labeled data and use it
to generate pseudo labels for the unlabeled data. We then train a student model
on both labels and pseudo labels to generate final feature embeddings. We use
self-supervised representation learning to initialize the teacher model. To
better deal with noisy pseudo labels generated by the teacher network, we
design a new feature basis learning component for the student network, which
learns basis functions of feature representations for unlabeled data. The
learned basis vectors better measure the pairwise similarity and are used to
select high-confident samples for training the student network. We evaluate our
method on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop.
Experimental results demonstrate that our approach significantly improves the
performance over the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.10269" target="_blank">arXiv:2011.10269</a> [<a href="http://arxiv.org/pdf/2011.10269" target="_blank">pdf</a>]

<h2>Learning Synthetic to Real Transfer for Localization and Navigational Tasks. (arXiv:2011.10274v1 [cs.RO])</h2>
<h3>Pietrantoni Maxime, Chidlovskii Boris, Silander Tomi</h3>
<p>Autonomous navigation consists in an agent being able to navigate without
human intervention or supervision, it affects both high level planning and low
level control. Navigation is at the crossroad of multiple disciplines, it
combines notions of computer vision, robotics and control. This work aimed at
creating, in a simulation, a navigation pipeline whose transfer to the real
world could be done with as few efforts as possible. Given the limited time and
the wide range of problematic to be tackled, absolute navigation performances
while important was not the main objective. The emphasis was rather put on
studying the sim2real gap which is one the major bottlenecks of modern robotics
and autonomous navigation. To design the navigation pipeline four main
challenges arise; environment, localization, navigation and planning. The
iGibson simulator is picked for its photo-realistic textures and physics
engine. A topological approach to tackle space representation was picked over
metric approaches because they generalize better to new environments and are
less sensitive to change of conditions. The navigation pipeline is decomposed
as a localization module, a planning module and a local navigation module.
These modules utilize three different networks, an image representation
extractor, a passage detector and a local policy. The laters are trained on
specifically tailored tasks with some associated datasets created for those
specific tasks. Localization is the ability for the agent to localize itself
against a specific space representation. It must be reliable, repeatable and
robust to a wide variety of transformations. Localization is tackled as an
image retrieval task using a deep neural network trained on an auxiliary task
as a feature descriptor extractor. The local policy is trained with behavioral
cloning from expert trajectories gathered with ROS navigation stack.
</p>
<a href="http://arxiv.org/abs/2011.10274" target="_blank">arXiv:2011.10274</a> [<a href="http://arxiv.org/pdf/2011.10274" target="_blank">pdf</a>]

<h2>Joint Representation of Temporal Image Sequences and Object Motion for Video Object Detection. (arXiv:2011.10278v1 [cs.CV])</h2>
<h3>Junho Koh, Jaekyum Kim, Younji Shin, Byeongwon Lee, Seungji Yang, Jun Won Choi</h3>
<p>In this paper, we propose a new video object detector (VoD) method referred
to as temporal feature aggregation and motion-aware VoD (TM-VoD), which
produces a joint representation of temporal image sequences and object motion.
The proposed TM-VoD aggregates visual feature maps extracted by convolutional
neural networks applying the temporal attention gating and spatial feature
alignment. This temporal feature aggregation is performed in two stages in a
hierarchical fashion. In the first stage, the visual feature maps are fused at
a pixel level via gated attention model. In the second stage, the proposed
method aggregates the features after aligning the object features using
temporal box offset calibration and weights them according to the cosine
similarity measure. The proposed TM-VoD also finds the representation of the
motion of objects in two successive steps. The pixel-level motion features are
first computed based on the incremental changes between the adjacent visual
feature maps. Then, box-level motion features are obtained from both the region
of interest (RoI)-aligned pixel-level motion features and the sequential
changes of the box coordinates. Finally, all these features are concatenated to
produce a joint representation of the objects for VoD. The experiments
conducted on the ImageNet VID dataset demonstrate that the proposed method
outperforms existing VoD methods and achieves a performance comparable to that
of state-of-the-art VoDs.
</p>
<a href="http://arxiv.org/abs/2011.10278" target="_blank">arXiv:2011.10278</a> [<a href="http://arxiv.org/pdf/2011.10278" target="_blank">pdf</a>]

<h2>Learning Object-Centric Video Models by Contrasting Sets. (arXiv:2011.10287v1 [cs.CV])</h2>
<h3>Sindy L&#xf6;we, Klaus Greff, Rico Jonschkowski, Alexey Dosovitskiy, Thomas Kipf</h3>
<p>Contrastive, self-supervised learning of object representations recently
emerged as an attractive alternative to reconstruction-based training. Prior
approaches focus on contrasting individual object representations (slots)
against one another. However, a fundamental problem with this approach is that
the overall contrastive loss is the same for (i) representing a different
object in each slot, as it is for (ii) (re-)representing the same object in all
slots. Thus, this objective does not inherently push towards the emergence of
object-centric representations in the slots. We address this problem by
introducing a global, set-based contrastive loss: instead of contrasting
individual slot representations against one another, we aggregate the
representations and contrast the joined sets against one another. Additionally,
we introduce attention-based encoders to this contrastive setup which
simplifies training and provides interpretable object masks. Our results on two
synthetic video datasets suggest that this approach compares favorably against
previous contrastive methods in terms of reconstruction, future prediction and
object separation performance.
</p>
<a href="http://arxiv.org/abs/2011.10287" target="_blank">arXiv:2011.10287</a> [<a href="http://arxiv.org/pdf/2011.10287" target="_blank">pdf</a>]

<h2>Image Denoising by Gaussian Patch Mixture Model and Low Rank Patches. (arXiv:2011.10290v1 [cs.CV])</h2>
<h3>Jing Guo (1), Shuping Wang (1), Chen Luo (1), Qiyu Jin (1), Michael Kwok-Po Ng (2) ((1) School of Mathematical Science, Inner Mongolia University, Hohhot, China, (2) Department of Mathematics, University of Hong Kong, Pokfulam, Hong Kong, China)</h3>
<p>Non-local self-similarity based low rank algorithms are the state-of-the-art
methods for image denoising. In this paper, a new method is proposed by solving
two issues: how to improve similar patches matching accuracy and build an
appropriate low rank matrix approximation model for Gaussian noise. For the
first issue, similar patches can be found locally or globally. Local patch
matching is to find similar patches in a large neighborhood which can alleviate
noise effect, but the number of patches may be insufficient. Global patch
matching is to determine enough similar patches but the error rate of patch
matching may be higher. Based on this, we first use local patch matching method
to reduce noise and then use Gaussian patch mixture model to achieve global
patch matching. The second issue is that there is no low rank matrix
approximation model to adapt to Gaussian noise. We build a new model according
to the characteristics of Gaussian noise, then prove that there is a globally
optimal solution of the model. By solving the two issues, experimental results
are reported to show that the proposed approach outperforms the
state-of-the-art denoising methods includes several deep learning ones in both
PSNR / SSIM values and visual quality.
</p>
<a href="http://arxiv.org/abs/2011.10290" target="_blank">arXiv:2011.10290</a> [<a href="http://arxiv.org/pdf/2011.10290" target="_blank">pdf</a>]

<h2>Simulation-based Testing for Early Safety-Validation of Robot Systems. (arXiv:2011.10294v1 [cs.RO])</h2>
<h3>Tom P. Huck, Christoph Ledermann, Torsten Kr&#xf6;ger</h3>
<p>Industrial human-robot collaborative systems must be validated thoroughly
with regard to safety. The sooner potential hazards for workers can be exposed,
the less costly is the implementation of necessary changes. Due to the
complexity of robot systems, safety flaws often stay hidden, especially at
early design stages, when a physical implementation is not yet available for
testing. Simulation-based testing is a possible way to identify hazards in an
early stage. However, creating simulation conditions in which hazards become
observable can be difficult. Brute-force or Monte-Carlo-approaches are often
not viable for hazard identification, due to large search spaces. This work
addresses this problem by using a human model and an optimization algorithm to
generate high-risk human behavior in simulation, thereby exposing potential
hazards. A proof of concept is shown in an application example where the method
is used to find hazards in an industrial robot cell.
</p>
<a href="http://arxiv.org/abs/2011.10294" target="_blank">arXiv:2011.10294</a> [<a href="http://arxiv.org/pdf/2011.10294" target="_blank">pdf</a>]

<h2>Convergence Analysis of Homotopy-SGD for non-convex optimization. (arXiv:2011.10298v1 [cs.LG])</h2>
<h3>Matilde Gargiani, Andrea Zanelli, Quoc Tran-Dinh, Moritz Diehl, Frank Hutter</h3>
<p>First-order stochastic methods for solving large-scale non-convex
optimization problems are widely used in many big-data applications, e.g.
training deep neural networks as well as other complex and potentially
non-convex machine learning models. Their inexpensive iterations generally come
together with slow global convergence rate (mostly sublinear), leading to the
necessity of carrying out a very high number of iterations before the iterates
reach a neighborhood of a minimizer. In this work, we present a first-order
stochastic algorithm based on a combination of homotopy methods and SGD, called
Homotopy-Stochastic Gradient Descent (H-SGD), which finds interesting
connections with some proposed heuristics in the literature, e.g. optimization
by Gaussian continuation, training by diffusion, mollifying networks. Under
some mild assumptions on the problem structure, we conduct a theoretical
analysis of the proposed algorithm. Our analysis shows that, with a
specifically designed scheme for the homotopy parameter, H-SGD enjoys a global
linear rate of convergence to a neighborhood of a minimum while maintaining
fast and inexpensive iterations. Experimental evaluations confirm the
theoretical results and show that H-SGD can outperform standard SGD.
</p>
<a href="http://arxiv.org/abs/2011.10298" target="_blank">arXiv:2011.10298</a> [<a href="http://arxiv.org/pdf/2011.10298" target="_blank">pdf</a>]

<h2>Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon. (arXiv:2011.10300v1 [cs.LG])</h2>
<h3>Ben Hambly, Renyuan Xu, Huining Yang</h3>
<p>We explore reinforcement learning methods for finding the optimal policy in
the linear quadratic regulator (LQR) problem. In particular, we consider the
convergence of policy gradient methods in the setting of known and unknown
parameters. We are able to produce a global linear convergence guarantee for
this approach in the setting of finite time horizon and stochastic state
dynamics under weak assumptions. The convergence of a projected policy gradient
method is also established in order to handle problems with constraints. We
illustrate the performance of the algorithm with two examples. The first
example is the optimal liquidation of a holding in an asset. We show results
for the case where we assume a model for the underlying dynamics and where we
apply the method to the data directly. The empirical evidence suggests that the
policy gradient method can learn the global optimal solution for a larger class
of stochastic systems containing the LQR framework and that it is more robust
with respect to model mis-specification when compared to a model-based
approach. The second example is an LQR system in a higher dimensional setting
with synthetic data.
</p>
<a href="http://arxiv.org/abs/2011.10300" target="_blank">arXiv:2011.10300</a> [<a href="http://arxiv.org/pdf/2011.10300" target="_blank">pdf</a>]

<h2>Filtering Rules for Flow Time Minimization in a Parallel Machine Scheduling Problem. (arXiv:2011.10307v1 [cs.AI])</h2>
<h3>Margaux Nattaf (G-SCOP), Arnaud Malapert</h3>
<p>This paper studies the scheduling of jobs of different families on parallel
machines with qualification constraints. Originating from semiconductor
manufacturing, this constraint imposes a time threshold between the execution
of two jobs of the same family. Otherwise, the machine becomes disqualified for
this family. The goal is to minimize both the flow time and the number of
disqualifications. Recently, an efficient constraint programming model has been
proposed. However, when priority is given to the flow time objective, the
efficiency of the model can be improved. This paper uses a polynomial-time
algorithm which minimize the flow time for a single machine relaxation where
disqualifications are not considered. Using this algorithm one can derived
filtering rules on different variables of the model. Experimental results are
presented showing the effectiveness of these rules. They improve the
competitiveness with the mixed integer linear program of the literature.
</p>
<a href="http://arxiv.org/abs/2011.10307" target="_blank">arXiv:2011.10307</a> [<a href="http://arxiv.org/pdf/2011.10307" target="_blank">pdf</a>]

<h2>Segmentation overlapping wear particles with few labelled data and imbalance sample. (arXiv:2011.10313v1 [cs.CV])</h2>
<h3>Peng Peng, Jiugen Wang</h3>
<p>Ferrograph image segmentation is of significance for obtaining features of
wear particles. However, wear particles are usually overlapped in the form of
debris chains, which makes challenges to segment wear debris. An overlapping
wear particle segmentation network (OWPSNet) is proposed in this study to
segment the overlapped debris chains. The proposed deep learning model includes
three parts: a region segmentation network, an edge detection network and a
feature refine module. The region segmentation network is an improved U shape
network, and it is applied to separate the wear debris form background of
ferrograph image. The edge detection network is used to detect the edges of
wear particles. Then, the feature refine module combines low-level features and
high-level semantic features to obtain the final results. In order to solve the
problem of sample imbalance, we proposed a square dice loss function to
optimize the model. Finally, extensive experiments have been carried out on a
ferrograph image dataset. Results show that the proposed model is capable of
separating overlapping wear particles. Moreover, the proposed square dice loss
function can improve the segmentation results, especially for the segmentation
results of wear particle edge.
</p>
<a href="http://arxiv.org/abs/2011.10313" target="_blank">arXiv:2011.10313</a> [<a href="http://arxiv.org/pdf/2011.10313" target="_blank">pdf</a>]

<h2>Assessing out-of-domain generalization for robust building damage detection. (arXiv:2011.10328v1 [cs.CV])</h2>
<h3>Vitus Benson, Alexander Ecker</h3>
<p>An important step for limiting the negative impact of natural disasters is
rapid damage assessment after a disaster occurred. For instance, building
damage detection can be automated by applying computer vision techniques to
satellite imagery. Such models operate in a multi-domain setting: every
disaster is inherently different (new geolocation, unique circumstances), and
models must be robust to a shift in distribution between disaster imagery
available for training and the images of the new event. Accordingly, estimating
real-world performance requires an out-of-domain (OOD) test set. However,
building damage detection models have so far been evaluated mostly in the
simpler yet unrealistic in-distribution (IID) test setting. Here we argue that
future work should focus on the OOD regime instead. We assess OOD performance
of two competitive damage detection models and find that existing
state-of-the-art models show a substantial generalization gap: their
performance drops when evaluated OOD on new disasters not used during training.
Moreover, IID performance is not predictive of OOD performance, rendering
current benchmarks uninformative about real-world performance. Code and model
weights are available at https://github.com/ecker-lab/robust-bdd.
</p>
<a href="http://arxiv.org/abs/2011.10328" target="_blank">arXiv:2011.10328</a> [<a href="http://arxiv.org/pdf/2011.10328" target="_blank">pdf</a>]

<h2>ANIMC: A Soft Framework for Auto-weighted Noisy and Incomplete Multi-view Clustering. (arXiv:2011.10331v1 [cs.CV])</h2>
<h3>Xiang Fang, Yuchong Hu, Pan Zhou, Xiao-Yang Liu, Dapeng Oliver Wu</h3>
<p>Multi-view clustering has wide applications in many image processing
scenarios. In these scenarios, original image data often contain missing
instances and noises, which is ignored by most multi-view clustering methods.
However, missing instances may make these methods difficult to use directly and
noises will lead to unreliable clustering results. In this paper, we propose a
novel Auto-weighted Noisy and Incomplete Multi-view Clustering framework
(ANIMC) via a soft auto-weighted strategy and a doubly soft regular regression
model. Firstly, by designing adaptive semi-regularized nonnegative matrix
factorization (adaptive semi-RNMF), the soft auto-weighted strategy assigns a
proper weight to each view and adds a soft boundary to balance the influence of
noises and incompleteness. Secondly, by proposing{\theta}-norm, the doubly soft
regularized regression model adjusts the sparsity of our model by choosing
different{\theta}. Compared with existing methods, ANIMC has three unique
advantages: 1) it is a soft algorithm to adjust our framework in different
scenarios, thereby improving its generalization ability; 2) it automatically
learns a proper weight for each view, thereby reducing the influence of noises;
3) it performs doubly soft regularized regression that aligns the same
instances in different views, thereby decreasing the impact of missing
instances. Extensive experimental results demonstrate its superior advantages
over other state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.10331" target="_blank">arXiv:2011.10331</a> [<a href="http://arxiv.org/pdf/2011.10331" target="_blank">pdf</a>]

<h2>Efficient Data-Dependent Learnability. (arXiv:2011.10334v1 [cs.LG])</h2>
<h3>Yaniv Fogel, Tal Shapira, Meir Feder</h3>
<p>The predictive normalized maximum likelihood (pNML) approach has recently
been proposed as the min-max optimal solution to the batch learning problem
where both the training set and the test data feature are individuals, known
sequences. This approach has yields a learnability measure that can also be
interpreted as a stability measure. This measure has shown some potential in
detecting out-of-distribution examples, yet it has considerable computational
costs. In this project, we propose and analyze an approximation of the pNML,
which is based on influence functions. Combining both theoretical analysis and
experiments, we show that when applied to neural networks, this approximation
can detect out-of-distribution examples effectively. We also compare its
performance to that achieved by conducting a single gradient step for each
possible label.
</p>
<a href="http://arxiv.org/abs/2011.10334" target="_blank">arXiv:2011.10334</a> [<a href="http://arxiv.org/pdf/2011.10334" target="_blank">pdf</a>]

<h2>Self-Supervised Small Soccer Player Detection and Tracking. (arXiv:2011.10336v1 [cs.CV])</h2>
<h3>Samuel Hurault, Coloma Ballester, Gloria Haro</h3>
<p>In a soccer game, the information provided by detecting and tracking brings
crucial clues to further analyze and understand some tactical aspects of the
game, including individual and team actions. State-of-the-art tracking
algorithms achieve impressive results in scenarios on which they have been
trained for, but they fail in challenging ones such as soccer games. This is
frequently due to the player small relative size and the similar appearance
among players of the same team. Although a straightforward solution would be to
retrain these models by using a more specific dataset, the lack of such
publicly available annotated datasets entails searching for other effective
solutions. In this work, we propose a self-supervised pipeline which is able to
detect and track low-resolution soccer players under different recording
conditions without any need of ground-truth data. Extensive quantitative and
qualitative experimental results are presented evaluating its performance. We
also present a comparison to several state-of-the-art methods showing that both
the proposed detector and the proposed tracker achieve top-tier results, in
particular in the presence of small players.
</p>
<a href="http://arxiv.org/abs/2011.10336" target="_blank">arXiv:2011.10336</a> [<a href="http://arxiv.org/pdf/2011.10336" target="_blank">pdf</a>]

<h2>Finding Prerequisite Relations between Concepts using Textbook. (arXiv:2011.10337v1 [cs.LG])</h2>
<h3>Shivam Pal, Vipul Arora, Pawan Goyal</h3>
<p>A prerequisite is anything that you need to know or understand first before
attempting to learn or understand something new. In the current work, we
present a method of finding prerequisite relations between concepts using
related textbooks. Previous researchers have focused on finding these relations
using Wikipedia link structure through unsupervised and supervised learning
approaches. In the current work, we have proposed two methods, one is
statistical method and another is learning-based method. We mine the rich and
structured knowledge available in the textbooks to find the content for those
concepts and the order in which they are discussed. Using this information,
proposed statistical method estimates explicit as well as implicit prerequisite
relations between concepts. During experiments, we have found performance of
proposed statistical method is better than the popular RefD method, which uses
Wikipedia link structure. And proposed learning-based method has shown a
significant increase in the efficiency of supervised learning method when
compared with graph and text-based learning-based approaches.
</p>
<a href="http://arxiv.org/abs/2011.10337" target="_blank">arXiv:2011.10337</a> [<a href="http://arxiv.org/pdf/2011.10337" target="_blank">pdf</a>]

<h2>Analytic Bipedal Walking with Fused Angles and Corrective Actions in the Tilt Phase Space. (arXiv:2011.10339v1 [cs.RO])</h2>
<h3>Philipp Allgeuer</h3>
<p>This thesis presents algorithms for the feedback-stabilised walking of
bipedal humanoid robotic platforms, along with the underlying theoretical and
sensorimotor frameworks required to achieve it. Bipedal walking is inherently
complex and difficult to control due to the high level of nonlinearity and
significant number of degrees of freedom of the concerned robots, the limited
observability and controllability of the corresponding states, and the
combination of imperfect actuation with less-than-ideal sensing. The presented
methods deal with these issues in a multitude of ways, ranging from the
development of an actuator control and feed-forward compensation scheme, to the
inclusion of filtering in almost all of the gait stabilisation feedback
pipelines. Two gaits are developed and investigated, the direct fused angle
feedback gait, and the tilt phase controller. Both gaits follow the design
philosophy of leveraging a semi-stable open-loop gait generator, and extending
it through stabilising feedback via the means of so-called corrective actions.
The idea of using corrective actions is to modify the generation of the
open-loop joint waveforms in such a way that the balance of the robot is
influenced and thereby ameliorated. Examples of such corrective actions include
modifications of the arm swing and leg swing trajectories, the application of
dynamic positional and rotational offsets to the hips and feet, and adjustments
of the commanded step size and timing. Underpinning both feedback gaits and
their corresponding gait generators are significant advances in the field of 3D
rotation theory. These advances include the development of three novel rotation
representations, the tilt angles, fused angles, and tilt phase space
representations. All three of these representations are founded on a new
innovative way of splitting 3D rotations into their respective yaw and tilt
components.
</p>
<a href="http://arxiv.org/abs/2011.10339" target="_blank">arXiv:2011.10339</a> [<a href="http://arxiv.org/pdf/2011.10339" target="_blank">pdf</a>]

<h2>Accelerating Probabilistic Volumetric Mapping using Ray-Tracing Graphics Hardware. (arXiv:2011.10348v1 [cs.RO])</h2>
<h3>Heajung Min, Kyung Min Han, Young J. Kim</h3>
<p>Probabilistic volumetric mapping (PVM) represents a 3D environmental map for
an autonomous robotic navigational task. A popular implementation such as
Octomap is widely used in the robotics community for such a purpose. The
Octomap relies on octree to represent a PVM and its main bottleneck lies in
massive ray-shooting to determine the occupancy of the underlying volumetric
voxel grids. In this paper, we propose GPU-based ray shooting to drastically
improve the ray shooting performance in Octomap. Our main idea is based on the
use of recent ray-tracing RTX GPU, mainly designed for real-time
photo-realistic computer graphics and the accompanying graphics API, known as
DXR. Our ray-shooting first maps leaf-level voxels in the given octree to a set
of axis-aligned bounding boxes (AABBs) and employ massively parallel ray
shooting on them using GPUs to find free and occupied voxels. These are fed
back into CPU to update the voxel occupancy and restructure the octree. In our
experiments, we have observed more than three-orders-of-magnitude performance
improvement in terms of ray shooting using ray-tracing RTX GPU over a
state-of-the-art Octomap CPU implementation, where the benchmarking
environments consist of more than 77K points and 25K~34K voxel grids.
</p>
<a href="http://arxiv.org/abs/2011.10348" target="_blank">arXiv:2011.10348</a> [<a href="http://arxiv.org/pdf/2011.10348" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Feedback Control in a Collective Flashing Ratchet. (arXiv:2011.10357v1 [cs.LG])</h2>
<h3>Dong-Kyum Kim, Hawoong Jeong</h3>
<p>A collective flashing ratchet transports Brownian particles using a spatially
periodic, asymmetric, and time-dependent on-off switchable potential. The net
current of the particles in this system can be substantially increased by
feedback control based on the particle positions. Several feedback policies for
maximizing the current have been proposed, but optimal policies have not been
found for a moderate number of particles. Here, we use deep reinforcement
learning (RL) to find optimal policies, with results showing that policies
built with a suitable neural network architecture outperform the previous
policies. Moreover, even in a time-delayed feedback situation where the on-off
switching of the potential is delayed, we demonstrate that the policies
provided by deep RL provide higher currents than the previous strategies.
</p>
<a href="http://arxiv.org/abs/2011.10357" target="_blank">arXiv:2011.10357</a> [<a href="http://arxiv.org/pdf/2011.10357" target="_blank">pdf</a>]

<h2>RidgeSfM: Structure from Motion via Robust Pairwise Matching Under Depth Uncertainty. (arXiv:2011.10359v1 [cs.CV])</h2>
<h3>Benjamin Graham, David Novotny</h3>
<p>We consider the problem of simultaneously estimating a dense depth map and
camera pose for a large set of images of an indoor scene. While classical SfM
pipelines rely on a two-step approach where cameras are first estimated using a
bundle adjustment in order to ground the ensuing multi-view stereo stage, both
our poses and dense reconstructions are a direct output of an altered bundle
adjuster. To this end, we parametrize each depth map with a linear combination
of a limited number of basis "depth-planes" predicted in a monocular fashion by
a deep net. Using a set of high-quality sparse keypoint matches, we optimize
over the per-frame linear combinations of depth planes and camera poses to form
a geometrically consistent cloud of keypoints. Although our bundle adjustment
only considers sparse keypoints, the inferred linear coefficients of the basis
planes immediately give us dense depth maps. RidgeSfM is able to collectively
align hundreds of frames, which is its main advantage over recent memory-heavy
deep alternatives that can align at most 10 frames. Quantitative comparisons
reveal performance superior to a state-of-the-art large-scale SfM pipeline.
</p>
<a href="http://arxiv.org/abs/2011.10359" target="_blank">arXiv:2011.10359</a> [<a href="http://arxiv.org/pdf/2011.10359" target="_blank">pdf</a>]

<h2>SophiaPop: Experiments in Human-AI Collaboration on Popular Music. (arXiv:2011.10363v1 [cs.AI])</h2>
<h3>David Hanson, Frankie Storm, Wenwei Huang, Vytas Krisciunas, Tiger Darrow, Audrey Brown, Mengna Lei, Matthew Aylett, Adam Pickrell, Sophia the Robot</h3>
<p>A diverse team of engineers, artists, and algorithms, collaborated to create
songs for SophiaPop, via various neural networks, robotics technologies, and
artistic tools, and animated the results on Sophia the Robot, a robotic
celebrity and animated character. Sophia is a platform for arts, research, and
other uses. To advance the art and technology of Sophia, we combine various AI
with a fictional narrative of her burgeoning career as a popstar. Her actual
AI-generated pop lyrics, music, and paintings, and animated conversations
wherein she interacts with humans real-time in narratives that discuss her
experiences. To compose the music, SophiaPop team built corpora from human and
AI-generated Sophia character personality content, along with pop music song
forms, to train and provide seeds for a number of AI algorithms including
expert models, and custom-trained transformer neural networks, which then
generated original pop-song lyrics and melodies. Our musicians including
Frankie Storm, Adam Pickrell, and Tiger Darrow, then performed interpretations
of the AI-generated musical content, including singing and instrumentation. The
human-performed singing data then was processed by a neural-network-based
Sophia voice, which was custom-trained from human performances by Cereproc.
This AI then generated the unique Sophia voice singing of the songs. Then we
animated Sophia to sing the songs in music videos, using a variety of animation
generators and human-generated animations. Being algorithms and humans, working
together, SophiaPop represents a human-AI collaboration, aspiring toward human
AI symbiosis. We believe that such a creative convergence of multiple
disciplines with humans and AI working together, can make AI relevant to human
culture in new and exciting ways, and lead to a hopeful vision for the future
of human-AI relations.
</p>
<a href="http://arxiv.org/abs/2011.10363" target="_blank">arXiv:2011.10363</a> [<a href="http://arxiv.org/pdf/2011.10363" target="_blank">pdf</a>]

<h2>Towards Abstract Relational Learning in Human Robot Interaction. (arXiv:2011.10364v1 [cs.AI])</h2>
<h3>Mohamadreza Faridghasemnia, Daniele Nardi, Alessandro Saffiotti</h3>
<p>Humans have a rich representation of the entities in their environment.
Entities are described by their attributes, and entities that share attributes
are often semantically related. For example, if two books have "Natural
Language Processing" as the value of their `title' attribute, we can expect
that their `topic' attribute will also be equal, namely, "NLP". Humans tend to
generalize such observations, and infer sufficient conditions under which the
`topic' attribute of any entity is "NLP". If robots need to interact
successfully with humans, they need to represent entities, attributes, and
generalizations in a similar way. This ends in a contextualized cognitive agent
that can adapt its understanding, where context provides sufficient conditions
for a correct understanding. In this work, we address the problem of how to
obtain these representations through human-robot interaction. We integrate
visual perception and natural language input to incrementally build a semantic
model of the world, and then use inductive reasoning to infer logical rules
that capture generic semantic relations, true in this model. These relations
can be used to enrich the human-robot interaction, to populate a knowledge base
with inferred facts, or to remove uncertainty in the robot's sensory inputs.
</p>
<a href="http://arxiv.org/abs/2011.10364" target="_blank">arXiv:2011.10364</a> [<a href="http://arxiv.org/pdf/2011.10364" target="_blank">pdf</a>]

<h2>PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v1 [cs.LG])</h2>
<h3>Neus Llop Torrent (1 and 2), Giorgio Visani (2 and 3), Enrico Bagli (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics &amp; Engineering)</h3>
<p>The aim of this paper is to develop and test advanced analytical methods to
improve the prediction accuracy of Credit Risk Models, preserving at the same
time the model interpretability. In particular, the project focuses on applying
an explainable machine learning model to PSD2-related databases. The input data
were obtained solely from account transactions from a pool of Italian
commercial banks. Over the total proven models, CatBoost has shown the highest
performance. The algorithm implementation produces a GINI of 0.45 after tuning
the hyper-parameters combined with their inherent class-weight resampling
method. SHAP package is used to provide a global and local interpretation of
the model predictions to formulate a human-comprehensive approach to
understanding the decision-maker algorithm. The 20 most important features are
selected using the Shapley values to present a full human-understandable model
that reveals how the attributes of an individual are related to its model
prediction.
</p>
<a href="http://arxiv.org/abs/2011.10367" target="_blank">arXiv:2011.10367</a> [<a href="http://arxiv.org/pdf/2011.10367" target="_blank">pdf</a>]

<h2>Neural Scene Graphs for Dynamic Scenes. (arXiv:2011.10379v1 [cs.CV])</h2>
<h3>Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide</h3>
<p>Recent implicit neural rendering methods have demonstrated that it is
possible to learn accurate view synthesis for complex scenes by predicting
their volumetric density and color supervised solely by a set of RGB images.
However, existing methods are restricted to learning efficient interpolations
of static scenes that encode all scene objects into a single neural network,
lacking the ability to represent dynamic scenes and decompositions into
individual scene objects. In this work, we present the first neural rendering
method that decomposes dynamic scenes into scene graphs. We propose a learned
scene graph representation, which encodes object transformation and radiance,
to efficiently render novel arrangements and views of the scene. To this end,
we learn implicitly encoded scenes, combined with a jointly learned latent
representation to describe objects with a single implicit function. We assess
the proposed method on synthetic and real automotive data, validating that our
approach learns dynamic scenes - only by observing a video of this scene - and
allows for rendering novel photo-realistic views of novel scene compositions
with unseen sets of objects at unseen poses.
</p>
<a href="http://arxiv.org/abs/2011.10379" target="_blank">arXiv:2011.10379</a> [<a href="http://arxiv.org/pdf/2011.10379" target="_blank">pdf</a>]

<h2>Born Identity Network: Multi-way Counterfactual Map Generation to Explain a Classifier's Decision. (arXiv:2011.10381v1 [cs.CV])</h2>
<h3>Kwanseok Oh, Jee Seok Yoon, Heung-Il Suk</h3>
<p>There exists an apparent negative correlation between performance and
interpretability of deep learning models. In an effort to reduce this negative
correlation, we propose Born Identity Network (BIN), which is a post-hoc
approach for producing multi-way counterfactual maps. A counterfactual map
transforms an input sample to be classified as a target label, which is similar
to how humans process knowledge through counterfactual thinking. Thus,
producing a better counterfactual map may be a step towards explanation at the
level of human knowledge. For example, a counterfactual map can localize
hypothetical abnormalities from a normal brain image that may cause it to be
diagnosed with a disease. Specifically, our proposed BIN consists of two core
components: Counterfactual Map Generator and Target Attribution Network. The
Counterfactual Map Generator is a variation of conditional GAN which can
synthesize a counterfactual map conditioned on an arbitrary target label. The
Target Attribution Network works in a complementary manner to enforce target
label attribution to the synthesized map. We have validated our proposed BIN in
qualitative, quantitative analysis on MNIST, 3D Shapes, and ADNI datasets, and
show the comprehensibility and fidelity of our method from various ablation
studies.
</p>
<a href="http://arxiv.org/abs/2011.10381" target="_blank">arXiv:2011.10381</a> [<a href="http://arxiv.org/pdf/2011.10381" target="_blank">pdf</a>]

<h2>Double Self-weighted Multi-view Clustering via Adaptive View Fusion. (arXiv:2011.10396v1 [cs.LG])</h2>
<h3>Xiang Fang, Yuchong Hu</h3>
<p>Multi-view clustering has been applied in many real-world applications where
original data often contain noises. Some graph-based multi-view clustering
methods have been proposed to try to reduce the negative influence of noises.
However, previous graph-based multi-view clustering methods treat all features
equally even if there are redundant features or noises, which is obviously
unreasonable. In this paper, we propose a novel multi-view clustering framework
Double Self-weighted Multi-view Clustering (DSMC) to overcome the
aforementioned deficiency. DSMC performs double self-weighted operations to
remove redundant features and noises from each graph, thereby obtaining robust
graphs. For the first self-weighted operation, it assigns different weights to
different features by introducing an adaptive weight matrix, which can
reinforce the role of the important features in the joint representation and
make each graph robust. For the second self-weighting operation, it weights
different graphs by imposing an adaptive weight factor, which can assign larger
weights to more robust graphs. Furthermore, by designing an adaptive multiple
graphs fusion, we can fuse the features in the different graphs to integrate
these graphs for clustering. Experiments on six real-world datasets demonstrate
its advantages over other state-of-the-art multi-view clustering methods.
</p>
<a href="http://arxiv.org/abs/2011.10396" target="_blank">arXiv:2011.10396</a> [<a href="http://arxiv.org/pdf/2011.10396" target="_blank">pdf</a>]

<h2>Cost-effective Variational Active Entity Resolution. (arXiv:2011.10406v1 [cs.LG])</h2>
<h3>Alex Bogatu, Norman W. Paton, Mark Douthwaite, Stuart Davie, Andre Freitas</h3>
<p>Accurately identifying different representations of the same real-world
entity is an integral part of data cleaning and many methods have been proposed
to accomplish it. The challenges of this entity resolution task that demand so
much research attention are often rooted in the task-specificity and
user-dependence of the process. Adopting deep learning techniques has the
potential to lessen these challenges. In this paper, we set out to devise an
entity resolution method that builds on the robustness conferred by deep
autoencoders to reduce human-involvement costs. Specifically, we reduce the
cost of training deep entity resolution models by performing unsupervised
representation learning. This unveils a transferability property of the
resulting model that can further reduce the cost of applying the approach to
new datasets by means of transfer learning. Finally, we reduce the cost of
labelling training data through an active learning approach that builds on the
properties conferred by the use of deep autoencoders. Empirical evaluation
confirms the accomplishment of our cost-reduction desideratum while achieving
comparable effectiveness with state-of-the-art alternatives.
</p>
<a href="http://arxiv.org/abs/2011.10406" target="_blank">arXiv:2011.10406</a> [<a href="http://arxiv.org/pdf/2011.10406" target="_blank">pdf</a>]

<h2>SalSum: Saliency-based Video Summarization using Generative Adversarial Networks. (arXiv:2011.10432v1 [cs.CV])</h2>
<h3>George Pantazis, George Dimas, Dimitris K. Iakovidis</h3>
<p>The huge amount of video data produced daily by camera-based systems, such as
surveilance, medical and telecommunication systems, emerges the need for
effective video summarization (VS) methods. These methods should be capable of
creating an overview of the video content. In this paper, we propose a novel VS
method based on a Generative Adversarial Network (GAN) model pre-trained with
human eye fixations. The main contribution of the proposed method is that it
can provide perceptually compatible video summaries by combining both perceived
color and spatiotemporal visual attention cues in a unsupervised scheme.
Several fusion approaches are considered for robustness under uncertainty, and
personalization. The proposed method is evaluated in comparison to
state-of-the-art VS approaches on the benchmark dataset VSUMM. The experimental
results conclude that SalSum outperforms the state-of-the-art approaches by
providing the highest f-measure score on the VSUMM benchmark.
</p>
<a href="http://arxiv.org/abs/2011.10432" target="_blank">arXiv:2011.10432</a> [<a href="http://arxiv.org/pdf/2011.10432" target="_blank">pdf</a>]

<h2>Crowdsourcing Airway Annotations in Chest Computed Tomography Images. (arXiv:2011.10433v1 [cs.CV])</h2>
<h3>Veronika Cheplygina, Adria Perez-Rovira, Wieying Kuo, Harm A. W. M. Tiddens, Marleen de Bruijne</h3>
<p>Measuring airways in chest computed tomography (CT) scans is important for
characterizing diseases such as cystic fibrosis, yet very time-consuming to
perform manually. Machine learning algorithms offer an alternative, but need
large sets of annotated scans for good performance. We investigate whether
crowdsourcing can be used to gather airway annotations. We generate image
slices at known locations of airways in 24 subjects and request the crowd
workers to outline the airway lumen and airway wall. After combining multiple
crowd workers, we compare the measurements to those made by the experts in the
original scans. Similar to our preliminary study, a large portion of the
annotations were excluded, possibly due to workers misunderstanding the
instructions. After excluding such annotations, moderate to strong correlations
with the expert can be observed, although these correlations are slightly lower
than inter-expert correlations. Furthermore, the results across subjects in
this study are quite variable. Although the crowd has potential in annotating
airways, further development is needed for it to be robust enough for gathering
annotations in practice. For reproducibility, data and code are available
online: \url{this http URL}.
</p>
<a href="http://arxiv.org/abs/2011.10433" target="_blank">arXiv:2011.10433</a> [<a href="http://arxiv.org/pdf/2011.10433" target="_blank">pdf</a>]

<h2>Low-Dimensional Manifolds Support Multiplexed Integrations in Recurrent Neural Networks. (arXiv:2011.10435v1 [cs.LG])</h2>
<h3>Arnaud Fanthomme (ENS Paris), R&#xe9;mi Monasson (ENS Paris)</h3>
<p>We study the learning dynamics and the representations emerging in Recurrent
Neural Networks trained to integrate one or multiple temporal signals.
Combining analytical and numerical investigations, we characterize the
conditions under which a RNN with n neurons learns to integrate D(n) scalar
signals of arbitrary duration. We show, both for linear and ReLU neurons, that
its internal state lives close to a D-dimensional manifold, whose shape is
related to the activation function. Each neuron therefore carries, to various
degrees, information about the value of all integrals. We discuss the deep
analogy between our results and the concept of mixed selectivity forged by
computational neuroscientists to interpret cortical recordings.
</p>
<a href="http://arxiv.org/abs/2011.10435" target="_blank">arXiv:2011.10435</a> [<a href="http://arxiv.org/pdf/2011.10435" target="_blank">pdf</a>]

<h2>Gradient Regularisation as Approximate Variational Inference. (arXiv:2011.10443v1 [stat.ML])</h2>
<h3>Ali Unlu, Laurence Aitchison</h3>
<p>Variational inference in Bayesian neural networks is usually performed using
stochastic sampling which gives very high-variance gradients, and hence slow
learning. Here, we show that it is possible to obtain a deterministic
approximation of the ELBO for a Bayesian neural network by doing a
Taylor-series expansion around the mean of the current variational
distribution. The resulting approximate ELBO is the training-log-likelihood
plus a squared gradient regulariser. In addition to learning the approximate
posterior variance, we also consider a uniform-variance approximate posterior,
inspired by the stationary distribution of SGD. The corresponding approximate
ELBO has a simple form, as the log-likelihood plus a simple squared-gradient
regulariser. We argue that this squared-gradient regularisation may at the root
of the excellent empirical performance of SGD.
</p>
<a href="http://arxiv.org/abs/2011.10443" target="_blank">arXiv:2011.10443</a> [<a href="http://arxiv.org/pdf/2011.10443" target="_blank">pdf</a>]

<h2>Bridging Scene Understanding and Task Execution with Flexible Simulation Environments. (arXiv:2011.10452v1 [cs.RO])</h2>
<h3>Zachary Ravichandran, J. Daniel Griffith, Benjamin Smith, Costas Frost</h3>
<p>Significant progress has been made in scene understanding which seeks to
build 3D, metric and object-oriented representations of the world.
Concurrently, reinforcement learning has made impressive strides largely
enabled by advances in simulation. Comparatively, there has been less focus in
simulation for perception algorithms. Simulation is becoming increasingly vital
as sophisticated perception approaches such as metric-semantic mapping or 3D
dynamic scene graph generation require precise 3D, 2D, and inertial information
in an interactive environment. To that end, we present TESSE (Task Execution
with Semantic Segmentation Environments), an open source simulator for
developing scene understanding and task execution algorithms. TESSE has been
used to develop state-of-the-art solutions for metric-semantic mapping and 3D
dynamic scene graph generation. Additionally, TESSE served as the platform for
the GOSEEK Challenge at the International Conference of Robotics and Automation
(ICRA) 2020, an object search competition with an emphasis on reinforcement
learning. Code for TESSE is available at https://github.com/MIT-TESSE.
</p>
<a href="http://arxiv.org/abs/2011.10452" target="_blank">arXiv:2011.10452</a> [<a href="http://arxiv.org/pdf/2011.10452" target="_blank">pdf</a>]

<h2>Towards Building a Robust and Fair Federated Learning System. (arXiv:2011.10464v1 [cs.LG])</h2>
<h3>Xinyi Xu, Lingjuan Lyu</h3>
<p>Federated Learning (FL) has emerged as a promising practical framework for
effective and scalable distributed machine learning. However, most existing FL
or distributed learning frameworks have not addressed two important issues well
together: collaborative fairness and robustness to non-contributing
participants (e.g. free-riders, adversaries). In particular, all participants
can receive the 'same' access to the global model, which is obviously unfair to
the high-contributing participants. Furthermore, due to the lack of a safeguard
mechanism, free-riders or malicious adversaries could game the system to access
the global model for free or to sabotage it. By identifying the underlying
similarity between these two issues, we investigate them simultaneously and
propose a novel Robust and Fair Federated Learning (RFFL) framework which
utilizes reputation scores to address both issues, thus ensuring the
high-contributing participants are rewarded with high-performing models while
the low- or non-contributing participants can be detected and removed.
Furthermore, our approach differentiates itself by not requiring any auxiliary
dataset for the reputation calculation. Extensive experiments on benchmark
datasets demonstrate that RFFL achieves high fairness, is robust against
several types of adversaries, delivers comparable accuracy to the conventional
federated framework and outperforms the Standalone framework.
</p>
<a href="http://arxiv.org/abs/2011.10464" target="_blank">arXiv:2011.10464</a> [<a href="http://arxiv.org/pdf/2011.10464" target="_blank">pdf</a>]

<h2>Improvement of Classification in One-Stage Detector. (arXiv:2011.10465v1 [cs.CV])</h2>
<h3>Wu Kehe, Chen Zuge, Zhang Xiaoliang, Li Wei</h3>
<p>RetinaNet proposed Focal Loss for classification task and improved one-stage
detectors greatly. However, there is still a gap between it and two-stage
detectors. We analyze the prediction of RetinaNet and find that the
misalignment of classification and localization is the main factor. Most of
predicted boxes, whose IoU with ground-truth boxes are greater than 0.5, while
their classification scores are lower than 0.5, which shows that the
classification task still needs to be optimized. In this paper we proposed an
object confidence task for this problem, and it shares features with
classification task. This task uses IoUs between samples and ground-truth boxes
as targets, and it only uses losses of positive samples in training, which can
increase loss weight of positive samples in classification task training. Also
the joint of classification score and object confidence will be used to guide
NMS. Our method can not only improve classification task, but also ease
misalignment of classification and localization. To evaluate the effectiveness
of this method, we show our experiments on MS COCO 2017 dataset. Without
whistles and bells, our method can improve AP by 0.7% and 1.0% on COCO
validation dataset with ResNet50 and ResNet101 respectively at same training
configs, and it can achieve 38.4% AP with two times training time. Code is at:
this http URL
</p>
<a href="http://arxiv.org/abs/2011.10465" target="_blank">arXiv:2011.10465</a> [<a href="http://arxiv.org/pdf/2011.10465" target="_blank">pdf</a>]

<h2>Empirical Evaluation of Deep Learning Model Compression Techniques on the WaveNet Vocoder. (arXiv:2011.10469v1 [cs.LG])</h2>
<h3>Sam Davis, Giuseppe Coccia, Sam Gooch, Julian Mack</h3>
<p>WaveNet is a state-of-the-art text-to-speech vocoder that remains challenging
to deploy due to its autoregressive loop. In this work we focus on ways to
accelerate the original WaveNet architecture directly, as opposed to modifying
the architecture, such that the model can be deployed as part of a scalable
text-to-speech system. We survey a wide variety of model compression techniques
that are amenable to deployment on a range of hardware platforms. In
particular, we compare different model sparsity methods and levels, and seven
widely used precisions as targets for quantization; and are able to achieve
models with a compression ratio of up to 13.84 without loss in audio fidelity
compared to a dense, single-precision floating-point baseline. All techniques
are implemented using existing open source deep learning frameworks and
libraries to encourage their wider adoption.
</p>
<a href="http://arxiv.org/abs/2011.10469" target="_blank">arXiv:2011.10469</a> [<a href="http://arxiv.org/pdf/2011.10469" target="_blank">pdf</a>]

<h2>Online Descriptor Enhancement via Self-Labelling Triplets for Visual Data Association. (arXiv:2011.10471v1 [cs.CV])</h2>
<h3>Yorai Shaoul, Katherine Liu, Kyel Ok, Nicholas Roy</h3>
<p>We propose a self-supervised method for incrementally refining visual
descriptors to improve performance in the task of object-level visual data
association. Our method optimizes deep descriptor generators online, by
fine-tuning a widely available network pre-trained for image classification. We
show that earlier layers in the network outperform later-stage layers for the
data association task while also allowing for a 94% reduction in the number of
parameters, enabling the online optimization. We show that choosing positive
examples separated by large temporal distances and negative examples close in
the descriptor space improves the quality of the learned descriptors for the
multi-object tracking task. Finally, we demonstrate a MOTA score of 21.25% on
the 2D-MOT-2015 dataset using visual information alone, outperforming methods
that incorporate motion information.
</p>
<a href="http://arxiv.org/abs/2011.10471" target="_blank">arXiv:2011.10471</a> [<a href="http://arxiv.org/pdf/2011.10471" target="_blank">pdf</a>]

<h2>Gender Transformation: Robustness of GenderDetection in Facial Recognition Systems with variation in Image Properties. (arXiv:2011.10472v1 [cs.CV])</h2>
<h3>Sharadha Srinivasan, Madan Musuvathi</h3>
<p>In recent times, there have been increasing accusations on artificial
intelligence systems and algorithms of computer vision of possessing implicit
biases. Even though these conversations are more prevalent now and systems are
improving by performing extensive testing and broadening their horizon, biases
still do exist. One such class of systems where bias is said to exist is facial
recognition systems, where bias has been observed on the basis of gender,
ethnicity, and skin tone, to name a few. This is even more disturbing, given
the fact that these systems are used in practically every sector of the
industries today. From as critical as criminal identification to as simple as
getting your attendance registered, these systems have gained a huge market,
especially in recent years. That in itself is a good enough reason for
developers of these systems to ensure that the bias is kept to a bare minimum
or ideally non-existent, to avoid major issues like favoring a particular
gender, race, or class of people or rather making a class of people susceptible
to false accusations due to inability of these systems to correctly recognize
those people.
</p>
<a href="http://arxiv.org/abs/2011.10472" target="_blank">arXiv:2011.10472</a> [<a href="http://arxiv.org/pdf/2011.10472" target="_blank">pdf</a>]

<h2>Probabilistic Radio-Visual Active Sensing for Search and Tracking. (arXiv:2011.10474v1 [cs.RO])</h2>
<h3>L. Varotto, A. Cenedese, A. Cavallaro</h3>
<p>Active Search and Tracking for search and rescue missions or collaborative
mobile robotics relies on the actuation of a sensing platform to detect and
localize a target. In this paper we focus on visually detecting a
radio-emitting target with an aerial robot equipped with a radio receiver and a
camera. Visual-based tracking provides high accuracy, but the directionality of
the sensing domain often requires long search times before detecting the
target. Conversely,radio signals have larger coverage, but lower tracking
accuracy. Thus, we design a Recursive Bayesian Estimation scheme that uses
camera observations to refine radio measurements. To regulate the camera pose,
we design an optimal controller whose cost function is built upon a
probabilistic map. Theoretical results support the proposed algorithm, while
numerical analyses show higher robustness and efficiency with respect to visual
and radio-only baselines.
</p>
<a href="http://arxiv.org/abs/2011.10474" target="_blank">arXiv:2011.10474</a> [<a href="http://arxiv.org/pdf/2011.10474" target="_blank">pdf</a>]

<h2>DeepPhaseCut: Deep Relaxation in Phase for Unsupervised Fourier Phase Retrieval. (arXiv:2011.10475v1 [cs.CV])</h2>
<h3>Eunju Cha, Chanseok Lee, Mooseok Jang, Jong Chul Ye</h3>
<p>Fourier phase retrieval is a classical problem of restoring a signal only
from the measured magnitude of its Fourier transform. Although Fienup-type
algorithms, which use prior knowledge in both spatial and Fourier domains, have
been widely used in practice, they can often stall in local minima. Modern
methods such as PhaseLift and PhaseCut may offer performance guarantees with
the help of convex relaxation. However, these algorithms are usually
computationally intensive for practical use. To address this problem, we
propose a novel, unsupervised, feed-forward neural network for Fourier phase
retrieval which enables immediate high quality reconstruction. Unlike the
existing deep learning approaches that use a neural network as a regularization
term or an end-to-end blackbox model for supervised training, our algorithm is
a feed-forward neural network implementation of PhaseCut algorithm in an
unsupervised learning framework. Specifically, our network is composed of two
generators: one for the phase estimation using PhaseCut loss, followed by
another generator for image reconstruction, all of which are trained
simultaneously using a cycleGAN framework without matched data. The link to the
classical Fienup-type algorithms and the recent symmetry-breaking learning
approach is also revealed. Extensive experiments demonstrate that the proposed
method outperforms all existing approaches in Fourier phase retrieval problems.
</p>
<a href="http://arxiv.org/abs/2011.10475" target="_blank">arXiv:2011.10475</a> [<a href="http://arxiv.org/pdf/2011.10475" target="_blank">pdf</a>]

<h2>On the coercivity condition in the learning of interacting particle systems. (arXiv:2011.10480v1 [stat.ML])</h2>
<h3>Zhongyang Li, Fei Lu</h3>
<p>In the learning of systems of interacting particles or agents, coercivity
condition ensures identifiability of the interaction functions, providing the
foundation of learning by nonparametric regression. The coercivity condition is
equivalent to the strictly positive definiteness of an integral kernel arising
in the learning. We show that for a class of interaction functions such that
the system is ergodic, the integral kernel is strictly positive definite, and
hence the coercivity condition holds true.
</p>
<a href="http://arxiv.org/abs/2011.10480" target="_blank">arXiv:2011.10480</a> [<a href="http://arxiv.org/pdf/2011.10480" target="_blank">pdf</a>]

<h2>Recovering the Imperfect: Cell Segmentation in the Presence of Dynamically Localized Proteins. (arXiv:2011.10486v1 [cs.CV])</h2>
<h3>&#xd6;zg&#xfc;n &#xc7;i&#xe7;ek, Yassine Marrakchi, Enoch Boasiako Antwi, Barbara Di Ventura, Thomas Brox</h3>
<p>Deploying off-the-shelf segmentation networks on biomedical data has become
common practice, yet if structures of interest in an image sequence are visible
only temporarily, existing frame-by-frame methods fail. In this paper, we
provide a solution to segmentation of imperfect data through time based on
temporal propagation and uncertainty estimation. We integrate uncertainty
estimation into Mask R-CNN network and propagate motion-corrected segmentation
masks from frames with low uncertainty to those frames with high uncertainty to
handle temporary loss of signal for segmentation. We demonstrate the value of
this approach over frame-by-frame segmentation and regular temporal propagation
on data from human embryonic kidney (HEK293T) cells transiently transfected
with a fluorescent protein that moves in and out of the nucleus over time. The
method presented here will empower microscopic experiments aimed at
understanding molecular and cellular function.
</p>
<a href="http://arxiv.org/abs/2011.10486" target="_blank">arXiv:2011.10486</a> [<a href="http://arxiv.org/pdf/2011.10486" target="_blank">pdf</a>]

<h2>Normalization effects on shallow neural networks and related asymptotic expansions. (arXiv:2011.10487v1 [stat.ML])</h2>
<h3>Jiahui Yu, Konstantinos Spiliopoulos</h3>
<p>We consider shallow (single hidden layer) neural networks and characterize
their performance when trained with stochastic gradient descent as the number
of hidden units $N$ and gradient descent steps grow to infinity. In particular,
we investigate the effect of different scaling schemes, which lead to different
normalizations of the neural network, on the network's statistical output,
closing the gap between the $1/\sqrt{N}$ and the mean-field $1/N$
normalization. We develop an asymptotic expansion for the neural network's
statistical output pointwise with respect to the scaling parameter as the
number of hidden units grows to infinity. Based on this expansion we
demonstrate mathematically that to leading order in $N$ there is no
bias-variance trade off, in that both bias and variance (both explicitly
characterized) decrease as the number of hidden units increases and time grows.
In addition, we show that to leading order in $N$, the variance of the neural
network's statistical output decays as the implied normalization by the scaling
parameter approaches the mean field normalization. Numerical studies on the
MNIST and CIFAR10 datasets show that test and train accuracy monotonically
improve as the neural network's normalization gets closer to the mean field
normalization.
</p>
<a href="http://arxiv.org/abs/2011.10487" target="_blank">arXiv:2011.10487</a> [<a href="http://arxiv.org/pdf/2011.10487" target="_blank">pdf</a>]

<h2>Utilizing ROS 1 and the Turtlebot3 in a Multi-Robot System. (arXiv:2011.10488v1 [cs.RO])</h2>
<h3>Corey Williams, Adam Schroeder</h3>
<p>ROS (Robot Operating System) has become ubiquitous for testing new
algorithms, alternative hardware configurations, and prototyping. By performing
research with its modular framework, it can streamline sharing new work and
integrations. However, it has many features and new terms that can take a
considerable amount of time to learn for a new user. This paper will explore
how to set up and configure ROS and ROS packages to work with a multi-robot
system on a single master network.
</p>
<a href="http://arxiv.org/abs/2011.10488" target="_blank">arXiv:2011.10488</a> [<a href="http://arxiv.org/pdf/2011.10488" target="_blank">pdf</a>]

<h2>Synthetic Image Rendering Solves Annotation Problem in Deep Learning Nanoparticle Segmentation. (arXiv:2011.10505v1 [cs.LG])</h2>
<h3>Leonid Mill, David Wolff, Nele Gerrits, Patrick Philipp, Lasse Kling, Florian Vollnhals, Andrew Ignatenko, Christian Jaremenko, Yixing Huang, Olivier De Castro, Jean-Nicolas Audinot, Inge Nelissen, Tom Wirtz, Andreas Maier, Silke Christiansen</h3>
<p>Nanoparticles occur in various environments as a consequence of man-made
processes, which raises concerns about their impact on the environment and
human health. To allow for proper risk assessment, a precise and statistically
relevant analysis of particle characteristics (such as e.g. size, shape and
composition) is required that would greatly benefit from automated image
analysis procedures. While deep learning shows impressive results in object
detection tasks, its applicability is limited by the amount of representative,
experimentally collected and manually annotated training data. Here, we present
an elegant, flexible and versatile method to bypass this costly and tedious
data acquisition process. We show that using a rendering software allows to
generate realistic, synthetic training data to train a state-of-the art deep
neural network. Using this approach, we derive a segmentation accuracy that is
comparable to man-made annotations for toxicologically relevant metal-oxide
nanoparticle ensembles which we chose as examples. Our study paves the way
towards the use of deep learning for automated, high-throughput particle
detection in a variety of imaging techniques such as microscopies and
spectroscopies, for a wide variety of studies and applications, including the
detection of plastic micro- and nanoparticles.
</p>
<a href="http://arxiv.org/abs/2011.10505" target="_blank">arXiv:2011.10505</a> [<a href="http://arxiv.org/pdf/2011.10505" target="_blank">pdf</a>]

<h2>Planning Folding Motion with Simulation in the Loop Using Laser Forming Origami and Thermal Behaviors as an Example. (arXiv:2011.10508v1 [cs.RO])</h2>
<h3>Yue Hao, Weilin Guan, Edwin A Peraza Hernandez, Jyh-Ming Lien</h3>
<p>Designing a robot or structure that can fold itself into a target shape is a
process that involves challenges originated from multiple sources. For example,
the designer of rigid self-folding robots must consider foldability from
geometric and kinematic aspects to avoid self-intersection and undesired
deformations. Recent works have shown success in estimating foldability of a
design using robot motion planners. However, many foldable structures are
actuated using physically coupled reactions (i.e., folding originated from
thermal, chemical, or electromagnetic loads). Therefore, a reliable foldability
analysis must consider additional constraints that resulted from these critical
phenomena. This work investigates the idea of efficiently incorporating
computationally expensive physics simulation within the folding motion planner
to provide a better estimation of the foldability. In this paper, we will use
laser forming origami as an example to demonstrate the benefits of considering
the properties beyond geometry. We show that the design produced by the
proposed method can be folded more efficiently.
</p>
<a href="http://arxiv.org/abs/2011.10508" target="_blank">arXiv:2011.10508</a> [<a href="http://arxiv.org/pdf/2011.10508" target="_blank">pdf</a>]

<h2>Intrinsic Image Decomposition using Paradigms. (arXiv:2011.10512v1 [cs.CV])</h2>
<h3>D. A. Forsyth, Jason J. Rock</h3>
<p>Intrinsic image decomposition is the classical task of mapping image to
albedo. The WHDR dataset allows methods to be evaluated by comparing
predictions to human judgements ("lighter", "same as", "darker"). The best
modern intrinsic image methods learn a map from image to albedo using rendered
models and human judgements. This is convenient for practical methods, but
cannot explain how a visual agent without geometric, surface and illumination
models and a renderer could learn to recover intrinsic images.

This paper describes a method that learns intrinsic image decomposition
without seeing WHDR annotations, rendered data, or ground truth data. The
method relies on paradigms - fake albedos and fake shading fields - together
with a novel smoothing procedure that ensures good behavior at short scales on
real images. Long scale error is controlled by averaging. Our method achieves
WHDR scores competitive with those of strong recent methods allowed to see
training WHDR annotations, rendered data, and ground truth data. Because our
method is unsupervised, we can compute estimates of the test/train variance of
WHDR scores; these are quite large, and it is unsafe to rely small differences
in reported WHDR.
</p>
<a href="http://arxiv.org/abs/2011.10512" target="_blank">arXiv:2011.10512</a> [<a href="http://arxiv.org/pdf/2011.10512" target="_blank">pdf</a>]

<h2>Meta-Learning for Time Series Forecasting Ensemble. (arXiv:2011.10545v1 [stat.ML])</h2>
<h3>Evaldas Vaiciukynas, Paulius Danenas, Vilius Kontrimas, Rimantas Butleris</h3>
<p>Amounts of historical data collected increase together with business
intelligence applicability and demands for automatic forecasting of time
series. While no single time series modeling method is universal to all types
of dynamics, forecasting using ensemble of several methods is often seen as a
compromise. Instead of fixing ensemble diversity and size we propose to
adaptively predict these aspects using meta-learning. Meta-learning here
considers two separate random forest regression models, built on 390 time
series features, to rank 22 univariate forecasting methods and to recommend
ensemble size. Forecasting ensemble is consequently formed from methods ranked
as the best and forecasts are pooled using either simple or weighted average
(with weight corresponding to reciprocal rank). Proposed approach was tested on
12561 micro-economic time series (expanded to 38633 for various forecasting
horizons) of M4 competition where meta-learning outperformed Theta and Comb
benchmarks by relative forecasting errors for all data types and horizons. Best
overall results were achieved by weighted pooling with symmetric mean absolute
percentage error of 9.21% versus 11.05% obtained using Theta method.
</p>
<a href="http://arxiv.org/abs/2011.10545" target="_blank">arXiv:2011.10545</a> [<a href="http://arxiv.org/pdf/2011.10545" target="_blank">pdf</a>]

<h2>Graph Signal Recovery Using Restricted Boltzmann Machines. (arXiv:2011.10549v1 [cs.LG])</h2>
<h3>Ankith Mohan, Aiichiro Nakano, Emilio Ferrara</h3>
<p>We propose a model-agnostic pipeline to recover graph signals from an expert
system by exploiting the content addressable memory property of restricted
Boltzmann machine and the representational ability of a neural network. The
proposed pipeline requires the deep neural network that is trained on a
downward machine learning task with clean data, data which is free from any
form of corruption or incompletion. We show that denoising the representations
learned by the deep neural networks is usually more effective than denoising
the data itself. Although this pipeline can deal with noise in any dataset, it
is particularly effective for graph-structured datasets.
</p>
<a href="http://arxiv.org/abs/2011.10549" target="_blank">arXiv:2011.10549</a> [<a href="http://arxiv.org/pdf/2011.10549" target="_blank">pdf</a>]

<h2>Exploring Simple Siamese Representation Learning. (arXiv:2011.10566v1 [cs.CV])</h2>
<h3>Xinlei Chen, Kaiming He</h3>
<p>Siamese networks have become a common structure in various recent models for
unsupervised visual representation learning. These models maximize the
similarity between two augmentations of one image, subject to certain
conditions for avoiding collapsing solutions. In this paper, we report
surprising empirical results that simple Siamese networks can learn meaningful
representations even using none of the following: (i) negative sample pairs,
(ii) large batches, (iii) momentum encoders. Our experiments show that
collapsing solutions do exist for the loss and structure, but a stop-gradient
operation plays an essential role in preventing collapsing. We provide a
hypothesis on the implication of stop-gradient, and further show
proof-of-concept experiments verifying it. Our "SimSiam" method achieves
competitive results on ImageNet and downstream tasks. We hope this simple
baseline will motivate people to rethink the roles of Siamese architectures for
unsupervised representation learning. Code will be made available.
</p>
<a href="http://arxiv.org/abs/2011.10566" target="_blank">arXiv:2011.10566</a> [<a href="http://arxiv.org/pdf/2011.10566" target="_blank">pdf</a>]

<h2>A Genetic Algorithm Approach for ImageRepresentation Learning through Color Quantization. (arXiv:1711.06809v3 [cs.CV] UPDATED)</h2>
<h3>&#xc9;rico M. Pereira, Ricardo da S. Torres, Jefersson A. dos Santos</h3>
<p>Over the last decades, hand-crafted feature extractors have been used to
encode image visual properties into feature vectors. Recently, data-driven
feature learning approaches have been successfully explored as alternatives for
producing more representative visual features. In this work, we combine both
research venues, focusing on the color quantization problem. We propose two
data-driven approaches to learn image representations through the search for
optimized quantization schemes, which lead to more effective feature extraction
algorithms and compact representations. Our strategy employs Genetic Algorithm,
a soft-computing apparatus successfully utilized in
Information-retrieval-related optimization problems. We hypothesize that
changing the quantization affects the quality of image description approaches,
leading to effective and efficient representations. We evaluate our approaches
in content-based image retrieval tasks, considering eight well-known datasets
with different visual properties. Results indicate that the approach focused on
representation effectiveness outperformed baselines in all tested scenarios.
The other approach, which also considers the size of created representations,
produced competitive results keeping or even reducing the dimensionality of
feature vectors up to 25%.
</p>
<a href="http://arxiv.org/abs/1711.06809" target="_blank">arXiv:1711.06809</a> [<a href="http://arxiv.org/pdf/1711.06809" target="_blank">pdf</a>]

<h2>WNGrad: Learn the Learning Rate in Gradient Descent. (arXiv:1803.02865v2 [stat.ML] UPDATED)</h2>
<h3>Xiaoxia Wu, Rachel Ward, L&#xe9;on Bottou</h3>
<p>Adjusting the learning rate schedule in stochastic gradient methods is an
important unresolved problem which requires tuning in practice. If certain
parameters of the loss function such as smoothness or strong convexity
constants are known, theoretical learning rate schedules can be applied.
However, in practice, such parameters are not known, and the loss function of
interest is not convex in any case. The recently proposed batch normalization
reparametrization is widely adopted in most neural network architectures today
because, among other advantages, it is robust to the choice of Lipschitz
constant of the gradient in loss function, allowing one to set a large learning
rate without worry. Inspired by batch normalization, we propose a general
nonlinear update rule for the learning rate in batch and stochastic gradient
descent so that the learning rate can be initialized at a high value, and is
subsequently decreased according to gradient observations along the way. The
proposed method is shown to achieve robustness to the relationship between the
learning rate and the Lipschitz constant, and near-optimal convergence rates in
both the batch and stochastic settings ($O(1/T)$ for smooth loss in the batch
setting, and $O(1/\sqrt{T})$ for convex loss in the stochastic setting). We
also show through numerical evidence that such robustness of the proposed
method extends to highly nonconvex and possibly non-smooth loss function in
deep learning problems.Our analysis establishes some first theoretical
understanding into the observed robustness for batch normalization and weight
normalization.
</p>
<a href="http://arxiv.org/abs/1803.02865" target="_blank">arXiv:1803.02865</a> [<a href="http://arxiv.org/pdf/1803.02865" target="_blank">pdf</a>]

<h2>Deep Learning Estimation of Absorbed Dose for Nuclear Medicine Diagnostics. (arXiv:1805.09108v7 [stat.ML] UPDATED)</h2>
<h3>Luciano Melodia</h3>
<p>The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated
by convolving an image of a time-integrated activity distribution with a dose
voxel kernel (DVK) consisting of different types of tissues. This fast and
inacurate approximation is inappropriate for personalized dosimetry as it
neglects tissue heterogenity. The latter can be calculated using different
imaging techniques such as CT and SPECT combined with a time consuming
monte-carlo simulation. The aim of this study is, for the first time, an
estimation of DVKs from CT-derived density kernels (DK) via deep learning in
convolutional neural networks (CNNs). The proposed CNN achieved, on the test
set, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a
corresponding mean squared error (MSE) $= 1.24 \cdot 10^{-4}$. This
generalization ability shows that the trained CNN can indeed learn the
difficult transfer function from DK to DVK. Future work will evaluate DVKs
estimated by CNNs with full monte-carlo simulations of a whole body CT to
predict patient specific voxel dose maps.
</p>
<a href="http://arxiv.org/abs/1805.09108" target="_blank">arXiv:1805.09108</a> [<a href="http://arxiv.org/pdf/1805.09108" target="_blank">pdf</a>]

<h2>FILDNE: A Framework for Incremental Learning of Dynamic Networks Embeddings. (arXiv:1904.03423v2 [stat.ML] UPDATED)</h2>
<h3>Piotr Bielak, Kamil Tagowski, Maciej Falkiewicz, Tomasz Kajdanowicz, Nitesh V. Chawla</h3>
<p>Representation learning on graphs has emerged as a powerful mechanism to
automate feature vector generation for downstream machine learning tasks. The
advances in representation on graphs have centered on both homogeneous and
heterogeneous graphs, where the latter presenting the challenges associated
with multi-typed nodes and/or edges. In this paper, we consider the additional
challenge of evolving graphs. We ask the question of whether the advances in
representation learning for static graphs can be leveraged for dynamic graphs
and how? It is important to be able to incorporate those advances to maximize
the utility and generalization of methods. To that end, we propose the
Framework for Incremental Learning of Dynamic Networks Embedding (FILDNE),
which can utilize any existing static representation learning method for
learning node embeddings, while keeping the computational costs low. FILDNE
integrates the feature vectors computed using the standard methods over
different timesteps into a single representation by developing a convex
combination function and alignment mechanism. Experimental results on several
downstream tasks, over seven real-world data sets, show that FILDNE is able to
reduce memory and computational time costs while providing competitive quality
measure gains with respect to the contemporary methods for representation
learning on dynamic graphs.
</p>
<a href="http://arxiv.org/abs/1904.03423" target="_blank">arXiv:1904.03423</a> [<a href="http://arxiv.org/pdf/1904.03423" target="_blank">pdf</a>]

<h2>Only Relevant Information Matters: Filtering Out Noisy Samples to Boost RL. (arXiv:1904.04025v5 [cs.LG] UPDATED)</h2>
<h3>Yannis Flet-Berliac, Philippe Preux</h3>
<p>In reinforcement learning, policy gradient algorithms optimize the policy
directly and rely on sampling efficiently an environment. Nevertheless, while
most sampling procedures are based on direct policy sampling, self-performance
measures could be used to improve such sampling prior to each policy update.
Following this line of thought, we introduce SAUNA, a method where
non-informative transitions are rejected from the gradient update. The level of
information is estimated according to the fraction of variance explained by the
value function: a measure of the discrepancy between V and the empirical
returns. In this work, we use this metric to select samples that are useful to
learn from, and we demonstrate that this selection can significantly improve
the performance of policy gradient methods. In this paper: (a) We define
SAUNA's metric and introduce its method to filter transitions. (b) We conduct
experiments on a set of benchmark continuous control problems. SAUNA
significantly improves performance. (c) We investigate how SAUNA reliably
selects samples with the most positive impact on learning and study its
improvement on both performance and sample efficiency.
</p>
<a href="http://arxiv.org/abs/1904.04025" target="_blank">arXiv:1904.04025</a> [<a href="http://arxiv.org/pdf/1904.04025" target="_blank">pdf</a>]

<h2>Neural Topographic Factor Analysis for fMRI Data. (arXiv:1906.08901v4 [cs.LG] UPDATED)</h2>
<h3>Eli Sennesh, Zulqarnain Khan, Yiyu Wang, Jennifer Dy, Ajay B. Satpute, J. Benjamin Hutchinson, Jan-Willem van de Meent</h3>
<p>Neuroimaging studies produce gigabytes of spatio-temporal data for a small
number of participants and stimuli. Rarely do researchers attempt to model and
examine how individual participants vary from each other -- a question that
should be addressable even in small samples given the right statistical tools.
We propose Neural Topographic Factor Analysis (NTFA), a probabilistic factor
analysis model that infers embeddings for participants and stimuli. These
embeddings allow us to reason about differences between participants and
stimuli as signal rather than noise. We evaluate NTFA on data from an in-house
pilot experiment, as well as two publicly available datasets. We demonstrate
that inferring representations for participants and stimuli improves predictive
generalization to unseen data when compared to previous topographic methods. We
also demonstrate that the inferred latent factor representations are useful for
downstream tasks such as multivoxel pattern analysis and functional
connectivity.
</p>
<a href="http://arxiv.org/abs/1906.08901" target="_blank">arXiv:1906.08901</a> [<a href="http://arxiv.org/pdf/1906.08901" target="_blank">pdf</a>]

<h2>A Novel Approach for Robust Multi Human Action Recognition and Summarization based on 3D Convolutional Neural Networks. (arXiv:1907.11272v2 [cs.CV] UPDATED)</h2>
<h3>Noor Almaadeed, Omar Elharrouss, Somaya Al-Maadeed, Ahmed Bouridane, Azeddine Beghdadi</h3>
<p>Human actions in videos are 3D signals. However, there are a few methods
available for multiple human action recognition. For long videos, it's
difficult to search within a video for a specific action and/or person. For
that, this paper proposes a new technic for multiple human action recognition
and summarization for surveillance videos. The proposed approach proposes a new
representation of the data by extracting the sequence of each person from the
scene. This is followed by an analysis of each sequence to detect and recognize
the corresponding actions using 3D convolutional neural networks (3DCNNs).
Action-based video summarization is performed by saving each person's action at
each time of the video. Results of this work revealed that the proposed method
provides accurate multi human action recognition that easily used for
summarization of any action. Further, for other videos that can be collected
from the internet, which are complex and not built for surveillance
applications, the proposed model was evaluated on some datasets like UCF101 and
YouTube without any preprocessing. For this category of videos, the
summarization is performed on the video sequences by summarizing the actions in
each subsequence. The results obtained demonstrate its efficiency compared to
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1907.11272" target="_blank">arXiv:1907.11272</a> [<a href="http://arxiv.org/pdf/1907.11272" target="_blank">pdf</a>]

<h2>Report on UG^2+ Challenge Track 1: Assessing Algorithms to Improve Video Object Detection and Classification from Unconstrained Mobility Platforms. (arXiv:1907.11529v4 [cs.CV] UPDATED)</h2>
<h3>Sreya Banerjee, Rosaura G. VidalMata, Zhangyang Wang, Walter J. Scheirer</h3>
<p>How can we effectively engineer a computer vision system that is able to
interpret videos from unconstrained mobility platforms like UAVs? One promising
option is to make use of image restoration and enhancement algorithms from the
area of computational photography to improve the quality of the underlying
frames in a way that also improves automatic visual recognition. Along these
lines, exploratory work is needed to find out which image pre-processing
algorithms, in combination with the strongest features and supervised machine
learning approaches, are good candidates for difficult scenarios like motion
blur, weather, and mis-focus -- all common artifacts in UAV acquired images.
This paper summarizes the protocols and results of Track 1 of the UG^2+
Challenge held in conjunction with IEEE/CVF CVPR 2019. The challenge looked at
two separate problems: (1) object detection improvement in video, and (2)
object classification improvement in video. The challenge made use of the UG^2
(UAV, Glider, Ground) dataset, which is an established benchmark for assessing
the interplay between image restoration and enhancement and visual recognition.
16 algorithms were submitted by academic and corporate teams, and a detailed
analysis of how they performed on each challenge problem is reported here.
</p>
<a href="http://arxiv.org/abs/1907.11529" target="_blank">arXiv:1907.11529</a> [<a href="http://arxiv.org/pdf/1907.11529" target="_blank">pdf</a>]

<h2>Regularized deep learning with nonconvex penalties. (arXiv:1909.05142v4 [stat.ML] UPDATED)</h2>
<h3>Sujit Vettam, Majnu John</h3>
<p>Regularization methods are often employed in deep learning neural networks
(DNNs) to prevent overfitting. For penalty based DNN regularization methods,
convex penalties are typically considered because of their optimization
guarantees. Recent theoretical work have shown that nonconvex penalties that
satisfy certain regularity conditions are also guaranteed to perform well with
standard optimization algorithms. In this paper, we examine new and currently
existing nonconvex penalties for DNN regularization. We provide theoretical
justifications for the new penalties and also assess the performance of all
penalties with DNN analyses of seven datasets.
</p>
<a href="http://arxiv.org/abs/1909.05142" target="_blank">arXiv:1909.05142</a> [<a href="http://arxiv.org/pdf/1909.05142" target="_blank">pdf</a>]

<h2>Safer End-to-End Autonomous Driving via Conditional Imitation Learning and Command Augmentation. (arXiv:1909.09721v3 [cs.RO] UPDATED)</h2>
<h3>Renhao Wang, Adam Scibior, Frank Wood</h3>
<p>Imitation learning is a promising approach to end-to-end training of
autonomous vehicle controllers. Typically the driving process with such
approaches is entirely automatic and black-box, although in practice it is
desirable to control the vehicle through high-level commands, such as telling
it which way to go at an intersection. In existing work this has been
accomplished by the application of a branched neural architecture, since
directly providing the command as an additional input to the controller often
results in the command being ignored. In this work we overcome this limitation
by learning a disentangled probabilistic latent variable model that generates
the steering commands. We achieve faithful command-conditional generation
without using a branched architecture and demonstrate improved stability of the
controller, applying only a variational objective without any domain-specific
adjustments. On top of that, we extend our model with an additional latent
variable and augment the dataset to train a controller that is robust to unsafe
commands, such as asking it to turn into a wall. The main contribution of this
work is a recipe for building controllable imitation driving agents that
improves upon multiple aspects of the current state of the art relating to
robustness and interpretability.
</p>
<a href="http://arxiv.org/abs/1909.09721" target="_blank">arXiv:1909.09721</a> [<a href="http://arxiv.org/pdf/1909.09721" target="_blank">pdf</a>]

<h2>Representation Learning with Statistical Independence to Mitigate Bias. (arXiv:1910.03676v4 [cs.CV] UPDATED)</h2>
<h3>Ehsan Adeli, Qingyu Zhao, Adolf Pfefferbaum, Edith V. Sullivan, Li Fei-Fei, Juan Carlos Niebles, Kilian M. Pohl</h3>
<p>Presence of bias (in datasets or tasks) is inarguably one of the most
critical challenges in machine learning applications that has alluded to
pivotal debates in recent years. Such challenges range from spurious
associations between variables in medical studies to the bias of race in gender
or face recognition systems. Controlling for all types of biases in the dataset
curation stage is cumbersome and sometimes impossible. The alternative is to
use the available data and build models incorporating fair representation
learning. In this paper, we propose such a model based on adversarial training
with two competing objectives to learn features that have (1) maximum
discriminative power with respect to the task and (2) minimal statistical mean
dependence with the protected (bias) variable(s). Our approach does so by
incorporating a new adversarial loss function that encourages a vanished
correlation between the bias and the learned features. We apply our method to
synthetic data, medical images (containing task bias), and a dataset for gender
classification (containing dataset bias). Our results show that the learned
features by our method not only result in superior prediction performance but
also are unbiased. The code is available at
https://github.com/QingyuZhao/BR-Net/.
</p>
<a href="http://arxiv.org/abs/1910.03676" target="_blank">arXiv:1910.03676</a> [<a href="http://arxiv.org/pdf/1910.03676" target="_blank">pdf</a>]

<h2>Probabilistic K-means Clustering via Nonlinear Programming. (arXiv:2001.03286v2 [cs.LG] UPDATED)</h2>
<h3>Yujian Li, Bowen Liu, Zhaoying Liu, Ting Zhang</h3>
<p>K-means is a classical clustering algorithm with wide applications. However,
soft K-means, or fuzzy c-means at m=1, remains unsolved since 1981. To address
this challenging open problem, we propose a novel clustering model, i.e.
Probabilistic K-Means (PKM), which is also a nonlinear programming model
constrained on linear equalities and linear inequalities. In theory, we can
solve the model by active gradient projection, while inefficiently. Thus, we
further propose maximum-step active gradient projection and fast maximum-step
active gradient projection to solve it more efficiently. By experiments, we
evaluate the performance of PKM and how well the proposed methods solve it in
five aspects: initialization robustness, clustering performance, descending
stability, iteration number, and convergence speed.
</p>
<a href="http://arxiv.org/abs/2001.03286" target="_blank">arXiv:2001.03286</a> [<a href="http://arxiv.org/pdf/2001.03286" target="_blank">pdf</a>]

<h2>Oblivious Data for Fairness with Kernels. (arXiv:2002.02901v2 [stat.ML] UPDATED)</h2>
<h3>Steffen Gr&#xfc;new&#xe4;lder, Azadeh Khaleghi</h3>
<p>We investigate the problem of algorithmic fairness in the case where
sensitive and non-sensitive features are available and one aims to generate
new, `oblivious', features that closely approximate the non-sensitive features,
and are only minimally dependent on the sensitive ones. We study this question
in the context of kernel methods. We analyze a relaxed version of the Maximum
Mean Discrepancy criterion which does not guarantee full independence but makes
the optimization problem tractable. We derive a closed-form solution for this
relaxed optimization problem and complement the result with a study of the
dependencies between the newly generated features and the sensitive ones. Our
key ingredient for generating such oblivious features is a Hilbert-space-valued
conditional expectation, which needs to be estimated from data. We propose a
plug-in approach and demonstrate how the estimation errors can be controlled.
While our techniques help reduce the bias, we would like to point out that no
post-processing of any dataset could possibly serve as an alternative to
well-designed experiments.
</p>
<a href="http://arxiv.org/abs/2002.02901" target="_blank">arXiv:2002.02901</a> [<a href="http://arxiv.org/pdf/2002.02901" target="_blank">pdf</a>]

<h2>Improving Generalization by Controlling Label-Noise Information in Neural Network Weights. (arXiv:2002.07933v2 [cs.LG] UPDATED)</h2>
<h3>Hrayr Harutyunyan, Kyle Reing, Greg Ver Steeg, Aram Galstyan</h3>
<p>In the presence of noisy or incorrect labels, neural networks have the
undesirable tendency to memorize information about the noise. Standard
regularization techniques such as dropout, weight decay or data augmentation
sometimes help, but do not prevent this behavior. If one considers neural
network weights as random variables that depend on the data and stochasticity
of training, the amount of memorized information can be quantified with the
Shannon mutual information between weights and the vector of all training
labels given inputs, $I(w ; \mathbf{y} \mid \mathbf{x})$. We show that for any
training algorithm, low values of this term correspond to reduction in
memorization of label-noise and better generalization bounds. To obtain these
low values, we propose training algorithms that employ an auxiliary network
that predicts gradients in the final layers of a classifier without accessing
labels. We illustrate the effectiveness of our approach on versions of MNIST,
CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a
large-scale dataset Clothing1M that has noisy labels.
</p>
<a href="http://arxiv.org/abs/2002.07933" target="_blank">arXiv:2002.07933</a> [<a href="http://arxiv.org/pdf/2002.07933" target="_blank">pdf</a>]

<h2>Online Learning in Contextual Bandits using Gated Linear Networks. (arXiv:2002.11611v2 [cs.LG] UPDATED)</h2>
<h3>Eren Sezener, Marcus Hutter, David Budden, Jianan Wang, Joel Veness</h3>
<p>We introduce a new and completely online contextual bandit algorithm called
Gated Linear Contextual Bandits (GLCB). This algorithm is based on Gated Linear
Networks (GLNs), a recently introduced deep learning architecture with
properties well-suited to the online setting. Leveraging data-dependent gating
properties of the GLN we are able to estimate prediction uncertainty with
effectively zero algorithmic overhead. We empirically evaluate GLCB compared to
9 state-of-the-art algorithms that leverage deep neural networks, on a standard
benchmark suite of discrete and continuous contextual bandit problems. GLCB
obtains median first-place despite being the only online method, and we further
support these results with a theoretical study of its convergence properties.
</p>
<a href="http://arxiv.org/abs/2002.11611" target="_blank">arXiv:2002.11611</a> [<a href="http://arxiv.org/pdf/2002.11611" target="_blank">pdf</a>]

<h2>DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-motion from Monocular Videos. (arXiv:2003.01360v3 [cs.CV] UPDATED)</h2>
<h3>Hualie Jiang, Laiyan Ding, Zhenglong Sun, Rui Huang</h3>
<p>Unsupervised learning of depth and ego-motion from unlabelled monocular
videos has recently drawn great attention, which avoids the use of expensive
ground truth in the supervised one. It achieves this by using the photometric
errors between the target view and the synthesized views from its adjacent
source views as the loss. Despite significant progress, the learning still
suffers from occlusion and scene dynamics. This paper shows that carefully
manipulating photometric errors can tackle these difficulties better. The
primary improvement is achieved by a statistical technique that can mask out
the invisible or nonstationary pixels in the photometric error map and thus
prevents misleading the networks. With this outlier masking approach, the depth
of objects moving in the opposite direction to the camera can be estimated more
accurately. To the best of our knowledge, such scenarios have not been
seriously considered in the previous works, even though they pose a higher risk
in applications like autonomous driving. We also propose an efficient weighted
multi-scale scheme to reduce the artifacts in the predicted depth maps.
Extensive experiments on the KITTI dataset show the effectiveness of the
proposed approaches. The overall system achieves state-of-theart performance on
both depth and ego-motion estimation.
</p>
<a href="http://arxiv.org/abs/2003.01360" target="_blank">arXiv:2003.01360</a> [<a href="http://arxiv.org/pdf/2003.01360" target="_blank">pdf</a>]

<h2>Causality-aware counterfactual confounding adjustment for feature representations learned by deep models. (arXiv:2004.09466v4 [cs.LG] UPDATED)</h2>
<h3>Elias Chaibub Neto</h3>
<p>Causal modeling has been recognized as a potential solution to many
challenging problems in machine learning (ML). Here, we describe how a recently
proposed counterfactual approach developed to deconfound linear structural
causal models can still be used to deconfound the feature representations
learned by deep neural network (DNN) models. The key insight is that by
training an accurate DNN using softmax activation at the classification layer,
and then adopting the representation learned by the last layer prior to the
output layer as our features, we have that, by construction, the learned
features will fit well a (multi-class) logistic regression model, and will be
linearly associated with the labels. As a consequence, deconfounding approaches
based on simple linear models can be used to deconfound the feature
representations learned by DNNs. We validate the proposed methodology using
colored versions of the MNIST dataset. Our results illustrate how the approach
can effectively combat confounding and improve model stability in the context
of dataset shifts generated by selection biases.
</p>
<a href="http://arxiv.org/abs/2004.09466" target="_blank">arXiv:2004.09466</a> [<a href="http://arxiv.org/pdf/2004.09466" target="_blank">pdf</a>]

<h2>Graph-based State Representation for Deep Reinforcement Learning. (arXiv:2004.13965v2 [cs.LG] UPDATED)</h2>
<h3>Vikram Waradpande, Daniel Kudenko, Megha Khosla</h3>
<p>Deep RL approaches build much of their success on the ability of the deep
neural network to generate useful internal representations. Nevertheless, they
suffer from a high sample-complexity and starting with a good input
representation can have a significant impact on the performance. In this paper,
we exploit the fact that the underlying Markov decision process (MDP)
represents a graph, which enables us to incorporate the topological information
for effective state representation learning.

Motivated by the recent success of node representations for several graph
analytical tasks we specifically investigate the capability of node
representation learning methods to effectively encode the topology of the
underlying MDP in Deep RL. To this end we perform a comparative analysis of
several models chosen from 4 different classes of representation learning
algorithms for policy learning in grid-world navigation tasks, which are
representative of a large class of RL problems. We find that all embedding
methods outperform the commonly used matrix representation of grid-world
environments in all of the studied cases. Moreoever, graph convolution based
methods are outperformed by simpler random walk based methods and graph linear
autoencoders.
</p>
<a href="http://arxiv.org/abs/2004.13965" target="_blank">arXiv:2004.13965</a> [<a href="http://arxiv.org/pdf/2004.13965" target="_blank">pdf</a>]

<h2>On the Transferability of Winning Tickets in Non-Natural Image Datasets. (arXiv:2005.05232v2 [cs.CV] UPDATED)</h2>
<h3>Matthia Sabatelli, Mike Kestemont, Pierre Geurts</h3>
<p>We study the generalization properties of pruned neural networks that are the
winners of the lottery ticket hypothesis on datasets of natural images. We
analyse their potential under conditions in which training data is scarce and
comes from a non-natural domain. Specifically, we investigate whether pruned
models that are found on the popular CIFAR-10/100 and Fashion-MNIST datasets,
generalize to seven different datasets that come from the fields of digital
pathology and digital heritage. Our results show that there are significant
benefits in transferring and training sparse architectures over larger
parametrized models, since in all of our experiments pruned networks, winners
of the lottery ticket hypothesis, significantly outperform their larger
unpruned counterparts. These results suggest that winning initializations do
contain inductive biases that are generic to some extent, although, as reported
by our experiments on the biomedical datasets, their generalization properties
can be more limiting than what has been so far observed in the literature.
</p>
<a href="http://arxiv.org/abs/2005.05232" target="_blank">arXiv:2005.05232</a> [<a href="http://arxiv.org/pdf/2005.05232" target="_blank">pdf</a>]

<h2>Visual Transformers: Token-based Image Representation and Processing for Computer Vision. (arXiv:2006.03677v4 [cs.CV] UPDATED)</h2>
<h3>Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda</h3>
<p>Computer vision has achieved remarkable success by (a) representing images as
uniformly-arranged pixel arrays and (b) convolving highly-localized features.
However, convolutions treat all image pixels equally regardless of importance;
explicitly model all concepts across all images, regardless of content; and
struggle to relate spatially-distant concepts. In this work, we challenge this
paradigm by (a) representing images as semantic visual tokens and (b) running
transformers to densely model token relationships. Critically, our Visual
Transformer operates in a semantic token space, judiciously attending to
different image parts based on context. This is in sharp contrast to
pixel-space transformers that require orders-of-magnitude more compute. Using
an advanced training recipe, our VTs significantly outperform their
convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to
7 points while using fewer FLOPs and parameters. For semantic segmentation on
LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points
higher mIoU while reducing the FPN module's FLOPs by 6.5x.
</p>
<a href="http://arxiv.org/abs/2006.03677" target="_blank">arXiv:2006.03677</a> [<a href="http://arxiv.org/pdf/2006.03677" target="_blank">pdf</a>]

<h2>Learning Long-Term Dependencies in Irregularly-Sampled Time Series. (arXiv:2006.04418v3 [cs.LG] UPDATED)</h2>
<h3>Mathias Lechner, Ramin Hasani</h3>
<p>Recurrent neural networks (RNNs) with continuous-time hidden states are a
natural fit for modeling irregularly-sampled time series. These models,
however, face difficulties when the input data possess long-term dependencies.
We prove that similar to standard RNNs, the underlying reason for this issue is
the vanishing or exploding of the gradient during training. This phenomenon is
expressed by the ordinary differential equation (ODE) representation of the
hidden state, regardless of the ODE solver's choice. We provide a solution by
designing a new algorithm based on the long short-term memory (LSTM) that
separates its memory from its time-continuous state. This way, we encode a
continuous-time dynamical flow within the RNN, allowing it to respond to inputs
arriving at arbitrary time-lags while ensuring a constant error propagation
through the memory path. We call these RNN models ODE-LSTMs. We experimentally
show that ODE-LSTMs outperform advanced RNN-based counterparts on non-uniformly
sampled data with long-term dependencies. All code and data is available at
https://github.com/mlech26l/ode-lstms.
</p>
<a href="http://arxiv.org/abs/2006.04418" target="_blank">arXiv:2006.04418</a> [<a href="http://arxiv.org/pdf/2006.04418" target="_blank">pdf</a>]

<h2>Liquid Time-constant Networks. (arXiv:2006.04439v3 [cs.LG] UPDATED)</h2>
<h3>Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, Radu Grosu</h3>
<p>We introduce a new class of time-continuous recurrent neural network models.
Instead of declaring a learning system's dynamics by implicit nonlinearities,
we construct networks of linear first-order dynamical systems modulated via
nonlinear interlinked gates. The resulting models represent dynamical systems
with varying (i.e., \emph{liquid}) time-constants coupled to their hidden
state, with outputs being computed by numerical differential equation solvers.
These neural networks exhibit stable and bounded behavior, yield superior
expressivity within the family of neural ordinary differential equations, and
give rise to improved performance on time-series prediction tasks. To
demonstrate these properties, we first take a theoretical approach to find
bounds over their dynamics, and compute their expressive power by the
\emph{trajectory length} measure in a latent trajectory space. We then conduct
a series of time-series prediction experiments to manifest the approximation
capability of Liquid Time-Constant Networks (LTCs) compared to modern RNNs.
Code and data are available at
https://github.com/raminmh/liquid_time_constant_networks
</p>
<a href="http://arxiv.org/abs/2006.04439" target="_blank">arXiv:2006.04439</a> [<a href="http://arxiv.org/pdf/2006.04439" target="_blank">pdf</a>]

<h2>Deep Residual Mixture Models. (arXiv:2006.12063v2 [cs.LG] UPDATED)</h2>
<h3>Perttu H&#xe4;m&#xe4;l&#xe4;inen, Tuure Saloheimo, Arno Solin</h3>
<p>We propose Deep Residual Mixture Models (DRMMs), a novel deep generative
model architecture. Compared to other deep models, DRMMs allow more flexible
conditional sampling: The model can be trained once with all variables, and
then used for sampling with arbitrary combinations of conditioning variables,
Gaussian priors, and (in)equality constraints. This provides new opportunities
for interactive and exploratory machine learning, where the user does not have
to wait for retraining a model. We demonstrate these benefits in constrained
multi-limb inverse kinematics, movement planning, and image completion.
</p>
<a href="http://arxiv.org/abs/2006.12063" target="_blank">arXiv:2006.12063</a> [<a href="http://arxiv.org/pdf/2006.12063" target="_blank">pdf</a>]

<h2>Adversarial Example Games. (arXiv:2007.00720v5 [cs.LG] UPDATED)</h2>
<h3>Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, William L. Hamilton</h3>
<p>The existence of adversarial examples capable of fooling trained neural
network classifiers calls for a much better understanding of possible attacks
to guide the development of safeguards against them. This includes attack
methods in the challenging non-interactive blackbox setting, where adversarial
attacks are generated without any access, including queries, to the target
model. Prior attacks in this setting have relied mainly on algorithmic
innovations derived from empirical observations (e.g., that momentum helps),
lacking principled transferability guarantees. In this work, we provide a
theoretical foundation for crafting transferable adversarial examples to entire
hypothesis classes. We introduce Adversarial Example Games (AEG), a framework
that models the crafting of adversarial examples as a min-max game between a
generator of attacks and a classifier. AEG provides a new way to design
adversarial examples by adversarially training a generator and a classifier
from a given hypothesis class (e.g., architecture). We prove that this game has
an equilibrium, and that the optimal generator is able to craft adversarial
examples that can attack any classifier from the corresponding hypothesis
class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets,
outperforming prior state-of-the-art approaches with an average relative
improvement of $29.9\%$ and $47.2\%$ against undefended and robust models
(Table 2 &amp; 3) respectively.
</p>
<a href="http://arxiv.org/abs/2007.00720" target="_blank">arXiv:2007.00720</a> [<a href="http://arxiv.org/pdf/2007.00720" target="_blank">pdf</a>]

<h2>Meta-Learning Stationary Stochastic Process Prediction with Convolutional Neural Processes. (arXiv:2007.01332v2 [stat.ML] UPDATED)</h2>
<h3>Andrew Y. K. Foong, Wessel P. Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, Richard E. Turner</h3>
<p>Stationary stochastic processes (SPs) are a key component of many
probabilistic models, such as those for off-the-grid spatio-temporal data. They
enable the statistical symmetry of underlying physical phenomena to be
leveraged, thereby aiding generalization. Prediction in such models can be
viewed as a translation equivariant map from observed data sets to predictive
SPs, emphasizing the intimate relationship between stationarity and
equivariance. Building on this, we propose the Convolutional Neural Process
(ConvNP), which endows Neural Processes (NPs) with translation equivariance and
extends convolutional conditional NPs to allow for dependencies in the
predictive distribution. The latter enables ConvNPs to be deployed in settings
which require coherent samples, such as Thompson sampling or conditional image
completion. Moreover, we propose a new maximum-likelihood objective to replace
the standard ELBO objective in NPs, which conceptually simplifies the framework
and empirically improves performance. We demonstrate the strong performance and
generalization capabilities of ConvNPs on 1D regression, image completion, and
various tasks with real-world spatio-temporal data.
</p>
<a href="http://arxiv.org/abs/2007.01332" target="_blank">arXiv:2007.01332</a> [<a href="http://arxiv.org/pdf/2007.01332" target="_blank">pdf</a>]

<h2>First Steps: Latent-Space Control with Semantic Constraints for Quadruped Locomotion. (arXiv:2007.01520v2 [cs.RO] UPDATED)</h2>
<h3>Alexander L. Mitchell, Martin Engelcke, Oiwi Parker Jones, David Surovik, Siddhant Gangapurwala, Oliwier Melon, Ioannis Havoutis, Ingmar Posner</h3>
<p>Traditional approaches to quadruped control frequently employ simplified,
hand-derived models. This significantly reduces the capability of the robot
since its effective kinematic range is curtailed. In addition, kinodynamic
constraints are often non-differentiable and difficult to implement in an
optimisation approach. In this work, these challenges are addressed by framing
quadruped control as optimisation in a structured latent space. A deep
generative model captures a statistical representation of feasible joint
configurations, whilst complex dynamic and terminal constraints are expressed
via high-level, semantic indicators and represented by learned classifiers
operating upon the latent space. As a consequence, complex constraints are
rendered differentiable and evaluated an order of magnitude faster than
analytical approaches. We validate the feasibility of locomotion trajectories
optimised using our approach both in simulation and on a real-world ANYmal
quadruped. Our results demonstrate that this approach is capable of generating
smooth and realisable trajectories. To the best of our knowledge, this is the
first time latent space control has been successfully applied to a complex,
real robot platform.
</p>
<a href="http://arxiv.org/abs/2007.01520" target="_blank">arXiv:2007.01520</a> [<a href="http://arxiv.org/pdf/2007.01520" target="_blank">pdf</a>]

<h2>Top-Related Meta-Learning Method for Few-Shot Detection. (arXiv:2007.06837v5 [cs.CV] UPDATED)</h2>
<h3>Qian Li, Nan Guo, Xiaochun Ye, Duo Wang, Dongrui Fan, Zhimin Tang</h3>
<p>Many meta-learning methods are proposed for few-shot detection. However,
previous most methods have two main problems, poor detection APs, and strong
bias because of imbalance datasets. Previous works mainly alleviate these
issues by additional datasets, multi-relation attention mechanisms and
sub-modules. However, they require more cost. In this work, for meta-learning,
we find that the main challenges focus on related or irrelevant semantic
features between different categories, and poor distribution of category-based
meta-features. Therefore, we propose a Top-C classification loss (i.e. TCL-C)
for classification task and a category-based grouping mechanism. The TCL
exploits true-label and the most similar class to improve detection performance
on few-shot classes. According to appearance and environment, the
category-based grouping mechanism groups categories into different groupings to
make similar semantic features more compact for different categories,
alleviating the strong bias problem and further improving detection APs. The
whole training consists of the base model and the fine-tuning phase. During
training detection model, the category-related meta-features are regarded as
the weights to convolve dynamically, exploiting the meta-features with a shared
distribution between categories within a group to improve the detection
performance. According to grouping mechanism, we group the meta-features
vectors, so that the distribution difference between groups is obvious, and the
one within each group is less. Extensive experiments on Pascal VOC dataset
demonstrate that ours which combines the TCL with category-based grouping
significantly outperforms previous state-of-the-art methods for few-shot
detection. Compared with previous competitive baseline, ours improves detection
AP by almost 4% for few-shot detection.
</p>
<a href="http://arxiv.org/abs/2007.06837" target="_blank">arXiv:2007.06837</a> [<a href="http://arxiv.org/pdf/2007.06837" target="_blank">pdf</a>]

<h2>Unsupervised Multi-Target Domain Adaptation Through Knowledge Distillation. (arXiv:2007.07077v4 [cs.CV] UPDATED)</h2>
<h3>Le Thanh Nguyen-Meidine, Atif Belal, Madhu Kiran, Jose Dolz, Louis-Antoine Blais-Morin, Eric Granger</h3>
<p>Unsupervised domain adaptation (UDA) seeks to alleviate the problem of domain
shift between the distribution of unlabeled data from the target domain w.r.t.
labeled data from the source domain. While the single-target UDA scenario is
well studied in the literature, Multi-Target Domain Adaptation (MTDA) remains
largely unexplored despite its practical importance, e.g., in multi-camera
video-surveillance applications. The MTDA problem can be addressed by adapting
one specialized model per target domain, although this solution is too costly
in many real-world applications. Blending multiple targets for MTDA has been
proposed, yet this solution may lead to a reduction in model specificity and
accuracy. In this paper, we propose a novel unsupervised MTDA approach to train
a CNN that can generalize well across multiple target domains. Our
Multi-Teacher MTDA (MT-MTDA) method relies on multi-teacher knowledge
distillation (KD) to iteratively distill target domain knowledge from multiple
teachers to a common student. The KD process is performed in a progressive
manner, where the student is trained by each teacher on how to perform UDA for
a specific target, instead of directly learning domain adapted features.
Finally, instead of combining the knowledge from each teacher, MT-MTDA
alternates between teachers that distill knowledge, thereby preserving the
specificity of each target (teacher) when learning to adapt to the student.
MT-MTDA is compared against state-of-the-art methods on several challenging UDA
benchmarks, and empirical results show that our proposed model can provide a
considerably higher level of accuracy across multiple target domains. Our code
is available at: https://github.com/LIVIAETS/MT-MTDA
</p>
<a href="http://arxiv.org/abs/2007.07077" target="_blank">arXiv:2007.07077</a> [<a href="http://arxiv.org/pdf/2007.07077" target="_blank">pdf</a>]

<h2>Puzzle-AE: Novelty Detection in Images through Solving Puzzles. (arXiv:2008.12959v3 [cs.CV] UPDATED)</h2>
<h3>Mohammadreza Salehi, Ainaz Eftekhar, Niousha Sadjadi, Mohammad Hossein Rohban, Hamid R. Rabiee</h3>
<p>Autoencoder, as an essential part of many anomaly detection methods, is
lacking flexibility on normal data in complex datasets. U-Net is proved to be
effective for this purpose but overfits on the training data if trained by just
using reconstruction error similar to other AE-based frameworks.
Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods,
has earlier proved its ability in learning semantically meaningful features. We
show that training U-Nets based on this task is an effective remedy that
prevents overfitting and facilitates learning beyond pixel-level features.
Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw
puzzles. We propose adversarial robust training as an effective automatic
shortcut removal. We achieve competitive or superior results compared to the
State of the Art (SOTA) anomaly detection methods on various toy and real-world
datasets. Unlike many competitors, the proposed framework is stable, fast,
data-efficient, and does not require unprincipled early stopping.
</p>
<a href="http://arxiv.org/abs/2008.12959" target="_blank">arXiv:2008.12959</a> [<a href="http://arxiv.org/pdf/2008.12959" target="_blank">pdf</a>]

<h2>Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information. (arXiv:2009.01454v2 [cs.LG] UPDATED)</h2>
<h3>Enyan Dai, Suhang Wang</h3>
<p>Graph neural networks (GNNs) have achieved state-of-the-art performance in
modeling graphs. Despite its great success, as with many other models, GNNs
have the risk to inherit the bias from the training data. In addition, the bias
of GNN can be magnified by the graph structures and message-passing mechanism
of GNNs. The risk of discrimination limits the adoption of GNNs in sensitive
domains such as credit score estimation. Though extensive studies of fair
classification have been conducted on i.i.d data, methods to address the
problem of discrimination on non-i.i.d data are rather limited. Furthermore,
the practical scenario of sparse annotations in sensitive attributes is rarely
considered in existing works. Therefore, we study the novel and important
problem of learning fair GNNs with limited sensitive information. We propose a
novel framework called FairGNN, which is able to reduce the bias of GNNs and
maintain high node classification accuracy by leveraging graph structured data
and sensitive information. Theoretical analysis is conducted to show that
FairGNN can ensure fairness under mild conditions given limited nodes with
known sensitive attributes. Experiments on real-world datasets demonstrated the
effectiveness of the proposed framework in eliminating discrimination while
maintaining high node classification accuracy.
</p>
<a href="http://arxiv.org/abs/2009.01454" target="_blank">arXiv:2009.01454</a> [<a href="http://arxiv.org/pdf/2009.01454" target="_blank">pdf</a>]

<h2>Multilinear Common Component Analysis via Kronecker Product Representation. (arXiv:2009.02695v2 [stat.ML] UPDATED)</h2>
<h3>Kohei Yoshikawa, Shuichi Kawano</h3>
<p>We consider the problem of extracting a common structure from multiple tensor
datasets. For this purpose, we propose multilinear common component analysis
(MCCA) based on Kronecker products of mode-wise covariance matrices. MCCA
constructs a common basis represented by linear combinations of the original
variables which loses as little information of the multiple tensor datasets. We
also develop an estimation algorithm for MCCA that guarantees mode-wise global
convergence. Numerical studies are conducted to show the effectiveness of MCCA.
</p>
<a href="http://arxiv.org/abs/2009.02695" target="_blank">arXiv:2009.02695</a> [<a href="http://arxiv.org/pdf/2009.02695" target="_blank">pdf</a>]

<h2>Federated Generalized Bayesian Learning via Distributed Stein Variational Gradient Descent. (arXiv:2009.06419v5 [cs.LG] UPDATED)</h2>
<h3>Rahif Kassab, Osvaldo Simeone</h3>
<p>This paper introduces Distributed Stein Variational Gradient Descent (DSVGD),
a non-parametric generalized Bayesian inference framework for federated
learning. DSVGD maintains a number of non-random and interacting particles at a
central server to represent the current iterate of the model global posterior.
The particles are iteratively downloaded and updated by one of the agents with
the end goal of minimizing the global free energy. By varying the number of
particles, DSVGD enables a flexible trade-off between per-iteration
communication load and number of communication rounds. DSVGD is shown to
compare favorably to benchmark frequentist and Bayesian federated learning
strategies, also scheduling a single device per iteration, in terms of accuracy
and scalability with respect to the number of agents, while also providing
well-calibrated, and hence trustworthy, predictions.
</p>
<a href="http://arxiv.org/abs/2009.06419" target="_blank">arXiv:2009.06419</a> [<a href="http://arxiv.org/pdf/2009.06419" target="_blank">pdf</a>]

<h2>Training Data Augmentation for Deep Learning RF Systems. (arXiv:2010.00178v3 [cs.LG] UPDATED)</h2>
<h3>William H. Clark IV, Steven Hauser, William C. Headley, Alan J. Michaels</h3>
<p>Applications of machine learning are subject to three major components that
contribute to the final performance metrics. Within the category of neural
networks, and deep learning specifically, the first two are the architecture
for the model being trained and the training approach used. This work focuses
on the third component, the data being used during training. The questions that
arise are then "what is in the data" and "what within the data matters?"
Looking into the Radio Frequency Machine Learning (RFML) field of Modulation
Classification, the use of synthetic, captured, and augmented data are examined
and compared to provide insights about the quantity and quality of the
available data presented to the training routine. In general, all three data
types have useful contributions to a final application, but captured data
germane to the intended use case will always provide more significant
information and enable the greatest performance. Despite the benefit of
captured data, the difficulties that arise from collection often make the
quantity of data needed to achieve peak performance impractical. This paper
helps quantify the balance between real and synthetic data, offering concrete
examples where training data is parametrically varied in size and source.
</p>
<a href="http://arxiv.org/abs/2010.00178" target="_blank">arXiv:2010.00178</a> [<a href="http://arxiv.org/pdf/2010.00178" target="_blank">pdf</a>]

<h2>Transformers for Modeling Physical Systems. (arXiv:2010.03957v2 [cs.LG] UPDATED)</h2>
<h3>Nicholas Geneva, Nicholas Zabaras</h3>
<p>Transformers are widely used in neural language processing due to their
ability to model longer-term dependencies in text. Although these models
achieve state-of-the-art performance for many language related tasks, their
applicability outside of the neural language processing field has been minimal.
In this work, we propose the use of transformer models for the prediction of
dynamical systems representative of physical phenomena. The use of Koopman
based embeddings provide a unique and powerful method for projecting any
dynamical system into a vector representation which can then be predicted by a
transformer model. The proposed model is able to accurately predict various
dynamical systems and outperform classical methods that are commonly used in
the scientific machine learning literature.
</p>
<a href="http://arxiv.org/abs/2010.03957" target="_blank">arXiv:2010.03957</a> [<a href="http://arxiv.org/pdf/2010.03957" target="_blank">pdf</a>]

<h2>Self-Supervised Ranking for Representation Learning. (arXiv:2010.07258v2 [cs.CV] UPDATED)</h2>
<h3>Ali Varamesh, Ali Diba, Tinne Tuytelaars, Luc Van Gool</h3>
<p>We present a new framework for self-supervised representation learning by
formulating it as a ranking problem in an image retrieval context on a large
number of random views (augmentations) obtained from images. Our work is based
on two intuitions: first, a good representation of images must yield a
high-quality image ranking in a retrieval task; second, we would expect random
views of an image to be ranked closer to a reference view of that image than
random views of other images. Hence, we model representation learning as a
learning to rank problem for image retrieval. We train a representation encoder
by maximizing average precision (AP) for ranking, where random views of an
image are considered positively related, and that of the other images
considered negatives. The new framework, dubbed S2R2, enables computing a
global objective on multiple views, compared to the local objective in the
popular contrastive learning framework, which is calculated on pairs of views.
In principle, by using a ranking criterion, we eliminate reliance on
object-centric curated datasets. When trained on STL10 and MS-COCO, S2R2
outperforms SimCLR and the clustering-based contrastive learning model, SwAV,
while being much simpler both conceptually and at implementation. On MS-COCO,
S2R2 outperforms both SwAV and SimCLR with a larger margin than on STl10. This
indicates that S2R2 is more effective on diverse scenes and could eliminate the
need for an object-centric large training dataset for self-supervised
representation learning.
</p>
<a href="http://arxiv.org/abs/2010.07258" target="_blank">arXiv:2010.07258</a> [<a href="http://arxiv.org/pdf/2010.07258" target="_blank">pdf</a>]

<h2>Training Generative Adversarial Networks via stochastic Nash games. (arXiv:2010.10013v2 [cs.LG] UPDATED)</h2>
<h3>Barbara Franci, Sergio Grammatico</h3>
<p>Generative adversarial networks (GANs) are a class of generative models with
two antagonistic neural networks: the generator and the discriminator. These
two neural networks compete against each other through an adversarial process
that can be modelled as a stochastic Nash equilibrium problem. Since the
associated training process is challenging, it is fundamental to design
reliable algorithms to compute an equilibrium. In this paper, we propose a
stochastic relaxed forward-backward algorithm for GANs and we show convergence
to an exact solution or to a neighbourhood of it, if the pseudogradient mapping
of the game is monotone. We apply our algorithm to the image generation problem
where we observe computational advantages with respect to the extragradient
scheme.
</p>
<a href="http://arxiv.org/abs/2010.10013" target="_blank">arXiv:2010.10013</a> [<a href="http://arxiv.org/pdf/2010.10013" target="_blank">pdf</a>]

<h2>An Asymptotically Optimal Primal-Dual Incremental Algorithm for Contextual Linear Bandits. (arXiv:2010.12247v2 [cs.LG] UPDATED)</h2>
<h3>Andrea Tirinzoni, Matteo Pirotta, Marcello Restelli, Alessandro Lazaric</h3>
<p>In the contextual linear bandit setting, algorithms built on the optimism
principle fail to exploit the structure of the problem and have been shown to
be asymptotically suboptimal. In this paper, we follow recent approaches of
deriving asymptotically optimal algorithms from problem-dependent regret lower
bounds and we introduce a novel algorithm improving over the state-of-the-art
along multiple dimensions. We build on a reformulation of the lower bound,
where context distribution and exploration policy are decoupled, and we obtain
an algorithm robust to unbalanced context distributions. Then, using an
incremental primal-dual approach to solve the Lagrangian relaxation of the
lower bound, we obtain a scalable and computationally efficient algorithm.
Finally, we remove forced exploration and build on confidence intervals of the
optimization problem to encourage a minimum level of exploration that is better
adapted to the problem structure. We demonstrate the asymptotic optimality of
our algorithm, while providing both problem-dependent and worst-case
finite-time regret guarantees. Our bounds scale with the logarithm of the
number of arms, thus avoiding the linear dependence common in all related prior
works. Notably, we establish minimax optimality for any learning horizon in the
special case of non-contextual linear bandits. Finally, we verify that our
algorithm obtains better empirical performance than state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/2010.12247" target="_blank">arXiv:2010.12247</a> [<a href="http://arxiv.org/pdf/2010.12247" target="_blank">pdf</a>]

<h2>A Clarifying Question Selection System from NTES_ALONG in Convai3 Challenge. (arXiv:2010.14202v3 [cs.AI] UPDATED)</h2>
<h3>Wenjie Ou, Yue Lin</h3>
<p>This paper presents the participation of NetEase Game AI Lab team for the
ClariQ challenge at Search-oriented Conversational AI (SCAI) EMNLP workshop in
2020. The challenge asks for a complete conversational information retrieval
system that can understanding and generating clarification questions. We
propose a clarifying question selection system which consists of response
understanding, candidate question recalling and clarifying question ranking. We
fine-tune a RoBERTa model to understand user's responses and use an enhanced
BM25 model to recall the candidate questions. In clarifying question ranking
stage, we reconstruct the training dataset and propose two models based on
ELECTRA. Finally we ensemble the models by summing up their output
probabilities and choose the question with the highest probability as the
clarification question. Experiments show that our ensemble ranking model
outperforms in the document relevance task and achieves the best recall@[20,30]
metrics in question relevance task. And in multi-turn conversation evaluation
in stage2, our system achieve the top score of all document relevance metrics.
</p>
<a href="http://arxiv.org/abs/2010.14202" target="_blank">arXiv:2010.14202</a> [<a href="http://arxiv.org/pdf/2010.14202" target="_blank">pdf</a>]

<h2>HypperSteer: Hypothetical Steering and Data Perturbation in Sequence Prediction with Deep Learning. (arXiv:2011.02149v2 [cs.LG] UPDATED)</h2>
<h3>Chuan Wang, Kwan-Liu Ma</h3>
<p>Deep Recurrent Neural Networks (RNN) continues to find success in predictive
decision-making with temporal event sequences. Recent studies have shown the
importance and practicality of visual analytics in interpreting deep learning
models for real-world applications. However, very limited work enables
interactions with deep learning models and guides practitioners to form
hypotheticals towards the desired prediction outcomes, especially for sequence
prediction. Specifically, no existing work has addressed the what-if analysis
and value perturbation along different time-steps for sequence outcome
prediction. We present a model-agnostic visual analytics tool, HypperSteer,
that steers hypothetical testing and allows users to perturb data for sequence
predictions interactively. We showcase how HypperSteer helps in steering
patient data to achieve desired treatment outcomes and discuss how HypperSteer
can serve as a comprehensive solution for other practical scenarios.
</p>
<a href="http://arxiv.org/abs/2011.02149" target="_blank">arXiv:2011.02149</a> [<a href="http://arxiv.org/pdf/2011.02149" target="_blank">pdf</a>]

<h2>Robust Shape Control of Gyroscopic Tensegrity Robotic Arm. (arXiv:2011.03829v2 [cs.RO] UPDATED)</h2>
<h3>Raman Goyal, Manoranjan Majji, Robert E. Skelton</h3>
<p>This paper proposes a model-based approach to control the shape of a
tensegrity system by driving its node position locations. The nonlinear
dynamics of the tensegrity system is used to regulate position, velocity, and
acceleration to the specified reference trajectory. State feedback control
design is used to obtain the solution for the control variable as a linear
programming problem. Shape control for the gyroscopic tensegrity systems is
discussed, and it is observed that these systems increase the reachable space
for the structure by providing independent control over certain rotational
degrees of freedom. Disturbance rejection of the tensegrity system is further
studied in the paper. A methodology to calculate the control gains to bound the
errors for five different types of problems is provided. The formulation uses a
Linear Matrix Inequality (LMI) approach to stipulate the desired performance
bounds on the error for $\mathcal{H}_\infty$, generalized $\mathcal{H}_2$, LQR,
covariance control and stabilizing control problem. A high degree of freedom
tensegrity $T_2D_1$ robotic arm is used as an example to show the efficacy of
the formulation.
</p>
<a href="http://arxiv.org/abs/2011.03829" target="_blank">arXiv:2011.03829</a> [<a href="http://arxiv.org/pdf/2011.03829" target="_blank">pdf</a>]

<h2>Online Sparse Reinforcement Learning. (arXiv:2011.04018v2 [cs.LG] UPDATED)</h2>
<h3>Botao Hao, Tor Lattimore, Csaba Szepesv&#xe1;ri, Mengdi Wang</h3>
<p>We investigate the hardness of online reinforcement learning in fixed
horizon, sparse linear Markov decision process (MDP), with a special focus on
the high-dimensional regime where the ambient dimension is larger than the
number of episodes. Our contribution is two-fold. First, we provide a lower
bound showing that linear regret is generally unavoidable in this case, even if
there exists a policy that collects well-conditioned data. The lower bound
construction uses an MDP with a fixed number of states while the number of
actions scales with the ambient dimension. Note that when the horizon is fixed
to one, the case of linear stochastic bandits, the linear regret can be
avoided. Second, we show that if the learner has oracle access to a policy that
collects well-conditioned data then a variant of Lasso fitted Q-iteration
enjoys a nearly dimension-free regret of $\tilde{O}( s^{2/3} N^{2/3})$ where
$N$ is the number of episodes and $s$ is the sparsity level. This shows that in
the large-action setting, the difficulty of learning can be attributed to the
difficulty of finding a good exploratory policy.
</p>
<a href="http://arxiv.org/abs/2011.04018" target="_blank">arXiv:2011.04018</a> [<a href="http://arxiv.org/pdf/2011.04018" target="_blank">pdf</a>]

<h2>Decentralized Motion Planning for Multi-Robot Navigation using Deep Reinforcement Learning. (arXiv:2011.05605v2 [cs.RO] UPDATED)</h2>
<h3>Sivanathan Kandhasamy, Vinayagam Babu Kuppusamy, Tanmay Vilas Samak, Chinmay Vilas Samak</h3>
<p>This work presents a decentralized motion planning framework for addressing
the task of multi-robot navigation using deep reinforcement learning. A custom
simulator was developed in order to experimentally investigate the navigation
problem of 4 cooperative non-holonomic robots sharing limited state information
with each other in 3 different settings. The notion of decentralized motion
planning with common and shared policy learning was adopted, which allowed
robust training and testing of this approach in a stochastic environment since
the agents were mutually independent and exhibited asynchronous motion
behavior. The task was further aggravated by providing the agents with a sparse
observation space and requiring them to generate continuous action commands so
as to efficiently, yet safely navigate to their respective goal locations,
while avoiding collisions with other dynamic peers and static obstacles at all
times. The experimental results are reported in terms of quantitative measures
and qualitative remarks for both training and deployment phases.
</p>
<a href="http://arxiv.org/abs/2011.05605" target="_blank">arXiv:2011.05605</a> [<a href="http://arxiv.org/pdf/2011.05605" target="_blank">pdf</a>]

<h2>Existence of Two View Chiral Reconstructions. (arXiv:2011.07197v2 [cs.CV] UPDATED)</h2>
<h3>Andrew Pryhuber, Rainer Sinn, Rekha R. Thomas</h3>
<p>A fundamental question in computer vision is whether a set of point pairs is
the image of a scene that lies in front of two cameras. Such a scene and the
cameras together are known as a chiral reconstruction of the point pairs. In
this paper we provide a complete classification of k point pairs for which a
chiral reconstruction exists. The existence of chiral reconstructions is
equivalent to the non-emptiness of certain semialgebraic sets. For up to three
point pairs, we prove that a chiral reconstruction always exists while the set
of five or more point pairs that do not have a chiral reconstruction is
Zariski-dense. We show that for five generic point pairs, the chiral region is
bounded by line segments in a Schl\"afli double six on a cubic surface with 27
real lines. Four point pairs have a chiral reconstruction unless they belong to
two non-generic combinatorial types, in which case they may or may not.
</p>
<a href="http://arxiv.org/abs/2011.07197" target="_blank">arXiv:2011.07197</a> [<a href="http://arxiv.org/pdf/2011.07197" target="_blank">pdf</a>]

<h2>Using Convolutional Variational Autoencoders to Predict Post-Trauma Health Outcomes from Actigraphy Data. (arXiv:2011.07406v2 [cs.LG] UPDATED)</h2>
<h3>Ayse S. Cakmak, Nina Thigpen, Garrett Honke, Erick Perez Alday, Ali Bahrami Rad, Rebecca Adaimi, Chia Jung Chang, Qiao Li, Pramod Gupta, Thomas Neylan, Samuel A. McLean, Gari D. Clifford</h3>
<p>Depression and post-traumatic stress disorder (PTSD) are psychiatric
conditions commonly associated with experiencing a traumatic event. Estimating
mental health status through non-invasive techniques such as activity-based
algorithms can help to identify successful early interventions. In this work,
we used locomotor activity captured from 1113 individuals who wore a research
grade smartwatch post-trauma. A convolutional variational autoencoder (VAE)
architecture was used for unsupervised feature extraction from four weeks of
actigraphy data. By using VAE latent variables and the participant's pre-trauma
physical health status as features, a logistic regression classifier achieved
an area under the receiver operating characteristic curve (AUC) of 0.64 to
estimate mental health outcomes. The results indicate that the VAE model is a
promising approach for actigraphy data analysis for mental health outcomes in
long-term studies.
</p>
<a href="http://arxiv.org/abs/2011.07406" target="_blank">arXiv:2011.07406</a> [<a href="http://arxiv.org/pdf/2011.07406" target="_blank">pdf</a>]

<h2>Predicting Human Strategies in Simulated Search and Rescue Task. (arXiv:2011.07656v2 [cs.LG] UPDATED)</h2>
<h3>Vidhi Jain, Rohit Jena, Huao Li, Tejus Gupta, Dana Hughes, Michael Lewis, Katia Sycara</h3>
<p>In a search and rescue scenario, rescuers may have different knowledge of the
environment and strategies for exploration. Understanding what is inside a
rescuer's mind will enable an observer agent to proactively assist them with
critical information that can help them perform their task efficiently. To this
end, we propose to build models of the rescuers based on their trajectory
observations to predict their strategies. In our efforts to model the rescuer's
mind, we begin with a simple simulated search and rescue task in Minecraft with
human participants. We formulate neural sequence models to predict the triage
strategy and the next location of the rescuer. As the neural networks are
data-driven, we design a diverse set of artificial "faux human" agents for
training, to test them with limited human rescuer trajectory data. To evaluate
the agents, we compare it to an evidence accumulation method that explicitly
incorporates all available background knowledge and provides an intended upper
bound for the expected performance. Further, we perform experiments where the
observer/predictor is human. We show results in terms of prediction accuracy of
our computational approaches as compared with that of human observers.
</p>
<a href="http://arxiv.org/abs/2011.07656" target="_blank">arXiv:2011.07656</a> [<a href="http://arxiv.org/pdf/2011.07656" target="_blank">pdf</a>]

<h2>Obstacle avoidance-driven controller for safety-critical aerial robots. (arXiv:2011.08178v2 [cs.RO] UPDATED)</h2>
<h3>Johann Lange</h3>
<p>The goal of this thesis is to propose the combination of
Control-Barrier-Functions (CBF) with Model-Predictive-Control (MPC) resulting
in the novel Model-Predictive-Control-Barrier-Function (MPCBF). It can be
shown, that the performance of the MPCBF surpasses the performance of the CBF
due to the increased time horizon of the MPC. Moreover, the MPCBF was applied
to a quadrotor, a system strongly in need of fast and predictive control. Using
the MPCBF, the quadrotor was able to avoid obstacles, which the CBF failed to
avoid due to the relative speed of the obstacle. The results of this work are
experimentally validated.
</p>
<a href="http://arxiv.org/abs/2011.08178" target="_blank">arXiv:2011.08178</a> [<a href="http://arxiv.org/pdf/2011.08178" target="_blank">pdf</a>]

<h2>FSPN: A New Class of Probabilistic Graphical Model. (arXiv:2011.09020v2 [cs.AI] UPDATED)</h2>
<h3>Ziniu Wu, Rong Zhu, Andreas Pfadler, Yuxing Han, Jiangneng Li, Zhengping Qian, Kai Zeng, Jingren Zhou</h3>
<p>We introduce factorize sum split product networks (FSPNs), a new class of
probabilistic graphical models (PGMs). FSPNs are designed to overcome the
drawbacks of existing PGMs in terms of estimation accuracy and inference
efficiency. Specifically, Bayesian networks (BNs) have low inference speed and
performance of tree structured sum product networks(SPNs) significantly
degrades in presence of highly correlated variables. FSPNs absorb their
advantages by adaptively modeling the joint distribution of variables according
to their dependence degree, so that one can simultaneously attain the two
desirable goals: high estimation accuracy and fast inference speed. We present
efficient probability inference and structure learning algorithms for FSPNs,
along with a theoretical analysis and extensive evaluation evidence. Our
experimental results on synthetic and benchmark datasets indicate the
superiority of FSPN over other PGMs.
</p>
<a href="http://arxiv.org/abs/2011.09020" target="_blank">arXiv:2011.09020</a> [<a href="http://arxiv.org/pdf/2011.09020" target="_blank">pdf</a>]

<h2>Positive-Congruent Training: Towards Regression-Free Model Updates. (arXiv:2011.09161v2 [cs.CV] UPDATED)</h2>
<h3>Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng Wang, Wei Xia, Stefano Soatto</h3>
<p>Reducing inconsistencies in the behavior of different versions of an AI
system can be as important in practice as reducing its overall error. In image
classification, sample-wise inconsistencies appear as "negative flips:" A new
model incorrectly predicts the output for a test sample that was correctly
classified by the old (reference) model. Positive-congruent (PC) training aims
at reducing error rate while at the same time reducing negative flips, thus
maximizing congruency with the reference model only on positive predictions,
unlike model distillation. We propose a simple approach for PC training, Focal
Distillation, which enforces congruence with the reference model by giving more
weights to samples that were correctly classified. We also found that, if the
reference model itself can be chosen as an ensemble of multiple deep neural
networks, negative flips can be further reduced without affecting the new
model's accuracy.
</p>
<a href="http://arxiv.org/abs/2011.09161" target="_blank">arXiv:2011.09161</a> [<a href="http://arxiv.org/pdf/2011.09161" target="_blank">pdf</a>]

<h2>Detecting Hierarchical Changes in Latent Variable Models. (arXiv:2011.09465v2 [stat.ML] UPDATED)</h2>
<h3>Shintaro Fukushima, Kenji Yamanishi</h3>
<p>This paper addresses the issue of detecting hierarchical changes in latent
variable models (HCDL) from data streams. There are three different levels of
changes for latent variable models: 1) the first level is the change in data
distribution for fixed latent variables, 2) the second one is that in the
distribution over latent variables, and 3) the third one is that in the number
of latent variables. It is important to detect these changes because we can
analyze the causes of changes by identifying which level a change comes from
(change interpretability). This paper proposes an information-theoretic
framework for detecting changes of the three levels in a hierarchical way. The
key idea to realize it is to employ the MDL (minimum description length) change
statistics for measuring the degree of change, in combination with DNML
(decomposed normalized maximum likelihood) code-length calculation. We give a
theoretical basis for making reliable alarms for changes. Focusing on
stochastic block models, we employ synthetic and benchmark datasets to
empirically demonstrate the effectiveness of our framework in terms of change
interpretability as well as change detection.
</p>
<a href="http://arxiv.org/abs/2011.09465" target="_blank">arXiv:2011.09465</a> [<a href="http://arxiv.org/pdf/2011.09465" target="_blank">pdf</a>]

<h2>FROST: Faster and more Robust One-shot Semi-supervised Training. (arXiv:2011.09471v2 [cs.LG] UPDATED)</h2>
<h3>Helena E. Liu, Leslie N. Smith</h3>
<p>Recent advances in one-shot semi-supervised learning have lowered the barrier
for deep learning of new applications. However, the state-of-the-art for
semi-supervised learning is slow to train and the performance is sensitive to
the choices of the labeled data and hyper-parameter values. In this paper, we
present a one-shot semi-supervised learning method that trains up to an order
of magnitude faster and is more robust than state-of-the-art methods.
Specifically, we show that by combining semi-supervised learning with a
one-stage, single network version of self-training, our FROST methodology
trains faster and is more robust to choices for the labeled samples and changes
in hyper-parameters. Our experiments demonstrate FROST's capability to perform
well when the composition of the unlabeled data is unknown; that is when the
unlabeled data contain unequal numbers of each class and can contain
out-of-distribution examples that don't belong to any of the training classes.
High performance, speed of training, and insensitivity to hyper-parameters make
FROST the most practical method for one-shot semi-supervised training.
</p>
<a href="http://arxiv.org/abs/2011.09471" target="_blank">arXiv:2011.09471</a> [<a href="http://arxiv.org/pdf/2011.09471" target="_blank">pdf</a>]

<h2>Learning Recurrent Neural Net Models of Nonlinear Systems. (arXiv:2011.09573v2 [cs.LG] UPDATED)</h2>
<h3>Joshua Hanson, Maxim Raginsky, Eduardo Sontag</h3>
<p>We consider the following learning problem: Given sample pairs of input and
output signals generated by an unknown nonlinear system (which is not assumed
to be causal or time-invariant), we wish to find a continuous-time recurrent
neural net with hyperbolic tangent activation function that approximately
reproduces the underlying i/o behavior with high confidence. Leveraging earlier
work concerned with matching output derivatives up to a given finite order, we
reformulate the learning problem in familiar system-theoretic language and
derive quantitative guarantees on the sup-norm risk of the learned model in
terms of the number of neurons, the sample size, the number of derivatives
being matched, and the regularity properties of the inputs, the outputs, and
the unknown i/o map.
</p>
<a href="http://arxiv.org/abs/2011.09573" target="_blank">arXiv:2011.09573</a> [<a href="http://arxiv.org/pdf/2011.09573" target="_blank">pdf</a>]

<h2>Attention-Based Transformers for Instance Segmentation of Cells in Microstructures. (arXiv:2011.09763v2 [cs.CV] UPDATED)</h2>
<h3>Tim Prangemeier, Christoph Reich, Heinz Koeppl</h3>
<p>Detecting and segmenting object instances is a common task in biomedical
applications. Examples range from detecting lesions on functional magnetic
resonance images, to the detection of tumours in histopathological images and
extracting quantitative single-cell information from microscopy imagery, where
cell segmentation is a major bottleneck. Attention-based transformers are
state-of-the-art in a range of deep learning fields. They have recently been
proposed for segmentation tasks where they are beginning to outperforming other
methods. We present a novel attention-based cell detection transformer
(Cell-DETR) for direct end-to-end instance segmentation. While the segmentation
performance is on par with a state-of-the-art instance segmentation method,
Cell-DETR is simpler and faster. We showcase the method's contribution in a the
typical use case of segmenting yeast in microstructured environments, commonly
employed in systems or synthetic biology. For the specific use case, the
proposed method surpasses the state-of-the-art tools for semantic segmentation
and additionally predicts the individual object instances. The fast and
accurate instance segmentation performance increases the experimental
information yield for a posteriori data processing and makes online monitoring
of experiments and closed-loop optimal experimental design feasible.
</p>
<a href="http://arxiv.org/abs/2011.09763" target="_blank">arXiv:2011.09763</a> [<a href="http://arxiv.org/pdf/2011.09763" target="_blank">pdf</a>]

<h2>Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video. (arXiv:2011.09846v2 [cs.CV] UPDATED)</h2>
<h3>Ben Saunders, Necati Cihan Camgoz, Richard Bowden</h3>
<p>To be truly understandable and accepted by Deaf communities, an automatic
Sign Language Production (SLP) system must generate a photo-realistic signer.
Prior approaches based on graphical avatars have proven unpopular, whereas
recent neural SLP works that produce skeleton pose sequences have been shown to
be not understandable to Deaf viewers.

In this paper, we propose SignGAN, the first SLP model to produce
photo-realistic continuous sign language videos directly from spoken language.
We employ a transformer architecture with a Mixture Density Network (MDN)
formulation to handle the translation from spoken language to skeletal pose. A
pose-conditioned human synthesis model is then introduced to generate a
photo-realistic sign language video from the skeletal pose sequence. This
allows the photo-realistic production of sign videos directly translated from
written text.

We further propose a novel keypoint-based loss function, which significantly
improves the quality of synthesized hand images, operating in the keypoint
space to avoid issues caused by motion blur. In addition, we introduce a method
for controllable video generation, enabling training on large, diverse sign
language datasets and providing the ability to control the signer appearance at
inference.

Using a dataset of eight different sign language interpreters extracted from
broadcast footage, we show that SignGAN significantly outperforms all baseline
methods for quantitative metrics and human perceptual studies.
</p>
<a href="http://arxiv.org/abs/2011.09846" target="_blank">arXiv:2011.09846</a> [<a href="http://arxiv.org/pdf/2011.09846" target="_blank">pdf</a>]

<h2>A Stable High-order Tuner for General Convex Functions. (arXiv:2011.09996v2 [cs.LG] UPDATED)</h2>
<h3>Jos&#xe9; M. Moreu, Anuradha M. Annaswamy</h3>
<p>Iterative gradient-based algorithms have been increasingly applied for the
training of a broad variety of machine learning models including large
neural-nets. In particular, momentum-based methods, with accelerated learning
guarantees, have received a lot of attention due to their provable guarantees
of fast learning in certain classes of problems and multiple algorithms have
been derived. However, properties for these methods hold true only for constant
regressors. When time-varying regressors occur, which is commonplace in dynamic
systems, many of these momentum-based methods cannot guarantee stability.
Recently, a new High-order Tuner (HT) was developed and shown to have 1)
stability and asymptotic convergence for time-varying regressors and 2)
non-asymptotic accelerated learning guarantees for constant regressors. These
results were derived for a linear regression framework producing a quadratic
loss function. In this paper, we extend and discuss the results of this same HT
for general convex loss functions. Through the exploitation of convexity and
smoothness definitions, we establish similar stability and asymptotic
convergence guarantees. Additionally we conjecture that the HT has an
accelerated convergence rate. Finally, we provide numerical simulations
supporting the satisfactory behavior of the HT algorithm as well as the
conjecture of accelerated learning.
</p>
<a href="http://arxiv.org/abs/2011.09996" target="_blank">arXiv:2011.09996</a> [<a href="http://arxiv.org/pdf/2011.09996" target="_blank">pdf</a>]

<h2>Neural Stochastic Contraction Metrics for Learning-based Robust Control and Estimation. (arXiv:2011.03168v2 [cs.LG] CROSS LISTED)</h2>
<h3>Hiroyasu Tsukamoto, Soon-Jo Chung, Jean-Jacques E. Slotine</h3>
<p>We present Neural Stochastic Contraction Metrics (NSCM), a new design
framework for provably-stable robust control and estimation for a class of
stochastic nonlinear systems. It uses a spectrally-normalized deep neural
network to construct a contraction metric, sampled via simplified convex
optimization in the stochastic setting. Spectral normalization constrains the
state-derivatives of the metric to be Lipschitz continuous, thereby ensuring
exponential boundedness of the mean squared distance of system trajectories
under stochastic disturbances. The NSCM framework allows autonomous agents to
approximate optimal stable control and estimation policies in real-time, and
outperforms existing nonlinear control and estimation techniques including the
state-dependent Riccati equation, iterative LQR, EKF, and the deterministic
neural contraction metric, as illustrated in simulation results.
</p>
<a href="http://arxiv.org/abs/2011.03168" target="_blank">arXiv:2011.03168</a> [<a href="http://arxiv.org/pdf/2011.03168" target="_blank">pdf</a>]

