---
title: Latest Deep Learning Papers
date: 2020-10-16 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>Learning, compression, and leakage: Minimizing classification error via meta-universal compression principles. (arXiv:2010.07382v1 [cs.LG])</h2>
<h3>Fernando E. Rosas, Pedro A.M. Mediano, Michael Gastpar</h3>
<p>Learning and compression are driven by the common aim of identifying and
exploiting statistical regularities in data, which opens the door for fertile
collaboration between these areas. A promising group of compression techniques
for learning scenarios is normalised maximum likelihood (NML) coding, which
provides strong guarantees for compression of small datasets - in contrast with
more popular estimators whose guarantees hold only in the asymptotic limit.
Here we put forward a novel NML-based decision strategy for supervised
classification problems, and show that it attains heuristic PAC learning when
applied to a wide variety of models. Furthermore, we show that the
misclassification rate of our method is upper bounded by the maximal leakage, a
recently proposed metric to quantify the potential of data leakage in
privacy-sensitive scenarios.
</p>
<a href="http://arxiv.org/abs/2010.07382" target="_blank">arXiv:2010.07382</a> [<a href="http://arxiv.org/pdf/2010.07382" target="_blank">pdf</a>]

<h2>Anomaly Detection for Bivariate Signals. (arXiv:2010.07420v1 [math.ST])</h2>
<h3>Marie Cottrell, Cynthia Faure, J&#xe9;r&#xf4;me Lacaille, Madalina Olteanu</h3>
<p>The anomaly detection problem for univariate or multivariate time series is a
critical question in many practical applications as industrial processes
control, biological measures, engine monitoring, supervision of all kinds of
behavior. In this paper we propose a simple and empirical approach to detect
anomalies in the behavior of multivariate time series. The approach is based on
the empirical estimation of the conditional quantiles of the data, which
provides upper and lower bounds for the confidence tubes. The method is tested
on artificial data and its effectiveness has been proven in a real framework
such as that of the monitoring of aircraft engines.
</p>
<a href="http://arxiv.org/abs/2010.07420" target="_blank">arXiv:2010.07420</a> [<a href="http://arxiv.org/pdf/2010.07420" target="_blank">pdf</a>]

<h2>An Open-Source Dataset on Dietary Behaviors and DASH Eating Plan Optimization Constraints. (arXiv:2010.07531v1 [math.OC])</h2>
<h3>Farzin Ahmadi, Fardin Ganjkhanloo, Kimia Ghobadi</h3>
<p>Linear constrained optimization techniques have been applied to many
real-world settings. In recent years, inferring the unknown parameters and
functions inside an optimization model has also gained traction. This inference
is often based on existing observations and/or known parameters. Consequently,
such models require reliable, easily accessed, and easily interpreted examples
to be evaluated. To facilitate research in such directions, we provide a
modified dataset based on dietary behaviors of different groups of people,
their demographics, and pre-existing conditions, among other factors. This data
is gathered from the National Health and Nutrition Examination Survey (NHANES)
and complemented with the nutritional data from the United States Department of
Agriculture (USDA). We additionally provide tailored datasets for hypertension
and pre-diabetic patients as groups of interest who may benefit from targetted
diets such as the Dietary Approaches to Stop Hypertension (DASH) eating plan.
The data is compiled and curated in such a way that it is suitable as input to
linear optimization models. We hope that this data and its supplementary,
open-accessed materials can accelerate and simplify interpretations and
research on linear optimization and constrained inference models. The complete
dataset can be found in the following repository:
https://github.com/CSSEHealthcare/InverseLearning
</p>
<a href="http://arxiv.org/abs/2010.07531" target="_blank">arXiv:2010.07531</a> [<a href="http://arxiv.org/pdf/2010.07531" target="_blank">pdf</a>]

<h2>Solving Trust Region Subproblems Using Riemannian Optimization. (arXiv:2010.07547v1 [math.OC])</h2>
<h3>Uria Mor, Haim Avron</h3>
<p>The Trust Region Subproblem is a fundamental optimization problem that takes
a pivotal role in Trust Region Methods. However, the problem, and variants of
it, also arise in quite a few other applications. In this article, we present a
family of globally convergent iterative Riemannian optimization algorithms for
a variant of the Trust Region Subproblem that replaces the inequality
constraint with an equality constraint. Our approach uses either a trivial or a
non-trivial Riemannian geometry of the search-space, and requires only minimal
spectral information about the quadratic component of the objective function.
We further show how the theory of Riemannian optimization promotes a deeper
understanding of the Trust Region Subproblem and its difficulties, e.g. a deep
connection between the Trust Region Subproblem and the problem of finding
affine eigenvectors, and a new examination of the so-called hard case in light
of the condition number of the Riemannian Hessian operator at a global optimum.
Finally, we propose to incorporate preconditioning via a careful selection of a
variable Riemannian metric, and establish bounds on the asymptotic convergence
rate in terms of how well the preconditioner approximates the input matrix.
</p>
<a href="http://arxiv.org/abs/2010.07547" target="_blank">arXiv:2010.07547</a> [<a href="http://arxiv.org/pdf/2010.07547" target="_blank">pdf</a>]

<h2>Revisiting Projection-free Online Learning: the Strongly Convex Case. (arXiv:2010.07572v1 [cs.LG])</h2>
<h3>Dan Garber, Ben Kretzu</h3>
<p>Projection-free optimization algorithms, which are mostly based on the
classical Frank-Wolfe method, have gained significant interest in the machine
learning community in recent years due to their ability to handle convex
constraints that are popular in many applications, but for which computing
projections is often computationally impractical in high-dimensional settings,
and hence prohibit the use of most standard projection-based methods. In
particular, a significant research effort was put on projection-free methods
for online learning. In this paper we revisit the Online Frank-Wolfe (OFW)
method suggested by Hazan and Kale \cite{Hazan12} and fill a gap that has been
left unnoticed for several years: OFW achieves a faster rate of $O(T^{2/3})$ on
strongly convex functions (as opposed to the standard $O(T^{3/4})$ for convex
but not strongly convex functions), where $T$ is the sequence length. This is
somewhat surprising since it is known that for offline optimization, in
general, strong convexity does not lead to faster rates for Frank-Wolfe. We
also revisit the bandit setting under strong convexity and prove a similar
bound of $\tilde O(T^{2/3})$ (instead of $O(T^{3/4})$ without strong
convexity). Hence, in the current state-of-affairs, the best projection-free
upper-bounds for the full-information and bandit settings with strongly convex
and nonsmooth functions match, up to logarithmic factors, in $T$.
</p>
<a href="http://arxiv.org/abs/2010.07572" target="_blank">arXiv:2010.07572</a> [<a href="http://arxiv.org/pdf/2010.07572" target="_blank">pdf</a>]

<h2>Nearly all subspaces of a classical polar space arise from its universal embedding. (arXiv:2010.07640v1 [math.RT])</h2>
<h3>Ilaria Cardinali, Luca Giuzzi, Antonio Pasini</h3>
<p>Let $\Gamma$ be an embeddable non-degenerate polar space of finite rank $n
\geq 2$. Assuming that $\Gamma$ admits the universal embedding (which is true
for all embeddable polar spaces except grids of order at least $5$ and certain
generalized quadrangles defined over quaternion division rings), let
$\varepsilon:\Gamma\to\mathrm{PG}(V)$ be the universal embedding of $\Gamma$.
Let $\cal S$ be a subspace of $\Gamma$ and suppose that $\cal S$, regarded as a
polar space, has non-degenerate rank at least $2$. We shall prove that $\cal S$
is the $\varepsilon$-preimage of a projective subspace of $\mathrm{PG}(V)$.
</p>
<a href="http://arxiv.org/abs/2010.07640" target="_blank">arXiv:2010.07640</a> [<a href="http://arxiv.org/pdf/2010.07640" target="_blank">pdf</a>]

<h2>Joint Beamforming and Phase Shift Optimization for Multicell IRS-aided OFDMA-URLLC Systems. (arXiv:2010.07698v1 [cs.IT])</h2>
<h3>Walid R. Ghanem, Vahid Jamali, Robert Schober</h3>
<p>This paper investigates the resource allocation algorithm design for
intelligent reflecting surface (IRS) aided multiple-input single-output (MISO)
orthogonal frequency division multiple access (OFDMA) multicell networks, where
a set of base stations cooperate to serve a set of ultra-reliable low-latency
communication (URLLC) users. The IRS is deployed to enhance the communication
channel and increase reliability by creating a virtual line of sight for URLLC
users with unfavorable propagation conditions. This is the first study on
IRS-enhanced OFDMA-URLLC systems. The resource allocation algorithm design is
formulated as an optimization problem for the maximization of the weighted
system sum throughput while guaranteeing the quality of service of the URLLC
users. The optimization problem is non-convex and finding the globally optimal
solution entails a high computational complexity which is not desirable for
real-time applications. Therefore, a suboptimal iterative algorithm is proposed
which \textit{jointly} optimizes all optimization variables in each iteration
using a new iterative rank minimization approach. The algorithm is guaranteed
to converge to a locally optimal solution of the formulated optimization
problem. Our simulation results show that the proposed IRS design facilitates
URLLC and yields large performance gains compared to two baseline schemes.
</p>
<a href="http://arxiv.org/abs/2010.07698" target="_blank">arXiv:2010.07698</a> [<a href="http://arxiv.org/pdf/2010.07698" target="_blank">pdf</a>]

<h2>Noise Recycling. (arXiv:2010.07791v1 [cs.IT])</h2>
<h3>Alejandro Cohen, Amit Solomon, Ken R. Duffy, Muriel M&#xe9;dard</h3>
<p>We introduce Noise Recycling, a method that enhances decoding performance of
channels subject to correlated noise without joint decoding. The method can be
used with any combination of codes, code-rates and decoding techniques. In the
approach, a continuous realization of noise is estimated from a lead channel by
subtracting its decoded output from its received signal. This estimate is then
used to improve the accuracy of decoding of an orthogonal channel that is
experiencing correlated noise. In this design, channels aid each other only
through the provision of noise estimates post-decoding. In a Gauss-Markov model
of correlated noise, we constructive establish that noise recycling employing a
simple successive order enables higher rates than not recycling noise.
Simulations illustrate noise recycling can be employed with any code and
decoder, and that noise recycling shows Block Error Rate (BLER) benefits when
applying the same predetermined order as used to enhance the rate region.
Finally, for short codes we establish that an additional BLER improvement is
possible through noise recycling with racing, where the lead channel is not
pre-determined, but is chosen on the fly based on which decoder completes
first.
</p>
<a href="http://arxiv.org/abs/2010.07791" target="_blank">arXiv:2010.07791</a> [<a href="http://arxiv.org/pdf/2010.07791" target="_blank">pdf</a>]

<h2>Benders Cut Classification via Support Vector Machines for Solving Two-stage Stochastic Programs. (arXiv:1906.05994v4 [math.OC] UPDATED)</h2>
<h3>Huiwen Jia, Siqian Shen</h3>
<p>We consider Benders decomposition for solving two-stage stochastic programs
with complete recourse based on finite samples of the uncertain parameters. We
define the Benders cuts binding at the final optimal solution or the ones
significantly improving bounds over iterations as valuable cuts. We propose a
learning-enhanced Benders decomposition (LearnBD) algorithm, which adds a cut
classification step in each iteration to selectively generate cuts that are
more likely to be valuable cuts. The LearnBD algorithm includes two phases: (i)
sampling cuts and collecting information from training problems and (ii)
solving testing problems with a support vector machine (SVM) cut classifier. We
run the LearnBD algorithm on instances of capacitated facility location and
multi-commodity network design under uncertain demand. Our results show that
SVM cut classifier works effectively for identifying valuable cuts, and the
LearnBD algorithm reduces the total solving time of all instances for different
problems with various sizes and complexities.
</p>
<a href="http://arxiv.org/abs/1906.05994" target="_blank">arXiv:1906.05994</a> [<a href="http://arxiv.org/pdf/1906.05994" target="_blank">pdf</a>]

<h2>On a class of random walks with reinforced memory. (arXiv:1909.04633v2 [math.PR] UPDATED)</h2>
<h3>Erich Baur</h3>
<p>This paper deals with different models of random walks with a reinforced
memory of preferential attachment type. We consider extensions of the Elephant
Random Walk introduced by Sch\"utz and Trimper [2004] with a stronger
reinforcement mechanism, where, roughly speaking, a step from the past is
remembered proportional to some weight and then repeated with probability $p$.
With probability $1-p$, the random walk performs a step independent of the
past. The weight of the remembered step is increased by an additive factor
$b\geq 0$, making it likelier to repeat the step again in the future. A
combination of techniques from the theory of urns, branching processes and
$\alpha$-stable processes enables us to discuss the limit behavior of
reinforced versions of both the Elephant Random Walk and its $\alpha$-stable
counterpart, the so-called Shark Random Swim introduced by Businger [2018]. We
establish phase transitions, separating subcritical from supercritical regimes.
</p>
<a href="http://arxiv.org/abs/1909.04633" target="_blank">arXiv:1909.04633</a> [<a href="http://arxiv.org/pdf/1909.04633" target="_blank">pdf</a>]

<h2>Learning deep linear neural networks: Riemannian gradient flows and convergence to global minimizers. (arXiv:1910.05505v5 [math.OC] UPDATED)</h2>
<h3>Bubacarr Bah, Holger Rauhut, Ulrich Terstiege, Michael Westdickenberg</h3>
<p>We study the convergence of gradient flows related to learning deep linear
neural networks (where the activation function is the identity map) from data.
In this case, the composition of the network layers amounts to simply
multiplying the weight matrices of all layers together, resulting in an
overparameterized problem. The gradient flow with respect to these factors can
be re-interpreted as a Riemannian gradient flow on the manifold of rank-$r$
matrices endowed with a suitable Riemannian metric. We show that the flow
always converges to a critical point of the underlying functional. Moreover, we
establish that, for almost all initializations, the flow converges to a global
minimum on the manifold of rank $k$ matrices for some $k\leq r$.
</p>
<a href="http://arxiv.org/abs/1910.05505" target="_blank">arXiv:1910.05505</a> [<a href="http://arxiv.org/pdf/1910.05505" target="_blank">pdf</a>]

<h2>Accelerating Generalized Benders Decomposition for Wireless Resource Allocation. (arXiv:2003.01294v2 [cs.IT] UPDATED)</h2>
<h3>Mengyuan Lee, Ning Ma, Guanding Yu, Huaiyu Dai</h3>
<p>Generalized Benders decomposition (GBD) is a globally optimal algorithm for
mixed integer nonlinear programming (MINLP) problems, which are NP-hard and can
be widely found in the area of wireless resource allocation. The main idea of
GBD is decomposing an MINLP problem into a primal problem and a master problem,
which are iteratively solved until their solutions converge. However, a direct
implementation of GBD is time- and memory-consuming. The main bottleneck is the
high complexity of the master problem, which increases over the iterations.
Therefore, we propose to leverage machine learning (ML) techniques to
accelerate GBD aiming at decreasing the complexity of the master problem.
Specifically, we utilize two different ML techniques, classification and
regression, to deal with this acceleration task. In this way, a cut classifier
and a cut regressor are learned, respectively, to distinguish between useful
and useless cuts. Only useful cuts are added to the master problem and thus the
complexity of the master problem is reduced. By using a resource allocation
problem in device-to-device communication networks as an example, we validate
that the proposed method can reduce the computational complexity of GBD without
loss of optimality and has strong generalization ability. The proposed method
is applicable for solving various MINLP problems in wireless networks since the
designs are invariant for different problems.
</p>
<a href="http://arxiv.org/abs/2003.01294" target="_blank">arXiv:2003.01294</a> [<a href="http://arxiv.org/pdf/2003.01294" target="_blank">pdf</a>]

<h2>Learning discrete distributions with infinite support. (arXiv:2004.12680v3 [math.ST] UPDATED)</h2>
<h3>Doron Cohen, Aryeh Kontorovich, Geoffrey Wolfer</h3>
<p>We present a novel approach to estimating discrete distributions with
(potentially) infinite support in the total variation metric. In a departure
from the established paradigm, we make no structural assumptions whatsoever on
the sampling distribution. In such a setting, distribution-free risk bounds are
impossible, and the best one could hope for is a fully empirical data-dependent
bound. We derive precisely such bounds, and demonstrate that these are, in a
well-defined sense, the best possible. Our main discovery is that the half-norm
of the empirical distribution provides tight upper and lower estimates on the
empirical risk. Furthermore, this quantity decays at a nearly optimal rate as a
function of the true distribution. The optimality follows from a minimax
result, of possible independent interest. Additional structural results are
provided, including an exact Rademacher complexity calculation and apparently a
first connection between the total variation risk and the missing mass.
</p>
<a href="http://arxiv.org/abs/2004.12680" target="_blank">arXiv:2004.12680</a> [<a href="http://arxiv.org/pdf/2004.12680" target="_blank">pdf</a>]

<h2>Infinite-dimensional gradient-based descent for alpha-divergence minimisation. (arXiv:2005.10618v2 [math.ST] UPDATED)</h2>
<h3>Kam&#xe9;lia Daudel, Randal Douc, Fran&#xe7;ois Portier</h3>
<p>This paper introduces the $(\alpha, \Gamma)$-descent, an iterative algorithm
which operates on measures and performs $\alpha$-divergence minimisation in a
Bayesian framework. This gradient-based procedure extends the commonly-used
variational approximation by adding a prior on the variational parameters in
the form of a measure. We prove that for a rich family of functions $\Gamma$,
this algorithm leads at each step to a systematic decrease in the
$\alpha$-divergence and derive convergence results. Our framework recovers the
Entropic Mirror Descent algorithm and provides an alternative algorithm that we
call the Power Descent. Moreover, in its stochastic formulation, the $(\alpha,
\Gamma)$-descent allows to optimise the mixture weights of any given mixture
model without any information on the underlying distribution of the variational
parameters. This renders our method compatible with many choices of parameters
updates and applicable to a wide range of Machine Learning tasks. We
demonstrate empirically on both toy and real-world examples the benefit of
using the Power descent and going beyond the Entropic Mirror Descent framework,
which fails as the dimension grows.
</p>
<a href="http://arxiv.org/abs/2005.10618" target="_blank">arXiv:2005.10618</a> [<a href="http://arxiv.org/pdf/2005.10618" target="_blank">pdf</a>]

<h2>A Topological Framework for Deep Learning. (arXiv:2008.13697v8 [cs.LG] UPDATED)</h2>
<h3>Mustafa Hajij, Kyle Istvan</h3>
<p>We utilize classical facts from topology to show that the classification
problem in machine learning is always solvable under very mild conditions.
Furthermore, we show that a softmax classification network acts on an input
topological space by a finite sequence of topological moves to achieve the
classification task. Moreover, given a training dataset, we show how
topological formalism can be used to suggest the appropriate architectural
choices for neural networks designed to be trained as classifiers on the data.
Finally, we show how the architecture of a neural network cannot be chosen
independently from the shape of the underlying data. To demonstrate these
results, we provide example datasets and show how they are acted upon by neural
nets from this topological perspective.
</p>
<a href="http://arxiv.org/abs/2008.13697" target="_blank">arXiv:2008.13697</a> [<a href="http://arxiv.org/pdf/2008.13697" target="_blank">pdf</a>]

<h2>Meta-learning for Multi-variable Non-convex Optimization Problems: Iterating Non-optimums Makes Optimum Possible. (arXiv:2009.04899v2 [cs.LG] UPDATED)</h2>
<h3>Jingyuan Xia, Shengxi Li, Jun-Jie Huang, Imad Jaimoukha, Xinwang Liu</h3>
<p>In this paper, we aim to address the problem of solving a non-convex
optimization problem over an intersection of multiple variable sets. This kind
of problems is typically solved by using an alternating minimization (AM)
strategy which splits the overall problem into a set of sub-problems
corresponding to each variable, and then iteratively performs minimization over
each sub-problem using a fixed updating rule. However, due to the intrinsic
non-convexity of the overall problem, the optimization can usually be trapped
into bad local minimum even when each sub-problem can be globally optimized at
each iteration. To tackle this problem, we propose a meta-learning based Global
Scope Optimization (GSO) method. It adaptively generates optimizers for
sub-problems via meta-learners and constantly updates these meta-learners with
respect to the global loss information of the overall problem. Therefore, the
sub-problems are optimized with the objective of minimizing the global loss
specifically. We evaluate the proposed model on a number of simulations,
including solving bi-linear inverse problems: matrix completion, and non-linear
problems: Gaussian mixture models. The experimental results show that our
proposed approach outperforms AM-based methods in standard settings, and is
able to achieve effective optimization in some challenging cases while other
methods would typically fail.
</p>
<a href="http://arxiv.org/abs/2009.04899" target="_blank">arXiv:2009.04899</a> [<a href="http://arxiv.org/pdf/2009.04899" target="_blank">pdf</a>]

<h2>Distributional Generalization: A New Kind of Generalization. (arXiv:2009.08092v2 [cs.LG] UPDATED)</h2>
<h3>Preetum Nakkiran, Yamini Bansal</h3>
<p>We introduce a new notion of generalization -- Distributional Generalization
-- which roughly states that outputs of a classifier at train and test time are
close *as distributions*, as opposed to close in just their average error. For
example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then
a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as
cats on the *test set* as well, while leaving other classes unaffected. This
behavior is not captured by classical generalization, which would only consider
the average error and not the distribution of errors over the input domain. Our
formal conjectures, which are much more general than this example, characterize
the form of distributional generalization that can be expected in terms of
problem parameters: model architecture, training procedure, number of samples,
and data distribution. We give empirical evidence for these conjectures across
a variety of domains in machine learning, including neural networks, kernel
machines, and decision trees. Our results thus advance our empirical
understanding of interpolating classifiers.
</p>
<a href="http://arxiv.org/abs/2009.08092" target="_blank">arXiv:2009.08092</a> [<a href="http://arxiv.org/pdf/2009.08092" target="_blank">pdf</a>]

<h2>Reinforcement Learning Based Temporal Logic Control with Maximum Probabilistic Satisfaction. (arXiv:2010.06797v2 [cs.FL] UPDATED)</h2>
<h3>Mingyu Cai, Shaoping Xiao, Baoluo Li, Zhiliang Li, Zhen Kan</h3>
<p>This paper presents a model-free reinforcement learning (RL) algorithm to
synthesize a control policy that maximizes the satisfaction probability of
linear temporal logic (LTL) specifications. Due to the consideration of
environment and motion uncertainties, we model the robot motion as a
probabilistic labeled Markov decision process with unknown transition
probabilities and unknown probabilistic label functions. The LTL task
specification is converted to a limit deterministic generalized B\"uchi
automaton (LDGBA) with several accepting sets to maintain dense rewards during
learning. The novelty of applying LDGBA is to construct an embedded LDGBA
(E-LDGBA) by designing a synchronous tracking-frontier function, which enables
the record of non-visited accepting sets without increasing dimensional and
computational complexity. With appropriate dependent reward and discount
functions, rigorous analysis shows that any method that optimizes the expected
discount return of the RL-based approach is guaranteed to find the optimal
policy that maximizes the satisfaction probability of the LTL specifications. A
model-free RL-based motion planning strategy is developed to generate the
optimal policy in this paper. The effectiveness of the RL-based control
synthesis is demonstrated via simulation and experimental results.
</p>
<a href="http://arxiv.org/abs/2010.06797" target="_blank">arXiv:2010.06797</a> [<a href="http://arxiv.org/pdf/2010.06797" target="_blank">pdf</a>]

<h2>XPDNet for MRI Reconstruction: an Application to the fastMRI 2020 Brain Challenge. (arXiv:2010.07290v1 [eess.IV])</h2>
<h3>Zaccharie Ramzi, Philippe Ciuciu, Jean-Luc Starck</h3>
<p>We present a modular cross-domain neural network the XPDNet and its
application to the MRI reconstruction task. This approach consists in unrolling
the PDHG algorithm as well as learning the acceleration scheme between steps.
We also adopt state-of-the-art techniques specific to Deep Learning for MRI
reconstruction. At the time of writing, this approach is the best performer in
PSNR on the fastMRI leaderboards for both knee and brain at acceleration factor
4.
</p>
<a href="http://arxiv.org/abs/2010.07290" target="_blank">arXiv:2010.07290</a> [<a href="http://arxiv.org/pdf/2010.07290" target="_blank">pdf</a>]

<h2>Collective defense of honeybee colonies: experimental results and theoretical modeling. (arXiv:2010.07326v1 [q-bio.PE])</h2>
<h3>Andrea L&#xf3;pez-Incera, Morgane Nouvian, Katja Ried, Thomas M&#xfc;ller, Hans J. Briegel</h3>
<p>Social insect colonies routinely face large vertebrate predators, against
which they need to mount a collective defense. To do so, honeybees use an alarm
pheromone that recruits nearby bees into mass stinging of the perceived threat.
This alarm pheromone is carried directly on the stinger, hence its
concentration builds up during the course of the attack. Here, we investigate
how individual bees react to different alarm pheromone concentrations, and how
this evolved response-pattern leads to better coordination at the group level.
We first present an individual dose-response curve to the alarm pheromone,
obtained experimentally. Second, we apply Projective Simulation to model each
bee as an artificial learning agent that relies on the pheromone concentration
to decide whether to sting or not. If the emergent collective performance
benefits the colony, the individual reactions that led to it are enhanced via
reinforcement learning, thus emulating natural selection. Predators are modeled
in a realistic way so that the effect of factors such as their resistance,
their killing rate or their frequency of attacks can be studied. We are able to
reproduce the experimentally measured response-pattern of real bees, and to
identify the main selection pressures that shaped it. Finally, we apply the
model to a case study: by tuning the parameters to represent the environmental
conditions of European or African bees, we can predict the difference in
aggressiveness observed between these two subspecies.
</p>
<a href="http://arxiv.org/abs/2010.07326" target="_blank">arXiv:2010.07326</a> [<a href="http://arxiv.org/pdf/2010.07326" target="_blank">pdf</a>]

<h2>Adaptive Deep Forest for Online Learning from Drifting Data Streams. (arXiv:2010.07340v1 [cs.LG])</h2>
<h3>&#x141;ukasz Korycki, Bartosz Krawczyk</h3>
<p>Learning from data streams is among the most vital fields of contemporary
data mining. The online analysis of information coming from those potentially
unbounded data sources allows for designing reactive up-to-date models capable
of adjusting themselves to continuous flows of data. While a plethora of
shallow methods have been proposed for simpler low-dimensional streaming
problems, almost none of them addressed the issue of learning from complex
contextual data, such as images or texts. The former is represented mainly by
adaptive decision trees that have been proven to be very efficient in streaming
scenarios. The latter has been predominantly addressed by offline deep
learning. In this work, we attempt to bridge the gap between these two worlds
and propose Adaptive Deep Forest (ADF) - a natural combination of the
successful tree-based streaming classifiers with deep forest, which represents
an interesting alternative idea for learning from contextual data. The
conducted experiments show that the deep forest approach can be effectively
transformed into an online algorithm, forming a model that outperforms all
state-of-the-art shallow adaptive classifiers, especially for high-dimensional
complex streams.
</p>
<a href="http://arxiv.org/abs/2010.07340" target="_blank">arXiv:2010.07340</a> [<a href="http://arxiv.org/pdf/2010.07340" target="_blank">pdf</a>]

<h2>Temperature check: theory and practice for training models with softmax-cross-entropy losses. (arXiv:2010.07344v1 [cs.LG])</h2>
<h3>Atish Agarwala, Jeffrey Pennington, Yann Dauphin, Sam Schoenholz</h3>
<p>The softmax function combined with a cross-entropy loss is a principled
approach to modeling probability distributions that has become ubiquitous in
deep learning. The softmax function is defined by a lone hyperparameter, the
temperature, that is commonly set to one or regarded as a way to tune model
confidence after training; however, less is known about how the temperature
impacts training dynamics or generalization performance. In this work we
develop a theory of early learning for models trained with
softmax-cross-entropy loss and show that the learning dynamics depend crucially
on the inverse-temperature $\beta$ as well as the magnitude of the logits at
initialization, $||\beta{\bf z}||_{2}$. We follow up these analytic results
with a large-scale empirical study of a variety of model architectures trained
on CIFAR10, ImageNet, and IMDB sentiment analysis. We find that generalization
performance depends strongly on the temperature, but only weakly on the initial
logit magnitude. We provide evidence that the dependence of generalization on
$\beta$ is not due to changes in model confidence, but is a dynamical
phenomenon. It follows that the addition of $\beta$ as a tunable hyperparameter
is key to maximizing model performance. Although we find the optimal $\beta$ to
be sensitive to the architecture, our results suggest that tuning $\beta$ over
the range $10^{-2}$ to $10^1$ improves performance over all architectures
studied. We find that smaller $\beta$ may lead to better peak performance at
the cost of learning stability.
</p>
<a href="http://arxiv.org/abs/2010.07344" target="_blank">arXiv:2010.07344</a> [<a href="http://arxiv.org/pdf/2010.07344" target="_blank">pdf</a>]

<h2>Online Learning with Vector Costs and Bandits with Knapsacks. (arXiv:2010.07346v1 [cs.LG])</h2>
<h3>Thomas Kesselheim, Sahil Singla</h3>
<p>We introduce online learning with vector costs (\OLVCp) where in each time
step $t \in \{1,\ldots, T\}$, we need to play an action $i \in \{1,\ldots,n\}$
that incurs an unknown vector cost in $[0,1]^{d}$. The goal of the online
algorithm is to minimize the $\ell_p$ norm of the sum of its cost vectors. This
captures the classical online learning setting for $d=1$, and is interesting
for general $d$ because of applications like online scheduling where we want to
balance the load between different machines (dimensions).

We study \OLVCp in both stochastic and adversarial arrival settings, and give
a general procedure to reduce the problem from $d$ dimensions to a single
dimension. This allows us to use classical online learning algorithms in both
full and bandit feedback models to obtain (near) optimal results. In
particular, we obtain a single algorithm (up to the choice of learning rate)
that gives sublinear regret for stochastic arrivals and a tight $O(\min\{p,
\log d\})$ competitive ratio for adversarial arrivals.

The \OLVCp problem also occurs as a natural subproblem when trying to solve
the popular Bandits with Knapsacks (\BwK) problem. This connection allows us to
use our \OLVCp techniques to obtain (near) optimal results for \BwK in both
stochastic and adversarial settings. In particular, we obtain a tight $O(\log d
\cdot \log T)$ competitive ratio algorithm for adversarial \BwK, which improves
over the $O(d \cdot \log T)$ competitive ratio algorithm of Immorlica et al.
[FOCS'19].
</p>
<a href="http://arxiv.org/abs/2010.07346" target="_blank">arXiv:2010.07346</a> [<a href="http://arxiv.org/pdf/2010.07346" target="_blank">pdf</a>]

<h2>Matching-space Stereo Networks for Cross-domain Generalization. (arXiv:2010.07347v1 [cs.CV])</h2>
<h3>Changjiang Cai, Matteo Poggi, Stefano Mattoccia, Philippos Mordohai</h3>
<p>End-to-end deep networks represent the state of the art for stereo matching.
While excelling on images framing environments similar to the training set,
major drops in accuracy occur in unseen domains (e.g., when moving from
synthetic to real scenes). In this paper we introduce a novel family of
architectures, namely Matching-Space Networks (MS-Nets), with improved
generalization properties. By replacing learning-based feature extraction from
image RGB values with matching functions and confidence measures from
conventional wisdom, we move the learning process from the color space to the
Matching Space, avoiding over-specialization to domain specific features.
Extensive experimental results on four real datasets highlight that our
proposal leads to superior generalization to unseen environments over
conventional deep architectures, keeping accuracy on the source domain almost
unaltered. Our code is available at https://github.com/ccj5351/MS-Nets.
</p>
<a href="http://arxiv.org/abs/2010.07347" target="_blank">arXiv:2010.07347</a> [<a href="http://arxiv.org/pdf/2010.07347" target="_blank">pdf</a>]

<h2>Do End-to-end Stereo Algorithms Under-utilize Information?. (arXiv:2010.07350v1 [cs.CV])</h2>
<h3>Changjiang Cai, Philippos Mordohai</h3>
<p>Deep networks for stereo matching typically leverage 2D or 3D convolutional
encoder-decoder architectures to aggregate cost and regularize the cost volume
for accurate disparity estimation. Due to content-insensitive convolutions and
down-sampling and up-sampling operations, these cost aggregation mechanisms do
not take full advantage of the information available in the images. Disparity
maps suffer from over-smoothing near occlusion boundaries, and erroneous
predictions in thin structures. In this paper, we show how deep adaptive
filtering and differentiable semi-global aggregation can be integrated in
existing 2D and 3D convolutional networks for end-to-end stereo matching,
leading to improved accuracy. The improvements are due to utilizing RGB
information from the images as a signal to dynamically guide the matching
process, in addition to being the signal we attempt to match across the images.
We show extensive experimental results on the KITTI 2015 and Virtual KITTI 2
datasets comparing four stereo networks (DispNetC, GCNet, PSMNet and GANet)
after integrating four adaptive filters (segmentation-aware bilateral
filtering, dynamic filtering networks, pixel adaptive convolution and
semi-global aggregation) into their architectures. Our code is available at
https://github.com/ccj5351/DAFStereoNets.
</p>
<a href="http://arxiv.org/abs/2010.07350" target="_blank">arXiv:2010.07350</a> [<a href="http://arxiv.org/pdf/2010.07350" target="_blank">pdf</a>]

<h2>Exploring the Uncertainty Properties of Neural Networks' Implicit Priors in the Infinite-Width Limit. (arXiv:2010.07355v1 [stat.ML])</h2>
<h3>Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, Jasper Snoek</h3>
<p>Modern deep learning models have achieved great success in predictive
accuracy for many data modalities. However, their application to many
real-world tasks is restricted by poor uncertainty estimates, such as
overconfidence on out-of-distribution (OOD) data and ungraceful failing under
distributional shift. Previous benchmarks have found that ensembles of neural
networks (NNs) are typically the best calibrated models on OOD data. Inspired
by this, we leverage recent theoretical advances that characterize the
function-space prior of an ensemble of infinitely-wide NNs as a Gaussian
process, termed the neural network Gaussian process (NNGP). We use the NNGP
with a softmax link function to build a probabilistic model for multi-class
classification and marginalize over the latent Gaussian outputs to sample from
the posterior. This gives us a better understanding of the implicit prior NNs
place on function space and allows a direct comparison of the calibration of
the NNGP and its finite-width analogue. We also examine the calibration of
previous approaches to classification with the NNGP, which treat classification
problems as regression to the one-hot labels. In this case the Bayesian
posterior is exact, and we compare several heuristics to generate a categorical
distribution over classes. We find these methods are well calibrated under
distributional shift. Finally, we consider an infinite-width final layer in
conjunction with a pre-trained embedding. This replicates the important
practical use case of transfer learning and allows scaling to significantly
larger datasets. As well as achieving competitive predictive accuracy, this
approach is better calibrated than its finite width analogue.
</p>
<a href="http://arxiv.org/abs/2010.07355" target="_blank">arXiv:2010.07355</a> [<a href="http://arxiv.org/pdf/2010.07355" target="_blank">pdf</a>]

<h2>Effects of the Nonlinearity in Activation Functions on the Performance of Deep Learning Models. (arXiv:2010.07359v1 [cs.LG])</h2>
<h3>Nalinda Kulathunga, Nishath Rajiv Ranasinghe, Daniel Vrinceanu, Zackary Kinsman, Lei Huang, Yunjiao Wang</h3>
<p>The nonlinearity of activation functions used in deep learning models are
crucial for the success of predictive models. There are several commonly used
simple nonlinear functions, including Rectified Linear Unit (ReLU) and
Leaky-ReLU (L-ReLU). In practice, these functions remarkably enhance the model
accuracy. However, there is limited insight into the functionality of these
nonlinear activation functions in terms of why certain models perform better
than others. Here, we investigate the model performance when using ReLU or
L-ReLU as activation functions in different model architectures and data
domains. Interestingly, we found that the application of L-ReLU is mostly
effective when the number of trainable parameters in a model is relatively
small. Furthermore, we found that the image classification models seem to
perform well with L-ReLU in fully connected layers, especially when pre-trained
models such as the VGG-16 are used for the transfer learning.
</p>
<a href="http://arxiv.org/abs/2010.07359" target="_blank">arXiv:2010.07359</a> [<a href="http://arxiv.org/pdf/2010.07359" target="_blank">pdf</a>]

<h2>Deep Learning in Ultrasound Elastography Imaging. (arXiv:2010.07360v1 [eess.IV])</h2>
<h3>Hongliang Li, Manish Bhatt, Zhen Qu, Shiming Zhang, Martin C. Hartel, Ali Khademhosseini, Guy Cloutier</h3>
<p>It is known that changes in the mechanical properties of tissues are
associated with the onset and progression of certain diseases. Ultrasound
elastography is a technique to characterize tissue stiffness using ultrasound
imaging either by measuring tissue strain using quasi-static elastography or
natural organ pulsation elastography, or by tracing a propagated shear wave
induced by a source or a natural vibration using dynamic elastography. In
recent years, deep learning has begun to emerge in ultrasound elastography
research. In this review, several common deep learning frameworks in the
computer vision community, such as multilayer perceptron, convolutional neural
network, and recurrent neural network are described. Then, recent advances in
ultrasound elastography using such deep learning techniques are revisited in
terms of algorithm development and clinical diagnosis. Finally, the current
challenges and future developments of deep learning in ultrasound elastography
are prospected.
</p>
<a href="http://arxiv.org/abs/2010.07360" target="_blank">arXiv:2010.07360</a> [<a href="http://arxiv.org/pdf/2010.07360" target="_blank">pdf</a>]

<h2>Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition. (arXiv:2010.07367v1 [cs.CV])</h2>
<h3>Shijie Li, Jinhui Yi, Yazan Abu Farha, Juergen Gall</h3>
<p>With the advances in capturing 2D or 3D skeleton data, skeleton-based action
recognition has received an increasing interest over the last years. As
skeleton data is commonly represented by graphs, graph convolutional networks
have been proposed for this task. While current graph convolutional networks
accurately recognize actions, they are too expensive for robotics applications
where limited computational resources are available. In this paper, we
therefore propose a highly efficient graph convolutional network that addresses
the limitations of previous works. This is achieved by a parallel structure
that gradually fuses motion and spatial information and by reducing the
temporal resolution as early as possible. Furthermore, we explicitly address
the issue that human poses can contain errors. To this end, the network first
refines the poses before they are further processed to recognize the action. We
therefore call the network Pose Refinement Graph Convolutional Network.
Compared to other graph convolutional networks, our network requires 86\%-93\%
less parameters and reduces the floating point operations by 89%-96% while
achieving a comparable accuracy. It therefore provides a much better trade-off
between accuracy, memory footprint and processing time, which makes it suitable
for robotics applications.
</p>
<a href="http://arxiv.org/abs/2010.07367" target="_blank">arXiv:2010.07367</a> [<a href="http://arxiv.org/pdf/2010.07367" target="_blank">pdf</a>]

<h2>Graph Deep Factors for Forecasting. (arXiv:2010.07373v1 [cs.LG])</h2>
<h3>Hongjie Chen, Ryan A. Rossi, Kanak Mahadik, Sungchul Kim, Hoda Eldardiry</h3>
<p>Deep probabilistic forecasting techniques have recently been proposed for
modeling large collections of time-series. However, these techniques explicitly
assume either complete independence (local model) or complete dependence
(global model) between time-series in the collection. This corresponds to the
two extreme cases where every time-series is disconnected from every other
time-series in the collection or likewise, that every time-series is related to
every other time-series resulting in a completely connected graph. In this
work, we propose a deep hybrid probabilistic graph-based forecasting framework
called Graph Deep Factors (GraphDF) that goes beyond these two extremes by
allowing nodes and their time-series to be connected to others in an arbitrary
fashion. GraphDF is a hybrid forecasting framework that consists of a
relational global and relational local model. In particular, we propose a
relational global model that learns complex non-linear time-series patterns
globally using the structure of the graph to improve both forecasting accuracy
and computational efficiency. Similarly, instead of modeling every time-series
independently, we learn a relational local model that not only considers its
individual time-series but also the time-series of nodes that are connected in
the graph. The experiments demonstrate the effectiveness of the proposed deep
hybrid graph-based forecasting model compared to the state-of-the-art methods
in terms of its forecasting accuracy, runtime, and scalability. Our case study
reveals that GraphDF can successfully generate cloud usage forecasts and
opportunistically schedule workloads to increase cloud cluster utilization by
47.5% on average.
</p>
<a href="http://arxiv.org/abs/2010.07373" target="_blank">arXiv:2010.07373</a> [<a href="http://arxiv.org/pdf/2010.07373" target="_blank">pdf</a>]

<h2>Decision trees as partitioning machines to characterize their generalization properties. (arXiv:2010.07374v1 [cs.LG])</h2>
<h3>Jean-Samuel Leboeuf, Fr&#xe9;d&#xe9;ric LeBlanc, Mario Marchand</h3>
<p>Decision trees are popular machine learning models that are simple to build
and easy to interpret. Even though algorithms to learn decision trees date back
to almost 50 years, key properties affecting their generalization error are
still weakly bounded. Hence, we revisit binary decision trees on real-valued
features from the perspective of partitions of the data. We introduce the
notion of partitioning function, and we relate it to the growth function and to
the VC dimension. Using this new concept, we are able to find the exact VC
dimension of decision stumps, which is given by the largest integer $d$ such
that $2\ell \ge \binom{d}{\left\lfloor\frac{d}{2}\right\rfloor}$, where $\ell$
is the number of real-valued features. We provide a recursive expression to
bound the partitioning functions, resulting in a upper bound on the growth
function of any decision tree structure. This allows us to show that the VC
dimension of a binary tree structure with $N$ internal nodes is of order $N
\log(N\ell)$. Finally, we elaborate a pruning algorithm based on these results
that performs better than the CART algorithm on a number of datasets, with the
advantage that no cross-validation is required.
</p>
<a href="http://arxiv.org/abs/2010.07374" target="_blank">arXiv:2010.07374</a> [<a href="http://arxiv.org/pdf/2010.07374" target="_blank">pdf</a>]

<h2>Human-interpretable model explainability on high-dimensional data. (arXiv:2010.07384v1 [cs.LG])</h2>
<h3>Damien de Mijolla, Christopher Frye, Markus Kunesch, John Mansir, Ilya Feige</h3>
<p>The importance of explainability in machine learning continues to grow, as
both neural-network architectures and the data they model become increasingly
complex. Unique challenges arise when a model's input features become high
dimensional: on one hand, principled model-agnostic approaches to
explainability become too computationally expensive; on the other, more
efficient explainability algorithms lack natural interpretations for general
users. In this work, we introduce a framework for human-interpretable
explainability on high-dimensional data, consisting of two modules. First, we
apply a semantically meaningful latent representation, both to reduce the raw
dimensionality of the data, and to ensure its human interpretability. These
latent features can be learnt, e.g. explicitly as disentangled representations
or implicitly through image-to-image translation, or they can be based on any
computable quantities the user chooses. Second, we adapt the Shapley paradigm
for model-agnostic explainability to operate on these latent features. This
leads to interpretable model explanations that are both theoretically
controlled and computationally tractable. We benchmark our approach on
synthetic data and demonstrate its effectiveness on several
image-classification tasks.
</p>
<a href="http://arxiv.org/abs/2010.07384" target="_blank">arXiv:2010.07384</a> [<a href="http://arxiv.org/pdf/2010.07384" target="_blank">pdf</a>]

<h2>Interpretable Machine Learning with an Ensemble of Gradient Boosting Machines. (arXiv:2010.07388v1 [cs.LG])</h2>
<h3>Andrei V. Konstantinov, Lev V. Utkin</h3>
<p>A method for the local and global interpretation of a black-box model on the
basis of the well-known generalized additive models is proposed. It can be
viewed as an extension or a modification of the algorithm using the neural
additive model. The method is based on using an ensemble of gradient boosting
machines (GBMs) such that each GBM is learned on a single feature and produces
a shape function of the feature. The ensemble is composed as a weighted sum of
separate GBMs resulting a weighted sum of shape functions which form the
generalized additive model. GBMs are built in parallel using randomized
decision trees of depth 1, which provide a very simple architecture. Weights of
GBMs as well as features are computed in each iteration of boosting by using
the Lasso method and then updated by means of a specific smoothing procedure.
In contrast to the neural additive model, the method provides weights of
features in the explicit form, and it is simply trained. A lot of numerical
experiments with an algorithm implementing the proposed method on synthetic and
real datasets demonstrate its efficiency and properties for local and global
interpretation.
</p>
<a href="http://arxiv.org/abs/2010.07388" target="_blank">arXiv:2010.07388</a> [<a href="http://arxiv.org/pdf/2010.07388" target="_blank">pdf</a>]

<h2>Explainability for fair machine learning. (arXiv:2010.07389v1 [cs.LG])</h2>
<h3>Tom Begley, Tobias Schwedes, Christopher Frye, Ilya Feige</h3>
<p>As the decisions made or influenced by machine learning models increasingly
impact our lives, it is crucial to detect, understand, and mitigate unfairness.
But even simply determining what "unfairness" should mean in a given context is
non-trivial: there are many competing definitions, and choosing between them
often requires a deep understanding of the underlying task. It is thus tempting
to use model explainability to gain insights into model fairness, however
existing explainability tools do not reliably indicate whether a model is
indeed fair. In this work we present a new approach to explaining fairness in
machine learning, based on the Shapley value paradigm. Our fairness
explanations attribute a model's overall unfairness to individual input
features, even in cases where the model does not operate on sensitive
attributes directly. Moreover, motivated by the linearity of Shapley
explainability, we propose a meta algorithm for applying existing training-time
fairness interventions, wherein one trains a perturbation to the original
model, rather than a new model entirely. By explaining the original model, the
perturbation, and the fair-corrected model, we gain insight into the
accuracy-fairness trade-off that is being made by the intervention. We further
show that this meta algorithm enjoys both flexibility and stability benefits
with no loss in performance.
</p>
<a href="http://arxiv.org/abs/2010.07389" target="_blank">arXiv:2010.07389</a> [<a href="http://arxiv.org/pdf/2010.07389" target="_blank">pdf</a>]

<h2>FAR: A General Framework for Attributional Robustness. (arXiv:2010.07393v1 [cs.LG])</h2>
<h3>Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard</h3>
<p>Attribution maps have gained popularity as tools for explaining neural
networks predictions. By assigning an importance value to each input dimension
that represents their influence towards the outcome, they give an intuitive
explanation of the decision process. However, recent work has discovered
vulnerability of these maps to imperceptible, carefully crafted changes in the
input that lead to significantly different attributions, rendering them
meaningless. By borrowing notions of traditional adversarial training - a
method to achieve robust predictions - we propose a novel framework for
attributional robustness (FAR) to mitigate this vulnerability. Central
assumption is that similar inputs should yield similar attribution maps, while
keeping the prediction of the network constant. Specifically, we define a new
generic regularization term and training objective that minimizes the maximal
dissimilarity of attribution maps in a local neighbourhood of the input. We
then show how current state-of-the-art methods can be recovered through
principled instantiations of these objectives. Moreover, we propose two new
training methods, AAT and AdvAAT, derived from the framework, that directly
optimize for robust attributions and predictions. We showcase the effectivity
of our training methods by comparing them to current state-of-the-art
attributional robustness approaches on widely used vision datasets. Experiments
show that they perform better or comparably to current methods in terms of
attributional robustness, while being applicable to any attribution method and
input data domain. We finally show that our methods mitigate undesired
dependencies of attributional robustness and some training and estimation
parameters, which seem to critically affect other methods.
</p>
<a href="http://arxiv.org/abs/2010.07393" target="_blank">arXiv:2010.07393</a> [<a href="http://arxiv.org/pdf/2010.07393" target="_blank">pdf</a>]

<h2>A Deep Learning Framework for Predicting Digital Asset Price Movement from Trade-by-trade Data. (arXiv:2010.07404v1 [q-fin.ST])</h2>
<h3>Qi Zhao</h3>
<p>This paper presents a deep learning framework based on Long Short-term Memory
Network(LSTM) that predicts price movement of cryptocurrencies from
trade-by-trade data. The main focus of this study is on predicting short-term
price changes in a fixed time horizon from a looking back period. By carefully
designing features and detailed searching for best hyper-parameters, the model
is trained to achieve high performance on nearly a year of trade-by-trade data.
The optimal model delivers stable high performance(over 60% accuracy) on
out-of-sample test periods. In a realistic trading simulation setting, the
prediction made by the model could be easily monetized. Moreover, this study
shows that the LSTM model could extract universal features from trade-by-trade
data, as the learned parameters well maintain their high performance on other
cryptocurrency instruments that were not included in training data. This study
exceeds existing researches in term of the scale and precision of data used, as
well as the high prediction accuracy achieved.
</p>
<a href="http://arxiv.org/abs/2010.07404" target="_blank">arXiv:2010.07404</a> [<a href="http://arxiv.org/pdf/2010.07404" target="_blank">pdf</a>]

<h2>Adaptive tracking control for task-based robot trajectory planning. (arXiv:2010.07406v1 [cs.RO])</h2>
<h3>Luis Trucios, Mahdi Tavakoli, Kim Adams</h3>
<p>This paper presents a -- Learning from Demonstration -- method to perform
robot movement trajectories that can be defined as you go. This way
unstructured tasks can be performed, without the need to know exactly all the
tasks and start and end positions beforehand. The long-term goal is for
children with disabilities to be able to control a robot to manipulate toys in
a play environment, and for a helper to demonstrate the desired trajectories as
the play tasks change. A relatively inexpensive 3-DOF haptic device made by
Novint is used to perform tasks where trajectories of the end-effector are
demonstrated and reproduced. Under the condition where the end-effector carries
different loads, conventional control systems possess the potential issue that
they cannot compensate for the load variation effect. Adaptive tracking control
can handle the above issue. Using the Lyapunov stability theory, a set of
update laws are derived to give closed-loop stability with proper tracking
performance.
</p>
<a href="http://arxiv.org/abs/2010.07406" target="_blank">arXiv:2010.07406</a> [<a href="http://arxiv.org/pdf/2010.07406" target="_blank">pdf</a>]

<h2>Harnessing Uncertainty in Domain Adaptation for MRI Prostate Lesion Segmentation. (arXiv:2010.07411v1 [eess.IV])</h2>
<h3>Eleni Chiou, Francesco Giganti, Shonit Punwani, Iasonas Kokkinos, Eleftheria Panagiotaki</h3>
<p>The need for training data can impede the adoption of novel imaging
modalities for learning-based medical image analysis. Domain adaptation methods
partially mitigate this problem by translating training data from a related
source domain to a novel target domain, but typically assume that a one-to-one
translation is possible. Our work addresses the challenge of adapting to a more
informative target domain where multiple target samples can emerge from a
single source sample. In particular we consider translating from mp-MRI to
VERDICT, a richer MRI modality involving an optimized acquisition protocol for
cancer characterization. We explicitly account for the inherent uncertainty of
this mapping and exploit it to generate multiple outputs conditioned on a
single input. Our results show that this allows us to extract systematically
better image representations for the target domain, when used in tandem with
both simple, CycleGAN-based baselines, as well as more powerful approaches that
integrate discriminative segmentation losses and/or residual adapters. When
compared to its deterministic counterparts, our approach yields substantial
improvements across a broad range of dataset sizes, increasingly strong
baselines, and evaluation measures.
</p>
<a href="http://arxiv.org/abs/2010.07411" target="_blank">arXiv:2010.07411</a> [<a href="http://arxiv.org/pdf/2010.07411" target="_blank">pdf</a>]

<h2>Theoretical Foundations of Hyperdimensional Computing. (arXiv:2010.07426v1 [cs.LG])</h2>
<h3>Anthony Thomas, Sanjoy Dasgupta, Tajana Rosing</h3>
<p>Hyperdimensional (HD) computing is a set of neurally inspired methods for
obtaining high-dimensional, low-precision, distributed representations of data.
These representations can be combined with simple, neurally plausible
algorithms to effect a variety of information processing tasks. HD computing
has recently garnered significant interest from the computer hardware community
as an energy-efficient, low-latency, and noise robust tool for solving learning
problems. In this work, we present a unified treatment of the theoretical
foundations of HD computing with a focus on the suitability of representations
for learning. In addition to providing a formal structure in which to study HD
computing, we provide useful guidance for practitioners and lay out important
open questions warranting further study.
</p>
<a href="http://arxiv.org/abs/2010.07426" target="_blank">arXiv:2010.07426</a> [<a href="http://arxiv.org/pdf/2010.07426" target="_blank">pdf</a>]

<h2>BlockFLA: Accountable Federated Learning via Hybrid Blockchain Architecture. (arXiv:2010.07427v1 [cs.CR])</h2>
<h3>Harsh Bimal Desai, Mustafa Safa Ozdayi, Murat Kantarcioglu</h3>
<p>Federated Learning (FL) is a distributed, and decentralized machine learning
protocol. By executing FL, a set of agents can jointly train a model without
sharing their datasets with each other, or a third-party. This makes FL
particularly suitable for settings where data privacy is desired.

At the same time, concealing training data gives attackers an opportunity to
inject backdoors into the trained model. It has been shown that an attacker can
inject backdoors to the trained model during FL, and then can leverage the
backdoor to make the model misclassify later. Several works tried to alleviate
this threat by designing robust aggregation functions. However, given more
sophisticated attacks are developed over time, which by-pass the existing
defenses, we approach this problem from a complementary angle in this work.
Particularly, we aim to discourage backdoor attacks by detecting, and punishing
the attackers, possibly after the end of training phase.

To this end, we develop a hybrid blockchain-based FL framework that uses
smart contracts to automatically detect, and punish the attackers via monetary
penalties. Our framework is general in the sense that, any aggregation
function, and any attacker detection algorithm can be plugged into it. We
conduct experiments to demonstrate that our framework preserves the
communication-efficient nature of FL, and provide empirical results to
illustrate that it can successfully penalize attackers by leveraging our novel
attacker detection algorithm.
</p>
<a href="http://arxiv.org/abs/2010.07427" target="_blank">arXiv:2010.07427</a> [<a href="http://arxiv.org/pdf/2010.07427" target="_blank">pdf</a>]

<h2>Skeleton-bridged Point Completion: From Global Inference to Local Adjustment. (arXiv:2010.07428v1 [cs.CV])</h2>
<h3>Yinyu Nie, Yiqun Lin, Xiaoguang Han, Shihui Guo, Jian Chang, Shuguang Cui, Jian Jun Zhang</h3>
<p>Point completion refers to complete the missing geometries of objects from
partial point clouds. Existing works usually estimate the missing shape by
decoding a latent feature encoded from the input points. However, real-world
objects are usually with diverse topologies and surface details, which a latent
feature may fail to represent to recover a clean and complete surface. To this
end, we propose a skeleton-bridged point completion network (SK-PCN) for shape
completion. Given a partial scan, our method first predicts its 3D skeleton to
obtain the global structure, and completes the surface by learning
displacements from skeletal points. We decouple the shape completion into
structure estimation and surface reconstruction, which eases the learning
difficulty and benefits our method to obtain on-surface details. Besides,
considering the missing features during encoding input points, SK-PCN adopts a
local adjustment strategy that merges the input point cloud to our predictions
for surface refinement. Comparing with previous methods, our skeleton-bridged
manner better supports point normal estimation to obtain the full surface mesh
beyond point clouds. The qualitative and quantitative experiments on both point
cloud and mesh completion show that our approach outperforms the existing
methods on various object categories.
</p>
<a href="http://arxiv.org/abs/2010.07428" target="_blank">arXiv:2010.07428</a> [<a href="http://arxiv.org/pdf/2010.07428" target="_blank">pdf</a>]

<h2>Fairness in Streaming Submodular Maximization: Algorithms and Hardness. (arXiv:2010.07431v1 [cs.LG])</h2>
<h3>Marwa El Halabi, Slobodan Mitrovi&#x107;, Ashkan Norouzi-Fard, Jakab Tardos, Jakub Tarnawski</h3>
<p>Submodular maximization has become established as the method of choice for
the task of selecting representative and diverse summaries of data. However, if
datapoints have sensitive attributes such as gender or age, such machine
learning algorithms, left unchecked, are known to exhibit bias: under- or
over-representation of particular groups. This has made the design of fair
machine learning algorithms increasingly important. In this work we address the
question: Is it possible to create fair summaries for massive datasets? To this
end, we develop the first streaming approximation algorithms for submodular
maximization under fairness constraints, for both monotone and non-monotone
functions. We validate our findings empirically on exemplar-based clustering,
movie recommendation, DPP-based summarization, and maximum coverage in social
networks, showing that fairness constraints do not significantly impact
utility.
</p>
<a href="http://arxiv.org/abs/2010.07431" target="_blank">arXiv:2010.07431</a> [<a href="http://arxiv.org/pdf/2010.07431" target="_blank">pdf</a>]

<h2>Viewmaker Networks: Learning Views for Unsupervised Representation Learning. (arXiv:2010.07432v1 [cs.LG])</h2>
<h3>Alex Tamkin, Mike Wu, Noah Goodman</h3>
<p>Many recent methods for unsupervised representation learning involve training
models to be invariant to different "views," or transformed versions of an
input. However, designing these views requires considerable human expertise and
experimentation, hindering widespread adoption of unsupervised representation
learning methods across domains and modalities. To address this, we propose
viewmaker networks: generative models that learn to produce input-dependent
views for contrastive learning. We train this network jointly with an encoder
network to produce adversarial $\ell_p$ perturbations for an input, which
yields challenging yet useful views without extensive human tuning. Our learned
views, when applied to CIFAR-10, enable comparable transfer accuracy to the the
well-studied augmentations used for the SimCLR model. Our views significantly
outperforming baseline augmentations in speech (+9% absolute) and wearable
sensor (+17% absolute) domains. We also show how viewmaker views can be
combined with handcrafted views to improve robustness to common image
corruptions. Our method demonstrates that learned views are a promising way to
reduce the amount of expertise and effort needed for unsupervised learning,
potentially extending its benefits to a much wider set of domains.
</p>
<a href="http://arxiv.org/abs/2010.07432" target="_blank">arXiv:2010.07432</a> [<a href="http://arxiv.org/pdf/2010.07432" target="_blank">pdf</a>]

<h2>AI-based BMI Inference from Facial Images: An Application to Weight Monitoring. (arXiv:2010.07442v1 [cs.CV])</h2>
<h3>Hera Siddiqui, Ajita Rattani, Dakshina Ranjan Kisku, Tanner Dean</h3>
<p>Self-diagnostic image-based methods for healthy weight monitoring is gaining
increased interest following the alarming trend of obesity. Only a handful of
academic studies exist that investigate AI-based methods for Body Mass Index
(BMI) inference from facial images as a solution to healthy weight monitoring
and management. To promote further research and development in this area, we
evaluate and compare the performance of five different deep-learning based
Convolutional Neural Network (CNN) architectures i.e., VGG19, ResNet50,
DenseNet, MobileNet, and lightCNN for BMI inference from facial images.
Experimental results on the three publicly available BMI annotated facial image
datasets assembled from social media, namely, VisualBMI, VIP-Attributes, and
Bollywood datasets, suggest the efficacy of the deep learning methods in BMI
inference from face images with minimum Mean Absolute Error (MAE) of $1.04$
obtained using ResNet50.
</p>
<a href="http://arxiv.org/abs/2010.07442" target="_blank">arXiv:2010.07442</a> [<a href="http://arxiv.org/pdf/2010.07442" target="_blank">pdf</a>]

<h2>SpaML: a Bimodal Ensemble Learning Spam Detector based on NLP Techniques. (arXiv:2010.07444v1 [cs.CR])</h2>
<h3>Jaouhar Fattahi, Mohamed Mejri</h3>
<p>In this paper, we put forward a new tool, called SpaML, for spam detection
using a set of supervised and unsupervised classifiers, and two techniques
imbued with Natural Language Processing (NLP), namely Bag of Words (BoW) and
Term Frequency-Inverse Document Frequency (TF-IDF). We first present the NLP
techniques used. Then, we present our classifiers and their performance on each
of these techniques. Then, we present our overall Ensemble Learning classifier
and the strategy we are using to combine them. Finally, we present the
interesting results shown by SpaML in terms of accuracy and precision.
</p>
<a href="http://arxiv.org/abs/2010.07444" target="_blank">arXiv:2010.07444</a> [<a href="http://arxiv.org/pdf/2010.07444" target="_blank">pdf</a>]

<h2>Deep Learning Models for Predicting Wildfires from Historical Remote-Sensing Data. (arXiv:2010.07445v1 [cs.CV])</h2>
<h3>Fantine Huot, R. Lily Hu, Matthias Ihme, Qing Wang, John Burge, Tianjian Lu, Jason Hickey, Yi-Fan Chen, John Anderson</h3>
<p>Identifying regions that have high likelihood for wildfires is a key
component of land and forestry management and disaster preparedness. We create
a data set by aggregating nearly a decade of remote-sensing data and historical
fire records to predict wildfires. This prediction problem is framed as three
machine learning tasks. Results are compared and analyzed for four different
deep learning models to estimate wildfire likelihood. The results demonstrate
that deep learning models can successfully identify areas of high fire
likelihood using aggregated data about vegetation, weather, and topography with
an AUC of 83%.
</p>
<a href="http://arxiv.org/abs/2010.07445" target="_blank">arXiv:2010.07445</a> [<a href="http://arxiv.org/pdf/2010.07445" target="_blank">pdf</a>]

<h2>Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs. (arXiv:2010.07459v1 [cs.LG])</h2>
<h3>Jueqing Lu, Lan Du, Ming Liu, Joanna Dipnall</h3>
<p>Few/Zero-shot learning is a big challenge of many classifications tasks,
where a classifier is required to recognise instances of classes that have very
few or even no training samples. It becomes more difficult in multi-label
classification, where each instance is labelled with more than one class. In
this paper, we present a simple multi-graph aggregation model that fuses
knowledge from multiple label graphs encoding different semantic label
relationships in order to study how the aggregated knowledge can benefit
multi-label zero/few-shot document classification. The model utilises three
kinds of semantic information, i.e., the pre-trained word embeddings, label
description, and pre-defined label relations. Experimental results derived on
two large clinical datasets (i.e., MIMIC-II and MIMIC-III) and the EU
legislation dataset show that methods equipped with the multi-graph knowledge
aggregation achieve significant performance improvement across almost all the
measures on few/zero-shot labels.
</p>
<a href="http://arxiv.org/abs/2010.07459" target="_blank">arXiv:2010.07459</a> [<a href="http://arxiv.org/pdf/2010.07459" target="_blank">pdf</a>]

<h2>Promise and Challenges of a Data-Driven Approach for Battery Lifetime Prognostics. (arXiv:2010.07460v1 [stat.AP])</h2>
<h3>Valentin Sulzer, Peyman Mohtat, Suhak Lee, Jason B. Siegel, Anna G. Stefanopoulou</h3>
<p>Recent data-driven approaches have shown great potential in early prediction
of battery cycle life by utilizing features from the discharge voltage curve.
However, these studies caution that data-driven approaches must be combined
with specific design of experiments in order to limit the range of aging
conditions, since the expected life of Li-ion batteries is a complex function
of various aging factors. In this work, we investigate the performance of the
data-driven approach for battery lifetime prognostics with Li-ion batteries
cycled under a variety of aging conditions, in order to determine when the
data-driven approach can successfully be applied. Results show a correlation
between the variance of the discharge capacity difference and the end-of-life
for cells aged under a wide range of charge/discharge C-rates and operating
temperatures. This holds despite the different conditions being used not only
to cycle the batteries but also to obtain the features: the features are
calculated directly from cycling data without separate slow characterization
cycles at a controlled temperature. However, the correlation weakens
considerably when the voltage data window for feature extraction is reduced, or
when features from the charge voltage curve instead of discharge are used. As
deep constant-current discharges rarely happen in practice, this imposes new
challenges for applying this method in a real-world system.
</p>
<a href="http://arxiv.org/abs/2010.07460" target="_blank">arXiv:2010.07460</a> [<a href="http://arxiv.org/pdf/2010.07460" target="_blank">pdf</a>]

<h2>Human-guided Robot Behavior Learning: A GAN-assisted Preference-based Reinforcement Learning Approach. (arXiv:2010.07467v1 [cs.RO])</h2>
<h3>Huixin Zhan, Feng Tao, Yongcan Cao</h3>
<p>Human demonstrations can provide trustful samples to train reinforcement
learning algorithms for robots to learn complex behaviors in real-world
environments. However, obtaining sufficient demonstrations may be impractical
because many behaviors are difficult for humans to demonstrate. A more
practical approach is to replace human demonstrations by human queries, i.e.,
preference-based reinforcement learning. One key limitation of the existing
algorithms is the need for a significant amount of human queries because a
large number of labeled data is needed to train neural networks for the
approximation of a continuous, high-dimensional reward function. To reduce and
minimize the need for human queries, we propose a new GAN-assisted human
preference-based reinforcement learning approach that uses a generative
adversarial network (GAN) to actively learn human preferences and then replace
the role of human in assigning preferences. The adversarial neural network is
simple and only has a binary output, hence requiring much less human queries to
train. Moreover, a maximum entropy based reinforcement learning algorithm is
designed to shape the loss towards the desired regions or away from the
undesired regions. To show the effectiveness of the proposed approach, we
present some studies on complex robotic tasks without access to the environment
reward in a typical MuJoCo robot locomotion environment. The obtained results
show our method can achieve a reduction of about 99.8% human time without
performance sacrifice.
</p>
<a href="http://arxiv.org/abs/2010.07467" target="_blank">arXiv:2010.07467</a> [<a href="http://arxiv.org/pdf/2010.07467" target="_blank">pdf</a>]

<h2>AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. (arXiv:2010.07468v1 [cs.LG])</h2>
<h3>Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, Yifan Ding, Xenophon Papademetris, James S. Duncan</h3>
<p>Most popular optimizers for deep learning can be broadly categorized as
adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient
descent (SGD) with momentum). For many models such as convolutional neural
networks (CNNs), adaptive methods typically converge faster but generalize
worse compared to SGD; for complex settings such as generative adversarial
networks (GANs), adaptive methods are typically the default because of their
stability.We propose AdaBelief to simultaneously achieve three goals: fast
convergence as in adaptive methods, good generalization as in SGD, and training
stability. The intuition for AdaBelief is to adapt the stepsize according to
the "belief" in the current gradient direction. Viewing the exponential moving
average (EMA) of the noisy gradient as the prediction of the gradient at the
next time step, if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step; if the observed
gradient is close to the prediction, we trust it and take a large step. We
validate AdaBelief in extensive experiments, showing that it outperforms other
methods with fast convergence and high accuracy on image classification and
language modeling. Specifically, on ImageNet, AdaBelief achieves comparable
accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief
demonstrates high stability and improves the quality of generated samples
compared to a well-tuned Adam optimizer. Code is available at
https://github.com/juntang-zhuang/Adabelief-Optimizer
</p>
<a href="http://arxiv.org/abs/2010.07468" target="_blank">arXiv:2010.07468</a> [<a href="http://arxiv.org/pdf/2010.07468" target="_blank">pdf</a>]

<h2>Unsupervised Self-training Algorithm Based on Deep Learning for Optical Aerial Images Change Detection. (arXiv:2010.07469v1 [cs.CV])</h2>
<h3>Yuan Zhou, Xiangrui Li</h3>
<p>Optical aerial images change detection is an important task in earth
observation and has been extensively investigated in the past few decades.
Generally, the supervised change detection methods with superior performance
require a large amount of labeled training data which is obtained by manual
annotation with high cost. In this paper, we present a novel unsupervised
self-training algorithm (USTA) for optical aerial images change detection. The
traditional method such as change vector analysis is used to generate the
pseudo labels. We use these pseudo labels to train a well designed
convolutional neural network. The network is used as a teacher to classify the
original multitemporal images to generate another set of pseudo labels. Then
two set of pseudo labels are used to jointly train a student network with the
same structure as the teacher. The final change detection result can be
obtained by the trained student network. Besides, we design an image filter to
control the usage of change information in the pseudo labels in the training
process of the network. The whole process of the algorithm is an unsupervised
process without manually marked labels. Experimental results on the real
datasets demonstrate competitive performance of our proposed method.
</p>
<a href="http://arxiv.org/abs/2010.07469" target="_blank">arXiv:2010.07469</a> [<a href="http://arxiv.org/pdf/2010.07469" target="_blank">pdf</a>]

<h2>Masked Contrastive Representation Learning for Reinforcement Learning. (arXiv:2010.07470v1 [cs.LG])</h2>
<h3>Jinhua Zhu, Yingce Xia, Lijun Wu, Jiajun Deng, Wengang Zhou, Tao Qin, Houqiang Li</h3>
<p>Improving sample efficiency is a key research problem in reinforcement
learning (RL), and CURL, which uses contrastive learning to extract high-level
features from raw pixels of individual video frames, is an efficient
algorithm~\citep{srinivas2020curl}. We observe that consecutive video frames in
a game are highly correlated but CURL deals with them independently. To further
improve data efficiency, we propose a new algorithm, masked contrastive
representation learning for RL, that takes the correlation among consecutive
inputs into consideration. In addition to the CNN encoder and the policy
network in CURL, our method introduces an auxiliary Transformer module to
leverage the correlations among video frames. During training, we randomly mask
the features of several frames, and use the CNN encoder and Transformer to
reconstruct them based on the context frames. The CNN encoder and Transformer
are jointly trained via contrastive learning where the reconstructed features
should be similar to the ground-truth ones while dissimilar to others. During
inference, the CNN encoder and the policy network are used to take actions, and
the Transformer module is discarded. Our method achieves consistent
improvements over CURL on $14$ out of $16$ environments from DMControl suite
and $21$ out of $26$ environments from Atari 2600 Games. The code is available
at https://github.com/teslacool/m-curl.
</p>
<a href="http://arxiv.org/abs/2010.07470" target="_blank">arXiv:2010.07470</a> [<a href="http://arxiv.org/pdf/2010.07470" target="_blank">pdf</a>]

<h2>Auto-STGCN: Autonomous Spatial-Temporal Graph Convolutional Network Search Based on Reinforcement Learning and Existing Research Results. (arXiv:2010.07474v1 [cs.LG])</h2>
<h3>Chunnan Wang, Kaixin Zhang, Hongzhi Wang, Bozhou Chen</h3>
<p>In recent years, many spatial-temporal graph convolutional network (STGCN)
models are proposed to deal with the spatial-temporal network data forecasting
problem. These STGCN models have their own advantages, i.e., each of them puts
forward many effective operations and achieves good prediction results in the
real applications. If users can effectively utilize and combine these excellent
operations integrating the advantages of existing models, then they may obtain
more effective STGCN models thus create greater value using existing work.
However, they fail to do so due to the lack of domain knowledge, and there is
lack of automated system to help users to achieve this goal. In this paper, we
fill this gap and propose Auto-STGCN algorithm, which makes use of existing
models to automatically explore high-performance STGCN model for specific
scenarios. Specifically, we design Unified-STGCN framework, which summarizes
the operations of existing architectures, and use parameters to control the
usage and characteristic attributes of each operation, so as to realize the
parameterized representation of the STGCN architecture and the reorganization
and fusion of advantages. Then, we present Auto-STGCN, an optimization method
based on reinforcement learning, to quickly search the parameter search space
provided by Unified-STGCN, and generate optimal STGCN models automatically.
Extensive experiments on real-world benchmark datasets show that our Auto-STGCN
can find STGCN models superior to existing STGCN models with heuristic
parameters, which demonstrates the effectiveness of our proposed method.
</p>
<a href="http://arxiv.org/abs/2010.07474" target="_blank">arXiv:2010.07474</a> [<a href="http://arxiv.org/pdf/2010.07474" target="_blank">pdf</a>]

<h2>Neural Deepfake Detection with Factual Structure of Text. (arXiv:2010.07475v1 [cs.CL])</h2>
<h3>Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin</h3>
<p>Deepfake detection, the task of automatically discriminating
machine-generated text, is increasingly critical with recent advances in
natural language generative models. Existing approaches to deepfake detection
typically represent documents with coarse-grained representations. However,
they struggle to capture factual structures of documents, which is a
discriminative factor between machine-generated and human-written text
according to our statistical analysis. To address this, we propose a
graph-based model that utilizes the factual structure of a document for
deepfake detection of text. Our approach represents the factual structure of a
given document as an entity graph, which is further utilized to learn sentence
representations with a graph neural network. Sentence representations are then
composed to a document representation for making predictions, where consistent
relations between neighboring sentences are sequentially modeled. Results of
experiments on two public deepfake datasets show that our approach
significantly improves strong base models built with RoBERTa. Model analysis
further indicates that our model can distinguish the difference in the factual
structure between machine-generated text and human-written text.
</p>
<a href="http://arxiv.org/abs/2010.07475" target="_blank">arXiv:2010.07475</a> [<a href="http://arxiv.org/pdf/2010.07475" target="_blank">pdf</a>]

<h2>Spherical Knowledge Distillation. (arXiv:2010.07485v1 [cs.LG])</h2>
<h3>Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, Deng Cai</h3>
<p>Knowledge distillation aims at obtaining a small but effective deep model by
transferring knowledge from a much larger one. The previous approaches try to
reach this goal by simply "logit-supervised" information transferring between
the teacher and student, which somehow can be subsequently decomposed as the
transfer of normalized logits and $l^2$ norm. We argue that the norm of logits
is actually interference, which damages the efficiency in the transfer process.
To address this problem, we propose Spherical Knowledge Distillation (SKD).
Specifically, we project the teacher and the student's logits into a unit
sphere, and then we can efficiently perform knowledge distillation on the
sphere. We verify our argument via theoretical analysis and ablation study.
Extensive experiments have demonstrated the superiority and scalability of our
method over the SOTAs.
</p>
<a href="http://arxiv.org/abs/2010.07485" target="_blank">arXiv:2010.07485</a> [<a href="http://arxiv.org/pdf/2010.07485" target="_blank">pdf</a>]

<h2>CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical Imaging. (arXiv:2010.07486v1 [eess.IV])</h2>
<h3>Lei Mou, Yitian Zhao, Huazhu Fu, Yonghuai Liu, Jun Cheng, Yalin Zheng, Pan Su, Jianlong Yang, Li Chen, Alejandro F Frang, Masahiro Akiba, Jiang Liu</h3>
<p>Automated detection of curvilinear structures, e.g., blood vessels or nerve
fibres, from medical and biomedical images is a crucial early step in automatic
image interpretation associated to the management of many diseases. Precise
measurement of the morphological changes of these curvilinear organ structures
informs clinicians for understanding the mechanism, diagnosis, and treatment of
e.g. cardiovascular, kidney, eye, lung, and neurological conditions. In this
work, we propose a generic and unified convolution neural network for the
segmentation of curvilinear structures and illustrate in several 2D/3D medical
imaging modalities. We introduce a new curvilinear structure segmentation
network (CS2-Net), which includes a self-attention mechanism in the encoder and
decoder to learn rich hierarchical representations of curvilinear structures.
Two types of attention modules - spatial attention and channel attention - are
utilized to enhance the inter-class discrimination and intra-class
responsiveness, to further integrate local features with their global
dependencies and normalization, adaptively. Furthermore, to facilitate the
segmentation of curvilinear structures in medical images, we employ a 1x3 and a
3x1 convolutional kernel to capture boundary features. ...
</p>
<a href="http://arxiv.org/abs/2010.07486" target="_blank">arXiv:2010.07486</a> [<a href="http://arxiv.org/pdf/2010.07486" target="_blank">pdf</a>]

<h2>RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2 Visual Field Data based on Retinal Structure. (arXiv:2010.07488v1 [cs.LG])</h2>
<h3>Shounak Datta, Eduardo B. Mariottoni, David Dov, Alessandro A. Jammal, Lawrence Carin, Felipe A. Medeiros</h3>
<p>Glaucoma is the leading cause of irreversible blindness in the world,
affecting over 70 million people. The cumbersome Standard Automated Perimetry
(SAP) test is most frequently used to detect visual loss due to glaucoma. Due
to the SAP test's innate difficulty and its high test-retest variability, we
propose the RetiNerveNet, a deep convolutional recursive neural network for
obtaining estimates of the SAP visual field. RetiNerveNet uses information from
the more objective Spectral-Domain Optical Coherence Tomography (SDOCT).
RetiNerveNet attempts to trace-back the arcuate convergence of the retinal
nerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness
around the optic disc, to estimate individual age-corrected 24-2 SAP values.
Recursive passes through the proposed network sequentially yield estimates of
the visual locations progressively farther from the optic disc. The proposed
network is able to obtain more accurate estimates of the individual visual
field values, compared to a number of baselines, implying its utility as a
proxy for SAP. We further augment RetiNerveNet to additionally predict the SAP
Mean Deviation values and also create an ensemble of RetiNerveNets that further
improves the performance, by increasingly weighting-up underrepresented parts
of the training data.
</p>
<a href="http://arxiv.org/abs/2010.07488" target="_blank">arXiv:2010.07488</a> [<a href="http://arxiv.org/pdf/2010.07488" target="_blank">pdf</a>]

<h2>Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing. (arXiv:2010.07489v1 [cs.LG])</h2>
<h3>Zhen Xiang, David J. Miller, George Kesidis</h3>
<p>Backdoor data poisoning is an emerging form of adversarial attack usually
against deep neural network image classifiers. The attacker poisons the
training set with a relatively small set of images from one (or several) source
class(es), embedded with a backdoor pattern and labeled to a target class. For
a successful attack, during operation, the trained classifier will: 1)
misclassify a test image from the source class(es) to the target class whenever
the same backdoor pattern is present; 2) maintain a high classification
accuracy for backdoor-free test images. In this paper, we make a break-through
in defending backdoor attacks with imperceptible backdoor patterns (e.g.
watermarks) before/during the training phase. This is a challenging problem
because it is a priori unknown which subset (if any) of the training set has
been poisoned. We propose an optimization-based reverse-engineering defense,
that jointly: 1) detects whether the training set is poisoned; 2) if so,
identifies the target class and the training images with the backdoor pattern
embedded; and 3) additionally, reversely engineers an estimate of the backdoor
pattern used by the attacker. In benchmark experiments on CIFAR-10, for a large
variety of attacks, our defense achieves a new state-of-the-art by reducing the
attack success rate to no more than 4.9% after removing detected suspicious
training images.
</p>
<a href="http://arxiv.org/abs/2010.07489" target="_blank">arXiv:2010.07489</a> [<a href="http://arxiv.org/pdf/2010.07489" target="_blank">pdf</a>]

<h2>Securing Manufacturing Using Blockchain. (arXiv:2010.07493v1 [cs.CR])</h2>
<h3>Zahra Jadidi, Ali Dorri, Raja Jurdak, Colin Fidge</h3>
<p>Due to the rise of Industrial Control Systems (ICSs) cyber-attacks in the
recent decade, various security frameworks have been designed for anomaly
detection. While advanced ICS attacks use sequential phases to launch their
final attacks, existing anomaly detection methods can only monitor a single
source of data. Therefore, analysis of multiple security data can provide
comprehensive and system-wide anomaly detection in industrial networks. In this
paper, we propose an anomaly detection framework for ICSs that consists of two
stages: i) blockchain-based log management where the logs of ICS devices are
collected in a secure and distributed manner, and ii) multi-source anomaly
detection where the blockchain logs are analysed using multi-source deep
learning which in turn provides a system wide anomaly detection method.

We validated our framework using two ICS datasets: a factory automation
dataset and a Secure Water Treatment (SWAT) dataset. These datasets contain
physical and network level normal and abnormal traffic. The performance of our
new framework is compared with single-source machine learning methods. The
precision of our framework is 95% which is comparable with single-source
anomaly detectors.
</p>
<a href="http://arxiv.org/abs/2010.07493" target="_blank">arXiv:2010.07493</a> [<a href="http://arxiv.org/pdf/2010.07493" target="_blank">pdf</a>]

<h2>Multi-Task Deep Reinforcement Learning with Knowledge Transfer for Continuous Control. (arXiv:2010.07494v1 [cs.LG])</h2>
<h3>Zhiyuan Xu, Kun Wu, Zhengping Che, Jian Tang, Jieping Ye</h3>
<p>While Deep Reinforcement Learning (DRL) has emerged as a promising approach
to many complex tasks, it remains challenging to train a single DRL agent that
is capable of undertaking multiple different continuous control tasks. In this
paper, we present a Knowledge Transfer based Multi-task Deep Reinforcement
Learning framework (KTM-DRL) for continuous control, which enables a single DRL
agent to achieve expert-level performance in multiple different tasks by
learning from task-specific teachers. In KTM-DRL, the multi-task agent first
leverages an offline knowledge transfer algorithm designed particularly for the
actor-critic architecture to quickly learn a control policy from the experience
of task-specific teachers, and then it employs an online learning algorithm to
further improve itself by learning from new online transition samples under the
guidance of those teachers. We perform a comprehensive empirical study with two
commonly-used benchmarks in the MuJoCo continuous control task suite. The
experimental results well justify the effectiveness of KTM-DRL and its
knowledge transfer and online learning algorithms, as well as its superiority
over the state-of-the-art by a large margin.
</p>
<a href="http://arxiv.org/abs/2010.07494" target="_blank">arXiv:2010.07494</a> [<a href="http://arxiv.org/pdf/2010.07494" target="_blank">pdf</a>]

<h2>Multi-Task Learning for Cross-Lingual Abstractive Summarization. (arXiv:2010.07503v1 [cs.CL])</h2>
<h3>Sho Takase, Naoaki Okazaki</h3>
<p>We present a multi-task learning framework for cross-lingual abstractive
summarization to augment training data. Recent studies constructed pseudo
cross-lingual abstractive summarization data to train their neural
encoder-decoders. Meanwhile, we introduce existing genuine data such as
translation pairs and monolingual abstractive summarization data into training.
Our proposed method, Transum, attaches a special token to the beginning of the
input sentence to indicate the target task. The special token enables us to
incorporate the genuine data into the training data easily. The experimental
results show that Transum achieves better performance than the model trained
with only pseudo cross-lingual summarization data. In addition, we achieve the
top ROUGE score on Chinese-English and Arabic-English abstractive
summarization. Moreover, Transum also has a positive effect on machine
translation. Experimental results indicate that Transum improves the
performance from the strong baseline, Transformer, in Chinese-English,
Arabic-English, and English-Japanese translation datasets.
</p>
<a href="http://arxiv.org/abs/2010.07503" target="_blank">arXiv:2010.07503</a> [<a href="http://arxiv.org/pdf/2010.07503" target="_blank">pdf</a>]

<h2>Optimal Dispatch in Emergency Service System via Reinforcement Learning. (arXiv:2010.07513v1 [eess.SY])</h2>
<h3>Cheng Hua, Tauhid Zaman</h3>
<p>In the United States, medical responses by fire departments over the last
four decades increased by 367%. This had made it critical to decision makers in
emergency response departments that existing resources are efficiently used. In
this paper, we model the ambulance dispatch problem as an average-cost Markov
decision process and present a policy iteration approach to find an optimal
dispatch policy. We then propose an alternative formulation using post-decision
states that is shown to be mathematically equivalent to the original model, but
with a much smaller state space. We present a temporal difference learning
approach to the dispatch problem based on the post-decision states. In our
numerical experiments, we show that our obtained temporal-difference policy
outperforms the benchmark myopic policy. Our findings suggest that emergency
response departments can improve their performance with minimal to no cost.
</p>
<a href="http://arxiv.org/abs/2010.07513" target="_blank">arXiv:2010.07513</a> [<a href="http://arxiv.org/pdf/2010.07513" target="_blank">pdf</a>]

<h2>Holistic Combination of Structural and Textual Code Information for Context based API Recommendation. (arXiv:2010.07514v1 [cs.SE])</h2>
<h3>Chi Chen, Xin Peng, Zhenchang Xing, Jun Sun, Xin Wang, Yifan Zhao, Wenyun Zhao</h3>
<p>Context based API recommendation is an important way to help developers find
the needed APIs effectively and efficiently. For effective API recommendation,
we need not only a joint view of both structural and textual code information,
but also a holistic view of correlated API usage in control and data flow graph
as a whole. Unfortunately, existing API recommendation methods exploit
structural or textual code information separately. In this work, we propose a
novel API recommendation approach called APIRec-CST (API Recommendation by
Combining Structural and Textual code information). APIRec-CST is a deep
learning model that combines the API usage with the text information in the
source code based on an API Context Graph Network and a Code Token Network that
simultaneously learn structural and textual features for API recommendation. We
apply APIRec-CST to train a model for JDK library based on 1,914 open-source
Java projects and evaluate the accuracy and MRR (Mean Reciprocal Rank) of API
recommendation with another 6 open-source projects. The results show that our
approach achieves respectively a top-1, top-5, top-10 accuracy and MRR of
60.3%, 81.5%, 87.7% and 69.4%, and significantly outperforms an existing
graph-based statistical approach and a tree-based deep learning approach for
API recommendation. A further analysis shows that textual code information
makes sense and improves the accuracy and MRR. We also conduct a user study in
which two groups of students are asked to finish 6 programming tasks with or
without our APIRec-CST plugin. The results show that APIRec-CST can help the
students to finish the tasks faster and more accurately and the feedback on the
usability is overwhelmingly positive.
</p>
<a href="http://arxiv.org/abs/2010.07514" target="_blank">arXiv:2010.07514</a> [<a href="http://arxiv.org/pdf/2010.07514" target="_blank">pdf</a>]

<h2>Unsupervised Video Anomaly Detection via Flow-based Generative Modeling on Appearance and Motion Latent Features. (arXiv:2010.07524v1 [cs.CV])</h2>
<h3>MyeongAh Cho, Taeoh Kim, Sangyoun Lee</h3>
<p>Surveillance video anomaly detection searches for anomalous events such as
crimes or accidents among normal scenes. Since anomalous events occur rarely,
there is a class imbalance problem between normal and abnormal data and it is
impossible to collect all potential anomalous events, which makes the task
challenging. Therefore, performing anomaly detection requires learning the
patterns of normal scenes to detect unseen and undefined anomalies. Since
abnormal scenes are distinguished from normal scenes by appearance or motion,
lots of previous approaches have used an explicit pre-trained model such as
optical flow for motion information, which makes the network complex and
dependent on the pre-training. We propose an implicit two-path AutoEncoder
(ITAE) that exploits the structure of a SlowFast network and focuses on spatial
and temporal information through appearance (slow) and motion (fast) encoders,
respectively. The two encoders and a single decoder learn normal appearance and
behavior by reconstructing normal videos of the training set. Furthermore, with
features from the two encoders, we suggest density estimation through
flow-based generative models to learn the tractable likelihoods of appearance
and motion features. Finally, we show the effectiveness of appearance and
motion encoders and their distribution modeling through experiments in three
benchmarks which result outperforms the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2010.07524" target="_blank">arXiv:2010.07524</a> [<a href="http://arxiv.org/pdf/2010.07524" target="_blank">pdf</a>]

<h2>Self-Supervised Domain Adaptation with Consistency Training. (arXiv:2010.07539v1 [cs.CV])</h2>
<h3>L. Xiao, J. Xu, D. Zhao, Z. Wang, L. Wang, Y. Nie, B. Dai</h3>
<p>We consider the problem of unsupervised domain adaptation for image
classification. To learn target-domain-aware features from the unlabeled data,
we create a self-supervised pretext task by augmenting the unlabeled data with
a certain type of transformation (specifically, image rotation) and ask the
learner to predict the properties of the transformation. However, the obtained
feature representation may contain a large amount of irrelevant information
with respect to the main task. To provide further guidance, we force the
feature representation of the augmented data to be consistent with that of the
original data. Intuitively, the consistency introduces additional constraints
to representation learning, therefore, the learned representation is more
likely to focus on the right information about the main task. Our experimental
results validate the proposed method and demonstrate state-of-the-art
performance on classical domain adaptation benchmarks. Code is available at
https://github.com/Jiaolong/ss-da-consistency.
</p>
<a href="http://arxiv.org/abs/2010.07539" target="_blank">arXiv:2010.07539</a> [<a href="http://arxiv.org/pdf/2010.07539" target="_blank">pdf</a>]

<h2>Multi-Objective PMU Allocation for Resilient Power System Monitoring. (arXiv:2010.07540v1 [eess.SY])</h2>
<h3>Hamed Haggi, Wei Sun, Junjian Qi</h3>
<p>Phasor measurement units (PMUs) enable better system monitoring and security
enhancement in smart grids. In order to enhance power system resilience against
outages and blackouts caused by extreme weather events or man-made attacks, it
remains a major challenge to determine the optimal number and location of PMUs.
In this paper, a multi-objective resilient PMU placement (MORPP) problem is
formulated, and solved by a modified Teaching-Learning-Based Optimization
(MO-TLBO) algorithm. Three objectives are considered in the MORPP problem,
minimizing the number of PMUs, maximizing the system observability, and
minimizing the voltage stability index. The effectiveness of the proposed
method is validated through testing on IEEE 14-bus, 30-bus, and 118-bus test
systems. The advantage of the MO-TLBO-based MORPP is demonstrated through the
comparison with other methods in the literature, in terms of iteration number,
optimality and time of convergence.
</p>
<a href="http://arxiv.org/abs/2010.07540" target="_blank">arXiv:2010.07540</a> [<a href="http://arxiv.org/pdf/2010.07540" target="_blank">pdf</a>]

<h2>Mitigating Byzantine Attacks in Federated Learning. (arXiv:2010.07541v1 [cs.DC])</h2>
<h3>Saurav Prakash, Amir Salman Avestimehr</h3>
<p>Prior solutions for mitigating Byzantine failures in federated learning, such
as element-wise median of the stochastic gradient descent (SGD) based updates
from the clients, tend to leverage the similarity of updates from the
non-Byzantine clients. However, when data is non-IID, as is typical in mobile
networks, the updates received from non-Byzantine clients are quite diverse,
resulting in poor convergence performance of such approaches. On the other
hand, current algorithms that address heterogeneous data distribution across
clients are limited in scope and do not perform well when there is variability
in the number and identities of the Byzantine clients, or when general
non-convex loss functions are considered. We propose `DiverseFL' that jointly
addresses three key challenges of Byzantine resilient federated learning -- (i)
non-IID data distribution across clients, (ii) variable Byzantine fault model,
and (iii) generalization to non-convex and non-smooth optimization. DiverseFL
leverages computing capability of the federated learning server that for each
iteration, computes a `guiding' gradient for each client over a tiny sample of
data received only once from the client before start of the training. The
server uses `per client' criteria for flagging Byzantine clients, by comparing
the corresponding guiding gradient with the client's gradient update. The
server then updates the model using the gradients received from the non-flagged
clients. As we demonstrate in our experiments with benchmark datasets and
popular Byzantine attacks, our proposed approach performs better than the prior
algorithms, almost matching the performance of the `Oracle SGD', where the
server knows the identities of the Byzantine clients.
</p>
<a href="http://arxiv.org/abs/2010.07541" target="_blank">arXiv:2010.07541</a> [<a href="http://arxiv.org/pdf/2010.07541" target="_blank">pdf</a>]

<h2>Adversarial Images through Stega Glasses. (arXiv:2010.07542v1 [cs.CR])</h2>
<h3>Beno&#xee;t Bonnet, Teddy Furon, Patrick Bas (CRIStAL)</h3>
<p>This paper explores the connection between steganography and adversarial
images. On the one hand, ste-ganalysis helps in detecting adversarial
perturbations. On the other hand, steganography helps in forging adversarial
perturbations that are not only invisible to the human eye but also
statistically undetectable. This work explains how to use these information
hiding tools for attacking or defending computer vision image classification.
We play this cat and mouse game with state-of-art classifiers, steganalyzers,
and steganographic embedding schemes. It turns out that steganography helps
more the attacker than the defender.
</p>
<a href="http://arxiv.org/abs/2010.07542" target="_blank">arXiv:2010.07542</a> [<a href="http://arxiv.org/pdf/2010.07542" target="_blank">pdf</a>]

<h2>Deep Learning of Koopman Representation for Control. (arXiv:2010.07546v1 [cs.LG])</h2>
<h3>Yiqiang Han, Wenjian Hao, Umesh Vaidya</h3>
<p>We develop a data-driven, model-free approach for the optimal control of the
dynamical system. The proposed approach relies on the Deep Neural Network (DNN)
based learning of Koopman operator for the purpose of control. In particular,
DNN is employed for the data-driven identification of basis function used in
the linear lifting of nonlinear control system dynamics. The controller
synthesis is purely data-driven and does not rely on a priori domain knowledge.
The OpenAI Gym environment, employed for Reinforcement Learning-based control
design, is used for data generation and learning of Koopman operator in control
setting. The method is applied to two classic dynamical systems on OpenAI Gym
environment to demonstrate the capability.
</p>
<a href="http://arxiv.org/abs/2010.07546" target="_blank">arXiv:2010.07546</a> [<a href="http://arxiv.org/pdf/2010.07546" target="_blank">pdf</a>]

<h2>MOTChallenge: A Benchmark for Single-camera Multiple Target Tracking. (arXiv:2010.07548v1 [cs.CV])</h2>
<h3>Patrick Dendorfer, Aljo&#x161;a O&#x161;ep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, Laura Leal-Taix&#xe9;</h3>
<p>Standardized benchmarks have been crucial in pushing the performance of
computer vision algorithms, especially since the advent of deep learning.
Although leaderboards should not be over-claimed, they often provide the most
objective measure of performance and are therefore important guides for
research. We present MOTChallenge, a benchmark for single-camera Multiple
Object Tracking (MOT) launched in late 2014, to collect existing and new data,
and create a framework for the standardized evaluation of multiple object
tracking methods. The benchmark is focused on multiple people tracking, since
pedestrians are by far the most studied object in the tracking community, with
applications ranging from robot navigation to self-driving cars. This paper
collects the first three releases of the benchmark: (i) MOT15, along with
numerous state-of-the-art results that were submitted in the last years, (ii)
MOT16, which contains new challenging videos, and (iii) MOT17, that extends
MOT16 sequences with more precise labels and evaluates tracking performance on
three different object detectors. The second and third release not only offers
a significant increase in the number of labeled boxes but also provide labels
for multiple object classes beside pedestrians, as well as the level of
visibility for every single object of interest. We finally provide a
categorization of state-of-the-art trackers and a broad error analysis. This
will help newcomers understand the related work and research trends in the MOT
community, and hopefully shred some light into potential future research
directions.
</p>
<a href="http://arxiv.org/abs/2010.07548" target="_blank">arXiv:2010.07548</a> [<a href="http://arxiv.org/pdf/2010.07548" target="_blank">pdf</a>]

<h2>Solar coronal magnetic field extrapolation from synchronic data with AI-generated farside. (arXiv:2010.07553v1 [astro-ph.SR])</h2>
<h3>Hyunjin Jeong, Yong-Jae Moon, Eunsu Park, Harim Lee</h3>
<p>Solar magnetic fields play a key role in understanding the nature of the
coronal phenomena. Global coronal magnetic fields are usually extrapolated from
photospheric fields for which farside data were taken about two weeks ago when
it was at the frontside. For the first time we have constructed the
extrapolations of global magnetic fields using frontside and AI-generated
farside magnetic fields at a near-real time basis. We generate the farside
magnetograms from three channel farside observations of Solar Terrestrial
Relations Observatory (STEREO) $-$Ahead (A) and $-$Behind (B) by our deep
learning model trained with frontside Solar Dynamics Observatory (SDO) EUV
images and magnetograms. For frontside testing data sets, we demonstrate that
the generated magnetic field distributions are consistent with the real ones;
not only active regions (ARs), but also quiet regions of the Sun. We make
global magnetic field synchronic maps in which conventional farside data are
replaced by farside ones generated by our model. The synchronic maps show much
better not only the appearance of ARs but also the disappearance of others on
the solar surface than before. We use these synchronized magnetic data to
extrapolate the global coronal fields using Potential Field Source Surface
(PFSS) model. We show that our results are much more consistent with coronal
observations than those of the conventional method in view of solar active
regions and coronal holes. We present several positive prospects of our new
methodology for the study of solar corona, heliosphere, and space weather.
</p>
<a href="http://arxiv.org/abs/2010.07553" target="_blank">arXiv:2010.07553</a> [<a href="http://arxiv.org/pdf/2010.07553" target="_blank">pdf</a>]

<h2>Encoder-decoder semantic segmentation models for electroluminescence images of thin-film photovoltaic modules. (arXiv:2010.07556v1 [eess.IV])</h2>
<h3>Evgenii Sovetkin, Elbert Jan Achterberg, Thomas Weber, Bart E. Pieters</h3>
<p>We consider a series of image segmentation methods based on the deep neural
networks in order to perform semantic segmentation of electroluminescence (EL)
images of thin-film modules. We utilize the encoder-decoder deep neural network
architecture. The framework is general such that it can easily be extended to
other types of images (e.g. thermography) or solar cell technologies (e.g.
crystalline silicon modules). The networks are trained and tested on a sample
of images from a database with 6000 EL images of Copper Indium Gallium
Diselenide (CIGS) thin film modules. We selected two types of features to
extract, shunts and so called "droplets". The latter feature is often observed
in the set of images. Several models are tested using various combinations of
encoder-decoder layers, and a procedure is proposed to select the best model.
We show exemplary results with the best selected model. Furthermore, we applied
the best model to the full set of 6000 images and demonstrate that the
automated segmentation of EL images can reveal many subtle features which
cannot be inferred from studying a small sample of images. We believe these
features can contribute to process optimization and quality control.
</p>
<a href="http://arxiv.org/abs/2010.07556" target="_blank">arXiv:2010.07556</a> [<a href="http://arxiv.org/pdf/2010.07556" target="_blank">pdf</a>]

<h2>A Robust Deep Unfolded Network for Sparse Signal Recovery from Noisy Binary Measurements. (arXiv:2010.07564v1 [cs.LG])</h2>
<h3>Y.Yang, P.Xiao, B.Liao, N.Deligiannis</h3>
<p>We propose a novel deep neural network, coined DeepFPC-$\ell_2$, for solving
the 1-bit compressed sensing problem. The network is designed by unfolding the
iterations of the fixed-point continuation (FPC) algorithm with one-sided
$\ell_2$-norm (FPC-$\ell_2$). The DeepFPC-$\ell_2$ method shows higher signal
reconstruction accuracy and convergence speed than the traditional FPC-$\ell_2$
algorithm. Furthermore, we compare its robustness to noise with the previously
proposed DeepFPC network---which stemmed from unfolding the FPC-$\ell_1$
algorithm---for different signal to noise ratio (SNR) and sign-flipped ratio
(flip ratio) scenarios. We show that the proposed network has better noise
immunity than the previous DeepFPC method. This result indicates that the
robustness of a deep-unfolded neural network is related with that of the
algorithm it stems from.
</p>
<a href="http://arxiv.org/abs/2010.07564" target="_blank">arXiv:2010.07564</a> [<a href="http://arxiv.org/pdf/2010.07564" target="_blank">pdf</a>]

<h2>Bi-GCN: Binary Graph Convolutional Network. (arXiv:2010.07565v1 [cs.LG])</h2>
<h3>Junfu Wang, Yunhong Wang, Zhen Yang, Liang Yang, Yuanfang Guo</h3>
<p>Graph Neural Networks (GNNs) have achieved tremendous success in graph
representation learning. Unfortunately, current GNNs usually rely on loading
the entire attributed graph into the network for processing. This implicit
assumption may not be satisfied with limited memory resources, especially when
the attributed graph is large. In this paper, we propose a Binary Graph
Convolutional Network (Bi-GCN), which binarizes both the network parameters and
input node features. Besides, the original matrix multiplications are revised
to binary operations for accelerations. According to the theoretical analysis,
our Bi-GCN can reduce the memory consumption by ~31x for both the network
parameters and input data, and accelerate the inference speed by ~53x.
Extensive experiments have demonstrated that our Bi-GCN can give a comparable
prediction performance compared to the full-precision baselines. Besides, our
binarization approach can be easily applied to other GNNs, which has been
verified in the experiments.
</p>
<a href="http://arxiv.org/abs/2010.07565" target="_blank">arXiv:2010.07565</a> [<a href="http://arxiv.org/pdf/2010.07565" target="_blank">pdf</a>]

<h2>Towards Self-Improving Hybrid Elasticity Control of Cloud-based Software Systems. (arXiv:2010.07584v1 [cs.DC])</h2>
<h3>Mohan Baruwal Chhetri, Abdur Rahim Mohammad Forkan, Anton V. Uzunov, Surya Nepal</h3>
<p>Elasticity is a form of self-adaptivity in cloud-based software systems that
is typically restricted to the infrastructure layer and realized through
auto-scaling. However, both reactive and proactive forms of infrastructure
auto-scaling have limitations, when used separately as well as together. To
address these limitations, we propose an approach for self-improving hybrid
elasticity control that combines (a) infrastructure and software elasticity,
and (b) proactive, reactive and responsive decision-making. At the
infrastructure layer, resources are provisioned proactively based on
one-step-ahead workload forecasts, and reactively, based on observed workload
variations. At the software layer, features are activated or deactivated in
response to transient, minor deviations from the predicted workload. The
proposed approach can lead to better performance-aware and cost-effective
resource management in cloud-based software systems. We validate our approach
via a partial realization and simulation with real-world datasets.
</p>
<a href="http://arxiv.org/abs/2010.07584" target="_blank">arXiv:2010.07584</a> [<a href="http://arxiv.org/pdf/2010.07584" target="_blank">pdf</a>]

<h2>Survive the Schema Changes: Integration of Unmanaged Data Using Deep Learning. (arXiv:2010.07586v1 [cs.DB])</h2>
<h3>Zijie Wang, Lixi Zhou, Amitabh Das, Valay Dave, Zhanpeng Jin, Jia Zou</h3>
<p>Data is the king in the age of AI. However data integration is often a
laborious task that is hard to automate. Schema change is one significant
obstacle to the automation of the end-to-end data integration process. Although
there exist mechanisms such as query discovery and schema modification language
to handle the problem, these approaches can only work with the assumption that
the schema is maintained by a database. However, we observe diversified schema
changes in heterogeneous data and open data, most of which has no schema
defined. In this work, we propose to use deep learning to automatically deal
with schema changes through a super cell representation and automatic injection
of perturbations to the training data to make the model robust to schema
changes. Our experimental results demonstrate that our proposed approach is
effective for two real-world data integration scenarios: coronavirus data
integration, and machine log integration.
</p>
<a href="http://arxiv.org/abs/2010.07586" target="_blank">arXiv:2010.07586</a> [<a href="http://arxiv.org/pdf/2010.07586" target="_blank">pdf</a>]

<h2>Depth-Width Trade-offs for Neural Networks via Topological Entropy. (arXiv:2010.07587v1 [cs.LG])</h2>
<h3>Kaifeng Bu, Yaobo Zhang, Qingxian Luo</h3>
<p>One of the central problems in the study of deep learning theory is to
understand how the structure properties, such as depth, width and the number of
nodes, affect the expressivity of deep neural networks. In this work, we show a
new connection between the expressivity of deep neural networks and topological
entropy from dynamical system, which can be used to characterize depth-width
trade-offs of neural networks. We provide an upper bound on the topological
entropy of neural networks with continuous semi-algebraic units by the
structure parameters. Specifically, the topological entropy of ReLU network
with $l$ layers and $m$ nodes per layer is upper bounded by $O(l\log m)$.
Besides, if the neural network is a good approximation of some function $f$,
then the size of the neural network has an exponential lower bound with respect
to the topological entropy of $f$. Moreover, we discuss the relationship
between topological entropy, the number of oscillations, periods and Lipschitz
constant.
</p>
<a href="http://arxiv.org/abs/2010.07587" target="_blank">arXiv:2010.07587</a> [<a href="http://arxiv.org/pdf/2010.07587" target="_blank">pdf</a>]

<h2>Respecting Domain Relations: Hypothesis Invariance for Domain Generalization. (arXiv:2010.07591v1 [cs.LG])</h2>
<h3>Ziqi Wang, Marco Loog, Jan van Gemert</h3>
<p>In domain generalization, multiple labeled non-independent and
non-identically distributed source domains are available during training while
neither the data nor the labels of target domains are. Currently, learning
so-called domain invariant representations (DIRs) is the prevalent approach to
domain generalization. In this work, we define DIRs employed by existing works
in probabilistic terms and show that by learning DIRs, overly strict
requirements are imposed concerning the invariance. Particularly, DIRs aim to
perfectly align representations of different domains, i.e. their input
distributions. This is, however, not necessary for good generalization to a
target domain and may even dispose of valuable classification information. We
propose to learn so-called hypothesis invariant representations (HIRs), which
relax the invariance assumptions by merely aligning posteriors, instead of
aligning representations. We report experimental results on public domain
generalization datasets to show that learning HIRs is more effective than
learning DIRs. In fact, our approach can even compete with approaches using
prior knowledge about domains.
</p>
<a href="http://arxiv.org/abs/2010.07591" target="_blank">arXiv:2010.07591</a> [<a href="http://arxiv.org/pdf/2010.07591" target="_blank">pdf</a>]

<h2>Object Tracking Using Spatio-Temporal Future Prediction. (arXiv:2010.07605v1 [cs.CV])</h2>
<h3>Yuan Liu, Ruoteng Li, Robby T. Tan, Yu Cheng, Xiubao Sui</h3>
<p>Occlusion is a long-standing problem that causes many modern tracking methods
to be erroneous. In this paper, we address the occlusion problem by exploiting
the current and future possible locations of the target object from its past
trajectory. To achieve this, we introduce a learning-based tracking method that
takes into account background motion modeling and trajectory prediction. Our
trajectory prediction module predicts the target object's locations in the
current and future frames based on the object's past trajectory. Since, in the
input video, the target object's trajectory is not only affected by the object
motion but also the camera motion, our background motion module estimates the
camera motion. So that the object's trajectory can be made independent from it.
To dynamically switch between the appearance-based tracker and the trajectory
prediction, we employ a network that can assess how good a tracking prediction
is, and we use the assessment scores to choose between the appearance-based
tracker's prediction and the trajectory-based prediction. Comprehensive
evaluations show that the proposed method sets a new state-of-the-art
performance on commonly used tracking benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.07605" target="_blank">arXiv:2010.07605</a> [<a href="http://arxiv.org/pdf/2010.07605" target="_blank">pdf</a>]

<h2>Learning Better Representation for Tables by Self-Supervised Tasks. (arXiv:2010.07606v1 [cs.CL])</h2>
<h3>Liang Li, Can Ma, Yinliang Yue, Linjun Shou, Dayong Hu</h3>
<p>Table-to-text generation aims at automatically generating natural text to
help people to conveniently obtain the important information in tables.
Although neural models for table-to-text have achieved remarkable progress,
some problems still overlooked. The first is that the values recorded in many
tables are mostly numbers in practice. The existing approaches do not do
special treatment for these, and still regard these as words in natural
language text. Secondly, the target texts in training dataset may contain
redundant information or facts do not exist in the input tables. These may give
wrong supervision signals to some methods based on content selection and
planning and auxiliary supervision. To solve these problems, we propose two
self-supervised tasks, Number Ordering and Significance Ordering, to help to
learn better table representation. The former works on the column dimension to
help to incorporate the size property of numbers into table representation. The
latter acts on row dimension and help to learn a significance-aware table
representation. We test our methods on the widely used dataset ROTOWIRE which
consists of NBA game statistic and related news. The experimental results
demonstrate that the model trained together with these two self-supervised
tasks can generate text that contains more salient and well-organized facts,
even without modeling context selection and planning. And we achieve the
state-of-the-art performance on automatic metrics.
</p>
<a href="http://arxiv.org/abs/2010.07606" target="_blank">arXiv:2010.07606</a> [<a href="http://arxiv.org/pdf/2010.07606" target="_blank">pdf</a>]

<h2>Unsupervised Constrative Person Re-identification. (arXiv:2010.07608v1 [cs.CV])</h2>
<h3>Bo Pang, Deming Zhai, Junjun Jiang, Xianming Liu</h3>
<p>Person re-identification (ReID) aims at searching the same identity person
among images captured by various cameras. Unsupervised person ReID attracts a
lot of attention recently, due to it works without intensive manual annotation
and thus shows great potential of adapting to new conditions. Representation
learning plays a critical role in unsupervised person ReID. In this work, we
propose a novel selective contrastive learning framework for unsupervised
feature learning. Specifically, different from traditional contrastive learning
strategies, we propose to use multiple positives and adaptively sampled
negatives for defining the contrastive loss, enabling to learn a feature
embedding model with stronger identity discriminative representation. Moreover,
we propose to jointly leverage global and local features to construct three
dynamic dictionaries, among which the global and local memory banks are used
for pairwise similarity computation and the mixture memory bank are used for
contrastive loss definition. Experimental results demonstrate the superiority
of our method in unsupervised person ReID compared with the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2010.07608" target="_blank">arXiv:2010.07608</a> [<a href="http://arxiv.org/pdf/2010.07608" target="_blank">pdf</a>]

<h2>A Deeper Look at the Layerwise Sparsity of Magnitude-based Pruning. (arXiv:2010.07611v1 [cs.LG])</h2>
<h3>Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, Jinwoo Shin</h3>
<p>Recent discoveries on neural network pruning reveal that, with a carefully
chosen layerwise sparsity, a simple magnitude-based pruning achieves
state-of-the-art tradeoff between sparsity and performance. However, without a
clear consensus on "how to choose," the layerwise sparsities are mostly
selected algorithm-by-algorithm, often resorting to handcrafted heuristics or
an extensive hyperparameter search. To fill this gap, we propose a novel
importance score for global pruning, coined layer-adaptive magnitude-based
pruning (LAMP) score; the score is a rescaled version of weight magnitude that
incorporates the model-level $\ell_2$ distortion incurred by pruning, and does
not require any hyperparameter tuning or heavy computation. Under diverse
datasets and models, LAMP consistently outperforms popular existing schemes for
layerwise sparsity selection. Furthermore, we observe that LAMP continues to
outperform baselines even in weight-rewinding setups, while the
connectivity-oriented layerwise sparsity (the strongest baseline overall)
performs worse than a simple global magnitude-based pruning in this case.
</p>
<a href="http://arxiv.org/abs/2010.07611" target="_blank">arXiv:2010.07611</a> [<a href="http://arxiv.org/pdf/2010.07611" target="_blank">pdf</a>]

<h2>THIN: THrowable Information Networks and Application for Facial Expression Recognition In The Wild. (arXiv:2010.07614v1 [cs.CV])</h2>
<h3>Estephe Arnaud, Arnaud Dapogny, Kevin Bailly</h3>
<p>For a number of tasks solved using deep learning techniques, an exogenous
variable can be identified such that (a) it heavily influences the appearance
of the different classes, and (b) an ideal classifier should be invariant to
this variable. An example of such exogenous variable is identity if facial
expression recognition (FER) is considered. In this paper, we propose a dual
exogenous/endogenous representation. The former captures the exogenous variable
whereas the second one models the task at hand (e.g. facial expression). We
design a prediction layer that uses a deep ensemble conditioned by the
exogenous representation. It employs a differential tree gate that learns an
adaptive weak predictor weighting, therefore modeling a partition of the
exogenous representation space, upon which the weak predictors specialize. This
layer explicitly models the dependency between the exogenous variable and the
predicted task (a). We also propose an exogenous dispelling loss to remove the
exogenous information from the endogenous representation, enforcing (b). Thus,
the exogenous information is used two times in a throwable fashion, first as a
conditioning variable for the target task, and second to create invariance
within the endogenous representation. We call this method THIN, standing for
THrowable Information Networks. We experimentally validate THIN in several
contexts where an exogenous information can be identified, such as digit
recognition under large rotations and shape recognition at multiple scales. We
also apply it to FER with identity as the exogenous variable. In particular, we
demonstrate that THIN significantly outperforms state-of-the-art approaches on
several challenging datasets.
</p>
<a href="http://arxiv.org/abs/2010.07614" target="_blank">arXiv:2010.07614</a> [<a href="http://arxiv.org/pdf/2010.07614" target="_blank">pdf</a>]

<h2>HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network. (arXiv:2010.07621v1 [cs.CV])</h2>
<h3>Pengcheng Yuan, Shufei Lin, Cheng Cui, Yuning Du, Ruoyu Guo, Dongliang He, Errui Ding, Shumin Han</h3>
<p>This paper addresses representational block named Hierarchical-Split Block,
which can be taken as a plug-and-play block to upgrade existing convolutional
neural networks, improves model performance significantly in a network.
Hierarchical-Split Block contains many hierarchical split and concatenate
connections within one single residual block. We find multi-scale features is
of great importance for numerous vision tasks. Moreover, Hierarchical-Split
block is very flexible and efficient, which provides a large space of potential
network architectures for different applications. In this work, we present a
common backbone based on Hierarchical-Split block for tasks: image
classification, object detection, instance segmentation and semantic image
segmentation/parsing. Our approach shows significant improvements over all
these core tasks in comparison with the baseline. As shown in Figure1, for
image classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1
accuracy with competitive latency on ImageNet-1k dataset. It also outperforms
most state-of-the-art models. The source code and models will be available on:
https://github.com/PaddlePaddle/PaddleClas
</p>
<a href="http://arxiv.org/abs/2010.07621" target="_blank">arXiv:2010.07621</a> [<a href="http://arxiv.org/pdf/2010.07621" target="_blank">pdf</a>]

<h2>Hierarchical Text Interaction for Rating Prediction. (arXiv:2010.07628v1 [cs.IR])</h2>
<h3>Jiahui Wen, Jingwei Ma, Hongkui Tu, Wei Yin, Jian Fang</h3>
<p>Traditional recommender systems encounter several challenges such as data
sparsity and unexplained recommendation. To address these challenges, many
works propose to exploit semantic information from review data. However, these
methods have two major limitations in terms of the way to model textual
features and capture textual interaction. For textual modeling, they simply
concatenate all the reviews of a user/item into a single review. However,
feature extraction at word/phrase level can violate the meaning of the original
reviews. As for textual interaction, they defer the interactions to the
prediction layer, making them fail to capture complex correlations between
users and items. To address those limitations, we propose a novel Hierarchical
Text Interaction model(HTI) for rating prediction. In HTI, we propose to model
low-level word semantics and high-level review representations hierarchically.
The hierarchy allows us to exploit textual features at different granularities.
To further capture complex user-item interactions, we propose to exploit
semantic correlations between each user-item pair at different hierarchies. At
word level, we propose an attention mechanism specialized to each user-item
pair, and capture the important words for representing each review. At review
level, we mutually propagate textual features between the user and item, and
capture the informative reviews. The aggregated review representations are
integrated into a collaborative filtering framework for rating prediction.
Experiments on five real-world datasets demonstrate that HTI outperforms
state-of-the-art models by a large margin. Further case studies provide a deep
insight into HTI's ability to capture semantic correlations at different levels
of granularities for rating prediction.
</p>
<a href="http://arxiv.org/abs/2010.07628" target="_blank">arXiv:2010.07628</a> [<a href="http://arxiv.org/pdf/2010.07628" target="_blank">pdf</a>]

<h2>The NeteaseGames System for Voice Conversion Challenge 2020 with Vector-quantization Variational Autoencoder and WaveNet. (arXiv:2010.07630v1 [cs.SD])</h2>
<h3>Haitong Zhang</h3>
<p>This paper presents the description of our submitted system for Voice
Conversion Challenge (VCC) 2020 with vector-quantization variational
autoencoder (VQ-VAE) with WaveNet as the decoder, i.e., VQ-VAE-WaveNet.
VQ-VAE-WaveNet is a nonparallel VAE-based voice conversion that reconstructs
the acoustic features along with separating the linguistic information with
speaker identity. The model is further improved with the WaveNet cycle as the
decoder to generate the high-quality speech waveform, since WaveNet, as an
autoregressive neural vocoder, has achieved the SoTA result of waveform
generation. In practice, our system can be developed with VCC 2020 dataset for
both Task 1 (intra-lingual) and Task 2 (cross-lingual). However, we only submit
our system for the intra-lingual voice conversion task. The results of VCC 2020
demonstrate that our system VQ-VAE-WaveNet achieves: 3.04 mean opinion score
(MOS) in naturalness and a 3.28 average score in similarity ( the speaker
similarity percentage (Sim) of 75.99%) for Task 1. The subjective evaluations
also reveal that our system gives top performance when no supervised learning
is involved. What's more, our system performs well in some objective
evaluations. Specifically, our system achieves an average score of 3.95 in
naturalness in automatic naturalness prediction and ranked the 6th and 8th,
respectively in ASV-based speaker similarity and spoofing countermeasures.
</p>
<a href="http://arxiv.org/abs/2010.07630" target="_blank">arXiv:2010.07630</a> [<a href="http://arxiv.org/pdf/2010.07630" target="_blank">pdf</a>]

<h2>Towards Reflectivity profile inversion through Artificial Neural Networks. (arXiv:2010.07634v1 [physics.comp-ph])</h2>
<h3>Juan Manuel Carmona-Loaiza</h3>
<p>The goal of Specular Neutron and X-ray Reflectometry is to infer materials
Scattering Length Density (SLD) profiles from experimental reflectivity curves.
This paper focuses on investigating an original approach to the ill-posed
non-invertible problem which involves the use of Artificial Neural Networks
(ANN). In particular, the numerical experiments described here deal with large
data sets of simulated reflectivity curves and SLD profiles, and aim to assess
the applicability of Data Science and Machine Learning technology to the
analysis of data generated at large scale facilities. It is demonstrated that,
under certain circumstances, properly trained Deep Neural Networks are capable
of correctly recovering plausible SLD profiles when presented with
never-seen-before simulated reflectivity curves. When the necessary conditions
are met, a proper implementation of the described approach would offer two main
advantages over traditional fitting methods when dealing with real experiments,
namely, 1. no prior assumptions about the sample physical model are required
and 2. the times-to-solution are shrank by orders of magnitude, enabling faster
batch analyses for large datasets.
</p>
<a href="http://arxiv.org/abs/2010.07634" target="_blank">arXiv:2010.07634</a> [<a href="http://arxiv.org/pdf/2010.07634" target="_blank">pdf</a>]

<h2>Combining Scatter Transform and Deep Neural Networks for Multilabel Electrocardiogram Signal Classification. (arXiv:2010.07639v1 [eess.SP])</h2>
<h3>Maximilian P Oppelt, Maximilian Riehl, Felix P Kemeth, Jan Steffan</h3>
<p>An essential part for the accurate classification of electrocardiogram (ECG)
signals is the extraction of informative yet general features, which are able
to discriminate diseases. Cardiovascular abnormalities manifest themselves in
features on different time scales: small scale morphological features, such as
missing P-waves, as well as rhythmical features apparent on heart rate scales.
For this reason we incorporate a variant of the complex wavelet transform,
called a scatter transform, in a deep residual neural network (ResNet). The
former has the advantage of being derived from theory, making it well behaved
under certain transformations of the input. The latter has proven useful in ECG
classification, allowing feature extraction and classification to be learned in
an end-to-end manner. Through the incorporation of trainable layers in between
scatter transforms, the model gains the ability to combine information from
different channels, yielding more informative features for the classification
task and adapting them to the specific domain. For evaluation, we submitted our
model in the official phase in the PhysioNet/Computing in Cardiology Challenge
2020. Our (Team Triage) approach achieved a challenge validation score of
0.640, and full test score of 0.485, placing us 4th out of 41 in the official
ranking.
</p>
<a href="http://arxiv.org/abs/2010.07639" target="_blank">arXiv:2010.07639</a> [<a href="http://arxiv.org/pdf/2010.07639" target="_blank">pdf</a>]

<h2>Empty Cities: a Dynamic-Object-Invariant Space for Visual SLAM. (arXiv:2010.07646v1 [cs.CV])</h2>
<h3>Berta Bescos, Cesar Cadena, Jose Neira</h3>
<p>In this paper we present a data-driven approach to obtain the static image of
a scene, eliminating dynamic objects that might have been present at the time
of traversing the scene with a camera. The general objective is to improve
vision-based localization and mapping tasks in dynamic environments, where the
presence (or absence) of different dynamic objects in different moments makes
these tasks less robust. We introduce an end-to-end deep learning framework to
turn images of an urban environment that include dynamic content, such as
vehicles or pedestrians, into realistic static frames suitable for localization
and mapping. This objective faces two main challenges: detecting the dynamic
objects, and inpainting the static occluded back-ground. The first challenge is
addressed by the use of a convolutional network that learns a multi-class
semantic segmentation of the image. The second challenge is approached with a
generative adversarial model that, taking as input the original dynamic image
and the computed dynamic/static binary mask, is capable of generating the final
static image. This framework makes use of two new losses, one based on image
steganalysis techniques, useful to improve the inpainting quality, and another
one based on ORB features, designed to enhance feature matching between real
and hallucinated image regions. To validate our approach, we perform an
extensive evaluation on different tasks that are affected by dynamic entities,
i.e., visual odometry, place recognition and multi-view stereo, with the
hallucinated images. Code has been made available on
https://github.com/bertabescos/EmptyCities_SLAM.
</p>
<a href="http://arxiv.org/abs/2010.07646" target="_blank">arXiv:2010.07646</a> [<a href="http://arxiv.org/pdf/2010.07646" target="_blank">pdf</a>]

<h2>A Graph Neural Network based approach for detecting Suspicious Users on Online Social Media. (arXiv:2010.07647v1 [cs.AI])</h2>
<h3>Shakshi Sharma, Rajesh Sharma</h3>
<p>Online Social Media platforms (such as Twitter and Facebook) are extensively
used for spreading the news to a wider public effortlessly at a rapid pace.
However, now a days these platforms are also used with an aim of spreading
rumors and fake news to a large audience in a short time span that can cause
panic, fear, and financial loss to society. Thus, it is important to detect and
control these rumors before it spreads to the masses. One way to control the
spread of these rumors is by identifying possible suspicious users who are
often involved in spreading the rumors. Our basic assumption is that the users
who are often involved in spreading rumors are more likely to be suspicious in
contrast to the users whose involvement in spreading rumors are less. This is
due to the fact that sometimes, users may posts the rumor tweets by accident.
In this paper, we use PHEME rumor tweet dataset which contains rumor and
non-rumor tweets information on five incidents, that is, i) Charlie hebdo,
ii)German wings crash, iii)Ottawa shooting, iv)Sydney siege, and v)Ferguson. We
transform this rumor tweets dataset into suspicious users dataset before
leveraging Graph Neural Network (GNN) based approach for identifying suspicious
users. Specifically, we explore Graph Convolutional Network (GCN),which is a
type of GNN, for identifying suspicious users and then we compare GCN results
with the other three approaches which act as baseline approaches: SVM, RF and
LSTM based deep learning architecture. Extensive experiments performed on
real-world dataset, where we achieve up to 0.864 value for F1-Score and 0.720
value for AUC ROC, shows the effectiveness of GNN based approach for
identifying suspicious users.
</p>
<a href="http://arxiv.org/abs/2010.07647" target="_blank">arXiv:2010.07647</a> [<a href="http://arxiv.org/pdf/2010.07647" target="_blank">pdf</a>]

<h2>Altruist: Argumentative Explanations through Local Interpretations of Predictive Models. (arXiv:2010.07650v1 [cs.LG])</h2>
<h3>Ioannis Mollas, Nick Bassiliades, Grigorios Tsoumakas</h3>
<p>Interpretable machine learning is an emerging field providing solutions on
acquiring insights into machine learning models' rationale. It has been put in
the map of machine learning by suggesting ways to tackle key ethical and
societal issues. However, existing techniques of interpretable machine learning
are far from being comprehensible and explainable to the end user. Another key
issue in this field is the lack of evaluation and selection criteria, making it
difficult for the end user to choose the most appropriate interpretation
technique for its use. In this study, we introduce a meta-explanation
methodology that will provide truthful interpretations, in terms of feature
importance, to the end user through argumentation. At the same time, this
methodology can be used as an evaluation or selection tool for multiple
interpretation techniques based on feature importance.
</p>
<a href="http://arxiv.org/abs/2010.07650" target="_blank">arXiv:2010.07650</a> [<a href="http://arxiv.org/pdf/2010.07650" target="_blank">pdf</a>]

<h2>A Deep Drift-Diffusion Model for Image Aesthetic Score Distribution Prediction. (arXiv:2010.07661v1 [cs.CV])</h2>
<h3>Xin Jin, Xiqiao Li, Heng Huang, Xiaodong Li, Xinghui Zhou</h3>
<p>The task of aesthetic quality assessment is complicated due to its
subjectivity. In recent years, the target representation of image aesthetic
quality has changed from a one-dimensional binary classification label or
numerical score to a multi-dimensional score distribution. According to current
methods, the ground truth score distributions are straightforwardly regressed.
However, the subjectivity of aesthetics is not taken into account, that is to
say, the psychological processes of human beings are not taken into
consideration, which limits the performance of the task. In this paper, we
propose a Deep Drift-Diffusion (DDD) model inspired by psychologists to predict
aesthetic score distribution from images. The DDD model can describe the
psychological process of aesthetic perception instead of traditional modeling
of the results of assessment. We use deep convolution neural networks to
regress the parameters of the drift-diffusion model. The experimental results
in large scale aesthetic image datasets reveal that our novel DDD model is
simple but efficient, which outperforms the state-of-the-art methods in
aesthetic score distribution prediction. Besides, different psychological
processes can also be predicted by our model.
</p>
<a href="http://arxiv.org/abs/2010.07661" target="_blank">arXiv:2010.07661</a> [<a href="http://arxiv.org/pdf/2010.07661" target="_blank">pdf</a>]

<h2>Integrating Coarse Granularity Part-level Features with Supervised Global-level Features for Person Re-identification. (arXiv:2010.07675v1 [cs.CV])</h2>
<h3>Xiaofei Mao, Jiahao Cao, Dongfang Li, Xia Jia, Qingfang Zheng</h3>
<p>Holistic person re-identification (Re-ID) and partial person
re-identification have achieved great progress respectively in recent years.
However, scenarios in reality often include both holistic and partial
pedestrian images, which makes single holistic or partial person Re-ID hard to
work. In this paper, we propose a robust coarse granularity part-level person
Re-ID network (CGPN), which not only extracts robust regional level body
features, but also integrates supervised global features for both holistic and
partial person images. CGPN gains two-fold benefit toward higher accuracy for
person Re-ID. On one hand, CGPN learns to extract effective body part features
for both holistic and partial person images. On the other hand, compared with
extracting global features directly by backbone network, CGPN learns to extract
more accurate global features with a supervision strategy. The single model
trained on three Re-ID datasets including Market-1501, DukeMTMC-reID and CUHK03
achieves state-of-the-art performances and outperforms any existing approaches.
Especially on CUHK03, which is the most challenging dataset for person Re-ID,
in single query mode, we obtain a top result of Rank-1/mAP=87.1\%/83.6\% with
this method without re-ranking, outperforming the current best method by
+7.0\%/+6.7\%.
</p>
<a href="http://arxiv.org/abs/2010.07675" target="_blank">arXiv:2010.07675</a> [<a href="http://arxiv.org/pdf/2010.07675" target="_blank">pdf</a>]

<h2>Linking average- and worst-case perturbation robustness via class selectivity and dimensionality. (arXiv:2010.07693v1 [cs.LG])</h2>
<h3>Matthew L. Leavitt, Ari Morcos</h3>
<p>Representational sparsity is known to affect robustness to input
perturbations in deep neural networks (DNNs), but less is known about how the
semantic content of representations affects robustness. Class selectivity-the
variability of a unit's responses across data classes or dimensions-is one way
of quantifying the sparsity of semantic representations. Given recent evidence
that class selectivity may not be necessary for, and can even impair
generalization, we investigated whether it also confers robustness (or
vulnerability) to perturbations of input data. We found that class selectivity
leads to increased vulnerability to average-case (naturalistic) perturbations
in ResNet18 and ResNet20, as measured using Tiny ImageNetC and CIFAR10C,
respectively. Networks regularized to have lower levels of class selectivity
are more robust to average-case perturbations, while networks with higher class
selectivity are more vulnerable. In contrast, we found that class selectivity
increases robustness to worst-case (i.e. white box adversarial) perturbations,
suggesting that while decreasing class selectivity is helpful for average-case
robustness, it is harmful for worst-case robustness. To explain this
difference, we studied the dimensionality of the networks' representations: we
found that the dimensionality of early-layer representations is inversely
proportional to a network's class selectivity, and that adversarial samples
cause a larger increase in early-layer dimensionality than corrupted samples.
We also found that the input-unit gradient was more variable across samples and
units in high-selectivity networks compared to low-selectivity networks. These
results lead to the conclusion that units participate more consistently in
low-selectivity regimes compared to high-selectivity regimes, effectively
creating a larger attack surface and hence vulnerability to worst-case
perturbations.
</p>
<a href="http://arxiv.org/abs/2010.07693" target="_blank">arXiv:2010.07693</a> [<a href="http://arxiv.org/pdf/2010.07693" target="_blank">pdf</a>]

<h2>Workload-Aware Systems and Interfaces for Cognitive Augmentation. (arXiv:2010.07703v1 [cs.HC])</h2>
<h3>Thomas Kosch</h3>
<p>In today's society, our cognition is constantly influenced by information
intake, attention switching, and task interruptions. This increases the
difficulty of a given task, adding to the existing workload and leading to
compromised cognitive performances. The human body expresses the use of
cognitive resources through physiological responses when confronted with a
plethora of cognitive workload. This temporarily mobilizes additional resources
to deal with the workload at the cost of accelerated mental exhaustion. We
predict that recent developments in physiological sensing will increasingly
create user interfaces that are aware of the user's cognitive capacities, hence
able to intervene when high or low states of cognitive workload are detected.
Subsequently, we investigate suitable feedback modalities in a user-centric
design process which are desirable for cognitive assistance. We then
investigate different physiological sensing modalities to enable suitable
real-time assessments of cognitive workload. We provide evidence that the human
brain and eye gaze are sensitive to fluctuations in cognitive resting states.
We show that electroencephalography and eye tracking are reliable modalities to
assess mental workload during user interface operation. In the end, we present
applications that regulate cognitive workload in home and work setting,
investigate how cognitive workload can be visualized to the user, and show how
cognitive workload measurements can be used to predict the efficiency of
information intake through reading interfaces. Finally, we present our vision
of future workload-aware interfaces. Previous interfaces were limited in their
ability to utilize cognitive workload for user interaction. Together with the
collected data sets, this thesis paves the way for methodical and technical
tools that integrate workload-awareness as a factor for context-aware systems.
</p>
<a href="http://arxiv.org/abs/2010.07703" target="_blank">arXiv:2010.07703</a> [<a href="http://arxiv.org/pdf/2010.07703" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video with Applications for Virtual Reality. (arXiv:2010.07704v1 [cs.CV])</h2>
<h3>Alisha Sharma, Ryan Nett, Jonathan Ventura</h3>
<p>We introduce a convolutional neural network model for unsupervised learning
of depth and ego-motion from cylindrical panoramic video. Panoramic depth
estimation is an important technology for applications such as virtual reality,
3D modeling, and autonomous robotic navigation. In contrast to previous
approaches for applying convolutional neural networks to panoramic imagery, we
use the cylindrical panoramic projection which allows for the use of the
traditional CNN layers such as convolutional filters and max pooling without
modification. Our evaluation of synthetic and real data shows that unsupervised
learning of depth and ego-motion on cylindrical panoramic images can produce
high-quality depth maps and that an increased field-of-view improves ego-motion
estimation accuracy. We create two new datasets to evaluate our approach: a
synthetic dataset created using the CARLA simulator, and Headcam, a novel
dataset of panoramic video collected from a helmet-mounted camera while biking
in an urban setting. We also apply our network to the problem of converting
monocular panoramas to stereo panoramas.
</p>
<a href="http://arxiv.org/abs/2010.07704" target="_blank">arXiv:2010.07704</a> [<a href="http://arxiv.org/pdf/2010.07704" target="_blank">pdf</a>]

<h2>Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains. (arXiv:2010.07717v1 [cs.CL])</h2>
<h3>Weijie Yu, Chen Xu, Jun Xu, Liang Pang, Xiaopeng Gao, Xiaozhao Wang, Ji-Rong Wen</h3>
<p>One approach to matching texts from asymmetrical domains is projecting the
input sequences into a common semantic space as feature vectors upon which the
matching function can be readily defined and learned. In real-world matching
practices, it is often observed that with the training goes on, the feature
vectors projected from different domains tend to be indistinguishable. The
phenomenon, however, is often overlooked in existing matching models. As a
result, the feature vectors are constructed without any regularization, which
inevitably increases the difficulty of learning the downstream matching
functions. In this paper, we propose a novel match method tailored for text
matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein
distance-based regularizer is defined to regularize the features vectors
projected from different domains. As a result, the method enforces the feature
projection function to generate vectors such that those correspond to different
domains cannot be easily discriminated. The training process of WD-Match
amounts to a game that minimizes the matching loss regularized by the
Wasserstein distance. WD-Match can be used to improve different text matching
methods, by using the method as its underlying matching model. Four popular
text matching methods have been exploited in the paper. Experimental results
based on four publicly available benchmarks showed that WD-Match consistently
outperformed the underlying methods and the baselines.
</p>
<a href="http://arxiv.org/abs/2010.07717" target="_blank">arXiv:2010.07717</a> [<a href="http://arxiv.org/pdf/2010.07717" target="_blank">pdf</a>]

<h2>Improving Neural Network Verification through Spurious Region Guided Refinement. (arXiv:2010.07722v1 [cs.AI])</h2>
<h3>Pengfei Yang, Renjue Li, Jianlin Li, Cheng-Chao Huang, Jingyi Wang, Jun Sun, Bai Xue, Lijun Zhang</h3>
<p>We propose a spurious region guided refinement approach for robustness
verification of deep neural networks. Our method starts with applying the
DeepPoly abstract domain to analyze the network. If the robustness property
cannot be verified, the result is inconclusive. Due to the over-approximation,
the computed region in the abstraction may be spurious in the sense that it
does not contain any true counterexample. Our goal is to identify such spurious
regions and use them to guide the abstraction refinement. The core idea is to
make use of the obtained constraints of the abstraction to infer new bounds for
the neurons. This is achieved by linear programming techniques. With the new
bounds, we iteratively apply DeepPoly, aiming to eliminate spurious regions. We
have implemented our approach in a prototypical tool DeepSRGR. Experimental
results show that a large amount of regions can be identified as spurious, and
as a result, the precision of DeepPoly can be significantly improved. As a side
contribution, we show that our approach can be applied to verify quantitative
robustness properties.
</p>
<a href="http://arxiv.org/abs/2010.07722" target="_blank">arXiv:2010.07722</a> [<a href="http://arxiv.org/pdf/2010.07722" target="_blank">pdf</a>]

<h2>LiteDepthwiseNet: An Extreme Lightweight Network for Hyperspectral Image Classification. (arXiv:2010.07726v1 [eess.IV])</h2>
<h3>Benlei Cui, XueMei Dong, Qiaoqiao Zhan, Jiangtao Peng, Weiwei Sun</h3>
<p>Deep learning methods have shown considerable potential for hyperspectral
image (HSI) classification, which can achieve high accuracy compared with
traditional methods. However, they often need a large number of training
samples and have a lot of parameters and high computational overhead. To solve
these problems, this paper proposes a new network architecture,
LiteDepthwiseNet, for HSI classification. Based on 3D depthwise convolution,
LiteDepthwiseNet can decompose standard convolution into depthwise convolution
and pointwise convolution, which can achieve high classification performance
with minimal parameters. Moreover, we remove the ReLU layer and Batch
Normalization layer in the original 3D depthwise convolution, which
significantly improves the overfitting phenomenon of the model on small sized
datasets. In addition, focal loss is used as the loss function to improve the
model's attention on difficult samples and unbalanced data, and its training
performance is significantly better than that of cross-entropy loss or balanced
cross-entropy loss. Experiment results on three benchmark hyperspectral
datasets show that LiteDepthwiseNet achieves state-of-the-art performance with
a very small number of parameters and low computational cost.
</p>
<a href="http://arxiv.org/abs/2010.07726" target="_blank">arXiv:2010.07726</a> [<a href="http://arxiv.org/pdf/2010.07726" target="_blank">pdf</a>]

<h2>Teaching Quantum Computing through a Practical Software-driven Approach: Experience Report. (arXiv:2010.07729v1 [physics.ed-ph])</h2>
<h3>Mariia Mykhailova, Krysta M. Svore</h3>
<p>Quantum computing harnesses quantum laws of nature to enable new types of
algorithms, not efficiently possible on traditional computers, that may lead to
breakthroughs in crucial areas like materials science and chemistry. There is
rapidly growing demand for a quantum workforce educated in the basics of
quantum computing, in particular in quantum programming. However, there are few
offerings for non-specialists and little information on best practices for
training computer science and engineering students.

In this report we describe our experience teaching an undergraduate course on
quantum computing using a practical, software-driven approach. We centered our
course around teaching quantum algorithms through hands-on programming,
reducing the significance of traditional written assignments and relying
instead on self-paced programming exercises ("Quantum Katas"), a variety of
programming assignments, and a final project. We observed that the programming
sections of the course helped students internalize theoretical material
presented during the lectures. In the survey results, students indicated that
the programming exercises and the final project contributed the most to their
learning process.

We describe the motivation for centering the course around quantum
programming, discuss major artifacts used in this course, and present our
lessons learned and best practices for a future improved course offering. We
hope that our experience will help guide instructors who want to adopt a
practical approach to teaching quantum computing and will enable more
undergraduate programs to offer quantum programming as an elective.
</p>
<a href="http://arxiv.org/abs/2010.07729" target="_blank">arXiv:2010.07729</a> [<a href="http://arxiv.org/pdf/2010.07729" target="_blank">pdf</a>]

<h2>R-GAP: Recursive Gradient Attack on Privacy. (arXiv:2010.07733v1 [cs.LG])</h2>
<h3>Junyi Zhu, Matthew Blaschko</h3>
<p>Federated learning frameworks have been regarded as a promising approach to
break the dilemma between demands on privacy and the promise of learning from
large collections of distributed data. Many such frameworks only ask
collaborators to share their local update of a common model, i.e. gradients
with respect to locally stored data, instead of exposing their raw data to
other collaborators. However, recent optimization-based gradient attacks show
that raw data can often be accurately recovered from gradients. It has been
shown that minimizing the Euclidean distance between true gradients and those
calculated from estimated data is often effective in fully recovering private
data. However, there is a fundamental lack of theoretical understanding of how
and when gradients can lead to unique recovery of original data. Our research
fills this gap by providing a closed-form recursive procedure to recover data
from gradients in deep neural networks. We demonstrate that gradient attacks
consist of recursively solving a sequence of systems of linear equations.
Furthermore, our closed-form approach works as well as or even better than
optimization-based approaches at a fraction of the computation, we name it
Recursive Gradient Attack on Privacy (R-GAP). Additionally, we propose a rank
analysis method, which can be used to estimate a network architecture's risk of
a gradient attack. Experimental results demonstrate the validity of the
closed-form attack and rank analysis, while demonstrating its superior
computational properties and lack of susceptibility to local optima vis a vis
optimization-based attacks. Source code is available for download from
https://github.com/JunyiZhu-AI/R-GAP.
</p>
<a href="http://arxiv.org/abs/2010.07733" target="_blank">arXiv:2010.07733</a> [<a href="http://arxiv.org/pdf/2010.07733" target="_blank">pdf</a>]

<h2>Self-training for Few-shot Transfer Across Extreme Task Differences. (arXiv:2010.07734v1 [cs.CV])</h2>
<h3>Cheng Perng Phoo, Bharath Hariharan</h3>
<p>All few-shot learning techniques must be pre-trained on a large, labeled
"base dataset". In problem domains where such large labeled datasets are not
available for pre-training (e.g., X-ray images), one must resort to
pre-training in a different "source" problem domain (e.g., ImageNet), which can
be very different from the desired target task. Traditional few-shot and
transfer learning techniques fail in the presence of such extreme differences
between the source and target tasks. In this paper, we present a simple and
effective solution to tackle this extreme domain gap: self-training a source
domain representation on unlabeled data from the target domain. We show that
this improves one-shot performance on the target domain by 2.9 points on
average on a challenging benchmark with multiple domains.
</p>
<a href="http://arxiv.org/abs/2010.07734" target="_blank">arXiv:2010.07734</a> [<a href="http://arxiv.org/pdf/2010.07734" target="_blank">pdf</a>]

<h2>Conditional Level Generation and Game Blending. (arXiv:2010.07735v1 [cs.LG])</h2>
<h3>Anurag Sarkar, Zhihan Yang, Seth Cooper</h3>
<p>Prior research has shown variational autoencoders (VAEs) to be useful for
generating and blending game levels by learning latent representations of
existing level data. We build on such models by exploring the level design
affordances and applications enabled by conditional VAEs (CVAEs). CVAEs augment
VAEs by allowing them to be trained using labeled data, thus enabling outputs
to be generated conditioned on some input. We studied how increased control in
the level generation process and the ability to produce desired outputs via
training on labeled game level data could build on prior PCGML methods. Through
our results of training CVAEs on levels from Super Mario Bros., Kid Icarus and
Mega Man, we show that such models can assist in level design by generating
levels with desired level elements and patterns as well as producing blended
levels with desired combinations of games.
</p>
<a href="http://arxiv.org/abs/2010.07735" target="_blank">arXiv:2010.07735</a> [<a href="http://arxiv.org/pdf/2010.07735" target="_blank">pdf</a>]

<h2>Do's and Don'ts for Human and Digital Worker Integration. (arXiv:2010.07738v1 [cs.AI])</h2>
<h3>Vinod Muthusamy, Merve Unuvar, Hagen V&#xf6;lzer, Justin D. Weisz</h3>
<p>Robotic process automation (RPA) and its next evolutionary stage, intelligent
process automation, promise to drive improvements in efficiencies and process
outcomes. However, how can business leaders evaluate how to integrate
intelligent automation into business processes? What is an appropriate division
of labor between humans and machines? How should combined human-AI teams be
evaluated? For RPA, often the human labor cost and the robotic labor cost are
directly compared to make an automation decision. In this position paper, we
argue for a broader view that incorporates the potential for multiple levels of
autonomy and human involvement, as well as a wider range of metrics beyond
productivity when integrating digital workers into a business process
</p>
<a href="http://arxiv.org/abs/2010.07738" target="_blank">arXiv:2010.07738</a> [<a href="http://arxiv.org/pdf/2010.07738" target="_blank">pdf</a>]

<h2>Music Classification in MIDI Format based on LSTM Mdel. (arXiv:2010.07739v1 [cs.SD])</h2>
<h3>Yiting Xia, Yiwei Jiang, Tao Ye</h3>
<p>Music classification between music made by AI or human composers can be done
by deep learning networks. We first transformed music samples in midi format to
natural language sequences, then classified these samples by mLSTM
(multiplicative Long Short Term Memory) + logistic regression. The accuracy of
the result evaluated by 10-fold cross validation can reach 90%. Our work
indicates that music generated by AI and human composers do have different
characteristics, which can be learned by deep learning networks.
</p>
<a href="http://arxiv.org/abs/2010.07739" target="_blank">arXiv:2010.07739</a> [<a href="http://arxiv.org/pdf/2010.07739" target="_blank">pdf</a>]

<h2>A Theory of Hyperbolic Prototype Learning. (arXiv:2010.07744v1 [stat.ML])</h2>
<h3>Martin Keller-Ressel</h3>
<p>We introduce Hyperbolic Prototype Learning, a type of supervised learning,
where class labels are represented by ideal points (points at infinity) in
hyperbolic space. Learning is achieved by minimizing the 'penalized Busemann
loss', a new loss function based on the Busemann function of hyperbolic
geometry. We discuss several theoretical features of this setup. In particular,
Hyperbolic Prototype Learning becomes equivalent to logistic regression in the
one-dimensional case.
</p>
<a href="http://arxiv.org/abs/2010.07744" target="_blank">arXiv:2010.07744</a> [<a href="http://arxiv.org/pdf/2010.07744" target="_blank">pdf</a>]

<h2>Recurrent convolutional neural network for the surrogate modeling of subsurface flow simulation. (arXiv:2010.07747v1 [cs.LG])</h2>
<h3>Hyung Jun Yang, Timothy Yeo, Jaewoo An</h3>
<p>The quantification of uncertainty on fluid flow in porous media is often
hampered by multi-scale heterogeneity and insufficient site characterization.
Monte-Carlo simulation (MCS), which runs numerical simulations for a large
number of realization of input parameters , becomes infeasible when simulation
cost is expensive or the degree of uncertainty is large. Many
deep-neural-network-based methods are developed in order to replace the
numerical flow simulation, but previous studies focused only on generating
several snapshots of outputs at the fixed time steps, and lack to reflect the
time dependent property of simulation data. Recently, the convolutional long
short term memory (ConvLSTM) is utilized to deal with time series image data.
Here, we propose to combine SegNet with ConvLSTM layers for the surrogate
modeling of numerical flow simulation. The results show that the proposed
method improves the performance of SegNet based surrogate model remarkably when
the output of the simulation is time series data.
</p>
<a href="http://arxiv.org/abs/2010.07747" target="_blank">arXiv:2010.07747</a> [<a href="http://arxiv.org/pdf/2010.07747" target="_blank">pdf</a>]

<h2>EnCoD: Distinguishing Compressed and Encrypted File Fragments. (arXiv:2010.07754v1 [cs.CR])</h2>
<h3>Fabio De Gaspari, Dorjan Hitaj, Giulio Pagnotta, Lorenzo De Carli, Luigi V. Mancini</h3>
<p>Reliable identification of encrypted file fragments is a requirement for
several security applications, including ransomware detection, digital
forensics, and traffic analysis. A popular approach consists of estimating high
entropy as a proxy for randomness. However, many modern content types (e.g.
office documents, media files, etc.) are highly compressed for storage and
transmission efficiency. Compression algorithms also output high-entropy data,
thus reducing the accuracy of entropy-based encryption detectors. Over the
years, a variety of approaches have been proposed to distinguish encrypted file
fragments from high-entropy compressed fragments. However, these approaches are
typically only evaluated over a few, select data types and fragment sizes,
which makes a fair assessment of their practical applicability impossible. This
paper aims to close this gap by comparing existing statistical tests on a
large, standardized dataset. Our results show that current approaches cannot
reliably tell apart encryption and compression, even for large fragment sizes.
To address this issue, we design EnCoD, a learning-based classifier which can
reliably distinguish compressed and encrypted data, starting with fragments as
small as 512 bytes. We evaluate EnCoD against current approaches over a large
dataset of different data types, showing that it outperforms current
state-of-the-art for most considered fragment sizes and data types.
</p>
<a href="http://arxiv.org/abs/2010.07754" target="_blank">arXiv:2010.07754</a> [<a href="http://arxiv.org/pdf/2010.07754" target="_blank">pdf</a>]

<h2>A Transformer Based Pitch Sequence Autoencoder with MIDI Augmentation. (arXiv:2010.07758v1 [cs.SD])</h2>
<h3>Mingshuo Ding, Yinghao Ma</h3>
<p>Algorithms based on deep learning have been widely put forward for automatic
music generated. However, few objective approaches have been proposed to assess
whether a melody was created by automatons or Homo sapiens. Conference of Sound
and Music Technology (2020) provides us a great opportunity to cope with the
problem. In this paper, a masked language model based on ALBERT trained with
AI-composed single-track MIDI is demonstrated for composers classification
tasks. Besides, music tune transposition and MIDI sequence truncation is
applied for data augments. To prevent from over-fitting, a refined loss
function is proposed and the amount of parameters is reduced. This work
provides a new approach to tackle the problem on obtaining features from tiny
dataset which is common in music signal analysis and deserve more attention.
</p>
<a href="http://arxiv.org/abs/2010.07758" target="_blank">arXiv:2010.07758</a> [<a href="http://arxiv.org/pdf/2010.07758" target="_blank">pdf</a>]

<h2>alurity, a toolbox for robot cybersecurity. (arXiv:2010.07759v1 [cs.RO])</h2>
<h3>V&#xed;ctor Mayoral-Vilches, Irati Abad-Fern&#xe1;ndez, Martin Pinzger, Stefan Rass, Bernhard Dieber, Alcino Cunha, Francisco J. Rodr&#xed;guez-Lera, Giovanni Lacava, Angelica Marotta, Fabio Martinelli, Endika Gil-Uriarte</h3>
<p>The reuse of technologies and inherent complexity of most robotic systems is
increasingly leading to robots with wide attack surfaces and a variety of
potential vulnerabilities. Given their growing presence in public environments,
security research is increasingly becoming more important than in any other
area, specially due to the safety implications that robot vulnerabilities could
cause on humans. We argue that security triage in robotics is still immature
and that new tools must be developed to accelerate the
testing-triage-exploitation cycle, necessary for prioritizing and accelerating
the mitigation of flaws.

The present work tackles the current lack of offensive cybersecurity research
in robotics by presenting a toolbox and the results obtained with it through
several use cases conducted over a year period. We propose a modular and
composable toolbox for robot cybersecurity: alurity. By ensuring that both
roboticists and security researchers working on a project have a common,
consistent and easily reproducible development environment, alurity aims to
facilitate the cybersecurity research and the collaboration across teams.
</p>
<a href="http://arxiv.org/abs/2010.07759" target="_blank">arXiv:2010.07759</a> [<a href="http://arxiv.org/pdf/2010.07759" target="_blank">pdf</a>]

<h2>On Convergence of Nearest Neighbor Classifiers over Feature Transformations. (arXiv:2010.07765v1 [cs.LG])</h2>
<h3>Luka Rimanic, Cedric Renggli, Bo Li, Ce Zhang</h3>
<p>The k-Nearest Neighbors (kNN) classifier is a fundamental non-parametric
machine learning algorithm. However, it is well known that it suffers from the
curse of dimensionality, which is why in practice one often applies a kNN
classifier on top of a (pre-trained) feature transformation. From a theoretical
perspective, most, if not all theoretical results aimed at understanding the
kNN classifier are derived for the raw feature space. This leads to an emerging
gap between our theoretical understanding of kNN and its practical
applications. In this paper, we take a first step towards bridging this gap. We
provide a novel analysis on the convergence rates of a kNN classifier over
transformed features. This analysis requires in-depth understanding of the
properties that connect both the transformed space and the raw feature space.
More precisely, we build our convergence bound upon two key properties of the
transformed space: (1) safety -- how well can one recover the raw posterior
from the transformed space, and (2) smoothness -- how complex this recovery
function is. Based on our result, we are able to explain why some (pre-trained)
feature transformations are better suited for a kNN classifier than other. We
empirically validate that both properties have an impact on the kNN convergence
on 30 feature transformations with 6 benchmark datasets spanning from the
vision to the text domain.
</p>
<a href="http://arxiv.org/abs/2010.07765" target="_blank">arXiv:2010.07765</a> [<a href="http://arxiv.org/pdf/2010.07765" target="_blank">pdf</a>]

<h2>The Benefit of Distraction: Denoising Remote Vitals Measurements using Inverse Attention. (arXiv:2010.07770v1 [eess.IV])</h2>
<h3>Ewa Nowara, Daniel McDuff, Ashok Veeraraghavan</h3>
<p>Attention is a powerful concept in computer vision. End-to-end networks that
learn to focus selectively on regions of an image or video often perform
strongly. However, other image regions, while not necessarily containing the
signal of interest, may contain useful context. We present an approach that
exploits the idea that statistics of noise may be shared between the regions
that contain the signal of interest and those that do not. Our technique uses
the inverse of an attention mask to generate a noise estimate that is then used
to denoise temporal observations. We apply this to the task of camera-based
physiological measurement. A convolutional attention network is used to learn
which regions of a video contain the physiological signal and generate a
preliminary estimate. A noise estimate is obtained by using the pixel
intensities in the inverse regions of the learned attention mask, this in turn
is used to refine the estimate of the physiological signal. We perform
experiments on two large benchmark datasets and show that this approach
produces state-of-the-art results, increasing the signal-to-noise ratio by up
to 5.8 dB, reducing heart rate and breathing rate estimation error by as much
as 30%, recovering subtle pulse waveform dynamics, and generalizing from RGB to
NIR videos without retraining.
</p>
<a href="http://arxiv.org/abs/2010.07770" target="_blank">arXiv:2010.07770</a> [<a href="http://arxiv.org/pdf/2010.07770" target="_blank">pdf</a>]

<h2>A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning. (arXiv:2010.07777v1 [cs.LG])</h2>
<h3>Arnu Pretorius, Scott Cameron, Elan van Biljon, Tom Makkink, Shahil Mawjee, Jeremy du Plessis, Jonathan Shock, Alexandre Laterre, Karim Beguir</h3>
<p>Multi-agent reinforcement learning has recently shown great promise as an
approach to networked system control. Arguably, one of the most difficult and
important tasks for which large scale networked system control is applicable is
common-pool resource management. Crucial common-pool resources include arable
land, fresh water, wetlands, wildlife, fish stock, forests and the atmosphere,
of which proper management is related to some of society's greatest challenges
such as food security, inequality and climate change. Here we take inspiration
from a recent research program investigating the game-theoretic incentives of
humans in social dilemma situations such as the well-known tragedy of the
commons. However, instead of focusing on biologically evolved human-like
agents, our concern is rather to better understand the learning and operating
behaviour of engineered networked systems comprising general-purpose
reinforcement learning agents, subject only to nonbiological constraints such
as memory, computation and communication bandwidth. Harnessing tools from
empirical game-theoretic analysis, we analyse the differences in resulting
solution concepts that stem from employing different information structures in
the design of networked multi-agent systems. These information structures
pertain to the type of information shared between agents as well as the
employed communication protocol and network topology. Our analysis contributes
new insights into the consequences associated with certain design choices and
provides an additional dimension of comparison between systems beyond
efficiency, robustness, scalability and mean control performance.
</p>
<a href="http://arxiv.org/abs/2010.07777" target="_blank">arXiv:2010.07777</a> [<a href="http://arxiv.org/pdf/2010.07777" target="_blank">pdf</a>]

<h2>Local Differentially Private Regret Minimization in Reinforcement Learning. (arXiv:2010.07778v1 [cs.LG])</h2>
<h3>Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, Matteo Pirotta</h3>
<p>Reinforcement learning algorithms are widely used in domains where it is
desirable to provide a personalized service. In these domains it is common that
user data contains sensitive information that needs to be protected from third
parties. Motivated by this, we study privacy in the context of finite-horizon
Markov Decision Processes (MDPs) by requiring information to be obfuscated on
the user side. We formulate this notion of privacy for RL by leveraging the
local differential privacy (LDP) framework. We present an optimistic algorithm
that simultaneously satisfies LDP requirements, and achieves sublinear regret.
We also establish a lower bound for regret minimization in finite-horizon MDPs
with LDP guarantees. These results show that while LDP is appealing in
practical applications, the setting is inherently more complex. In particular,
our results demonstrate that the cost of privacy is multiplicative when
compared to non-private settings.
</p>
<a href="http://arxiv.org/abs/2010.07778" target="_blank">arXiv:2010.07778</a> [<a href="http://arxiv.org/pdf/2010.07778" target="_blank">pdf</a>]

<h2>Improved Multi-Source Domain Adaptation by Preservation of Factors. (arXiv:2010.07783v1 [cs.CV])</h2>
<h3>Sebastian Schrom, Stephan Hasler, J&#xfc;rgen Adamy</h3>
<p>Domain Adaptation (DA) is a highly relevant research topic when it comes to
image classification with deep neural networks. Combining multiple source
domains in a sophisticated way to optimize a classification model can improve
the generalization to a target domain. Here, the difference in data
distributions of source and target image datasets plays a major role. In this
paper, we describe based on a theory of visual factors how real-world scenes
appear in images in general and how recent DA datasets are composed of such. We
show that different domains can be described by a set of so called domain
factors, whose values are consistent within a domain, but can change across
domains. Many DA approaches try to remove all domain factors from the feature
representation to be domain invariant. In this paper we show that this can lead
to negative transfer since task-informative factors can get lost as well. To
address this, we propose Factor-Preserving DA (FP-DA), a method to train a deep
adversarial unsupervised DA model, which is able to preserve specific task
relevant factors in a multi-domain scenario. We demonstrate on CORe50, a
dataset with many domains, how such factors can be identified by standard
one-to-one transfer experiments between single domains combined with PCA. By
applying FP-DA, we show that the highest average and minimum performance can be
achieved.
</p>
<a href="http://arxiv.org/abs/2010.07783" target="_blank">arXiv:2010.07783</a> [<a href="http://arxiv.org/pdf/2010.07783" target="_blank">pdf</a>]

<h2>Response Selection for Multi-Party Conversations with Dynamic Topic Tracking. (arXiv:2010.07785v1 [cs.CL])</h2>
<h3>Weishi Wang, Shafiq Joty, Steven C.H. Hoi</h3>
<p>While participants in a multi-party multi-turn conversation simultaneously
engage in multiple conversation topics, existing response selection methods are
developed mainly focusing on a two-party single-conversation scenario. Hence,
the prolongation and transition of conversation topics are ignored by current
methods. In this work, we frame response selection as a dynamic topic tracking
task to match the topic between the response and relevant conversation context.
With this new formulation, we propose a novel multi-task learning framework
that supports efficient encoding through large pretrained models with only two
utterances at once to perform dynamic topic disentanglement and response
selection. We also propose Topic-BERT an essential pretraining step to embed
topic information into BERT with self-supervised learning. Experimental results
on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response
selection and topic disentanglement tasks outperforming existing methods by a
good margin.
</p>
<a href="http://arxiv.org/abs/2010.07785" target="_blank">arXiv:2010.07785</a> [<a href="http://arxiv.org/pdf/2010.07785" target="_blank">pdf</a>]

<h2>Generalizing Universal Adversarial Attacks Beyond Additive Perturbations. (arXiv:2010.07788v1 [cs.CV])</h2>
<h3>Yanghao Zhang, Wenjie Ruan, Fu Wang, Xiaowei Huang</h3>
<p>The previous study has shown that universal adversarial attacks can fool deep
neural networks over a large set of input images with a single human-invisible
perturbation. However, current methods for universal adversarial attacks are
based on additive perturbation, which cause misclassification when the
perturbation is directly added to the input images. In this paper, for the
first time, we show that a universal adversarial attack can also be achieved
via non-additive perturbation (e.g., spatial transformation). More importantly,
to unify both additive and non-additive perturbations, we propose a novel
unified yet flexible framework for universal adversarial attacks, called GUAP,
which is able to initiate attacks by additive perturbation, non-additive
perturbation, or the combination of both. Extensive experiments are conducted
on CIFAR-10 and ImageNet datasets with six deep neural network models including
GoogleLeNet, VGG16/19, ResNet101/152, and DenseNet121. The empirical
experiments demonstrate that GUAP can obtain up to 90.9% and 99.24% successful
attack rates on CIFAR-10 and ImageNet datasets, leading to over 15% and 19%
improvements respectively than current state-of-the-art universal adversarial
attacks. The code for reproducing the experiments in this paper is available at
https://github.com/TrustAI/GUAP.
</p>
<a href="http://arxiv.org/abs/2010.07788" target="_blank">arXiv:2010.07788</a> [<a href="http://arxiv.org/pdf/2010.07788" target="_blank">pdf</a>]

<h2>Task-Adaptive Robot Learning from Demonstration under Replication with Gaussian Process Models. (arXiv:2010.07795v1 [cs.RO])</h2>
<h3>Miguel Arduengo, Adri&#xe0; Colom&#xe9;, J&#xfa;lia Borr&#xe0;s, Luis Sentis, Carme Torras</h3>
<p>Learning from Demonstration (LfD) is a paradigm that allows robots to learn
complex manipulation tasks that can not be easily scripted, but can be
demonstrated by a human teacher. One of the challenges of LfD is to enable
robots to acquire skills that can be adapted to different scenarios. In this
paper, we propose to achieve this by exploiting the variations in the
demonstrations to retrieve an adaptive and robust policy, using Gaussian
Process (GP) models. Adaptability is enhanced by incorporating task parameters
into the model, which encode different specifications within the same task.
With our formulation, these parameters can either be real, integer, or
categorical. Furthermore, we propose a GP design that exploits the structure of
replications, i.e., repeated demonstrations at identical conditions within
data. Our method significantly reduces the computational cost of model fitting
in complex tasks, where replications are essential to obtain a robust model. We
illustrate our approach through several experiments on a handwritten letter
demonstration dataset.
</p>
<a href="http://arxiv.org/abs/2010.07795" target="_blank">arXiv:2010.07795</a> [<a href="http://arxiv.org/pdf/2010.07795" target="_blank">pdf</a>]

<h2>Spiking Neural Networks with Single-Spike Temporal-Coded Neurons for Network Intrusion Detection. (arXiv:2010.07803v1 [cs.LG])</h2>
<h3>Shibo Zhou, Xiaohua Li</h3>
<p>Spiking neural network (SNN) is interesting due to its strong
bio-plausibility and high energy efficiency. However, its performance is
falling far behind conventional deep neural networks (DNNs). In this paper,
considering a general class of single-spike temporal-coded integrate-and-fire
neurons, we analyze the input-output expressions of both leaky and nonleaky
neurons. We show that SNNs built with leaky neurons suffer from the
overly-nonlinear and overly-complex input-output response, which is the major
reason for their difficult training and low performance. This reason is more
fundamental than the commonly believed problem of nondifferentiable spikes. To
support this claim, we show that SNNs built with nonleaky neurons can have a
less-complex and less-nonlinear input-output response. They can be easily
trained and can have superior performance, which is demonstrated by
experimenting with the SNNs over two popular network intrusion detection
datasets, i.e., the NSL-KDD and the AWID datasets. Our experiment results show
that the proposed SNNs outperform a comprehensive list of DNN models and
classic machine learning models. This paper demonstrates that SNNs can be
promising and competitive in contrast to common beliefs.
</p>
<a href="http://arxiv.org/abs/2010.07803" target="_blank">arXiv:2010.07803</a> [<a href="http://arxiv.org/pdf/2010.07803" target="_blank">pdf</a>]

<h2>CIMON: Towards High-quality Hash Codes. (arXiv:2010.07804v1 [cs.CV])</h2>
<h3>Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Huasong Zhong, Minghua Deng, Jianqiang Huang, Xian-sheng Hua</h3>
<p>Recently, hashing is widely-used in approximate nearest neighbor search for
its storage and computational efficiency. Due to the lack of labeled data in
practice, many studies focus on unsupervised hashing. Most of the unsupervised
hashing methods learn to map images into semantic similarity-preserving hash
codes by constructing local semantic similarity structure from the pre-trained
model as guiding information, i.e., treating each point pair similar if their
distance is small in feature space. However, due to the inefficient
representation ability of the pre-trained model, many false positives and
negatives in local semantic similarity will be introduced and lead to error
propagation during hash code learning. Moreover, most of hashing methods ignore
the basic characteristics of hash codes such as collisions, which will cause
instability of hash codes to disturbance. In this paper, we propose a new
method named Comprehensive sImilarity Mining and cOnsistency learNing (CIMON).
First, we use global constraint learning and similarity statistical
distribution to obtain reliable and smooth guidance. Second, image augmentation
and consistency learning will be introduced to explore both semantic and
contrastive consistency to derive robust hash codes with fewer collisions.
Extensive experiments on several benchmark datasets show that the proposed
method consistently outperforms a wide range of state-of-the-art methods in
both retrieval performance and robustness.
</p>
<a href="http://arxiv.org/abs/2010.07804" target="_blank">arXiv:2010.07804</a> [<a href="http://arxiv.org/pdf/2010.07804" target="_blank">pdf</a>]

<h2>Federated Learning in Adversarial Settings. (arXiv:2010.07808v1 [cs.CR])</h2>
<h3>Raouf Kerkouche, Gergely &#xc1;cs, Claude Castelluccia</h3>
<p>Federated Learning enables entities to collaboratively learn a shared
prediction model while keeping their training data locally. It prevents data
collection and aggregation and, therefore, mitigates the associated privacy
risks. However, it still remains vulnerable to various security attacks where
malicious participants aim at degrading the generated model, inserting
backdoors, or inferring other participants' training data. This paper presents
a new federated learning scheme that provides different trade-offs between
robustness, privacy, bandwidth efficiency, and model accuracy. Our scheme uses
biased quantization of model updates and hence is bandwidth efficient. It is
also robust against state-of-the-art backdoor as well as model degradation
attacks even when a large proportion of the participant nodes are malicious. We
propose a practical differentially private extension of this scheme which
protects the whole dataset of participating entities. We show that this
extension performs as efficiently as the non-private but robust scheme, even
with stringent privacy requirements but are less robust against model
degradation and backdoor attacks. This suggests a possible fundamental
trade-off between Differential Privacy and robustness.
</p>
<a href="http://arxiv.org/abs/2010.07808" target="_blank">arXiv:2010.07808</a> [<a href="http://arxiv.org/pdf/2010.07808" target="_blank">pdf</a>]

<h2>Does Data Augmentation Benefit from Split BatchNorms. (arXiv:2010.07810v1 [cs.CV])</h2>
<h3>Amil Merchant, Barret Zoph, Ekin Dogus Cubuk</h3>
<p>Data augmentation has emerged as a powerful technique for improving the
performance of deep neural networks and led to state-of-the-art results in
computer vision. However, state-of-the-art data augmentation strongly distorts
training images, leading to a disparity between examples seen during training
and inference. In this work, we explore a recently proposed training paradigm
in order to correct for this disparity: using an auxiliary BatchNorm for the
potentially out-of-distribution, strongly augmented images. Our experiments
then focus on how to define the BatchNorm parameters that are used at
evaluation. To eliminate the train-test disparity, we experiment with using the
batch statistics defined by clean training images only, yet surprisingly find
that this does not yield improvements in model performance. Instead, we
investigate using BatchNorm parameters defined by weak augmentations and find
that this method significantly improves the performance of common image
classification benchmarks such as CIFAR-10, CIFAR-100, and ImageNet. We then
explore a fundamental trade-off between accuracy and robustness coming from
using different BatchNorm parameters, providing greater insight into the
benefits of data augmentation on model performance.
</p>
<a href="http://arxiv.org/abs/2010.07810" target="_blank">arXiv:2010.07810</a> [<a href="http://arxiv.org/pdf/2010.07810" target="_blank">pdf</a>]

<h2>Where's the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data. (arXiv:2010.07816v1 [cs.CL])</h2>
<h3>George Michalopoulos, Helen Chen, Alexander Wong</h3>
<p>In most clinical practice settings, there is no rigorous reviewing of the
clinical documentation, resulting in inaccurate information captured in the
patient medical records. The gold standard in clinical data capturing is
achieved via "expert-review", where clinicians can have a dialogue with a
domain expert (reviewers) and ask them questions about data entry rules.
Automatically identifying "real questions" in these dialogues could uncover
ambiguities or common problems in data capturing in a given clinical setting.

In this study, we proposed a novel multi-channel deep convolutional neural
network architecture, namely Quest-CNN, for the purpose of separating real
questions that expect an answer (information or help) about an issue from
sentences that are not questions, as well as from questions referring to an
issue mentioned in a nearby sentence (e.g., can you clarify this?), which we
will refer as "c-questions". We conducted a comprehensive performance
comparison analysis of the proposed multi-channel deep convolutional neural
network against other deep neural networks. Furthermore, we evaluated the
performance of traditional rule-based and learning-based methods for detecting
question sentences. The proposed Quest-CNN achieved the best F1 score both on a
dataset of data entry-review dialogue in a dialysis care setting, and on a
general domain dataset.
</p>
<a href="http://arxiv.org/abs/2010.07816" target="_blank">arXiv:2010.07816</a> [<a href="http://arxiv.org/pdf/2010.07816" target="_blank">pdf</a>]

<h2>Interpreting Deep Learning Model Using Rule-based Method. (arXiv:2010.07824v1 [cs.LG])</h2>
<h3>Xiaojian Wang, Jingyuan Wang, Ke Tang</h3>
<p>Deep learning models are favored in many research and industry areas and have
reached the accuracy of approximating or even surpassing human level. However
they've long been considered by researchers as black-box models for their
complicated nonlinear property. In this paper, we propose a multi-level
decision framework to provide comprehensive interpretation for the deep neural
network model.

In this multi-level decision framework, by fitting decision trees for each
neuron and aggregate them together, a multi-level decision structure (MLD) is
constructed at first, which can approximate the performance of the target
neural network model with high efficiency and high fidelity. In terms of local
explanation for sample, two algorithms are proposed based on MLD structure:
forward decision generation algorithm for providing sample decisions, and
backward rule induction algorithm for extracting sample rule-mapping
recursively. For global explanation, frequency-based and out-of-bag based
methods are proposed to extract important features in the neural network
decision. Furthermore, experiments on the MNIST and National Free Pre-Pregnancy
Check-up (NFPC) dataset are carried out to demonstrate the effectiveness and
interpretability of MLD framework. In the evaluation process, both
functionally-grounded and human-grounded methods are used to ensure
credibility.
</p>
<a href="http://arxiv.org/abs/2010.07824" target="_blank">arXiv:2010.07824</a> [<a href="http://arxiv.org/pdf/2010.07824" target="_blank">pdf</a>]

<h2>Interpretation of Swedish Sign Language using Convolutional Neural Networks and Transfer Learning. (arXiv:2010.07827v1 [cs.CV])</h2>
<h3>Gustaf Halvardsson, Johanna Peterson, C&#xe9;sar Soto-Valero, Benoit Baudry</h3>
<p>The automatic interpretation of sign languages is a challenging task, as it
requires the usage of high-level vision and high-level motion processing
systems for providing accurate image perception. In this paper, we use
Convolutional Neural Networks (CNNs) and transfer learning in order to make
computers able to interpret signs of the Swedish Sign Language (SSL) hand
alphabet. Our model consist of the implementation of a pre-trained InceptionV3
network, and the usage of the mini-batch gradient descent optimization
algorithm. We rely on transfer learning during the pre-training of the model
and its data. The final accuracy of the model, based on 8 study subjects and
9,400 images, is 85%. Our results indicate that the usage of CNNs is a
promising approach to interpret sign languages, and transfer learning can be
used to achieve high testing accuracy despite using a small training dataset.
Furthermore, we describe the implementation details of our model to interpret
signs as a user-friendly web application.
</p>
<a href="http://arxiv.org/abs/2010.07827" target="_blank">arXiv:2010.07827</a> [<a href="http://arxiv.org/pdf/2010.07827" target="_blank">pdf</a>]

<h2>Semi-Supervised Semantic Segmentation in Earth Observation: The MiniFrance Suite, Dataset Analysis and Multi-task Network Study. (arXiv:2010.07830v1 [cs.CV])</h2>
<h3>Javiera Castillo-Navarro, Bertrand Le Saux, Alexandre Boulch, Nicolas Audebert, S&#xe9;bastien Lef&#xe8;vre</h3>
<p>The development of semi-supervised learning techniques is essential to
enhance the generalization capacities of machine learning algorithms. Indeed,
raw image data are abundant while labels are scarce, therefore it is crucial to
leverage unlabeled inputs to build better models. The availability of large
databases have been key for the development of learning algorithms with high
level performance.

Despite the major role of machine learning in Earth Observation to derive
products such as land cover maps, datasets in the field are still limited,
either because of modest surface coverage, lack of variety of scenes or
restricted classes to identify. We introduce a novel large-scale dataset for
semi-supervised semantic segmentation in Earth Observation, the MiniFrance
suite. MiniFrance has several unprecedented properties: it is large-scale,
containing over 2000 very high resolution aerial images, accounting for more
than 200 billions samples (pixels); it is varied, covering 16 conurbations in
France, with various climates, different landscapes, and urban as well as
countryside scenes; and it is challenging, considering land use classes with
high-level semantics. Nevertheless, the most distinctive quality of MiniFrance
is being the only dataset in the field especially designed for semi-supervised
learning: it contains labeled and unlabeled images in its training partition,
which reproduces a life-like scenario. Along with this dataset, we present
tools for data representativeness analysis in terms of appearance similarity
and a thorough study of MiniFrance data, demonstrating that it is suitable for
learning and generalizes well in a semi-supervised setting. Finally, we present
semi-supervised deep architectures based on multi-task learning and the first
experiments on MiniFrance.
</p>
<a href="http://arxiv.org/abs/2010.07830" target="_blank">arXiv:2010.07830</a> [<a href="http://arxiv.org/pdf/2010.07830" target="_blank">pdf</a>]

<h2>Guidelines Towards Information-driven MobilityManagement. (arXiv:2010.07832v1 [cs.NI])</h2>
<h3>Rute C. Sofia</h3>
<p>The architectural semantics of \emph{Information-Centric Networking} bring in
interesting features in regards to mobility management: Information-Centric
Networking is content-oriented, connection-less, and receiver-driven. Despite
such intrinsic advantages, the support for node movement is being based on the
principles of IP solutions. IP-based solutions are, however, host-oriented, and
Information-Centric Networking paradigms are information-oriented. By following
IP mobility management principles, some of the natural mobility support
advantages of Information-Centric Networking are not being adequately explored.

This paper contributes with an overview on how Information-Centric Networking
paradigms handle mobility management as of today, highlighting current
challenges and proposing a set of design guidelines to overcome them, thus
steering a vision towards a content-centric mobility management approach.
</p>
<a href="http://arxiv.org/abs/2010.07832" target="_blank">arXiv:2010.07832</a> [<a href="http://arxiv.org/pdf/2010.07832" target="_blank">pdf</a>]

<h2>PIMOD: A Tool for Configuring Single-Board Computer Operating System Images. (arXiv:2010.07833v1 [cs.OS])</h2>
<h3>Jonas H&#xf6;chst, Alvar Penning, Patrick Lampe, Bernd Freisleben</h3>
<p>Computer systems used in the field of humanitarian technology are often based
on general-purpose single-board computers, such as Raspberry Pis. While these
systems offer great flexibility for developers and users, configuration and
deployment either introduces overhead by executing scripts on multiple devices
or requires deeper technical understanding when building operating system
images for such small computers from scratch. In this paper, we present PIMOD,
a software tool for configuring operating system images for single-board
computer systems. We propose a simple yet comprehensive configuration language.
In a configuration profile, called Pifile, a small set of commands is used to
describe the configuration of an operating system image. Virtualization
techniques are used during the execution of the profile in order to be
distribution and platform independent. Commands can be issued in the guest
operating system, providing access to the distribution specific tools, e.g., to
configure hardware parameters. The implementation of PIMOD is made public under
a free and open source license. PIMOD is evaluated in terms of user benefits,
performance compared to on-system configuration, and applicability across
different hardware platforms and operating systems.
</p>
<a href="http://arxiv.org/abs/2010.07833" target="_blank">arXiv:2010.07833</a> [<a href="http://arxiv.org/pdf/2010.07833" target="_blank">pdf</a>]

<h2>Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach. (arXiv:2010.07835v1 [cs.CL])</h2>
<h3>Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang</h3>
<p>Fine-tuned pre-trained language models (LMs) achieve enormous success in many
natural language processing (NLP) tasks, but they still require excessive
labeled data in the fine-tuning stage. We study the problem of fine-tuning
pre-trained LMs using only weak supervision, without any labeled data. This
problem is challenging because the high capacity of LMs makes them prone to
overfitting the noisy labels generated by weak supervision. To address this
problem, we develop a contrastive self-training framework, COSINE, to enable
fine-tuning LMs with weak supervision. Underpinned by contrastive
regularization and confidence-based reweighting, this contrastive self-training
framework can gradually improve model fitting while effectively suppressing
error propagation. Experiments on sequence, token, and sentence pair
classification tasks show that our model outperforms the strongest baseline by
large margins on 7 benchmarks in 6 tasks, and achieves competitive performance
with fully-supervised fine-tuning methods.
</p>
<a href="http://arxiv.org/abs/2010.07835" target="_blank">arXiv:2010.07835</a> [<a href="http://arxiv.org/pdf/2010.07835" target="_blank">pdf</a>]

<h2>Deep Learning on Real Geophysical Data: A Case Study for Distributed Acoustic Sensing Research. (arXiv:2010.07842v1 [cs.LG])</h2>
<h3>Vincent Dumont, Ver&#xf3;nica Rodr&#xed;guez Tribaldos, Jonathan Ajo-Franklin, Kesheng Wu</h3>
<p>Deep Learning approaches for real, large, and complex scientific data sets
can be very challenging to design. In this work, we present a complete search
for a finely-tuned and efficiently scaled deep learning classifier to identify
usable energy from seismic data acquired using Distributed Acoustic Sensing
(DAS). While using only a subset of labeled images during training, we were
able to identify suitable models that can be accurately generalized to unknown
signal patterns. We show that by using 16 times more GPUs, we can increase the
training speed by more than two orders of magnitude on a 50,000-image data set.
</p>
<a href="http://arxiv.org/abs/2010.07842" target="_blank">arXiv:2010.07842</a> [<a href="http://arxiv.org/pdf/2010.07842" target="_blank">pdf</a>]

<h2>A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack and Learning. (arXiv:2010.07849v1 [cs.CV])</h2>
<h3>Hongjun Wang, Guanbin Li, Xiaobai Liu, Liang Lin</h3>
<p>Although deep convolutional neural networks (CNNs) have demonstrated
remarkable performance on multiple computer vision tasks, researches on
adversarial learning have shown that deep models are vulnerable to adversarial
examples, which are crafted by adding visually imperceptible perturbations to
the input images. Most of the existing adversarial attack methods only create a
single adversarial example for the input, which just gives a glimpse of the
underlying data manifold of adversarial examples. An attractive solution is to
explore the solution space of the adversarial examples and generate a diverse
bunch of them, which could potentially improve the robustness of real-world
systems and help prevent severe security threats and vulnerabilities. In this
paper, we present an effective method, called Hamiltonian Monte Carlo with
Accumulated Momentum (HMCAM), aiming to generate a sequence of adversarial
examples. To improve the efficiency of HMC, we propose a new regime to
automatically control the length of trajectories, which allows the algorithm to
move with adaptive step sizes along the search direction at different
positions. Moreover, we revisit the reason for high computational cost of
adversarial training under the view of MCMC and design a new generative method
called Contrastive Adversarial Training (CAT), which approaches equilibrium
distribution of adversarial examples with only few iterations by building from
small modifications of the standard Contrastive Divergence (CD) and achieve a
trade-off between efficiency and accuracy. Both quantitative and qualitative
analysis on several natural image datasets and practical systems have confirmed
the superiority of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2010.07849" target="_blank">arXiv:2010.07849</a> [<a href="http://arxiv.org/pdf/2010.07849" target="_blank">pdf</a>]

<h2>Bi-level Score Matching for Learning Energy-based Latent Variable Models. (arXiv:2010.07856v1 [cs.LG])</h2>
<h3>Fan Bao, Chongxuan Li, Kun Xu, Hang Su, Jun Zhu, Bo Zhang</h3>
<p>Score matching (SM) provides a compelling approach to learn energy-based
models (EBMs) by avoiding the calculation of partition function. However, it
remains largely open to learn energy-based latent variable models (EBLVMs),
except some special cases. This paper presents a bi-level score matching (BiSM)
method to learn EBLVMs with general structures by reformulating SM as a
bi-level optimization problem. The higher level introduces a variational
posterior of the latent variables and optimizes a modified SM objective, and
the lower level optimizes the variational posterior to fit the true posterior.
To solve BiSM efficiently, we develop a stochastic optimization algorithm with
gradient unrolling. Theoretically, we analyze the consistency of BiSM and the
convergence of the stochastic algorithm. Empirically, we show the promise of
BiSM in Gaussian restricted Boltzmann machines and highly nonstructural EBLVMs
parameterized by deep convolutional neural networks. BiSM is comparable to the
widely adopted contrastive divergence and SM methods when they are applicable;
and can learn complex EBLVMs with intractable posteriors to generate natural
images.
</p>
<a href="http://arxiv.org/abs/2010.07856" target="_blank">arXiv:2010.07856</a> [<a href="http://arxiv.org/pdf/2010.07856" target="_blank">pdf</a>]

<h2>What you need to know to train recurrent neural networks to make Flip Flops memories and more. (arXiv:2010.07858v1 [cs.LG])</h2>
<h3>Cecilia Jarne</h3>
<p>Training neural networks to perform different tasks is relevant across
various disciplines that go beyond Machine Learning. In particular, Recurrent
Neural Networks (RNN) are of great interest to different scientific
communities. Open-source frameworks dedicated to Machine Learning such as
Tensorflow \cite{chollet2015keras} and Keras \cite{tensorflow2015-whitepaper}
has produced significative changes in the development of technologies that we
currently use. One relevant problem that can be approach is how to build the
models for the study of dynamical systems, and how to extract the relevant
information to be able to answer the scientific questions of interest. The
purpose of the present work is to contribute to this aim by using a temporal
processing task, in this case, a 3-bit Flip Flop memory, to show the modeling
procedure in every step: from equations to the software code, using Tensorflow
and Keras. The obtained networks are analyzed to describe the dynamics and to
show different visualization and analysis tools. The code developed in this
premier is provided to be used as a base for model other systems.
</p>
<a href="http://arxiv.org/abs/2010.07858" target="_blank">arXiv:2010.07858</a> [<a href="http://arxiv.org/pdf/2010.07858" target="_blank">pdf</a>]

<h2>EqSpike: Spike-driven Equilibrium Propagation for Neuromorphic Implementations. (arXiv:2010.07859v1 [cs.NE])</h2>
<h3>Erwann Martin, Maxence Ernoult, J&#xe9;r&#xe9;mie Laydevant, Shuai Li, Damien Querlioz, Teodora Petrisor, Julie Grollier</h3>
<p>Neuromorphic systems achieve high energy efficiency by computing with spikes,
in a brain-inspired way. However, finding spike-based learning algorithms that
can be implemented within the local constraints of neuromorphic systems, while
achieving high accuracy, remains a formidable challenge. Equilibrium
Propagation is a hardware-friendly counterpart of backpropagation which only
involves spatially local computations and applies to recurrent neural networks
with static inputs. So far, hardware-oriented studies of Equilibrium
Propagation focused on rate-based networks. In this work, we develop a spiking
neural network algorithm called EqSpike, compatible with neuromorphic systems,
which learns by Equilibrium Propagation. Through simulations, we obtain a test
recognition accuracy of 96.9% on MNIST, similar to rate-based Equilibrium
Propagation, and comparing favourably to alternative learning techniques for
spiking neural networks. We show that EqSpike implemented in silicon
neuromorphic technology could reduce the energy consumption of inference and
training by up to three orders of magnitude compared to GPUs. Finally, we also
show that during learning, EqSpike weight updates exhibit a form of Spike
Timing Dependent Plasticity, highlighting a possible connection with biology.
</p>
<a href="http://arxiv.org/abs/2010.07859" target="_blank">arXiv:2010.07859</a> [<a href="http://arxiv.org/pdf/2010.07859" target="_blank">pdf</a>]

<h2>Deep Conditional Transformation Models. (arXiv:2010.07860v1 [cs.LG])</h2>
<h3>Philipp F.M. Baumann, Torsten Hothorn, David R&#xfc;gamer</h3>
<p>Learning the cumulative distribution function (CDF) of an outcome variable
conditional on a set of features remains challenging, especially in
high-dimensional settings. Conditional transformation models provide a
semi-parametric approach that allows to model a large class of conditional CDFs
without an explicit parametric distribution assumption and with only a few
parameters. Existing estimation approaches within the class of transformation
models are, however, either limited in their complexity and applicability to
unstructured data sources such as images or text, or can incorporate complex
effects of different features but lack interpretability. We close this gap by
introducing the class of deep conditional transformation models which unify
existing approaches and allow to learn both interpretable (non-)linear model
terms and more complex predictors in one holistic neural network. To this end
we propose a novel network architecture, provide details on different model
definitions and derive suitable constraints and derive suitable network
regularization terms. We demonstrate the efficacy of our approach through
numerical experiments and applications.
</p>
<a href="http://arxiv.org/abs/2010.07860" target="_blank">arXiv:2010.07860</a> [<a href="http://arxiv.org/pdf/2010.07860" target="_blank">pdf</a>]

<h2>Continual Learning for Neural Semantic Parsing. (arXiv:2010.07865v1 [cs.CL])</h2>
<h3>Vladislav Lialin, Rahul Goel, Andrey Simanovsky, Anna Rumshisky, Rushin Shah</h3>
<p>A semantic parsing model is crucial to natural language processing
applications such as goal-oriented dialogue systems. Such models can have
hundreds of classes with a highly non-uniform distribution. In this work, we
show how to efficiently (in terms of computational budget) improve model
performance given a new portion of labeled data for a specific low-resource
class or a set of classes. We demonstrate that a simple approach with a
specific fine-tuning procedure for the old model can reduce the computational
costs by ~90% compared to the training of a new model. The resulting
performance is on-par with a model trained from scratch on a full dataset. We
showcase the efficacy of our approach on two popular semantic parsing datasets,
Facebook TOP, and SNIPS.
</p>
<a href="http://arxiv.org/abs/2010.07865" target="_blank">arXiv:2010.07865</a> [<a href="http://arxiv.org/pdf/2010.07865" target="_blank">pdf</a>]

<h2>Double Robust Representation Learning for Counterfactual Prediction. (arXiv:2010.07866v1 [stat.ML])</h2>
<h3>Shuxi Zeng, Serge Assaad, Chenyang Tao, Shounak Datta, Lawrence Carin, Fan Li</h3>
<p>Causal inference, or counterfactual prediction, is central to decision making
in healthcare, policy and social sciences. To de-bias causal estimators with
high-dimensional data in observational studies, recent advances suggest the
importance of combining machine learning models for both the propensity score
and the outcome function. We propose a novel scalable method to learn
double-robust representations for counterfactual predictions, leading to
consistent causal estimation if the model for either the propensity score or
the outcome, but not necessarily both, is correctly specified. Specifically, we
use the entropy balancing method to learn the weights that minimize the
Jensen-Shannon divergence of the representation between the treated and control
groups, based on which we make robust and efficient counterfactual predictions
for both individual and average treatment effects. We provide theoretical
justifications for the proposed method. The algorithm shows competitive
performance with the state-of-the-art on real world and synthetic data.
</p>
<a href="http://arxiv.org/abs/2010.07866" target="_blank">arXiv:2010.07866</a> [<a href="http://arxiv.org/pdf/2010.07866" target="_blank">pdf</a>]

<h2>Deep Generative Modeling in Network Science with Applications to Public Policy Research. (arXiv:2010.07870v1 [cs.LG])</h2>
<h3>Gavin Hartnett, Raffaele Vardavas, Lawrence Baker, Michael Chaykowski, C. Ben Gibson, Federico Girosi, David Kennedy, Osonde Osoba</h3>
<p>Network data is increasingly being used in quantitative, data-driven public
policy research. These are typically very rich datasets that contain complex
correlations and inter-dependencies. This richness both promises to be quite
useful for policy research, while at the same time posing a challenge for the
useful extraction of information from these datasets - a challenge which calls
for new data analysis methods. In this report, we formulate a research agenda
of key methodological problems whose solutions would enable new advances across
many areas of policy research. We then review recent advances in applying deep
learning to network data, and show how these methods may be used to address
many of the methodological problems we identified. We particularly emphasize
deep generative methods, which can be used to generate realistic synthetic
networks useful for microsimulation and agent-based models capable of informing
key public policy questions. We extend these recent advances by developing a
new generative framework which applies to large social contact networks
commonly used in epidemiological modeling. For context, we also compare and
contrast these recent neural network-based approaches with the more traditional
Exponential Random Graph Models. Lastly, we discuss some open problems where
more progress is needed.
</p>
<a href="http://arxiv.org/abs/2010.07870" target="_blank">arXiv:2010.07870</a> [<a href="http://arxiv.org/pdf/2010.07870" target="_blank">pdf</a>]

<h2>Neograd: gradient descent with an adaptive learning rate. (arXiv:2010.07873v1 [cs.LG])</h2>
<h3>Michael F. Zimmer</h3>
<p>Since its inception by Cauchy in 1847, the gradient descent algorithm has
been without guidance as to how to efficiently set the learning rate. This
paper identifies a concept, defines metrics, and introduces algorithms to
provide such guidance. The result is a family of algorithms (Neograd) based on
a {\em constant $\rho$ ansatz}, where $\rho$ is a metric based on the error of
the updates. This allows one to adjust the learning rate at each step, using a
formulaic estimate based on $\rho$. It is now no longer necessary to do trial
runs beforehand to estimate a single learning rate for an entire optimization
run. The additional costs to operate this metric are trivial. One member of
this family of algorithms, NeogradM, can quickly reach much lower cost function
values than other first order algorithms. Comparisons are made mainly between
NeogradM and Adam on an array of test functions and on a neural network model
for identifying hand-written digits. The results show great performance
improvements with NeogradM.
</p>
<a href="http://arxiv.org/abs/2010.07873" target="_blank">arXiv:2010.07873</a> [<a href="http://arxiv.org/pdf/2010.07873" target="_blank">pdf</a>]

<h2>Tokenization Repair in the Presence of Spelling Errors. (arXiv:2010.07878v1 [cs.CL])</h2>
<h3>Hannah Bast, Matthias Hertel, Mostafa M. Mohamed</h3>
<p>We consider the following tokenization repair problem: Given a natural
language text with any combination of missing or spurious spaces, correct
these. Spelling errors can be present, but it's not part of the problem to
correct them. For example, given: "Tispa per isabout token izaionrep air",
compute "Tis paper is about tokenizaion repair". It is tempting to think of
this problem as a special case of spelling correction or to treat the two
problems together. We make a case that tokenization repair and spelling
correction should and can be treated as separate problems. We investigate a
variety of neural models as well as a number of strong baselines. We identify
three main ingredients to high-quality tokenization repair: deep language
models with a bidirectional component, training the models on text with
spelling errors, and making use of the space information already present. Our
best methods can repair all tokenization errors on 97.5% of the correctly
spelled test sentences and on 96.0% of the misspelled test sentences. With all
spaces removed from the given text (the scenario from previous work), the
accuracy falls to 94.5% and 90.1%, respectively. We conduct a detailed error
analysis.
</p>
<a href="http://arxiv.org/abs/2010.07878" target="_blank">arXiv:2010.07878</a> [<a href="http://arxiv.org/pdf/2010.07878" target="_blank">pdf</a>]

<h2>An Empirical Analysis of Visual Features for Multiple Object Tracking in Urban Scenes. (arXiv:2010.07881v1 [cs.CV])</h2>
<h3>Mehdi Miah, Justine Pepin, Nicolas Saunier, Guillaume-Alexandre Bilodeau</h3>
<p>This paper addresses the problem of selecting appearance features for
multiple object tracking (MOT) in urban scenes. Over the years, a large number
of features has been used for MOT. However, it is not clear whether some of
them are better than others. Commonly used features are color histograms,
histograms of oriented gradients, deep features from convolutional neural
networks and re-identification (ReID) features. In this study, we assess how
good these features are at discriminating objects enclosed by a bounding box in
urban scene tracking scenarios. Several affinity measures, namely the
$\mathrm{L}_1$, $\mathrm{L}_2$ and the Bhattacharyya distances, Rank-1 counts
and the cosine similarity, are also assessed for their impact on the
discriminative power of the features. Results on several datasets show that
features from ReID networks are the best for discriminating instances from one
another regardless of the quality of the detector. If a ReID model is not
available, color histograms may be selected if the detector has a good recall
and there are few occlusions; otherwise, deep features are more robust to
detectors with lower recall. The project page is
this http URL
</p>
<a href="http://arxiv.org/abs/2010.07881" target="_blank">arXiv:2010.07881</a> [<a href="http://arxiv.org/pdf/2010.07881" target="_blank">pdf</a>]

<h2>Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention. (arXiv:2010.07891v1 [cs.CL])</h2>
<h3>Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling</h3>
<p>A lack of corpora has so far limited advances in integrating human gaze data
as a supervisory signal in neural attention mechanisms for natural language
processing(NLP). We propose a novel hybrid text saliency model (TSM) that, for
the first time, combines a cognitive model of reading with explicit human gaze
supervision in a single machine learning framework. On four different corpora
we demonstrate that our hybrid TSM duration predictions are highly correlated
with human gaze ground truth. We further propose a novel joint modelling
approach to integrate TSM predictions into the attention layer of a network
designed for a specific upstream NLP task without the need for any
task-specific human gaze data. We demonstrate that our joint model outperforms
the state of the art in paraphrase generation on the Quora Question Pairs
corpus by more than 10% in BLEU-4 and achieves state-of-the-art performance for
sentence compression on the challenging Google Sentence Compression corpus. As
such, our work introduces a practical approach for bridging between data-driven
and cognitive models and demonstrates a new way to integrate human gaze-guided
neural attention into NLP tasks.
</p>
<a href="http://arxiv.org/abs/2010.07891" target="_blank">arXiv:2010.07891</a> [<a href="http://arxiv.org/pdf/2010.07891" target="_blank">pdf</a>]

<h2>An Alternative to Backpropagation in Deep Reinforcement Learning. (arXiv:2010.07893v1 [cs.LG])</h2>
<h3>Stephen Chung</h3>
<p>State-of-the-art deep learning algorithms mostly rely on gradient
backpropagation to train a deep artificial neural network, which is generally
regarded to be biologically implausible. For a network of stochastic units
trained on a reinforcement learning task or a supervised learning task, one
biologically plausible way of learning is to train each unit by REINFORCE. In
this case, only a global reward signal has to be broadcast to all units, and
the learning rule given is local, which can be interpreted as reward-modulated
spike-timing-dependent plasticity (R-STDP) that is observed biologically.
Although this learning rule follows the gradient of return in expectation, it
suffers from high variance and cannot be used to train a deep network in
practice. In this paper, we propose an algorithm called MAP propagation that
can reduce this variance significantly while retaining the local property of
learning rule. Different from prior works on local learning rules (e.g.
Contrastive Divergence) which mostly applies to undirected models in
unsupervised learning tasks, our proposed algorithm applies to directed models
in reinforcement learning tasks. We show that the newly proposed algorithm can
solve common reinforcement learning tasks at a speed similar to that of
backpropagation when applied to an actor-critic network.
</p>
<a href="http://arxiv.org/abs/2010.07893" target="_blank">arXiv:2010.07893</a> [<a href="http://arxiv.org/pdf/2010.07893" target="_blank">pdf</a>]

<h2>Deep Convolutional Neural Network-based Inverse Filtering Approach for Speech De-reverberation. (arXiv:2010.07895v1 [cs.SD])</h2>
<h3>Hanwook Chung, Vikrant Singh Tomar, Benoit Champagne</h3>
<p>In this paper, we introduce a spectral-domain inverse filtering approach for
single-channel speech de-reverberation using deep convolutional neural network
(CNN). The main goal is to better handle realistic reverberant conditions where
the room impulse response (RIR) filter is longer than the short-time Fourier
transform (STFT) analysis window. To this end, we consider the convolutive
transfer function (CTF) model for the reverberant speech signal. In the
proposed framework, the CNN architecture is trained to directly estimate the
inverse filter of the CTF model. Among various choices for the CNN structure,
we consider the U-net which consists of a fully-convolutional auto-encoder
network with skip-connections. Experimental results show that the proposed
method provides better de-reverberation performance than the prevalent
benchmark algorithms under various reverberation conditions.
</p>
<a href="http://arxiv.org/abs/2010.07895" target="_blank">arXiv:2010.07895</a> [<a href="http://arxiv.org/pdf/2010.07895" target="_blank">pdf</a>]

<h2>Marginal Contribution Feature Importance -- an Axiomatic Approach for The Natural Case. (arXiv:2010.07910v1 [cs.LG])</h2>
<h3>Amnon Catav, Boyang Fu, Jason Ernst, Sriram Sankararaman, Ran Gilad-Bachrach</h3>
<p>When training a predictive model over medical data, the goal is sometimes to
gain insights about a certain disease. In such cases, it is common to use
feature importance as a tool to highlight significant factors contributing to
that disease. As there are many existing methods for computing feature
importance scores, understanding their relative merits is not trivial. Further,
the diversity of scenarios in which they are used lead to different
expectations from the feature importance scores. While it is common to make the
distinction between local scores that focus on individual predictions and
global scores that look at the contribution of a feature to the model, another
important division distinguishes model scenarios, in which the goal is to
understand predictions of a given model from natural scenarios, in which the
goal is to understand a phenomenon such as a disease. We develop a set of
axioms that represent the properties expected from a feature importance
function in the natural scenario and prove that there exists only one function
that satisfies all of them, the Marginal Contribution Feature Importance (MCI).
We analyze this function for its theoretical and empirical properties and
compare it to other feature importance scores. While our focus is the natural
scenario, we suggest that our axiomatic approach could be carried out in other
scenarios too.
</p>
<a href="http://arxiv.org/abs/2010.07910" target="_blank">arXiv:2010.07910</a> [<a href="http://arxiv.org/pdf/2010.07910" target="_blank">pdf</a>]

<h2>Multi-Agent Trust Region Policy Optimization. (arXiv:2010.07916v1 [cs.AI])</h2>
<h3>Hepeng Li, Haibo He</h3>
<p>We extend trust region policy optimization (TRPO) to multi-agent
reinforcement learning (MARL) problems. We show that the policy update of TRPO
can be transformed into a distributed consensus optimization problem for
multi-agent cases. By making a series of approximations to the consensus
optimization model, we propose a decentralized MARL algorithm, which we call
multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies
based on local observations and private rewards. The agents do not need to know
observations, rewards, policies or value/action-value functions of other
agents. The agents only share a likelihood ratio with their neighbors during
the training process. The algorithm is fully decentralized and
privacy-preserving. Our experiments on two cooperative games demonstrate its
robust performance on complicated MARL tasks.
</p>
<a href="http://arxiv.org/abs/2010.07916" target="_blank">arXiv:2010.07916</a> [<a href="http://arxiv.org/pdf/2010.07916" target="_blank">pdf</a>]

<h2>Rainfall-Runoff Prediction at Multiple Timescales with a Single Long Short-Term Memory Network. (arXiv:2010.07921v1 [cs.LG])</h2>
<h3>Martin Gauch, Frederik Kratzert, Daniel Klotz, Grey Nearing, Jimmy Lin, Sepp Hochreiter</h3>
<p>Long Short-Term Memory Networks (LSTMs) have been applied to daily discharge
prediction with remarkable success. Many practical scenarios, however, require
predictions at more granular timescales. For instance, accurate prediction of
short but extreme flood peaks can make a life-saving difference, yet such peaks
may escape the coarse temporal resolution of daily predictions. Naively
training an LSTM on hourly data, however, entails very long input sequences
that make learning hard and computationally expensive. In this study, we
propose two Multi-Timescale LSTM (MTS-LSTM) architectures that jointly predict
multiple timescales within one model, as they process long-past inputs at a
single temporal resolution and branch out into each individual timescale for
more recent input steps. We test these models on 516 basins across the
continental United States and benchmark against the US National Water Model.
Compared to naive prediction with a distinct LSTM per timescale, the
multi-timescale architectures are computationally more efficient with no loss
in accuracy. Beyond prediction quality, the multi-timescale LSTM can process
different input variables at different timescales, which is especially relevant
to operational applications where the lead time of meteorological forcings
depends on their temporal resolution.
</p>
<a href="http://arxiv.org/abs/2010.07921" target="_blank">arXiv:2010.07921</a> [<a href="http://arxiv.org/pdf/2010.07921" target="_blank">pdf</a>]

<h2>Representation Learning via Invariant Causal Mechanisms. (arXiv:2010.07922v1 [cs.LG])</h2>
<h3>Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, Charles Blundell</h3>
<p>Self-supervised learning has emerged as a strategy to reduce the reliance on
costly supervised signal by pretraining representations only using unlabeled
data. These methods combine heuristic proxy classification tasks with data
augmentations and have achieved significant success, but our theoretical
understanding of this success remains limited. In this paper we analyze
self-supervised representation learning using a causal framework. We show how
data augmentations can be more effectively utilized through explicit invariance
constraints on the proxy classifiers employed during pretraining. Based on
this, we propose a novel self-supervised objective, Representation Learning via
Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of
proxy targets across augmentations through an invariance regularizer which
yields improved generalization guarantees. Further, using causality we
generalize contrastive learning, a particular kind of self-supervised method,
and provide an alternative theoretical explanation for the success of these
methods. Empirically, ReLIC significantly outperforms competing methods in
terms of robustness and out-of-distribution generalization on ImageNet, while
also significantly outperforming these methods on Atari achieving above
human-level performance on $51$ out of $57$ games.
</p>
<a href="http://arxiv.org/abs/2010.07922" target="_blank">arXiv:2010.07922</a> [<a href="http://arxiv.org/pdf/2010.07922" target="_blank">pdf</a>]

<h2>Learning Energy-Based Models as Generative ConvNets via Multi-grid Modeling and Sampling. (arXiv:1709.08868v3 [stat.ML] UPDATED)</h2>
<h3>Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, Ying Nian Wu</h3>
<p>This paper proposes a multi-grid method for learning energy-based generative
ConvNet models of images. For each grid, we learn an energy-based probabilistic
model where the energy function is defined by a bottom-up convolutional neural
network (ConvNet or CNN). Learning such a model requires generating synthesized
examples from the model. Within each iteration of our learning algorithm, for
each observed training image, we generate synthesized images at multiple grids
by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of
the training image. The synthesized image at each subsequent grid is obtained
by a finite-step MCMC initialized from the synthesized image generated at the
previous coarser grid. After obtaining the synthesized examples, the parameters
of the models at multiple grids are updated separately and simultaneously based
on the differences between synthesized and observed examples. We show that this
multi-grid method can learn realistic energy-based generative ConvNet models,
and it outperforms the original contrastive divergence (CD) and persistent CD.
</p>
<a href="http://arxiv.org/abs/1709.08868" target="_blank">arXiv:1709.08868</a> [<a href="http://arxiv.org/pdf/1709.08868" target="_blank">pdf</a>]

<h2>Learning Data-adaptive Nonparametric Kernels. (arXiv:1808.10724v3 [cs.LG] UPDATED)</h2>
<h3>Fanghui Liu, Xiaolin Huang, Chen Gong, Jie Yang, Li Li</h3>
<p>In this paper, we propose a data-adaptive non-parametric kernel learning
framework in margin based kernel methods. In model formulation, given an
initial kernel matrix, a data-adaptive matrix with two constraints is imposed
in an entry-wise scheme. Learning this data-adaptive matrix in a
formulation-free strategy enlarges the margin between classes and thus improves
the model flexibility. The introduced two constraints are imposed either
exactly (on small data sets) or approximately (on large data sets) in our
model, which provides a controllable trade-off between model flexibility and
complexity with theoretical demonstration. In algorithm optimization, the
objective function of our learning framework is proven to be gradient-Lipschitz
continuous. Thereby, kernel and classifier/regressor learning can be
efficiently optimized in a unified framework via Nesterov's acceleration. For
the scalability issue, we study a decomposition-based approach to our model in
the large sample case. The effectiveness of this approximation is illustrated
by both empirical studies and theoretical guarantees. Experimental results on
various classification and regression benchmark data sets demonstrate that our
non-parametric kernel learning framework achieves good performance when
compared with other representative kernel learning based algorithms.
</p>
<a href="http://arxiv.org/abs/1808.10724" target="_blank">arXiv:1808.10724</a> [<a href="http://arxiv.org/pdf/1808.10724" target="_blank">pdf</a>]

<h2>A Simple Non-i.i.d. Sampling Approach for Efficient Training and Better Generalization. (arXiv:1811.09347v2 [cs.CV] UPDATED)</h2>
<h3>Bowen Cheng, Yunchao Wei, Jiahui Yu, Shiyu Chang, Jinjun Xiong, Wen-Mei Hwu, Thomas S. Huang, Humphrey Shi</h3>
<p>While training on samples drawn from independent and identical distribution
has been a de facto paradigm for optimizing image classification networks,
humans learn new concepts in an easy-to-hard manner and on the selected
examples progressively. Driven by this fact, we investigate the training
paradigms where the samples are not drawn from independent and identical
distribution. We propose a data sampling strategy, named Drop-and-Refresh
(DaR), motivated by the learning behaviors of humans that selectively drop easy
samples and refresh them only periodically. We show in our experiments that the
proposed DaR strategy can maintain (and in many cases improve) the predictive
accuracy even when the training cost is reduced by 15% on various datasets
(CIFAR 10, CIFAR 100 and ImageNet) and with different backbone architectures
(ResNets, DenseNets and MobileNets). Furthermore and perhaps more importantly,
we find the ImageNet pre-trained models using our DaR sampling strategy
achieves better transferability for the downstream tasks including object
detection (+0.3 AP), instance segmentation (+0.3 AP), scene parsing (+0.5 mIoU)
and human pose estimation (+0.6 AP). Our investigation encourages people to
rethink the connections between the sampling strategy for training and the
transferability of its learned features for pre-training ImageNet models.
</p>
<a href="http://arxiv.org/abs/1811.09347" target="_blank">arXiv:1811.09347</a> [<a href="http://arxiv.org/pdf/1811.09347" target="_blank">pdf</a>]

<h2>The Music Streaming Sessions Dataset. (arXiv:1901.09851v2 [cs.IR] UPDATED)</h2>
<h3>Brian Brost, Rishabh Mehrotra, Tristan Jehan</h3>
<p>At the core of many important machine learning problems faced by online
streaming services is a need to model how users interact with the content they
are served. Unfortunately, there are no public datasets currently available
that enable researchers to explore this topic. In order to spur that research,
we release the Music Streaming Sessions Dataset (MSSD), which consists of 160
million listening sessions and associated user actions. Furthermore, we provide
audio features and metadata for the approximately 3.7 million unique tracks
referred to in the logs. This is the largest collection of such track metadata
currently available to the public. This dataset enables research on important
problems including how to model user listening and interaction behaviour in
streaming, as well as Music Information Retrieval (MIR), and session-based
sequential recommendations. Additionally, a subset of sessions were collected
using a uniformly random recommendation setting, enabling their use for
counterfactual evaluation of such sequential recommendations. Finally, we
provide an analysis of user behavior and suggest further research problems
which can be addressed using the dataset.
</p>
<a href="http://arxiv.org/abs/1901.09851" target="_blank">arXiv:1901.09851</a> [<a href="http://arxiv.org/pdf/1901.09851" target="_blank">pdf</a>]

<h2>Early Recognition of Sepsis with Gaussian Process Temporal Convolutional Networks and Dynamic Time Warping. (arXiv:1902.01659v4 [cs.LG] UPDATED)</h2>
<h3>Michael Moor, Max Horn, Bastian Rieck, Damian Roqueiro, Karsten Borgwardt</h3>
<p>Sepsis is a life-threatening host response to infection associated with high
mortality, morbidity, and health costs. Its management is highly time-sensitive
since each hour of delayed treatment increases mortality due to irreversible
organ damage. Meanwhile, despite decades of clinical research, robust
biomarkers for sepsis are missing. Therefore, detecting sepsis early by
utilizing the affluence of high-resolution intensive care records has become a
challenging machine learning problem. Recent advances in deep learning and data
mining promise to deliver a powerful set of tools to efficiently address this
task. This empirical study proposes two novel approaches for the early
detection of sepsis: a deep learning model and a lazy learner based on time
series distances. Our deep learning model employs a temporal convolutional
network that is embedded in a Multi-task Gaussian Process Adapter framework,
making it directly applicable to irregularly-spaced time series data. Our lazy
learner, by contrast, is an ensemble approach that employs dynamic time
warping. We frame the timely detection of sepsis as a supervised time series
classification task. For this, we derive the most recent sepsis definition in
an hourly resolution to provide the first fully accessible early sepsis
detection environment. Seven hours before sepsis onset, our methods improve
area under the precision--recall curve from 0.25 to 0.35/0.40 over the state of
the art. This demonstrates that they are well-suited for detecting sepsis in
the crucial earlier stages when management is most effective.
</p>
<a href="http://arxiv.org/abs/1902.01659" target="_blank">arXiv:1902.01659</a> [<a href="http://arxiv.org/pdf/1902.01659" target="_blank">pdf</a>]

<h2>Foolproof Cooperative Learning. (arXiv:1906.09831v3 [cs.GT] UPDATED)</h2>
<h3>Alexis Jacq, Julien Perolat, Matthieu Geist, Olivier Pietquin</h3>
<p>This paper extends the notion of learning equilibrium in game theory from
matrix games to stochastic games. We introduce Foolproof Cooperative Learning
(FCL), an algorithm that converges to a Tit-for-Tat behavior. It allows
cooperative strategies when played against itself while being not exploitable
by selfish players. We prove that in repeated symmetric games, this algorithm
is a learning equilibrium. We illustrate the behavior of FCL on symmetric
matrix and grid games, and its robustness to selfish learners.
</p>
<a href="http://arxiv.org/abs/1906.09831" target="_blank">arXiv:1906.09831</a> [<a href="http://arxiv.org/pdf/1906.09831" target="_blank">pdf</a>]

<h2>On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift. (arXiv:1908.00261v5 [cs.LG] UPDATED)</h2>
<h3>Alekh Agarwal, Sham M. Kakade, Jason D. Lee, Gaurav Mahajan</h3>
<p>Policy gradient methods are among the most effective methods in challenging
reinforcement learning problems with large state and/or action spaces. However,
little is known about even their most basic theoretical convergence properties,
including: if and how fast they converge to a globally optimal solution or how
they cope with approximation error due to using a restricted class of
parametric policies. This work provides provable characterizations of the
computational, approximation, and sample size properties of policy gradient
methods in the context of discounted Markov Decision Processes (MDPs). We focus
on both: "tabular" policy parameterizations, where the optimal policy is
contained in the class and where we show global convergence to the optimal
policy; and parametric policy classes (considering both log-linear and neural
policy classes), which may not contain the optimal policy and where we provide
agnostic learning results. One central contribution of this work is in
providing approximation guarantees that are average case -- which avoid
explicit worst-case dependencies on the size of state space -- by making a
formal connection to supervised learning under distribution shift. This
characterization shows an important interplay between estimation error,
approximation error, and exploration (as characterized through a precisely
defined condition number).
</p>
<a href="http://arxiv.org/abs/1908.00261" target="_blank">arXiv:1908.00261</a> [<a href="http://arxiv.org/pdf/1908.00261" target="_blank">pdf</a>]

<h2>ATOL: Measure Vectorization for Automatic Topologically-Oriented Learning. (arXiv:1909.13472v3 [cs.CG] UPDATED)</h2>
<h3>Martin Royer (DATASHAPE), Fr&#xe9;d&#xe9;ric Chazal (DATASHAPE), Cl&#xe9;ment Levrard (LPSM (UMR\_8001)), Umeda Yuhei, Ike Yuichi</h3>
<p>Robust topological information commonly comes in the form of a set of
persistence diagrams, finite measures that are in nature uneasy to affix to
generic machine learning frameworks. We introduce a fast, learnt, unsupervised
vectorization method for measures in Euclidean spaces and use it for reflecting
underlying changes in topological behaviour in machine learning contexts. The
algorithm is simple and efficiently discriminates important space regions where
meaningful differences to the mean measure arise. It is proven to be able to
separate clusters of persistence diagrams. We showcase the strength and
robustness of our approach on a number of applications, from emulous and modern
graph collections where the method reaches state-of-the-art performance to a
geometric synthetic dynamical orbits problem. The proposed methodology comes
with a single high level tuning parameter: the total measure encoding budget.
We provide a completely open access software.
</p>
<a href="http://arxiv.org/abs/1909.13472" target="_blank">arXiv:1909.13472</a> [<a href="http://arxiv.org/pdf/1909.13472" target="_blank">pdf</a>]

<h2>Adversarially Robust Few-Shot Learning: A Meta-Learning Approach. (arXiv:1910.00982v3 [cs.LG] UPDATED)</h2>
<h3>Micah Goldblum, Liam Fowl, Tom Goldstein</h3>
<p>Previous work on adversarially robust neural networks for image
classification requires large training sets and computationally expensive
training procedures. On the other hand, few-shot learning methods are highly
vulnerable to adversarial examples. The goal of our work is to produce networks
which both perform well at few-shot classification tasks and are simultaneously
robust to adversarial examples. We develop an algorithm, called Adversarial
Querying (AQ), for producing adversarially robust meta-learners, and we
thoroughly investigate the causes for adversarial vulnerability. Moreover, our
method achieves far superior robust performance on few-shot image
classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer
learning.
</p>
<a href="http://arxiv.org/abs/1910.00982" target="_blank">arXiv:1910.00982</a> [<a href="http://arxiv.org/pdf/1910.00982" target="_blank">pdf</a>]

<h2>A new method for flow-based network intrusion detection using the inverse Potts model. (arXiv:1910.07266v4 [cs.NI] UPDATED)</h2>
<h3>Camila Pontes, Manuela Souza, Jo&#xe3;o Gondim, Matt Bishop, Marcelo Marotta</h3>
<p>Network Intrusion Detection Systems (NIDS) play an important role as tools
for identifying potential network threats. In the context of ever-increasing
traffic volume on computer networks, flow-based NIDS arise as good solutions
for real-time traffic classification. In recent years, different flow-based
classifiers have been proposed using Machine Learning (ML) algorithms.
Nevertheless, classical ML-based classifiers have some limitations. For
instance, they require large amounts of labeled data for training, which might
be difficult to obtain. Additionally, most ML-based classifiers are not capable
of domain adaptation, i.e. after being trained on an specific data
distribution, they are not general enough to be applied to other related data
distributions. And, finally, many of the models inferred by these algorithms
are black boxes, which do not provide explainable results. To overcome these
limitations, we propose a new algorithm, called Energy-based Flow Classifier
(EFC). This anomaly-based classifier uses inverse statistics to infer a
statistical model based on labeled benign examples. We show that EFC is capable
of accurately performing binary flow classification and is more adaptable to
different data distributions than classical ML-based classifiers. Given the
positive results obtained on three different datasets (CIDDS-001, CICIDS17 and
CICDDoS19), we consider EFC to be a promising algorithm to perform robust
flow-based traffic classification.
</p>
<a href="http://arxiv.org/abs/1910.07266" target="_blank">arXiv:1910.07266</a> [<a href="http://arxiv.org/pdf/1910.07266" target="_blank">pdf</a>]

<h2>Knowledge Graph Transfer Network for Few-Shot Recognition. (arXiv:1911.09579v2 [cs.CV] UPDATED)</h2>
<h3>Riquan Chen, Tianshui Chen, Xiaolu Hui, Hefeng Wu, Guanbin Li, Liang Lin</h3>
<p>Few-shot learning aims to learn novel categories from very few samples given
some base categories with sufficient training samples. The main challenge of
this task is the novel categories are prone to dominated by color, texture,
shape of the object or background context (namely specificity), which are
distinct for the given few training samples but not common for the
corresponding categories (see Figure 1). Fortunately, we find that transferring
information of the correlated based categories can help learn the novel
concepts and thus avoid the novel concept being dominated by the specificity.
Besides, incorporating semantic correlations among different categories can
effectively regularize this information transfer. In this work, we represent
the semantic correlations in the form of structured knowledge graph and
integrate this graph into deep neural networks to promote few-shot learning by
a novel Knowledge Graph Transfer Network (KGTN). Specifically, by initializing
each node with the classifier weight of the corresponding category, a
propagation mechanism is learned to adaptively propagate node message through
the graph to explore node interaction and transfer classifier information of
the base categories to those of the novel ones. Extensive experiments on the
ImageNet dataset show significant performance improvement compared with current
leading competitors. Furthermore, we construct an ImageNet-6K dataset that
covers larger scale categories, i.e, 6,000 categories, and experiments on this
dataset further demonstrate the effectiveness of our proposed model. Our codes
and models are available at https://github.com/MyChocer/KGTN .
</p>
<a href="http://arxiv.org/abs/1911.09579" target="_blank">arXiv:1911.09579</a> [<a href="http://arxiv.org/pdf/1911.09579" target="_blank">pdf</a>]

<h2>On Large-Scale Dynamic Topic Modeling with Nonnegative CP Tensor Decomposition. (arXiv:2001.00631v2 [cs.LG] UPDATED)</h2>
<h3>Miju Ahn, Nicole Eikmeier, Jamie Haddock, Lara Kassab, Alona Kryshchenko, Kathryn Leonard, Deanna Needell, R. W. M. A. Madushani, Elena Sizikova, Chuntian Wang</h3>
<p>There is currently an unprecedented demand for large-scale temporal data
analysis due to the explosive growth of data. Dynamic topic modeling has been
widely used in social and data sciences with the goal of learning latent topics
that emerge, evolve, and fade over time. Previous work on dynamic topic
modeling primarily employ the method of nonnegative matrix factorization (NMF),
where slices of the data tensor are each factorized into the product of
lower-dimensional nonnegative matrices. With this approach, however,
information contained in the temporal dimension of the data is often neglected
or underutilized. To overcome this issue, we propose instead adopting the
method of nonnegative CANDECOMP/PARAPAC (CP) tensor decomposition (NNCPD),
where the data tensor is directly decomposed into a minimal sum of outer
products of nonnegative vectors, thereby preserving the temporal information.
The viability of NNCPD is demonstrated through application to both synthetic
and real data, where significantly improved results are obtained compared to
those of typical NMF-based methods. The advantages of NNCPD over such
approaches are studied and discussed. To the best of our knowledge, this is the
first time that NNCPD has been utilized for the purpose of dynamic topic
modeling, and our findings will be transformative for both applications and
further developments.
</p>
<a href="http://arxiv.org/abs/2001.00631" target="_blank">arXiv:2001.00631</a> [<a href="http://arxiv.org/pdf/2001.00631" target="_blank">pdf</a>]

<h2>A Boolean Task Algebra for Reinforcement Learning. (arXiv:2001.01394v2 [cs.LG] UPDATED)</h2>
<h3>Geraud Nangue Tasse, Steven James, Benjamin Rosman</h3>
<p>The ability to compose learned skills to solve new tasks is an important
property of lifelong-learning agents. In this work, we formalise the logical
composition of tasks as a Boolean algebra. This allows us to formulate new
tasks in terms of the negation, disjunction and conjunction of a set of base
tasks. We then show that by learning goal-oriented value functions and
restricting the transition dynamics of the tasks, an agent can solve these new
tasks with no further learning. We prove that by composing these value
functions in specific ways, we immediately recover the optimal policies for all
tasks expressible under the Boolean algebra. We verify our approach in two
domains---including a high-dimensional video game environment requiring
function approximation---where an agent first learns a set of base skills, and
then composes them to solve a super-exponential number of new tasks.
</p>
<a href="http://arxiv.org/abs/2001.01394" target="_blank">arXiv:2001.01394</a> [<a href="http://arxiv.org/pdf/2001.01394" target="_blank">pdf</a>]

<h2>Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits. (arXiv:2002.02518v2 [cs.LG] UPDATED)</h2>
<h3>Jack Parker-Holder, Vu Nguyen, Stephen Roberts</h3>
<p>Many of the recent triumphs in machine learning are dependent on well-tuned
hyperparameters. This is particularly prominent in reinforcement learning (RL)
where a small change in the configuration can lead to failure. Despite the
importance of tuning hyperparameters, it remains expensive and is often done in
a naive and laborious way. A recent solution to this problem is Population
Based Training (PBT) which updates both weights and hyperparameters in a single
training run of a population of agents. PBT has been shown to be particularly
effective in RL, leading to widespread use in the field. However, PBT lacks
theoretical guarantees since it relies on random heuristics to explore the
hyperparameter space. This inefficiency means it typically requires vast
computational resources, which is prohibitive for many small and medium sized
labs. In this work, we introduce the first provably efficient PBT-style
algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to
guide the search in an efficient way, making it possible to discover high
performing hyperparameter configurations with far fewer agents than typically
required by PBT. We show in a series of RL experiments that PB2 is able to
achieve high performance with a modest computational budget.
</p>
<a href="http://arxiv.org/abs/2002.02518" target="_blank">arXiv:2002.02518</a> [<a href="http://arxiv.org/pdf/2002.02518" target="_blank">pdf</a>]

<h2>Representation of Reinforcement Learning Policies in Reproducing Kernel Hilbert Spaces. (arXiv:2002.02863v2 [cs.LG] UPDATED)</h2>
<h3>Bogdan Mazoure, Thang Doan, Tianyu Li, Vladimir Makarenkov, Joelle Pineau, Doina Precup, Guillaume Rabusseau</h3>
<p>We propose a general framework for policy representation for reinforcement
learning tasks. This framework involves finding a low-dimensional embedding of
the policy on a reproducing kernel Hilbert space (RKHS). The usage of RKHS
based methods allows us to derive strong theoretical guarantees on the expected
return of the reconstructed policy. Such guarantees are typically lacking in
black-box models, but are very desirable in tasks requiring stability. We
conduct several experiments on classic RL domains. The results confirm that the
policies can be robustly embedded in a low-dimensional space while the embedded
policy incurs almost no decrease in return.
</p>
<a href="http://arxiv.org/abs/2002.02863" target="_blank">arXiv:2002.02863</a> [<a href="http://arxiv.org/pdf/2002.02863" target="_blank">pdf</a>]

<h2>Compositional Embeddings for Multi-Label One-Shot Learning. (arXiv:2002.04193v3 [cs.LG] UPDATED)</h2>
<h3>Zeqian Li, Michael C. Mozer, Jacob Whitehill</h3>
<p>We present a compositional embedding framework that infers not just a single
class per input image, but a set of classes, in the setting of one-shot
learning. Specifically, we propose and evaluate several novel models consisting
of (1) an embedding function f trained jointly with a "composition" function g
that computes set union operations between the classes encoded in two embedding
vectors; and (2) embedding f trained jointly with a "query" function h that
computes whether the classes encoded in one embedding subsume the classes
encoded in another embedding. In contrast to prior work, these models must both
perceive the classes associated with the input examples and encode the
relationships between different class label sets, and they are trained using
only weak one-shot supervision consisting of the label-set relationships among
training examples. Experiments on the OmniGlot, Open Images, and COCO datasets
show that the proposed compositional embedding models outperform existing
embedding methods. Our compositional embedding models have applications to
multi-label object recognition for both one-shot and supervised learning.
</p>
<a href="http://arxiv.org/abs/2002.04193" target="_blank">arXiv:2002.04193</a> [<a href="http://arxiv.org/pdf/2002.04193" target="_blank">pdf</a>]

<h2>Hold me tight! Influence of discriminative features on deep network boundaries. (arXiv:2002.06349v4 [cs.LG] UPDATED)</h2>
<h3>Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard</h3>
<p>Important insights towards the explainability of neural networks reside in
the characteristics of their decision boundaries. In this work, we borrow tools
from the field of adversarial robustness, and propose a new perspective that
relates dataset features to the distance of samples to the decision boundary.
This enables us to carefully tweak the position of the training samples and
measure the induced changes on the boundaries of CNNs trained on large-scale
vision datasets. We use this framework to reveal some intriguing properties of
CNNs. Specifically, we rigorously confirm that neural networks exhibit a high
invariance to non-discriminative features, and show that the decision
boundaries of a DNN can only exist as long as the classifier is trained with
some features that hold them together. Finally, we show that the construction
of the decision boundary is extremely sensitive to small perturbations of the
training samples, and that changes in certain directions can lead to sudden
invariances in the orthogonal ones. This is precisely the mechanism that
adversarial training uses to achieve robustness.
</p>
<a href="http://arxiv.org/abs/2002.06349" target="_blank">arXiv:2002.06349</a> [<a href="http://arxiv.org/pdf/2002.06349" target="_blank">pdf</a>]

<h2>Generalized Octave Convolutions for Learned Multi-Frequency Image Compression. (arXiv:2002.10032v2 [eess.IV] UPDATED)</h2>
<h3>Mohammad Akbari, Jie Liang, Jingning Han, Chengjie Tu</h3>
<p>Learned image compression has recently shown the potential to outperform all
standard codecs. The state-of-the-art rate-distortion performance has been
achieved by context-adaptive entropy approaches in which hyperprior and
autoregressive models are jointly utilized to effectively capture the spatial
dependencies in the latent representations. However, the latents contain a
mixture of high and low frequency information, which has inefficiently been
represented by features maps of the same spatial resolution in previous works.
In this paper, we propose the first learned multi-frequency image compression
approach that uses the recently developed octave convolutions to factorize the
latents into high and low frequencies. Since the low frequency is represented
by a lower resolution, their spatial redundancy is reduced, which improves the
compression rate. Moreover, octave convolutions impose effective high and low
frequency communication, which can improve the reconstruction quality. We also
develop novel generalized octave convolution and octave transposed-convolution
architectures with internal activation layers to preserve the spatial structure
of the information. Our experiments show that the proposed scheme outperforms
all standard codecs and learning-based methods in both PSNR and MS-SSIM
metrics, and establishes the new state of the art for learned image
compression.
</p>
<a href="http://arxiv.org/abs/2002.10032" target="_blank">arXiv:2002.10032</a> [<a href="http://arxiv.org/pdf/2002.10032" target="_blank">pdf</a>]

<h2>Off-Policy Evaluation and Learning for External Validity under a Covariate Shift. (arXiv:2002.11642v2 [stat.ML] UPDATED)</h2>
<h3>Masahiro Kato, Masatoshi Uehara, Shota Yasui</h3>
<p>We consider the evaluation and training of a new policy for the evaluation
data by using the historical data obtained from a different policy. The goal of
off-policy evaluation (OPE) is to estimate the expected reward of a new policy
over the evaluation data, and that of off-policy learning (OPL) is to find a
new policy that maximizes the expected reward over the evaluation data.
Although the standard OPE and OPL assume the same distribution of covariate
between the historical and evaluation data, there often exists a problem of a
covariate shift, i.e., the distribution of the covariate of the historical data
is different from that of the evaluation data. In this paper, we derive the
efficiency bound of OPE under a covariate shift. Then, we propose doubly robust
and efficient estimators for OPE and OPL under a covariate shift by using an
estimator of the density ratio between the distributions of the historical and
evaluation data. We also discuss other possible estimators and compare their
theoretical properties. Finally, we confirm the effectiveness of the proposed
estimators through experiments.
</p>
<a href="http://arxiv.org/abs/2002.11642" target="_blank">arXiv:2002.11642</a> [<a href="http://arxiv.org/pdf/2002.11642" target="_blank">pdf</a>]

<h2>Self-Supervised Deep Pose Corrections for Robust Visual Odometry. (arXiv:2002.12339v2 [cs.RO] UPDATED)</h2>
<h3>Brandon Wagstaff, Valentin Peretroukhin, Jonathan Kelly</h3>
<p>We present a self-supervised deep pose correction (DPC) network that applies
pose corrections to a visual odometry estimator to improve its accuracy.
Instead of regressing inter-frame pose changes directly, we build on prior work
that uses data-driven learning to regress pose corrections that account for
systematic errors due to violations of modelling assumptions. Our
self-supervised formulation removes any requirement for six-degrees-of-freedom
ground truth and, in contrast to expectations, often improves overall
navigation accuracy compared to a supervised approach. Through extensive
experiments, we show that our self-supervised DPC network can significantly
enhance the performance of classical monocular and stereo odometry estimators
and substantially out-performs state-of-the-art learning-only approaches.
</p>
<a href="http://arxiv.org/abs/2002.12339" target="_blank">arXiv:2002.12339</a> [<a href="http://arxiv.org/pdf/2002.12339" target="_blank">pdf</a>]

<h2>The Implicit and Explicit Regularization Effects of Dropout. (arXiv:2002.12915v3 [cs.LG] UPDATED)</h2>
<h3>Colin Wei, Sham Kakade, Tengyu Ma</h3>
<p>Dropout is a widely-used regularization technique, often required to obtain
state-of-the-art for a number of architectures. This work demonstrates that
dropout introduces two distinct but entangled regularization effects: an
explicit effect (also studied in prior work) which occurs since dropout
modifies the expected training objective, and, perhaps surprisingly, an
additional implicit effect from the stochasticity in the dropout training
update. This implicit regularization effect is analogous to the effect of
stochasticity in small mini-batch stochastic gradient descent. We disentangle
these two effects through controlled experiments. We then derive analytic
simplifications which characterize each effect in terms of the derivatives of
the model and the loss, for deep neural networks. We demonstrate these
simplified, analytic regularizers accurately capture the important aspects of
dropout, showing they faithfully replace dropout in practice.
</p>
<a href="http://arxiv.org/abs/2002.12915" target="_blank">arXiv:2002.12915</a> [<a href="http://arxiv.org/pdf/2002.12915" target="_blank">pdf</a>]

<h2>Taking a hint: How to leverage loss predictors in contextual bandits?. (arXiv:2003.01922v2 [cs.LG] UPDATED)</h2>
<h3>Chen-Yu Wei, Haipeng Luo, Alekh Agarwal</h3>
<p>We initiate the study of learning in contextual bandits with the help of loss
predictors. The main question we address is whether one can improve over the
minimax regret $\mathcal{O}(\sqrt{T})$ for learning over $T$ rounds, when the
total error of the predictor $\mathcal{E} \leq T$ is relatively small. We
provide a complete answer to this question, including upper and lower bounds
for various settings: adversarial versus stochastic environments, known versus
unknown $\mathcal{E}$, and single versus multiple predictors. We show several
surprising results, such as 1) the optimal regret is
$\mathcal{O}(\min\{\sqrt{T}, \sqrt{\mathcal{E}}T^\frac{1}{4}\})$ when
$\mathcal{E}$ is known, a sharp contrast to the standard and better bound
$\mathcal{O}(\sqrt{\mathcal{E}})$ for non-contextual problems (such as
multi-armed bandits); 2) the same bound cannot be achieved if $\mathcal{E}$ is
unknown, but as a remedy, $\mathcal{O}(\sqrt{\mathcal{E}}T^\frac{1}{3})$ is
achievable; 3) with $M$ predictors, a linear dependence on $M$ is necessary,
even if logarithmic dependence is possible for non-contextual problems.

We also develop several novel algorithmic techniques to achieve matching
upper bounds, including 1) a key action remapping technique for optimal regret
with known $\mathcal{E}$, 2) implementing Catoni's robust mean estimator
efficiently via an ERM oracle leading to an efficient algorithm in the
stochastic setting with optimal regret, 3) constructing an underestimator for
$\mathcal{E}$ via estimating the histogram with bins of exponentially
increasing size for the stochastic setting with unknown $\mathcal{E}$, and 4) a
self-referential scheme for learning with multiple predictors, all of which
might be of independent interest.
</p>
<a href="http://arxiv.org/abs/2003.01922" target="_blank">arXiv:2003.01922</a> [<a href="http://arxiv.org/pdf/2003.01922" target="_blank">pdf</a>]

<h2>Bayesian Domain Randomization for Sim-to-Real Transfer. (arXiv:2003.02471v3 [cs.LG] UPDATED)</h2>
<h3>Fabio Muratore, Christian Eilers, Michael Gienger, Jan Peters</h3>
<p>When learning policies for robot control, the required real-world data is
typically prohibitively expensive to acquire, so learning in simulation is a
popular strategy. Unfortunately, such polices are often not transferable to the
real world due to a mismatch between the simulation and reality, called
'reality gap'. Domain randomization methods tackle this problem by randomizing
the physics simulator (source domain) during training according to a
distribution over domain parameters in order to obtain more robust policies
that are able to overcome the reality gap. Most domain randomization approaches
sample the domain parameters from a fixed distribution. This solution is
suboptimal in the context of sim-to-real transferability, since it yields
policies that have been trained without explicitly optimizing for the reward on
the real system (target domain). Additionally, a fixed distribution assumes
there is prior knowledge about the uncertainty over the domain parameters. In
this paper, we propose Bayesian Domain Randomization (BayRn), a black-box
sim-to-real algorithm that solves tasks efficiently by adapting the domain
parameter distribution during learning given sparse data from the real-world
target domain. BayRn uses Bayesian optimization to search the space of source
domain distribution parameters such that this leads to a policy which maximizes
the real-word objective, allowing for adaptive distributions during policy
optimization. We experimentally validate the proposed approach in sim-to-sim as
well as in sim-to-real experiments, comparing against three baseline methods on
two robotic tasks. Our results show that BayRn is able to perform sim-to-real
transfer, while significantly reducing the required prior knowledge.
</p>
<a href="http://arxiv.org/abs/2003.02471" target="_blank">arXiv:2003.02471</a> [<a href="http://arxiv.org/pdf/2003.02471" target="_blank">pdf</a>]

<h2>Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation. (arXiv:2003.03773v3 [cs.CV] UPDATED)</h2>
<h3>Zhedong Zheng, Yi Yang</h3>
<p>This paper focuses on the unsupervised domain adaptation of transferring the
knowledge from the source domain to the target domain in the context of
semantic segmentation. Existing approaches usually regard the pseudo label as
the ground truth to fully exploit the unlabeled target-domain data. Yet the
pseudo labels of the target-domain data are usually predicted by the model
trained on the source domain. Thus, the generated labels inevitably contain the
incorrect prediction due to the discrepancy between the training domain and the
test domain, which could be transferred to the final adapted model and largely
compromises the training process. To overcome the problem, this paper proposes
to explicitly estimate the prediction uncertainty during training to rectify
the pseudo label learning for unsupervised semantic segmentation adaptation.
Given the input image, the model outputs the semantic segmentation prediction
as well as the uncertainty of the prediction. Specifically, we model the
uncertainty via the prediction variance and involve the uncertainty into the
optimization objective. To verify the effectiveness of the proposed method, we
evaluate the proposed method on two prevalent synthetic-to-real semantic
segmentation benchmarks, i.e., GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes, as
well as one cross-city benchmark, i.e., Cityscapes -&gt; Oxford RobotCar. We
demonstrate through extensive experiments that the proposed approach (1)
dynamically sets different confidence thresholds according to the prediction
variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves
significant improvements over the conventional pseudo label learning and yields
competitive performance on all three benchmarks.
</p>
<a href="http://arxiv.org/abs/2003.03773" target="_blank">arXiv:2003.03773</a> [<a href="http://arxiv.org/pdf/2003.03773" target="_blank">pdf</a>]

<h2>Learning by Sampling and Compressing: Efficient Graph Representation Learning with Extremely Limited Annotations. (arXiv:2003.06100v2 [cs.LG] UPDATED)</h2>
<h3>Xiaoming Liu, Qirui Li, Chao Shen, Xi Peng, Yadong Zhou, Xiaohong Guan</h3>
<p>Graph convolution network (GCN) attracts intensive research interest with
broad applications. While existing work mainly focused on designing novel GCN
architectures for better performance, few of them studied a practical yet
challenging problem: How to learn GCNs from data with extremely limited
annotation? In this paper, we propose a new learning method by sampling
strategy and model compression to overcome this challenge. Our approach has
multifold advantages: 1) the adaptive sampling strategy largely suppresses the
GCN training deviation over uniform sampling; 2) compressed GCN-based methods
with a smaller scale of parameters need fewer labeled data to train; 3) the
smaller scale of training data is beneficial to reduce the human resource cost
to label them. We choose six popular GCN baselines and conduct extensive
experiments on three real-world datasets. The results show that by applying our
method, all GCN baselines cut down the annotation requirement by as much as
90$\%$ and compress the scale of parameters more than 6$\times$ without
sacrificing their strong performance. It verifies that the training method
could extend the existing semi-supervised GCN-based methods to the scenarios
with the extremely small scale of labeled data.
</p>
<a href="http://arxiv.org/abs/2003.06100" target="_blank">arXiv:2003.06100</a> [<a href="http://arxiv.org/pdf/2003.06100" target="_blank">pdf</a>]

<h2>Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies. (arXiv:2003.08197v3 [cs.LG] UPDATED)</h2>
<h3>Paul Pu Liang, Manzil Zaheer, Yuan Wang, Amr Ahmed</h3>
<p>Learning continuous representations of discrete objects such as text, users,
movies, and URLs lies at the heart of many applications including language and
user modeling. When using discrete objects as input to neural networks, we
often ignore the underlying structures (e.g. natural groupings and
similarities) and embed the objects independently into individual vectors. As a
result, existing methods do not scale to large vocabulary sizes. In this paper,
we design a simple and efficient embedding algorithm that learns a small set of
anchor embeddings and a sparse transformation matrix. We call our method Anchor
&amp; Transform (ANT) as the embeddings of discrete objects are a sparse linear
combination of the anchors, weighted according to the transformation matrix.
ANT is scalable, flexible, and end-to-end trainable. We further provide a
statistical interpretation of our algorithm as a Bayesian nonparametric prior
for embeddings that encourages sparsity and leverages natural groupings among
objects. By deriving an approximate inference algorithm based on Small Variance
Asymptotics, we obtain a natural extension that automatically learns the
optimal number of anchors instead of having to tune it as a hyperparameter. On
text classification, language modeling, and movie recommendation benchmarks, we
show that ANT is particularly suitable for large vocabulary sizes and
demonstrates stronger performance with fewer parameters (up to 40x compression)
as compared to existing compression baselines.
</p>
<a href="http://arxiv.org/abs/2003.08197" target="_blank">arXiv:2003.08197</a> [<a href="http://arxiv.org/pdf/2003.08197" target="_blank">pdf</a>]

<h2>Across Scales & Across Dimensions: Temporal Super-Resolution using Deep Internal Learning. (arXiv:2003.08872v3 [cs.CV] UPDATED)</h2>
<h3>Liad Pollak Zuckerman, Eyal Naor, George Pisha, Shai Bagon, Michal Irani</h3>
<p>When a very fast dynamic event is recorded with a low-framerate camera, the
resulting video suffers from severe motion blur (due to exposure time) and
motion aliasing (due to low sampling rate in time). True Temporal
Super-Resolution (TSR) is more than just Temporal-Interpolation (increasing
framerate). It can also recover new high temporal frequencies beyond the
temporal Nyquist limit of the input video, thus resolving both motion-blur and
motion-aliasing effects that temporal frame interpolation (as sophisticated as
it maybe) cannot undo. In this paper we propose a "Deep Internal Learning"
approach for true TSR. We train a video-specific CNN on examples extracted
directly from the low-framerate input video. Our method exploits the strong
recurrence of small space-time patches inside a single video sequence, both
within and across different spatio-temporal scales of the video. We further
observe (for the first time) that small space-time patches recur also
across-dimensions of the video sequence - i.e., by swapping the spatial and
temporal dimensions. In particular, the higher spatial resolution of video
frames provides strong examples as to how to increase the temporal resolution
of that video. Such internal video-specific examples give rise to strong
self-supervision, requiring no data but the input video itself. This results in
Zero-Shot Temporal-SR of complex videos, which removes both motion blur and
motion aliasing, outperforming previous supervised methods trained on external
video datasets.
</p>
<a href="http://arxiv.org/abs/2003.08872" target="_blank">arXiv:2003.08872</a> [<a href="http://arxiv.org/pdf/2003.08872" target="_blank">pdf</a>]

<h2>OptiGAN: Generative Adversarial Networks for Goal Optimized Sequence Generation. (arXiv:2004.07534v8 [cs.LG] UPDATED)</h2>
<h3>Mahmoud Hossam, Trung Le, Viet Huynh, Michael Papasimeon, Dinh Phung</h3>
<p>One of the challenging problems in sequence generation tasks is the optimized
generation of sequences with specific desired goals. Current sequential
generative models mainly generate sequences to closely mimic the training data,
without direct optimization of desired goals or properties specific to the
task. We introduce OptiGAN, a generative model that incorporates both
Generative Adversarial Networks (GAN) and Reinforcement Learning (RL) to
optimize desired goal scores using policy gradients. We apply our model to text
and real-valued sequence generation, where our model is able to achieve higher
desired scores out-performing GAN and RL baselines, while not sacrificing
output sample diversity.
</p>
<a href="http://arxiv.org/abs/2004.07534" target="_blank">arXiv:2004.07534</a> [<a href="http://arxiv.org/pdf/2004.07534" target="_blank">pdf</a>]

<h2>CRUDE: Calibrating Regression Uncertainty Distributions Empirically. (arXiv:2005.12496v5 [cs.LG] UPDATED)</h2>
<h3>Eric Zelikman, Christopher Healy, Sharon Zhou, Anand Avati</h3>
<p>The importance of calibrated uncertainty estimates in machine learning is
growing apparent across many fields such as autonomous vehicles, medicine, and
weather and climate forecasting. While there is extensive literature on
uncertainty calibration for classification, the classification findings do not
always translate to regression. As a result, modern models for predicting
uncertainty in regression settings typically produce uncalibrated and
overconfident estimates. To address these gaps, we present a calibration method
for regression settings that does not assume a particular uncertainty
distribution over the error: Calibrating Regression Uncertainty Distributions
Empirically (CRUDE). CRUDE makes the weaker assumption that error distributions
have a constant arbitrary shape across the output space, shifted by predicted
mean and scaled by predicted standard deviation. Across an extensive set of
regression tasks, CRUDE demonstrates consistently sharper, better calibrated,
and more accurate uncertainty estimates than state-of-the-art techniques.
</p>
<a href="http://arxiv.org/abs/2005.12496" target="_blank">arXiv:2005.12496</a> [<a href="http://arxiv.org/pdf/2005.12496" target="_blank">pdf</a>]

<h2>Deep learning method to remove chemical, kinetic and electric artifacts on ISEs. (arXiv:2005.13400v4 [cs.LG] UPDATED)</h2>
<h3>Byunghyun Ban</h3>
<p>We suggest a deep learning based sensor signal processing method to remove
chemical, kinetic and electrical artifacts from ion selective electrodes'
measured values. An ISE is used to investigate the concentration of a specific
ion from aqueous solution, by measuring the Nernst potential along the glass
membrane. However, application of ISE on a mixture of multiple ion has some
problem. First problem is a chemical artifact which is called ion interference
effect. Electrically charged particles interact with each other and flows
through the glass membrane of different ISEs. Second problem is the kinetic
artifact caused by the movement of the liquid. Water molecules collide with the
glass membrane causing abnormal peak values of voltage. The last artifact is
the interference of ISEs. When multiple ISEs are dipped into same solution, one
electrode's signal emission interference voltage measurement of other
electrodes. Therefore, an ISE is recommended to be applied on single-ion
solution, without any other sensors applied at the same time. Deep learning
approach can remove both 3 artifacts at the same time. The proposed method used
5 layers of artificial neural networks to regress correct signal to remove
complex artifacts with one-shot calculation. Its MAPE was less than 1.8% and R2
of regression was 0.997. A randomly chosen value of AI-processed data has MAPE
less than 5% (p-value 0.016).
</p>
<a href="http://arxiv.org/abs/2005.13400" target="_blank">arXiv:2005.13400</a> [<a href="http://arxiv.org/pdf/2005.13400" target="_blank">pdf</a>]

<h2>Neural Methods for Point-wise Dependency Estimation. (arXiv:2006.05553v4 [cs.LG] UPDATED)</h2>
<h3>Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, Ruslan Salakhutdinov</h3>
<p>Since its inception, the neural estimation of mutual information (MI) has
demonstrated the empirical success of modeling expected dependency between
high-dimensional random variables. However, MI is an aggregate statistic and
cannot be used to measure point-wise dependency between different events. In
this work, instead of estimating the expected dependency, we focus on
estimating point-wise dependency (PD), which quantitatively measures how likely
two outcomes co-occur. We show that we can naturally obtain PD when we are
optimizing MI neural variational bounds. However, optimizing these bounds is
challenging due to its large variance in practice. To address this issue, we
develop two methods (free of optimizing MI variational bounds): Probabilistic
Classifier and Density-Ratio Fitting. We demonstrate the effectiveness of our
approaches in 1) MI estimation, 2) self-supervised representation learning, and
3) cross-modal retrieval task.
</p>
<a href="http://arxiv.org/abs/2006.05553" target="_blank">arXiv:2006.05553</a> [<a href="http://arxiv.org/pdf/2006.05553" target="_blank">pdf</a>]

<h2>Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning. (arXiv:2006.06119v4 [cs.CV] UPDATED)</h2>
<h3>Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang</h3>
<p>Dancing to music is one of human's innate abilities since ancient times. In
machine learning research, however, synthesizing dance movements from music is
a challenging problem. Recently, researchers synthesize human motion sequences
through autoregressive models like recurrent neural network (RNN). Such an
approach often generates short sequences due to an accumulation of prediction
errors that are fed back into the neural network. This problem becomes even
more severe in the long motion sequence generation. Besides, the consistency
between dance and music in terms of style, rhythm and beat is yet to be taken
into account during modeling. In this paper, we formalize the music-driven
dance generation as a sequence-to-sequence learning problem and devise a novel
seq2seq architecture to efficiently process long sequences of music features
and capture the fine-grained correspondence between music and dance.
Furthermore, we propose a curriculum learning strategy to alleviate error
accumulation of autoregressive models in long motion sequence generation, which
gently changes the training process from a fully guided teacher-forcing scheme
using the previous ground-truth movements, towards a less guided autoregressive
scheme mostly using the generated movements instead. Extensive experiments
demonstrate that our approach significantly outperforms the existing methods on
automatic metrics and human evaluation. The code and data are now available at
https://github.com/stonyhu/DanceRevolution.
</p>
<a href="http://arxiv.org/abs/2006.06119" target="_blank">arXiv:2006.06119</a> [<a href="http://arxiv.org/pdf/2006.06119" target="_blank">pdf</a>]

<h2>Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v3 [cs.LG] UPDATED)</h2>
<h3>Yehuda Dar, Richard G. Baraniuk</h3>
<p>We study the transfer learning process between two linear regression
problems. An important and timely special case is when the regressors are
overparameterized and perfectly interpolate their training data. We examine a
parameter transfer mechanism whereby a subset of the parameters of the target
task solution are constrained to the values learned for a related source task.
We analytically characterize the generalization error of the target task in
terms of the salient factors in the transfer learning architecture, i.e., the
number of examples available, the number of (free) parameters in each of the
tasks, the number of parameters transferred from the source to target task, and
the correlation between the two tasks. Our non-asymptotic analysis shows that
the generalization error of the target task follows a two-dimensional double
descent trend (with respect to the number of free parameters in each of the
tasks) that is controlled by the transfer learning factors. Our analysis points
to specific cases where the transfer of parameters is beneficial.
</p>
<a href="http://arxiv.org/abs/2006.07002" target="_blank">arXiv:2006.07002</a> [<a href="http://arxiv.org/pdf/2006.07002" target="_blank">pdf</a>]

<h2>Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives. (arXiv:2006.09589v2 [cs.CL] UPDATED)</h2>
<h3>Elisa Kreiss, Zijian Wang, Christopher Potts</h3>
<p>Crime reporting is a prevalent form of journalism with the power to shape
public perceptions and social policies. How does the language of these reports
act on readers? We seek to address this question with the SuspectGuilt Corpus
of annotated crime stories from English-language newspapers in the U.S. For
SuspectGuilt, annotators read short crime articles and provided text-level
ratings concerning the guilt of the main suspect as well as span-level
annotations indicating which parts of the story they felt most influenced their
ratings. SuspectGuilt thus provides a rich picture of how linguistic choices
affect subjective guilt judgments. In addition, we use SuspectGuilt to train
and assess predictive models, and show that these models benefit from genre
pretraining and joint supervision from the text-level ratings and span-level
annotations. Such models might be used as tools for understanding the societal
effects of crime reporting.
</p>
<a href="http://arxiv.org/abs/2006.09589" target="_blank">arXiv:2006.09589</a> [<a href="http://arxiv.org/pdf/2006.09589" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. (arXiv:2006.09882v4 [cs.CV] UPDATED)</h2>
<h3>Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin</h3>
<p>Unsupervised image representations have significantly reduced the gap with
supervised pretraining, notably with the recent achievements of contrastive
learning methods. These contrastive methods typically work online and rely on a
large number of explicit pairwise feature comparisons, which is computationally
challenging. In this paper, we propose an online algorithm, SwAV, that takes
advantage of contrastive methods without requiring to compute pairwise
comparisons. Specifically, our method simultaneously clusters the data while
enforcing consistency between cluster assignments produced for different
augmentations (or views) of the same image, instead of comparing features
directly as in contrastive learning. Simply put, we use a swapped prediction
mechanism where we predict the cluster assignment of a view from the
representation of another view. Our method can be trained with large and small
batches and can scale to unlimited amounts of data. Compared to previous
contrastive methods, our method is more memory efficient since it does not
require a large memory bank or a special momentum network. In addition, we also
propose a new data augmentation strategy, multi-crop, that uses a mix of views
with different resolutions in place of two full-resolution views, without
increasing the memory or compute requirements much. We validate our findings by
achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as
surpassing supervised pretraining on all the considered transfer tasks.
</p>
<a href="http://arxiv.org/abs/2006.09882" target="_blank">arXiv:2006.09882</a> [<a href="http://arxiv.org/pdf/2006.09882" target="_blank">pdf</a>]

<h2>Anomaly Detection with Deep Perceptual Autoencoders. (arXiv:2006.13265v2 [cs.CV] UPDATED)</h2>
<h3>Nina Tuluptceva, Bart Bakker, Irina Fedulova, Heinrich Schulz, Dmitry V. Dylov</h3>
<p>Anomaly detection is the problem of recognizing abnormal inputs based on the
seen examples of normal data. Despite recent advances of deep learning in
recognizing image anomalies, these methods still prove incapable of handling
complex medical images, such as barely visible abnormalities in chest X-rays
and metastases in lymph nodes. To address this problem, we introduce a new
powerful method of image anomaly detection. It relies on the classical
autoencoder approach with a re-designed training pipeline to handle
high-resolution, complex images and a robust way of computing an image
abnormality score. We revisit the very problem statement of fully unsupervised
anomaly detection, where no abnormal examples at all are provided during the
model setup. We propose to relax this unrealistic assumption by using a very
small number of anomalies of confined variability merely to initiate the search
of hyperparameters of the model. We evaluate our solution on natural image
datasets with a known benchmark, as well as on two medical datasets containing
radiology and digital pathology images. The proposed approach suggests a new
strong baseline for image anomaly detection and outperforms state-of-the-art
approaches in complex medical image analysis tasks.
</p>
<a href="http://arxiv.org/abs/2006.13265" target="_blank">arXiv:2006.13265</a> [<a href="http://arxiv.org/pdf/2006.13265" target="_blank">pdf</a>]

<h2>LFQ: Online Learning of Per-flow Queuing Policies using Deep Reinforcement Learning. (arXiv:2007.02735v3 [cs.NI] UPDATED)</h2>
<h3>Maximilian Bachl, Joachim Fabini, Tanja Zseby</h3>
<p>The increasing number of different, incompatible congestion control
algorithms has led to an increased deployment of fair queuing. Fair queuing
isolates each network flow and can thus guarantee fairness for each flow even
if the flows' congestion controls are not inherently fair. So far, each queue
in the fair queuing system either has a fixed, static maximum size or is
managed by an Active Queue Management (AQM) algorithm like CoDel. In this paper
we design an AQM mechanism (Learning Fair Qdisc (LFQ)) that dynamically learns
the optimal buffer size for each flow according to a specified reward function
online. We show that our Deep Learning based algorithm can dynamically assign
the optimal queue size to each flow depending on its congestion control, delay
and bandwidth. Comparing to competing fair AQM schedulers, it provides
significantly smaller queues while achieving the same or higher throughput.
</p>
<a href="http://arxiv.org/abs/2007.02735" target="_blank">arXiv:2007.02735</a> [<a href="http://arxiv.org/pdf/2007.02735" target="_blank">pdf</a>]

<h2>Latent-space time evolution of non-intrusive reduced-order models using Gaussian process emulation. (arXiv:2007.12167v2 [physics.comp-ph] UPDATED)</h2>
<h3>Romit Maulik, Themistoklis Botsas, Nesar Ramachandra, Lachlan Robert Mason, Indranil Pan</h3>
<p>Non-intrusive reduced-order models (ROMs) have recently generated
considerable interest for constructing computationally efficient counterparts
of nonlinear dynamical systems emerging from various domain sciences. They
provide a low-dimensional emulation framework for systems that may be
intrinsically high-dimensional. This is accomplished by utilizing a
construction algorithm that is purely data-driven. It is no surprise,
therefore, that the algorithmic advances of machine learning have led to
non-intrusive ROMs with greater accuracy and computational gains. However, in
bypassing the utilization of an equation-based evolution, it is often seen that
the interpretability of the ROM framework suffers. This becomes more
problematic when black-box deep learning methods are used which are notorious
for lacking robustness outside the physical regime of the observed data. In
this article, we propose the use of a novel latent-space interpolation
algorithm based on Gaussian process regression. Notably, this reduced-order
evolution of the system is parameterized by control parameters to allow for
interpolation in space. The use of this procedure also allows for a continuous
interpretation of time which allows for temporal interpolation. The latter
aspect provides information, with quantified uncertainty, about full-state
evolution at a finer resolution than that utilized for training the ROMs. We
assess the viability of this algorithm for an advection-dominated system given
by the inviscid shallow water equations.
</p>
<a href="http://arxiv.org/abs/2007.12167" target="_blank">arXiv:2007.12167</a> [<a href="http://arxiv.org/pdf/2007.12167" target="_blank">pdf</a>]

<h2>EagerNet: Early Predictions of Neural Networks for Computationally Efficient Intrusion Detection. (arXiv:2007.13444v2 [cs.LG] UPDATED)</h2>
<h3>Fares Meghdouri, Maximilian Bachl, Tanja Zseby</h3>
<p>Fully Connected Neural Networks (FCNNs) have been the core of most
state-of-the-art Machine Learning (ML) applications in recent years and also
have been widely used for Intrusion Detection Systems (IDSs). Experimental
results from the last years show that generally deeper neural networks with
more layers perform better than shallow models. Nonetheless, with the growing
number of layers, obtaining fast predictions with less resources has become a
difficult task despite the use of special hardware such as GPUs. We propose a
new architecture to detect network attacks with minimal resources. The
architecture is able to deal with either binary or multiclass classification
problems and trades prediction speed for the accuracy of the network. We
evaluate our proposal with two different network intrusion detection datasets.
Results suggest that it is possible to obtain comparable accuracies to simple
FCNNs without evaluating all layers for the majority of samples, thus obtaining
early predictions and saving energy and computational efforts.
</p>
<a href="http://arxiv.org/abs/2007.13444" target="_blank">arXiv:2007.13444</a> [<a href="http://arxiv.org/pdf/2007.13444" target="_blank">pdf</a>]

<h2>Byzantine-Fault-Tolerant Consensus via Reinforcement Learning for Permissioned Blockchain Implemented in a V2X Network. (arXiv:2007.13957v4 [cs.NI] UPDATED)</h2>
<h3>Seungmo Kim, Ahmed S. Ibrahim</h3>
<p>Blockchain has been forming the central piece of various types of
vehicle-to-everything (V2X) network for trusted data exchange. Recently,
permissioned blockchains garner particular attention thanks to their improved
scalability and diverse needs from different organizations. One representative
example of permissioned blockchain is Hyperledger Fabric ("Fabric"). Due to its
unique execute-order procedure, there is a critical need for a client to select
an optimal number of peers. The interesting problem that this paper targets to
address is the tradeoff in the number of peers: a too large number will degrade
scalability while a too small number will make the network vulnerable to faulty
nodes. This optimization issue gets especially challenging in V2X networks due
to mobility of nodes: a transaction must be executed and the associated block
must be committed before the vehicle leaves a network. To this end, this paper
proposes an optimal peers selection mechanism based on reinforcement learning
(RL) to keep a Fabric-empowered V2X network impervious to dynamicity due to
mobility. We model the RL as a contextual multi-armed bandit (MAB) problem. The
results demonstrate the outperformance of the proposed scheme.
</p>
<a href="http://arxiv.org/abs/2007.13957" target="_blank">arXiv:2007.13957</a> [<a href="http://arxiv.org/pdf/2007.13957" target="_blank">pdf</a>]

<h2>Communication-Efficient Federated Learning via Optimal Client Sampling. (arXiv:2007.15197v2 [cs.LG] UPDATED)</h2>
<h3>Monica Ribero, Haris Vikalo</h3>
<p>Federated learning (FL) ameliorates privacy concerns in settings where a
central server coordinates learning from data distributed across many clients.
The clients train locally and communicate the models they learn to the server;
aggregation of local models requires frequent communication of large amounts of
information between the clients and the central server. We propose a novel,
simple and efficient way of updating the central model in
communication-constrained settings based on collecting models from clients with
informative updates and estimating local updates that were not communicated. In
particular, modeling the progression of model's weights by an
Ornstein-Uhlenbeck process allows us to derive an optimal sampling strategy for
selecting a subset of clients with significant weight updates. The central
server collects updated local models from only the selected clients and
combines them with estimated model updates of the clients that were not
selected for communication. We test this policy on a synthetic dataset for
logistic regression and two FL benchmarks, namely, a classification task on
EMNIST and a realistic language modeling task using the Shakespeare dataset.
The results demonstrate that the proposed framework provides significant
reduction in communication while maintaining competitive or achieving superior
performance compared to a baseline. Our method represents a new line of
strategies for communication-efficient FL that is orthogonal to the existing
user-local methods such as quantization or sparsification, thus complementing
rather than aiming to replace those existing methods.
</p>
<a href="http://arxiv.org/abs/2007.15197" target="_blank">arXiv:2007.15197</a> [<a href="http://arxiv.org/pdf/2007.15197" target="_blank">pdf</a>]

<h2>Forecasting Photovoltaic Power Production using a Deep Learning Sequence to Sequence Model with Attention. (arXiv:2008.02775v2 [cs.LG] UPDATED)</h2>
<h3>Elizaveta Kharlova, Daniel May, Petr Musilek (University of Alberta)</h3>
<p>Rising penetration levels of (residential) photovoltaic (PV) power as
distributed energy resource pose a number of challenges to the electricity
infrastructure. High quality, general tools to provide accurate forecasts of
power production are urgently needed. In this article, we propose a supervised
deep learning model for end-to-end forecasting of PV power production. The
proposed model is based on two seminal concepts that led to significant
performance improvements of deep learning approaches in other sequence-related
fields, but not yet in the area of time series prediction: the sequence to
sequence architecture and attention mechanism as a context generator. The
proposed model leverages numerical weather predictions and high-resolution
historical measurements to forecast a binned probability distribution over the
prognostic time intervals, rather than the expected values of the prognostic
variable. This design offers significant performance improvements compared to
common baseline approaches, such as fully connected neural networks and
one-block long short-term memory architectures. Using normalized root mean
square error based forecast skill score as a performance indicator, the
proposed approach is compared to other models. The results show that the new
design performs at or above the current state of the art of PV power
forecasting.
</p>
<a href="http://arxiv.org/abs/2008.02775" target="_blank">arXiv:2008.02775</a> [<a href="http://arxiv.org/pdf/2008.02775" target="_blank">pdf</a>]

<h2>GPU-Accelerated Primal Learning for Extremely Fast Large-Scale Classification. (arXiv:2008.03433v2 [cs.LG] UPDATED)</h2>
<h3>John T. Halloran, David M. Rocke</h3>
<p>One of the most efficient methods to solve L2-regularized primal problems,
such as logistic regression and linear support vector machine (SVM)
classification, is the widely used trust region Newton algorithm, TRON. While
TRON has recently been shown to enjoy substantial speedups on shared-memory
multi-core systems, exploiting graphical processing units (GPUs) to speed up
the method is significantly more difficult, owing to the highly complex and
heavily sequential nature of the algorithm. In this work, we show that using
judicious GPU-optimization principles, TRON training time for different losses
and feature representations may be drastically reduced. For sparse feature
sets, we show that using GPUs to train logistic regression classifiers in
LIBLINEAR is up to an order-of-magnitude faster than solely using
multithreading. For dense feature sets--which impose far more stringent memory
constraints--we show that GPUs substantially reduce the lengthy SVM learning
times required for state-of-the-art proteomics analysis, leading to dramatic
improvements over recently proposed speedups. Furthermore, we show how GPU
speedups may be mixed with multithreading to enable such speedups when the
dataset is too large for GPU memory requirements; on a massive dense proteomics
dataset of nearly a quarter-billion data instances, these mixed-architecture
speedups reduce SVM analysis time from over half a week to less than a single
day while using limited GPU memory.
</p>
<a href="http://arxiv.org/abs/2008.03433" target="_blank">arXiv:2008.03433</a> [<a href="http://arxiv.org/pdf/2008.03433" target="_blank">pdf</a>]

<h2>Very Deep Transformers for Neural Machine Translation. (arXiv:2008.07772v2 [cs.CL] UPDATED)</h2>
<h3>Xiaodong Liu, Kevin Duh, Liyuan Liu, Jianfeng Gao</h3>
<p>We explore the application of very deep Transformer models for Neural Machine
Translation (NMT). Using a simple yet effective initialization technique that
stabilizes training, we show that it is feasible to build standard
Transformer-based models with up to 60 encoder layers and 12 decoder layers.
These deep models outperform their baseline 6-layer counterparts by as much as
2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14
English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14
English-German (30.1 BLEU).The code and trained models will be publicly
available at: https://github.com/namisan/exdeep-nmt.
</p>
<a href="http://arxiv.org/abs/2008.07772" target="_blank">arXiv:2008.07772</a> [<a href="http://arxiv.org/pdf/2008.07772" target="_blank">pdf</a>]

<h2>Deep Relighting Networks for Image Light Source Manipulation. (arXiv:2008.08298v2 [cs.CV] UPDATED)</h2>
<h3>Li-Wen Wang, Wan-Chi Siu, Zhi-Song Liu, Chu-Tak Li, Daniel P.K. Lun</h3>
<p>Manipulating the light source of given images is an interesting task and
useful in various applications, including photography and cinematography.
Existing methods usually require additional information like the geometric
structure of the scene, which may not be available for most images. In this
paper, we formulate the single image relighting task and propose a novel Deep
Relighting Network (DRN) with three parts: 1) scene reconversion, which aims to
reveal the primary scene structure through a deep auto-encoder network, 2)
shadow prior estimation, to predict light effect from the new light direction
through adversarial learning, and 3) re-renderer, to combine the primary
structure with the reconstructed shadow view to form the required estimation
under the target light source. Experimental results show that the proposed
method outperforms other possible methods, both qualitatively and
quantitatively. Specifically, the proposed DRN has achieved the best PSNR in
the "AIM2020 - Any to one relighting challenge" of the 2020 ECCV conference.
</p>
<a href="http://arxiv.org/abs/2008.08298" target="_blank">arXiv:2008.08298</a> [<a href="http://arxiv.org/pdf/2008.08298" target="_blank">pdf</a>]

<h2>Discrete Word Embedding for Logical Natural Language Understanding. (arXiv:2008.11649v2 [cs.CL] UPDATED)</h2>
<h3>Masataro Asai, Zilu Tang</h3>
<p>We propose an unsupervised neural model for learning a discrete embedding of
words. Unlike existing discrete embeddings, our binary embedding supports
vector arithmetic operations similar to continuous embeddings. Our embedding
represents each word as a set of propositional statements describing a
transition rule in classical/STRIPS planning formalism. This makes the
embedding directly compatible with symbolic, state of the art classical
planning solvers.
</p>
<a href="http://arxiv.org/abs/2008.11649" target="_blank">arXiv:2008.11649</a> [<a href="http://arxiv.org/pdf/2008.11649" target="_blank">pdf</a>]

<h2>Multi-Attention-Network for Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2009.02130v2 [eess.IV] UPDATED)</h2>
<h3>Rui Li, Shunyi Zheng, Chenxi Duan, Ce Zhang, Jianlin Su</h3>
<p>Semantic segmentation of remote sensing images plays an important role in
land resource management, yield estimation, and economic assessment. Even
though the semantic segmentation of remote sensing images has been prominently
improved by convolutional neural networks, there are still several limitations
contained in standard models. First, for encoder-decoder architectures like
U-Net, the utilization of multi-scale features causes overuse of information,
where similar low-level features are exploited at multiple scales for multiple
times. Second, long-range dependencies of feature maps are not sufficiently
explored, leading to feature representations associated with each semantic
class are not optimal. Third, despite the dot-product attention mechanism has
been introduced and harnessed widely in semantic segmentation to model
long-range dependencies, the high time and space complexities of attention
impede the usage of attention in application scenarios with large input. In
this paper, we proposed a Multi-Attention-Network (MANet) to remedy these
drawbacks, which extracts contextual dependencies by multi efficient attention
mechanisms. A novel attention mechanism named kernel attention with linear
complexity is proposed to alleviate the high computational demand of attention.
Based on kernel attention and channel attention, we integrate local feature
maps extracted by ResNeXt-101 with their corresponding global dependencies, and
adaptively signalize interdependent channel maps. Experiments conducted on two
remote sensing image datasets captured by variant satellites demonstrate that
the performance of our MANet transcends the DeepLab V3+, PSPNet, FastFCN, and
other baseline algorithms.
</p>
<a href="http://arxiv.org/abs/2009.02130" target="_blank">arXiv:2009.02130</a> [<a href="http://arxiv.org/pdf/2009.02130" target="_blank">pdf</a>]

<h2>QR-MIX: Distributional Value Function Factorisation for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2009.04197v4 [cs.LG] UPDATED)</h2>
<h3>Jian Hu, Seth Austin Harding, Haibin Wu, Siyue Hu, Shih-wei Liao</h3>
<p>In Cooperative Multi-Agent Reinforcement Learning (MARL) and under the
setting of Centralized Training with Decentralized Execution (CTDE), agents
observe and interact with their environment locally and independently. With
local observation and random sampling, the randomness in rewards and
observations leads to randomness in long-term returns. Existing methods such as
Value Decomposition Network (VDN) and QMIX estimate the value of long-term
returns as a scalar that does not contain the information of randomness. Our
proposed model QR-MIX introduces quantile regression, modeling joint
state-action values as a distribution, combining QMIX with Implicit Quantile
Network (IQN). However, the monotonicity in QMIX limits the expression of joint
state-action value distribution and may lead to incorrect estimation results in
non-monotonic cases. Therefore, we proposed a flexible loss function to
approximate the monotonicity found in QMIX. Our model is not only more tolerant
of the randomness of returns, but also more tolerant of the randomness of
monotonic constraints. The experimental results demonstrate that QR-MIX
outperforms the previous state-of-the-art method QMIX in the StarCraft
Multi-Agent Challenge (SMAC) environment.
</p>
<a href="http://arxiv.org/abs/2009.04197" target="_blank">arXiv:2009.04197</a> [<a href="http://arxiv.org/pdf/2009.04197" target="_blank">pdf</a>]

<h2>Segmentation and Defect Classification of the Power Line Insulators: A Deep Learning-based Approach. (arXiv:2009.10163v2 [cs.CV] UPDATED)</h2>
<h3>Arman Alahyari, Anton Hinneck, Rahim Tariverdi, David Pozo</h3>
<p>Power transmission networks physically connect the power generators to the
electric consumers. Such systems extend over hundreds of kilometers. There are
many components in the transmission infrastructure that require a proper
inspection to guarantee flawless performance and reliable delivery, which, if
done manually, can be very costly and time consuming. One essential component
is the insulator. Its failure can cause an interruption of the entire
transmission line or a widespread power failure. Automated fault detection
could significantly decrease inspection time and related costs. Recently,
several works have been proposed based on convolutional neural networks, which
address the issue mentioned above. However, existing studies focus on a
specific type of insulator faults. Thus, in this study, we introduce a
two-stage model that segments insulators from their background to then classify
their states based on four different categories, namely: healthy, broken,
burned/corroded and missing cap. The test results show that the proposed
approach can realize the effective segmentation of insulators and achieve high
accuracy in detecting several types of faults.
</p>
<a href="http://arxiv.org/abs/2009.10163" target="_blank">arXiv:2009.10163</a> [<a href="http://arxiv.org/pdf/2009.10163" target="_blank">pdf</a>]

<h2>Kernel Based Progressive Distillation for Adder Neural Networks. (arXiv:2009.13044v3 [cs.CV] UPDATED)</h2>
<h3>Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, Yunhe Wang</h3>
<p>Adder Neural Networks (ANNs) which only contain additions bring us a new way
of developing deep neural networks with low energy consumption. Unfortunately,
there is an accuracy drop when replacing all convolution filters by adder
filters. The main reason here is the optimization difficulty of ANNs using
$\ell_1$-norm, in which the estimation of gradient in back propagation is
inaccurate. In this paper, we present a novel method for further improving the
performance of ANNs without increasing the trainable parameters via a
progressive kernel based knowledge distillation (PKKD) method. A convolutional
neural network (CNN) with the same architecture is simultaneously initialized
and trained as a teacher network, features and weights of ANN and CNN will be
transformed to a new space to eliminate the accuracy drop. The similarity is
conducted in a higher-dimensional space to disentangle the difference of their
distributions using a kernel based method. Finally, the desired ANN is learned
based on the information from both the ground-truth and teacher, progressively.
The effectiveness of the proposed method for learning ANN with higher
performance is then well-verified on several benchmarks. For instance, the
ANN-50 trained using the proposed PKKD method obtains a 76.8\% top-1 accuracy
on ImageNet dataset, which is 0.6\% higher than that of the ResNet-50.
</p>
<a href="http://arxiv.org/abs/2009.13044" target="_blank">arXiv:2009.13044</a> [<a href="http://arxiv.org/pdf/2009.13044" target="_blank">pdf</a>]

<h2>Towards Heterogeneous Multi-Agent Reinforcement Learning with Graph Neural Networks. (arXiv:2009.13161v2 [cs.AI] UPDATED)</h2>
<h3>Douglas De Rizzo Meneghetti, Reinaldo Augusto da Costa Bianchi</h3>
<p>This work proposes a neural network architecture that learns policies for
multiple agent classes in a heterogeneous multi-agent reinforcement setting.
The proposed network uses directed labeled graph representations for states,
encodes feature vectors of different sizes for different entity classes, uses
relational graph convolution layers to model different communication channels
between entity types and learns distinct policies for different agent classes,
sharing parameters wherever possible. Results have shown that specializing the
communication channels between entity classes is a promising step to achieve
higher performance in environments composed of heterogeneous entities.
</p>
<a href="http://arxiv.org/abs/2009.13161" target="_blank">arXiv:2009.13161</a> [<a href="http://arxiv.org/pdf/2009.13161" target="_blank">pdf</a>]

<h2>Adaptive Sampling for Best Policy Identification in Markov Decision Processes. (arXiv:2009.13405v2 [stat.ML] UPDATED)</h2>
<h3>Aymen Al Marjani, Alexandre Proutiere</h3>
<p>We investigate the problem of best-policy identification in discounted Markov
Decision Processes (MDPs) when the learner has access to a generative model.
The objective is to devise a learning algorithm returning the best policy as
early as possible. We first derive a problem-specific lower bound of the sample
complexity satisfied by any learning algorithm. This lower bound corresponds to
an optimal sample allocation that solves a non-convex program, and hence, is
hard to exploit in the design of efficient algorithms. We then provide a simple
and tight upper bound of the sample complexity lower bound, whose corresponding
nearly-optimal sample allocation becomes explicit. The upper bound depends on
specific functionals of the MDP such as the sub-optimality gaps and the
variance of the next-state value function, and thus really captures the
hardness of the MDP. Finally, we devise KLB-TS (KL Ball Track-and-Stop), an
algorithm tracking this nearly-optimal allocation, and provide asymptotic
guarantees for its sample complexity (both almost surely and in expectation).
The advantages of KLB-TS against state-of-the-art algorithms are discussed and
illustrated numerically.
</p>
<a href="http://arxiv.org/abs/2009.13405" target="_blank">arXiv:2009.13405</a> [<a href="http://arxiv.org/pdf/2009.13405" target="_blank">pdf</a>]

<h2>A Supervised Machine Learning Approach for Accelerating the Design of Particulate Composites: Application to Thermal Conductivity. (arXiv:2010.00041v2 [physics.comp-ph] UPDATED)</h2>
<h3>Mohammad Saber Hashemi, Masoud Safdari, Azadeh Sheidaei</h3>
<p>A supervised machine learning (ML) based computational methodology for the
design of particulate multifunctional composite materials with desired thermal
conductivity (TC) is presented. The design variables are physical descriptors
of the material microstructure that directly link microstructure to the
material's properties. A sufficiently large and uniformly sampled database was
generated based on the Sobol sequence. Microstructures were realized using an
efficient dense packing algorithm, and the TCs were obtained using our
previously developed Fast Fourier Transform (FFT) homogenization method. Our
optimized ML method is trained over the generated database and establishes the
complex relationship between the structure and properties. Finally, the
application of the trained ML model in the inverse design of a new class of
composite materials, liquid metal (LM) elastomer, with desired TC is discussed.
The results show that the surrogate model is accurate in predicting the
microstructure behavior with respect to high-fidelity FFT simulations, and
inverse design is robust in finding microstructure parameters according to case
studies.
</p>
<a href="http://arxiv.org/abs/2010.00041" target="_blank">arXiv:2010.00041</a> [<a href="http://arxiv.org/pdf/2010.00041" target="_blank">pdf</a>]

<h2>Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting. (arXiv:2010.00294v3 [cs.CL] UPDATED)</h2>
<h3>Anshul Wadhawan</h3>
<p>This paper presents the approach that we employed to tackle the EMNLP
WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English
Tweets. The task is to develop a system that automatically identifies whether
an English Tweet related to the novel coronavirus (COVID-19) is informative or
not. We solve the task in three stages. The first stage involves pre-processing
the dataset by filtering only relevant information. This is followed by
experimenting with multiple deep learning models like CNNs, RNNs and
Transformer based models. In the last stage, we propose an ensemble of the best
model trained on different subsets of the provided dataset. Our final approach
achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score
as the evaluation criteria.
</p>
<a href="http://arxiv.org/abs/2010.00294" target="_blank">arXiv:2010.00294</a> [<a href="http://arxiv.org/pdf/2010.00294" target="_blank">pdf</a>]

<h2>"Did you really mean what you said?" : Sarcasm Detection in Hindi-English Code-Mixed Data using Bilingual Word Embeddings. (arXiv:2010.00310v3 [cs.CL] UPDATED)</h2>
<h3>Akshita Aggarwal, Anshul Wadhawan, Anshima Chaudhary, Kavita Maurya</h3>
<p>With the increased use of social media platforms by people across the world,
many new interesting NLP problems have come into existence. One such being the
detection of sarcasm in the social media texts. We present a corpus of tweets
for training custom word embeddings and a Hinglish dataset labelled for sarcasm
detection. We propose a deep learning based approach to address the issue of
sarcasm detection in Hindi-English code mixed tweets using bilingual word
embeddings derived from FastText and Word2Vec approaches. We experimented with
various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with
and without attention). We were able to outperform all state-of-the-art
performances with our deep learning models, with attention based Bi-directional
LSTMs giving the best performance exhibiting an accuracy of 78.49%.
</p>
<a href="http://arxiv.org/abs/2010.00310" target="_blank">arXiv:2010.00310</a> [<a href="http://arxiv.org/pdf/2010.00310" target="_blank">pdf</a>]

<h2>PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet Lab Protocols using Structured Learning Ensemble and Contextualised Embeddings. (arXiv:2010.02142v2 [cs.CL] UPDATED)</h2>
<h3>Janvijay Singh, Anshul Wadhawan</h3>
<p>In this paper, we describe the approach that we employed to address the task
of Entity Recognition over Wet Lab Protocols -- a shared task in EMNLP
WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase,
we experiment with various contextualised word embeddings (like Flair,
BERT-based) and a BiLSTM-CRF model to arrive at the best-performing
architecture. In the second phase, we create an ensemble composed of eleven
BiLSTM-CRF models. The individual models are trained on random train-validation
splits of the complete dataset. Here, we also experiment with different output
merging schemes, including Majority Voting and Structured Learning Ensembling
(SLE). Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for
the partial and exact match of the entity spans, respectively. We were ranked
first and second, in terms of partial and exact match, respectively.
</p>
<a href="http://arxiv.org/abs/2010.02142" target="_blank">arXiv:2010.02142</a> [<a href="http://arxiv.org/pdf/2010.02142" target="_blank">pdf</a>]

<h2>DaNetQA: a yes/no Question Answering Dataset for the Russian Language. (arXiv:2010.02605v2 [cs.CL] UPDATED)</h2>
<h3>Taisia Glushkova, Alexey Machnev, Alena Fenogenova, Tatiana Shavrina, Ekaterina Artemova, Dmitry I. Ignatov</h3>
<p>DaNetQA, a new question-answering corpus, follows (Clark et. al, 2019)
design: it comprises natural yes/no questions. Each question is paired with a
paragraph from Wikipedia and an answer, derived from the paragraph. The task is
to take both the question and a paragraph as input and come up with a yes/no
answer, i.e. to produce a binary output. In this paper, we present a
reproducible approach to DaNetQA creation and investigate transfer learning
methods for task and language transferring. For task transferring we leverage
three similar sentence modelling tasks: 1) a corpus of paraphrases,
Paraphraser, 2) an NLI task, for which we use the Russian part of XNLI, 3)
another question answering task, SberQUAD. For language transferring we use
English to Russian translation together with multilingual language fine-tuning.
</p>
<a href="http://arxiv.org/abs/2010.02605" target="_blank">arXiv:2010.02605</a> [<a href="http://arxiv.org/pdf/2010.02605" target="_blank">pdf</a>]

<h2>Deep Learning in Diabetic Foot Ulcers Detection: A Comprehensive Evaluation. (arXiv:2010.03341v2 [cs.CV] UPDATED)</h2>
<h3>Moi Hoon Yap, Ryo Hachiuma, Azadeh Alavi, Raphael Brungel, Manu Goyal, Hongtao Zhu, Bill Cassidy, Johannes Ruckert, Moshe Olshansky, Xiao Huang, Hideo Saito, Saeed Hassanpour, Christoph M. Friedrich, David Ascher, Anping Song, Hiroki Kajita, David Gillespie, Neil D. Reeves, Joseph Pappachan, Claire O&#x27;Shea, Eibe Frank</h3>
<p>There has been a substantial amount of research on computer methods and
technology for the detection and recognition of diabetic foot ulcers (DFUs),
but there is a lack of systematic comparisons of state-of-the-art deep learning
object detection frameworks applied to this problem. With recent development
and data sharing performed as part of the DFU Challenge (DFUC2020) such a
comparison becomes possible: DFUC2020 provided participants with a
comprehensive dataset consisting of 2,000 images for training each method and
2,000 images for testing them. The following deep learning-based algorithms are
compared in this paper: Faster R-CNN, three variants of Faster R-CNN and an
ensemble method; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention
Network. For each deep learning method, we provide a detailed description of
model architecture, parameter settings for training and additional stages
including pre-processing, data augmentation and post-processing. We provide a
comprehensive evaluation for each method. All the methods required a data
augmentation stage to increase the number of images available for training and
a post-processing stage to remove false positives. The best performance is
obtained Deformable Convolution, a variant of Faster R-CNN, with a mAP of
0.6940 and an F1-Score of 0.7434. Finally, we demonstrate that the ensemble
method based on different deep learning methods can enhanced the F1-Score but
not the mAP. Our results show that state-of-the-art deep learning methods can
detect DFU with some accuracy, but there are many challenges ahead before they
can be implemented in real world settings.
</p>
<a href="http://arxiv.org/abs/2010.03341" target="_blank">arXiv:2010.03341</a> [<a href="http://arxiv.org/pdf/2010.03341" target="_blank">pdf</a>]

<h2>Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data. (arXiv:2010.03622v2 [cs.LG] UPDATED)</h2>
<h3>Colin Wei, Kendrick Shen, Yining Chen, Tengyu Ma</h3>
<p>Self-training algorithms, which train a model to fit pseudolabels predicted
by another previously-learned model, have been very successful for learning
with unlabeled data using neural networks. However, the current theoretical
understanding of self-training only applies to linear models. This work
provides a unified theoretical analysis of self-training with deep networks for
semi-supervised learning, unsupervised domain adaptation, and unsupervised
learning. At the core of our analysis is a simple but realistic "expansion"
assumption, which states that a low-probability subset of the data must expand
to a neighborhood with large probability relative to the subset. We also assume
that neighborhoods of examples in different classes have minimal overlap. We
prove that under these assumptions, the minimizers of population objectives
based on self-training and input-consistency regularization will achieve high
accuracy with respect to ground-truth labels. By using off-the-shelf
generalization bounds, we immediately convert this result to sample complexity
guarantees for neural nets that are polynomial in the margin and Lipschitzness.
Our results help explain the empirical successes of recently proposed
self-training algorithms which use input consistency regularization.
</p>
<a href="http://arxiv.org/abs/2010.03622" target="_blank">arXiv:2010.03622</a> [<a href="http://arxiv.org/pdf/2010.03622" target="_blank">pdf</a>]

<h2>Nonstationary Reinforcement Learning with Linear Function Approximation. (arXiv:2010.04244v2 [cs.LG] UPDATED)</h2>
<h3>Huozhi Zhou, Jinglin Chen, Lav R. Varshney, Ashish Jagmohan</h3>
<p>We consider reinforcement learning (RL) in episodic Markov decision processes
(MDPs) with linear function approximation under drifting environment.
Specifically, both the reward and state transition functions can evolve over
time, as long as their respective total variations, quantified by suitable
metrics, do not exceed certain \textit{variation budgets}. We first develop the
$\texttt{LSVI-UCB-Restart}$ algorithm, an optimistic modification of
least-squares value iteration combined with periodic restart, and establish its
dynamic regret bound when variation budgets are known. We then propose a
parameter-free algorithm, $\texttt{Ada-LSVI-UCB-Restart}$, that works without
knowing the variation budgets, but with a slightly worse dynamic regret bound.
We also derive the first minimax dynamic regret lower bound for nonstationary
MDPs to show that our proposed algorithms are near-optimal. As a byproduct, we
establish a minimax regret lower bound for linear MDPs, which is unsolved by
\cite{jin2020provably}. In addition, we provide numerical experiments to
demonstrate the effectiveness of our proposed algorithms. As far as we know,
this is the first dynamic regret analysis in nonstationary reinforcement
learning with function approximation.
</p>
<a href="http://arxiv.org/abs/2010.04244" target="_blank">arXiv:2010.04244</a> [<a href="http://arxiv.org/pdf/2010.04244" target="_blank">pdf</a>]

<h2>Interpreting Multivariate Interactions in DNNs. (arXiv:2010.05045v2 [cs.LG] UPDATED)</h2>
<h3>Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, Quanshi Zhang</h3>
<p>This paper aims to explain deep neural networks (DNNs) from the perspective
of multivariate interactions. In this paper, we define and quantify the
significance of interactions among multiple input variables of the DNN. Input
variables with strong interactions usually form a coalition and reflect
prototype features, which are memorized and used by the DNN for inference. We
define the significance of interactions based on the Shapley value, which is
designed to assign the attribution value of each input variable to the
inference. We have conducted experiments with various DNNs. Experimental
results have demonstrated the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2010.05045" target="_blank">arXiv:2010.05045</a> [<a href="http://arxiv.org/pdf/2010.05045" target="_blank">pdf</a>]

<h2>RNN Training along Locally Optimal Trajectories via Frank-Wolfe Algorithm. (arXiv:2010.05397v3 [cs.LG] UPDATED)</h2>
<h3>Yun Yue, Ming Li, Venkatesh Saligrama, Ziming Zhang</h3>
<p>We propose a novel and efficient training method for RNNs by iteratively
seeking a local minima on the loss surface within a small region, and leverage
this directional vector for the update, in an outer-loop. We propose to utilize
the Frank-Wolfe (FW) algorithm in this context. Although, FW implicitly
involves normalized gradients, which can lead to a slow convergence rate, we
develop a novel RNN training method that, surprisingly, even with the
additional cost, the overall training cost is empirically observed to be lower
than back-propagation. Our method leads to a new Frank-Wolfe method, that is in
essence an SGD algorithm with a restart scheme. We prove that under certain
conditions our algorithm has a sublinear convergence rate of $O(1/\epsilon)$
for $\epsilon$ error. We then conduct empirical experiments on several
benchmark datasets including those that exhibit long-term dependencies, and
show significant performance improvement. We also experiment with deep RNN
architectures and show efficient training performance. Finally, we demonstrate
that our training method is robust to noisy data.
</p>
<a href="http://arxiv.org/abs/2010.05397" target="_blank">arXiv:2010.05397</a> [<a href="http://arxiv.org/pdf/2010.05397" target="_blank">pdf</a>]

<h2>Structural Forecasting for Tropical Cyclone Intensity Prediction: Providing Insight with Deep Learning. (arXiv:2010.05783v2 [cs.LG] UPDATED)</h2>
<h3>Trey McNeely, Niccol&#xf2; Dalmasso, Kimberly M. Wood, Ann B. Lee</h3>
<p>Tropical cyclone (TC) intensity forecasts are ultimately issued by human
forecasters. The human in-the-loop pipeline requires that any forecasting
guidance must be easily digestible by TC experts if it is to be adopted at
operational centers like the National Hurricane Center. Our proposed framework
leverages deep learning to provide forecasters with something neither
end-to-end prediction models nor traditional intensity guidance does: a
powerful tool for monitoring high-dimensional time series of key physically
relevant predictors and the means to understand how the predictors relate to
one another and to short-term intensity changes.
</p>
<a href="http://arxiv.org/abs/2010.05783" target="_blank">arXiv:2010.05783</a> [<a href="http://arxiv.org/pdf/2010.05783" target="_blank">pdf</a>]

<h2>How does Weight Correlation Affect the Generalisation Ability of Deep Neural Networks. (arXiv:2010.05983v2 [cs.LG] UPDATED)</h2>
<h3>Gaojie Jin, Xinping Yi, Liang Zhang, Lijun Zhang, Sven Schewe, Xiaowei Huang</h3>
<p>This paper studies the novel concept of weight correlation in deep neural
networks and discusses its impact on the networks' generalisation ability. For
fully-connected layers, the weight correlation is defined as the average cosine
similarity between weight vectors of neurons, and for convolutional layers, the
weight correlation is defined as the cosine similarity between filter matrices.
Theoretically, we show that, weight correlation can, and should, be
incorporated into the PAC Bayesian framework for the generalisation of neural
networks, and the resulting generalisation bound is monotonic with respect to
the weight correlation. We formulate a new complexity measure, which lifts the
PAC Bayes measure with weight correlation, and experimentally confirm that it
is able to rank the generalisation errors of a set of networks more precisely
than existing measures. More importantly, we develop a new regulariser for
training, and provide extensive experiments that show that the generalisation
error can be greatly reduced with our novel approach.
</p>
<a href="http://arxiv.org/abs/2010.05983" target="_blank">arXiv:2010.05983</a> [<a href="http://arxiv.org/pdf/2010.05983" target="_blank">pdf</a>]

<h2>Deep Reservoir Networks with Learned Hidden Reservoir Weights using Direct Feedback Alignment. (arXiv:2010.06209v3 [cs.NE] UPDATED)</h2>
<h3>Matthew Evanusa, Cornelia Ferm&#xfc;ller, Yiannis Aloimonos</h3>
<p>Deep Reservoir Computing has emerged as a new paradigm for deep learning,
which is based around the reservoir computing principle of maintaining random
pools of neurons combined with hierarchical deep learning. The reservoir
paradigm reflects and respects the high degree of recurrence in biological
brains, and the role that neuronal dynamics play in learning. However, one
issue hampering deep reservoir network development is that one cannot
backpropagate through the reservoir layers. Recent deep reservoir architectures
do not learn hidden or hierarchical representations in the same manner as deep
artificial neural networks, but rather concatenate all hidden reservoirs
together to perform traditional regression. Here we present a novel Deep
Reservoir Network for time series prediction and classification that learns
through the non-differentiable hidden reservoir layers using a
biologically-inspired backpropagation alternative called Direct Feedback
Alignment, which resembles global dopamine signal broadcasting in the brain. We
demonstrate its efficacy on two real world multidimensional time series
datasets.
</p>
<a href="http://arxiv.org/abs/2010.06209" target="_blank">arXiv:2010.06209</a> [<a href="http://arxiv.org/pdf/2010.06209" target="_blank">pdf</a>]

<h2>Direct Federated Neural Architecture Search. (arXiv:2010.06223v2 [cs.LG] UPDATED)</h2>
<h3>Anubhav Garg, Amit Kumar Saha, Debo Dutta</h3>
<p>Neural Architecture Search (NAS) is a collection of methods to craft the way
neural networks are built. We apply this idea to Federated Learning (FL),
wherein predefined neural network models are trained on the client/device data.
This approach is not optimal as the model developers can't observe the local
data, and hence, are unable to build highly accurate and efficient models. NAS
is promising for FL which can search for global and personalized models
automatically for the non-IID data. Most NAS methods are computationally
expensive and require fine-tuning after the search, making it a two-stage
complex process with possible human intervention. Thus there is a need for
end-to-end NAS which can run on the heterogeneous data and resource
distribution typically seen in the FL scenario. In this paper, we present an
effective approach for direct federated NAS which is hardware agnostic,
computationally lightweight, and a one-stage method to search for
ready-to-deploy neural network models. Our results show an order of magnitude
reduction in resource consumption while edging out prior art in accuracy. This
opens up a window of opportunity to create optimized and computationally
efficient federated learning systems.
</p>
<a href="http://arxiv.org/abs/2010.06223" target="_blank">arXiv:2010.06223</a> [<a href="http://arxiv.org/pdf/2010.06223" target="_blank">pdf</a>]

<h2>Tire Force Estimation in Intelligent Tires Using Machine Learning. (arXiv:2010.06299v3 [eess.SY] UPDATED)</h2>
<h3>Nan Xu, Hassan Askari, Yanjun Huang, Jianfeng Zhou, Amir Khajepour</h3>
<p>The concept of intelligent tires has drawn attention of researchers in the
areas of autonomous driving, advanced vehicle control, and artificial
intelligence. The focus of this paper is on intelligent tires and the
application of machine learning techniques to tire force estimation. We present
an intelligent tire system with a tri-axial acceleration sensor, which is
installed onto the inner liner of the tire, and Neural Network techniques for
real-time processing of the sensor data. The accelerometer is capable of
measuring the acceleration in x,y, and z directions. When the accelerometer
enters the tire contact patch, it starts generating signals until it fully
leaves it. Simultaneously, by using MTS Flat-Trac test platform, tire actual
forces are measured. Signals generated by the accelerometer and MTS Flat-Trac
testing system are used for training three different machine learning
techniques with the purpose of online prediction of tire forces. It is shown
that the developed intelligent tire in conjunction with machine learning is
effective in accurate prediction of tire forces under different driving
conditions. The results presented in this work will open a new avenue of
research in the area of intelligent tires, vehicle systems, and tire force
estimation.
</p>
<a href="http://arxiv.org/abs/2010.06299" target="_blank">arXiv:2010.06299</a> [<a href="http://arxiv.org/pdf/2010.06299" target="_blank">pdf</a>]

<h2>Tire Slip Angle Estimation based on the Intelligent Tire Technology. (arXiv:2010.06803v2 [eess.SY] UPDATED)</h2>
<h3>Nan Xu, Yanjun Huang, Hassan Askari, Zepeng Tang</h3>
<p>Tire slip angle is a vital parameter in tire/vehicle dynamics and control.
This paper proposes an accurate estimation method by the fusion of intelligent
tire technology and machine-learning techniques. The intelligent tire is
equipped by MEMS accelerometers attached to its inner liner. First, we describe
the intelligent tire system along with the implemented testing apparatus.
Second, experimental results under different loading and velocity conditions
are provided. Then, we show the procedure of data processing, which will be
used for training three different machine learning techniques to estimate tire
slip angles. The results show that the machine learning techniques, especially
in frequency domain, can accurately estimate tire slip angles up to 10 degrees.
More importantly, with the accurate tire slip angle estimation, all other
states and parameters can be easily and precisely obtained, which is
significant to vehicle advanced control, and thus this study has a high
potential to obviously improve the vehicle safety especially in extreme
maneuvers.
</p>
<a href="http://arxiv.org/abs/2010.06803" target="_blank">arXiv:2010.06803</a> [<a href="http://arxiv.org/pdf/2010.06803" target="_blank">pdf</a>]

<h2>FedGroup: Ternary Cosine Similarity-based Clustered Federated Learning Framework toward High Accuracy in Heterogeneous Data. (arXiv:2010.06870v2 [cs.LG] UPDATED)</h2>
<h3>Moming Duan, Duo Liu, Xinyuan Ji, Renping Liu, Liang Liang, Xianzhang Chen, Yujuan Tan</h3>
<p>Federated Learning (FL) enables the multiple participating devices to
collaboratively contribute to a global neural network model while keeping the
training data locally. Unlike the centralized training setting, the non-IID and
imbalanced (statistical heterogeneity) training data of FL is distributed in
the federated network, which will increase the divergences between the local
models and global model and further degrade the performance. In this paper, we
propose a novel federated learning framework FedGroup based on a
similarity-based clustering strategy, in which we 1) group the training of
clients based on the similarities between the clients' optimize directions; 2)
reduce the complexity of high-dimension low-sample size (HDLSS) parameter
updates data clustering by decomposing the direction vectors to derive the
ternary cosine similarity. FedGroup can achieve improvements by dividing joint
optimization into groups of sub-optimization, and can be combined with FedProx,
the state-of-the-art federated optimization algorithm. We evaluate FedGroup and
FedGrouProx (combined with FedProx) on several open datasets. The experimental
results show that our proposed frameworks significantly improving absolute test
accuracy by +14.7% on FEMNIST compared to FedAvg, +5.4% on Sentiment140
compared to FedProx.
</p>
<a href="http://arxiv.org/abs/2010.06870" target="_blank">arXiv:2010.06870</a> [<a href="http://arxiv.org/pdf/2010.06870" target="_blank">pdf</a>]

<h2>AMPA-Net: Optimization-Inspired Attention Neural Network for Deep Compressed Sensing. (arXiv:2010.06907v2 [cs.CV] UPDATED)</h2>
<h3>Nanyu Li, Charles C. Zhou</h3>
<p>Compressed sensing (CS) is a challenging problem in image processing due to
reconstructing an almost complete image from a limited measurement. To achieve
fast and accurate CS reconstruction, we synthesize the advantages of two
well-known methods (neural network and optimization algorithm) to propose a
novel optimization inspired neural network which dubbed AMP-Net. AMP-Net
realizes the fusion of the Approximate Message Passing (AMP) algorithm and
neural network. All of its parameters are learned automatically. Furthermore,
we propose an AMPA-Net which uses three attention networks to improve the
representation ability of AMP-Net. Finally, We demonstrate the effectiveness of
AMP-Net and AMPA-Net on four CS reconstruction benchmark data sets.
</p>
<a href="http://arxiv.org/abs/2010.06907" target="_blank">arXiv:2010.06907</a> [<a href="http://arxiv.org/pdf/2010.06907" target="_blank">pdf</a>]

<h2>Concise Outlines for a Complex Logic: A Proof Outline Checker for TaDA (Full Paper). (arXiv:2010.07080v2 [cs.PL] UPDATED)</h2>
<h3>Felix A. Wolf, Malte Schwerhoff, Peter M&#xfc;ller</h3>
<p>Modern separation logics allow one to prove rich properties of intricate
code, e.g. functional correctness and linearizability of non-blocking
concurrent code. However, this expressiveness leads to a complexity that makes
these logics difficult to apply. Manual proofs or proofs in interactive theorem
provers consist of a large number of steps, often with subtle side conditions.
On the other hand, automation with dedicated verifiers typically requires
sophisticated proof search algorithms that are specific to the given program
logic, resulting in limited tool support that makes it difficult to experiment
with program logics, e.g. when learning, improving, or comparing them. Proof
outline checkers fill this gap. Their input is a program annotated with the
most essential proof steps, just like the proof outlines typically presented in
papers. The tool then checks automatically that this outline represents a valid
proof in the program logic. In this paper, we systematically develop a proof
outline checker for the TaDA logic, which reduces the checking to a simpler
verification problem, for which automated tools exist. Our approach leads to
proof outline checkers that provide substantially more automation than
interactive provers, but are much simpler to develop than custom automatic
verifiers.
</p>
<a href="http://arxiv.org/abs/2010.07080" target="_blank">arXiv:2010.07080</a> [<a href="http://arxiv.org/pdf/2010.07080" target="_blank">pdf</a>]

<h2>Effective Algorithm-Accelerator Co-design for AI Solutions on Edge Devices. (arXiv:2010.07185v2 [cs.AR] UPDATED)</h2>
<h3>Cong Hao, Yao Chen, Xiaofan Zhang, Yuhong Li, Jinjun Xiong, Wen-mei Hwu, Deming Chen</h3>
<p>High quality AI solutions require joint optimization of AI algorithms, such
as deep neural networks (DNNs), and their hardware accelerators. To improve the
overall solution quality as well as to boost the design productivity, efficient
algorithm and accelerator co-design methodologies are indispensable. In this
paper, we first discuss the motivations and challenges for the
Algorithm/Accelerator co-design problem and then provide several effective
solutions. Especially, we highlight three leading works of effective co-design
methodologies: 1) the first simultaneous DNN/FPGA co-design method; 2) a
bi-directional lightweight DNN and accelerator co-design method; 3) a
differentiable and efficient DNN and accelerator co-search method. We
demonstrate the effectiveness of the proposed co-design approaches using
extensive experiments on both FPGAs and GPUs, with comparisons to existing
works. This paper emphasizes the importance and efficacy of
algorithm-accelerator co-design and calls for more research breakthroughs in
this interesting and demanding area.
</p>
<a href="http://arxiv.org/abs/2010.07185" target="_blank">arXiv:2010.07185</a> [<a href="http://arxiv.org/pdf/2010.07185" target="_blank">pdf</a>]

<h2>Data Readiness Report. (arXiv:2010.07213v2 [cs.DB] UPDATED)</h2>
<h3>Shazia Afzal, Rajmohan C, Manish Kesarwani, Sameep Mehta, Hima Patel</h3>
<p>Data exploration and quality analysis is an important yet tedious process in
the AI pipeline. Current practices of data cleaning and data readiness
assessment for machine learning tasks are mostly conducted in an arbitrary
manner which limits their reuse and results in loss of productivity. We
introduce the concept of a Data Readiness Report as an accompanying
documentation to a dataset that allows data consumers to get detailed insights
into the quality of input data. Data characteristics and challenges on various
quality dimensions are identified and documented keeping in mind the principles
of transparency and explainability. The Data Readiness Report also serves as a
record of all data assessment operations including applied transformations.
This provides a detailed lineage for the purpose of data governance and
management. In effect, the report captures and documents the actions taken by
various personas in a data readiness and assessment workflow. Overtime this
becomes a repository of best practices and can potentially drive a
recommendation system for building automated data readiness workflows on the
lines of AutoML [8]. We anticipate that together with the Datasheets [9],
Dataset Nutrition Label [11], FactSheets [1] and Model Cards [15], the Data
Readiness Report makes significant progress towards Data and AI lifecycle
documentation.
</p>
<a href="http://arxiv.org/abs/2010.07213" target="_blank">arXiv:2010.07213</a> [<a href="http://arxiv.org/pdf/2010.07213" target="_blank">pdf</a>]

<h2>Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning. (arXiv:2010.07217v2 [cs.CV] UPDATED)</h2>
<h3>Xinyu Yang, Majid Mirmehdi, Tilo Burghardt</h3>
<p>In this paper we show that learning video feature spaces in which temporal
cycles are maximally predictable benefits action classification. In particular,
we propose a novel learning approach termed Cycle Encoding Prediction (CEP)
that is able to effectively represent high-level spatio-temporal structure of
unlabelled video content. CEP builds a latent space wherein the concept of
closed forward-backward as well as backward-forward temporal loops is
approximately preserved. As a self-supervision signal, CEP leverages the
bi-directional temporal coherence of the video stream and applies loss
functions that encourage both temporal cycle closure as well as contrastive
feature separation. Architecturally, the underpinning network structure
utilises a single feature encoder for all video snippets, adding two predictive
modules that learn temporal forward and backward transitions. We apply our
framework for pretext training of networks for action recognition tasks. We
report significantly improved results for the standard datasets UCF101 and
HMDB51. Detailed ablation studies support the effectiveness of the proposed
components. We publish source code for the CEP components in full with this
paper.
</p>
<a href="http://arxiv.org/abs/2010.07217" target="_blank">arXiv:2010.07217</a> [<a href="http://arxiv.org/pdf/2010.07217" target="_blank">pdf</a>]

<h2>Learning Improvised Chatbots from Adversarial Modifications of Natural Language Feedback. (arXiv:2010.07261v2 [cs.CL] UPDATED)</h2>
<h3>Makesh Narsimhan Sreedhar, Kun Ni, Siva Reddy</h3>
<p>The ubiquitous nature of chatbots and their interaction with users generate
an enormous amount of data. Can we improve chatbots using this data? A
self-feeding chatbot improves itself by asking natural language feedback when a
user is dissatisfied with its response and uses this feedback as an additional
training sample. However, user feedback in most cases contains extraneous
sequences hindering their usefulness as a training sample. In this work, we
propose a generative adversarial model that converts noisy feedback into a
plausible natural response in a conversation. The generator's goal is to
convert the feedback into a response that answers the user's previous utterance
and to fool the discriminator which distinguishes feedback from natural
responses. We show that augmenting original training data with these modified
feedback responses improves the original chatbot performance from 69.94% to
75.96% in ranking correct responses on the Personachat dataset, a large
improvement given that the original model is already trained on 131k samples.
</p>
<a href="http://arxiv.org/abs/2010.07261" target="_blank">arXiv:2010.07261</a> [<a href="http://arxiv.org/pdf/2010.07261" target="_blank">pdf</a>]

<h2>Online Simultaneous State and Parameter Estimation. (arXiv:1703.07068v2 [eess.SY] CROSS LISTED)</h2>
<h3>Ryan Self, Moad Abudia, S. M. Nahid Mahmud, Rushikesh Kamalapurkar</h3>
<p>In this paper, a concurrent learning based adaptive observer is developed for
a class of second-order nonlinear time-invariant systems with uncertain
dynamics. The developed technique results in uniformly ultimately bounded state
and parameter estimation errors. As opposed to persistent excitation which is
required for parameter convergence in traditional adaptive control methods, the
developed technique only requires excitation over a finite time interval to
achieve parameter convergence. Simulation results in both noise-free and noisy
environments are presented to validate the design.
</p>
<a href="http://arxiv.org/abs/1703.07068" target="_blank">arXiv:1703.07068</a> [<a href="http://arxiv.org/pdf/1703.07068" target="_blank">pdf</a>]

<h2>A Semi-Supervised Assessor of Neural Architectures. (arXiv:2005.06821v1 [cs.CV] CROSS LISTED)</h2>
<h3>Yehui Tang, Yunhe Wang, Yixing Xu, Hanting Chen, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu</h3>
<p>Neural architecture search (NAS) aims to automatically design deep neural
networks of satisfactory performance. Wherein, architecture performance
predictor is critical to efficiently value an intermediate neural architecture.
But for the training of this predictor, a number of neural architectures and
their corresponding real performance often have to be collected. In contrast
with classical performance predictor optimized in a fully supervised way, this
paper suggests a semi-supervised assessor of neural architectures. We employ an
auto-encoder to discover meaningful representations of neural architectures.
Taking each neural architecture as an individual instance in the search space,
we construct a graph to capture their intrinsic similarities, where both
labeled and unlabeled architectures are involved. A graph convolutional neural
network is introduced to predict the performance of architectures based on the
learned representations and their relation modeled by the graph. Extensive
experimental results on the NAS-Benchmark-101 dataset demonstrated that our
method is able to make a significant reduction on the required fully trained
architectures for finding efficient architectures.
</p>
<a href="http://arxiv.org/abs/2005.06821" target="_blank">arXiv:2005.06821</a> [<a href="http://arxiv.org/pdf/2005.06821" target="_blank">pdf</a>]

<h2>Detecting conflicting summary statistics in likelihood-free inference. (arXiv:2010.07465v1 [stat.ME])</h2>
<h3>Yinan Mao, Xueou Wang, David J. Nott, Michael Evans</h3>
<p>Bayesian likelihood-free methods implement Bayesian inference using
simulation of data from the model to substitute for intractable likelihood
evaluations. Most likelihood-free inference methods replace the full data set
with a summary statistic before performing Bayesian inference, and the choice
of this statistic is often difficult. The summary statistic should be
low-dimensional for computational reasons, while retaining as much information
as possible about the parameter. Using a recent idea from the interpretable
machine learning literature, we develop some regression-based diagnostic
methods which are useful for detecting when different parts of a summary
statistic vector contain conflicting information about the model parameters.
Conflicts of this kind complicate summary statistic choice, and detecting them
can be insightful about model deficiencies and guide model improvement. The
diagnostic methods developed are based on regression approaches to
likelihood-free inference, in which the regression model estimates the
posterior density using summary statistics as features. Deletion and imputation
of part of the summary statistic vector within the regression model can remove
conflicts and approximate posterior distributions for summary statistic
subsets. A larger than expected change in the estimated posterior density
following deletion and imputation can indicate a conflict in which inferences
of interest are affected. The usefulness of the new methods is demonstrated in
a number of real examples.
</p>
<a href="http://arxiv.org/abs/2010.07465" target="_blank">arXiv:2010.07465</a> [<a href="http://arxiv.org/pdf/2010.07465" target="_blank">pdf</a>]

<h2>Potentials and challenges of polymer informatics: exploiting machine learning for polymer design. (arXiv:2010.07683v1 [cond-mat.soft])</h2>
<h3>Stephen Wu, Hironao Yamada, Yoshihiro Hayashi, Massimiliano Zamengo, Ryo Yoshida</h3>
<p>There has been rapidly growing demand of polymeric materials coming from
different aspects of modern life because of the highly diverse physical and
chemical properties of polymers. Polymer informatics is an interdisciplinary
research field of polymer science, computer science, information science and
machine learning that serves as a platform to exploit existing polymer data for
efficient design of functional polymers. Despite many potential benefits of
employing a data-driven approach to polymer design, there has been notable
challenges of the development of polymer informatics attributed to the complex
hierarchical structures of polymers, such as the lack of open databases and
unified structural representation. In this study, we review and discuss the
applications of machine learning on different aspects of the polymer design
process through four perspectives: polymer databases, representation
(descriptor) of polymers, predictive models for polymer properties, and polymer
design strategy. We hope that this paper can serve as an entry point for
researchers interested in the field of polymer informatics.
</p>
<a href="http://arxiv.org/abs/2010.07683" target="_blank">arXiv:2010.07683</a> [<a href="http://arxiv.org/pdf/2010.07683" target="_blank">pdf</a>]

<h2>Robust weights that optimally balance confounders for estimating the effect of binary and continuous treatments with time-to-event data. (arXiv:2010.07695v1 [stat.ME])</h2>
<h3>Michele Santacatterina</h3>
<p>Covariate balance is crucial in obtaining unbiased estimates of treatment
effects in observational studies. Methods based on Inverse Probability Weights
(IPW) have been widely used to estimate treatment effects with observational
data. Machine learning techniques have been proposed to estimate propensity
scores. These techniques however target accuracy instead of covariate balance.
Methods that target covariate balance have been successfully proposed and
largely applied to estimate treatment effects on continuous outcomes. However,
in many medical and epidemiological applications, the interest lies in
estimating treatment effects on time-to-an-event outcomes. In this paper, we
start by presenting robust orthogonality weights (ROW), a set of weights
obtained by solving a quadratic constrained optimization problem that maximizes
precision while constraining covariate balance defined as the sample
correlation between confounders and treatment. By doing so, ROW optimally deal
with both binary and continuous treatments. We then evaluate the performance of
the proposed weights in estimating hazard ratios of binary and continuous
treatments with time-to-event outcomes in a simulation study. We finally apply
ROW on the evaluation of the effect of hormone therapy on time to coronary
heart disease and on the effect of red meat consumption on time to colon cancer
among 24,069 postmenopausal women enrolled in the Women's Health Initiative
observational study.
</p>
<a href="http://arxiv.org/abs/2010.07695" target="_blank">arXiv:2010.07695</a> [<a href="http://arxiv.org/pdf/2010.07695" target="_blank">pdf</a>]

