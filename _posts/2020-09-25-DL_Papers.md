---
title: Latest Deep Learning Papers
date: 2021-01-09 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (146 Articles)</h1>
<h2>Learn Dynamic-Aware State Embedding for Transfer Learning. (arXiv:2101.02230v1 [cs.LG])</h2>
<h3>Kaige Yang</h3>
<p>Transfer reinforcement learning aims to improve the sample efficiency of
solving unseen new tasks by leveraging experiences obtained from previous
tasks. We consider the setting where all tasks (MDPs) share the same
environment dynamic except reward function. In this setting, the MDP dynamic is
a good knowledge to transfer, which can be inferred by uniformly random policy.
However, trajectories generated by uniform random policy are not useful for
policy improvement, which impairs the sample efficiency severely. Instead, we
observe that the binary MDP dynamic can be inferred from trajectories of any
policy which avoids the need of uniform random policy. As the binary MDP
dynamic contains the state structure shared over all tasks we believe it is
suitable to transfer. Built on this observation, we introduce a method to infer
the binary MDP dynamic on-line and at the same time utilize it to guide state
embedding learning, which is then transferred to new tasks. We keep state
embedding learning and policy learning separately. As a result, the learned
state embedding is task and policy agnostic which makes it ideal for transfer
learning. In addition, to facilitate the exploration over the state space, we
propose a novel intrinsic reward based on the inferred binary MDP dynamic. Our
method can be used out-of-box in combination with model-free RL algorithms. We
show two instances on the basis of \algo{DQN} and \algo{A2C}. Empirical results
of intensive experiments show the advantage of our proposed method in various
transfer learning tasks.
</p>
<a href="http://arxiv.org/abs/2101.02230" target="_blank">arXiv:2101.02230</a> [<a href="http://arxiv.org/pdf/2101.02230" target="_blank">pdf</a>]

<h2>Controlling Synthetic Characters in Simulations: A Case for Cognitive Architectures and Sigma. (arXiv:2101.02231v1 [cs.AI])</h2>
<h3>Volkan Ustun, Paul S. Rosenbloom, Seyed Sajjadi, Jeremy Nuttal</h3>
<p>Simulations, along with other similar applications like virtual worlds and
video games, require computational models of intelligence that generate
realistic and credible behavior for the participating synthetic characters.
Cognitive architectures, which are models of the fixed structure underlying
intelligent behavior in both natural and artificial systems, provide a
conceptually valid common basis, as evidenced by the current efforts towards a
standard model of the mind, to generate human-like intelligent behavior for
these synthetic characters. Sigma is a cognitive architecture and system that
strives to combine what has been learned from four decades of independent work
on symbolic cognitive architectures, probabilistic graphical models, and more
recently neural models, under its graphical architecture hypothesis. Sigma
leverages an extended form of factor graphs towards a uniform grand unification
of not only traditional cognitive capabilities but also key non-cognitive
aspects, creating unique opportunities for the construction of new kinds of
cognitive models that possess a Theory-of-Mind and that are perceptual,
autonomous, interactive, affective, and adaptive. In this paper, we will
introduce Sigma along with its diverse capabilities and then use three distinct
proof-of-concept Sigma models to highlight combinations of these capabilities:
(1) Distributional reinforcement learning models in; (2) A pair of adaptive and
interactive agent models that demonstrate rule-based, probabilistic, and social
reasoning; and (3) A knowledge-free exploration model in which an agent
leverages only architectural appraisal variables, namely attention and
curiosity, to locate an item while building up a map in a Unity environment.
</p>
<a href="http://arxiv.org/abs/2101.02231" target="_blank">arXiv:2101.02231</a> [<a href="http://arxiv.org/pdf/2101.02231" target="_blank">pdf</a>]

<h2>Single Shot Multitask Pedestrian Detection and Behavior Prediction. (arXiv:2101.02232v1 [cs.AI])</h2>
<h3>Prateek Agrawal, Pratik Prabhanjan Brahma</h3>
<p>Detecting and predicting the behavior of pedestrians is extremely crucial for
self-driving vehicles to plan and interact with them safely. Although there
have been several research works in this area, it is important to have fast and
memory efficient models such that it can operate in embedded hardware in these
autonomous machines. In this work, we propose a novel architecture using
spatial-temporal multi-tasking to do camera based pedestrian detection and
intention prediction. Our approach significantly reduces the latency by being
able to detect and predict all pedestrians' intention in a single shot manner
while also being able to attain better accuracy by sharing features with
relevant object level information and interactions.
</p>
<a href="http://arxiv.org/abs/2101.02232" target="_blank">arXiv:2101.02232</a> [<a href="http://arxiv.org/pdf/2101.02232" target="_blank">pdf</a>]

<h2>A Novel Shaft-to-Tissue Force Model for Safer Motion Planning of Steerable Needles. (arXiv:2101.02246v1 [cs.RO])</h2>
<h3>Michael Bentley, Caleb Rucker, Chakravarthy Reddy, Oren Salzman, Alan Kuntz</h3>
<p>Steerable needles are capable of accurately targeting difficult-to-reach
clinical sites in the body. By bending around sensitive anatomical structures,
steerable needles have the potential to reduce the invasiveness of many medical
procedures. However, inserting these needles with curved trajectories increases
the risk of tissue shearing due to large forces being exerted on the
surrounding tissue by the needle's shaft. Such shearing can cause significant
damage to surrounding tissue, potentially worsening patient outcomes. In this
work, we derive a tissue and needle force model based on a Cosserat string
formulation, which describes the normal forces and frictional forces along the
shaft as a function of the planned needle path, friction parameters, and tip
piercing force. We then incorporate this force model as a cost function in an
asymptotically near-optimal motion planner and demonstrate the ability to plan
motions that consider the tissue normal forces from the needle shaft during
planning in a simulated steering environment and a simulated lung tumor biopsy
scenario. By planning motions for the needle that aim to minimize the tissue
normal force explicitly, our method plans needle paths that reduce the risk of
tissue shearing while still reaching desired targets in the body.
</p>
<a href="http://arxiv.org/abs/2101.02246" target="_blank">arXiv:2101.02246</a> [<a href="http://arxiv.org/pdf/2101.02246" target="_blank">pdf</a>]

<h2>Playing with Food: Learning Food Item Representations through Interactive Exploration. (arXiv:2101.02252v1 [cs.RO])</h2>
<h3>Amrita Sawhney, Steven Lee, Kevin Zhang, Manuela Veloso, Oliver Kroemer</h3>
<p>A key challenge in robotic food manipulation is modeling the material
properties of diverse and deformable food items. We propose using a multimodal
sensory approach to interact and play with food that facilitates the ability to
distinguish these properties across food items. First, we use a robotic arm and
an array of sensors, which are synchronized using ROS, to collect a diverse
dataset consisting of 21 unique food items with varying slices and properties.
Afterwards, we learn visual embedding networks that utilize a combination of
proprioceptive, audio, and visual data to encode similarities among food items
using a triplet loss formulation. Our evaluations show that embeddings learned
through interactions can successfully increase performance in a wide range of
material and shape classification tasks. We envision that these learned
embeddings can be utilized as a basis for planning and selecting optimal
parameters for more material-aware robotic food manipulation skills.
Furthermore, we hope to stimulate further innovations in the field of food
robotics by sharing this food playing dataset with the research community.
</p>
<a href="http://arxiv.org/abs/2101.02252" target="_blank">arXiv:2101.02252</a> [<a href="http://arxiv.org/pdf/2101.02252" target="_blank">pdf</a>]

<h2>Teach me to play, gamer! Imitative learning in computer games via linguistic description of complex phenomena and decision tree. (arXiv:2101.02264v1 [cs.LG])</h2>
<h3>Clemente Rubio-Manzano, Tomas Lermanda, CLaudia Martinez, Alejandra Segura, Christian Vidal</h3>
<p>In this article, we present a new machine learning model by imitation based
on the linguistic description of complex phenomena. The idea consists of,
first, capturing the behaviour of human players by creating a computational
perception network based on the execution traces of the games and, second,
representing it using fuzzy logic (linguistic variables and if-then rules).
From this knowledge, a set of data (dataset) is automatically created to
generate a learning model based on decision trees. This model will be used
later to automatically control the movements of a bot. The result is an
artificial agent that mimics the human player. We have implemented, tested and
evaluated this technology. The results obtained are interesting and promising,
showing that this method can be a good alternative to design and implement the
behaviour of intelligent agents in video game development.
</p>
<a href="http://arxiv.org/abs/2101.02264" target="_blank">arXiv:2101.02264</a> [<a href="http://arxiv.org/pdf/2101.02264" target="_blank">pdf</a>]

<h2>LightLayers: Parameter Efficient Dense and Convolutional Layers for Image Classification. (arXiv:2101.02268v1 [cs.CV])</h2>
<h3>Debesh Jha, Anis Yazidi, Michael A. Riegler, Dag Johansen, H&#xe5;vard D. Johansen, P&#xe5;l Halvorsen</h3>
<p>Deep Neural Networks (DNNs) have become the de-facto standard in computer
vision, as well as in many other pattern recognition tasks. A key drawback of
DNNs is that the training phase can be very computationally expensive.
Organizations or individuals that cannot afford purchasing state-of-the-art
hardware or tapping into cloud-hosted infrastructures may face a long waiting
time before the training completes or might not be able to train a model at
all. Investigating novel ways to reduce the training time could be a potential
solution to alleviate this drawback, and thus enabling more rapid development
of new algorithms and models. In this paper, we propose LightLayers, a method
for reducing the number of trainable parameters in deep neural networks (DNN).
The proposed LightLayers consists of LightDense andLightConv2D layer that are
as efficient as regular Conv2D and Dense layers, but uses less parameters. We
resort to Matrix Factorization to reduce the complexity of the DNN models
resulting into lightweight DNNmodels that require less computational power,
without much loss in the accuracy. We have tested LightLayers on MNIST, Fashion
MNIST, CI-FAR 10, and CIFAR 100 datasets. Promising results are obtained for
MNIST, Fashion MNIST, CIFAR-10 datasets whereas CIFAR 100 shows acceptable
performance by using fewer parameters.
</p>
<a href="http://arxiv.org/abs/2101.02268" target="_blank">arXiv:2101.02268</a> [<a href="http://arxiv.org/pdf/2101.02268" target="_blank">pdf</a>]

<h2>Partial Domain Adaptation Using Selective Representation Learning For Class-Weight Computation. (arXiv:2101.02275v1 [cs.CV])</h2>
<h3>Sandipan Choudhuri, Riti Paul, Arunabha Sen, Baoxin Li, Hemanth Venkateswara</h3>
<p>The generalization power of deep-learning models is dependent on
rich-labelled data. This supervision using large-scaled annotated information
is restrictive in most real-world scenarios where data collection and their
annotation involve huge cost. Various domain adaptation techniques exist in
literature that bridge this distribution discrepancy. However, a majority of
these models require the label sets of both the domains to be identical. To
tackle a more practical and challenging scenario, we formulate the problem
statement from a partial domain adaptation perspective, where the source label
set is a super set of the target label set. Driven by the motivation that image
styles are private to each domain, in this work, we develop a method that
identifies outlier classes exclusively from image content information and train
a label classifier exclusively on class-content from source images.
Additionally, elimination of negative transfer of samples from classes private
to the source domain is achieved by transforming the soft class-level weights
into two clusters, 0 (outlier source classes) and 1 (shared classes) by
maximizing the between-cluster variance between them.
</p>
<a href="http://arxiv.org/abs/2101.02275" target="_blank">arXiv:2101.02275</a> [<a href="http://arxiv.org/pdf/2101.02275" target="_blank">pdf</a>]

<h2>On State Estimation for Legged Locomotion over Soft Terrain. (arXiv:2101.02279v1 [cs.RO])</h2>
<h3>Shamel Fahmi, Geoff Fink, Claudio Semini</h3>
<p>Locomotion over soft terrain remains a challenging problem for legged robots.
Most of the work done on state estimation for legged robots is designed for
rigid contacts, and does not take into account the physical parameters of the
terrain. That said, this letter answers the following questions: how and why
does soft terrain affect state estimation for legged robots? To do so, we
utilized a state estimator that fuses IMU measurements with leg odometry that
is designed with rigid contact assumptions. We experimentally validated the
state estimator with the HyQ robot trotting over both soft and rigid terrain.
We demonstrate that soft terrain negatively affects state estimation for legged
robots, and that the state estimates have a noticeable drift over soft terrain
compared to rigid terrain.
</p>
<a href="http://arxiv.org/abs/2101.02279" target="_blank">arXiv:2101.02279</a> [<a href="http://arxiv.org/pdf/2101.02279" target="_blank">pdf</a>]

<h2>Navigation Framework for a Hybrid Steel Bridge Inspection Robot. (arXiv:2101.02282v1 [cs.RO])</h2>
<h3>Hoang-Dung Bui, Hung M. La</h3>
<p>Autonomous navigation is essential for steel bridge inspection robot to
monitor and maintain the working condition of steel bridges. Majority of
existing robotic solutions requires human support to navigate the robot doing
the inspection. In this paper, a navigation framework is proposed for ARA robot
[1], [2] to run on mobile mode. In this mode, the robot needs to cross and
inspect all the available steel bars. The most significant contributions of
this research are four algorithms, which can process the depth data, segment it
into clusters, estimate the boundaries, construct a graph to represent the
structure, generate a shortest inspection path with any starting and ending
points, and determine available robot configuration for path planning.
Experiments on steel bridge structures setup highlight the effective
performance of the algorithms, and the potential to apply to the ARA robot to
run on real bridge structures. We released our source code in Github for the
research community to use.
</p>
<a href="http://arxiv.org/abs/2101.02282" target="_blank">arXiv:2101.02282</a> [<a href="http://arxiv.org/pdf/2101.02282" target="_blank">pdf</a>]

<h2>Handling many conversions per click in modeling delayed feedback. (arXiv:2101.02284v1 [cs.LG])</h2>
<h3>Ashwinkumar Badanidiyuru, Andrew Evdokimov, Vinodh Krishnan, Pan Li, Wynn Vonnegut, Jayden Wang</h3>
<p>Predicting the expected value or number of post-click conversions (purchases
or other events) is a key task in performance-based digital advertising. In
training a conversion optimizer model, one of the most crucial aspects is
handling delayed feedback with respect to conversions, which can happen
multiple times with varying delay. This task is difficult, as the delay
distribution is different for each advertiser, is long-tailed, often does not
follow any particular class of parametric distributions, and can change over
time. We tackle these challenges using an unbiased estimation model based on
three core ideas. The first idea is to split the label as a sum of labels with
different delay buckets, each of which trains only on mature label, the second
is to use thermometer encoding to increase accuracy and reduce inference cost,
and the third is to use auxiliary information to increase the stability of the
model and to handle drift in the distribution.
</p>
<a href="http://arxiv.org/abs/2101.02284" target="_blank">arXiv:2101.02284</a> [<a href="http://arxiv.org/pdf/2101.02284" target="_blank">pdf</a>]

<h2>VOGUE: Try-On by StyleGAN Interpolation Optimization. (arXiv:2101.02285v1 [cs.CV])</h2>
<h3>Kathleen M Lewis, Srivatsan Varadharajan, Ira Kemelmacher-Shlizerman</h3>
<p>Given an image of a target person and an image of another person wearing a
garment, we automatically generate the target person in the given garment. At
the core of our method is a pose-conditioned StyleGAN2 latent space
interpolation, which seamlessly combines the areas of interest from each image,
i.e., body shape, hair, and skin color are derived from the target person,
while the garment with its folds, material properties, and shape comes from the
garment image. By automatically optimizing for interpolation coefficients per
layer in the latent space, we can perform a seamless, yet true to source,
merging of the garment and target person. Our algorithm allows for garments to
deform according to the given body shape, while preserving pattern and material
details. Experiments demonstrate state-of-the-art photo-realistic results at
high resolution ($512\times 512$).
</p>
<a href="http://arxiv.org/abs/2101.02285" target="_blank">arXiv:2101.02285</a> [<a href="http://arxiv.org/pdf/2101.02285" target="_blank">pdf</a>]

<h2>Hyperboost: Hyperparameter Optimization by Gradient Boosting surrogate models. (arXiv:2101.02289v1 [cs.LG])</h2>
<h3>Jeroen van Hoof, Joaquin Vanschoren</h3>
<p>Bayesian Optimization is a popular tool for tuning algorithms in automatic
machine learning (AutoML) systems. Current state-of-the-art methods leverage
Random Forests or Gaussian processes to build a surrogate model that predicts
algorithm performance given a certain set of hyperparameter settings. In this
paper, we propose a new surrogate model based on gradient boosting, where we
use quantile regression to provide optimistic estimates of the performance of
an unobserved hyperparameter setting, and combine this with a distance metric
between unobserved and observed hyperparameter settings to help regulate
exploration. We demonstrate empirically that the new method is able to
outperform some state-of-the art techniques across a reasonable sized set of
classification problems.
</p>
<a href="http://arxiv.org/abs/2101.02289" target="_blank">arXiv:2101.02289</a> [<a href="http://arxiv.org/pdf/2101.02289" target="_blank">pdf</a>]

<h2>Demand Forecasting for Platelet Usage: from Univariate Time Series to Multivariate Models. (arXiv:2101.02305v1 [cs.LG])</h2>
<h3>Maryam Motamedi, Na Li, Douglas G. Down, Nancy M. Heddle</h3>
<p>Platelet products are both expensive and have very short shelf lives. As
usage rates for platelets are highly variable, the effective management of
platelet demand and supply is very important yet challenging. The primary goal
of this paper is to present an efficient forecasting model for platelet demand
at Canadian Blood Services (CBS). To accomplish this goal, four different
demand forecasting methods, ARIMA (Auto Regressive Moving Average), Prophet,
lasso regression (least absolute shrinkage and selection operator) and LSTM
(Long Short-Term Memory) networks are utilized and evaluated. We use a large
clinical dataset for a centralized blood distribution centre for four hospitals
in Hamilton, Ontario, spanning from 2010 to 2018 and consisting of daily
platelet transfusions along with information such as the product
specifications, the recipients' characteristics, and the recipients' laboratory
test results. This study is the first to utilize different methods from
statistical time series models to data-driven regression and a machine learning
technique for platelet transfusion using clinical predictors and with different
amounts of data. We find that the multivariate approaches have the highest
accuracy in general, however, if sufficient data are available, a simpler time
series approach such as ARIMA appears to be sufficient. We also comment on the
approach to choose clinical indicators (inputs) for the multivariate models.
</p>
<a href="http://arxiv.org/abs/2101.02305" target="_blank">arXiv:2101.02305</a> [<a href="http://arxiv.org/pdf/2101.02305" target="_blank">pdf</a>]

<h2>Bipartite mixed membership stochastic blockmodel. (arXiv:2101.02307v1 [stat.ML])</h2>
<h3>Huan Qing, Jingli Wang</h3>
<p>Mixed membership problem for undirected network has been well studied in
network analysis recent years. However, the more general case of mixed
membership for directed network remains a challenge. Here, we propose an
interpretable model: bipartite mixed membership stochastic blockmodel (BiMMSB
for short) for directed mixed membership networks. BiMMSB allows that row nodes
and column nodes of the adjacency matrix can be different and these nodes may
have distinct community structure in a directed network. We also develop an
efficient spectral algorithm called BiMPCA to estimate the mixed memberships
for both row nodes and column nodes in a directed network. We show that the
approach is asymptotically consistent under BiMMSB. We demonstrate the
advantages of BiMMSB with applications to a small-scale simulation study, the
directed Political blogs network and the Papers Citations network.
</p>
<a href="http://arxiv.org/abs/2101.02307" target="_blank">arXiv:2101.02307</a> [<a href="http://arxiv.org/pdf/2101.02307" target="_blank">pdf</a>]

<h2>Coding for Distributed Multi-Agent Reinforcement Learning. (arXiv:2101.02308v1 [cs.LG])</h2>
<h3>Baoqian Wang, Junfei Xie, Nikolay Atanasov</h3>
<p>This paper aims to mitigate straggler effects in synchronous distributed
learning for multi-agent reinforcement learning (MARL) problems. Stragglers
arise frequently in a distributed learning system, due to the existence of
various system disturbances such as slow-downs or failures of compute nodes and
communication bottlenecks. To resolve this issue, we propose a coded
distributed learning framework, which speeds up the training of MARL algorithms
in the presence of stragglers, while maintaining the same accuracy as the
centralized approach. As an illustration, a coded distributed version of the
multi-agent deep deterministic policy gradient(MADDPG) algorithm is developed
and evaluated. Different coding schemes, including maximum distance separable
(MDS)code, random sparse code, replication-based code, and regular low density
parity check (LDPC) code are also investigated. Simulations in several
multi-robot problems demonstrate the promising performance of the proposed
framework.
</p>
<a href="http://arxiv.org/abs/2101.02308" target="_blank">arXiv:2101.02308</a> [<a href="http://arxiv.org/pdf/2101.02308" target="_blank">pdf</a>]

<h2>Diminishing Uncertainty within the Training Pool: Active Learning for Medical Image Segmentation. (arXiv:2101.02323v1 [cs.CV])</h2>
<h3>Vishwesh Nath, Dong Yang, Bennett A. Landman, Daguang Xu, Holger R. Roth</h3>
<p>Active learning is a unique abstraction of machine learning techniques where
the model/algorithm could guide users for annotation of a set of data points
that would be beneficial to the model, unlike passive machine learning. The
primary advantage being that active learning frameworks select data points that
can accelerate the learning process of a model and can reduce the amount of
data needed to achieve full accuracy as compared to a model trained on a
randomly acquired data set. Multiple frameworks for active learning combined
with deep learning have been proposed, and the majority of them are dedicated
to classification tasks. Herein, we explore active learning for the task of
segmentation of medical imaging data sets. We investigate our proposed
framework using two datasets: 1.) MRI scans of the hippocampus, 2.) CT scans of
pancreas and tumors. This work presents a query-by-committee approach for
active learning where a joint optimizer is used for the committee. At the same
time, we propose three new strategies for active learning: 1.) increasing
frequency of uncertain data to bias the training data set; 2.) Using mutual
information among the input images as a regularizer for acquisition to ensure
diversity in the training dataset; 3.) adaptation of Dice log-likelihood for
Stein variational gradient descent (SVGD). The results indicate an improvement
in terms of data reduction by achieving full accuracy while only using 22.69 %
and 48.85 % of the available data for each dataset, respectively.
</p>
<a href="http://arxiv.org/abs/2101.02323" target="_blank">arXiv:2101.02323</a> [<a href="http://arxiv.org/pdf/2101.02323" target="_blank">pdf</a>]

<h2>GraphHop: An Enhanced Label Propagation Method for Node Classification. (arXiv:2101.02326v1 [cs.LG])</h2>
<h3>Tian Xie, Bin Wang, C.-C. Jay Kuo</h3>
<p>A scalable semi-supervised node classification method on graph-structured
data, called GraphHop, is proposed in this work. The graph contains attributes
of all nodes but labels of a few nodes. The classical label propagation (LP)
method and the emerging graph convolutional network (GCN) are two popular
semi-supervised solutions to this problem. The LP method is not effective in
modeling node attributes and labels jointly or facing a slow convergence rate
on large-scale graphs. GraphHop is proposed to its shortcoming. With proper
initial label vector embeddings, each iteration of GraphHop contains two steps:
1) label aggregation and 2) label update. In Step 1, each node aggregates its
neighbors' label vectors obtained in the previous iteration. In Step 2, a new
label vector is predicted for each node based on the label of the node itself
and the aggregated label information obtained in Step 1. This iterative
procedure exploits the neighborhood information and enables GraphHop to perform
well in an extremely small label rate setting and scale well for very large
graphs. Experimental results show that GraphHop outperforms state-of-the-art
graph learning methods on a wide range of tasks (e.g., multi-label and
multi-class classification on citation networks, social graphs, and commodity
consumption graphs) in graphs of various sizes. Our codes are publicly
available on GitHub (https://github.com/TianXieUSC/GraphHop).
</p>
<a href="http://arxiv.org/abs/2101.02326" target="_blank">arXiv:2101.02326</a> [<a href="http://arxiv.org/pdf/2101.02326" target="_blank">pdf</a>]

<h2>A design of human-like robust AI machines in object identification. (arXiv:2101.02327v1 [cs.AI])</h2>
<h3>Bao-Gang Hu, Wei-Ming Dong</h3>
<p>This is a perspective paper inspired from the study of Turing Test proposed
by A.M. Turing (23 June 1912 - 7 June 1954) in 1950. Following one important
implication of Turing Test for enabling a machine with a human-like behavior or
performance, we define human-like robustness (HLR) for AI machines. The
objective of the new definition aims to enforce AI machines with HLR, including
to evaluate them in terms of HLR. A specific task is discussed only on object
identification, because it is the most common task for every person in daily
life. Similar to the perspective, or design, position by Turing, we provide a
solution of how to achieve HLR AI machines without constructing them and
conducting real experiments. The solution should consists of three important
features in the machines. The first feature of HLR machines is to utilize
common sense from humans for realizing a causal inference. The second feature
is to make a decision from a semantic space for having interpretations to the
decision. The third feature is to include a "human-in-the-loop" setting for
advancing HLR machines. We show an "identification game" using proposed design
of HLR machines. The present paper shows an attempt to learn and explore
further from Turing Test towards the design of human-like AI machines.
</p>
<a href="http://arxiv.org/abs/2101.02327" target="_blank">arXiv:2101.02327</a> [<a href="http://arxiv.org/pdf/2101.02327" target="_blank">pdf</a>]

<h2>Copula Quadrant Similarity for Anomaly Scores. (arXiv:2101.02330v1 [stat.ML])</h2>
<h3>Matthew Davidow, David Matteson</h3>
<p>Practical anomaly detection requires applying numerous approaches due to the
inherent difficulty of unsupervised learning. Direct comparison between complex
or opaque anomaly detection algorithms is intractable; we instead propose a
framework for associating the scores of multiple methods. Our aim is to answer
the question: how should one measure the similarity between anomaly scores
generated by different methods? The scoring crux is the extremes, which
identify the most anomalous observations. A pair of algorithms are defined here
to be similar if they assign their highest scores to roughly the same small
fraction of observations. To formalize this, we propose a measure based on
extremal similarity in scoring distributions through a novel upper quadrant
modeling approach, and contrast it with tail and other dependence measures. We
illustrate our method with simulated and real experiments, applying spectral
methods to cluster multiple anomaly detection methods and to contrast our
similarity measure with others. We demonstrate that our method is able to
detect the clusters of anomaly detection algorithms to achieve an accurate and
robust ensemble algorithm.
</p>
<a href="http://arxiv.org/abs/2101.02330" target="_blank">arXiv:2101.02330</a> [<a href="http://arxiv.org/pdf/2101.02330" target="_blank">pdf</a>]

<h2>Identification of Latent Variables From Graphical Model Residuals. (arXiv:2101.02332v1 [stat.ML])</h2>
<h3>Boris Hayete, Fred Gruber, Anna Decker, Raymond Yan</h3>
<p>Graph-based causal discovery methods aim to capture conditional
independencies consistent with the observed data and differentiate causal
relationships from indirect or induced ones. Successful construction of
graphical models of data depends on the assumption of causal sufficiency: that
is, that all confounding variables are measured. When this assumption is not
met, learned graphical structures may become arbitrarily incorrect and effects
implied by such models may be wrongly attributed, carry the wrong magnitude, or
mis-represent direction of correlation. Wide application of graphical models to
increasingly less curated "big data" draws renewed attention to the unobserved
confounder problem.

We present a novel method that aims to control for the latent space when
estimating a DAG by iteratively deriving proxies for the latent space from the
residuals of the inferred model. Under mild assumptions, our method improves
structural inference of Gaussian graphical models and enhances identifiability
of the causal effect. In addition, when the model is being used to predict
outcomes, it un-confounds the coefficients on the parents of the outcomes and
leads to improved predictive performance when out-of-sample regime is very
different from the training data. We show that any improvement of prediction of
an outcome is intrinsically capped and cannot rise beyond a certain limit as
compared to the confounded model. We extend our methodology beyond GGMs to
ordinal variables and nonlinear cases. Our R package provides both PCA and
autoencoder implementations of the methodology, suitable for GGMs with some
guarantees and for better performance in general cases but without such
guarantees.
</p>
<a href="http://arxiv.org/abs/2101.02332" target="_blank">arXiv:2101.02332</a> [<a href="http://arxiv.org/pdf/2101.02332" target="_blank">pdf</a>]

<h2>Infinitely Wide Tensor Networks as Gaussian Process. (arXiv:2101.02333v1 [stat.ML])</h2>
<h3>Erdong Guo, David Draper</h3>
<p>Gaussian Process is a non-parametric prior which can be understood as a
distribution on the function space intuitively. It is known that by introducing
appropriate prior to the weights of the neural networks, Gaussian Process can
be obtained by taking the infinite-width limit of the Bayesian neural networks
from a Bayesian perspective. In this paper, we explore the infinitely wide
Tensor Networks and show the equivalence of the infinitely wide Tensor Networks
and the Gaussian Process. We study the pure Tensor Network and another two
extended Tensor Network structures: Neural Kernel Tensor Network and Tensor
Network hidden layer Neural Network and prove that each one will converge to
the Gaussian Process as the width of each model goes to infinity. (We note here
that Gaussian Process can also be obtained by taking the infinite limit of at
least one of the bond dimensions $\alpha_{i}$ in the product of tensor nodes,
and the proofs can be done with the same ideas in the proofs of the
infinite-width cases.) We calculate the mean function (mean vector) and the
covariance function (covariance matrix) of the finite dimensional distribution
of the induced Gaussian Process by the infinite-width tensor network with a
general set-up. We study the properties of the covariance function and derive
the approximation of the covariance function when the integral in the
expectation operator is intractable. In the numerical experiments, we implement
the Gaussian Process corresponding to the infinite limit tensor networks and
plot the sample paths of these models. We study the hyperparameters and plot
the sample path families in the induced Gaussian Process by varying the
standard deviations of the prior distributions. As expected, the parameters in
the prior distribution namely the hyper-parameters in the induced Gaussian
Process controls the characteristic lengthscales of the Gaussian Process.
</p>
<a href="http://arxiv.org/abs/2101.02333" target="_blank">arXiv:2101.02333</a> [<a href="http://arxiv.org/pdf/2101.02333" target="_blank">pdf</a>]

<h2>Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v1 [cs.CV])</h2>
<h3>Dave Epstein, Jiajun Wu, Cordelia Schmid, Chen Sun</h3>
<p>Learning to model how the world changes as time elapses has proven a
challenging problem for the computer vision community. We propose a
self-supervised solution to this problem using temporal cycle consistency
jointly in vision and language, training on narrated video. Our model learns
modality-agnostic functions to predict forward and backward in time, which must
undo each other when composed. This constraint leads to the discovery of
high-level transitions between moments in time, since such transitions are
easily inverted and shared across modalities. We justify the design of our
model with an ablation study on different configurations of the cycle
consistency problem. We then show qualitatively and quantitatively that our
approach yields a meaningful, high-level model of the future and past. We apply
the learned dynamics model without further training to various tasks, such as
predicting future action and temporally ordering sets of images.
</p>
<a href="http://arxiv.org/abs/2101.02337" target="_blank">arXiv:2101.02337</a> [<a href="http://arxiv.org/pdf/2101.02337" target="_blank">pdf</a>]

<h2>Max-Affine Spline Insights Into Deep Network Pruning. (arXiv:2101.02338v1 [cs.LG])</h2>
<h3>Randall Balestriero, Haoran You, Zhihan Lu, Yutong Kou, Yingyan Lin, Richard Baraniuk</h3>
<p>In this paper, we study the importance of pruning in Deep Networks (DNs) and
motivate it based on the current absence of data aware weight initialization.
Current DN initializations, focusing primarily at maintaining first order
statistics of the feature maps through depth, force practitioners to
overparametrize a model in order to reach high performances. This
overparametrization can then be pruned a posteriori, leading to a phenomenon
known as "winning tickets". However, the pruning literature still relies on
empirical investigations, lacking a theoretical understanding of (1) how
pruning affects the decision boundary, (2) how to interpret pruning, (3) how to
design principled pruning techniques, and (4) how to theoretically study
pruning. To tackle those questions, we propose to employ recent advances in the
theoretical analysis of Continuous Piecewise Affine (CPA) DNs. From this
viewpoint, we can study the DNs' input space partitioning and detect the
early-bird (EB) phenomenon, guide practitioners by identifying when to stop the
first training step, provide interpretability into current pruning techniques,
and develop a principled pruning criteria towards efficient DN training.
Finally, we conduct extensive experiments to show the effectiveness of the
proposed spline pruning criteria in terms of both layerwise and global pruning
over state-of-the-art pruning methods.
</p>
<a href="http://arxiv.org/abs/2101.02338" target="_blank">arXiv:2101.02338</a> [<a href="http://arxiv.org/pdf/2101.02338" target="_blank">pdf</a>]

<h2>DICE: Deep Significance Clustering for Outcome-Aware Stratification. (arXiv:2101.02344v1 [cs.LG])</h2>
<h3>Yufang Huang, Kelly M. Axsom, John Lee, Lakshminarayanan Subramanian, Yiye Zhang</h3>
<p>We present deep significance clustering (DICE), a framework for jointly
performing representation learning and clustering for "outcome-aware"
stratification. DICE is intended to generate cluster membership that may be
used to categorize a population by individual risk level for a targeted
outcome. Following the representation learning and clustering steps, we embed
the objective function in DICE with a constraint which requires a statistically
significant association between the outcome and cluster membership of learned
representations. DICE further includes a neural architecture search step to
maximize both the likelihood of representation learning and outcome
classification accuracy with cluster membership as the predictor. To
demonstrate its utility in medicine for patient risk-stratification, the
performance of DICE was evaluated using two datasets with different outcome
ratios extracted from real-world electronic health records. Outcomes are
defined as acute kidney injury (30.4\%) among a cohort of COVID-19 patients,
and discharge disposition (36.8\%) among a cohort of heart failure patients,
respectively. Extensive results demonstrate that DICE has superior performance
as measured by the difference in outcome distribution across clusters,
Silhouette score, Calinski-Harabasz index, and Davies-Bouldin index for
clustering, and Area under the ROC Curve (AUC) for outcome classification
compared to several baseline approaches.
</p>
<a href="http://arxiv.org/abs/2101.02344" target="_blank">arXiv:2101.02344</a> [<a href="http://arxiv.org/pdf/2101.02344" target="_blank">pdf</a>]

<h2>Attention Actor-Critic algorithm for Multi-Agent Constrained Co-operative Reinforcement Learning. (arXiv:2101.02349v1 [cs.AI])</h2>
<h3>P.Parnika, Raghuram Bharadwaj Diddigi, Sai Koti Reddy Danda, Shalabh Bhatnagar</h3>
<p>In this work, we consider the problem of computing optimal actions for
Reinforcement Learning (RL) agents in a co-operative setting, where the
objective is to optimize a common goal. However, in many real-life
applications, in addition to optimizing the goal, the agents are required to
satisfy certain constraints specified on their actions. Under this setting, the
objective of the agents is to not only learn the actions that optimize the
common objective but also meet the specified constraints. In recent times, the
Actor-Critic algorithm with an attention mechanism has been successfully
applied to obtain optimal actions for RL agents in multi-agent environments. In
this work, we extend this algorithm to the constrained multi-agent RL setting.
The idea here is that optimizing the common goal and satisfying the constraints
may require different modes of attention. By incorporating different attention
modes, the agents can select useful information required for optimizing the
objective and satisfying the constraints separately, thereby yielding better
actions. Through experiments on benchmark multi-agent environments, we show the
effectiveness of our proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2101.02349" target="_blank">arXiv:2101.02349</a> [<a href="http://arxiv.org/pdf/2101.02349" target="_blank">pdf</a>]

<h2>M\"{o}biusE: Knowledge Graph Embedding on M\"{o}bius Ring. (arXiv:2101.02352v1 [cs.AI])</h2>
<h3>Yao Chen, Jiangang Liu, Zhe Zhang, Shiping Wen, Wenjun Xiong</h3>
<p>In this work, we propose a novel Knowledge Graph Embedding (KGE) strategy,
called M\"{o}biusE, in which the entities and relations are embedded to the
surface of a M\"{o}bius ring. The proposition of such a strategy is inspired by
the classic TorusE, in which the addition of two arbitrary elements is subject
to a modulus operation. In this sense, TorusE naturally guarantees the critical
boundedness of embedding vectors in KGE. However, the nonlinear property of
addition operation on Torus ring is uniquely derived by the modulus operation,
which in some extent restricts the expressiveness of TorusE. As a further
generalization of TorusE, M\"{o}biusE also uses modulus operation to preserve
the closeness of addition operation on it, but the coordinates on M\"{o}bius
ring interacts with each other in the following way: {\em \color{red} any
vector on the surface of a M\"{o}bius ring moves along its parametric trace
will goes to the right opposite direction after a cycle}. Hence, M\"{o}biusE
assumes much more nonlinear representativeness than that of TorusE, and in turn
it generates much more precise embedding results. In our experiments,
M\"{o}biusE outperforms TorusE and other classic embedding strategies in
several key indicators.
</p>
<a href="http://arxiv.org/abs/2101.02352" target="_blank">arXiv:2101.02352</a> [<a href="http://arxiv.org/pdf/2101.02352" target="_blank">pdf</a>]

<h2>Low-cost and high-performance data augmentation for deep-learning-based skin lesion classification. (arXiv:2101.02353v1 [cs.CV])</h2>
<h3>Shuwei Shen, Mengjuan Xu, Fan Zhang, Pengfei Shao, Honghong Liu, Liang Xu, Chi Zhang, Peng Liu, Zhihong Zhang, Peng Yao, Ronald X. Xu</h3>
<p>Although deep convolutional neural networks (DCNNs) have achieved significant
accuracy in skin lesion classification comparable or even superior to those of
dermatologists, practical implementation of these models for skin cancer
screening in low resource settings is hindered by their limitations in
computational cost and training dataset. To overcome these limitations, we
propose a low-cost and high-performance data augmentation strategy that
includes two consecutive stages of augmentation search and network search. At
the augmentation search stage, the augmentation strategy is optimized in the
search space of Low-Cost-Augment (LCA) under the criteria of balanced accuracy
(BACC) with 5-fold cross validation. At the network search stage, the DCNNs are
fine-tuned with the full training set in order to select the model with the
highest BACC. The efficiency of the proposed data augmentation strategy is
verified on the HAM10000 dataset using EfficientNets as a baseline. With the
proposed strategy, we are able to reduce the search space to 60 and achieve a
high BACC of 0.853 by using a single DCNN model without external database,
suitable to be implemented in mobile devices for DCNN-based skin lesion
detection in low resource settings.
</p>
<a href="http://arxiv.org/abs/2101.02353" target="_blank">arXiv:2101.02353</a> [<a href="http://arxiv.org/pdf/2101.02353" target="_blank">pdf</a>]

<h2>OAAE: Adversarial Autoencoders for Novelty Detection in Multi-modal Normality Case via Orthogonalized Latent Space. (arXiv:2101.02358v1 [cs.LG])</h2>
<h3>Sungkwon An, Jeonghoon Kim, Myungjoo Kang, Shahbaz Razaei, Xin Liu</h3>
<p>Novelty detection using deep generative models such as autoencoder,
generative adversarial networks mostly takes image reconstruction error as
novelty score function. However, image data, high dimensional as it is,
contains a lot of different features other than class information which makes
models hard to detect novelty data. The problem gets harder in multi-modal
normality case. To address this challenge, we propose a new way of measuring
novelty score in multi-modal normality cases using orthogonalized latent space.
Specifically, we employ orthogonal low-rank embedding in the latent space to
disentangle the features in the latent space using mutual class information.
With the orthogonalized latent space, novelty score is defined by the change of
each latent vector. Proposed algorithm was compared to state-of-the-art novelty
detection algorithms using GAN such as RaPP and OCGAN, and experimental results
show that ours outperforms those algorithms.
</p>
<a href="http://arxiv.org/abs/2101.02358" target="_blank">arXiv:2101.02358</a> [<a href="http://arxiv.org/pdf/2101.02358" target="_blank">pdf</a>]

<h2>Architectural Patterns for the Design of Federated Learning Systems. (arXiv:2101.02373v1 [cs.LG])</h2>
<h3>Sin Kit Lo, Qinghua Lu, Liming Zhu, Hye-young Paik, Xiwei Xu, Chen Wang</h3>
<p>Federated learning has received fast-growing interests from academia and
industry to tackle the challenges of data hungriness and privacy in machine
learning. A federated learning system can be viewed as a large-scale
distributed system with different components and stakeholders as numerous
client devices participate in federated learning. Designing a federated
learning system requires software system design thinking apart from machine
learning knowledge. Although much effort has been put into federated learning
from the machine learning technique aspects, the software architecture design
concerns in building federated learning systems have been largely ignored.
Therefore, in this paper, we present a collection of architectural patterns to
deal with the design challenges of federated learning systems. Architectural
patterns present reusable solutions to a commonly occurring problem within a
given context during software architecture design. The presented patterns are
based on the results of a systematic literature review and include three client
management patterns, four model management patterns, three model training
patterns, and four model aggregation patterns. The patterns are associated to
particular state transitions in a federated learning model lifecycle, serving
as a guidance for effective use of the patterns in the design of federated
learning systems.
</p>
<a href="http://arxiv.org/abs/2101.02373" target="_blank">arXiv:2101.02373</a> [<a href="http://arxiv.org/pdf/2101.02373" target="_blank">pdf</a>]

<h2>Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition. (arXiv:2101.02374v1 [cs.CV])</h2>
<h3>Le Hui, Mingmei Cheng, Jin Xie, Jian Yang</h3>
<p>Point cloud based retrieval for place recognition is still a challenging
problem due to drastic appearance and illumination changes of scenes in
changing environments. Existing deep learning based global descriptors for the
retrieval task usually consume a large amount of computation resources (e.g.,
memory), which may not be suitable for the cases of limited hardware resources.
In this paper, we develop an efficient point cloud learning network (EPC-Net)
to form a global descriptor for visual place recognition, which can obtain good
performance and reduce computation memory and inference time. First, we propose
a lightweight but effective neural network module, called ProxyConv, to
aggregate the local geometric features of point clouds. We leverage the spatial
adjacent matrix and proxy points to simplify the original edge convolution for
lower memory consumption. Then, we design a lightweight grouped VLAD network
(G-VLAD) to form global descriptors for retrieval. Compared with the original
VLAD network, we propose a grouped fully connected (GFC) layer to decompose the
high-dimensional vectors into a group of low-dimensional vectors, which can
reduce the number of parameters of the network and maintain the discrimination
of the feature vector. Finally, to further reduce the inference time, we
develop a simple version of EPC-Net, called EPC-Net-L, which consists of two
ProxyConv modules and one max pooling layer to aggregate global descriptors. By
distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative
global descriptors for retrieval. Extensive experiments on the Oxford dataset
and three in-house datasets demonstrate that our proposed method can achieve
state-of-the-art performance with lower parameters, FLOPs, and runtime per
frame.
</p>
<a href="http://arxiv.org/abs/2101.02374" target="_blank">arXiv:2101.02374</a> [<a href="http://arxiv.org/pdf/2101.02374" target="_blank">pdf</a>]

<h2>Who's a Good Boy? Reinforcing Canine Behavior using Machine Learning in Real-Time. (arXiv:2101.02380v1 [cs.CV])</h2>
<h3>Jason Stock, Tom Cavey</h3>
<p>In this paper we outline the development methodology for an automatic dog
treat dispenser which combines machine learning and embedded hardware to
identify and reward dog behaviors in real-time. Using machine learning
techniques for training an image classification model we identify three
behaviors of our canine companions: "sit", "stand", and "lie down" with up to
92% test accuracy and 39 frames per second. We evaluate a variety of neural
network architectures, interpretability methods, model quantization and
optimization techniques to develop a model specifically for an NVIDIA Jetson
Nano. We detect the aforementioned behaviors in real-time and reinforce
positive actions by making inference on the Jetson Nano and transmitting a
signal to a servo motor to release rewards from a treat delivery apparatus.
</p>
<a href="http://arxiv.org/abs/2101.02380" target="_blank">arXiv:2101.02380</a> [<a href="http://arxiv.org/pdf/2101.02380" target="_blank">pdf</a>]

<h2>Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds. (arXiv:2101.02381v1 [cs.CV])</h2>
<h3>Jingyu Gong, Jiachen Xu, Xin Tan, Jie Zhou, Yanyun Qu, Yuan Xie, Lizhuang Ma</h3>
<p>Boundary information plays a significant role in 2D image segmentation, while
usually being ignored in 3D point cloud segmentation where ambiguous features
might be generated in feature extraction, leading to misclassification in the
transition area between two objects. In this paper, firstly, we propose a
Boundary Prediction Module (BPM) to predict boundary points. Based on the
predicted boundary, a boundary-aware Geometric Encoding Module (GEM) is
designed to encode geometric information and aggregate features with
discrimination in a neighborhood, so that the local features belonging to
different categories will not be polluted by each other. To provide extra
geometric information for boundary-aware GEM, we also propose a light-weight
Geometric Convolution Operation (GCO), making the extracted features more
distinguishing. Built upon the boundary-aware GEM, we build our network and
test it on benchmarks like ScanNet v2, S3DIS. Results show our methods can
significantly improve the baseline and achieve state-of-the-art performance.
Code is available at https://github.com/JchenXu/BoundaryAwareGEM.
</p>
<a href="http://arxiv.org/abs/2101.02381" target="_blank">arXiv:2101.02381</a> [<a href="http://arxiv.org/pdf/2101.02381" target="_blank">pdf</a>]

<h2>Safety-Oriented Pedestrian Motion and Scene Occupancy Forecasting. (arXiv:2101.02385v1 [cs.CV])</h2>
<h3>Katie Luo, Sergio Casas, Renjie Liao, Xinchen Yan, Yuwen Xiong, Wenyuan Zeng, Raquel Urtasun</h3>
<p>In this paper, we address the important problem in self-driving of
forecasting multi-pedestrian motion and their shared scene occupancy map,
critical for safe navigation. Our contributions are two-fold. First, we
advocate for predicting both the individual motions as well as the scene
occupancy map in order to effectively deal with missing detections caused by
postprocessing, e.g., confidence thresholding and non-maximum suppression.
Second, we propose a Scene-Actor Graph Neural Network (SA-GNN) which preserves
the relative spatial information of pedestrians via 2D convolution, and
captures the interactions among pedestrians within the same scene, including
those that have not been detected, via message passing. On two large-scale
real-world datasets, nuScenes and ATG4D, we showcase that our scene-occupancy
predictions are more accurate and better calibrated than those from
state-of-the-art motion forecasting methods, while also matching their
performance in pedestrian motion forecasting metrics.
</p>
<a href="http://arxiv.org/abs/2101.02385" target="_blank">arXiv:2101.02385</a> [<a href="http://arxiv.org/pdf/2101.02385" target="_blank">pdf</a>]

<h2>Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed. (arXiv:2101.02388v1 [cs.LG])</h2>
<h3>Eric Luhman, Troy Luhman</h3>
<p>Iterative generative models, such as noise conditional score networks and
denoising diffusion probabilistic models, produce high quality samples by
gradually denoising an initial noise vector. However, their denoising process
has many steps, making them 2-3 orders of magnitude slower than other
generative models such as GANs and VAEs. In this paper, we establish a novel
connection between knowledge distillation and image generation with a technique
that distills a multi-step denoising process into a single step, resulting in a
sampling speed similar to other single-step generative models. Our Denoising
Student generates high quality samples comparable to GANs on the CIFAR-10 and
CelebA datasets, without adversarial training. We demonstrate that our method
scales to higher resolutions through experiments on 256 x 256 LSUN. Code and
checkpoints are available at https://github.com/tcl9876/Denoising_Student
</p>
<a href="http://arxiv.org/abs/2101.02388" target="_blank">arXiv:2101.02388</a> [<a href="http://arxiv.org/pdf/2101.02388" target="_blank">pdf</a>]

<h2>Multi-scale Information Assembly for Image Matting. (arXiv:2101.02391v1 [cs.CV])</h2>
<h3>Yu Qiao, Yuhao Liu, Qiang Zhu, Xin Yang, Yuxin Wang, Qiang Zhang, Xiaopeng Wei</h3>
<p>Image matting is a long-standing problem in computer graphics and vision,
mostly identified as the accurate estimation of the foreground in input images.
We argue that the foreground objects can be represented by different-level
information, including the central bodies, large-grained boundaries, refined
details, etc. Based on this observation, in this paper, we propose a
multi-scale information assembly framework (MSIA-matte) to pull out
high-quality alpha mattes from single RGB images. Technically speaking, given
an input image, we extract advanced semantics as our subject content and retain
initial CNN features to encode different-level foreground expression, then
combine them by our well-designed information assembly strategy. Extensive
experiments can prove the effectiveness of the proposed MSIA-matte, and we can
achieve state-of-the-art performance compared to most existing matting
networks.
</p>
<a href="http://arxiv.org/abs/2101.02391" target="_blank">arXiv:2101.02391</a> [<a href="http://arxiv.org/pdf/2101.02391" target="_blank">pdf</a>]

<h2>Detecting Log Anomalies with Multi-Head Attention (LAMA). (arXiv:2101.02392v1 [cs.LG])</h2>
<h3>Yicheng Guo, Yujin Wen, Congwei Jiang, Yixin Lian, Yi Wan</h3>
<p>Anomaly detection is a crucial and challenging subject that has been studied
within diverse research areas. In this work, we explore the task of log anomaly
detection (especially computer system logs and user behavior logs) by analyzing
logs' sequential information. We propose LAMA, a multi-head attention based
sequential model to process log streams as template activity (event) sequences.

A next event prediction task is applied to train the model for anomaly
detection. Extensive empirical studies demonstrate that our new model
outperforms existing log anomaly detection methods including statistical and
deep learning methodologies, which validate the effectiveness of our proposed
method in learning sequence patterns of log data.
</p>
<a href="http://arxiv.org/abs/2101.02392" target="_blank">arXiv:2101.02392</a> [<a href="http://arxiv.org/pdf/2101.02392" target="_blank">pdf</a>]

<h2>A Comprehensive Study on Optimization Strategies for Gradient Descent In Deep Learning. (arXiv:2101.02397v1 [cs.LG])</h2>
<h3>Kaustubh Yadav</h3>
<p>One of the most important parts of Artificial Neural Networks is minimizing
the loss functions which tells us how good or bad our model is. To minimize
these losses we need to tune the weights and biases. Also to calculate the
minimum value of a function we need gradient. And to update our weights we need
gradient descent. But there are some problems with regular gradient descent ie.
it is quite slow and not that accurate. This article aims to give an
introduction to optimization strategies to gradient descent. In addition, we
shall also discuss the architecture of these algorithms and further
optimization of Neural Networks in general
</p>
<a href="http://arxiv.org/abs/2101.02397" target="_blank">arXiv:2101.02397</a> [<a href="http://arxiv.org/pdf/2101.02397" target="_blank">pdf</a>]

<h2>On the Management of Type 1 Diabetes Mellitus with IoT Devices and ML Techniques. (arXiv:2101.02409v1 [cs.LG])</h2>
<h3>Ignacio Rodriguez</h3>
<p>The purpose of this Conference is to present the main lines of base projects
that are founded on research already begun in previous years. In this sense,
this manuscript will present the main lines of research in Diabetes Mellitus
type 1 and Machine Learning techniques in an Internet of Things environment, so
that we can summarize the future lines to be developed as follows: data
collection through biosensors, massive data processing in the cloud,
interconnection of biodevices, local computing vs. cloud computing, and
possibilities of machine learning techniques to predict blood glucose values,
including both variable selection algorithms and predictive techniques.
</p>
<a href="http://arxiv.org/abs/2101.02409" target="_blank">arXiv:2101.02409</a> [<a href="http://arxiv.org/pdf/2101.02409" target="_blank">pdf</a>]

<h2>Progressive Self-Guided Loss for Salient Object Detection. (arXiv:2101.02412v1 [cs.CV])</h2>
<h3>Sheng Yang, Weisi Lin, Guosheng Lin, Qiuping Jiang, Zichuan Liu</h3>
<p>We present a simple yet effective progressive self-guided loss function to
facilitate deep learning-based salient object detection (SOD) in images. The
saliency maps produced by the most relevant works still suffer from incomplete
predictions due to the internal complexity of salient objects. Our proposed
progressive self-guided loss simulates a morphological closing operation on the
model predictions for epoch-wisely creating progressive and auxiliary training
supervisions to step-wisely guide the training process. We demonstrate that
this new loss function can guide the SOD model to highlight more complete
salient objects step-by-step and meanwhile help to uncover the spatial
dependencies of the salient object pixels in a region growing manner. Moreover,
a new feature aggregation module is proposed to capture multi-scale features
and aggregate them adaptively by a branch-wise attention mechanism. Benefiting
from this module, our SOD framework takes advantage of adaptively aggregated
multi-scale features to locate and detect salient objects effectively.
Experimental results on several benchmark datasets show that our loss function
not only advances the performance of existing SOD models without architecture
modification but also helps our proposed framework to achieve state-of-the-art
performance.
</p>
<a href="http://arxiv.org/abs/2101.02412" target="_blank">arXiv:2101.02412</a> [<a href="http://arxiv.org/pdf/2101.02412" target="_blank">pdf</a>]

<h2>Simplified DOM Trees for Transferable Attribute Extraction from the Web. (arXiv:2101.02415v1 [cs.LG])</h2>
<h3>Yichao Zhou, Ying Sheng, Nguyen Vo, Nick Edmonds, Sandeep Tata</h3>
<p>There has been a steady need to precisely extract structured knowledge from
the web (i.e. HTML documents). Given a web page, extracting a structured object
along with various attributes of interest (e.g. price, publisher, author, and
genre for a book) can facilitate a variety of downstream applications such as
large-scale knowledge base construction, e-commerce product search, and
personalized recommendation. Considering each web page is rendered from an HTML
DOM tree, existing approaches formulate the problem as a DOM tree node tagging
task. However, they either rely on computationally expensive visual feature
engineering or are incapable of modeling the relationship among the tree nodes.
In this paper, we propose a novel transferable method, Simplified DOM Trees for
Attribute Extraction (SimpDOM), to tackle the problem by efficiently retrieving
useful context for each node by leveraging the tree structure. We study two
challenging experimental settings: (i) intra-vertical few-shot extraction, and
(ii) cross-vertical fewshot extraction with out-of-domain knowledge, to
evaluate our approach. Extensive experiments on the SWDE public dataset show
that SimpDOM outperforms the state-of-the-art (SOTA) method by 1.44% on the F1
score. We also find that utilizing knowledge from a different vertical
(cross-vertical extraction) is surprisingly useful and helps beat the SOTA by a
further 1.37%.
</p>
<a href="http://arxiv.org/abs/2101.02415" target="_blank">arXiv:2101.02415</a> [<a href="http://arxiv.org/pdf/2101.02415" target="_blank">pdf</a>]

<h2>Towards Optimally Efficient Tree Search with Deep Temporal Difference Learning. (arXiv:2101.02420v1 [cs.LG])</h2>
<h3>Ke He, Lisheng Fan</h3>
<p>This paper investigates the classical integer least-squares problem which
estimates integer signals from linear models. The problem is NP-hard and often
arises in diverse applications such as signal processing, bioinformatics,
communications and machine learning, to name a few. Since the existing optimal
search strategies involve prohibitive complexities, they are hard to be adopted
in large-scale problems. To address this issue, we propose a general
hyper-accelerated tree search (HATS) algorithm by employing a deep neural
network to estimate the optimal heuristic for the underlying simplified
memory-bounded A* algorithm, and the proposed algorithm can be easily
generalized with other heuristic search algorithms. Inspired by the temporal
difference learning, we further propose a training strategy which enables the
network to approach the optimal heuristic precisely and consistently, thus the
proposed algorithm can reach nearly the optimal efficiency when the estimation
error is small enough. Experiments show that the proposed algorithm can reach
almost the optimal maximum likelihood estimate performance in large-scale
problems, with a very low complexity in both time and space. The code of this
paper is avaliable at https://github.com/skypitcher/hats.
</p>
<a href="http://arxiv.org/abs/2101.02420" target="_blank">arXiv:2101.02420</a> [<a href="http://arxiv.org/pdf/2101.02420" target="_blank">pdf</a>]

<h2>Detecting Suspicious Events in Fast Information Flows. (arXiv:2101.02424v1 [cs.LG])</h2>
<h3>Kristiaan Pelckmans, Moustafa Aboushady, Andreas Brosemyr</h3>
<p>We describe a computational feather-light and intuitive, yet provably
efficient algorithm, named HALFADO. HALFADO is designed for detecting
suspicious events in a high-frequency stream of complex entries, based on a
relatively small number of examples of human judgement. Operating a
sufficiently accurate detection system is vital for {\em assisting} teams of
human experts in many different areas of the modern digital society. These
systems have intrinsically a far-reaching normative effect, and public
knowledge of the workings of such technology should be a human right.

On a conceptual level, the present approach extends one of the most classical
learning algorithms for classification, inheriting its theoretical properties.
It however works in a semi-supervised way integrating human and computational
intelligence. On a practical level, this algorithm transcends existing
approaches (expert systems) by managing and boosting their performance into a
single global detector.

We illustrate HALFADO's efficacy on two challenging applications: (1) for
detecting {\em hate speech} messages in a flow of text messages gathered from a
social media platform, and (2) for a Transaction Monitoring System (TMS) in
FinTech detecting fraudulent transactions in a stream of financial
transactions.

This algorithm illustrates that - contrary to popular belief - advanced
methods of machine learning need not require neither advanced levels of
computation power nor expensive annotation efforts.
</p>
<a href="http://arxiv.org/abs/2101.02424" target="_blank">arXiv:2101.02424</a> [<a href="http://arxiv.org/pdf/2101.02424" target="_blank">pdf</a>]

<h2>Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization of Polynomial Activation Neural Networks in Fully Polynomial-Time. (arXiv:2101.02429v1 [cs.LG])</h2>
<h3>Burak Bartan, Mert Pilanci</h3>
<p>The training of two-layer neural networks with nonlinear activation functions
is an important non-convex optimization problem with numerous applications and
promising performance in layerwise deep learning. In this paper, we develop
exact convex optimization formulations for two-layer neural networks with
second degree polynomial activations based on semidefinite programming.
Remarkably, we show that semidefinite lifting is always exact and therefore
computational complexity for global optimization is polynomial in the input
dimension and sample size for all input data. The developed convex formulations
are proven to achieve the same global optimal solution set as their non-convex
counterparts. More specifically, the globally optimal two-layer neural network
with polynomial activations can be found by solving a semidefinite program
(SDP) and decomposing the solution using a procedure we call Neural
Decomposition. Moreover, the choice of regularizers plays a crucial role in the
computational tractability of neural network training. We show that the
standard weight decay regularization formulation is NP-hard, whereas other
simple convex penalties render the problem tractable in polynomial time via
convex programming. We extend the results beyond the fully connected
architecture to different neural network architectures including networks with
vector outputs and convolutional architectures with pooling. We provide
extensive numerical simulations showing that the standard backpropagation
approach often fails to achieve the global optimum of the training loss. The
proposed approach is significantly faster to obtain better test accuracy
compared to the standard backpropagation procedure.
</p>
<a href="http://arxiv.org/abs/2101.02429" target="_blank">arXiv:2101.02429</a> [<a href="http://arxiv.org/pdf/2101.02429" target="_blank">pdf</a>]

<h2>Drift anticipation with forgetting to improve evolving fuzzy system. (arXiv:2101.02442v1 [cs.AI])</h2>
<h3>Cl&#xe9;ment Leroy (INTUIDOC), Eric Anquetil (INTUIDOC), Nathalie Girard (INTUIDOC)</h3>
<p>Working with a non-stationary stream of data requires for the analysis system
to evolve its model (the parameters as well as the structure) over time. In
particular, concept drifts can occur, which makes it necessary to forget
knowledge that has become obsolete. However, the forgetting is subjected to the
stability-plasticity dilemma, that is, increasing forgetting improve reactivity
of adapting to the new data while reducing the robustness of the system. Based
on a set of inference rules, Evolving Fuzzy Systems-EFS-have proven to be
effective in solving the data stream learning problem. However tackling the
stability-plasticity dilemma is still an open question. This paper proposes a
coherent method to integrate forgetting in Evolving Fuzzy System, based on the
recently introduced notion of concept drift anticipation. The forgetting is
applied with two methods: an exponential forgetting of the premise part and a
deferred directional forgetting of the conclusion part of EFS to preserve the
coherence between both parts. The originality of the approach consists in
applying the forgetting only in the anticipation module and in keeping the EFS
(called principal system) learned without any forgetting. Then, when a drift is
detected in the stream, a selection mechanism is proposed to replace the
obsolete parameters of the principal system with more suitable parameters of
the anticipation module. An evaluation of the proposed methods is carried out
on benchmark online datasets, with a comparison with state-of-the-art online
classifiers (Learn++.NSE, PENsemble, pclass) as well as with the original
system using different forgetting strategies.
</p>
<a href="http://arxiv.org/abs/2101.02442" target="_blank">arXiv:2101.02442</a> [<a href="http://arxiv.org/pdf/2101.02442" target="_blank">pdf</a>]

<h2>Practical Evaluation of Out-of-Distribution Detection Methods for Image Classification. (arXiv:2101.02447v1 [cs.CV])</h2>
<h3>Engkarat Techapanurak, Takayuki Okatani</h3>
<p>We reconsider the evaluation of OOD detection methods for image recognition.
Although many studies have been conducted so far to build better OOD detection
methods, most of them follow Hendrycks and Gimpel's work for the method of
experimental evaluation. While the unified evaluation method is necessary for a
fair comparison, there is a question of if its choice of tasks and datasets
reflect real-world applications and if the evaluation results can generalize to
other OOD detection application scenarios. In this paper, we experimentally
evaluate the performance of representative OOD detection methods for three
scenarios, i.e., irrelevant input detection, novel class detection, and domain
shift detection, on various datasets and classification tasks. The results show
that differences in scenarios and datasets alter the relative performance among
the methods. Our results can also be used as a guide for practitioners for the
selection of OOD detection methods.
</p>
<a href="http://arxiv.org/abs/2101.02447" target="_blank">arXiv:2101.02447</a> [<a href="http://arxiv.org/pdf/2101.02447" target="_blank">pdf</a>]

<h2>RobustSleepNet: Transfer learning for automated sleep staging at scale. (arXiv:2101.02452v1 [stat.ML])</h2>
<h3>Antoine Guillot, Valentin Thorey</h3>
<p>Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)
records. Sleep stages are systematically determined as a preliminary step of
this examination. In practice, sleep stage classification relies on the visual
inspection of 30-seconds epochs of polysomnography signals. Numerous automatic
approaches have been developed to replace this tedious and expensive task.
Although these methods demonstrated better performance than human sleep experts
on specific datasets, they remain largely unused in sleep clinics. The main
reason is that each sleep clinic uses a specific PSG montage that most
automatic approaches are unable to handle out-of-the-box. Moreover, even when
the PSG montage is compatible, publications have shown that automatic
approaches perform poorly on unseen data with different demographics. To
address these issues, we introduce RobustSleepNet, a deep learning model for
automatic sleep stage classification able to handle arbitrary PSG montages. We
trained and evaluated this model in a leave-one-out-dataset fashion on a large
corpus of 8 heterogeneous sleep staging datasets to make it robust to
demographic changes. When evaluated on an unseen dataset, RobustSleepNet
reaches 97% of the F1 of a model trained specifically on this dataset. We then
show that finetuning RobustSleepNet, using a part of the unseen dataset,
increase the F1 by 2% when compared to a model trained specifically for this
dataset. Hence, RobustSleepNet unlocks the possibility to perform high-quality
out-of-the-box automatic sleep staging with any clinical setup. It can also be
finetuned to reach a state-of-the-art level of performance on a specific
population.
</p>
<a href="http://arxiv.org/abs/2101.02452" target="_blank">arXiv:2101.02452</a> [<a href="http://arxiv.org/pdf/2101.02452" target="_blank">pdf</a>]

<h2>Neural Fitted Q Iteration based Optimal Bidding Strategy in Real Time Reactive Power Market_1. (arXiv:2101.02456v1 [cs.AI])</h2>
<h3>Jahnvi Patel, Devika Jay, Balaraman Ravindran, K.Shanti Swarup</h3>
<p>In real time electricity markets, the objective of generation companies while
bidding is to maximize their profit. The strategies for learning optimal
bidding have been formulated through game theoretical approaches and stochastic
optimization problems. Similar studies in reactive power markets have not been
reported so far because the network voltage operating conditions have an
increased impact on reactive power markets than on active power markets.
Contrary to active power markets, the bids of rivals are not directly related
to fuel costs in reactive power markets. Hence, the assumption of a suitable
probability distribution function is unrealistic, making the strategies adopted
in active power markets unsuitable for learning optimal bids in reactive power
market mechanisms. Therefore, a bidding strategy is to be learnt from market
observations and experience in imperfect oligopolistic competition-based
markets. In this paper, a pioneer work on learning optimal bidding strategies
from observation and experience in a three-stage reactive power market is
reported.
</p>
<a href="http://arxiv.org/abs/2101.02456" target="_blank">arXiv:2101.02456</a> [<a href="http://arxiv.org/pdf/2101.02456" target="_blank">pdf</a>]

<h2>Associated Spatio-Temporal Capsule Network for Gait Recognition. (arXiv:2101.02458v1 [cs.CV])</h2>
<h3>Aite Zhao, Junyu Dong, Jianbo Li, Lin Qi, Huiyu Zhou</h3>
<p>It is a challenging task to identify a person based on her/his gait patterns.
State-of-the-art approaches rely on the analysis of temporal or spatial
characteristics of gait, and gait recognition is usually performed on single
modality data (such as images, skeleton joint coordinates, or force signals).
Evidence has shown that using multi-modality data is more conducive to gait
research. Therefore, we here establish an automated learning system, with an
associated spatio-temporal capsule network (ASTCapsNet) trained on multi-sensor
datasets, to analyze multimodal information for gait recognition. Specifically,
we first design a low-level feature extractor and a high-level feature
extractor for spatio-temporal feature extraction of gait with a novel recurrent
memory unit and a relationship layer. Subsequently, a Bayesian model is
employed for the decision-making of class labels. Extensive experiments on
several public datasets (normal and abnormal gait) validate the effectiveness
of the proposed ASTCapsNet, compared against several state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2101.02458" target="_blank">arXiv:2101.02458</a> [<a href="http://arxiv.org/pdf/2101.02458" target="_blank">pdf</a>]

<h2>Incorporating Vision Bias into Click Models for Image-oriented Search Engine. (arXiv:2101.02459v1 [cs.AI])</h2>
<h3>Ningxin Xu, Cheng Yang, Yixin Zhu, Xiaowei Hu, Changhu Wang</h3>
<p>Most typical click models assume that the probability of a document to be
examined by users only depends on position, such as PBM and UBM. It works well
in various kinds of search engines. However, in a search engine where massive
candidate documents display images as responses to the query, the examination
probability should not only depend on position. The visual appearance of an
image-oriented document also plays an important role in its opportunity to be
examined. In this paper, we assume that vision bias exists in an image-oriented
search engine as another crucial factor affecting the examination probability
aside from position. Specifically, we apply this assumption to classical click
models and propose an extended model, to better capture the examination
probabilities of documents. We use regression-based EM algorithm to predict the
vision bias given the visual features extracted from candidate documents.
Empirically, we evaluate our model on a dataset developed from a real-world
online image-oriented search engine, and demonstrate that our proposed model
can achieve significant improvements over its baseline model in data fitness
and sparsity handling.
</p>
<a href="http://arxiv.org/abs/2101.02459" target="_blank">arXiv:2101.02459</a> [<a href="http://arxiv.org/pdf/2101.02459" target="_blank">pdf</a>]

<h2>Multimodal Gait Recognition for Neurodegenerative Diseases. (arXiv:2101.02469v1 [cs.CV])</h2>
<h3>Aite Zhao, Jianbo Li, Junyu Dong, Lin Qi, Qianni Zhang, Ning Li, Xin Wang, Huiyu Zhou</h3>
<p>In recent years, single modality based gait recognition has been extensively
explored in the analysis of medical images or other sensory data, and it is
recognised that each of the established approaches has different strengths and
weaknesses. As an important motor symptom, gait disturbance is usually used for
diagnosis and evaluation of diseases; moreover, the use of multi-modality
analysis of the patient's walking pattern compensates for the one-sidedness of
single modality gait recognition methods that only learn gait changes in a
single measurement dimension. The fusion of multiple measurement resources has
demonstrated promising performance in the identification of gait patterns
associated with individual diseases. In this paper, as a useful tool, we
propose a novel hybrid model to learn the gait differences between three
neurodegenerative diseases, between patients with different severity levels of
Parkinson's disease and between healthy individuals and patients, by fusing and
aggregating data from multiple sensors. A spatial feature extractor (SFE) is
applied to generating representative features of images or signals. In order to
capture temporal information from the two modality data, a new correlative
memory neural network (CorrMNN) architecture is designed for extracting
temporal features. Afterwards, we embed a multi-switch discriminator to
associate the observations with individual state estimations. Compared with
several state-of-the-art techniques, our proposed framework shows more accurate
classification results.
</p>
<a href="http://arxiv.org/abs/2101.02469" target="_blank">arXiv:2101.02469</a> [<a href="http://arxiv.org/pdf/2101.02469" target="_blank">pdf</a>]

<h2>PandaNet : Anchor-Based Single-Shot Multi-Person 3D Pose Estimation. (arXiv:2101.02471v1 [cs.CV])</h2>
<h3>Abdallah Benzine, Florian Chabot, Bertrand Luvison, Quoc Cong Pham, Cahterine Achrd</h3>
<p>Recently, several deep learning models have been proposed for 3D human pose
estimation. Nevertheless, most of these approaches only focus on the
single-person case or estimate 3D pose of a few people at high resolution.
Furthermore, many applications such as autonomous driving or crowd analysis
require pose estimation of a large number of people possibly at low-resolution.
In this work, we present PandaNet (Pose estimAtioN and Dectection Anchor-based
Network), a new single-shot, anchor-based and multi-person 3D pose estimation
approach. The proposed model performs bounding box detection and, for each
detected person, 2D and 3D pose regression into a single forward pass. It does
not need any post-processing to regroup joints since the network predicts a
full 3D pose for each bounding box and allows the pose estimation of a possibly
large number of people at low resolution. To manage people overlapping, we
introduce a Pose-Aware Anchor Selection strategy. Moreover, as imbalance exists
between different people sizes in the image, and joints coordinates have
different uncertainties depending on these sizes, we propose a method to
automatically optimize weights associated to different people scales and joints
for efficient training. PandaNet surpasses previous single-shot methods on
several challenging datasets: a multi-person urban virtual but very realistic
dataset (JTA Dataset), and two real world 3D multi-person datasets (CMU
Panoptic and MuPoTS-3D).
</p>
<a href="http://arxiv.org/abs/2101.02471" target="_blank">arXiv:2101.02471</a> [<a href="http://arxiv.org/pdf/2101.02471" target="_blank">pdf</a>]

<h2>GAN-Control: Explicitly Controllable GANs. (arXiv:2101.02477v1 [cs.CV])</h2>
<h3>Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard Medioni</h3>
<p>We present a framework for training GANs with explicit control over generated
images. We are able to control the generated image by settings exact attributes
such as age, pose, expression, etc. Most approaches for editing GAN-generated
images achieve partial control by leveraging the latent space disentanglement
properties, obtained implicitly after standard GAN training. Such methods are
able to change the relative intensity of certain attributes, but not explicitly
set their values. Recently proposed methods, designed for explicit control over
human faces, harness morphable 3D face models to allow fine-grained control
capabilities in GANs. Unlike these methods, our control is not constrained to
morphable 3D face model parameters and is extendable beyond the domain of human
faces. Using contrastive learning, we obtain GANs with an explicitly
disentangled latent space. This disentanglement is utilized to train
control-encoders mapping human-interpretable inputs to suitable latent vectors,
thus allowing explicit control. In the domain of human faces we demonstrate
control over identity, age, pose, expression, hair color and illumination. We
also demonstrate control capabilities of our framework in the domains of
painted portraits and dog image generation. We demonstrate that our approach
achieves state-of-the-art performance both qualitatively and quantitatively.
</p>
<a href="http://arxiv.org/abs/2101.02477" target="_blank">arXiv:2101.02477</a> [<a href="http://arxiv.org/pdf/2101.02477" target="_blank">pdf</a>]

<h2>Active learning for object detection in high-resolution satellite images. (arXiv:2101.02480v1 [cs.CV])</h2>
<h3>Alex Goupilleau, Tugdual Ceillier, Marie-Caroline Corbineau</h3>
<p>In machine learning, the term active learning regroups techniques that aim at
selecting the most useful data to label from a large pool of unlabelled
examples. While supervised deep learning techniques have shown to be
increasingly efficient on many applications, they require a huge number of
labelled examples to reach operational performances. Therefore, the labelling
effort linked to the creation of the datasets required is also increasing. When
working on defense-related remote sensing applications, labelling can be
challenging due to the large areas covered and often requires military experts
who are rare and whose time is primarily dedicated to operational needs.
Limiting the labelling effort is thus of utmost importance. This study aims at
reviewing the most relevant active learning techniques to be used for object
detection on very high resolution imagery and shows an example of the value of
such techniques on a relevant operational use case: aircraft detection.
</p>
<a href="http://arxiv.org/abs/2101.02480" target="_blank">arXiv:2101.02480</a> [<a href="http://arxiv.org/pdf/2101.02480" target="_blank">pdf</a>]

<h2>Distances with mixed type variables some modified Gower's coefficients. (arXiv:2101.02481v1 [stat.ML])</h2>
<h3>Marcello D&#x27;Orazio</h3>
<p>Nearest neighbor methods have become popular in official statistics, mainly
in imputation or in statistical matching problems; they play a key role in
machine learning too, where a high number of variants have been proposed. The
choice of the distance function depends mainly on the type of the selected
variables. Unfortunately, relatively few options permit to handle mixed type
variables, a situation frequently encountered in official statistics. The most
popular distance for mixed type variables is derived as the complement of the
Gower's similarity coefficient; it is appealing because ranges between 0 and 1
and allows to handle missing values. Unfortunately, the unweighted standard
setting the contribution of the single variables to the overall Gower's
distance is unbalanced because of the different nature of the variables
themselves. This article tries to address the main drawbacks that affect the
overall unweighted Gower's distance by suggesting some modifications in
calculating the distance on the interval and ratio scaled variables. Simple
modifications try to attenuate the impact of outliers on the scaled Manhattan
distance; other modifications, relying on the kernel density estimation methods
attempt to reduce the unbalanced contribution of the different types of
variables. The performance of the proposals is evaluated in simulations
mimicking the imputation of missing values through nearest neighbor distance
hotdeck method.
</p>
<a href="http://arxiv.org/abs/2101.02481" target="_blank">arXiv:2101.02481</a> [<a href="http://arxiv.org/pdf/2101.02481" target="_blank">pdf</a>]

<h2>Robust Text CAPTCHAs Using Adversarial Examples. (arXiv:2101.02483v1 [cs.LG])</h2>
<h3>Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh</h3>
<p>CAPTCHA (Completely Automated Public Truing test to tell Computers and Humans
Apart) is a widely used technology to distinguish real users and automated
users such as bots. However, the advance of AI technologies weakens many
CAPTCHA tests and can induce security concerns. In this paper, we propose a
user-friendly text-based CAPTCHA generation method named Robust Text CAPTCHA
(RTC). At the first stage, the foregrounds and backgrounds are constructed with
randomly sampled font and background images, which are then synthesized into
identifiable pseudo adversarial CAPTCHAs. At the second stage, we design and
apply a highly transferable adversarial attack for text CAPTCHAs to better
obstruct CAPTCHA solvers. Our experiments cover comprehensive models including
shallow models such as KNN, SVM and random forest, various deep neural networks
and OCR models. Experiments show that our CAPTCHAs have a failure rate lower
than one millionth in general and high usability. They are also robust against
various defensive techniques that attackers may employ, including adversarial
training, data pre-processing and manual tagging.
</p>
<a href="http://arxiv.org/abs/2101.02483" target="_blank">arXiv:2101.02483</a> [<a href="http://arxiv.org/pdf/2101.02483" target="_blank">pdf</a>]

<h2>Deep Learning Methods for Vessel Trajectory Prediction based on Recurrent Neural Networks. (arXiv:2101.02486v1 [cs.CV])</h2>
<h3>Samuele Capobianco, Leonardo M. Millefiori, Nicola Forti, Paolo Braca, Peter Willett</h3>
<p>Data-driven methods open up unprecedented possibilities for maritime
surveillance using Automatic Identification System (AIS) data. In this work, we
explore deep learning strategies using historical AIS observations to address
the problem of predicting future vessel trajectories with a prediction horizon
of several hours. We propose novel sequence-to-sequence vessel trajectory
prediction models based on encoder-decoder recurrent neural networks (RNNs)
that are trained on historical trajectory data to predict future trajectory
samples given previous observations. The proposed architecture combines Long
Short-Term Memory (LSTM) RNNs for sequence modeling to encode the observed data
and generate future predictions with different intermediate aggregation layers
to capture space-time dependencies in sequential data. Experimental results on
vessel trajectories from an AIS dataset made freely available by the Danish
Maritime Authority show the effectiveness of deep-learning methods for
trajectory prediction based on sequence-to-sequence neural networks, which
achieve better performance than baseline approaches based on linear regression
or feed-forward networks. The comparative evaluation of results shows: i) the
superiority of attention pooling over static pooling for the specific
application, and ii) the remarkable performance improvement that can be
obtained with labeled trajectories, i.e. when predictions are conditioned on a
low-level context representation encoded from the sequence of past
observations, as well as on additional inputs (e.g., the port of departure or
arrival) about the vessel's high-level intention which may be available from
AIS.
</p>
<a href="http://arxiv.org/abs/2101.02486" target="_blank">arXiv:2101.02486</a> [<a href="http://arxiv.org/pdf/2101.02486" target="_blank">pdf</a>]

<h2>Corner case data description and detection. (arXiv:2101.02494v1 [cs.LG])</h2>
<h3>Tinghui Ouyang, Vicent Sant Marco, Yoshinao Isobe, Hideki Asoh, Yutaka Oiwa, Yoshiki Seo</h3>
<p>As the major factors affecting the safety of deep learning models, corner
cases and related detection are crucial in AI quality assurance for
constructing safety- and security-critical systems. The generic corner case
researches involve two interesting topics. One is to enhance DL models
robustness to corner case data via the adjustment on parameters/structure. The
other is to generate new corner cases for model retraining and improvement.
However, the complex architecture and the huge amount of parameters make the
robust adjustment of DL models not easy, meanwhile it is not possible to
generate all real-world corner cases for DL training. Therefore, this paper
proposes to a simple and novel study aiming at corner case data detection via a
specific metric. This metric is developed on surprise adequacy (SA) which has
advantages on capture data behaviors. Furthermore, targeting at characteristics
of corner case data, three modifications on distanced-based SA are developed
for classification applications in this paper. Consequently, through the
experiment analysis on MNIST data and industrial data, the feasibility and
usefulness of the proposed method on corner case data detection are verified.
</p>
<a href="http://arxiv.org/abs/2101.02494" target="_blank">arXiv:2101.02494</a> [<a href="http://arxiv.org/pdf/2101.02494" target="_blank">pdf</a>]

<h2>The joint role of geometry and illumination on material recognition. (arXiv:2101.02496v1 [cs.CV])</h2>
<h3>Manuel Lagunas, Ana Serrano, Diego Gutierrez, Belen Masia</h3>
<p>Observing and recognizing materials is a fundamental part of our daily life.
Under typical viewing conditions, we are capable of effortlessly identifying
the objects that surround us and recognizing the materials they are made of.
Nevertheless, understanding the underlying perceptual processes that take place
to accurately discern the visual properties of an object is a long-standing
problem. In this work, we perform a comprehensive and systematic analysis of
how the interplay of geometry, illumination, and their spatial frequencies
affects human performance on material recognition tasks. We carry out
large-scale behavioral experiments where participants are asked to recognize
different reference materials among a pool of candidate samples. In the
different experiments, we carefully sample the information in the frequency
domain of the stimuli. From our analysis, we find significant first-order
interactions between the geometry and the illumination, of both the reference
and the candidates. In addition, we observe that simple image statistics and
higher-order image histograms do not correlate with human performance.
Therefore, we perform a high-level comparison of highly non-linear statistics
by training a deep neural network on material recognition tasks. Our results
show that such models can accurately classify materials, which suggests that
they are capable of defining a meaningful representation of material appearance
from labeled proximal image data. Last, we find preliminary evidence that these
highly non-linear models and humans may use similar high-level factors for
material recognition tasks.
</p>
<a href="http://arxiv.org/abs/2101.02496" target="_blank">arXiv:2101.02496</a> [<a href="http://arxiv.org/pdf/2101.02496" target="_blank">pdf</a>]

<h2>Bridging In- and Out-of-distribution Samples for Their Better Discriminability. (arXiv:2101.02500v1 [cs.CV])</h2>
<h3>Engkarat Techapanurak, Anh-Chuong Dang, Takayuki Okatani</h3>
<p>This paper proposes a method for OOD detection. Questioning the premise of
previous studies that ID and OOD samples are separated distinctly, we consider
samples lying in the intermediate of the two and use them for training a
network. We generate such samples using multiple image transformations that
corrupt inputs in various ways and with different severity levels. We estimate
where the generated samples by a single image transformation lie between ID and
OOD using a network trained on clean ID samples. To be specific, we make the
network classify the generated samples and calculate their mean classification
accuracy, using which we create a soft target label for them. We train the same
network from scratch using the original ID samples and the generated samples
with the soft labels created for them. We detect OOD samples by thresholding
the entropy of the predicted softmax probability. The experimental results show
that our method outperforms the previous state-of-the-art in the standard
benchmark tests. We also analyze the effect of the number and particular
combinations of image corrupting transformations on the performance.
</p>
<a href="http://arxiv.org/abs/2101.02500" target="_blank">arXiv:2101.02500</a> [<a href="http://arxiv.org/pdf/2101.02500" target="_blank">pdf</a>]

<h2>Object Detection for Understanding Assembly Instruction Using Context-aware Data Augmentation and Cascade Mask R-CNN. (arXiv:2101.02509v1 [cs.RO])</h2>
<h3>J. Lee, S. Lee, S. Back, S. Shin, K. Lee</h3>
<p>Understanding assembly instruction has the potential to enhance the robot s
task planning ability and enables advanced robotic applications. To recognize
the key components from the 2D assembly instruction image, We mainly focus on
segmenting the speech bubble area, which contains lots of information about
instructions. For this, We applied Cascade Mask R-CNN and developed a
context-aware data augmentation scheme for speech bubble segmentation, which
randomly combines images cuts by considering the context of assembly
instructions. We showed that the proposed augmentation scheme achieves a better
segmentation performance compared to the existing augmentation algorithm by
increasing the diversity of trainable data while considering the distribution
of components locations. Also, we showed that deep learning can be useful to
understand assembly instruction by detecting the essential objects in the
assembly instruction, such as tools and parts.
</p>
<a href="http://arxiv.org/abs/2101.02509" target="_blank">arXiv:2101.02509</a> [<a href="http://arxiv.org/pdf/2101.02509" target="_blank">pdf</a>]

<h2>Learning Anthropometry from Rendered Humans. (arXiv:2101.02515v1 [cs.CV])</h2>
<h3>Song Yan, Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</h3>
<p>Accurate estimation of anthropometric body measurements from RGB images has
many potential applications in industrial design, online clothing, medical
diagnosis and ergonomics. Research on this topic is limited by the fact that
there exist only generated datasets which are based on fitting a 3D body mesh
to 3D body scans in the commercial CAESAR dataset. For 2D only silhouettes are
generated. To circumvent the data bottleneck, we introduce a new 3D scan
dataset of 2,675 female and 1,474 male scans. We also introduce a small dataset
of 200 RGB images and tape measured ground truth. With the help of the two new
datasets we propose a part-based shape model and a deep neural network for
estimating anthropometric measurements from 2D images. All data will be made
publicly available.
</p>
<a href="http://arxiv.org/abs/2101.02515" target="_blank">arXiv:2101.02515</a> [<a href="http://arxiv.org/pdf/2101.02515" target="_blank">pdf</a>]

<h2>Merging with unknown reliability. (arXiv:2101.02516v1 [cs.AI])</h2>
<h3>Paolo Liberatore</h3>
<p>Merging beliefs depends on the relative reliability of their sources. When
unknown, assuming equal reliability is unwarranted. The solution proposed in
this article is that every reliability profile is possible, and only what holds
according to all is accepted. Alternatively, one source is completely reliable,
but which one is unknown. These two cases motivate two existing forms of
merging: maxcons-based merging and arbitration.
</p>
<a href="http://arxiv.org/abs/2101.02516" target="_blank">arXiv:2101.02516</a> [<a href="http://arxiv.org/pdf/2101.02516" target="_blank">pdf</a>]

<h2>Few-Shot Learning with Class Imbalance. (arXiv:2101.02523v1 [cs.LG])</h2>
<h3>Mateusz Ochal, Massimiliano Patacchiola, Amos Storkey, Jose Vazquez, Sen Wang</h3>
<p>Few-shot learning aims to train models on a limited number of labeled samples
given in a support set in order to generalize to unseen samples from a query
set. In the standard setup, the support set contains an equal amount of data
points for each class. However, this assumption overlooks many practical
considerations arising from the dynamic nature of the real world, such as
class-imbalance. In this paper, we present a detailed study of few-shot
class-imbalance along three axes: meta-dataset vs. task imbalance, effect of
different imbalance distributions (linear, step, random), and effect of
rebalancing techniques. We extensively compare over 10 state-of-the-art
few-shot learning and meta-learning methods using unbalanced tasks and
meta-datasets. Our analysis using Mini-ImageNet reveals that 1) compared to the
balanced task, the performances on class-imbalance tasks counterparts always
drop, by up to $18.0\%$ for optimization-based methods, and up to $8.4$ for
metric-based methods, 2) contrary to popular belief, meta-learning algorithms,
such as MAML, do not automatically learn to balance by being exposed to
imbalanced tasks during (meta-)training time, 3) strategies used to mitigate
imbalance in supervised learning, such as oversampling, can offer a stronger
solution to the class imbalance problem, 4) the effect of imbalance at the
meta-dataset level is less significant than the effect at the task level with
similar imbalance magnitude. The code to reproduce the experiments is released
under an open-source license.
</p>
<a href="http://arxiv.org/abs/2101.02523" target="_blank">arXiv:2101.02523</a> [<a href="http://arxiv.org/pdf/2101.02523" target="_blank">pdf</a>]

<h2>MSED: a multi-modal sleep event detection model for clinical sleep analysis. (arXiv:2101.02530v1 [cs.CV])</h2>
<h3>Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B. D. Sorensen</h3>
<p>Study objective: Clinical sleep analysis require manual analysis of sleep
patterns for correct diagnosis of sleep disorders. Several studies show
significant variability in scoring discrete sleep events. We wished to
investigate, whether an automatic method could be used for detection of
arousals (Ar), leg movements (LM) and sleep disordered breathing (SDB) events,
and if the joint detection of these events performed better than having three
separate models.

Methods: We designed a single deep neural network architecture to jointly
detect sleep events in a polysomnogram. We trained the model on 1653 recordings
of individuals, and tested the optimized model on 1000 separate recordings. The
performance of the model was quantified by F1, precision, and recall scores,
and by correlating index values to clinical values using Pearson's correlation
coefficient.

Results: F1 scores for the optimized model was 0.70, 0.63, and 0.62 for Ar,
LM, and SDB, respectively. The performance was higher, when detecting events
jointly compared to corresponding single-event models. Index values computed
from detected events correlated well with manual annotations ($r^2$ = 0.73,
$r^2$ = 0.77, $r^2$ = 0.78, respectively).

Conclusion: Detecting arousals, leg movements and sleep disordered breathing
events jointly is possible, and the computed index values correlates well with
human annotations.
</p>
<a href="http://arxiv.org/abs/2101.02530" target="_blank">arXiv:2101.02530</a> [<a href="http://arxiv.org/pdf/2101.02530" target="_blank">pdf</a>]

<h2>Towards Understanding Learning in Neural Networks with Linear Teachers. (arXiv:2101.02533v1 [cs.LG])</h2>
<h3>Roei Sarussi, Alon Brutzkus, Amir Globerson</h3>
<p>Can a neural network minimizing cross-entropy learn linearly separable data?
Despite progress in the theory of deep learning, this question remains
unsolved. Here we prove that SGD globally optimizes this learning problem for a
two-layer network with Leaky ReLU activations. The learned network can in
principle be very complex. However, empirical evidence suggests that it often
turns out to be approximately linear. We provide theoretical support for this
phenomenon by proving that if network weights converge to two weight clusters,
this will imply an approximately linear decision boundary. Finally, we show a
condition on the optimization that leads to weight clustering. We provide
empirical results that validate our theoretical analysis.
</p>
<a href="http://arxiv.org/abs/2101.02533" target="_blank">arXiv:2101.02533</a> [<a href="http://arxiv.org/pdf/2101.02533" target="_blank">pdf</a>]

<h2>MRNet: a Multi-scale Residual Network for EEG-based Sleep Staging. (arXiv:2101.02538v1 [cs.LG])</h2>
<h3>Xue Jiang</h3>
<p>Sleep staging based on electroencephalogram (EEG) plays an important role in
the clinical diagnosis and treatment of sleep disorders. In order to emancipate
human experts from heavy labeling work, deep neural networks have been employed
to formulate automated sleep staging systems recently. However, EEG signals
lose considerable detailed information in network propagation, which affects
the representation of deep features. To address this problem, we propose a new
framework, called MRNet, for data-driven sleep staging by integrating a
multi-scale feature fusion model and a Markov-based sequential correction
algorithm. The backbone of MRNet is a residual block-based network, which
performs as a feature extractor.Then the fusion model constructs a feature
pyramid by concatenating the outputs from the different depths of the backbone,
which can help the network better comprehend the signals in different scales.
The Markov-based sequential correction algorithm is designed to reduce the
output jitters generated by the classifier. The algorithm depends on a prior
stage distribution associated with the sleep stage transition rule and the
Markov chain. Experiment results demonstrate the competitive performance of our
proposed approach on both accuracy and F1 score (e.g., 85.14% Acc and 78.91% F1
score on Sleep-EDFx, and 87.59% Acc and 79.62% F1 score on Sleep-EDF).
</p>
<a href="http://arxiv.org/abs/2101.02538" target="_blank">arXiv:2101.02538</a> [<a href="http://arxiv.org/pdf/2101.02538" target="_blank">pdf</a>]

<h2>On the Convergence of Tsetlin Machines for the XOR Operator. (arXiv:2101.02547v1 [cs.LG])</h2>
<h3>Lei Jiao, Xuan Zhang, Ole-Christoffer Granmo, K. Darshana Abeyrathna</h3>
<p>The Tsetlin Machine (TM) is a novel machine learning algorithm with several
distinct properties, including transparent inference and learning using
hardware-near building blocks. Although numerous papers explore the TM
empirically, many of its properties have not yet been analyzed mathematically.
In this article, we analyze the convergence of the TM when input is
non-linearly related to output by the XOR-operator. Our analysis reveals that
the TM, with just two conjunctive clauses, can converge almost surely to
reproducing XOR, learning from training data over an infinite time horizon.
Furthermore, the analysis shows how the hyper-parameter T guides clause
construction so that the clauses capture the distinct sub-patterns in the data.
Our analysis of convergence for XOR thus lays the foundation for analyzing
other more complex logical expressions. These analyses altogether, from a
mathematical perspective, provide new insights on why TMs have obtained
state-of-the-art performance on several pattern recognition problems
</p>
<a href="http://arxiv.org/abs/2101.02547" target="_blank">arXiv:2101.02547</a> [<a href="http://arxiv.org/pdf/2101.02547" target="_blank">pdf</a>]

<h2>Off-Policy Evaluation of Slate Policies under Bayes Risk. (arXiv:2101.02553v1 [cs.LG])</h2>
<h3>Nikos Vlassis, Fernando Amat Gil, Ashok Chandrashekar</h3>
<p>We study the problem of off-policy evaluation for slate bandits, for the
typical case in which the logging policy factorizes over the slots of the
slate. We slightly depart from the existing literature by taking Bayes risk as
the criterion by which to evaluate estimators, and we analyze the family of
'additive' estimators that includes the pseudoinverse (PI) estimator of
Swaminathan et al.\ (2017; arXiv:1605.04812). Using a control variate approach,
we identify a new estimator in this family that is guaranteed to have lower
risk than PI in the above class of problems. In particular, we show that the
risk improvement over PI grows linearly with the number of slots, and linearly
with the gap between the arithmetic and the harmonic mean of a set of
slot-level divergences between the logging and the target policy. In the
typical case of a uniform logging policy and a deterministic target policy,
each divergence corresponds to slot size, showing that maximal gains can be
obtained for slate problems with diverse numbers of actions per slot.
</p>
<a href="http://arxiv.org/abs/2101.02553" target="_blank">arXiv:2101.02553</a> [<a href="http://arxiv.org/pdf/2101.02553" target="_blank">pdf</a>]

<h2>Using BART for Multiobjective Optimization of Noisy Multiple Objectives. (arXiv:2101.02558v1 [cs.LG])</h2>
<h3>Akira Horiguchi, Thomas J. Santner, Ying Sun, Matthew T. Pratola</h3>
<p>Techniques to reduce the energy burden of an Industry 4.0 ecosystem often
require solving a multiobjective optimization problem. However, collecting
experimental data can often be either expensive or time-consuming. In such
cases, statistical methods can be helpful. This article proposes Pareto Front
(PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression
Trees (BART), which is a non-parametric model whose assumptions are typically
less restrictive than popular alternatives, such as Gaussian Processes. The
performance of our BART-based method is compared to a GP-based method using
analytic test functions, demonstrating convincing advantages. Finally, our
BART-based methodology is applied to a motivating Industry 4.0 engineering
problem.
</p>
<a href="http://arxiv.org/abs/2101.02558" target="_blank">arXiv:2101.02558</a> [<a href="http://arxiv.org/pdf/2101.02558" target="_blank">pdf</a>]

<h2>Open Set Domain Adaptation by Extreme Value Theory. (arXiv:2101.02561v1 [stat.ML])</h2>
<h3>Yiming Xu, Diego Klabjan</h3>
<p>Common domain adaptation techniques assume that the source domain and the
target domain share an identical label space, which is problematic since when
target samples are unlabeled we have no knowledge on whether the two domains
share the same label space. When this is not the case, the existing methods
fail to perform well because the additional unknown classes are also matched
with the source domain during adaptation. In this paper, we tackle the open set
domain adaptation problem under the assumption that the source and the target
label spaces only partially overlap, and the task becomes when the unknown
classes exist, how to detect the target unknown classes and avoid aligning them
with the source domain. We propose to utilize an instance-level reweighting
strategy for domain adaptation where the weights indicate the likelihood of a
sample belonging to known classes and to model the tail of the entropy
distribution with Extreme Value Theory for unknown class detection. Experiments
on conventional domain adaptation datasets show that the proposed method
outperforms the state-of-the-art models.
</p>
<a href="http://arxiv.org/abs/2101.02561" target="_blank">arXiv:2101.02561</a> [<a href="http://arxiv.org/pdf/2101.02561" target="_blank">pdf</a>]

<h2>HAVANA: Hierarchical and Variation-Normalized Autoencoder for Person Re-identification. (arXiv:2101.02568v1 [cs.CV])</h2>
<h3>Jiawei Ren, Xiao Ma, Chen Xu, Haiyu Zhao, Shuai Yi</h3>
<p>Person Re-Identification (Re-ID) is of great importance to the many video
surveillance systems. Learning discriminative features for Re-ID remains a
challenge due to the large variations in the image space, e.g., continuously
changing human poses, illuminations and point of views. In this paper, we
propose HAVANA, a novel extensible, light-weight HierArchical and
VAriation-Normalized Autoencoder that learns features robust to intra-class
variations. In contrast to existing generative approaches that prune the
variations with heavy extra supervised signals, HAVANA suppresses the
intra-class variations with a Variation-Normalized Autoencoder trained with no
additional supervision. We also introduce a novel Jensen-Shannon triplet loss
for contrastive distribution learning in Re-ID. In addition, we present
Hierarchical Variation Distiller, a hierarchical VAE to factorize the latent
representation and explicitly model the variations. To the best of our
knowledge, HAVANA is the first VAE-based framework for person ReID.
</p>
<a href="http://arxiv.org/abs/2101.02568" target="_blank">arXiv:2101.02568</a> [<a href="http://arxiv.org/pdf/2101.02568" target="_blank">pdf</a>]

<h2>A Clinical Evaluation of a Low-Cost Strain Gauge Respiration Belt and Machine Learning to Detect Sleep Apnea. (arXiv:2101.02595v1 [cs.LG])</h2>
<h3>Stein Kristiansen, Konstantinos Nikolaidis, Thomas Plagemann, Vera Goebel, Gunn Marit Traaen, Britt &#xd8;verland, Lars Aaker&#xf8;y, Tove-Elizabeth Hunt, Jan P&#xe5;l Loennechen, Sigurd Loe Steinshamn, Christina Holt Bendz, Ole-Gunnar Anfinsen, Lars Gullestad, Harriet Akre</h3>
<p>Sleep apnea is a serious and severely under-diagnosed sleep-related
respiration disorder characterized by repeated disrupted breathing events
during sleep. It is diagnosed via polysomnography which is an expensive test
conducted in a sleep lab requiring sleep experts to manually score the recorded
data. Since the symptoms of sleep apnea are often ambiguous, it is difficult
for a physician to decide whether to prescribe polysomnography. In this study,
we investigate whether helpful information can be obtained by collecting and
automatically analysing sleep data using a smartphone and an inexpensive strain
gauge respiration belt. We evaluate how accurately we can detect sleep apnea
with wide variety of machine learning techniques with data from a clinical
study with 49 overnight sleep recordings. With less than one hour of training,
we can distinguish between normal and apneic minutes with an accuracy,
sensitivity, and specificity of 0.7609, 0.7833, and 0.7217, respectively. These
results can be achieved even if we train only on high-quality data from an
entirely separate, clinically certified sensor, which has the potential to
substantially reduce the cost of data collection. Data from a complete night
can be analyzed in about one second on a smartphone.
</p>
<a href="http://arxiv.org/abs/2101.02595" target="_blank">arXiv:2101.02595</a> [<a href="http://arxiv.org/pdf/2101.02595" target="_blank">pdf</a>]

<h2>Learning a binary search with a recurrent neural network. A novel approach to ordinal regression analysis. (arXiv:2101.02609v1 [stat.ML])</h2>
<h3>Louis Falissard, Karim Bounebache, Gr&#xe9;goire Rey</h3>
<p>Deep neural networks are a family of computational models that are naturally
suited to the analysis of hierarchical data such as, for instance, sequential
data with the use of recurrent neural networks. In the other hand, ordinal
regression is a well-known predictive modelling problem used in fields as
diverse as psychometry to deep neural network based voice modelling. Their
specificity lies in the properties of their outcome variable, typically
considered as a categorical variable with natural ordering properties,
typically allowing comparisons between different states ("a little" is less
than "somewhat" which is itself less than "a lot", with transitivity allowed).
This article investigates the application of sequence-to-sequence learning
methods provided by the deep learning framework in ordinal regression, by
formulating the ordinal regression problem as a sequential binary search. A
method for visualizing the model's explanatory variables according to the
ordinal target variable is proposed, that bears some similarities to linear
discriminant analysis. The method is compared to traditional ordinal regression
methods on a number of benchmark dataset, and is shown to have comparable or
significantly better predictive power.
</p>
<a href="http://arxiv.org/abs/2101.02609" target="_blank">arXiv:2101.02609</a> [<a href="http://arxiv.org/pdf/2101.02609" target="_blank">pdf</a>]

<h2>Reinforced Imitative Graph Representation Learning for Mobile User Profiling: An Adversarial Training Perspective. (arXiv:2101.02634v1 [cs.AI])</h2>
<h3>Dongjie Wang, Pengyang Wang, Kunpeng Liu, Yuanchun Zhou, Charles Hughes, Yanjie Fu</h3>
<p>In this paper, we study the problem of mobile user profiling, which is a
critical component for quantifying users' characteristics in the human mobility
modeling pipeline. Human mobility is a sequential decision-making process
dependent on the users' dynamic interests. With accurate user profiles, the
predictive model can perfectly reproduce users' mobility trajectories. In the
reverse direction, once the predictive model can imitate users' mobility
patterns, the learned user profiles are also optimal. Such intuition motivates
us to propose an imitation-based mobile user profiling framework by exploiting
reinforcement learning, in which the agent is trained to precisely imitate
users' mobility patterns for optimal user profiles. Specifically, the proposed
framework includes two modules: (1) representation module, which produces state
combining user profiles and spatio-temporal context in real-time; (2) imitation
module, where Deep Q-network (DQN) imitates the user behavior (action) based on
the state that is produced by the representation module. However, there are two
challenges in running the framework effectively. First, epsilon-greedy strategy
in DQN makes use of the exploration-exploitation trade-off by randomly pick
actions with the epsilon probability. Such randomness feeds back to the
representation module, causing the learned user profiles unstable. To solve the
problem, we propose an adversarial training strategy to guarantee the
robustness of the representation module. Second, the representation module
updates users' profiles in an incremental manner, requiring integrating the
temporal effects of user profiles. Inspired by Long-short Term Memory (LSTM),
we introduce a gated mechanism to incorporate new and old user characteristics
into the user profile.
</p>
<a href="http://arxiv.org/abs/2101.02634" target="_blank">arXiv:2101.02634</a> [<a href="http://arxiv.org/pdf/2101.02634" target="_blank">pdf</a>]

<h2>qRRT: Quality-Biased Incremental RRT for Optimal Motion Planning in Non-Holonomic Systems. (arXiv:2101.02635v1 [cs.RO])</h2>
<h3>Nahas Pareekutty, Francis James, Balaraman Ravindran, Suril V. Shah</h3>
<p>This paper presents a sampling-based method for optimal motion planning in
non-holonomic systems in the absence of known cost functions. It uses the
principle of learning through experience to deduce the cost-to-go of regions
within the workspace. This cost information is used to bias an incremental
graph-based search algorithm that produces solution trajectories. Iterative
improvement of cost information and search biasing produces solutions that are
proven to be asymptotically optimal. The proposed framework builds on
incremental Rapidly-exploring Random Trees (RRT) for random sampling-based
search and Reinforcement Learning (RL) to learn workspace costs. A series of
experiments were performed to evaluate and demonstrate the performance of the
proposed method.
</p>
<a href="http://arxiv.org/abs/2101.02635" target="_blank">arXiv:2101.02635</a> [<a href="http://arxiv.org/pdf/2101.02635" target="_blank">pdf</a>]

<h2>A Large-Scale, Time-Synchronized Visible and Thermal Face Dataset. (arXiv:2101.02637v1 [cs.CV])</h2>
<h3>Domenick Poster, Matthew Thielke, Robert Nguyen, Srinivasan Rajaraman, Xing Di, Cedric Nimpa Fondje, Vishal M. Patel, Nathaniel J. Short, Benjamin S. Riggan, Nasser M. Nasrabadi, Shuowen Hu</h3>
<p>Thermal face imagery, which captures the naturally emitted heat from the
face, is limited in availability compared to face imagery in the visible
spectrum. To help address this scarcity of thermal face imagery for research
and algorithm development, we present the DEVCOM Army Research Laboratory
Visible-Thermal Face Dataset (ARL-VTF). With over 500,000 images from 395
subjects, the ARL-VTF dataset represents, to the best of our knowledge, the
largest collection of paired visible and thermal face images to date. The data
was captured using a modern long wave infrared (LWIR) camera mounted alongside
a stereo setup of three visible spectrum cameras. Variability in expressions,
pose, and eyewear has been systematically recorded. The dataset has been
curated with extensive annotations, metadata, and standardized protocols for
evaluation. Furthermore, this paper presents extensive benchmark results and
analysis on thermal face landmark detection and thermal-to-visible face
verification by evaluating state-of-the-art models on the ARL-VTF dataset.
</p>
<a href="http://arxiv.org/abs/2101.02637" target="_blank">arXiv:2101.02637</a> [<a href="http://arxiv.org/pdf/2101.02637" target="_blank">pdf</a>]

<h2>More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI Combination. (arXiv:2101.02639v1 [cs.CV])</h2>
<h3>Jian Dai, Shuge Lei, Licong Dong, Xiaona Lin, Huabin Zhang, Desheng Sun, Kehong Yuan</h3>
<p>Objective: Breast cancer screening is of great significance in contemporary
women's health prevention. The existing machines embedded in the AI system do
not reach the accuracy that clinicians hope. How to make intelligent systems
more reliable is a common problem. Methods: 1) Ultrasound image
super-resolution: the SRGAN super-resolution network reduces the unclearness of
ultrasound images caused by the device itself and improves the accuracy and
generalization of the detection model. 2) In response to the needs of medical
images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI
model: based on the respective advantages of different AI models, we employ two
AI models to determine clinical resuls cross validation. And we accept the same
results and refuses others. Results: 1) With the help of the super-resolution
model, the YOLOv4 model and the CenterNet model both increased the mAP score by
9.6% and 13.8%. 2) Two methods for transforming the target model into a
classification model are proposed. And the unified output is in a specified
format to facilitate the call of the molti-AI model. 3) In the classification
evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,
specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity
92.54%), the multi-AI model will refuse to make judgments on 23.55% of the
input data. Correspondingly, the performance has been greatly improved to
95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work
makes the AI model more reliable in medical image diagnosis. Significance: 1)
The proposed method makes the target detection model more suitable for
diagnosing breast ultrasound images. 2) It provides a new idea for artificial
intelligence in medical diagnosis, which can more conveniently introduce target
detection models from other fields to serve medical lesion screening.
</p>
<a href="http://arxiv.org/abs/2101.02639" target="_blank">arXiv:2101.02639</a> [<a href="http://arxiv.org/pdf/2101.02639" target="_blank">pdf</a>]

<h2>From Learning to Relearning: A Framework for Diminishing Bias in Social Robot Navigation. (arXiv:2101.02647v1 [cs.RO])</h2>
<h3>Juana Valeria Hurtado, Laura Londo&#xf1;o, Abhinav Valada</h3>
<p>The exponentially increasing advances in robotics and machine learning are
facilitating the transition of robots from being confined to controlled
industrial spaces to performing novel everyday tasks in domestic and urban
environments. In order to make the presence of robots safe as well as
comfortable for humans, and to facilitate their acceptance in public
environments, they are often equipped with social abilities for navigation and
interaction. Socially compliant robot navigation is increasingly being learned
from human observations or demonstrations. We argue that these techniques that
typically aim to mimic human behavior do not guarantee fair behavior. As a
consequence, social navigation models can replicate, promote, and amplify
societal unfairness such as discrimination and segregation. In this work, we
investigate a framework for diminishing bias in social robot navigation models
so that robots are equipped with the capability to plan as well as adapt their
paths based on both physical and social demands. Our proposed framework
consists of two components: learning which incorporates social context into the
learning process to account for safety and comfort, and relearning to detect
and correct potentially harmful outcomes before the onset. We provide both
technological and sociological analysis using three diverse case studies in
different social scenarios of interaction. Moreover, we present ethical
implications of deploying robots in social environments and propose potential
solutions. Through this study, we highlight the importance and advocate for
fairness in human-robot interactions in order to promote more equitable social
relationships, roles, and dynamics and consequently positively influence our
society.
</p>
<a href="http://arxiv.org/abs/2101.02647" target="_blank">arXiv:2101.02647</a> [<a href="http://arxiv.org/pdf/2101.02647" target="_blank">pdf</a>]

<h2>Argument Schemes and Dialogue for Explainable Planning. (arXiv:2101.02648v1 [cs.AI])</h2>
<h3>Quratul-ain Mahesar, Simon Parsons</h3>
<p>Artificial Intelligence (AI) is being increasingly deployed in practical
applications. However, there is a major concern whether AI systems will be
trusted by humans. In order to establish trust in AI systems, there is a need
for users to understand the reasoning behind their solutions. Therefore,
systems should be able to explain and justify their output. In this paper, we
propose an argument scheme-based approach to provide explanations in the domain
of AI planning. We present novel argument schemes to create arguments that
explain a plan and its key elements; and a set of critical questions that allow
interaction between the arguments and enable the user to obtain further
information regarding the key elements of the plan. Furthermore, we present a
novel dialogue system using the argument schemes and critical questions for
providing interactive dialectical explanations.
</p>
<a href="http://arxiv.org/abs/2101.02648" target="_blank">arXiv:2101.02648</a> [<a href="http://arxiv.org/pdf/2101.02648" target="_blank">pdf</a>]

<h2>CoachNet: An Adversarial Sampling Approach for Reinforcement Learning. (arXiv:2101.02649v1 [cs.LG])</h2>
<h3>Elmira Amirloo Abolfathi, Jun Luo, Peyman Yadmellat, Kasra Rezaee</h3>
<p>Despite the recent successes of reinforcement learning in games and robotics,
it is yet to become broadly practical. Sample efficiency and unreliable
performance in rare but challenging scenarios are two of the major obstacles.
Drawing inspiration from the effectiveness of deliberate practice for achieving
expert-level human performance, we propose a new adversarial sampling approach
guided by a failure predictor named "CoachNet". CoachNet is trained online
along with the agent to predict the probability of failure. This probability is
then used in a stochastic sampling process to guide the agent to more
challenging episodes. This way, instead of wasting time on scenarios that the
agent has already mastered, training is focused on the agent's "weak spots". We
present the design of CoachNet, explain its underlying principles, and
empirically demonstrate its effectiveness in improving sample efficiency and
test-time robustness in common continuous control tasks.
</p>
<a href="http://arxiv.org/abs/2101.02649" target="_blank">arXiv:2101.02649</a> [<a href="http://arxiv.org/pdf/2101.02649" target="_blank">pdf</a>]

<h2>An automated machine learning-genetic algorithm (AutoML-GA) approach for efficient simulation-driven engine design optimization. (arXiv:2101.02653v1 [cs.LG])</h2>
<h3>Opeoluwa Owoyele, Pinaki Pal, Alvaro Vidal Torreira, Daniel Probst, Matthew Shaxted, Michael Wilde, Peter Kelly Senecal</h3>
<p>In recent years, the use of machine learning techniques as surrogate models
for computational fluid dynamics (CFD) simulations has emerged as a promising
method for reducing the computational cost associated with engine design
optimization. However, such methods still suffer from drawbacks. One main
disadvantage of such methods is that the default machine learning
hyperparameters are often severely suboptimal for a given problem. This has
often been addressed by manually trying out different hyperparameter settings,
but this solution is ineffective in a high-dimensional hyperparameter space.
Besides this problem, the amount of data needed for training is also not known
a priori. In response to these issues which need to be addressed, this work
describes and validates an automated active learning approach for
surrogate-based optimization of internal combustion engines, AutoML-GA. In this
approach, a Bayesian optimization technique is used to find the best machine
learning hyperparameters based on an initial dataset obtained from a small
number of CFD simulations. Subsequently, a genetic algorithm is employed to
locate the design optimum on the surrogate surface trained with the optimal
hyperparameters. In the vicinity of the design optimum, the solution is refined
by repeatedly running CFD simulations at the projected optimum and adding the
newly obtained data to the training dataset. It is shown that this approach
leads to a better optimum with a lower number of CFD simulations, compared to
the use of default hyperparameters. The developed approach offers the advantage
of being a more hands-off approach that can be easily applied by researchers
and engineers in industry who do not have a machine learning background.
</p>
<a href="http://arxiv.org/abs/2101.02653" target="_blank">arXiv:2101.02653</a> [<a href="http://arxiv.org/pdf/2101.02653" target="_blank">pdf</a>]

<h2>A review of robotics taxonomies in terms of form and structure. (arXiv:2101.02659v1 [cs.RO])</h2>
<h3>Signe A. Redfield</h3>
<p>Identifying and categorizing specific robot tasks, behaviors, and resources
is an essential precursor to reproducing and evaluating robotics experiments
across laboratories and platforms. Without some means of capturing how one
environment, platform, or behavior differs from another, we cannot begin to
establish the performance impact of these changes or predict a robot's
performance in a novel environment. As a first step towards experimental
reproducibility, existing taxonomies in the field of robotics are reviewed and
common patterns of structure and form extracted, identifying both the
properties they share with traditional taxonomies and the necessary structural
elements that draw from other classification and categorization systems. The
diversity of taxonomy subjects and subsequent difficulty in harmonization of
conceptual underpinnings is noted. Robotics taxonomies are shown to be deeply
fragmented in structure and form and to require notation that can support
complex relationships.
</p>
<a href="http://arxiv.org/abs/2101.02659" target="_blank">arXiv:2101.02659</a> [<a href="http://arxiv.org/pdf/2101.02659" target="_blank">pdf</a>]

<h2>L2PF -- Learning to Prune Faster. (arXiv:2101.02663v1 [cs.CV])</h2>
<h3>Manoj-Rohit Vemparala, Nael Fasfous, Alexander Frickenstein, Mhd Ali Moraly, Aquib Jamal, Lukas Frickenstein, Christian Unger, Naveen-Shankar Nagaraja, Walter Stechele</h3>
<p>Various applications in the field of autonomous driving are based on
convolutional neural networks (CNNs), especially for processing camera data.
The optimization of such CNNs is a major challenge in continuous development.
Newly learned features must be brought into vehicles as quickly as possible,
and as such, it is not feasible to spend redundant GPU hours during
compression. In this context, we present Learning to Prune Faster which details
a multi-task, try-and-learn method, discretely learning redundant filters of
the CNN and a continuous action of how long the layers have to be fine-tuned.
This allows us to significantly speed up the convergence process of learning
how to find an embedded-friendly filter-wise pruned CNN. For ResNet20, we have
achieved a compression ratio of 3.84 x with minimal accuracy degradation.
Compared to the state-of-the-art pruning method, we reduced the GPU hours by
1.71 x.
</p>
<a href="http://arxiv.org/abs/2101.02663" target="_blank">arXiv:2101.02663</a> [<a href="http://arxiv.org/pdf/2101.02663" target="_blank">pdf</a>]

<h2>Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v1 [cs.CV])</h2>
<h3>Prarthana Bhattacharyya, Chengjie Huang, Krzysztof Czarnecki</h3>
<p>Most existing point-cloud based 3D object detectors use convolution-like
operators to process information in a local neighbourhood with fixed-weight
kernels and aggregate global context hierarchically. However, recent work on
non-local neural networks and self-attention for 2D vision has shown that
explicitly modeling global context and long-range interactions between
positions can lead to more robust and competitive models. In this paper, we
explore two variants of self-attention for contextual modeling in 3D object
detection by augmenting convolutional features with self-attention features. We
first incorporate the pairwise self-attention mechanism into the current
state-of-the-art BEV, voxel and point-based detectors and show consistent
improvement over strong baseline models while simultaneously significantly
reducing their parameter footprint and computational cost. We also propose a
self-attention variant that samples a subset of the most representative
features by learning deformations over randomly sampled locations. This not
only allows us to scale explicit global contextual modeling to larger
point-clouds, but also leads to more discriminative and informative feature
descriptors. Our method can be flexibly applied to most state-of-the-art
detectors with increased accuracy and parameter and compute efficiency. We
achieve new state-of-the-art detection performance on KITTI and nuScenes
datasets. Code is available at
\url{https://github.com/AutoVision-cloud/SA-Det3D}.
</p>
<a href="http://arxiv.org/abs/2101.02672" target="_blank">arXiv:2101.02672</a> [<a href="http://arxiv.org/pdf/2101.02672" target="_blank">pdf</a>]

<h2>Planning for Multi-stage Forceful Manipulation. (arXiv:2101.02679v1 [cs.RO])</h2>
<h3>Rachel Holladay, Tom&#xe1;s Lozano-P&#xe9;rez, Alberto Rodriguez</h3>
<p>Multi-stage forceful manipulation tasks, such as twisting a nut on a bolt,
require reasoning over interlocking constraints over discrete as well as
continuous choices. The robot must choose a sequence of discrete actions, or
strategy, such as whether to pick up an object, and the continuous parameters
of each of those actions, such as how to grasp the object. In forceful
manipulation tasks, the force requirements substantially impact the choices of
both strategy and parameters. To enable planning and executing forceful
manipulation, we augment an existing task and motion planner with controllers
that exert wrenches and constraints that explicitly consider torque and
frictional limits. In two domains, opening a childproof bottle and twisting a
nut, we demonstrate how the system considers a combinatorial number of
strategies and how choosing actions that are robust to parameter variations
impacts the choice of strategy.
</p>
<a href="http://arxiv.org/abs/2101.02679" target="_blank">arXiv:2101.02679</a> [<a href="http://arxiv.org/pdf/2101.02679" target="_blank">pdf</a>]

<h2>Zero-shot sim-to-real transfer of tactile control policies for aggressive swing-up manipulation. (arXiv:2101.02680v1 [cs.RO])</h2>
<h3>Thomas Bi, Carmelo Sferrazza, Raffaello D&#x27;Andrea</h3>
<p>This paper aims to show that robots equipped with a vision-based tactile
sensor can perform dynamic manipulation tasks without prior knowledge of all
the physical attributes of the objects to be manipulated. For this purpose, a
robotic system is presented that is able to swing up poles of different masses,
radii and lengths, to an angle of 180 degrees, while relying solely on the
feedback provided by the tactile sensor. This is achieved by developing a novel
simulator that accurately models the interaction of a pole with the soft
sensor. A feedback policy that is conditioned on a sensory observation history,
and which has no prior knowledge of the physical features of the pole, is then
learned in the aforementioned simulation. When evaluated on the physical
system, the policy is able to swing up a wide range of poles that differ
significantly in their physical attributes without further adaptation. To the
authors' knowledge, this is the first work where a feedback policy from
high-dimensional tactile observations is used to control the swing-up
manipulation of poles in closed-loop.
</p>
<a href="http://arxiv.org/abs/2101.02680" target="_blank">arXiv:2101.02680</a> [<a href="http://arxiv.org/pdf/2101.02680" target="_blank">pdf</a>]

<h2>The Effect of Prior Lipschitz Continuity on the Adversarial Robustness of Bayesian Neural Networks. (arXiv:2101.02689v1 [stat.ML])</h2>
<h3>Arno Blaas, Stephen J. Roberts</h3>
<p>It is desirable, and often a necessity, for machine learning models to be
robust against adversarial attacks. This is particularly true for Bayesian
models, as they are well-suited for safety-critical applications, in which
adversarial attacks can have catastrophic outcomes. In this work, we take a
deeper look at the adversarial robustness of Bayesian Neural Networks (BNNs).
In particular, we consider whether the adversarial robustness of a BNN can be
increased by model choices, particularly the Lipschitz continuity induced by
the prior. Conducting in-depth analysis on the case of i.i.d., zero-mean
Gaussian priors and posteriors approximated via mean-field variational
inference, we find evidence that adversarial robustness is indeed sensitive to
the prior variance.
</p>
<a href="http://arxiv.org/abs/2101.02689" target="_blank">arXiv:2101.02689</a> [<a href="http://arxiv.org/pdf/2101.02689" target="_blank">pdf</a>]

<h2>Self-Supervised Pretraining of 3D Features on any Point-Cloud. (arXiv:2101.02691v1 [cs.CV])</h2>
<h3>Zaiwei Zhang, Rohit Girdhar, Armand Joulin, Ishan Misra</h3>
<p>Pretraining on large labeled datasets is a prerequisite to achieve good
performance in many computer vision tasks like 2D object recognition, video
classification etc. However, pretraining is not widely used for 3D recognition
tasks where state-of-the-art methods train models from scratch. A primary
reason is the lack of large annotated datasets because 3D data is both
difficult to acquire and time consuming to label. We present a simple
self-supervised pertaining method that can work with any 3D data - single or
multiview, indoor or outdoor, acquired by varied sensors, without 3D
registration. We pretrain standard point cloud and voxel based model
architectures, and show that joint pretraining further improves performance. We
evaluate our models on 9 benchmarks for object detection, semantic
segmentation, and object classification, where they achieve state-of-the-art
results and can outperform supervised pretraining. We set a new
state-of-the-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5%
mAP). Our pretrained models are label efficient and improve performance for
classes with few examples.
</p>
<a href="http://arxiv.org/abs/2101.02691" target="_blank">arXiv:2101.02691</a> [<a href="http://arxiv.org/pdf/2101.02691" target="_blank">pdf</a>]

<h2>Where2Act: From Pixels to Actions for Articulated 3D Objects. (arXiv:2101.02692v1 [cs.CV])</h2>
<h3>Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, Shubham Tulsiani</h3>
<p>One of the fundamental goals of visual perception is to allow agents to
meaningfully interact with their environment. In this paper, we take a step
towards that long-term goal -- we extract highly localized actionable
information related to elementary actions such as pushing or pulling for
articulated objects with movable parts. For example, given a drawer, our
network predicts that applying a pulling force on the handle opens the drawer.
We propose, discuss, and evaluate novel network architectures that given image
and depth data, predict the set of actions possible at each pixel, and the
regions over articulated parts that are likely to move under the force. We
propose a learning-from-interaction framework with an online data sampling
strategy that allows us to train the network in simulation (SAPIEN) and
generalizes across categories. But more importantly, our learned models even
transfer to real-world data. Check the project website for the code and data
release.
</p>
<a href="http://arxiv.org/abs/2101.02692" target="_blank">arXiv:2101.02692</a> [<a href="http://arxiv.org/pdf/2101.02692" target="_blank">pdf</a>]

<h2>PVA: Pixel-aligned Volumetric Avatars. (arXiv:2101.02697v1 [cs.CV])</h2>
<h3>Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi</h3>
<p>Acquisition and rendering of photo-realistic human heads is a highly
challenging research problem of particular importance for virtual telepresence.
Currently, the highest quality is achieved by volumetric approaches trained in
a person specific manner on multi-view data. These models better represent fine
structure, such as hair, compared to simpler mesh-based models. Volumetric
models typically employ a global code to represent facial expressions, such
that they can be driven by a small set of animation parameters. While such
architectures achieve impressive rendering quality, they can not easily be
extended to the multi-identity setting. In this paper, we devise a novel
approach for predicting volumetric avatars of the human head given just a small
number of inputs. We enable generalization across identities by a novel
parameterization that combines neural radiance fields with local, pixel-aligned
features extracted directly from the inputs, thus sidestepping the need for
very deep or complex networks. Our approach is trained in an end-to-end manner
solely based on a photometric re-rendering loss without requiring explicit 3D
supervision.We demonstrate that our approach outperforms the existing state of
the art in terms of quality and is able to generate faithful facial expressions
in a multi-identity setting.
</p>
<a href="http://arxiv.org/abs/2101.02697" target="_blank">arXiv:2101.02697</a> [<a href="http://arxiv.org/pdf/2101.02697" target="_blank">pdf</a>]

<h2>TrackFormer: Multi-Object Tracking with Transformers. (arXiv:2101.02702v1 [cs.CV])</h2>
<h3>Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, Christoph Feichtenhofer</h3>
<p>We present TrackFormer, an end-to-end multi-object tracking and segmentation
model based on an encoder-decoder Transformer architecture. Our approach
introduces track query embeddings which follow objects through a video sequence
in an autoregressive fashion. New track queries are spawned by the DETR object
detector and embed the position of their corresponding object over time. The
Transformer decoder adjusts track query embeddings from frame to frame, thereby
following the changing object positions. TrackFormer achieves a seamless data
association between frames in a new tracking-by-attention paradigm by self- and
encoder-decoder attention mechanisms which simultaneously reason about
location, occlusion, and object identity. TrackFormer yields state-of-the-art
performance on the tasks of multi-object tracking (MOT17) and segmentation
(MOTS20). We hope our unified way of performing detection and tracking will
foster future research in multi-object tracking and video understanding. Code
will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2101.02702" target="_blank">arXiv:2101.02702</a> [<a href="http://arxiv.org/pdf/2101.02702" target="_blank">pdf</a>]

<h2>Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v1 [cs.LG])</h2>
<h3>Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, Michael I. Jordan</h3>
<p>To communicate instance-wise uncertainty for prediction tasks, we show how to
generate set-valued predictions for black-box predictors that control the
expected loss on future test points at a user-specified level. Our approach
provides explicit finite-sample guarantees for any dataset by using a holdout
set to calibrate the size of the prediction sets. This framework enables
simple, distribution-free, rigorous error control for many tasks, and we
demonstrate it in five large-scale machine learning problems: (1)
classification problems where some mistakes are more costly than others; (2)
multi-label classification, where each observation has multiple associated
labels; (3) classification problems where the labels have a hierarchical
structure; (4) image segmentation, where we wish to predict a set of pixels
containing an object of interest; and (5) protein structure prediction. Lastly,
we discuss extensions to uncertainty quantification for ranking, metric
learning and distributionally robust learning.
</p>
<a href="http://arxiv.org/abs/2101.02703" target="_blank">arXiv:2101.02703</a> [<a href="http://arxiv.org/pdf/2101.02703" target="_blank">pdf</a>]

<h2>Subspace Clustering using Ensembles of $K$-Subspaces. (arXiv:1709.04744v3 [cs.CV] UPDATED)</h2>
<h3>John Lipor, David Hong, Yan Shuo Tan, Laura Balzano</h3>
<p>Subspace clustering is the unsupervised grouping of points lying near a union
of low-dimensional linear subspaces. Algorithms based directly on geometric
properties of such data tend to either provide poor empirical performance, lack
theoretical guarantees, or depend heavily on their initialization. We present a
novel geometric approach to the subspace clustering problem that leverages
ensembles of the K-subspaces (KSS) algorithm via the evidence accumulation
clustering framework. Our algorithm, referred to as ensemble K-subspaces
(EKSS), forms a co-association matrix whose (i,j)th entry is the number of
times points i and j are clustered together by several runs of KSS with random
initializations. We prove general recovery guarantees for any algorithm that
forms an affinity matrix with entries close to a monotonic transformation of
pairwise absolute inner products. We then show that a specific instance of EKSS
results in an affinity matrix with entries of this form, and hence our proposed
algorithm can provably recover subspaces under similar conditions to
state-of-the-art algorithms. The finding is, to the best of our knowledge, the
first recovery guarantee for evidence accumulation clustering and for KSS
variants. We show on synthetic data that our method performs well in the
traditionally challenging settings of subspaces with large intersection,
subspaces with small principal angles, and noisy data. Finally, we evaluate our
algorithm on six common benchmark datasets and show that unlike existing
methods, EKSS achieves excellent empirical performance when there are both a
small and large number of points per subspace.
</p>
<a href="http://arxiv.org/abs/1709.04744" target="_blank">arXiv:1709.04744</a> [<a href="http://arxiv.org/pdf/1709.04744" target="_blank">pdf</a>]

<h2>Deep Knowledge Tracing and Dynamic Student Classification for Knowledge Tracing. (arXiv:1809.08713v2 [cs.AI] UPDATED)</h2>
<h3>Sein Minn, Yi Yu, Michel C. Desmarais, Feida Zhu, Jill Jenn Vie</h3>
<p>In Intelligent Tutoring System (ITS), tracing the student's knowledge state
during learning has been studied for several decades in order to provide more
supportive learning instructions. In this paper, we propose a novel model for
knowledge tracing that i) captures students' learning ability and dynamically
assigns students into distinct groups with similar ability at regular time
intervals, and ii) combines this information with a Recurrent Neural Network
architecture known as Deep Knowledge Tracing. Experimental results confirm that
the proposed model is significantly better at predicting student performance
than well known state-of-the-art techniques for student modelling.
</p>
<a href="http://arxiv.org/abs/1809.08713" target="_blank">arXiv:1809.08713</a> [<a href="http://arxiv.org/pdf/1809.08713" target="_blank">pdf</a>]

<h2>Construction of Differentially Private Empirical Distributions from a low-order Marginals Set through Solving Linear Equations with l2 Regularization. (arXiv:1812.05671v3 [cs.LG] UPDATED)</h2>
<h3>Evercita C. Eugenio, Fang Liu</h3>
<p>We introduce a new algorithm, Construction of dIfferentially Private
Empirical Distributions from a low-order marginals set tHrough solving linear
Equations with l2 Regularization (CIPHER), that produces differentially private
empirical joint distributions from a set of low-order marginals. CIPHER is
conceptually simple and requires no more than decomposing joint probabilities
via basic probability rules to construct a linear equation set and subsequently
solving the equations. Compared to the full-dimensional histogram (FDH)
sanitization, CIPHER has drastic\-ally lower requirements on computational
storage and memory, which is practically attractive especially considering that
the high-order signals preserved by the FDH sanitization are likely just sample
randomness and rarely of interest. Our experiments demonstrate that CIPHER
outperforms the multiplicative weighting exponential mechanism in preserving
original information and has similar or superior cost-normalized utility to FDH
sanitization at the same privacy budget.
</p>
<a href="http://arxiv.org/abs/1812.05671" target="_blank">arXiv:1812.05671</a> [<a href="http://arxiv.org/pdf/1812.05671" target="_blank">pdf</a>]

<h2>Universal Bayes consistency in metric spaces. (arXiv:1906.09855v7 [cs.LG] UPDATED)</h2>
<h3>Steve Hanneke, Aryeh Kontorovich, Sivan Sabato, Roi Weiss</h3>
<p>We extend a recently proposed 1-nearest-neighbor based multiclass learning
algorithm and prove that our modification is universally strongly
Bayes-consistent in all metric spaces admitting any such learner, making it an
"optimistically universal" Bayes-consistent learner. This is the first learning
algorithm known to enjoy this property; by comparison, the $k$-NN classifier
and its variants are not generally universally Bayes-consistent, except under
additional structural assumptions, such as an inner product, a norm, finite
dimension, or a Besicovitch-type property. The metric spaces in which universal
Bayes consistency is possible are the "essentially separable" ones -- a notion
that we define, which is more general than standard separability. The existence
of metric spaces that are not essentially separable is widely believed to be
independent of the ZFC axioms of set theory. We prove that essential
separability exactly characterizes the existence of a universal
Bayes-consistent learner for the given metric space. In particular, this yields
the first impossibility result for universal Bayes consistency. Taken together,
our results completely characterize strong and weak universal Bayes consistency
in metric spaces.
</p>
<a href="http://arxiv.org/abs/1906.09855" target="_blank">arXiv:1906.09855</a> [<a href="http://arxiv.org/pdf/1906.09855" target="_blank">pdf</a>]

<h2>Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate. (arXiv:1910.04858v2 [cs.CV] UPDATED)</h2>
<h3>Lu Mi, Hao Wang, Yonglong Tian, Nir Shavit</h3>
<p>Uncertainty estimation is an essential step in the evaluation of the
robustness for deep learning models in computer vision, especially when applied
in risk-sensitive areas. However, most state-of-the-art deep learning models
either fail to obtain uncertainty estimation or need significant modification
(e.g., formulating a proper Bayesian treatment) to obtain it. Most previous
methods are not able to take an arbitrary model off the shelf and generate
uncertainty estimation without retraining or redesigning it. To address this
gap, we perform a systematic exploration into training-free uncertainty
estimation for dense regression, an unrecognized yet important problem, and
provide a theoretical construction justifying such estimations. We propose
three simple and scalable methods to analyze the variance of outputs from a
trained network under tolerable perturbations: infer-transformation,
infer-noise, and infer-dropout. They operate solely during inference, without
the need to re-train, re-design, or fine-tune the model, as typically required
by state-of-the-art uncertainty estimation methods. Surprisingly, even without
involving such perturbations in training, our methods produce comparable or
even better uncertainty estimation when compared to training-required
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/1910.04858" target="_blank">arXiv:1910.04858</a> [<a href="http://arxiv.org/pdf/1910.04858" target="_blank">pdf</a>]

<h2>Interaction Relational Network for Mutual Action Recognition. (arXiv:1910.04963v2 [cs.CV] UPDATED)</h2>
<h3>Mauricio Perez, Jun Liu, Alex C. Kot</h3>
<p>Person-person mutual action recognition (also referred to as interaction
recognition) is an important research branch of human activity analysis.
Current solutions in the field -- mainly dominated by CNNs, GCNs and LSTMs --
often consist of complicated architectures and mechanisms to embed the
relationships between the two persons on the architecture itself, to ensure the
interaction patterns can be properly learned. Our main contribution with this
work is by proposing a simpler yet very powerful architecture, named
Interaction Relational Network, which utilizes minimal prior knowledge about
the structure of the human body. We drive the network to identify by itself how
to relate the body parts from the individuals interacting. In order to better
represent the interaction, we define two different relationships, leading to
specialized architectures and models for each. These multiple relationship
models will then be fused into a single and special architecture, in order to
leverage both streams of information for further enhancing the relational
reasoning capability. Furthermore we define important structured pair-wise
operations to extract meaningful extra information from each pair of joints --
distance and motion. Ultimately, with the coupling of an LSTM, our IRN is
capable of paramount sequential relational reasoning. These important
extensions we made to our network can also be valuable to other problems that
require sophisticated relational reasoning. Our solution is able to achieve
state-of-the-art performance on the traditional interaction recognition
datasets SBU and UT, and also on the mutual actions from the large-scale
dataset NTU RGB+D. Furthermore, it obtains competitive performance in the NTU
RGB+D 120 dataset interactions subset.
</p>
<a href="http://arxiv.org/abs/1910.04963" target="_blank">arXiv:1910.04963</a> [<a href="http://arxiv.org/pdf/1910.04963" target="_blank">pdf</a>]

<h2>Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation. (arXiv:1910.07779v2 [stat.ML] UPDATED)</h2>
<h3>Ryan-Rhys Griffiths, Alexander A. Aldrick, Miguel Garcia-Ortegon, Vidhi R. Lalchand, Alpha A. Lee</h3>
<p>Bayesian optimisation is a sample-efficient search methodology that holds
great promise for accelerating drug and materials discovery programs. A
frequently-overlooked modelling consideration in Bayesian optimisation
strategies however, is the representation of heteroscedastic aleatoric
uncertainty. In many practical applications it is desirable to identify inputs
with low aleatoric noise, an example of which might be a material composition
which consistently displays robust properties in response to a noisy
fabrication process. In this paper, we propose a heteroscedastic Bayesian
optimisation scheme capable of representing and minimising aleatoric noise
across the input space. Our scheme employs a heteroscedastic Gaussian process
(GP) surrogate model in conjunction with two straightforward adaptations of
existing acquisition functions. First, we extend the augmented expected
improvement (AEI) heuristic to the heteroscedastic setting and second, we
introduce the aleatoric noise-penalised expected improvement (ANPEI) heuristic.
Both methodologies are capable of penalising aleatoric noise in the suggestions
and yield improved performance relative to homoscedastic Bayesian optimisation
and random sampling on toy problems as well as on two real-world scientific
datasets. Code is available at:
\url{https://github.com/Ryan-Rhys/Heteroscedastic-BO}
</p>
<a href="http://arxiv.org/abs/1910.07779" target="_blank">arXiv:1910.07779</a> [<a href="http://arxiv.org/pdf/1910.07779" target="_blank">pdf</a>]

<h2>Single-shot 3D multi-person pose estimation in complex images. (arXiv:1911.03391v2 [cs.CV] UPDATED)</h2>
<h3>Abdallah Benzine, Bertrand Luvison, Quoc Cuong Pham, Catherine Achard</h3>
<p>In this paper, we propose a new single shot method for multi-person 3D human
pose estimation in complex images. The model jointly learns to locate the human
joints in the image, to estimate their 3D coordinates and to group these
predictions into full human skeletons. The proposed method deals with a
variable number of people and does not need bounding boxes to estimate the 3D
poses. It leverages and extends the Stacked Hourglass Network and its
multi-scale feature learning to manage multi-person situations. Thus, we
exploit a robust 3D human pose formulation to fully describe several 3D human
poses even in case of strong occlusions or crops. Then, joint grouping and
human pose estimation for an arbitrary number of people are performed using the
associative embedding method. Our approach significantly outperforms the state
of the art on the challenging CMU Panoptic and a previous single shot method on
the MuPoTS-3D dataset. Furthermore, it leads to good results on the complex and
synthetic images from the newly proposed JTA Dataset.
</p>
<a href="http://arxiv.org/abs/1911.03391" target="_blank">arXiv:1911.03391</a> [<a href="http://arxiv.org/pdf/1911.03391" target="_blank">pdf</a>]

<h2>Attack-Resistant Federated Learning with Residual-based Reweighting. (arXiv:1912.11464v2 [cs.LG] UPDATED)</h2>
<h3>Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen</h3>
<p>Federated learning has a variety of applications in multiple domains by
utilizing private training data stored on different devices. However, the
aggregation process in federated learning is highly vulnerable to adversarial
attacks so that the global model may behave abnormally under attacks. To tackle
this challenge, we present a novel aggregation algorithm with residual-based
reweighting to defend federated learning. Our aggregation algorithm combines
repeated median regression with the reweighting scheme in iteratively
reweighted least squares. Our experiments show that our aggregation algorithm
outperforms other alternative algorithms in the presence of label-flipping,
backdoor, and Gaussian noise attacks. We also provide theoretical guarantees
for our aggregation algorithm.
</p>
<a href="http://arxiv.org/abs/1912.11464" target="_blank">arXiv:1912.11464</a> [<a href="http://arxiv.org/pdf/1912.11464" target="_blank">pdf</a>]

<h2>L6DNet: Light 6 DoF Network for Robust and Precise Object Pose Estimation with Small Datasets. (arXiv:2002.00911v5 [cs.CV] UPDATED)</h2>
<h3>Mathieu Gonzalez, Amine Kacete, Albert Murienne, Eric Marchand</h3>
<p>Estimating the 3D pose of an object is a challenging task that can be
considered within augmented reality or robotic applications. In this paper, we
propose a novel approach to perform 6 DoF object pose estimation from a single
RGB-D image. We adopt a hybrid pipeline in two stages: data-driven and
geometric respectively. The data-driven step consists of a classification CNN
to estimate the object 2D location in the image from local patches, followed by
a regression CNN trained to predict the 3D location of a set of keypoints in
the camera coordinate system. To extract the pose information, the geometric
step consists in aligning the 3D points in the camera coordinate system with
the corresponding 3D points in world coordinate system by minimizing a
registration error, thus computing the pose. Our experiments on the standard
dataset LineMod show that our approach is more robust and accurate than
state-of-the-art methods. The approach is also validated to achieve a 6 DoF
positioning task by visual servoing.
</p>
<a href="http://arxiv.org/abs/2002.00911" target="_blank">arXiv:2002.00911</a> [<a href="http://arxiv.org/pdf/2002.00911" target="_blank">pdf</a>]

<h2>DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving. (arXiv:2002.06987v3 [cs.LG] UPDATED)</h2>
<h3>Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, Guang Lin</h3>
<p>Click-through rate (CTR) prediction is a crucial task in online display
advertising. The embedding-based neural networks have been proposed to learn
both explicit feature interactions through a shallow component and deep feature
interactions using a deep neural network (DNN) component. These sophisticated
models, however, slow down the prediction inference by at least hundreds of
times. To address the issue of significantly increased serving delay and high
memory usage for ad serving in production, this paper presents
\emph{DeepLight}: a framework to accelerate the CTR predictions in three
aspects: 1) accelerate the model inference via explicitly searching informative
feature interactions in the shallow component; 2) prune redundant layers and
parameters at intra-layer and inter-layer level in the DNN component; 3)
promote the sparsity of the embedding layer to preserve the most discriminant
signals. By combining the above efforts, the proposed approach accelerates the
model inference by 46X on Criteo dataset and 27X on Avazu dataset without any
loss on the prediction accuracy. This paves the way for successfully deploying
complicated embedding-based neural networks in production for ad serving.
</p>
<a href="http://arxiv.org/abs/2002.06987" target="_blank">arXiv:2002.06987</a> [<a href="http://arxiv.org/pdf/2002.06987" target="_blank">pdf</a>]

<h2>Informative Bayesian Neural Network Priors for Weak Signals. (arXiv:2002.10243v2 [stat.ML] UPDATED)</h2>
<h3>Tianyu Cui, Aki Havulinna, Pekka Marttinen, Samuel Kaski</h3>
<p>Encoding domain knowledge into the prior over the high-dimensional weight
space of a neural network is challenging but essential in applications with
limited data and weak signals. Two types of domain knowledge are commonly
available in scientific applications: 1. feature sparsity (fraction of features
deemed relevant); 2. signal-to-noise ratio, quantified, for instance, as the
proportion of variance explained (PVE). We show how to encode both types of
domain knowledge into the widely used Gaussian scale mixture priors with
Automatic Relevance Determination. Specifically, we propose a new joint prior
over the local (i.e., feature-specific) scale parameters that encodes knowledge
about feature sparsity, and a Stein gradient optimization to tune the
hyperparameters in such a way that the distribution induced on the model's PVE
matches the prior distribution. We show empirically that the new prior improves
prediction accuracy, compared to existing neural network priors, on several
publicly available datasets and in a genetics application where signals are
weak and sparse, often outperforming even computationally intensive
cross-validation for hyperparameter tuning.
</p>
<a href="http://arxiv.org/abs/2002.10243" target="_blank">arXiv:2002.10243</a> [<a href="http://arxiv.org/pdf/2002.10243" target="_blank">pdf</a>]

<h2>Fast Online Adaptation in Robotics through Meta-Learning Embeddings of Simulated Priors. (arXiv:2003.04663v2 [cs.RO] UPDATED)</h2>
<h3>Rituraj Kaushik, Timoth&#xe9;e Anne, Jean-Baptiste Mouret</h3>
<p>Meta-learning algorithms can accelerate the model-based reinforcement
learning (MBRL) algorithms by finding an initial set of parameters for the
dynamical model such that the model can be trained to match the actual dynamics
of the system with only a few data-points. However, in the real world, a robot
might encounter any situation starting from motor failures to finding itself in
a rocky terrain where the dynamics of the robot can be significantly different
from one another. In this paper, first, we show that when meta-training
situations (the prior situations) have such diverse dynamics, using a single
set of meta-trained parameters as a starting point still requires a large
number of observations from the real system to learn a useful model of the
dynamics. Second, we propose an algorithm called FAMLE that mitigates this
limitation by meta-training several initial starting points (i.e., initial
parameters) for training the model and allows the robot to select the most
suitable starting point to adapt the model to the current situation with only a
few gradient steps. We compare FAMLE to MBRL, MBRL with a meta-trained model
with MAML, and model-free policy search algorithm PPO for various simulated and
real robotic tasks, and show that FAMLE allows the robots to adapt to novel
damages in significantly fewer time-steps than the baselines.
</p>
<a href="http://arxiv.org/abs/2003.04663" target="_blank">arXiv:2003.04663</a> [<a href="http://arxiv.org/pdf/2003.04663" target="_blank">pdf</a>]

<h2>MIM-Based GAN: Information Metric to Amplify Small Probability Events Importance in Generative Adversarial Networks. (arXiv:2003.11285v2 [cs.LG] UPDATED)</h2>
<h3>Rui She, Pingyi Fan</h3>
<p>In terms of Generative Adversarial Networks (GANs), the information metric to
discriminate the generative data from the real data, lies in the key point of
generation efficiency, which plays an important role in GAN-based applications,
especially in anomaly detection. As for the original GAN, there exist drawbacks
for its hidden information measure based on KL divergence on rare events
generation and training performance for adversarial networks. Therefore, it is
significant to investigate the metrics used in GANs to improve the generation
ability as well as bring gains in the training process. In this paper, we adopt
the exponential form, referred from the information measure, i.e. MIM, to
replace the logarithm form of the original GAN. This approach is called
MIM-based GAN, has better performance on networks training and rare events
generation. Specifically, we first discuss the characteristics of training
process in this approach. Moreover, we also analyze its advantages on
generating rare events in theory. In addition, we do simulations on the
datasets of MNIST and ODDS to see that the MIM-based GAN achieves
state-of-the-art performance on anomaly detection compared with some classical
GANs.
</p>
<a href="http://arxiv.org/abs/2003.11285" target="_blank">arXiv:2003.11285</a> [<a href="http://arxiv.org/pdf/2003.11285" target="_blank">pdf</a>]

<h2>Learning from Aggregate Observations. (arXiv:2004.06316v3 [stat.ML] UPDATED)</h2>
<h3>Yivan Zhang, Nontawat Charoenphakdee, Zhenguo Wu, Masashi Sugiyama</h3>
<p>We study the problem of learning from aggregate observations where
supervision signals are given to sets of instances instead of individual
instances, while the goal is still to predict labels of unseen individuals. A
well-known example is multiple instance learning (MIL). In this paper, we
extend MIL beyond binary classification to other problems such as multiclass
classification and regression. We present a general probabilistic framework
that accommodates a variety of aggregate observations, e.g., pairwise
similarity/triplet comparison for classification and mean/difference/rank
observation for regression. Simple maximum likelihood solutions can be applied
to various differentiable models such as deep neural networks and gradient
boosting machines. Moreover, we develop the concept of consistency up to an
equivalence relation to characterize our estimator and show that it has nice
convergence properties under mild assumptions. Experiments on three problem
settings -- classification via triplet comparison and regression via mean/rank
observation indicate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2004.06316" target="_blank">arXiv:2004.06316</a> [<a href="http://arxiv.org/pdf/2004.06316" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation. (arXiv:2004.08878v2 [cs.CV] UPDATED)</h2>
<h3>Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma</h3>
<p>Unsupervised domain adaptation (UDA) aims to adapt existing models of the
source domain to a new target domain with only unlabeled data. Many
adversarial-based UDA methods involve high-instability training and have to
carefully tune the optimization procedure. Some non-adversarial UDA methods
employ a consistency regularization on the target predictions of a student
model and a teacher model under different perturbations, where the teacher
shares the same architecture with the student and is updated by the exponential
moving average of the student. However, these methods suffer from noticeable
negative transfer resulting from either the error-prone discriminator network
or the unreasonable teacher model. In this paper, we propose an
uncertainty-aware consistency regularization method for cross-domain semantic
segmentation. By exploiting the latent uncertainty information of the target
samples, more meaningful and reliable knowledge from the teacher model can be
transferred to the student model. In addition, we further reveal the reason why
the current consistency regularization is often unstable in minimizing the
distribution discrepancy. We also show that our method can effectively ease
this issue by mining the most reliable and meaningful samples with a dynamic
weighting scheme of consistency loss. Experiments demonstrate that the proposed
method outperforms the state-of-the-art methods on two domain adaptation
benchmarks, $i.e.,$ GTAV $\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $
Cityscapes.
</p>
<a href="http://arxiv.org/abs/2004.08878" target="_blank">arXiv:2004.08878</a> [<a href="http://arxiv.org/pdf/2004.08878" target="_blank">pdf</a>]

<h2>Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation. (arXiv:2005.14553v2 [cs.CV] UPDATED)</h2>
<h3>Christos Sakaridis, Dengxin Dai, Luc Van Gool</h3>
<p>We address the problem of semantic nighttime image segmentation and improve
the state-of-the-art, by adapting daytime models to nighttime without using
nighttime annotations. Moreover, we design a new evaluation framework to
address the substantial uncertainty of semantics in nighttime images. Our
central contributions are: 1) a curriculum framework to gradually adapt
semantic segmentation models from day to night through progressively darker
times of day, exploiting cross-time-of-day correspondences between daytime
images from a reference map and dark images to guide the label inference in the
dark domains; 2) a novel uncertainty-aware annotation and evaluation framework
and metric for semantic segmentation, including image regions beyond human
recognition capability in the evaluation in a principled fashion; 3) the Dark
Zurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilight
images with correspondences to their daytime counterparts plus a set of 201
nighttime images with fine pixel-level annotations created with our protocol,
which serves as a first benchmark for our novel evaluation. Experiments show
that our map-guided curriculum adaptation significantly outperforms
state-of-the-art methods on nighttime sets both for standard metrics and our
uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals
that selective invalidation of predictions can improve results on data with
ambiguous content such as our benchmark and profit safety-oriented applications
involving invalid inputs.
</p>
<a href="http://arxiv.org/abs/2005.14553" target="_blank">arXiv:2005.14553</a> [<a href="http://arxiv.org/pdf/2005.14553" target="_blank">pdf</a>]

<h2>Conditional Classification: A Solution for Computational Energy Reduction. (arXiv:2006.15799v3 [cs.LG] UPDATED)</h2>
<h3>Ali Mirzaeian, Sai Manoj, Ashkan Vakil, Houman Homayoun, Avesta Sasan</h3>
<p>Deep convolutional neural networks have shown high efficiency in computer
visions and other applications. However, with the increase in the depth of the
networks, the computational complexity is growing exponentially. In this paper,
we propose a novel solution to reduce the computational complexity of
convolutional neural network models used for many class image classification.
Our proposed technique breaks the classification task into two steps: 1)
coarse-grain classification, in which the input samples are classified among a
set of hyper-classes, 2) fine-grain classification, in which the final labels
are predicted among those hyper-classes detected at the first step. We
illustrate that our proposed classifier can reach the level of accuracy
reported by the best in class classification models with less computational
complexity (Flop Count) by only activating parts of the model that are needed
for the image classification.
</p>
<a href="http://arxiv.org/abs/2006.15799" target="_blank">arXiv:2006.15799</a> [<a href="http://arxiv.org/pdf/2006.15799" target="_blank">pdf</a>]

<h2>Personalized Cross-Silo Federated Learning on Non-IID Data. (arXiv:2007.03797v3 [cs.LG] UPDATED)</h2>
<h3>Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, Yong Zhang</h3>
<p>Non-IID data present a tough challenge for federated learning. In this paper,
we explore a novel idea of facilitating pairwise collaborations between clients
with similar data. We propose FedAMP, a new method employing federated
attentive message passing to facilitate similar clients to collaborate more. We
establish the convergence of FedAMP for both convex and non-convex models, and
propose a heuristic method to further improve the performance of FedAMP when
clients adopt deep neural networks as personalized models. Our extensive
experiments on benchmark data sets demonstrate the superior performance of the
proposed methods.
</p>
<a href="http://arxiv.org/abs/2007.03797" target="_blank">arXiv:2007.03797</a> [<a href="http://arxiv.org/pdf/2007.03797" target="_blank">pdf</a>]

<h2>Distributed Training of Graph Convolutional Networks. (arXiv:2007.06281v2 [cs.LG] UPDATED)</h2>
<h3>Simone Scardapane, Indro Spinelli, Paolo Di Lorenzo</h3>
<p>The aim of this work is to develop a fully-distributed algorithmic framework
for training graph convolutional networks (GCNs). The proposed method is able
to exploit the meaningful relational structure of the input data, which are
collected by a set of agents that communicate over a sparse network topology.
After formulating the centralized GCN training problem, we first show how to
make inference in a distributed scenario where the underlying data graph is
split among different agents. Then, we propose a distributed gradient descent
procedure to solve the GCN training problem. The resulting model distributes
computation along three lines: during inference, during back-propagation, and
during optimization. Convergence to stationary solutions of the GCN training
problem is also established under mild conditions. Finally, we propose an
optimization criterion to design the communication topology between agents in
order to match with the graph describing data relationships. A wide set of
numerical results validate our proposal. To the best of our knowledge, this is
the first work combining graph convolutional neural networks with distributed
optimization.
</p>
<a href="http://arxiv.org/abs/2007.06281" target="_blank">arXiv:2007.06281</a> [<a href="http://arxiv.org/pdf/2007.06281" target="_blank">pdf</a>]

<h2>Time-Reversal Symmetric ODE Network. (arXiv:2007.11362v3 [cs.LG] UPDATED)</h2>
<h3>In Huh, Eunho Yang, Sung Ju Hwang, Jinwoo Shin</h3>
<p>Time-reversal symmetry, which requires that the dynamics of a system should
not change with the reversal of time axis, is a fundamental property that
frequently holds in classical and quantum mechanics. In this paper, we propose
a novel loss function that measures how well our ordinary differential equation
(ODE) networks comply with this time-reversal symmetry; it is formally defined
by the discrepancy in the time evolutions of ODE networks between forward and
backward dynamics. Then, we design a new framework, which we name as
Time-Reversal Symmetric ODE Networks (TRS-ODENs), that can learn the dynamics
of physical systems more sample-efficiently by learning with the proposed loss
function. We evaluate TRS-ODENs on several classical dynamics, and find they
can learn the desired time evolution from observed noisy and complex
trajectories. We also show that, even for systems that do not possess the full
time-reversal symmetry, TRS-ODENs can achieve better predictive performances
over baselines.
</p>
<a href="http://arxiv.org/abs/2007.11362" target="_blank">arXiv:2007.11362</a> [<a href="http://arxiv.org/pdf/2007.11362" target="_blank">pdf</a>]

<h2>Neural Sparse Voxel Fields. (arXiv:2007.11571v2 [cs.CV] UPDATED)</h2>
<h3>Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt</h3>
<p>Photo-realistic free-viewpoint rendering of real-world scenes using classical
computer graphics techniques is challenging, because it requires the difficult
step of capturing detailed appearance and geometry models. Recent studies have
demonstrated promising results by learning scene representations that
implicitly encode both geometry and appearance without 3D supervision. However,
existing approaches in practice often show blurry renderings caused by the
limited network capacity or the difficulty in finding accurate intersections of
camera rays with the scene geometry. Synthesizing high-resolution imagery from
these representations often requires time-consuming optical ray marching. In
this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene
representation for fast and high-quality free-viewpoint rendering. NSVF defines
a set of voxel-bounded implicit fields organized in a sparse voxel octree to
model local properties in each cell. We progressively learn the underlying
voxel structures with a differentiable ray-marching operation from only a set
of posed RGB images. With the sparse voxel octree structure, rendering novel
views can be accelerated by skipping the voxels containing no relevant scene
content. Our method is typically over 10 times faster than the state-of-the-art
(namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving
higher quality results. Furthermore, by utilizing an explicit sparse voxel
representation, our method can easily be applied to scene editing and scene
composition. We also demonstrate several challenging tasks, including
multi-scene learning, free-viewpoint rendering of a moving human, and
large-scale scene rendering. Code and data are available at our website:
https://github.com/facebookresearch/NSVF.
</p>
<a href="http://arxiv.org/abs/2007.11571" target="_blank">arXiv:2007.11571</a> [<a href="http://arxiv.org/pdf/2007.11571" target="_blank">pdf</a>]

<h2>Low Dimensional State Representation Learning with Reward-shaped Priors. (arXiv:2007.16044v2 [cs.LG] UPDATED)</h2>
<h3>Nicol&#xf2; Botteghi, Ruben Obbink, Daan Geijs, Mannes Poel, Beril Sirmacek, Christoph Brune, Abeje Mersha, Stefano Stramigioli</h3>
<p>Reinforcement Learning has been able to solve many complicated robotics tasks
without any need for feature engineering in an end-to-end fashion. However,
learning the optimal policy directly from the sensory inputs, i.e the
observations, often requires processing and storage of a huge amount of data.
In the context of robotics, the cost of data from real robotics hardware is
usually very high, thus solutions that achieve high sample-efficiency are
needed. We propose a method that aims at learning a mapping from the
observations into a lower-dimensional state space. This mapping is learned with
unsupervised learning using loss functions shaped to incorporate prior
knowledge of the environment and the task. Using the samples from the state
space, the optimal policy is quickly and efficiently learned. We test the
method on several mobile robot navigation tasks in a simulation environment and
also on a real robot.
</p>
<a href="http://arxiv.org/abs/2007.16044" target="_blank">arXiv:2007.16044</a> [<a href="http://arxiv.org/pdf/2007.16044" target="_blank">pdf</a>]

<h2>End-to-end Full Projector Compensation. (arXiv:2008.00965v3 [cs.CV] UPDATED)</h2>
<h3>Bingyao Huang, Tao Sun, Haibin Ling</h3>
<p>Full projector compensation aims to modify a projector input image to
compensate for both geometric and photometric disturbance of the projection
surface. Traditional methods usually solve the two parts separately and may
suffer from suboptimal solutions. In this paper, we propose the first
end-to-end differentiable solution, named CompenNeSt++, to solve the two
problems jointly. First, we propose a novel geometric correction subnet, named
WarpingNet, which is designed with a cascaded coarse-to-fine structure to learn
the sampling grid directly from sampling images. Second, we propose a novel
photometric compensation subnet, named CompenNeSt, which is designed with a
siamese architecture to capture the photometric interactions between the
projection surface and the projected images, and to use such information to
compensate the geometrically corrected images. By concatenating WarpingNet with
CompenNeSt, CompenNeSt++ accomplishes full projector compensation and is
end-to-end trainable. Third, to improve practicability, we propose a novel
synthetic data-based pre-training strategy to significantly reduce the number
of training images and training time. Moreover, we construct the first
setup-independent full compensation benchmark to facilitate future studies. In
thorough experiments, our method shows clear advantages over prior art with
promising compensation quality and meanwhile being practically convenient.
</p>
<a href="http://arxiv.org/abs/2008.00965" target="_blank">arXiv:2008.00965</a> [<a href="http://arxiv.org/pdf/2008.00965" target="_blank">pdf</a>]

<h2>Attribute Prototype Network for Zero-Shot Learning. (arXiv:2008.08290v3 [cs.CV] UPDATED)</h2>
<h3>Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata</h3>
<p>From the beginning of zero-shot learning research, visual attributes have
been shown to play an important role. In order to better transfer
attribute-based knowledge from known to unknown classes, we argue that an image
representation with integrated attribute localization ability would be
beneficial for zero-shot learning. To this end, we propose a novel zero-shot
representation learning framework that jointly learns discriminative global and
local features using only class-level attributes. While a visual-semantic
embedding layer learns global features, local features are learned through an
attribute prototype network that simultaneously regresses and decorrelates
attributes from intermediate features. We show that our locality augmented
image representations achieve a new state-of-the-art on three zero-shot
learning benchmarks. As an additional benefit, our model points to the visual
evidence of the attributes in an image, e.g. for the CUB dataset, confirming
the improved attribute localization ability of our image representation.
</p>
<a href="http://arxiv.org/abs/2008.08290" target="_blank">arXiv:2008.08290</a> [<a href="http://arxiv.org/pdf/2008.08290" target="_blank">pdf</a>]

<h2>Reimagining City Configuration: Automated Urban Planning via Adversarial Learning. (arXiv:2008.09912v2 [cs.AI] UPDATED)</h2>
<h3>Dongjie Wang, Yanjie Fu, Pengyang Wang, Bo Huang, Chang-Tien Lu</h3>
<p>Urban planning refers to the efforts of designing land-use configurations.
Effective urban planning can help to mitigate the operational and social
vulnerability of a urban system, such as high tax, crimes, traffic congestion
and accidents, pollution, depression, and anxiety. Due to the high complexity
of urban systems, such tasks are mostly completed by professional planners.
But, human planners take longer time. The recent advance of deep learning
motivates us to ask: can machines learn at a human capability to automatically
and quickly calculate land-use configuration, so human planners can finally
adjust machine-generated plans for specific needs? To this end, we formulate
the automated urban planning problem into a task of learning to configure
land-uses, given the surrounding spatial contexts. To set up the task, we
define a land-use configuration as a longitude-latitude-channel tensor, where
each channel is a category of POIs and the value of an entry is the number of
POIs. The objective is then to propose an adversarial learning framework that
can automatically generate such tensor for an unplanned area. In particular, we
first characterize the contexts of surrounding areas of an unplanned area by
learning representations from spatial graphs using geographic and human
mobility data. Second, we combine each unplanned area and its surrounding
context representation as a tuple, and categorize all the tuples into positive
(well-planned areas) and negative samples (poorly-planned areas). Third, we
develop an adversarial land-use configuration approach, where the surrounding
context representation is fed into a generator to generate a land-use
configuration, and a discriminator learns to distinguish among positive and
negative samples.
</p>
<a href="http://arxiv.org/abs/2008.09912" target="_blank">arXiv:2008.09912</a> [<a href="http://arxiv.org/pdf/2008.09912" target="_blank">pdf</a>]

<h2>A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection. (arXiv:2009.13592v4 [cs.CV] UPDATED)</h2>
<h3>Kemal Oksuz, Baris Can Cam, Emre Akbas, Sinan Kalkan</h3>
<p>We propose average Localisation-Recall-Precision (aLRP), a unified, bounded,
balanced and ranking-based loss function for both classification and
localisation tasks in object detection. aLRP extends the
Localisation-Recall-Precision (LRP) performance metric (Oksuz et al., 2018)
inspired from how Average Precision (AP) Loss extends precision to a
ranking-based loss function for classification (Chen et al., 2020). aLRP has
the following distinct advantages: (i) aLRP is the first ranking-based loss
function for both classification and localisation tasks. (ii) Thanks to using
ranking for both tasks, aLRP naturally enforces high-quality localisation for
high-precision classification. (iii) aLRP provides provable balance between
positives and negatives. (iv) Compared to on average $\sim$6 hyperparameters in
the loss functions of state-of-the-art detectors, aLRP Loss has only one
hyperparameter, which we did not tune in practice. On the COCO dataset, aLRP
Loss improves its ranking-based predecessor, AP Loss, up to around $5$ AP
points, achieves $48.9$ AP without test time augmentation and outperforms all
one-stage detectors. Code available at: https://github.com/kemaloksuz/aLRPLoss .
</p>
<a href="http://arxiv.org/abs/2009.13592" target="_blank">arXiv:2009.13592</a> [<a href="http://arxiv.org/pdf/2009.13592" target="_blank">pdf</a>]

<h2>Vehicle predictive trajectory patterns from isochronous data. (arXiv:2010.05026v2 [cs.LG] UPDATED)</h2>
<h3>D. Damian</h3>
<p>Measuring and analyzing sensor data is the basic technique in vehicle
dynamics development and with the advancement of embedded and data acquisition
systems it is possible to analyze large data sets. In this paper a detailed
method is presented for assessing and mapping isochronous trajectory patterns
in Graz (Austria) by using data fusion from video, ArduinoUno and the compass
sensor HDMM01. The predictive isochronous trajectory patterns are derived from
the data values for a predefined time horizon. Both extreme driving behavior
and hazardous road geometries can be identified. It is possible to provide
instant road sensor data which can be used to compare the data from a
trajectory path as well as for different time instances. Results of this study
show that the trajectory patterns are successful in predicting the likely
evolution of a current trajectory pattern and can provide assessment on future
driving situations. The obtained data from this study can be useful as
reference in future city planning for energy saving driving pathways as well as
vehicle design and engineering improvements based on quantitative and relevant
dynamic measurements.
</p>
<a href="http://arxiv.org/abs/2010.05026" target="_blank">arXiv:2010.05026</a> [<a href="http://arxiv.org/pdf/2010.05026" target="_blank">pdf</a>]

<h2>Adaptive-Attentive Geolocalization from few queries: a hybrid approach. (arXiv:2010.06897v2 [cs.CV] UPDATED)</h2>
<h3>Gabriele Moreno Berton, Valerio Paolicelli, Carlo Masone, Barbara Caputo</h3>
<p>We address the task of cross-domain visual place recognition, where the goal
is to geolocalize a given query image against a labeled gallery, in the case
where the query and the gallery belong to different visual domains. To achieve
this, we focus on building a domain robust deep network by leveraging over an
attention mechanism combined with few-shot unsupervised domain adaptation
techniques, where we use a small number of unlabeled target domain images to
learn about the target distribution. With our method, we are able to outperform
the current state of the art while using two orders of magnitude less target
domain images. Finally we propose a new large-scale dataset for cross-domain
visual place recognition, called SVOX. The pytorch code is available at
https://github.com/valeriopaolicelli/AdAGeo .
</p>
<a href="http://arxiv.org/abs/2010.06897" target="_blank">arXiv:2010.06897</a> [<a href="http://arxiv.org/pdf/2010.06897" target="_blank">pdf</a>]

<h2>DIFER: Differentiable Automated Feature Engineering. (arXiv:2010.08784v2 [cs.LG] UPDATED)</h2>
<h3>Guanghui Zhu, Zhuoer Xu, Xu Guo, Chunfeng Yuan, Yihua Huang</h3>
<p>Feature engineering, a crucial step of machine learning, aims to extract
useful features from raw data to improve data quality. In recent years, great
efforts have been devoted to Automated Feature Engineering (AutoFE) to replace
expensive human labor. However, existing methods are computationally demanding
due to treating AutoFE as a coarse-grained black-box optimization problem over
a discrete space. In this work, we propose an efficient gradient-based method
called DIFER to perform differentiable automated feature engineering in a
continuous vector space. DIFER selects potential features based on evolutionary
algorithm and leverages an encoder-predictor-decoder controller to optimize
existing features. We map features into the continuous vector space via the
encoder, optimize the embedding along the gradient direction induced by the
predicted score, and recover better features from the optimized embedding by
the decoder. Extensive experiments on classification and regression datasets
demonstrate that DIFER can significantly improve the performance of various
machine learning algorithms and outperform current state-of-the-art AutoFE
methods in terms of both efficiency and performance.
</p>
<a href="http://arxiv.org/abs/2010.08784" target="_blank">arXiv:2010.08784</a> [<a href="http://arxiv.org/pdf/2010.08784" target="_blank">pdf</a>]

<h2>Meta-path Free Semi-supervised Learning for Heterogeneous Networks. (arXiv:2010.08924v2 [cs.LG] UPDATED)</h2>
<h3>Shin-woo Park, Byung Jun Bae, Jinyoung Yeo, Seung-won Hwang</h3>
<p>Graph neural networks (GNNs) have been widely used in representation learning
on graphs and achieved superior performance in tasks such as node
classification. However, analyzing heterogeneous graph of different types of
nodes and links still brings great challenges for injecting the heterogeneity
into a graph neural network. A general remedy is to manually or automatically
design meta-paths to transform a heterogeneous graph into a homogeneous graph,
but this is suboptimal since the features from the first-order neighbors are
not fully leveraged for training and inference. In this paper, we propose
simple and effective graph neural networks for heterogeneous graph, excluding
the use of meta-paths. Specifically, our models focus on relaxing the
heterogeneity stress for model parameters by expanding model capacity of
general GNNs in an effective way. Extensive experimental results on six
real-world graphs not only show the superior performance of our proposed models
over the state-of-the-arts, but also demonstrate the potentially good balance
between reducing the heterogeneity stress and increasing the parameter size.
Our code is freely available for reproducing our results.
</p>
<a href="http://arxiv.org/abs/2010.08924" target="_blank">arXiv:2010.08924</a> [<a href="http://arxiv.org/pdf/2010.08924" target="_blank">pdf</a>]

<h2>Model-Based Inverse Reinforcement Learning from Visual Demonstrations. (arXiv:2010.09034v2 [cs.RO] UPDATED)</h2>
<h3>Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayaraman, Akshara Rai, Franziska Meier</h3>
<p>Scaling model-based inverse reinforcement learning (IRL) to real robotic
manipulation tasks with unknown dynamics remains an open problem. The key
challenges lie in learning good dynamics models, developing algorithms that
scale to high-dimensional state-spaces and being able to learn from both visual
and proprioceptive demonstrations. In this work, we present a gradient-based
inverse reinforcement learning framework that utilizes a pre-trained visual
dynamics model to learn cost functions when given only visual human
demonstrations. The learned cost functions are then used to reproduce the
demonstrated behavior via visual model predictive control. We evaluate our
framework on hardware on two basic object manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2010.09034" target="_blank">arXiv:2010.09034</a> [<a href="http://arxiv.org/pdf/2010.09034" target="_blank">pdf</a>]

<h2>Weakly-Supervised Amodal Instance Segmentation with Compositional Priors. (arXiv:2010.13175v2 [cs.CV] UPDATED)</h2>
<h3>Yihong Sun, Adam Kortylewski, Alan Yuille</h3>
<p>Amodal segmentation in biological vision refers to the perception of the
entire object when only a fraction is visible. This ability of seeing through
occluders and reasoning about occlusion is innate to biological vision but not
adequately modeled in current machine vision approaches. A key challenge is
that ground-truth supervisions of amodal object segmentation are inherently
difficult to obtain. In this paper, we present a neural network architecture
that is capable of amodal perception, when weakly supervised with standard
(modal) bounding box annotations. Our model extends compositional convolutional
neural networks (CompositionalNets), which have been shown to be robust to
partial occlusion by explicitly representing objects as a composition of parts.
In particular, we extend CompositionalNets to perform three new vision tasks
from bounding box supervision only: 1) Learning compositional shape priors of
objects in varying 3D poses from modal bounding box supervision; 2) Predicting
instance segmentation by integrating the compositional shape priors into the
part-voting mechanism in the CompositionalNets; 3) Predicting amodal completion
for both the bounding box and the instance segmentation mask by implementing
compositional feature alignment in CompositionalNets. Our extensive experiments
show that our proposed model can segment amodal masks robustly, with much
improved mask prediction qualities compared to state-of-the-art segmentation
approaches.
</p>
<a href="http://arxiv.org/abs/2010.13175" target="_blank">arXiv:2010.13175</a> [<a href="http://arxiv.org/pdf/2010.13175" target="_blank">pdf</a>]

<h2>Lyapunov-Based Reinforcement Learning State Estimator. (arXiv:2010.13529v2 [cs.LG] UPDATED)</h2>
<h3>Liang Hu, Chengwei Wu, Wei Pan</h3>
<p>In this paper, we consider the state estimation problem for nonlinear
stochastic discrete-time systems. We combine Lyapunov's method in control
theory and deep reinforcement learning to design the state estimator. We
theoretically prove the convergence of the bounded estimate error solely using
the data simulated from the model. An actor-critic reinforcement learning
algorithm is proposed to learn the state estimator approximated by a deep
neural network. The convergence of the algorithm is analysed. The proposed
Lyapunov-based reinforcement learning state estimator is compared with a number
of existing nonlinear filtering methods through Monte Carlo simulations,
showing its advantage in terms of estimate convergence even under some system
uncertainties such as covariance shift in system noise and randomly missing
measurements. To the best of our knowledge, this is the first reinforcement
learning based nonlinear state estimator with bounded estimate error
performance guarantee.
</p>
<a href="http://arxiv.org/abs/2010.13529" target="_blank">arXiv:2010.13529</a> [<a href="http://arxiv.org/pdf/2010.13529" target="_blank">pdf</a>]

<h2>BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image Restoration. (arXiv:2011.01406v2 [cs.CV] UPDATED)</h2>
<h3>Majed El Helou, Sabine S&#xfc;sstrunk</h3>
<p>Image restoration encompasses fundamental image processing tasks that have
been addressed with different algorithms and deep learning methods. Classical
restoration algorithms leverage a variety of priors, either implicitly or
explicitly. Their priors are hand-designed and their corresponding weights are
heuristically assigned. Thus, deep learning methods often produce superior
restoration quality. Deep networks are, however, capable of strong and
hardly-predictable hallucinations. Networks jointly and implicitly learn to be
faithful to the observed data while learning an image prior, and the separation
of original and hallucinated data downstream is then not possible. This limits
their wide-spread adoption in restoration applications. Furthermore, it is
often the hallucinated part that is victim to degradation-model overfitting. We
present an approach with decoupled network-prior hallucination and data
fidelity. We refer to our framework as the Bayesian Integration of a Generative
Prior (BIGPrior). Our BIGPrior method is rooted in a Bayesian restoration
framework, and tightly connected to classical restoration methods. In fact, our
approach can be viewed as a generalization of a large family of classical
restoration algorithms. We leverage a recent network inversion method to
extract image prior information from a generative network. We show on image
colorization, inpainting, and denoising that our framework consistently
improves the prior results through good integration of data fidelity. Our
method, though partly reliant on the quality of the generative network
inversion, is competitive with state-of-the-art supervised and task-specific
restoration methods. It also provides an additional metric that sets forth the
degree of prior reliance per pixel. Indeed, the per pixel contributions of the
decoupled data fidelity and prior terms are readily available in our proposed
framework.
</p>
<a href="http://arxiv.org/abs/2011.01406" target="_blank">arXiv:2011.01406</a> [<a href="http://arxiv.org/pdf/2011.01406" target="_blank">pdf</a>]

<h2>Efficient Subspace Search in Data Streams. (arXiv:2011.06959v2 [cs.LG] UPDATED)</h2>
<h3>Edouard Fouch&#xe9;, Florian Kalinke, Klemens B&#xf6;hm</h3>
<p>In the real world, data streams are ubiquitous -- think of network traffic or
sensor data. Mining patterns, e.g., outliers or clusters, from such data must
take place in real time. This is challenging because (1) streams often have
high dimensionality, and (2) the data characteristics may change over time.
Existing approaches tend to focus on only one aspect, either high
dimensionality or the specifics of the streaming setting. For static data, a
common approach to deal with high dimensionality -- known as subspace search --
extracts low-dimensional, `interesting' projections (subspaces), in which
patterns are easier to find. In this paper, we address both Challenge (1) and
(2) by generalising subspace search to data streams. Our approach, Streaming
Greedy Maximum Random Deviation (SGMRD), monitors interesting subspaces in
high-dimensional data streams. It leverages novel multivariate dependency
estimators and monitoring techniques based on bandit theory. We show that the
benefits of SGMRD are twofold: (i) It monitors subspaces efficiently, and (ii)
this improves the results of downstream data mining tasks, such as outlier
detection. Our experiments, performed against synthetic and real-world data,
demonstrate that SGMRD outperforms its competitors by a large margin.
</p>
<a href="http://arxiv.org/abs/2011.06959" target="_blank">arXiv:2011.06959</a> [<a href="http://arxiv.org/pdf/2011.06959" target="_blank">pdf</a>]

<h2>DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer. (arXiv:2011.07589v3 [cs.CV] UPDATED)</h2>
<h3>Ajay Kumar Tanwani</h3>
<p>Generating large-scale synthetic data in simulation is a feasible alternative
to collecting/labelling real data for training vision-based deep learning
models, albeit the modelling inaccuracies do not generalize to the physical
world. In this paper, we present a domain-invariant representation learning
(DIRL) algorithm to adapt deep models to the physical environment with a small
amount of real data. Existing approaches that only mitigate the covariate shift
by aligning the marginal distributions across the domains and assume the
conditional distributions to be domain-invariant can lead to ambiguous transfer
in real scenarios. We propose to jointly align the marginal (input domains) and
the conditional (output labels) distributions to mitigate the covariate and the
conditional shift across the domains with adversarial learning, and combine it
with a triplet distribution loss to make the conditional distributions disjoint
in the shared feature space. Experiments on digit domains yield
state-of-the-art performance on challenging benchmarks, while sim-to-real
transfer of object recognition for vision-based decluttering with a mobile
robot improves from 26.8 % to 91.0 %, resulting in 86.5 % grasping accuracy of
a wide variety of objects. Code and supplementary details are available at
https://sites.google.com/view/dirl
</p>
<a href="http://arxiv.org/abs/2011.07589" target="_blank">arXiv:2011.07589</a> [<a href="http://arxiv.org/pdf/2011.07589" target="_blank">pdf</a>]

<h2>Language Acquisition Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v2 [cs.AI] UPDATED)</h2>
<h3>Deokgun Park</h3>
<p>Despite recent advances in many application-specific domains, we do not know
how to build a human-level artificial intelligence (HLAI). We conjecture that
learning from others' experience with the language is the essential
characteristic that differentiates human intelligence from the rest. Humans can
update the action-value function only with the verbal description as if they
experience states, actions, and corresponding rewards sequences first hand. In
this paper, we present our ongoing effort to build an environment to facilitate
the research for models of this capability. In this environment, there are no
explicit definitions of tasks or rewards given when accomplishing those tasks.
Rather the models experience the experience of the human infants from fetus to
12 months. The agent should learn to speak the first words as a human child
does. We expect the environment will contribute to the research for HLAI.
</p>
<a href="http://arxiv.org/abs/2011.09410" target="_blank">arXiv:2011.09410</a> [<a href="http://arxiv.org/pdf/2011.09410" target="_blank">pdf</a>]

<h2>Deep-learning coupled with novel classification method to classify the urban environment of the developing world. (arXiv:2011.12847v2 [cs.CV] UPDATED)</h2>
<h3>Qianwei Cheng, AKM Mahbubur Rahman, Anis Sarker, Abu Bakar Siddik Nayem, Ovi Paul, Amin Ahsan Ali, M Ashraful Amin, Ryosuke Shibasaki, Moinul Zaber</h3>
<p>Rapid globalization and the interdependence of humanity that engender
tremendous in-flow of human migration towards the urban spaces. With advent of
high definition satellite images, high resolution data, computational methods
such as deep neural network, capable hardware; urban planning is seeing a
paradigm shift. Legacy data on urban environments are now being complemented
with high-volume, high-frequency data. In this paper we propose a novel
classification method that is readily usable for machine analysis and show
applicability of the methodology on a developing world setting. The
state-of-the-art is mostly dominated by classification of building structures,
building types etc. and largely represents the developed world which are
insufficient for developing countries such as Bangladesh where the surrounding
is crucial for the classification. Moreover, the traditional methods propose
small-scale classifications, which give limited information with poor
scalability and are slow to compute. We categorize the urban area in terms of
informal and formal spaces taking the surroundings into account. 50 km x 50 km
Google Earth image of Dhaka, Bangladesh was visually annotated and categorized
by an expert. The classification is based broadly on two dimensions:
urbanization and the architectural form of urban environment. Consequently, the
urban space is divided into four classes: 1) highly informal; 2) moderately
informal; 3) moderately formal; and 4) highly formal areas. In total 16
sub-classes were identified. For semantic segmentation, Google's DeeplabV3+
model was used which increases the field of view of the filters to incorporate
larger context. Image encompassing 70% of the urban space was used for training
and the remaining 30% was used for testing and validation. The model is able to
segment with 75% accuracy and 60% Mean IoU.
</p>
<a href="http://arxiv.org/abs/2011.12847" target="_blank">arXiv:2011.12847</a> [<a href="http://arxiv.org/pdf/2011.12847" target="_blank">pdf</a>]

<h2>Exploiting Diverse Characteristics and Adversarial Ambivalence for Domain Adaptive Segmentation. (arXiv:2012.05608v2 [cs.CV] UPDATED)</h2>
<h3>Bowen Cai, Huan Fu, Rongfei Jia, Binqiang Zhao, Hua Li, Yinghui Xu</h3>
<p>Adapting semantic segmentation models to new domains is an important but
challenging problem. Recently enlightening progress has been made, but the
performance of existing methods are unsatisfactory on real datasets where the
new target domain comprises of heterogeneous sub-domains (e.g., diverse weather
characteristics). We point out that carefully reasoning about the multiple
modalities in the target domain can improve the robustness of adaptation
models. To this end, we propose a condition-guided adaptation framework that is
empowered by a special attentive progressive adversarial training (APAT)
mechanism and a novel self-training policy. The APAT strategy progressively
performs condition-specific alignment and attentive global feature matching.
The new self-training scheme exploits the adversarial ambivalences of easy and
hard adaptation regions and the correlations among target sub-domains
effectively. We evaluate our method (DCAA) on various adaptation scenarios
where the target images vary in weather conditions. The comparisons against
baselines and the state-of-the-art approaches demonstrate the superiority of
DCAA over the competitors.
</p>
<a href="http://arxiv.org/abs/2012.05608" target="_blank">arXiv:2012.05608</a> [<a href="http://arxiv.org/pdf/2012.05608" target="_blank">pdf</a>]

<h2>Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes. (arXiv:2012.08507v2 [cs.LG] UPDATED)</h2>
<h3>Dongruo Zhou, Quanquan Gu, Csaba Szepesvari</h3>
<p>We study reinforcement learning (RL) with linear function approximation where
the underlying transition probability kernel of the Markov decision process
(MDP) is a linear mixture model (Jia et al., 2020; Ayoub et al., 2020; Zhou et
al., 2020) and the learning agent has access to either an integration or a
sampling oracle of the individual basis kernels. We propose a new
Bernstein-type concentration inequality for self-normalized martingales for
linear bandit problems with bounded noise. Based on the new inequality, we
propose a new, computationally efficient algorithm with linear function
approximation named $\text{UCRL-VTR}^{+}$ for the aforementioned linear mixture
MDPs in the episodic undiscounted setting. We show that $\text{UCRL-VTR}^{+}$
attains an $\tilde O(dH\sqrt{T})$ regret where $d$ is the dimension of feature
mapping, $H$ is the length of the episode and $T$ is the number of interactions
with the MDP. We also prove a matching lower bound $\Omega(dH\sqrt{T})$ for
this setting, which shows that $\text{UCRL-VTR}^{+}$ is minimax optimal up to
logarithmic factors. In addition, we propose the $\text{UCLK}^{+}$ algorithm
for the same family of MDPs under discounting and show that it attains an
$\tilde O(d\sqrt{T}/(1-\gamma)^{1.5})$ regret, where $\gamma\in [0,1)$ is the
discount factor. Our upper bound matches the lower bound
$\Omega(d\sqrt{T}/(1-\gamma)^{1.5})$ proved by Zhou et al. (2020) up to
logarithmic factors, suggesting that $\text{UCLK}^{+}$ is nearly minimax
optimal. To the best of our knowledge, these are the first computationally
efficient, nearly minimax optimal algorithms for RL with linear function
approximation.
</p>
<a href="http://arxiv.org/abs/2012.08507" target="_blank">arXiv:2012.08507</a> [<a href="http://arxiv.org/pdf/2012.08507" target="_blank">pdf</a>]

<h2>Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v2 [cs.AI] UPDATED)</h2>
<h3>Reut Apel, Ido Erev, Roi Reichart, Moshe Tennenholtz</h3>
<p>Sender-receiver interactions, and specifically persuasion games, are widely
researched in economic modeling and artificial intelligence, and serve as a
solid foundation for powerful applications. However, in the classic persuasion
games setting, the messages sent from the expert to the decision-maker are
abstract or well-structured application-specific signals rather than natural
(human) language messages, although natural language is a very common
communication signal in real-world persuasion setups. This paper addresses the
use of natural language in persuasion games, exploring its impact on the
decisions made by the players and aiming to construct effective models for the
prediction of these decisions. For this purpose, we conduct an online repeated
interaction experiment. At each trial of the interaction, an informed expert
aims to sell an uninformed decision-maker a vacation in a hotel, by sending her
a review that describes the hotel. While the expert is exposed to several
scored reviews, the decision-maker observes only the single review sent by the
expert, and her payoff in case she chooses to take the hotel is a random draw
from the review score distribution available to the expert only. The expert's
payoff, in turn, depends on the number of times the decision-maker chooses the
hotel. We consider a number of modeling approaches for this setup, differing
from each other in the model type (deep neural network (DNN) vs. linear
classifier), the type of features used by the model (textual, behavioral or
both) and the source of the textual features (DNN-based vs. hand-crafted). Our
results demonstrate that given a prefix of the interaction sequence, our models
can predict the future decisions of the decision-maker, particularly when a
sequential modeling approach and hand-crafted textual features are applied.
</p>
<a href="http://arxiv.org/abs/2012.09966" target="_blank">arXiv:2012.09966</a> [<a href="http://arxiv.org/pdf/2012.09966" target="_blank">pdf</a>]

<h2>Variational Determinant Estimation with Spherical Normalizing Flows. (arXiv:2012.13311v2 [cs.LG] UPDATED)</h2>
<h3>Simon Passenheim, Emiel Hoogeboom</h3>
<p>This paper introduces the Variational Determinant Estimator (VDE), a
variational extension of the recently proposed determinant estimator discovered
by arXiv:2005.06553v2. Our estimator significantly reduces the variance even
for low sample sizes by combining (importance-weighted) variational inference
and a family of normalizing flows which allow density estimation on
hyperspheres. In the ideal case of a tight variational bound, the VDE becomes a
zero variance estimator, and a single sample is sufficient for an exact (log)
determinant estimate.
</p>
<a href="http://arxiv.org/abs/2012.13311" target="_blank">arXiv:2012.13311</a> [<a href="http://arxiv.org/pdf/2012.13311" target="_blank">pdf</a>]

<h2>Patch-wise++ Perturbation for Adversarial Targeted Attacks. (arXiv:2012.15503v2 [cs.CV] UPDATED)</h2>
<h3>Lianli Gao, Qilong Zhang, Jingkuan Song, Heng Tao Shen</h3>
<p>Although great progress has been made on adversarial attacks for deep neural
networks (DNNs), their transferability is still unsatisfactory, especially for
targeted attacks. There are two problems behind that have been long overlooked:
1) the conventional setting of $T$ iterations with the step size of
$\epsilon/T$ to comply with the $\epsilon$-constraint. In this case, most of
the pixels are allowed to add very small noise, much less than $\epsilon$; and
2) usually manipulating pixel-wise noise. However, features of a pixel
extracted by DNNs are influenced by its surrounding regions, and different DNNs
generally focus on different discriminative regions in recognition. To tackle
these issues, we propose a patch-wise iterative method (PIM) aimed at crafting
adversarial examples with high transferability. Specifically, we introduce an
amplification factor to the step size in each iteration, and one pixel's
overall gradient overflowing the $\epsilon$-constraint is properly assigned to
its surrounding regions by a project kernel. But targeted attacks aim to push
the adversarial examples into the territory of a specific class, and the
amplification factor may lead to underfitting. Thus, we introduce the
temperature and propose a patch-wise++ iterative method (PIM++) to further
improve transferability without significantly sacrificing the performance of
the white-box attack. Our method can be generally integrated to any
gradient-based attack method. Compared with the current state-of-the-art attack
methods, we significantly improve the success rate by 35.9\% for defense models
and 32.7\% for normally trained models on average.
</p>
<a href="http://arxiv.org/abs/2012.15503" target="_blank">arXiv:2012.15503</a> [<a href="http://arxiv.org/pdf/2012.15503" target="_blank">pdf</a>]

<h2>Stereo Correspondence and Reconstruction of Endoscopic Data Challenge. (arXiv:2101.01133v2 [cs.CV] UPDATED)</h2>
<h3>Max Allan, Jonathan Mcleod, Cong Cong Wang, Jean Claude Rosenthal, Ke Xue Fu, Trevor Zeffiro, Wenyao Xia, Zhu Zhanshi, Huoling Luo, Fucang Jia, Xiran Zhang, Xiaohong Li, Lalith Sharan, Tom Kurmann, Sebastian Schmid, Dimitris Psychogyios, Mahdi Azizian, Danail Stoyanov, Lena Maier-Hein, Stefanie Speidel</h3>
<p>The stereo correspondence and reconstruction of endoscopic data sub-challenge
was organized during the Endovis challenge at MICCAI 2019 in Shenzhen, China.
The task was to perform dense depth estimation using 7 training datasets and 2
test sets of structured light data captured using porcine cadavers. These were
provided by a team at Intuitive Surgical. 10 teams participated in the
challenge day. This paper contains 3 additional methods which were submitted
after the challenge finished as well as a supplemental section from these teams
on issues they found with the dataset.
</p>
<a href="http://arxiv.org/abs/2101.01133" target="_blank">arXiv:2101.01133</a> [<a href="http://arxiv.org/pdf/2101.01133" target="_blank">pdf</a>]

<h2>Multi-Model Least Squares-Based Recomputation Framework for Large Data Analysis. (arXiv:2101.01271v2 [cs.LG] UPDATED)</h2>
<h3>Wandong Zhang (1 and 2), QM Jonathan Wu (1), Yimin Yang (2 and 3), WG Will Zhao (2 and 4), Hui Zhang (5) ((1) University of Windsor, (2) Lakehead University, (3) Vector Institute for Artificial Intelligence, (4) CEGEP de Ste Foy, (5) Hunan University)</h3>
<p>Most multilayer least squares (LS)-based neural networks are structured with
two separate stages: unsupervised feature encoding and supervised pattern
classification. Once the unsupervised learning is finished, the latent encoding
would be fixed without supervised fine-tuning. However, in complex tasks such
as handling the ImageNet dataset, there are often many more clues that can be
directly encoded, while the unsupervised learning, by definition cannot know
exactly what is useful for a certain task. This serves as the motivation to
retrain the latent space representations to learn some clues that unsupervised
learning has not yet learned. In particular, the error matrix from the output
layer is pulled back to each hidden layer, and the parameters of the hidden
layer are recalculated with Moore-Penrose (MP) inverse for more generalized
representations. In this paper, a recomputation-based multilayer network using
MP inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the
performance of RML-MP is then proposed. The experimental results with varying
training samples (from 3 K to 1.8 M) show that the proposed models provide
better generalization performance than most representation learning algorithms.
</p>
<a href="http://arxiv.org/abs/2101.01271" target="_blank">arXiv:2101.01271</a> [<a href="http://arxiv.org/pdf/2101.01271" target="_blank">pdf</a>]

<h2>Scale-Aware Network with Regional and Semantic Attentions for Crowd Counting under Cluttered Background. (arXiv:2101.01479v2 [cs.CV] UPDATED)</h2>
<h3>Qiaosi Yi, Yunxing Liu, Aiwen Jiang, Juncheng Li, Kangfu Mei, Mingwen Wang</h3>
<p>Crowd counting is an important task that shown great application value in
public safety-related fields, which has attracted increasing attention in
recent years. In the current research, the accuracy of counting numbers and
crowd density estimation are the main concerns. Although the emergence of deep
learning has greatly promoted the development of this field, crowd counting
under cluttered background is still a serious challenge. In order to solve this
problem, we propose a ScaleAware Crowd Counting Network (SACCN) with regional
and semantic attentions. The proposed SACCN distinguishes crowd and background
by applying regional and semantic self-attention mechanisms on the shallow
layers and deep layers, respectively. Moreover, the asymmetric multi-scale
module (AMM) is proposed to deal with the problem of scale diversity, and
regional attention based dense connections and skip connections are designed to
alleviate the variations on crowd scales. Extensive experimental results on
multiple public benchmarks demonstrate that our proposed SACCN achieves
satisfied superior performances and outperform most state-of-the-art methods.
All codes and pretrained models will be released soon.
</p>
<a href="http://arxiv.org/abs/2101.01479" target="_blank">arXiv:2101.01479</a> [<a href="http://arxiv.org/pdf/2101.01479" target="_blank">pdf</a>]

<h2>ISETAuto: Detecting vehicles with depth and radiance information. (arXiv:2101.01843v2 [cs.CV] UPDATED)</h2>
<h3>Zhenyi Liu, Joyce Farrell, Brian Wandell</h3>
<p>Autonomous driving applications use two types of sensor systems to identify
vehicles - depth sensing LiDAR and radiance sensing cameras. We compare the
performance (average precision) of a ResNet for vehicle detection in complex,
daytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance
image (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of
the depth map and radiance image are equal to typical camera resolutions, a
ResNet detects vehicles at higher average precision from depth than radiance.
(2) As the spatial sampling of the depth map declines to the range of current
LiDAR devices, the ResNet average precision is higher for radiance than depth.
(3) For a hybrid system that combines a depth map and radiance image, the
average precision is higher than using depth or radiance alone. We established
these observations in simulation and then confirmed them using realworld data.
The advantage of combining depth and radiance can be explained by noting that
the two type of information have complementary weaknesses. The radiance data
are limited by dynamic range and motion blur. The LiDAR data have relatively
low spatial resolution. The ResNet combines the two data sources effectively to
improve overall vehicle detection.
</p>
<a href="http://arxiv.org/abs/2101.01843" target="_blank">arXiv:2101.01843</a> [<a href="http://arxiv.org/pdf/2101.01843" target="_blank">pdf</a>]

<h2>TextBox: A Unified, Modularized, and Extensible Framework for Text Generation. (arXiv:2101.02046v2 [cs.AI] UPDATED)</h2>
<h3>Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaoxuan Hu, Puzhao Xie, Wayne Xin Zhao, Ji-Rong Wen</h3>
<p>We release an open library, called TextBox, which provides a unified,
modularized, and extensible text generation framework. TextBox aims to support
a broad set of text generation tasks and models. In TextBox, we implements
several text generation models on benchmark datasets, covering the categories
of VAE, GAN, pre-trained language models, etc. Meanwhile, our library maintains
sufficient modularity and extensibility by properly decomposing the model
architecture, inference, learning process into highly reusable modules, which
allows easily incorporating new models into our framework. It is specially
suitable for researchers and practitioners to efficiently reproduce baseline
models and develop new models. TextBox is implemented based on PyTorch, and
released under Apache License 2.0 at https://github.com/RUCAIBox/TextBox.
</p>
<a href="http://arxiv.org/abs/2101.02046" target="_blank">arXiv:2101.02046</a> [<a href="http://arxiv.org/pdf/2101.02046" target="_blank">pdf</a>]

<h2>Geometric Entropic Exploration. (arXiv:2101.02055v2 [cs.LG] UPDATED)</h2>
<h3>Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Alaa Saade, Shantanu Thakoor, Bilal Piot, Bernardo Avila Pires, Michal Valko, Thomas Mesnard, Tor Lattimore, R&#xe9;mi Munos</h3>
<p>Exploration is essential for solving complex Reinforcement Learning (RL)
tasks. Maximum State-Visitation Entropy (MSVE) formulates the exploration
problem as a well-defined policy optimization problem whose solution aims at
visiting all states as uniformly as possible. This is in contrast to standard
uncertainty-based approaches where exploration is transient and eventually
vanishes. However, existing approaches to MSVE are theoretically justified only
for discrete state-spaces as they are oblivious to the geometry of continuous
domains. We address this challenge by introducing Geometric Entropy
Maximisation (GEM), a new algorithm that maximises the geometry-aware Shannon
entropy of state-visits in both discrete and continuous domains. Our key
theoretical contribution is casting geometry-aware MSVE exploration as a
tractable problem of optimising a simple and novel noise-contrastive objective
function. In our experiments, we show the efficiency of GEM in solving several
RL problems with sparse rewards, compared against other deep RL exploration
approaches.
</p>
<a href="http://arxiv.org/abs/2101.02055" target="_blank">arXiv:2101.02055</a> [<a href="http://arxiv.org/pdf/2101.02055" target="_blank">pdf</a>]

<h2>Predicting Illness for a Sustainable Dairy Agriculture: Predicting and Explaining the Onset of Mastitis in Dairy Cows. (arXiv:2101.02188v2 [cs.LG] UPDATED)</h2>
<h3>Cathal Ryan, Christophe Gu&#xe9;ret, Donagh Berry, Medb Corcoran, Mark T. Keane, Brian Mac Namee</h3>
<p>Mastitis is a billion dollar health problem for the modern dairy industry,
with implications for antibiotic resistance. The use of AI techniques to
identify the early onset of this disease, thus has significant implications for
the sustainability of this agricultural sector. Current approaches to treating
mastitis involve antibiotics and this practice is coming under ever increasing
scrutiny. Using machine learning models to identify cows at risk of developing
mastitis and applying targeted treatment regimes to only those animals promotes
a more sustainable approach. Incorrect predictions from such models, however,
can lead to monetary losses, unnecessary use of antibiotics, and even the
premature death of animals, so it is important to generate compelling
explanations for predictions to build trust with users and to better support
their decision making. In this paper we demonstrate a system developed to
predict mastitis infections in cows and provide explanations of these
predictions using counterfactuals. We demonstrate the system and describe the
engagement with farmers undertaken to build it.
</p>
<a href="http://arxiv.org/abs/2101.02188" target="_blank">arXiv:2101.02188</a> [<a href="http://arxiv.org/pdf/2101.02188" target="_blank">pdf</a>]

<h2>StarNet: Gradient-free Training of Deep Generative Models using Determined System of Linear Equations. (arXiv:2101.00574v1 [cs.LG] CROSS LISTED)</h2>
<h3>Amir Zadeh, Santiago Benoit, Louis-Philippe Morency</h3>
<p>In this paper we present an approach for training deep generative models
solely based on solving determined systems of linear equations. A network that
uses this approach, called a StarNet, has the following desirable properties:
1) training requires no gradient as solution to the system of linear equations
is not stochastic, 2) is highly scalable when solving the system of linear
equations w.r.t the latent codes, and similarly for the parameters of the
model, and 3) it gives desirable least-square bounds for the estimation of
latent codes and network parameters within each layer.
</p>
<a href="http://arxiv.org/abs/2101.00574" target="_blank">arXiv:2101.00574</a> [<a href="http://arxiv.org/pdf/2101.00574" target="_blank">pdf</a>]

<h2>Domain Generalization by Marginal Transfer Learning. (arXiv:1711.07910v3 [stat.ML] UPDATED)</h2>
<h3>Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, Clayton Scott</h3>
<p>In the problem of domain generalization (DG), there are labeled training data
sets from several related prediction problems, and the goal is to make accurate
predictions on future unlabeled data sets that are not known to the learner.
This problem arises in several applications where data distributions fluctuate
because of environmental, technical, or other sources of variation. We
introduce a formal framework for DG, and argue that it can be viewed as a kind
of supervised learning problem by augmenting the original feature space with
the marginal distribution of feature vectors. While our framework has several
connections to conventional analysis of supervised learning algorithms, several
unique aspects of DG require new methods of analysis.

This work lays the learning theoretic foundations of domain generalization,
building on our earlier conference paper where the problem of DG was introduced
(Blanchard et al., 2011). We present two formal models of data generation,
corresponding notions of risk, and distribution-free generalization error
analysis. By focusing our attention on kernel methods, we also provide more
quantitative results and a universally consistent algorithm. An efficient
implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.
</p>
<a href="http://arxiv.org/abs/1711.07910" target="_blank">arXiv:1711.07910</a> [<a href="http://arxiv.org/pdf/1711.07910" target="_blank">pdf</a>]

