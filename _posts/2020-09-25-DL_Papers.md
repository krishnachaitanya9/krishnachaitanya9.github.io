---
title: Latest Deep Learning Papers
date: 2020-11-08 01:32:40 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>The dynamic effect of mechanical losses of actuators on the equations of motion of legged robots. (arXiv:2011.02506v1 [cs.RO])</h2>
<h3>Young-woo Sim, Joao Ramos</h3>
<p>Industrial manipulators do not collapse under their own weight when powered
off due to the friction in their joints. Although these mechanism are effective
for stiff position control of pick-and-place, they are inappropriate for legged
robots which must rapidly regulate compliant interactions with the environment.
However, no metric exists to quantify the robot's perform degradation due to
mechanical losses in the actuators. This letter provides a novel formulation
which describes how the efficiency of individual actuators propagate to the
equations of motion of the whole robot. We quantitatively demonstrate the
intuitive fact that the apparent inertia of the robots increase in the presence
of joint friction. We also reproduce the empirical result that robots which
employ high gearing and low efficiency actuators can statically sustain more
substantial external loads. We expect that the framework presented here will
provide the foundations to design the next generation of legged robots which
can effectively interact with the world.
</p>
<a href="http://arxiv.org/abs/2011.02506" target="_blank">arXiv:2011.02506</a> [<a href="http://arxiv.org/pdf/2011.02506" target="_blank">pdf</a>]

<h2>A Comparison Between Joint Space and Task Space Mappings for Dynamic Teleoperation of an Anthropomorphic Robotic Arm in Reaction Tests. (arXiv:2011.02508v1 [cs.RO])</h2>
<h3>Sunyu Wang, Kevin Murphy, Dillan Kenney, Joao Ramos</h3>
<p>Teleoperation (i.e., controlling a robot with human motion) proves promising
in enabling a humanoid robot to move as dynamically as a human. But how to map
human motion to a humanoid robot matters because a human and a humanoid robot
rarely have identical topologies and dimensions. This work presents an
experimental study that utilizes reaction tests to compare the proposed joint
space mapping and the proposed task space mapping for dynamic teleoperation of
an anthropomorphic robotic arm that possesses human-level dynamic motion
capabilities. The experimental results suggest that the robot achieved similar
and, in some cases, human-level dynamic performances with both mappings for the
six participating human subjects. All subjects became proficient at
teleoperating the robot with both mappings after practice, despite that the
subjects and the robot differed in size and link length ratio and that the
teleoperation required the subjects to move unintuitively. Yet, most subjects
developed their teleoperation proficiencies more quickly with the task space
mapping than with the joint space mapping after similar amounts of practice.
This study also indicates the potential values of a three-dimensional task
space mapping, a teleoperation training simulator, and force feedback to the
human pilot for intuitive and dynamic teleoperation of a humanoid robot's arms.
</p>
<a href="http://arxiv.org/abs/2011.02508" target="_blank">arXiv:2011.02508</a> [<a href="http://arxiv.org/pdf/2011.02508" target="_blank">pdf</a>]

<h2>Monitoring the Impact of Wildfires on Tree Species with Deep Learning. (arXiv:2011.02514v1 [cs.CV])</h2>
<h3>Wang Zhou, Levente Klein</h3>
<p>One of the impacts of climate change is the difficulty of tree regrowth after
wildfires over areas that traditionally were covered by certain tree species.
Here a deep learning model is customized to classify land covers from four-band
aerial imagery before and after wildfires to study the prolonged consequences
of wildfires on tree species. The tree species labels are generated from
manually delineated maps for five land cover classes: Conifer, Hardwood, Shrub,
ReforestedTree and Barren land. With an accuracy of $92\%$ on the test split,
the model is applied to three wildfires on data from 2009 to 2018. The model
accurately delineates areas damaged by wildfires, changes in tree species and
rebound of burned areas. The result shows clear evidence of wildfires impacting
the local ecosystem and the outlined approach can help monitor reforested
areas, observe changes in forest composition and track wildfire impact on tree
species.
</p>
<a href="http://arxiv.org/abs/2011.02514" target="_blank">arXiv:2011.02514</a> [<a href="http://arxiv.org/pdf/2011.02514" target="_blank">pdf</a>]

<h2>Physics-Informed Neural Network Super Resolution for Advection-Diffusion Models. (arXiv:2011.02519v1 [cs.CV])</h2>
<h3>Chulin Wang, Eloisa Bentivegna, Wang Zhou, Levente Klein, Bruce Elmegreen</h3>
<p>Physics-informed neural networks (NN) are an emerging technique to improve
spatial resolution and enforce physical consistency of data from physics models
or satellite observations. A super-resolution (SR) technique is explored to
reconstruct high-resolution images ($4\times$) from lower resolution images in
an advection-diffusion model of atmospheric pollution plumes. SR performance is
generally increased when the advection-diffusion equation constrains the NN in
addition to conventional pixel-based constraints. The ability of SR techniques
to also reconstruct missing data is investigated by randomly removing image
pixels from the simulations and allowing the system to learn the content of
missing data. Improvements in S/N of $11\%$ are demonstrated when physics
equations are included in SR with $40\%$ pixel loss. Physics-informed NNs
accurately reconstruct corrupted images and generate better results compared to
the standard SR approaches.
</p>
<a href="http://arxiv.org/abs/2011.02519" target="_blank">arXiv:2011.02519</a> [<a href="http://arxiv.org/pdf/2011.02519" target="_blank">pdf</a>]

<h2>Filtering for Aggregate Hidden Markov Models with Continuous Observations. (arXiv:2011.02521v1 [stat.ML])</h2>
<h3>Qinsheng Zhang, Rahul Singh, Yongxin Chen</h3>
<p>We consider a class of filtering problems for large populations where each
individual is modeled by the same hidden Markov model (HMM). In this paper, we
focus on aggregate inference problems in HMMs with discrete state space and
continuous observation space. The continuous observations are aggregated in a
way such that the individuals are indistinguishable from measurements. We
propose an aggregate inference algorithm called continuous observation
collective forward-backward algorithm. It extends the recently proposed
collective forward-backward algorithm for aggregate inference in HMMs with
discrete observations to the case of continuous observations. The efficacy of
this algorithm is illustrated through several numerical experiments.
</p>
<a href="http://arxiv.org/abs/2011.02521" target="_blank">arXiv:2011.02521</a> [<a href="http://arxiv.org/pdf/2011.02521" target="_blank">pdf</a>]

<h2>Gradient-Based Empirical Risk Minimization using Local Polynomial Regression. (arXiv:2011.02522v1 [cs.LG])</h2>
<h3>Ali Jadbabaie, Anuran Makur, Devavrat Shah</h3>
<p>In this paper, we consider the problem of empirical risk minimization (ERM)
of smooth, strongly convex loss functions using iterative gradient-based
methods. A major goal of this literature has been to compare different
algorithms, such as gradient descent (GD) or stochastic gradient descent (SGD),
by analyzing their rates of convergence to $\epsilon$-approximate solutions.
For example, the oracle complexity of GD is $O(n\log(\epsilon^{-1}))$, where
$n$ is the number of training samples. When $n$ is large, this can be expensive
in practice, and SGD is preferred due to its oracle complexity of
$O(\epsilon^{-1})$. Such standard analyses only utilize the smoothness of the
loss function in the parameter being optimized. In contrast, we demonstrate
that when the loss function is smooth in the data, we can learn the oracle at
every iteration and beat the oracle complexities of both GD and SGD in
important regimes. Specifically, at every iteration, our proposed algorithm
performs local polynomial regression to learn the gradient of the loss
function, and then estimates the true gradient of the ERM objective function.
We establish that the oracle complexity of our algorithm scales like
$\tilde{O}((p \epsilon^{-1})^{d/(2\eta)})$ (neglecting sub-dominant factors),
where $d$ and $p$ are the data and parameter space dimensions, respectively,
and the gradient of the loss function belongs to a $\eta$-H\"{o}lder class with
respect to the data. Our proof extends the analysis of local polynomial
regression in non-parametric statistics to provide interpolation guarantees in
multivariate settings, and also exploits tools from the inexact GD literature.
Unlike GD and SGD, the complexity of our method depends on $d$ and $p$.
However, when $d$ is small and the loss function exhibits modest smoothness in
the data, our algorithm beats GD and SGD in oracle complexity for a very broad
range of $p$ and $\epsilon$.
</p>
<a href="http://arxiv.org/abs/2011.02522" target="_blank">arXiv:2011.02522</a> [<a href="http://arxiv.org/pdf/2011.02522" target="_blank">pdf</a>]

<h2>Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v1 [cs.CV])</h2>
<h3>Mike Roberts, Nathan Paczan</h3>
<p>For many fundamental scene understanding tasks, it is difficult or impossible
to obtain per-pixel ground truth labels from real images. We address this
challenge by introducing Hypersim, a photorealistic synthetic dataset for
holistic indoor scene understanding. To create our dataset, we leverage a large
repository of synthetic scenes created by professional artists, and we generate
77,400 images of 461 indoor scenes with detailed per-pixel labels and
corresponding ground truth geometry. Our dataset: (1) relies exclusively on
publicly available 3D assets; (2) includes complete scene geometry, material
information, and lighting information for every scene; (3) includes dense
per-pixel semantic instance segmentations for every image; and (4) factors
every image into diffuse reflectance, diffuse illumination, and a non-diffuse
residual term that captures view-dependent lighting effects. Together, these
features make our dataset well-suited for geometric learning problems that
require direct 3D supervision, multi-task learning problems that require
reasoning jointly over multiple input and output modalities, and inverse
rendering problems. We analyze our dataset at the level of scenes, objects, and
pixels, and we analyze costs in terms of money, annotation effort, and
computation time. Remarkably, we find that it is possible to generate our
entire dataset from scratch, for roughly half the cost of training a
state-of-the-art natural language processing model. All the code we used to
generate our dataset will be made available online.
</p>
<a href="http://arxiv.org/abs/2011.02523" target="_blank">arXiv:2011.02523</a> [<a href="http://arxiv.org/pdf/2011.02523" target="_blank">pdf</a>]

<h2>Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection. (arXiv:2011.02526v1 [cs.LG])</h2>
<h3>Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami</h3>
<p>This paper proposes a new defense against neural network backdooring attacks
that are maliciously trained to mispredict in the presence of attacker-chosen
triggers. Our defense is based on the intuition that the feature extraction
layers of a backdoored network embed new features to detect the presence of a
trigger and the subsequent classification layers learn to mispredict when
triggers are detected. Therefore, to detect backdoors, the proposed defense
uses two synergistic anomaly detectors trained on clean validation data: the
first is a novelty detector that checks for anomalous features, while the
second detects anomalous mappings from features to outputs by comparing with a
separate classifier trained on validation data. The approach is evaluated on a
wide range of backdoored networks (with multiple variations of triggers) that
successfully evade state-of-the-art defenses. Additionally, we evaluate the
robustness of our approach on imperceptible perturbations, scalability on
large-scale datasets, and effectiveness under domain shift. This paper also
shows that the defense can be further improved using data augmentation.
</p>
<a href="http://arxiv.org/abs/2011.02526" target="_blank">arXiv:2011.02526</a> [<a href="http://arxiv.org/pdf/2011.02526" target="_blank">pdf</a>]

<h2>Direction Matters: On the Implicit Regularization Effect of Stochastic Gradient Descent with Moderate Learning Rate. (arXiv:2011.02538v1 [cs.LG])</h2>
<h3>Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu</h3>
<p>Understanding the algorithmic regularization effect of stochastic gradient
descent (SGD) is one of the key challenges in modern machine learning and deep
learning theory. Most of the existing works, however, focus on very small or
even infinitesimal learning rate regime, and fail to cover practical scenarios
where the learning rate is moderate and annealing. In this paper, we make an
initial attempt to characterize the particular regularization effect of SGD in
the moderate learning rate regime by studying its behavior for optimizing an
overparameterized linear regression problem. In this case, SGD and GD are known
to converge to the unique minimum-norm solution; however, with the moderate and
annealing learning rate, we show that they exhibit different directional bias:
SGD converges along the large eigenvalue directions of the data matrix, while
GD goes after the small eigenvalue directions. Furthermore, we show that such
directional bias does matter when early stopping is adopted, where the SGD
output is nearly optimal but the GD output is suboptimal. Finally, our theory
explains several folk arts in practice used for SGD hyperparameter tuning, such
as (1) linearly scaling the initial learning rate with batch size; and (2)
overrunning SGD with high learning rate even when the loss stops decreasing.
</p>
<a href="http://arxiv.org/abs/2011.02538" target="_blank">arXiv:2011.02538</a> [<a href="http://arxiv.org/pdf/2011.02538" target="_blank">pdf</a>]

<h2>Mutual Modality Learning for Video Action Classification. (arXiv:2011.02543v1 [cs.CV])</h2>
<h3>Stepan Komkov, Maksim Dzabraev, Aleksandr Petiushko</h3>
<p>The construction of models for video action classification progresses
rapidly. However, the performance of those models can still be easily improved
by ensembling with the same models trained on different modalities (e.g.
Optical flow). Unfortunately, it is computationally expensive to use several
modalities during inference. Recent works examine the ways to integrate
advantages of multi-modality into a single RGB-model. Yet, there is still a
room for improvement. In this paper, we explore the various methods to embed
the ensemble power into a single model. We show that proper initialization, as
well as mutual modality learning, enhances single-modality models. As a result,
we achieve state-of-the-art results in the Something-Something-v2 benchmark.
</p>
<a href="http://arxiv.org/abs/2011.02543" target="_blank">arXiv:2011.02543</a> [<a href="http://arxiv.org/pdf/2011.02543" target="_blank">pdf</a>]

<h2>Re-Assessing the "Classify and Count" Quantification Method. (arXiv:2011.02552v1 [cs.LG])</h2>
<h3>Alejandro Moreo, Fabrizio Sebastiani</h3>
<p>Learning to quantify (a.k.a.\ quantification) is a task concerned with
training unbiased estimators of class prevalence via supervised learning. This
task originated with the observation that "Classify and Count" (CC), the
trivial method of obtaining class prevalence estimates, is often a biased
estimator, and thus delivers suboptimal quantification accuracy; following this
observation, several methods for learning to quantify have been proposed that
have been shown to outperform CC. In this work we contend that previous works
have failed to use properly optimised versions of CC. We thus reassess the real
merits of CC (and its variants), and argue that, while still inferior to some
cutting-edge methods, they deliver near-state-of-the-art accuracy once (a)
hyperparameter optimisation is performed, and (b) this optimisation is
performed by using a true quantification loss instead of a standard
classification-based loss. Experiments on three publicly available binary
sentiment classification datasets support these conclusions.
</p>
<a href="http://arxiv.org/abs/2011.02552" target="_blank">arXiv:2011.02552</a> [<a href="http://arxiv.org/pdf/2011.02552" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Voxel based 3D Object Detection and Tracking with von-Mises Loss. (arXiv:2011.02553v1 [cs.CV])</h2>
<h3>Yuanxin Zhong, Minghan Zhu, Huei Peng</h3>
<p>Object detection and tracking is a key task in autonomy. Specifically, 3D
object detection and tracking have been an emerging hot topic recently.
Although various methods have been proposed for object detection, uncertainty
in the 3D detection and tracking tasks has been less explored. Uncertainty
helps us tackle the error in the perception system and improve robustness. In
this paper, we propose a method for improving target tracking performance by
adding uncertainty regression to the SECOND detector, which is one of the most
representative algorithms of 3D object detection. Our method estimates
positional and dimensional uncertainties with Gaussian Negative Log-Likelihood
(NLL) Loss for estimation and introduces von-Mises NLL Loss for angular
uncertainty estimation. We fed the uncertainty output into a classical object
tracking framework and proved that our method increased the tracking
performance compared against the vanilla tracker with constant covariance
assumption.
</p>
<a href="http://arxiv.org/abs/2011.02553" target="_blank">arXiv:2011.02553</a> [<a href="http://arxiv.org/pdf/2011.02553" target="_blank">pdf</a>]

<h2>Adaptive Stress Testing of Trajectory Predictions in Flight Management Systems. (arXiv:2011.02559v1 [cs.LG])</h2>
<h3>Robert J. Moss, Ritchie Lee, Nicholas Visser, Joachim Hochwarth, James G. Lopez, Mykel J. Kochenderfer</h3>
<p>To find failure events and their likelihoods in flight-critical systems, we
investigate the use of an advanced black-box stress testing approach called
adaptive stress testing. We analyze a trajectory predictor from a developmental
commercial flight management system which takes as input a collection of
lateral waypoints and en-route environmental conditions. Our aim is to search
for failure events relating to inconsistencies in the predicted lateral
trajectories. The intention of this work is to find likely failures and report
them back to the developers so they can address and potentially resolve
shortcomings of the system before deployment. To improve search performance,
this work extends the adaptive stress testing formulation to be applied more
generally to sequential decision-making problems with episodic reward by
collecting the state transitions during the search and evaluating at the end of
the simulated rollout. We use a modified Monte Carlo tree search algorithm with
progressive widening as our adversarial reinforcement learner. The performance
is compared to direct Monte Carlo simulations and to the cross-entropy method
as an alternative importance sampling baseline. The goal is to find potential
problems otherwise not found by traditional requirements-based testing. Results
indicate that our adaptive stress testing approach finds more failures and
finds failures with higher likelihood relative to the baseline approaches.
</p>
<a href="http://arxiv.org/abs/2011.02559" target="_blank">arXiv:2011.02559</a> [<a href="http://arxiv.org/pdf/2011.02559" target="_blank">pdf</a>]

<h2>Diversity-Enriched Option-Critic. (arXiv:2011.02565v1 [cs.LG])</h2>
<h3>Anand Kamat, Doina Precup</h3>
<p>Temporal abstraction allows reinforcement learning agents to represent
knowledge and develop strategies over different temporal scales. The
option-critic framework has been demonstrated to learn temporally extended
actions, represented as options, end-to-end in a model-free setting. However,
feasibility of option-critic remains limited due to two major challenges,
multiple options adopting very similar behavior, or a shrinking set of task
relevant options. These occurrences not only void the need for temporal
abstraction, they also affect performance. In this paper, we tackle these
problems by learning a diverse set of options. We introduce an
information-theoretic intrinsic reward, which augments the task reward, as well
as a novel termination objective, in order to encourage behavioral diversity in
the option set. We show empirically that our proposed method is capable of
learning options end-to-end on several discrete and continuous control tasks,
outperforms option-critic by a wide margin. Furthermore, we show that our
approach sustainably generates robust, reusable, reliable and interpretable
options, in contrast to option-critic.
</p>
<a href="http://arxiv.org/abs/2011.02565" target="_blank">arXiv:2011.02565</a> [<a href="http://arxiv.org/pdf/2011.02565" target="_blank">pdf</a>]

<h2>DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation of Complex 3D Surfaces. (arXiv:2011.02570v1 [cs.CV])</h2>
<h3>Rahul Venkatesh, Sarthak Sharma, Aurobrata Ghosh, Laszlo Jeni, Maneesh Singh</h3>
<p>High fidelity representation of shapes with arbitrary topology is an
important problem for a variety of vision and graphics applications. Owing to
their limited resolution, classical discrete shape representations using point
clouds, voxels and meshes produce low quality results when used in these
applications. Several implicit 3D shape representation approaches using deep
neural networks have been proposed leading to significant improvements in both
quality of representations as well as the impact on downstream applications.
However, these methods can only be used to represent topologically closed
shapes which greatly limits the class of shapes that they can represent. As a
consequence, they also often require clean, watertight meshes for training. In
this work, we propose DUDE - a Deep Unsigned Distance Embedding method which
alleviates both of these shortcomings. DUDE is a disentangled shape
representation that utilizes an unsigned distance field (uDF) to represent
proximity to a surface, and a normal vector field (nVF) to represent surface
orientation. We show that a combination of these two (uDF+nVF) can be used to
learn high fidelity representations for arbitrary open/closed shapes. As
opposed to prior work such as DeepSDF, our shape representations can be
directly learnt from noisy triangle soups, and do not need watertight meshes.
Additionally, we propose novel algorithms for extracting and rendering
iso-surfaces from the learnt representations. We validate DUDE on benchmark 3D
datasets and demonstrate that it produces significant improvements over the
state of the art.
</p>
<a href="http://arxiv.org/abs/2011.02570" target="_blank">arXiv:2011.02570</a> [<a href="http://arxiv.org/pdf/2011.02570" target="_blank">pdf</a>]

<h2>Multi-layer Feature Aggregation for Deep Scene Parsing Models. (arXiv:2011.02572v1 [cs.CV])</h2>
<h3>Litao Yu, Yongsheng Gao, Jun Zhou, Jian Zhang, Qiang Wu</h3>
<p>Scene parsing from images is a fundamental yet challenging problem in visual
content understanding. In this dense prediction task, the parsing model assigns
every pixel to a categorical label, which requires the contextual information
of adjacent image patches. So the challenge for this learning task is to
simultaneously describe the geometric and semantic properties of objects or a
scene. In this paper, we explore the effective use of multi-layer feature
outputs of the deep parsing networks for spatial-semantic consistency by
designing a novel feature aggregation module to generate the appropriate global
representation prior, to improve the discriminative power of features. The
proposed module can auto-select the intermediate visual features to correlate
the spatial and semantic information. At the same time, the multiple skip
connections form a strong supervision, making the deep parsing network easy to
train. Extensive experiments on four public scene parsing datasets prove that
the deep parsing network equipped with the proposed feature aggregation module
can achieve very promising results.
</p>
<a href="http://arxiv.org/abs/2011.02572" target="_blank">arXiv:2011.02572</a> [<a href="http://arxiv.org/pdf/2011.02572" target="_blank">pdf</a>]

<h2>EEGS: A Transparent Model of Emotions. (arXiv:2011.02573v1 [cs.AI])</h2>
<h3>Suman Ojha, Jonathan Vitale, Mary-Anne Williams</h3>
<p>This paper presents the computational details of our emotion model, EEGS, and
also provides an overview of a three-stage validation methodology used for the
evaluation of our model, which can also be applicable for other computational
models of emotion. A major gap in existing emotion modelling literature has
been the lack of computational/technical details of the implemented models,
which not only makes it difficult for early-stage researchers to understand the
area but also prevents benchmarking of the developed models for expert
researchers. We partly addressed these issues by presenting technical details
for the computation of appraisal variables in our previous work. In this paper,
we present mathematical formulas for the calculation of emotion intensities
based on the theoretical premises of appraisal theory. Moreover, we will
discuss how we enable our emotion model to reach to a regulated emotional state
for social acceptability of autonomous agents. We hope this paper will allow a
better transparency of knowledge, accurate benchmarking and further evolution
of the field of emotion modelling.
</p>
<a href="http://arxiv.org/abs/2011.02573" target="_blank">arXiv:2011.02573</a> [<a href="http://arxiv.org/pdf/2011.02573" target="_blank">pdf</a>]

<h2>Learning Trajectories for Visual-Inertial System Calibration via Model-based Heuristic Deep Reinforcement Learning. (arXiv:2011.02574v1 [cs.RO])</h2>
<h3>Le Chen, Yunke Ao, Florian Tschopp, Andrei Cramariuc, Michel Breyer, Jen Jen Chung, Roland Siegwart, Cesar Cadena</h3>
<p>Visual-inertial systems rely on precise calibrations of both camera
intrinsics and inter-sensor extrinsics, which typically require manually
performing complex motions in front of a calibration target. In this work we
present a novel approach to obtain favorable trajectories for visual-inertial
system calibration, using model-based deep reinforcement learning. Our key
contribution is to model the calibration process as a Markov decision process
and then use model-based deep reinforcement learning with particle swarm
optimization to establish a sequence of calibration trajectories to be
performed by a robot arm. Our experiments show that while maintaining similar
or shorter path lengths, the trajectories generated by our learned policy
result in lower calibration errors compared to random or handcrafted
trajectories.
</p>
<a href="http://arxiv.org/abs/2011.02574" target="_blank">arXiv:2011.02574</a> [<a href="http://arxiv.org/pdf/2011.02574" target="_blank">pdf</a>]

<h2>Learning and Evaluating Representations for Deep One-class Classification. (arXiv:2011.02578v1 [cs.CV])</h2>
<h3>Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister</h3>
<p>We present a two-stage framework for deep one-class classification. We first
learn self-supervised representations from one-class data, and then build
one-class classifiers on learned representations. The framework not only allows
to learn better representations, but also permits building one-class
classifiers that are faithful to the target task. In particular, we present a
novel distribution-augmented contrastive learning that extends training
distributions via data augmentation to obstruct the uniformity of contrastive
representations. Moreover, we argue that classifiers inspired by the
statistical perspective in generative or discriminative models are more
effective than existing approaches, such as an average of normality scores from
a surrogate classifier. In experiments, we demonstrate state-of-the-art
performance on visual domain one-class classification benchmarks. Finally, we
present visual explanations, confirming that the decision-making process of our
deep one-class classifier is intuitive to humans. The code is available at:
https://github.com/google-research/google-research/tree/master/deep_representation_one_class.
</p>
<a href="http://arxiv.org/abs/2011.02578" target="_blank">arXiv:2011.02578</a> [<a href="http://arxiv.org/pdf/2011.02578" target="_blank">pdf</a>]

<h2>Improved Algorithm for Seamlessly Creating Infinite Loops from a Video Clip, while Preserving Variety in Textures. (arXiv:2011.02579v1 [cs.CV])</h2>
<h3>Kunjal Panchal</h3>
<p>This project implements the paper "Video Textures" by Szeliski. The aim is to
create a "Moving Picture" or as we popularly call it, a GIF; which is
"somewhere between a photograph and a video". The idea is to input a video
which has some repeated motion (the texture), such as a flag waving, rain, or a
candle flame. The output is a new video that infinitely extends the original
video in a seamless way. In practice, the output isn't really infinte, but is
instead looped using a video player and is sufficiently long as to appear to
never repeat.

Our goal from this implementation was to: improve distance metric by
switching from a crude sum of squared distance to most sophisticated
wavelet-based distance; add intensity normalization, cross-fading and morphing
to the suggested basic algorithm. We also experiment on the trade-off between
variety and smoothness.
</p>
<a href="http://arxiv.org/abs/2011.02579" target="_blank">arXiv:2011.02579</a> [<a href="http://arxiv.org/pdf/2011.02579" target="_blank">pdf</a>]

<h2>AML-SVM: Adaptive Multilevel Learning with Support Vector Machines. (arXiv:2011.02592v1 [cs.LG])</h2>
<h3>Ehsan Sadrfaridpour, Korey Palmer, Ilya Safro (Clemson University)</h3>
<p>The support vector machines (SVM) is one of the most widely used and
practical optimization based classification models in machine learning because
of its interpretability and flexibility to produce high quality results.
However, the big data imposes a certain difficulty to the most sophisticated
but relatively slow versions of SVM, namely, the nonlinear SVM. The complexity
of nonlinear SVM solvers and the number of elements in the kernel matrix
quadratically increases with the number of samples in training data. Therefore,
both runtime and memory requirements are negatively affected. Moreover, the
parameter fitting has extra kernel parameters to tune, which exacerbate the
runtime even further. This paper proposes an adaptive multilevel learning
framework for the nonlinear SVM, which addresses these challenges, improves the
classification quality across the refinement process, and leverages
multi-threaded parallel processing for better performance. The integration of
parameter fitting in the hierarchical learning framework and adaptive process
to stop unnecessary computation significantly reduce the running time while
increase the overall performance. The experimental results demonstrate reduced
variance on prediction over validation and test data across levels in the
hierarchy, and significant speedup compared to state-of-the-art nonlinear SVM
libraries without a decrease in the classification quality. The code is
accessible at https://github.com/esadr/amlsvm.
</p>
<a href="http://arxiv.org/abs/2011.02592" target="_blank">arXiv:2011.02592</a> [<a href="http://arxiv.org/pdf/2011.02592" target="_blank">pdf</a>]

<h2>Universal Multi-Source Domain Adaptation. (arXiv:2011.02594v1 [cs.CV])</h2>
<h3>Yueming Yin, Zhen Yang, Haifeng Hu, Xiaofu Wu</h3>
<p>Unsupervised domain adaptation enables intelligent models to transfer
knowledge from a labeled source domain to a similar but unlabeled target
domain. Recent study reveals that knowledge can be transferred from one source
domain to another unknown target domain, called Universal Domain Adaptation
(UDA). However, in the real-world application, there are often more than one
source domain to be exploited for domain adaptation. In this paper, we formally
propose a more general domain adaptation setting, universal multi-source domain
adaptation (UMDA), where the label sets of multiple source domains can be
different and the label set of target domain is completely unknown. The main
challenges in UMDA are to identify the common label set between each source
domain and target domain, and to keep the model scalable as the number of
source domains increases. To address these challenges, we propose a universal
multi-source adaptation network (UMAN) to solve the domain adaptation problem
without increasing the complexity of the model in various UMDA settings. In
UMAN, we estimate the reliability of each known class in the common label set
via the prediction margin, which helps adversarial training to better align the
distributions of multiple source domains and target domain in the common label
set. Moreover, the theoretical guarantee for UMAN is also provided. Massive
experimental results show that existing UDA and multi-source DA (MDA) methods
cannot be directly applied to UMDA and the proposed UMAN achieves the
state-of-the-art performance in various UMDA settings.
</p>
<a href="http://arxiv.org/abs/2011.02594" target="_blank">arXiv:2011.02594</a> [<a href="http://arxiv.org/pdf/2011.02594" target="_blank">pdf</a>]

<h2>Binary classification with ambiguous training data. (arXiv:2011.02598v1 [cs.LG])</h2>
<h3>Naoya Otani, Yosuke Otsubo, Tetsuya Koike, Masashi Sugiyama</h3>
<p>In supervised learning, we often face with ambiguous (A) samples that are
difficult to label even by domain experts. In this paper, we consider a binary
classification problem in the presence of such A samples. This problem is
substantially different from semi-supervised learning since unlabeled samples
are not necessarily difficult samples. Also, it is different from 3-class
classification with the positive (P), negative (N), and A classes since we do
not want to classify test samples into the A class. Our proposed method extends
binary classification with reject option, which trains a classifier and a
rejector simultaneously using P and N samples based on the 0-1-$c$ loss with
rejection cost $c$. More specifically, we propose to train a classifier and a
rejector under the 0-1-$c$-$d$ loss using P, N, and A samples, where $d$ is the
misclassification penalty for ambiguous samples. In our practical
implementation, we use a convex upper bound of the 0-1-$c$-$d$ loss for
computational tractability. Numerical experiments demonstrate that our method
can successfully utilize the additional information brought by such A training
data.
</p>
<a href="http://arxiv.org/abs/2011.02598" target="_blank">arXiv:2011.02598</a> [<a href="http://arxiv.org/pdf/2011.02598" target="_blank">pdf</a>]

<h2>Merchant Category Identification Using Credit Card Transactions. (arXiv:2011.02602v1 [cs.LG])</h2>
<h3>Chin-Chia Michael Yeh, Zhongfang Zhuang, Yan Zheng, Liang Wang, Junpeng Wang, Wei Zhang</h3>
<p>Digital payment volume has proliferated in recent years with the rapid growth
of small businesses and online shops. When processing these digital
transactions, recognizing each merchant's real identity (i.e., business type)
is vital to ensure the integrity of payment processing systems. Conventionally,
this problem is formulated as a time series classification problem solely using
the merchant transaction history. However, with the large scale of the data,
and changing behaviors of merchants and consumers over time, it is extremely
challenging to achieve satisfying performance from off-the-shelf classification
methods. In this work, we approach this problem from a multi-modal learning
perspective, where we use not only the merchant time series data but also the
information of merchant-merchant relationship (i.e., affinity) to verify the
self-reported business type (i.e., merchant category) of a given merchant.
Specifically, we design two individual encoders, where one is responsible for
encoding temporal information and the other is responsible for affinity
information, and a mechanism to fuse the outputs of the two encoders to
accomplish the identification task. Our experiments on real-world credit card
transaction data between 71,668 merchants and 433,772,755 customers have
demonstrated the effectiveness and efficiency of the proposed model.
</p>
<a href="http://arxiv.org/abs/2011.02602" target="_blank">arXiv:2011.02602</a> [<a href="http://arxiv.org/pdf/2011.02602" target="_blank">pdf</a>]

<h2>Leveraging Post Hoc Context for Faster Learning in Bandit Settings with Applications in Robot-Assisted Feeding. (arXiv:2011.02604v1 [cs.RO])</h2>
<h3>Ethan K. Gordon, Sumegh Roychowdhury, Tapomayukh Bhattacharjee, Kevin Jamieson, Siddhartha S. Srinivasa</h3>
<p>Autonomous robot-assisted feeding requires the ability to acquire a wide
variety of food items. However, it is impossible for such a system to be
trained on all types of food in existence. Therefore, a key challenge is
choosing a manipulation strategy for a previously unseen food item. Previous
work showed that the problem can be represented as a linear contextual bandit
on visual information. However, food has a wide variety of multi-modal
properties relevant to manipulation that can be hard to distinguish visually.
Our key insight is that we can leverage the haptic information we collect
during manipulation to learn some of these properties and more quickly adapt
our visual model to previously unseen food. In general, we propose a modified
linear contextual bandit framework augmented with post hoc context observed
after action selection to empirically increase learning speed (as measured by
cross-validation mean square error) and reduce cumulative regret. Experiments
on synthetic data demonstrate that this effect is more pronounced when the
dimensionality of the context is large relative to the post hoc context or when
the post hoc context model is particularly easy to learn. Finally, we apply
this framework to the bite acquisition problem and demonstrate the acquisition
of 8 previously unseen types of food with 21% fewer failures across 64
attempts.
</p>
<a href="http://arxiv.org/abs/2011.02604" target="_blank">arXiv:2011.02604</a> [<a href="http://arxiv.org/pdf/2011.02604" target="_blank">pdf</a>]

<h2>Transforming Facial Weight of Real Images by Editing Latent Space of StyleGAN. (arXiv:2011.02606v1 [cs.CV])</h2>
<h3>V N S Rama Krishna Pinnimty, Matt Zhao, Palakorn Achananuparp, Ee-Peng Lim</h3>
<p>We present an invert-and-edit framework to automatically transform facial
weight of an input face image to look thinner or heavier by leveraging semantic
facial attributes encoded in the latent space of Generative Adversarial
Networks (GANs). Using a pre-trained StyleGAN as the underlying generator, we
first employ an optimization-based embedding method to invert the input image
into the StyleGAN latent space. Then, we identify the facial-weight attribute
direction in the latent space via supervised learning and edit the inverted
latent code by moving it positively or negatively along the extracted feature
axis. Our framework is empirically shown to produce high-quality and realistic
facial-weight transformations without requiring training GANs with a large
amount of labeled face images from scratch. Ultimately, our framework can be
utilized as part of an intervention to motivate individuals to make healthier
food choices by visualizing the future impacts of their behavior on appearance.
</p>
<a href="http://arxiv.org/abs/2011.02606" target="_blank">arXiv:2011.02606</a> [<a href="http://arxiv.org/pdf/2011.02606" target="_blank">pdf</a>]

<h2>Learning a Decentralized Multi-arm Motion Planner. (arXiv:2011.02608v1 [cs.RO])</h2>
<h3>Huy Ha, Jingxi Xu, Shuran Song</h3>
<p>We present a closed-loop multi-arm motion planner that is scalable and
flexible with team size. Traditional multi-arm robot systems have relied on
centralized motion planners, whose runtimes often scale exponentially with team
size, and thus, fail to handle dynamic environments with open-loop control. In
this paper, we tackle this problem with multi-agent reinforcement learning,
where a decentralized policy is trained to control one robot arm in the
multi-arm system to reach its target end-effector pose given observations of
its workspace state and target end-effector pose. The policy is trained using
Soft Actor-Critic with expert demonstrations from a sampling-based motion
planning algorithm (i.e., BiRRT). By leveraging classical planning algorithms,
we can improve the learning efficiency of the reinforcement learning algorithm
while retaining the fast inference time of neural networks. The resulting
policy scales sub-linearly and can be deployed on multi-arm systems with
variable team sizes. Thanks to the closed-loop and decentralized formulation,
our approach generalizes to 5-10 multi-arm systems and dynamic moving targets
(&gt;90% success rate for a 10-arm system), despite being trained on only 1-4 arm
planning tasks with static targets. Code and data links can be found at
https://multiarm.cs.columbia.edu.
</p>
<a href="http://arxiv.org/abs/2011.02608" target="_blank">arXiv:2011.02608</a> [<a href="http://arxiv.org/pdf/2011.02608" target="_blank">pdf</a>]

<h2>Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity. (arXiv:2011.02614v1 [cs.LG])</h2>
<h3>Tanmay Gangwani, Jian Peng, Yuan Zhou</h3>
<p>Quality-Diversity (QD) is a concept from Neuroevolution with some intriguing
applications to Reinforcement Learning. It facilitates learning a population of
agents where each member is optimized to simultaneously accumulate high
task-returns and exhibit behavioral diversity compared to other members. In
this paper, we build on a recent kernel-based method for training a QD policy
ensemble with Stein variational gradient descent. With kernels based on
$f$-divergence between the stationary distributions of policies, we convert the
problem to that of efficient estimation of the ratio of these stationary
distributions. We then study various distribution ratio estimators used
previously for off-policy evaluation and imitation and re-purpose them to
compute the gradients for policies in an ensemble such that the resultant
population is diverse and of high-quality.
</p>
<a href="http://arxiv.org/abs/2011.02614" target="_blank">arXiv:2011.02614</a> [<a href="http://arxiv.org/pdf/2011.02614" target="_blank">pdf</a>]

<h2>Lets Play Music: Audio-driven Performance Video Generation. (arXiv:2011.02631v1 [cs.CV])</h2>
<h3>Hao Zhu, Yi Li, Feixia Zhu, Aihua Zheng, Ran He</h3>
<p>We propose a new task named Audio-driven Per-formance Video Generation
(APVG), which aims to synthesizethe video of a person playing a certain
instrument guided bya given music audio clip. It is a challenging task to
gener-ate the high-dimensional temporal consistent videos from low-dimensional
audio modality. In this paper, we propose a multi-staged framework to achieve
this new task to generate realisticand synchronized performance video from
given music. Firstly,we provide both global appearance and local spatial
informationby generating the coarse videos and keypoints of body and handsfrom
a given music respectively. Then, we propose to transformthe generated
keypoints to heatmap via a differentiable spacetransformer, since the heatmap
offers more spatial informationbut is harder to generate directly from audio.
Finally, wepropose a Structured Temporal UNet (STU) to extract bothintra-frame
structured information and inter-frame temporalconsistency. They are obtained
via graph-based structure module,and CNN-GRU based high-level temporal module
respectively forfinal video generation. Comprehensive experiments validate
theeffectiveness of our proposed framework.
</p>
<a href="http://arxiv.org/abs/2011.02631" target="_blank">arXiv:2011.02631</a> [<a href="http://arxiv.org/pdf/2011.02631" target="_blank">pdf</a>]

<h2>GPR-based Model Reconstruction System for Underground Utilities Using GPRNet. (arXiv:2011.02635v1 [cs.CV])</h2>
<h3>Jinglun Feng, Liang Yang, Ejup Hoxha, Stanislav Sotnikov, Diar Sanakov, Jizhong Xiao</h3>
<p>Ground Penetrating Radar (GPR) is one of the most important non-destructive
evaluation (NDE) instruments to detect and locate underground objects (i.e.
rebars, utility pipes). Many of the previous researches focus on GPR
image-based feature detection only, and none can process sparse GPR
measurements to successfully reconstruct a very fine and detailed 3D model of
underground objects for better visualization. To address this problem, this
paper presents a novel robotic system to collect GPR data, localize the
underground utilities, and reconstruct the underground objects' dense point
cloud model. This system is composed of three modules: 1) visual-inertial-based
GPR data collection module which tags the GPR measurements with positioning
information provided by an omnidirectional robot; 2) a deep neural network
(DNN) migration module to interpret the raw GPR B-scan image into a
cross-section of object model; 3) a DNN-based 3D reconstruction module, i.e.,
GPRNet, to generate underground utility model with the fine 3D point cloud. The
experiments show that our method can generate a dense and complete point cloud
model of pipe-shaped utilities based on a sparse input, i.e., GPR raw data,
with various levels of incompleteness and noise. The experiment results on
synthetic data as well as field test data verified the effectiveness of our
method.
</p>
<a href="http://arxiv.org/abs/2011.02635" target="_blank">arXiv:2011.02635</a> [<a href="http://arxiv.org/pdf/2011.02635" target="_blank">pdf</a>]

<h2>Disentangling Latent Space for Unsupervised Semantic Face Editing. (arXiv:2011.02638v1 [cs.CV])</h2>
<h3>Kanglin Liu, Gaofeng Cao, Fei Zhou, Bozhi Liu, Jiang Duan, Guoping Qiu</h3>
<p>Editing facial images created by StyleGAN is a popular research topic with
important applications. Through editing the latent vectors, it is possible to
control the facial attributes such as smile, age, \textit{etc}. However, facial
attributes are entangled in the latent space and this makes it very difficult
to independently control a specific attribute without affecting the others. The
key to developing neat semantic control is to completely disentangle the latent
space and perform image editing in an unsupervised manner. In this paper, we
present a new technique termed Structure-Texture Independent Architecture with
Weight Decomposition and Orthogonal Regularization (STIA-WO) to disentangle the
latent space. The GAN model, applying STIA-WO, is referred to as STGAN-WO.
STGAN-WO performs weight decomposition by utilizing the style vector to
construct a fully controllable weight matrix for controlling the image
synthesis, and utilizes orthogonal regularization to ensure each entry of the
style vector only controls one factor of variation. To further disentangle the
facial attributes, STGAN-WO introduces a structure-texture independent
architecture which utilizes two independently and identically distributed
(i.i.d.) latent vectors to control the synthesis of the texture and structure
components in a disentangled way.Unsupervised semantic editing is achieved by
moving the latent code in the coarse layers along its orthogonal directions to
change texture related attributes or changing the latent code in the fine
layers to manipulate structure related ones. We present experimental results
which show that our new STGAN-WO can achieve better attribute editing than
state of the art methods (The code is available at
https://github.com/max-liu-112/STGAN-WO)
</p>
<a href="http://arxiv.org/abs/2011.02638" target="_blank">arXiv:2011.02638</a> [<a href="http://arxiv.org/pdf/2011.02638" target="_blank">pdf</a>]

<h2>Utilizing Every Image Object for Semi-supervised Phrase Grounding. (arXiv:2011.02655v1 [cs.CV])</h2>
<h3>Haidong Zhu, Arka Sadhu, Zhaoheng Zheng, Ram Nevatia</h3>
<p>Phrase grounding models localize an object in the image given a referring
expression. The annotated language queries available during training are
limited, which also limits the variations of language combinations that a model
can see during training. In this paper, we study the case applying objects
without labeled queries for training the semi-supervised phrase grounding. We
propose to use learned location and subject embedding predictors (LSEP) to
generate the corresponding language embeddings for objects lacking annotated
queries in the training set. With the assistance of the detector, we also apply
LSEP to train a grounding model on images without any annotation. We evaluate
our method based on MAttNet on three public datasets: RefCOCO, RefCOCO+, and
RefCOCOg. We show that our predictors allow the grounding system to learn from
the objects without labeled queries and improve accuracy by 34.9\% relatively
with the detection results.
</p>
<a href="http://arxiv.org/abs/2011.02655" target="_blank">arXiv:2011.02655</a> [<a href="http://arxiv.org/pdf/2011.02655" target="_blank">pdf</a>]

<h2>Compositional Scalable Object SLAM. (arXiv:2011.02658v1 [cs.RO])</h2>
<h3>Akash Sharma, Wei Dong, Michael Kaess</h3>
<p>We present a fast, scalable, and accurate Simultaneous Localization and
Mapping (SLAM) system that represents indoor scenes as a graph of objects.
Leveraging the observation that artificial environments are structured and
occupied by recognizable objects, we show that a compositional scalable object
mapping formulation is amenable to a robust SLAM solution for drift-free large
scale indoor reconstruction. To achieve this, we propose a novel semantically
assisted data association strategy that obtains unambiguous persistent object
landmarks, and a 2.5D compositional rendering method that enables reliable
frame-to-model RGB-D tracking. Consequently, we deliver an optimized online
implementation that can run at near frame rate with a single graphics card, and
provide a comprehensive evaluation against state of the art baselines. An open
source implementation will be provided at https://placeholder.
</p>
<a href="http://arxiv.org/abs/2011.02658" target="_blank">arXiv:2011.02658</a> [<a href="http://arxiv.org/pdf/2011.02658" target="_blank">pdf</a>]

<h2>Generating Large-Scale Trajectories Efficiently using Descriptions of Polynomials. (arXiv:2011.02662v1 [cs.RO])</h2>
<h3>Zhepei Wang, Hongkai Ye, Chao Xu, Fei Gao</h3>
<p>For quadrotor trajectory planning, describing a polynomial trajectory through
coefficients and end-derivatives both enjoy their own convenience in energy
minimization. We name them double descriptions of polynomial trajectories. The
transformation between them, causing most of the inefficiency and instability,
is formally analyzed in this paper. Leveraging its analytic structure, we
design a linear-complexity scheme for both jerk/snap minimization and parameter
gradient evaluation, which possesses efficiency, stability, flexibility, and
scalability. With the help of our scheme, generating an energy optimal (minimum
snap) trajectory only costs 1 $\mu s$ per piece at the scale up to 1,000,000
pieces. Moreover, generating large-scale energy-time optimal trajectories is
also accelerated by an order of magnitude against conventional methods.
</p>
<a href="http://arxiv.org/abs/2011.02662" target="_blank">arXiv:2011.02662</a> [<a href="http://arxiv.org/pdf/2011.02662" target="_blank">pdf</a>]

<h2>Restless-UCB, an Efficient and Low-complexity Algorithm for Online Restless Bandits. (arXiv:2011.02664v1 [cs.LG])</h2>
<h3>Siwei Wang, Longbo Huang, John C.S. Lui</h3>
<p>We study the online restless bandit problem, where the state of each arm
evolves according to a Markov chain, and the reward of pulling an arm depends
on both the pulled arm and the current state of the corresponding Markov chain.
In this paper, we propose Restless-UCB, a learning policy that follows the
explore-then-commit framework. In Restless-UCB, we present a novel method to
construct offline instances, which only requires $O(N)$ time-complexity ($N$ is
the number of arms) and is exponentially better than the complexity of existing
learning policy. We also prove that Restless-UCB achieves a regret upper bound
of $\tilde{O}((N+M^3)T^{2\over 3})$, where $M$ is the Markov chain state space
size and $T$ is the time horizon. Compared to existing algorithms, our result
eliminates the exponential factor (in $M,N$) in the regret upper bound, due to
a novel exploitation of the sparsity in transitions in general restless bandit
problems. As a result, our analysis technique can also be adopted to tighten
the regret bounds of existing algorithms. Finally, we conduct experiments based
on real-world dataset, to compare the Restless-UCB policy with state-of-the-art
benchmarks. Our results show that Restless-UCB outperforms existing algorithms
in regret, and significantly reduces the running time.
</p>
<a href="http://arxiv.org/abs/2011.02664" target="_blank">arXiv:2011.02664</a> [<a href="http://arxiv.org/pdf/2011.02664" target="_blank">pdf</a>]

<h2>Deep Active Learning with Augmentation-based Consistency Estimation. (arXiv:2011.02666v1 [cs.CV])</h2>
<h3>SeulGi Hong, Heonjin Ha, Junmo Kim, Min-Kook Choi</h3>
<p>In active learning, the focus is mainly on the selection strategy of
unlabeled data for enhancing the generalization capability of the next learning
cycle. For this, various uncertainty measurement methods have been proposed. On
the other hand, with the advent of data augmentation metrics as the regularizer
on general deep learning, we notice that there can be a mutual influence
between the method of unlabeled data selection and the data augmentation-based
regularization techniques in active learning scenarios. Through various
experiments, we confirmed that consistency-based regularization from analytical
learning theory could affect the generalization capability of the classifier in
combination with the existing uncertainty measurement method. By this fact, we
propose a methodology to improve generalization ability, by applying data
augmentation-based techniques to an active learning scenario. For the data
augmentation-based regularization loss, we redefined cutout (co) and cutmix
(cm) strategies as quantitative metrics and applied at both model training and
unlabeled data selection steps. We have shown that the augmentation-based
regularizer can lead to improved performance on the training step of active
learning, while that same approach can be effectively combined with the
uncertainty measurement metrics proposed so far. We used datasets such as
FashionMNIST, CIFAR10, CIFAR100, and STL10 to verify the performance of the
proposed active learning technique for multiple image classification tasks. Our
experiments show consistent performance gains for each dataset and budget
scenario.
</p>
<a href="http://arxiv.org/abs/2011.02666" target="_blank">arXiv:2011.02666</a> [<a href="http://arxiv.org/pdf/2011.02666" target="_blank">pdf</a>]

<h2>Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping. (arXiv:2011.02669v1 [cs.LG])</h2>
<h3>Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, Changjie Fan</h3>
<p>Reward shaping is an effective technique for incorporating domain knowledge
into reinforcement learning (RL). Existing approaches such as potential-based
reward shaping normally make full use of a given shaping reward function.
However, since the transformation of human knowledge into numeric reward values
is often imperfect due to reasons such as human cognitive bias, completely
utilizing the shaping reward function may fail to improve the performance of RL
algorithms. In this paper, we consider the problem of adaptively utilizing a
given shaping reward function. We formulate the utilization of shaping rewards
as a bi-level optimization problem, where the lower level is to optimize policy
using the shaping rewards and the upper level is to optimize a parameterized
shaping weight function for true reward maximization. We formally derive the
gradient of the expected true reward with respect to the shaping weight
function parameters and accordingly propose three learning algorithms based on
different assumptions. Experiments in sparse-reward cartpole and MuJoCo
environments show that our algorithms can fully exploit beneficial shaping
rewards, and meanwhile ignore unbeneficial shaping rewards or even transform
them into beneficial ones.
</p>
<a href="http://arxiv.org/abs/2011.02669" target="_blank">arXiv:2011.02669</a> [<a href="http://arxiv.org/pdf/2011.02669" target="_blank">pdf</a>]

<h2>HILONet: Hierarchical Imitation Learning from Non-Aligned Observations. (arXiv:2011.02671v1 [cs.LG])</h2>
<h3>Shanqi Liu, Junjie Cao, Wenzhou Chen, Licheng Wen, Yong Liu</h3>
<p>It is challenging learning from demonstrated observation-only trajectories in
a non-time-aligned environment because most imitation learning methods aim to
imitate experts by following the demonstration step-by-step. However, aligned
demonstrations are seldom obtainable in real-world scenarios. In this work, we
propose a new imitation learning approach called Hierarchical Imitation
Learning from Observation(HILONet), which adopts a hierarchical structure to
choose feasible sub-goals from demonstrated observations dynamically. Our
method can solve all kinds of tasks by achieving these sub-goals, whether it
has a single goal position or not. We also present three different ways to
increase sample efficiency in the hierarchical structure. We conduct extensive
experiments using several environments. The results show the improvement in
both performance and learning efficiency.
</p>
<a href="http://arxiv.org/abs/2011.02671" target="_blank">arXiv:2011.02671</a> [<a href="http://arxiv.org/pdf/2011.02671" target="_blank">pdf</a>]

<h2>AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection. (arXiv:2011.02674v1 [cs.CV])</h2>
<h3>Hao Zhu, Chaoyou Fu, Qianyi Wu, Wayne Wu, Chen Qian, Ran He</h3>
<p>Recent studies have shown that the performance of forgery detection can be
improved with diverse and challenging Deepfakes datasets. However, due to the
lack of Deepfakes datasets with large variance in appearance, which can be
hardly produced by recent identity swapping methods, the detection algorithm
may fail in this situation. In this work, we provide a new identity swapping
algorithm with large differences in appearance for face forgery detection. The
appearance gaps mainly arise from the large discrepancies in illuminations and
skin colors that widely exist in real-world scenarios. However, due to the
difficulties of modeling the complex appearance mapping, it is challenging to
transfer fine-grained appearances adaptively while preserving identity traits.
This paper formulates appearance mapping as an optimal transport problem and
proposes an Appearance Optimal Transport model (AOT) to formulate it in both
latent and pixel space. Specifically, a relighting generator is designed to
simulate the optimal transport plan. It is solved via minimizing the
Wasserstein distance of the learned features in the latent space, enabling
better performance and less computation than conventional optimization. To
further refine the solution of the optimal transport plan, we develop a
segmentation game to minimize the Wasserstein distance in the pixel space. A
discriminator is introduced to distinguish the fake parts from a mix of real
and fake image patches. Extensive experiments reveal that the superiority of
our method when compared with state-of-the-art methods and the ability of our
generated data to improve the performance of face forgery detection.
</p>
<a href="http://arxiv.org/abs/2011.02674" target="_blank">arXiv:2011.02674</a> [<a href="http://arxiv.org/pdf/2011.02674" target="_blank">pdf</a>]

<h2>Defense-friendly Images in Adversarial Attacks: Dataset and Metrics forPerturbation Difficulty. (arXiv:2011.02675v1 [cs.CV])</h2>
<h3>Camilo Pestana, Wei Liu, David Glance, Ajmal Mian</h3>
<p>Dataset bias is a problem in adversarial machine learn-ing, especially in the
evaluation of defenses. An adversarial attack or defense algorithm may show
better results on the reported dataset than can be replicated on other
datasets.Even when two algorithms are compared, their relative performance can
vary depending on the dataset. Deep learn-ing offers state-of-the-art solutions
for image recognition, but deep models are vulnerable even to small
perturbations.Research in this area focuses primarily on adversarial at-tacks
and defense algorithms. In this paper, we report for the first time, a class of
robust images that are both resilient to attacks and that recover better than
random images un-der adversarial attacks using simple defense techniques.Thus,
a test dataset with a high proportion of robust images gives a misleading
impression about the performance of an adversarial attack or defense. We
propose three metrics to determine the proportion of robust images in a dataset
and provide scoring to determine the dataset bias. We also pro-vide an
ImageNet-R dataset of 15000+ robust images to facilitate further research on
this intriguing phenomenon of image strength under attack. Our dataset,
combined with the proposed metrics, is valuable for unbiased benchmark-ing of
adversarial attack and defense algorithms
</p>
<a href="http://arxiv.org/abs/2011.02675" target="_blank">arXiv:2011.02675</a> [<a href="http://arxiv.org/pdf/2011.02675" target="_blank">pdf</a>]

<h2>Nonparametric Variable Screening with Optimal Decision Stumps. (arXiv:2011.02683v1 [stat.ML])</h2>
<h3>Jason M. Klusowski, Peter M. Tian</h3>
<p>Decision trees and their ensembles are endowed with a rich set of diagnostic
tools for ranking and screening input variables in a predictive model. One of
the most commonly used in practice is the Mean Decrease in Impurity (MDI),
which calculates an importance score for a variable by summing the weighted
impurity reductions over all non-terminal nodes split with that variable.
Despite the widespread use of tree based variable importance measures such as
MDI, pinning down their theoretical properties has been challenging and
therefore largely unexplored. To address this gap between theory and practice,
we derive rigorous finite sample performance guarantees for variable ranking
and selection in nonparametric models with MDI for a single-level CART decision
tree (decision stump). We find that the marginal signal strength of each
variable and ambient dimensionality can be considerably weaker and higher,
respectively, than state-of-the-art nonparametric variable selection methods.
Furthermore, unlike previous marginal screening methods that attempt to
directly estimate each marginal projection via a truncated basis expansion, the
fitted model used here is a simple, parsimonious decision stump, thereby
eliminating the need for tuning the number of basis terms. Thus, surprisingly,
even though decision stumps are highly inaccurate for estimation purposes, they
can still be used to perform consistent model selection.
</p>
<a href="http://arxiv.org/abs/2011.02683" target="_blank">arXiv:2011.02683</a> [<a href="http://arxiv.org/pdf/2011.02683" target="_blank">pdf</a>]

<h2>Amortized Conditional Normalized Maximum Likelihood. (arXiv:2011.02696v1 [cs.LG])</h2>
<h3>Aurick Zhou, Sergey Levine</h3>
<p>While deep neural networks provide good performance for a range of
challenging tasks, calibration and uncertainty estimation remain major
challenges. In this paper, we propose the amortized conditional normalized
maximum likelihood (ACNML) method as a scalable general-purpose approach for
uncertainty estimation, calibration, and out-of-distribution robustness with
deep networks. Our algorithm builds on the conditional normalized maximum
likelihood (CNML) coding scheme, which has minimax optimal properties according
to the minimum description length principle, but is computationally intractable
to evaluate exactly for all but the simplest of model classes. We propose to
use approximate Bayesian inference technqiues to produce a tractable
approximation to the CNML distribution. Our approach can be combined with any
approximate inference algorithm that provides tractable posterior densities
over model parameters. We demonstrate that ACNML compares favorably to a number
of prior techniques for uncertainty estimation in terms of calibration on
out-of-distribution inputs.
</p>
<a href="http://arxiv.org/abs/2011.02696" target="_blank">arXiv:2011.02696</a> [<a href="http://arxiv.org/pdf/2011.02696" target="_blank">pdf</a>]

<h2>Center-wise Local Image Mixture For Contrastive Representation Learning. (arXiv:2011.02697v1 [cs.CV])</h2>
<h3>Hao Li, Xiaopeng Zhang, Ruoyu Sun, Hongkai Xiong, Qi Tian</h3>
<p>Recent advances in unsupervised representation learning have experienced
remarkable progress, especially with the achievements of contrastive learning,
which regards each image as well its augmentations as a separate class, while
does not consider the semantic similarity among images. This paper proposes a
new kind of data augmentation, named Center-wise Local Image Mixture, to expand
the neighborhood space of an image. CLIM encourages both local similarity and
global aggregation while pulling similar images. This is achieved by searching
local similar samples of an image, and only selecting images that are closer to
the corresponding cluster center, which we denote as center-wise local
selection. As a result, similar representations are progressively approaching
the clusters, while do not break the local similarity. Furthermore, image
mixture is used as a smoothing regularization to avoid overconfidence on the
selected samples. Besides, we introduce multi-resolution augmentation, which
enables the representation to be scale invariant. Integrating the two
augmentations produces better feature representation on several unsupervised
benchmarks. Notably, we reach 75.5% top-1 accuracy with linear evaluation over
ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels, as
well as consistently outperforming supervised pretraining on several downstream
transfer tasks.
</p>
<a href="http://arxiv.org/abs/2011.02697" target="_blank">arXiv:2011.02697</a> [<a href="http://arxiv.org/pdf/2011.02697" target="_blank">pdf</a>]

<h2>A Black-Box Attack Model for Visually-Aware Recommender Systems. (arXiv:2011.02701v1 [cs.LG])</h2>
<h3>Rami Cohen, Oren Sar Shalom, Dietmar Jannach, Amihood Amir</h3>
<p>Due to the advances in deep learning, visually-aware recommender systems (RS)
have recently attracted increased research interest. Such systems combine
collaborative signals with images, usually represented as feature vectors
outputted by pre-trained image models. Since item catalogs can be huge,
recommendation service providers often rely on images that are supplied by the
item providers. In this work, we show that relying on such external sources can
make an RS vulnerable to attacks, where the goal of the attacker is to unfairly
promote certain pushed items. Specifically, we demonstrate how a new visual
attack model can effectively influence the item scores and rankings in a
black-box approach, i.e., without knowing the parameters of the model. The main
underlying idea is to systematically create small human-imperceptible
perturbations of the pushed item image and to devise appropriate gradient
approximation methods to incrementally raise the pushed item's score.
Experimental evaluations on two datasets show that the novel attack model is
effective even when the contribution of the visual features to the overall
performance of the recommender system is modest.
</p>
<a href="http://arxiv.org/abs/2011.02701" target="_blank">arXiv:2011.02701</a> [<a href="http://arxiv.org/pdf/2011.02701" target="_blank">pdf</a>]

<h2>Sampled Nonlocal Gradients for Stronger Adversarial Attacks. (arXiv:2011.02707v1 [cs.LG])</h2>
<h3>Leo Schwinn, Daniel Tenbrinck, An Nguyen, Ren&#xe9; Raab, Martin Burger, Bjoern Eskofier</h3>
<p>The vulnerability of deep neural networks to small and even imperceptible
perturbations has become a central topic in deep learning research. The
evaluation of new defense mechanisms for these so-called adversarial attacks
has proven to be challenging. Although several sophisticated defense mechanisms
were introduced, most of them were later shown to be ineffective. However, a
reliable evaluation of model robustness is mandatory for deployment in
safety-critical real-world scenarios. We propose a simple yet effective
modification to the gradient calculation of state-of-the-art first-order
adversarial attacks, which increases their success rate and thus leads to more
accurate robustness estimates. Normally, the gradient update of an attack is
directly calculated for the given data point. In general, this approach is
sensitive to noise and small local optima of the loss function. Inspired by
gradient sampling techniques from non-convex optimization, we propose to
calculate the gradient direction of the adversarial attack as the weighted
average over multiple points in the local vicinity. We empirically show that by
incorporating this additional gradient information, we are able to give a more
accurate estimation of the global descent direction on noisy and non-convex
loss surfaces. Additionally, we show that the proposed method achieves higher
success rates than a variety of state-of-the-art attacks on the benchmark
datasets MNIST, Fashion-MNIST, and CIFAR10.
</p>
<a href="http://arxiv.org/abs/2011.02707" target="_blank">arXiv:2011.02707</a> [<a href="http://arxiv.org/pdf/2011.02707" target="_blank">pdf</a>]

<h2>DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation. (arXiv:2011.02709v1 [cs.CV])</h2>
<h3>Zhenxing Zhang, Lambert Schomaker</h3>
<p>Most existing text-to-image generation methods adopt a multi-stage modular
architecture which has three significant problems: (1) Training multiple
networks can increase the run time and affect the convergence and stability of
the generative model; (2) These approaches ignore the quality of early-stage
generator images; (3) Many discriminators need to be trained. To this end, we
propose the Dual Attention Generative Adversarial Network (DTGAN) which can
synthesize high quality and visually realistic images only employing a single
generator/discriminator pair. The proposed model introduces channel-aware and
pixel-aware attention modules that can guide the generator to focus on
text-relevant channels and pixels based on the global sentence vector and to
fine-tune original feature maps using attention weights. Also, Conditional
Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our
attention modules flexibly control the amount of change in shape and texture by
the input natural-language description. Furthermore, a new type of visual loss
is utilized to enhance the image quality by ensuring the vivid shape and the
perceptually uniform color distributions of generated images. Experimental
results on benchmark datasets demonstrate the superiority of our proposed
method compared to the state-of-the-art models with a multi-stage framework.
Visualization of the attention maps shows that the channel-aware attention
module is able to localize the discriminative regions, while the pixel-aware
attention module has the ability to capture the globally visual contents for
the generation of an image.
</p>
<a href="http://arxiv.org/abs/2011.02709" target="_blank">arXiv:2011.02709</a> [<a href="http://arxiv.org/pdf/2011.02709" target="_blank">pdf</a>]

<h2>Architecture Agnostic Neural Networks. (arXiv:2011.02712v1 [cs.LG])</h2>
<h3>Sabera Talukder, Guruprasad Raghavan, Yisong Yue</h3>
<p>In this paper, we explore an alternate method for synthesizing neural network
architectures, inspired by the brain's stochastic synaptic pruning. During a
person's lifetime, numerous distinct neuronal architectures are responsible for
performing the same tasks. This indicates that biological neural networks are,
to some degree, architecture agnostic. However, artificial networks rely on
their fine-tuned weights and hand-crafted architectures for their remarkable
performance. This contrast begs the question: Can we build artificial
architecture agnostic neural networks? To ground this study we utilize sparse,
binary neural networks that parallel the brain's circuits. Within this sparse,
binary paradigm we sample many binary architectures to create families of
architecture agnostic neural networks not trained via backpropagation. These
high-performing network families share the same sparsity, distribution of
binary weights, and succeed in both static and dynamic tasks. In summation, we
create an architecture manifold search procedure to discover families or
architecture agnostic neural networks.
</p>
<a href="http://arxiv.org/abs/2011.02712" target="_blank">arXiv:2011.02712</a> [<a href="http://arxiv.org/pdf/2011.02712" target="_blank">pdf</a>]

<h2>Few-Shot Object Detection in Real Life: Case Study on Auto-Harvest. (arXiv:2011.02719v1 [cs.CV])</h2>
<h3>Kevin Riou, Jingwen Zhu, Suiyi Ling, Mathis Piquet, Vincent Truffault, Patrick Le Callet</h3>
<p>Confinement during COVID-19 has caused serious effects on agriculture all
over the world. As one of the efficient solutions, mechanical
harvest/auto-harvest that is based on object detection and robotic harvester
becomes an urgent need. Within the auto-harvest system, robust few-shot object
detection model is one of the bottlenecks, since the system is required to deal
with new vegetable/fruit categories and the collection of large-scale annotated
datasets for all the novel categories is expensive. There are many few-shot
object detection models that were developed by the community. Yet whether they
could be employed directly for real life agricultural applications is still
questionable, as there is a context-gap between the commonly used training
datasets and the images collected in real life agricultural scenarios. To this
end, in this study, we present a novel cucumber dataset and propose two data
augmentation strategies that help to bridge the context-gap. Experimental
results show that 1) the state-of-the-art few-shot object detection model
performs poorly on the novel `cucumber' category; and 2) the proposed
augmentation strategies outperform the commonly used ones.
</p>
<a href="http://arxiv.org/abs/2011.02719" target="_blank">arXiv:2011.02719</a> [<a href="http://arxiv.org/pdf/2011.02719" target="_blank">pdf</a>]

<h2>An analysis of the transfer learning of convolutional neural networks for artistic images. (arXiv:2011.02727v1 [cs.CV])</h2>
<h3>Nicolas Gonthier, Yann Gousseau, Sa&#xef;d Ladjal</h3>
<p>Transfer learning from huge natural image datasets, fine-tuning of deep
neural networks and the use of the corresponding pre-trained networks have
become de facto the core of art analysis applications. Nevertheless, the
effects of transfer learning are still poorly understood. In this paper, we
first use techniques for visualizing the network internal representations in
order to provide clues to the understanding of what the network has learned on
artistic images. Then, we provide a quantitative analysis of the changes
introduced by the learning process thanks to metrics in both the feature and
parameter spaces, as well as metrics computed on the set of maximal activation
images. These analyses are performed on several variations of the transfer
learning procedure. In particular, we observed that the network could
specialize some pre-trained filters to the new image modality and also that
higher layers tend to concentrate classes. Finally, we have shown that a double
fine-tuning involving a medium-size artistic dataset can improve the
classification on smaller datasets, even when the task changes.
</p>
<a href="http://arxiv.org/abs/2011.02727" target="_blank">arXiv:2011.02727</a> [<a href="http://arxiv.org/pdf/2011.02727" target="_blank">pdf</a>]

<h2>Why robots should be technical: Correcting mental models through technical architecture concepts. (arXiv:2011.02731v1 [cs.RO])</h2>
<h3>Lukas Hindemith, Anna-Lisa Vollmer, Jan Phillip G&#xf6;pfert, Christiane B. Wiebel-Herboth, Britta Wrede</h3>
<p>Research in social robotics is commonly focused on designing robots that
imitate human behavior. While this might increase a user's satisfaction and
acceptance of robots at first glance, it does not automatically aid a
non-expert user in naturally interacting with robots, and might actually hurt
their ability to correctly anticipate a robot's capabilities. We argue that a
faulty mental model, that the user has of the robot, is one of the main sources
of confusion. In this work we investigate how communicating technical concepts
of robotic systems to users affects their mental models, and how this can
increase the quality of human-robot interaction. We conducted an online study
and investigated possible ways of improving users' mental models. Our results
underline that communicating technical concepts can form an improved mental
model. Consequently, we show the importance of consciously designing robots
that express their capabilities and limitations.
</p>
<a href="http://arxiv.org/abs/2011.02731" target="_blank">arXiv:2011.02731</a> [<a href="http://arxiv.org/pdf/2011.02731" target="_blank">pdf</a>]

<h2>Switching Scheme: A Novel Approach for Handling Incremental Concept Drift in Real-World Data Sets. (arXiv:2011.02738v1 [cs.LG])</h2>
<h3>Lucas Baier, Vincent Kellner, Niklas K&#xfc;hl, Gerhard Satzger</h3>
<p>Machine learning models nowadays play a crucial role for many applications in
business and industry. However, models only start adding value as soon as they
are deployed into production. One challenge of deployed models is the effect of
changing data over time, which is often described with the term concept drift.
Due to their nature, concept drifts can severely affect the prediction
performance of a machine learning system. In this work, we analyze the effects
of concept drift in the context of a real-world data set. For efficient concept
drift handling, we introduce the switching scheme which combines the two
principles of retraining and updating of a machine learning model. Furthermore,
we systematically analyze existing regular adaptation as well as triggered
adaptation strategies. The switching scheme is instantiated on New York City
taxi data, which is heavily influenced by changing demand patterns over time.
We can show that the switching scheme outperforms all other baselines and
delivers promising prediction results.
</p>
<a href="http://arxiv.org/abs/2011.02738" target="_blank">arXiv:2011.02738</a> [<a href="http://arxiv.org/pdf/2011.02738" target="_blank">pdf</a>]

<h2>Goal-driven Long-Term Trajectory Prediction. (arXiv:2011.02751v1 [cs.CV])</h2>
<h3>Hung Tran, Vuong Le, Truyen Tran</h3>
<p>The prediction of humans' short-term trajectories has advanced significantly
with the use of powerful sequential modeling and rich environment feature
extraction. However, long-term prediction is still a major challenge for the
current methods as the errors could accumulate along the way. Indeed,
consistent and stable prediction far to the end of a trajectory inherently
requires deeper analysis into the overall structure of that trajectory, which
is related to the pedestrian's intention on the destination of the journey. In
this work, we propose to model a hypothetical process that determines
pedestrians' goals and the impact of such process on long-term future
trajectories. We design Goal-driven Trajectory Prediction model - a
dual-channel neural network that realizes such intuition. The two channels of
the network take their dedicated roles and collaborate to generate future
trajectories. Different than conventional goal-conditioned, planning-based
methods, the model architecture is designed to generalize the patterns and work
across different scenes with arbitrary geometrical and semantic structures. The
model is shown to outperform the state-of-the-art in various settings,
especially in large prediction horizons. This result is another evidence for
the effectiveness of adaptive structured representation of visual and
geometrical features in human behavior analysis.
</p>
<a href="http://arxiv.org/abs/2011.02751" target="_blank">arXiv:2011.02751</a> [<a href="http://arxiv.org/pdf/2011.02751" target="_blank">pdf</a>]

<h2>Robust Unsupervised Video Anomaly Detection by Multi-Path Frame Prediction. (arXiv:2011.02763v1 [cs.CV])</h2>
<h3>Xuanzhao Wang, Zhengping Che, Ke Yang, Bo Jiang, Jian Tang, Jieping Ye, Jingyu Wang, Qi Qi</h3>
<p>Video anomaly detection is commonly used in many applications such as
security surveillance and is very challenging. A majority of recent video
anomaly detection approaches utilize deep reconstruction models, but their
performance is often suboptimal because of insufficient reconstruction error
differences between normal and abnormal video frames in practice. Meanwhile,
frame prediction-based anomaly detection methods have shown promising
performance. In this paper, we propose a novel and robust unsupervised video
anomaly detection method by frame prediction with proper design which is more
in line with the characteristics of surveillance videos. The proposed method is
equipped with a multi-path ConvGRU-based frame prediction network that can
better handle semantically informative objects and areas of different scales
and capture spatial-temporal dependencies in normal videos. A noise tolerance
loss is introduced during training to mitigate the interference caused by
background noise. Extensive experiments have been conducted on the CUHK Avenue,
ShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that
our proposed method outperforms existing state-of-the-art approaches.
Remarkably, our proposed method obtains the frame-level AUC score of 88.3% on
the CUHK Avenue dataset.
</p>
<a href="http://arxiv.org/abs/2011.02763" target="_blank">arXiv:2011.02763</a> [<a href="http://arxiv.org/pdf/2011.02763" target="_blank">pdf</a>]

<h2>A Bregman Method for Structure Learning on Sparse Directed Acyclic Graphs. (arXiv:2011.02764v1 [stat.ML])</h2>
<h3>Manon Romain, Alexandre d&#x27;Aspremont</h3>
<p>We develop a Bregman proximal gradient method for structure learning on
linear structural causal models. While the problem is non-convex, has high
curvature and is in fact NP-hard, Bregman gradient methods allow us to
neutralize at least part of the impact of curvature by measuring smoothness
against a highly nonlinear kernel. This allows the method to make longer steps
and significantly improves convergence. Each iteration requires solving a
Bregman proximal step which is convex and efficiently solvable for our
particular choice of kernel. We test our method on various synthetic and real
data sets.
</p>
<a href="http://arxiv.org/abs/2011.02764" target="_blank">arXiv:2011.02764</a> [<a href="http://arxiv.org/pdf/2011.02764" target="_blank">pdf</a>]

<h2>Fast Object Detection with Latticed Multi-Scale Feature Fusion. (arXiv:2011.02780v1 [cs.CV])</h2>
<h3>Yue Shi, Bo Jiang, Zhengping Che, Jian Tang</h3>
<p>Scale variance is one of the crucial challenges in multi-scale object
detection. Early approaches address this problem by exploiting the image and
feature pyramid, which raises suboptimal results with computation burden and
constrains from inherent network structures. Pioneering works also propose
multi-scale (i.e., multi-level and multi-branch) feature fusions to remedy the
issue and have achieved encouraging progress. However, existing fusions still
have certain limitations such as feature scale inconsistency, ignorance of
level-wise semantic transformation, and coarse granularity. In this work, we
present a novel module, the Fluff block, to alleviate drawbacks of current
multi-scale fusion methods and facilitate multi-scale object detection.
Specifically, Fluff leverages both multi-level and multi-branch schemes with
dilated convolutions to have rapid, effective and finer-grained feature
fusions. Furthermore, we integrate Fluff to SSD as FluffNet, a powerful
real-time single-stage detector for multi-scale object detection. Empirical
results on MS COCO and PASCAL VOC have demonstrated that FluffNet obtains
remarkable efficiency with state-of-the-art accuracy. Additionally, we indicate
the great generality of the Fluff block by showing how to embed it to other
widely-used detectors as well.
</p>
<a href="http://arxiv.org/abs/2011.02780" target="_blank">arXiv:2011.02780</a> [<a href="http://arxiv.org/pdf/2011.02780" target="_blank">pdf</a>]

<h2>ROS-Mobile: An Android application for the Robot Operating System. (arXiv:2011.02781v1 [cs.RO])</h2>
<h3>Nils Rottmann (1), Nico Studt (1), Floris Ernst, Elmar Rueckert</h3>
<p>Controlling and monitoring complex autonomous and semi autonomous robotic
systems is a challenging task. The Robot Operating System (ROS) was developed
to act as a robotic middleware system running on Ubuntu Linux which allows,
amongst others, hardware abstraction, message-passing between individual
processes and package management. However, active support of ROS applications
for mobile devices, such as smarthphones or tablets, are missing. We developed
a ROS application for Android, which comes with an intuitive user interface for
controlling and monitoring robotic systems. Our open source contribution can be
used in a large variety of tasks and with many different kinds of robots.
Moreover, it can easily be customized and new features added. In this paper, we
give an outline over the software architecture, the main functionalities and
show some possible use-cases on different mobile robotic systems.
</p>
<a href="http://arxiv.org/abs/2011.02781" target="_blank">arXiv:2011.02781</a> [<a href="http://arxiv.org/pdf/2011.02781" target="_blank">pdf</a>]

<h2>Deep Metric Learning with Spherical Embedding. (arXiv:2011.02785v1 [cs.CV])</h2>
<h3>Dingyi Zhang, Yingming Li, Zhongfei Zhang</h3>
<p>Deep metric learning has attracted much attention in recent years, due to
seamlessly combining the distance metric learning and deep neural network. Many
endeavors are devoted to design different pair-based angular loss functions,
which decouple the magnitude and direction information for embedding vectors
and ensure the training and testing measure consistency. However, these
traditional angular losses cannot guarantee that all the sample embeddings are
on the surface of the same hypersphere during the training stage, which would
result in unstable gradient in batch optimization and may influence the quick
convergence of the embedding learning. In this paper, we first investigate the
effect of the embedding norm for deep metric learning with angular distance,
and then propose a spherical embedding constraint (SEC) to regularize the
distribution of the norms. SEC adaptively adjusts the embeddings to fall on the
same hypersphere and performs more balanced direction update. Extensive
experiments on deep metric learning, face recognition, and contrastive
self-supervised learning show that the SEC-based angular space learning
strategy significantly improves the performance of the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2011.02785" target="_blank">arXiv:2011.02785</a> [<a href="http://arxiv.org/pdf/2011.02785" target="_blank">pdf</a>]

<h2>Capture Steps: Robust Walking for Humanoid Robots. (arXiv:2011.02793v1 [cs.RO])</h2>
<h3>Marcell Missura, Maren Bennewitz, Sven Behnke</h3>
<p>Stable bipedal walking is a key prerequisite for humanoid robots to reach
their potential of being versatile helpers in our everyday environments.
Bipedal walking is, however, a complex motion that requires the coordination of
many degrees of freedom while it is also inherently unstable and sensitive to
disturbances. The balance of a walking biped has to be constantly maintained.
The most effective way of controlling balance are well timed and placed
recovery steps -- capture steps -- that absorb the expense momentum gained from
a push or a stumble. We present a bipedal gait generation framework that
utilizes step timing and foot placement techniques in order to recover the
balance of a biped even after strong disturbances. Our framework modifies the
next footstep location instantly when responding to a disturbance and generates
controllable omnidirectional walking using only very little sensing and
computational power. We exploit the open-loop stability of a central pattern
generated gait to fit a linear inverted pendulum model to the observed center
of mass trajectory. Then, we use the fitted model to predict suitable footstep
locations and timings in order to maintain balance while following a target
walking velocity. Our experiments show qualitative and statistical evidence of
one of the strongest push-recovery capabilities among humanoid robots to date.
</p>
<a href="http://arxiv.org/abs/2011.02793" target="_blank">arXiv:2011.02793</a> [<a href="http://arxiv.org/pdf/2011.02793" target="_blank">pdf</a>]

<h2>Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v1 [cs.LG])</h2>
<h3>Ting Chen, Lala Li</h3>
<p>Contrastive loss and its variants have become very popular recently for
learning visual representations without supervision. In this work, we first
generalize the standard contrastive loss based on cross entropy to a broader
family of losses that share an abstract form of $\mathcal{L}_{\text{alignment}}
+ \lambda \mathcal{L}_{\text{distribution}}$, where hidden representations are
encouraged to (1) be aligned under some transformations/augmentations, and (2)
match a prior distribution of high entropy. We show that various instantiations
of the generalized loss perform similarly under the presence of a multi-layer
non-linear projection head, and the temperature scaling ($\tau$) widely used in
the standard contrastive loss is (within a range) inversely related to the
weighting ($\lambda$) between the two loss terms. We then study an intriguing
phenomenon of feature suppression among competing features shared acros
augmented views, such as "color distribution" vs "object class". We construct
datasets with explicit and controllable competing features, and show that, for
contrastive learning, a few bits of easy-to-learn shared features could
suppress, and even fully prevent, the learning of other sets of competing
features. Interestingly, this characteristic is much less detrimental in
autoencoders based on a reconstruction loss. Existing contrastive learning
methods critically rely on data augmentation to favor certain sets of features
than others, while one may wish that a network would learn all competing
features as much as its capacity allows.
</p>
<a href="http://arxiv.org/abs/2011.02803" target="_blank">arXiv:2011.02803</a> [<a href="http://arxiv.org/pdf/2011.02803" target="_blank">pdf</a>]

<h2>Efficient Online Learning of Optimal Rankings: Dimensionality Reduction via Gradient Descent. (arXiv:2011.02817v1 [cs.LG])</h2>
<h3>Dimitris Fotakis, Thanasis Lianeas, Georgios Piliouras, Stratis Skoulakis</h3>
<p>We consider a natural model of online preference aggregation, where sets of
preferred items $R_1, R_2, \ldots, R_t$ along with a demand for $k_t$ items in
each $R_t$, appear online. Without prior knowledge of $(R_t, k_t)$, the learner
maintains a ranking $\pi_t$ aiming that at least $k_t$ items from $R_t$ appear
high in $\pi_t$. This is a fundamental problem in preference aggregation with
applications to, e.g., ordering product or news items in web pages based on
user scrolling and click patterns. The widely studied Generalized
Min-Sum-Set-Cover (GMSSC) problem serves as a formal model for the setting
above. GMSSC is NP-hard and the standard application of no-regret online
learning algorithms is computationally inefficient, because they operate in the
space of rankings. In this work, we show how to achieve low regret for GMSSC in
polynomial-time. We employ dimensionality reduction from rankings to the space
of doubly stochastic matrices, where we apply Online Gradient Descent. A key
step is to show how subgradients can be computed efficiently, by solving the
dual of a configuration LP. Using oblivious deterministic and randomized
rounding schemes, we map doubly stochastic matrices back to rankings with a
small loss in the GMSSC objective.
</p>
<a href="http://arxiv.org/abs/2011.02817" target="_blank">arXiv:2011.02817</a> [<a href="http://arxiv.org/pdf/2011.02817" target="_blank">pdf</a>]

<h2>Learning a Centroidal Motion Planner for Legged Locomotion. (arXiv:2011.02818v1 [cs.RO])</h2>
<h3>Julian Viereck, Ludovic Righetti</h3>
<p>Whole-body optimizers have been successful at automatically computing complex
dynamic locomotion behaviors. However they are often limited to offline
planning as they are computationally too expensive to replan with a high
frequency. Simpler models are then typically used for online replanning. In
this paper we present a method to generate whole body movements in real-time
for locomotion tasks. Our approach consists in learning a centroidal neural
network that predicts the desired centroidal motion given the current state of
the robot and a desired contact plan. The network is trained using an existing
whole body motion optimizer. Our approach enables to learn with few training
samples dynamic motions that can be used in a complete whole-body control
framework at high frequency, which is usually not attainable with typical
full-body optimizers. We demonstrate our method to generate a rich set of
walking and jumping motions on a real quadruped robot.
</p>
<a href="http://arxiv.org/abs/2011.02818" target="_blank">arXiv:2011.02818</a> [<a href="http://arxiv.org/pdf/2011.02818" target="_blank">pdf</a>]

<h2>Predictive Process Model Monitoring using Recurrent Neural Networks. (arXiv:2011.02819v1 [cs.LG])</h2>
<h3>Johannes De Smedt, Jochen De Weerdt, Junichiro Mori, Masanao Ochi</h3>
<p>The field of predictive process monitoring focuses on modelling future
characteristics of running business process instances, typically by either
predicting the outcome of particular objectives (e.g. completion (time), cost),
or next-in-sequence prediction (e.g. what is the next activity to execute).
This paper introduces Processes-As-Movies (PAM), a technique that provides a
middle ground between these predictive monitoring. It does so by capturing
declarative process constraints between activities in various windows of a
process execution trace, which represent a declarative process model at
subsequent stages of execution. This high-dimensional representation of a
process model allows the application of predictive modelling on how such
constraints appear and vanish throughout a process' execution. Various
recurrent neural network topologies tailored to high-dimensional input are used
to model the process model evolution with windows as time steps, including
encoder-decoder long short-term memory networks, and convolutional long
short-term memory networks. Results show that these topologies are very
effective in terms of accuracy and precision to predict a process model's
future state, which allows process owners to simultaneously verify what linear
temporal logic rules hold in a predicted process window (objective-based), and
verify what future execution traces are allowed by all the constraints together
(trace-based).
</p>
<a href="http://arxiv.org/abs/2011.02819" target="_blank">arXiv:2011.02819</a> [<a href="http://arxiv.org/pdf/2011.02819" target="_blank">pdf</a>]

<h2>Local SGD: Unified Theory and New Efficient Methods. (arXiv:2011.02828v1 [cs.LG])</h2>
<h3>Eduard Gorbunov, Filip Hanzely, Peter Richt&#xe1;rik</h3>
<p>We present a unified framework for analyzing local SGD methods in the convex
and strongly convex regimes for distributed/federated training of supervised
machine learning models. We recover several known methods as a special case of
our general framework, including Local-SGD/FedAvg, SCAFFOLD, and several
variants of SGD not originally designed for federated learning. Our framework
covers both the identical and heterogeneous data settings, supports both random
and deterministic number of local steps, and can work with a wide array of
local stochastic gradient estimators, including shifted estimators which are
able to adjust the fixed points of local iterations for faster convergence. As
an application of our framework, we develop multiple novel FL optimizers which
are superior to existing methods. In particular, we develop the first linearly
converging local SGD method which does not require any data homogeneity or
other strong assumptions.
</p>
<a href="http://arxiv.org/abs/2011.02828" target="_blank">arXiv:2011.02828</a> [<a href="http://arxiv.org/pdf/2011.02828" target="_blank">pdf</a>]

<h2>Deep tree-ensembles for multi-output prediction. (arXiv:2011.02829v1 [cs.LG])</h2>
<h3>Felipe Kenji Nakano, Konstantinos Pliakos, Celine Vens</h3>
<p>Recently, deep neural networks have expanded the state-of-art in various
scientific fields and provided solutions to long standing problems across
multiple application domains. Nevertheless, they also suffer from weaknesses
since their optimal performance depends on massive amounts of training data and
the tuning of an extended number of parameters. As a countermeasure, some
deep-forest methods have been recently proposed, as efficient and low-scale
solutions. Despite that, these approaches simply employ label classification
probabilities as induced features and primarily focus on traditional
classification and regression tasks, leaving multi-output prediction
under-explored. Moreover, recent work has demonstrated that tree-embeddings are
highly representative, especially in structured output prediction. In this
direction, we propose a novel deep tree-ensemble (DTE) model, where every layer
enriches the original feature set with a representation learning component
based on tree-embeddings. In this paper, we specifically focus on two
structured output prediction tasks, namely multi-label classification and
multi-target regression. We conducted experiments using multiple benchmark
datasets and the obtained results confirm that our method provides superior
results to state-of-the-art methods in both tasks.
</p>
<a href="http://arxiv.org/abs/2011.02829" target="_blank">arXiv:2011.02829</a> [<a href="http://arxiv.org/pdf/2011.02829" target="_blank">pdf</a>]

<h2>Pitfalls in Machine Learning Research: Reexamining the Development Cycle. (arXiv:2011.02832v1 [cs.LG])</h2>
<h3>Stella Biderman, Walter J. Scheirer</h3>
<p>Machine learning has the potential to fuel further advances in data science,
but it is greatly hindered by an ad hoc design process, poor data hygiene, and
a lack of statistical rigor in model evaluation. Recently, these issues have
begun to attract more attention as they have caused public and embarrassing
issues in research and development. Drawing from our experience as machine
learning researchers, we follow the machine learning process from algorithm
design to data collection to model evaluation, drawing attention to common
pitfalls and providing practical recommendations for improvements. At each
step, case studies are introduced to highlight how these pitfalls occur in
practice, and where things could be improved.
</p>
<a href="http://arxiv.org/abs/2011.02832" target="_blank">arXiv:2011.02832</a> [<a href="http://arxiv.org/pdf/2011.02832" target="_blank">pdf</a>]

<h2>Digital Twins: State of the Art Theory and Practice, Challenges, and Open Research Questions. (arXiv:2011.02833v1 [cs.LG])</h2>
<h3>Angira Sharma, Edward Kosasih, Jie Zhang, Alexandra Brintrup, Anisoara Calinescu</h3>
<p>Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.
</p>
<a href="http://arxiv.org/abs/2011.02833" target="_blank">arXiv:2011.02833</a> [<a href="http://arxiv.org/pdf/2011.02833" target="_blank">pdf</a>]

<h2>Augmenting Organizational Decision-Making with Deep Learning Algorithms: Principles, Promises, and Challenges. (arXiv:2011.02834v1 [cs.LG])</h2>
<h3>Yash Raj Shrestha, Vaibhav Krishna, Georg von Krogh</h3>
<p>The current expansion of theory and research on artificial intelligence in
management and organization studies has revitalized the theory and research on
decision-making in organizations. In particular, recent advances in deep
learning (DL) algorithms promise benefits for decision-making within
organizations, such as assisting employees with information processing, thereby
augment their analytical capabilities and perhaps help their transition to more
creative work.
</p>
<a href="http://arxiv.org/abs/2011.02834" target="_blank">arXiv:2011.02834</a> [<a href="http://arxiv.org/pdf/2011.02834" target="_blank">pdf</a>]

<h2>Dynamically Throttleable Neural Networks (TNN). (arXiv:2011.02836v1 [cs.LG])</h2>
<h3>Hengyue Liu, Samyak Parajuli, Jesse Hostetler, Sek Chai, Bir Bhanu</h3>
<p>Conditional computation for Deep Neural Networks (DNNs) reduce overall
computational load and improve model accuracy by running a subset of the
network. In this work, we present a runtime throttleable neural network (TNN)
that can adaptively self-regulate its own performance target and computing
resources. We designed TNN with several properties that enable more flexibility
for dynamic execution based on runtime context. TNNs are defined as
throttleable modules gated with a separately trained controller that generates
a single utilization control parameter. We validate our proposal on a number of
experiments, including Convolution Neural Networks (CNNs such as VGG, ResNet,
ResNeXt, DenseNet) using CiFAR-10 and ImageNet dataset, for object
classification and recognition tasks. We also demonstrate the effectiveness of
dynamic TNN execution on a 3D Convolustion Network (C3D) for a hand gesture
task. Results show that TNN can maintain peak accuracy performance compared to
vanilla solutions, while providing a graceful reduction in computational
requirement, down to 74% reduction in latency and 52% energy savings.
</p>
<a href="http://arxiv.org/abs/2011.02836" target="_blank">arXiv:2011.02836</a> [<a href="http://arxiv.org/pdf/2011.02836" target="_blank">pdf</a>]

<h2>Real-time parameter inference in reduced-order flame models with heteroscedastic Bayesian neural network ensembles. (arXiv:2011.02838v1 [cs.LG])</h2>
<h3>Ushnish Sengupta, Maximilian L. Croci, Matthew P. Juniper</h3>
<p>The estimation of model parameters with uncertainties from observed data is a
ubiquitous inverse problem in science and engineering. In this paper, we
suggest an inexpensive and easy to implement parameter estimation technique
that uses a heteroscedastic Bayesian Neural Network trained using anchored
ensembling. The heteroscedastic aleatoric error of the network models the
irreducible uncertainty due to parameter degeneracies in our inverse problem,
while the epistemic uncertainty of the Bayesian model captures uncertainties
which may arise from an input observation's out-of-distribution nature. We use
this tool to perform real-time parameter inference in a 6 parameter G-equation
model of a ducted, premixed flame from observations of acoustically excited
flames. We train our networks on a library of 2.1 million simulated flame
videos. Results on the test dataset of simulated flames show that the network
recovers flame model parameters, with the correlation coefficient between
predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated
uncertainty estimates. The trained neural networks are then used to infer model
parameters from real videos of a premixed Bunsen flame captured using a
high-speed camera in our lab. Re-simulation using inferred parameters shows
excellent agreement between the real and simulated flames. Compared to Ensemble
Kalman Filter-based tools that have been proposed for this problem in the
combustion literature, our neural network ensemble achieves better
data-efficiency and our sub-millisecond inference times represent a savings on
computational costs by several orders of magnitude. This allows us to calibrate
our reduced-order flame model in real-time and predict the thermoacoustic
instability behaviour of the flame more accurately.
</p>
<a href="http://arxiv.org/abs/2011.02838" target="_blank">arXiv:2011.02838</a> [<a href="http://arxiv.org/pdf/2011.02838" target="_blank">pdf</a>]

<h2>A generalized model for data science. (arXiv:2011.02842v1 [cs.LG])</h2>
<h3>Ziqi Zhang, Chuanxu Zhao</h3>
<p>Neural network is a powerful tool, which is often regarded as a black box.
However, different task requires different parameters to be set up or couldn't
work on, thus variable parameters might be a good solution for different tasks.
We present a two-stage model based on Deep reinforcement learning as well as
the pre-train method, this model could configure different parameters according
to different data, improving and optimizing those parameters furthermore
according to the returned loss value in each iteration. We apply this model to
Boston housing pricing dataset, and it got a good result in restricted
condition which was consistent with our expectations.
</p>
<a href="http://arxiv.org/abs/2011.02842" target="_blank">arXiv:2011.02842</a> [<a href="http://arxiv.org/pdf/2011.02842" target="_blank">pdf</a>]

<h2>UAV-AdNet: Unsupervised Anomaly Detection using Deep Neural Networks for Aerial Surveillance. (arXiv:2011.02853v1 [cs.CV])</h2>
<h3>Ilker Bozcan, Erdal Kayacan</h3>
<p>Anomaly detection is a key goal of autonomous surveillance systems that
should be able to alert unusual observations. In this paper, we propose a
holistic anomaly detection system using deep neural networks for surveillance
of critical infrastructures (e.g., airports, harbors, warehouses) using an
unmanned aerial vehicle (UAV). First, we present a heuristic method for the
explicit representation of spatial layouts of objects in bird-view images.
Then, we propose a deep neural network architecture for unsupervised anomaly
detection (UAV-AdNet), which is trained on environment representations and GPS
labels of bird-view images jointly. Unlike studies in the literature, we
combine GPS and image data to predict abnormal observations. We evaluate our
model against several baselines on our aerial surveillance dataset and show
that it performs better in scene reconstruction and several anomaly detection
tasks. The codes, trained models, dataset, and video will be available at
https://bozcani.github.io/uavadnet.
</p>
<a href="http://arxiv.org/abs/2011.02853" target="_blank">arXiv:2011.02853</a> [<a href="http://arxiv.org/pdf/2011.02853" target="_blank">pdf</a>]

<h2>This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition. (arXiv:2011.02863v1 [cs.CV])</h2>
<h3>Meike Nauta, Annemarie Jutte, Jesper Provoost, Christin Seifert</h3>
<p>Image recognition with prototypes is considered an interpretable alternative
for black box deep learning models. Classification depends on the extent to
which a test image "looks like" a prototype. However, perceptual similarity for
humans can be different from the similarity learnt by the model. A user is
unaware of the underlying classification strategy and does not know which image
characteristics (e.g., color or shape) is the dominant characteristic for the
decision. We address this ambiguity and argue that prototypes should be
explained. Only visualizing prototypes can be insufficient for understanding
what a prototype exactly represents, and why a prototype and an image are
considered similar. We improve interpretability by automatically enhancing
prototypes with extra information about visual characteristics considered
important by the model. Specifically, our method quantifies the influence of
color hue, shape, texture, contrast and saturation in a prototype. We apply our
method to the existing Prototypical Part Network (ProtoPNet) and show that our
explanations clarify the meaning of a prototype which might have been
interpreted incorrectly otherwise. We also reveal that visually similar
prototypes can have the same explanations, indicating redundancy. Because of
the generality of our approach, it can improve the interpretability of any
similarity-based method for prototypical image recognition.
</p>
<a href="http://arxiv.org/abs/2011.02863" target="_blank">arXiv:2011.02863</a> [<a href="http://arxiv.org/pdf/2011.02863" target="_blank">pdf</a>]

<h2>Transfer Meta-Learning: Information-Theoretic Bounds and Information Meta-Risk Minimization. (arXiv:2011.02872v1 [cs.LG])</h2>
<h3>Sharu Theresa Jose, Osvaldo Simeone</h3>
<p>Meta-learning automatically infers an inductive bias by observing data from a
number of related tasks. The inductive bias is encoded by hyperparameters that
determine aspects of the model class or training algorithm, such as
initialization or learning rate. Meta-learning assumes that the learning tasks
belong to a task environment, and that tasks are drawn from the same task
environment both during meta-training and meta-testing. This, however, may not
hold true in practice. In this paper, we introduce the problem of transfer
meta-learning, in which tasks are drawn from a target task environment during
meta-testing that may differ from the source task environment observed during
meta-training. Novel information-theoretic upper bounds are obtained on the
transfer meta-generalization gap, which measures the difference between the
meta-training loss, available at the meta-learner, and the average loss on
meta-test data from a new, randomly selected, task in the target task
environment. The first bound, on the average transfer meta-generalization gap,
captures the meta-environment shift between source and target task environments
via the KL divergence between source and target data distributions. The second,
PAC-Bayesian bound, and the third, single-draw bound, account for this shift
via the log-likelihood ratio between source and target task distributions.
Furthermore, two transfer meta-learning solutions are introduced. For the
first, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the
average optimality gap. The second, referred to as Information Meta-Risk
Minimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is
shown via experiments to potentially outperform EMRM.
</p>
<a href="http://arxiv.org/abs/2011.02872" target="_blank">arXiv:2011.02872</a> [<a href="http://arxiv.org/pdf/2011.02872" target="_blank">pdf</a>]

<h2>Against Adversarial Learning: Naturally Distinguish Known and Unknown in Open Set Domain Adaptation. (arXiv:2011.02876v1 [cs.LG])</h2>
<h3>Sitong Mao, Xiao Shen, Fu-lai Chung</h3>
<p>Open set domain adaptation refers to the scenario that the target domain
contains categories that do not exist in the source domain. It is a more common
situation in the reality compared with the typical closed set domain adaptation
where the source domain and the target domain contain the same categories. The
main difficulty of open set domain adaptation is that we need to distinguish
which target data belongs to the unknown classes when machine learning models
only have concepts about what they know. In this paper, we propose an "against
adversarial learning" method that can distinguish unknown target data and known
data naturally without setting any additional hyper parameters and the target
data predicted to the known classes can be classified at the same time.
Experimental results show that the proposed method can make significant
improvement in performance compared with several state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.02876" target="_blank">arXiv:2011.02876</a> [<a href="http://arxiv.org/pdf/2011.02876" target="_blank">pdf</a>]

<h2>Mixed Set Domain Adaptation. (arXiv:2011.02877v1 [cs.LG])</h2>
<h3>Sitong Mao, Keli Zhang, Fu-lai Chung</h3>
<p>In the settings of conventional domain adaptation, categories of the source
dataset are from the same domain (or domains for multi-source domain
adaptation), which is not always true in reality. In this paper, we propose
\textbf{\textit{Mixed Set Domain Adaptation} (MSDA)}. Under the settings of
MSDA, different categories of the source dataset are not all collected from the
same domain(s). For instance, category $1\sim k$ are collected from domain
$\alpha$ while category $k+1\sim c$ are collected from domain $\beta$. Under
such situation, domain adaptation performance will be further influenced
because of the distribution discrepancy inside the source data. A feature
element-wise weighting (FEW) method that can reduce distribution discrepancy
between different categories is also proposed for MSDA. Experimental results
and quality analysis show the significance of solving MSDA problem and the
effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2011.02877" target="_blank">arXiv:2011.02877</a> [<a href="http://arxiv.org/pdf/2011.02877" target="_blank">pdf</a>]

<h2>Robust building footprint extraction from big multi-sensor data using deep competition network. (arXiv:2011.02879v1 [cs.CV])</h2>
<h3>Mehdi Khoshboresh-Masouleh, Mohammad R. Saradjian</h3>
<p>Building footprint extraction (BFE) from multi-sensor data such as optical
images and light detection and ranging (LiDAR) point clouds is widely used in
various fields of remote sensing applications. However, it is still challenging
research topic due to relatively inefficient building extraction techniques
from variety of complex scenes in multi-sensor data. In this study, we develop
and evaluate a deep competition network (DCN) that fuses very high spatial
resolution optical remote sensing images with LiDAR data for robust BFE. DCN is
a deep superpixelwise convolutional encoder-decoder architecture using the
encoder vector quantization with classified structure. DCN consists of five
encoding-decoding blocks with convolutional weights for robust binary
representation (superpixel) learning. DCN is trained and tested in a big
multi-sensor dataset obtained from the state of Indiana in the United States
with multiple building scenes. Comparison results of the accuracy assessment
showed that DCN has competitive BFE performance in comparison with other deep
semantic binary segmentation architectures. Therefore, we conclude that the
proposed model is a suitable solution to the robust BFE from big multi-sensor
data.
</p>
<a href="http://arxiv.org/abs/2011.02879" target="_blank">arXiv:2011.02879</a> [<a href="http://arxiv.org/pdf/2011.02879" target="_blank">pdf</a>]

<h2>Collaborative City Digital Twin For Covid-19 Pandemic: A Federated Learning Solution. (arXiv:2011.02883v1 [cs.LG])</h2>
<h3>Junjie Pang, Jianbo Li, Zhenzhen Xie, Yan Huang, Zhipeng Cai</h3>
<p>In this work, we propose a collaborative city digital twin based on FL, a
novel paradigm that allowing multiple city DT to share the local strategy and
status in a timely manner. In particular, an FL central server manages the
local updates of multiple collaborators (city DT), provides a global model
which is trained in multiple iterations at different city DT systems, until the
model gains the correlations between various response plan and infection trend.
That means, a collaborative city DT paradigm based on FL techniques can obtain
knowledge and patterns from multiple DTs, and eventually establish a `global
view' for city crisis management. Meanwhile, it also helps to improve each city
digital twin selves by consolidating other DT's respective data without
violating privacy rules. To validate the proposed solution, we take COVID-19
pandemic as a case study. The experimental results on the real dataset with
various response plan validate our proposed solution and demonstrate the
superior performance.
</p>
<a href="http://arxiv.org/abs/2011.02883" target="_blank">arXiv:2011.02883</a> [<a href="http://arxiv.org/pdf/2011.02883" target="_blank">pdf</a>]

<h2>Short-Term Memory Optimization in Recurrent Neural Networks by Autoencoder-based Initialization. (arXiv:2011.02886v1 [cs.LG])</h2>
<h3>Antonio Carta, Alessandro Sperduti, Davide Bacciu</h3>
<p>Training RNNs to learn long-term dependencies is difficult due to vanishing
gradients. We explore an alternative solution based on explicit memorization
using linear autoencoders for sequences, which allows to maximize the
short-term memory and that can be solved with a closed-form solution without
backpropagation. We introduce an initialization schema that pretrains the
weights of a recurrent neural network to approximate the linear autoencoder of
the input sequences and we show how such pretraining can better support solving
hard classification tasks with long sequences. We test our approach on
sequential and permuted MNIST. We show that the proposed approach achieves a
much lower reconstruction error for long sequences and a better gradient
propagation during the finetuning phase.
</p>
<a href="http://arxiv.org/abs/2011.02886" target="_blank">arXiv:2011.02886</a> [<a href="http://arxiv.org/pdf/2011.02886" target="_blank">pdf</a>]

<h2>Improving Robotic Grasping on Monocular Images Via Multi-Task Learning and Positional Loss. (arXiv:2011.02888v1 [cs.RO])</h2>
<h3>William Prew, Toby Breckon, Magnus Bordewich, Ulrik Beierholm</h3>
<p>In this paper, we introduce two methods of improving real-time object
grasping performance from monocular colour images in an end-to-end CNN
architecture. The first is the addition of an auxiliary task during model
training (multi-task learning). Our multi-task CNN model improves grasping
performance from a baseline average of 72.04% to 78.14% on the large Jacquard
grasping dataset when performing a supplementary depth reconstruction task. The
second is introducing a positional loss function that emphasises loss per pixel
for secondary parameters (gripper angle and width) only on points of an object
where a successful grasp can take place. This increases performance from a
baseline average of 72.04% to 78.92% as well as reducing the number of training
epochs required. These methods can be also performed in tandem resulting in a
further performance increase to 79.12% while maintaining sufficient inference
speed to afford real-time grasp processing.
</p>
<a href="http://arxiv.org/abs/2011.02888" target="_blank">arXiv:2011.02888</a> [<a href="http://arxiv.org/pdf/2011.02888" target="_blank">pdf</a>]

<h2>Hyperrealistic Image Inpainting with Hypergraphs. (arXiv:2011.02904v1 [cs.CV])</h2>
<h3>Gourav Wadhwa, Abhinav Dhall, Subrahmanyam Murala, Usman Tariq</h3>
<p>Image inpainting is a non-trivial task in computer vision due to multiple
possibilities for filling the missing data, which may be dependent on the
global information of the image. Most of the existing approaches use the
attention mechanism to learn the global context of the image. This attention
mechanism produces semantically plausible but blurry results because of
incapability to capture the global context. In this paper, we introduce
hypergraph convolution on spatial features to learn the complex relationship
among the data. We introduce a trainable mechanism to connect nodes using
hyperedges for hypergraph convolution. To the best of our knowledge, hypergraph
convolution have never been used on spatial features for any image-to-image
tasks in computer vision. Further, we introduce gated convolution in the
discriminator to enforce local consistency in the predicted image. The
experiments on Places2, CelebA-HQ, Paris Street View, and Facades datasets,
show that our approach achieves state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2011.02904" target="_blank">arXiv:2011.02904</a> [<a href="http://arxiv.org/pdf/2011.02904" target="_blank">pdf</a>]

<h2>Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers. (arXiv:2011.02910v1 [cs.CV])</h2>
<h3>Zhaoshuo Li, Xingtong Liu, Francis X. Creighton, Russell H. Taylor, Mathias Unberath</h3>
<p>Stereo depth estimation relies on optimal correspondence matching between
pixels on epipolar lines in the left and right image to infer depth. Rather
than matching individual pixels, in this work, we revisit the problem from a
sequence-to-sequence correspondence perspective to replace cost volume
construction with dense pixel matching using position information and
attention. This approach, named STereo TRansformer (STTR), has several
advantages: It 1) relaxes the limitation of a fixed disparity range, 2)
identifies occluded regions and provides confidence of estimation, and 3)
imposes uniqueness constraints during the matching process. We report promising
results on both synthetic and real-world datasets and demonstrate that STTR
generalizes well across different domains, even without fine-tuning. Our code
is publicly available at https://github.com/mli0603/stereo-transformer.
</p>
<a href="http://arxiv.org/abs/2011.02910" target="_blank">arXiv:2011.02910</a> [<a href="http://arxiv.org/pdf/2011.02910" target="_blank">pdf</a>]

<h2>EM Based Bounding of Unidentifiable Queries in Structural Causal Models. (arXiv:2011.02912v1 [cs.AI])</h2>
<h3>Marco Zaffalon, Alessandro Antonucci, Rafael Caba&#xf1;as</h3>
<p>A structural causal model is made of endogenous (manifest) and exogenous
(latent) variables. In a recent paper, it has been shown that endogenous
observations induce linear constraints on the probabilities of the exogenous
variables. This allows to exactly map a causal model into a \emph{credal
network}. Causal inferences, such as interventions and counterfactuals, can
consequently be obtained by standard credal network algorithms. These natively
return sharp values in the identifiable case, while intervals corresponding to
the exact bounds are produced for unidentifiable queries. In this paper we
present an approximate characterization of the constraints on the exogenous
probabilities. This is based on a specialization of the EM algorithm to the
treatment of the missing values in the exogenous observations. Multiple EM runs
can be consequently used to describe the causal model as a set of Bayesian
networks and, hence, a credal network to be queried for the bounding of
unidentifiable queries. Preliminary empirical tests show how this approach
might provide good inner bounds with relatively few runs. This is a promising
direction for causal analysis in models whose topology prevents a
straightforward specification of the credal mapping.
</p>
<a href="http://arxiv.org/abs/2011.02912" target="_blank">arXiv:2011.02912</a> [<a href="http://arxiv.org/pdf/2011.02912" target="_blank">pdf</a>]

<h2>Domain-independent generation and classification of behavior traces. (arXiv:2011.02918v1 [cs.AI])</h2>
<h3>Daniel Borrajo, Manuela Veloso</h3>
<p>Financial institutions mostly deal with people. Therefore, characterizing
different kinds of human behavior can greatly help institutions for improving
their relation with customers and with regulatory offices. In many of such
interactions, humans have some internal goals, and execute some actions within
the financial system that lead them to achieve their goals. In this paper, we
tackle these tasks as a behavior-traces classification task. An observer agent
tries to learn characterizing other agents by observing their behavior when
taking actions in a given environment. The other agents can be of several types
and the goal of the observer is to identify the type of the other agent given a
trace of observations. We present CABBOT, a learning technique that allows the
agent to perform on-line classification of the type of planning agent whose
behavior is observing. In this work, the observer agent has partial and noisy
observability of the environment (state and actions of the other agents). In
order to evaluate the performance of the learning technique, we have generated
a domain-independent goal-based simulator of agents. We present experiments in
several (both financial and non-financial) domains with promising results.
</p>
<a href="http://arxiv.org/abs/2011.02918" target="_blank">arXiv:2011.02918</a> [<a href="http://arxiv.org/pdf/2011.02918" target="_blank">pdf</a>]

<h2>Asynchronous Deep Model Reference Adaptive Control. (arXiv:2011.02920v1 [cs.RO])</h2>
<h3>Girish Joshi, Jasvir Virdi, Girish Chowdhary</h3>
<p>In this paper, we present Asynchronous implementation of Deep Neural
Network-based Model Reference Adaptive Control (DMRAC). We evaluate this new
neuro-adaptive control architecture through flight tests on a small quadcopter.
We demonstrate that a single DMRAC controller can handle significant
nonlinearities due to severe system faults and deliberate wind disturbances
while executing high-bandwidth attitude control. We also show that the
architecture has long-term learning abilities across different flight regimes,
and can generalize to fly different flight trajectories than those on which it
was trained. These results demonstrating the efficacy of this architecture for
high bandwidth closed-loop attitude control of unstable and nonlinear robots
operating in adverse situations. To achieve these results, we designed a
software+communication architecture to ensure online real-time inference of the
deep network on a high-bandwidth computation-limited platform. We expect that
this architecture will benefit other deep learning in the closed-loop
experiments on robots.
</p>
<a href="http://arxiv.org/abs/2011.02920" target="_blank">arXiv:2011.02920</a> [<a href="http://arxiv.org/pdf/2011.02920" target="_blank">pdf</a>]

<h2>An SMT-Based Approach for Verifying Binarized Neural Networks. (arXiv:2011.02948v1 [cs.LG])</h2>
<h3>Guy Amir, Haoze Wu, Clark Barrett, Guy Katz</h3>
<p>Deep learning has emerged as an effective approach for creating modern
software systems, with neural networks often surpassing hand-crafted systems.
Unfortunately, neural networks are known to suffer from various safety and
security issues. Formal verification is a promising avenue for tackling this
difficulty, by formally certifying that networks are correct. We propose an
SMT-based technique for verifying \emph{binarized neural networks} - a popular
kind of neural networks, where some weights have been binarized in order to
render the neural network more memory and energy efficient, and quicker to
evaluate. One novelty of our technique is that it allows the verification of
neural networks that include both binarized and non-binarized components.
Neural network verification is computationally very difficult, and so we
propose here various optimizations, integrated into our SMT procedure as
deduction steps, as well as an approach for parallelizing verification queries.
We implement our technique as an extension to the Marabou framework, and use it
to evaluate the approach on popular binarized neural network architectures.
</p>
<a href="http://arxiv.org/abs/2011.02948" target="_blank">arXiv:2011.02948</a> [<a href="http://arxiv.org/pdf/2011.02948" target="_blank">pdf</a>]

<h2>Generalized Negative Correlation Learning for Deep Ensembling. (arXiv:2011.02952v1 [cs.LG])</h2>
<h3>Sebastian Buschj&#xe4;ger, Lukas Pfahler, Katharina Morik</h3>
<p>Ensemble algorithms offer state of the art performance in many machine
learning applications. A common explanation for their excellent performance is
due to the bias-variance decomposition of the mean squared error which shows
that the algorithm's error can be decomposed into its bias and variance. Both
quantities are often opposed to each other and ensembles offer an effective way
to manage them as they reduce the variance through a diverse set of base
learners while keeping the bias low at the same time. Even though there have
been numerous works on decomposing other loss functions, the exact mathematical
connection is rarely exploited explicitly for ensembling, but merely used as a
guiding principle. In this paper, we formulate a generalized bias-variance
decomposition for arbitrary twice differentiable loss functions and study it in
the context of Deep Learning. We use this decomposition to derive a Generalized
Negative Correlation Learning (GNCL) algorithm which offers explicit control
over the ensemble's diversity and smoothly interpolates between the two
extremes of independent training and the joint training of the ensemble. We
show how GNCL encapsulates many previous works and discuss under which
circumstances training of an ensemble of Neural Networks might fail and what
ensembling method should be favored depending on the choice of the individual
networks.
</p>
<a href="http://arxiv.org/abs/2011.02952" target="_blank">arXiv:2011.02952</a> [<a href="http://arxiv.org/pdf/2011.02952" target="_blank">pdf</a>]

<h2>Low-Complexity Models for Acoustic Scene Classification Based on Receptive Field Regularization and Frequency Damping. (arXiv:2011.02955v1 [cs.LG])</h2>
<h3>Khaled Koutini, Florian Henkel, Hamid Eghbal-zadeh, Gerhard Widmer</h3>
<p>Deep Neural Networks are known to be very demanding in terms of computing and
memory requirements. Due to the ever increasing use of embedded systems and
mobile devices with a limited resource budget, designing low-complexity models
without sacrificing too much of their predictive performance gained great
importance. In this work, we investigate and compare several well-known methods
to reduce the number of parameters in neural networks. We further put these
into the context of a recent study on the effect of the Receptive Field (RF) on
a model's performance, and empirically show that we can achieve high-performing
low-complexity models by applying specific restrictions on the RFs, in
combination with parameter reduction methods. Additionally, we propose a
filter-damping technique for regularizing the RF of models, without altering
their architecture and changing their parameter counts. We will show that
incorporating this technique improves the performance in various low-complexity
settings such as pruning and decomposed convolution. Using our proposed filter
damping, we achieved the 1st rank at the DCASE-2020 Challenge in the task of
Low-Complexity Acoustic Scene Classification.
</p>
<a href="http://arxiv.org/abs/2011.02955" target="_blank">arXiv:2011.02955</a> [<a href="http://arxiv.org/pdf/2011.02955" target="_blank">pdf</a>]

<h2>Conflicting Bundles: Adapting Architectures Towards the Improved Training of Deep Neural Networks. (arXiv:2011.02956v1 [cs.LG])</h2>
<h3>David Peer, Sebastian Stabinger, Antonio Rodriguez-Sanchez</h3>
<p>Designing neural network architectures is a challenging task and knowing
which specific layers of a model must be adapted to improve the performance is
almost a mystery. In this paper, we introduce a novel theory and metric to
identify layers that decrease the test accuracy of the trained models, this
identification is done as early as at the beginning of training. In the
worst-case, such a layer could lead to a network that can not be trained at
all. More precisely, we identified those layers that worsen the performance
because they produce conflicting training bundles as we show in our novel
theoretical analysis, complemented by our extensive empirical studies. Based on
these findings, a novel algorithm is introduced to remove performance
decreasing layers automatically. Architectures found by this algorithm achieve
a competitive accuracy when compared against the state-of-the-art
architectures. While keeping such high accuracy, our approach drastically
reduces memory consumption and inference time for different computer vision
tasks.
</p>
<a href="http://arxiv.org/abs/2011.02956" target="_blank">arXiv:2011.02956</a> [<a href="http://arxiv.org/pdf/2011.02956" target="_blank">pdf</a>]

<h2>Measuring Data Collection Quality for Community Healthcare. (arXiv:2011.02962v1 [cs.LG])</h2>
<h3>Ramesha Karunasena (1), Mohammad Sarparajul Ambiya (2), Arunesh Sinha (1), Ruchit Nagar (2), Saachi Dalal (2), Divy Thakkar (3), Milind Tambe (3) ((1) Singapore Management University, (2) Khushi Baby, India, (3) Google Research, India)</h3>
<p>Machine learning has tremendous potential to provide targeted interventions
in low-resource communities, however the availability of high-quality public
health data is a significant challenge. In this work, we partner with field
experts at an non-governmental organization (NGO) in India to define and test a
data collection quality score for each health worker who collects data. This
challenging unlabeled data problem is handled by building upon domain-expert's
guidance to design a useful data representation that is then clustered to infer
a data quality score. We also provide a more interpretable version of the
score. These scores already provide for a measurement of data collection
quality; in addition, we also predict the quality for future time steps and
find our results to be very accurate. Our work was successfully field tested
and is in the final stages of deployment in Rajasthan, India.
</p>
<a href="http://arxiv.org/abs/2011.02962" target="_blank">arXiv:2011.02962</a> [<a href="http://arxiv.org/pdf/2011.02962" target="_blank">pdf</a>]

<h2>On the Information Complexity of Proper Learners for VC Classes in the Realizable Case. (arXiv:2011.02970v1 [cs.LG])</h2>
<h3>Mahdi Haghifam, Gintare Karolina Dziugaite, Shay Moran, Daniel M. Roy</h3>
<p>We provide a negative resolution to a conjecture of Steinke and Zakynthinou
(2020a), by showing that their bound on the conditional mutual information
(CMI) of proper learners of Vapnik--Chervonenkis (VC) classes cannot be
improved from $d \log n +2$ to $O(d)$, where $n$ is the number of i.i.d.
training examples. In fact, we exhibit VC classes for which the CMI of any
proper learner cannot be bounded by any real-valued function of the VC
dimension only.
</p>
<a href="http://arxiv.org/abs/2011.02970" target="_blank">arXiv:2011.02970</a> [<a href="http://arxiv.org/pdf/2011.02970" target="_blank">pdf</a>]

<h2>CPR: Understanding and Improving Failure Tolerant Training for Deep Learning Recommendation with Partial Recovery. (arXiv:2011.02999v1 [cs.LG])</h2>
<h3>Kiwan Maeng, Shivam Bharuka, Isabel Gao, Mark C. Jeffrey, Vikram Saraph, Bor-Yiing Su, Caroline Trippel, Jiyan Yang, Mike Rabbat, Brandon Lucia, Carole-Jean Wu</h3>
<p>The paper proposes and optimizes a partial recovery training system, CPR, for
recommendation models. CPR relaxes the consistency requirement by enabling
non-failed nodes to proceed without loading checkpoints when a node fails
during training, improving failure-related overheads. The paper is the first to
the extent of our knowledge to perform a data-driven, in-depth analysis of
applying partial recovery to recommendation models and identified a trade-off
between accuracy and performance. Motivated by the analysis, we present CPR, a
partial recovery training system that can reduce the training time and maintain
the desired level of model accuracy by (1) estimating the benefit of partial
recovery, (2) selecting an appropriate checkpoint saving interval, and (3)
prioritizing to save updates of more frequently accessed parameters. Two
variants of CPR, CPR-MFU and CPR-SSU, reduce the checkpoint-related overhead
from 8.2-8.5% to 0.53-0.68% compared to full recovery, on a configuration
emulating the failure pattern and overhead of a production-scale cluster. While
reducing overhead significantly, CPR achieves model quality on par with the
more expensive full recovery scheme, training the state-of-the-art
recommendation model using Criteo's Ads CTR dataset. Our preliminary results
also suggest that CPR can speed up training on a real production-scale cluster,
without notably degrading the accuracy.
</p>
<a href="http://arxiv.org/abs/2011.02999" target="_blank">arXiv:2011.02999</a> [<a href="http://arxiv.org/pdf/2011.02999" target="_blank">pdf</a>]

<h2>Data Augmentation via Structured Adversarial Perturbations. (arXiv:2011.03010v1 [cs.LG])</h2>
<h3>Calvin Luo, Hossein Mobahi, Samy Bengio</h3>
<p>Data augmentation is a major component of many machine learning methods with
state-of-the-art performance. Common augmentation strategies work by drawing
random samples from a space of transformations. Unfortunately, such sampling
approaches are limited in expressivity, as they are unable to scale to rich
transformations that depend on numerous parameters due to the curse of
dimensionality. Adversarial examples can be considered as an alternative scheme
for data augmentation. By being trained on the most difficult modifications of
the inputs, the resulting models are then hopefully able to handle other,
presumably easier, modifications as well. The advantage of adversarial
augmentation is that it replaces sampling with the use of a single, calculated
perturbation that maximally increases the loss. The downside, however, is that
these raw adversarial perturbations appear rather unstructured; applying them
often does not produce a natural transformation, contrary to a desirable data
augmentation technique. To address this, we propose a method to generate
adversarial examples that maintain some desired natural structure. We first
construct a subspace that only contains perturbations with the desired
structure. We then project the raw adversarial gradient onto this space to
select a structured transformation that would maximally increase the loss when
applied. We demonstrate this approach through two types of image
transformations: photometric and geometric. Furthermore, we show that training
on such structured adversarial images improves generalization.
</p>
<a href="http://arxiv.org/abs/2011.03010" target="_blank">arXiv:2011.03010</a> [<a href="http://arxiv.org/pdf/2011.03010" target="_blank">pdf</a>]

<h2>CompressAI: a PyTorch library and evaluation platform for end-to-end compression research. (arXiv:2011.03029v1 [cs.CV])</h2>
<h3>Jean B&#xe9;gaint, Fabien Racap&#xe9;, Simon Feltman, Akshay Pushparaja</h3>
<p>This paper presents CompressAI, a platform that provides custom operations,
layers, models and tools to research, develop and evaluate end-to-end image and
video compression codecs. In particular, CompressAI includes pre-trained models
and evaluation tools to compare learned methods with traditional codecs.
Multiple models from the state-of-the-art on learned end-to-end compression
have thus been reimplemented in PyTorch and trained from scratch. We also
report objective comparison results using PSNR and MS-SSIM metrics vs.
bit-rate, using the Kodak image dataset as test set. Although this framework
currently implements models for still-picture compression, it is intended to be
soon extended to the video compression domain.
</p>
<a href="http://arxiv.org/abs/2011.03029" target="_blank">arXiv:2011.03029</a> [<a href="http://arxiv.org/pdf/2011.03029" target="_blank">pdf</a>]

<h2>Fast Rates for Contextual Linear Optimization. (arXiv:2011.03030v1 [stat.ML])</h2>
<h3>Yichun Hu, Nathan Kallus, Xiaojie Mao</h3>
<p>Incorporating side observations of predictive features can help reduce
uncertainty in operational decision making, but it also requires we tackle a
potentially complex predictive relationship. Although one may use a variety of
off-the-shelf machine learning methods to learn a predictive model and then
plug it into our decision-making problem, a variety of recent work has instead
advocated integrating estimation and optimization by taking into consideration
downstream decision performance. Surprisingly, in the case of contextual linear
optimization, we show that the naive plug-in approach actually achieves regret
convergence rates that are significantly faster than the best-possible by
methods that directly optimize down-stream decision performance. We show this
by leveraging the fact that specific problem instances do not have arbitrarily
bad near-degeneracy. While there are other pros and cons to consider as we
discuss, our results highlight a very nuanced landscape for the enterprise to
integrate estimation and optimization.
</p>
<a href="http://arxiv.org/abs/2011.03030" target="_blank">arXiv:2011.03030</a> [<a href="http://arxiv.org/pdf/2011.03030" target="_blank">pdf</a>]

<h2>Teaching with Commentaries. (arXiv:2011.03037v1 [cs.LG])</h2>
<h3>Aniruddh Raghu, Maithra Raghu, Simon Kornblith, David Duvenaud, Geoffrey Hinton</h3>
<p>Effective training of deep neural networks can be challenging, and there
remain many open questions on how to best learn these models. Recently
developed methods to improve neural network training examine teaching:
providing learned information during the training process to improve downstream
model performance. In this paper, we take steps towards extending the scope of
teaching. We propose a flexible teaching framework using commentaries,
meta-learned information helpful for training on a particular task or dataset.
We present an efficient and scalable gradient-based method to learn
commentaries, leveraging recent work on implicit differentiation. We explore
diverse applications of commentaries, from learning weights for individual
training examples, to parameterizing label-dependent data augmentation
policies, to representing attention masks that highlight salient image regions.
In these settings, we find that commentaries can improve training speed and/or
performance and also provide fundamental insights about the dataset and
training process.
</p>
<a href="http://arxiv.org/abs/2011.03037" target="_blank">arXiv:2011.03037</a> [<a href="http://arxiv.org/pdf/2011.03037" target="_blank">pdf</a>]

<h2>Multi-level Chaotic Maps for 3D Textured Model Encryption. (arXiv:1709.08364v2 [cs.CV] UPDATED)</h2>
<h3>Xin Jin, Shuyun Zhu, Le Wu, Geng Zhao, Xiaodong Li, Quan Zhou, Huimin Lu</h3>
<p>With rapid progress of Virtual Reality and Augmented Reality technologies, 3D
contents are the next widespread media in many applications. Thus, the
protection of 3D models is primarily important. Encryption of 3D models is
essential to maintain confidentiality. Previous work on encryption of 3D
surface model often consider the point clouds, the meshes and the textures
individually. In this work, a multi-level chaotic maps models for 3D textured
encryption was presented by observing the different contributions for
recognizing cipher 3D models between vertices (point cloud), polygons and
textures. For vertices which make main contribution for recognizing, we use
high level 3D Lu chaotic map to encrypt them. For polygons and textures which
make relatively smaller contributions for recognizing, we use 2D Arnold's cat
map and 1D Logistic map to encrypt them, respectively. The experimental results
show that our method can get similar performance with the other method use the
same high level chaotic map for point cloud, polygons and textures, while we
use less time. Besides, our method can resist more method of attacks such as
statistic attack, brute-force attack, correlation attack.
</p>
<a href="http://arxiv.org/abs/1709.08364" target="_blank">arXiv:1709.08364</a> [<a href="http://arxiv.org/pdf/1709.08364" target="_blank">pdf</a>]

<h2>Nonlinear Approximation via Compositions. (arXiv:1902.10170v5 [cs.LG] UPDATED)</h2>
<h3>Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>Given a function dictionary $\cal D$ and an approximation budget
$N\in\mathbb{N}^+$, nonlinear approximation seeks the linear combination of the
best $N$ terms $\{T_n\}_{1\le n\le N}\subseteq{\cal D}$ to approximate a given
function $f$ with the minimum approximation
error\[\varepsilon_{L,f}:=\min_{\{g_n\}\subseteq{\mathbb{R}},\{T_n\}\subseteq{\cal
D}}\|f(x)-\sum_{n=1}^N g_n T_n(x)\|.\]Motivated by recent success of deep
learning, we propose dictionaries with functions in a form of compositions,
i.e.,\[T(x)=T^{(L)}\circ T^{(L-1)}\circ\cdots\circ T^{(1)}(x)\]for all
$T\in\cal D$, and implement $T$ using ReLU feed-forward neural networks (FNNs)
with $L$ hidden layers. We further quantify the improvement of the best
$N$-term approximation rate in terms of $N$ when $L$ is increased from $1$ to
$2$ or $3$ to show the power of compositions. In the case when $L&gt;3$, our
analysis shows that increasing $L$ cannot improve the approximation rate in
terms of $N$.

In particular, for any function $f$ on $[0,1]$, regardless of its smoothness
and even the continuity, if $f$ can be approximated using a dictionary when
$L=1$ with the best $N$-term approximation rate $\varepsilon_{L,f}={\cal
O}(N^{-\eta})$, we show that dictionaries with $L=2$ can improve the best
$N$-term approximation rate to $\varepsilon_{L,f}={\cal O}(N^{-2\eta})$. We
also show that for H\"older continuous functions of order $\alpha$ on
$[0,1]^d$, the application of a dictionary with $L=3$ in nonlinear
approximation can achieve an essentially tight best $N$-term approximation rate
$\varepsilon_{L,f}={\cal O}(N^{-2\alpha/d})$. Finally, we show that
dictionaries consisting of wide FNNs with a few hidden layers are more
attractive in terms of computational efficiency than dictionaries with narrow
and very deep FNNs for approximating H\"older continuous functions if the
number of computer cores is larger than $N$ in parallel computing.
</p>
<a href="http://arxiv.org/abs/1902.10170" target="_blank">arXiv:1902.10170</a> [<a href="http://arxiv.org/pdf/1902.10170" target="_blank">pdf</a>]

<h2>Unsupervised Data Augmentation for Consistency Training. (arXiv:1904.12848v6 [cs.LG] UPDATED)</h2>
<h3>Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le</h3>
<p>Semi-supervised learning lately has shown much promise in improving deep
learning models when labeled data is scarce. Common among recent approaches is
the use of consistency training on a large amount of unlabeled data to
constrain model predictions to be invariant to input noise. In this work, we
present a new perspective on how to effectively noise unlabeled examples and
argue that the quality of noising, specifically those produced by advanced data
augmentation methods, plays a crucial role in semi-supervised learning. By
substituting simple noising operations with advanced data augmentation methods
such as RandAugment and back-translation, our method brings substantial
improvements across six language and three vision tasks under the same
consistency training framework. On the IMDb text classification dataset, with
only 20 labeled examples, our method achieves an error rate of 4.20,
outperforming the state-of-the-art model trained on 25,000 labeled examples. On
a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms
all previous approaches and achieves an error rate of 5.43 with only 250
examples. Our method also combines well with transfer learning, e.g., when
finetuning from BERT, and yields improvements in high-data regime, such as
ImageNet, whether when there is only 10% labeled data or when a full labeled
set with 1.3M extra unlabeled examples is used. Code is available at
https://github.com/google-research/uda.
</p>
<a href="http://arxiv.org/abs/1904.12848" target="_blank">arXiv:1904.12848</a> [<a href="http://arxiv.org/pdf/1904.12848" target="_blank">pdf</a>]

<h2>Interpretable and Personalized Apprenticeship Scheduling: Learning Interpretable Scheduling Policies from Heterogeneous User Demonstrations. (arXiv:1906.06397v4 [cs.LG] UPDATED)</h2>
<h3>Rohan Paleja, Andrew Silva, Letian Chen, Matthew Gombolay</h3>
<p>Resource scheduling and coordination is an NP-hard optimization requiring an
efficient allocation of agents to a set of tasks with upper- and lower bound
temporal and resource constraints. Due to the large-scale and dynamic nature of
resource coordination in hospitals and factories, human domain experts manually
plan and adjust schedules on the fly. To perform this job, domain experts
leverage heterogeneous strategies and rules-of-thumb honed over years of
apprenticeship. What is critically needed is the ability to extract this domain
knowledge in a heterogeneous and interpretable apprenticeship learning
framework to scale beyond the power of a single human expert, a necessity in
safety-critical domains. We propose a personalized and interpretable
apprenticeship scheduling algorithm that infers an interpretable representation
of all human task demonstrators by extracting decision-making criteria via an
inferred, personalized embedding non-parametric in the number of demonstrator
types. We achieve near-perfect LfD accuracy in synthetic domains and 88.22\%
accuracy on a planning domain with real-world, outperforming baselines.
Finally, our user study showed our methodology produces more interpretable and
easier-to-use models than neural networks ($p &lt; 0.05$).
</p>
<a href="http://arxiv.org/abs/1906.06397" target="_blank">arXiv:1906.06397</a> [<a href="http://arxiv.org/pdf/1906.06397" target="_blank">pdf</a>]

<h2>Improving CNN-based Planar Object Detection with Geometric Prior Knowledge. (arXiv:1909.10245v2 [cs.RO] UPDATED)</h2>
<h3>Jianxiong Cai, Jiawei Hou, Yiren Lu, Hongyu Chen, Laurent Kneip, S&#xf6;ren Schwertfeger</h3>
<p>In this paper, we focus on the question: how might mobile robots take
advantage of affordable RGB-D sensors for object detection? Although current
CNN-based object detectors have achieved impressive results, there are three
main drawbacks for practical usage on mobile robots: 1) It is hard and
time-consuming to collect and annotate large-scale training sets. 2) It usually
needs a long training time. 3) CNN-based object detection shows significant
weakness in predicting location. We propose an improved method for the
detection of planar objects, which rectifies images with geometric information
to compensate for the perspective distortion before feeding it to the CNN
detector module, typically a CNN-based detector like YOLO or MASK RCNN. By
dealing with the perspective distortion in advance, we eliminate the need for
the CNN detector to learn that. Experiments show that this approach
significantly boosts the detection performance. Besides, it effectively reduces
the number of training images required. In addition to the novel detection
framework proposed, we also release an RGBD dataset and source code for hazmat
sign detection. To the best of our knowledge, this is the first work of image
rectification for CNN-based object detection, and the dataset is the first
public available hazmat sign detection dataset with RGB-D sensors.
</p>
<a href="http://arxiv.org/abs/1909.10245" target="_blank">arXiv:1909.10245</a> [<a href="http://arxiv.org/pdf/1909.10245" target="_blank">pdf</a>]

<h2>Distributed Learning of Deep Neural Networks using Independent Subnet Training. (arXiv:1910.02120v5 [cs.LG] UPDATED)</h2>
<h3>Binhang Yuan, Cameron R. Wolfe, Chen Dun, Yuxin Tang, Anastasios Kyrillidis, Christopher M. Jermaine</h3>
<p>Distributed machine learning (ML) can bring more computational resources to
bear than single-machine learning, reducing training time. Further,
distribution allows models to be partitioned over many machines, allowing very
large models to be trained -- models that may be much larger than the available
memory of any individual machine. However, in practice, distributed ML remains
challenging, primarily due to high communication costs. We propose a new
approach to distributed neural network learning, called independent subnet
training (IST). In IST, a neural network is decomposed into a set of
subnetworks of the same depth as the original network, each of which is trained
locally, before the various subnets are exchanged and the process is repeated.
IST training has many advantages over standard data parallel approaches.
Because the subsets are independent, communication frequency is reduced.
Because the original network is decomposed into independent parts,
communication volume is reduced. Further, the decomposition makes IST naturally
model parallel, and so IST scales to very large models that cannot fit on any
single machine. We show experimentally that IST results in training time that
are much lower than data parallel approaches to distributed learning, and that
it scales to large models that cannot be learned using standard approaches.
</p>
<a href="http://arxiv.org/abs/1910.02120" target="_blank">arXiv:1910.02120</a> [<a href="http://arxiv.org/pdf/1910.02120" target="_blank">pdf</a>]

<h2>Multiplierless and Sparse Machine Learning based on Margin Propagation Networks. (arXiv:1910.02304v2 [cs.LG] UPDATED)</h2>
<h3>Nazreen P.M., Shantanu Chakrabartty, Chetan Singh Thakur</h3>
<p>The new generation of machine learning processors have evolved from
multi-core and parallel architectures that were designed to efficiently
implement matrix-vector-multiplications (MVMs). This is because at the
fundamental level, neural network and machine learning operations extensively
use MVM operations and hardware compilers exploit the inherent parallelism in
MVM operations to achieve hardware acceleration on GPUs and FPGAs. However,
many IoT and edge computing platforms require embedded ML devices close to the
network in order to compensate for communication cost and latency. Hence a
natural question to ask is whether MVM operations are even necessary to
implement ML algorithms and whether simpler hardware primitives can be used to
implement an ultra-energy-efficient ML processor/architecture. In this paper we
propose an alternate hardware-software codesign of ML and neural network
architectures where instead of using MVM operations and non-linear activation
functions, the architecture only uses simple addition and thresholding
operations to implement inference and learning. At the core of the proposed
approach is margin-propagation (MP) based computation that maps multiplications
into additions and additions into a dynamic rectifying-linear-unit (ReLU)
operations. This mapping results in significant improvement in computational
and hence energy cost. In this paper, we show how the MP network formulation
can be applied for designing linear classifiers, shallow multi-layer
perceptrons and support vector networks suitable fot IoT platforms and tiny ML
applications. We show that these MP based classifiers give comparable results
to that of their traditional counterparts for benchmark UCI datasets, with the
added advantage of reduction in computational complexity enabling an
improvement in energy efficiency.
</p>
<a href="http://arxiv.org/abs/1910.02304" target="_blank">arXiv:1910.02304</a> [<a href="http://arxiv.org/pdf/1910.02304" target="_blank">pdf</a>]

<h2>GraphAIR: Graph Representation Learning with Neighborhood Aggregation and Interaction. (arXiv:1911.01731v3 [cs.LG] UPDATED)</h2>
<h3>Fenyu Hu, Yanqiao Zhu, Shu Wu, Weiran Huang, Liang Wang, Tieniu Tan</h3>
<p>Graph representation learning is of paramount importance for a variety of
graph analytical tasks, ranging from node classification to community
detection. Recently, graph convolutional networks (GCNs) have been successfully
applied for graph representation learning. These GCNs generate node
representation by aggregating features from the neighborhoods, which follows
the "neighborhood aggregation" scheme. In spite of having achieved promising
performance on various tasks, existing GCN-based models have difficulty in well
capturing complicated non-linearity of graph data. In this paper, we first
theoretically prove that coefficients of the neighborhood interacting terms are
relatively small in current models, which explains why GCNs barely outperforms
linear models. Then, in order to better capture the complicated non-linearity
of graph data, we present a novel GraphAIR framework which models the
neighborhood interaction in addition to neighborhood aggregation. Comprehensive
experiments conducted on benchmark tasks including node classification and link
prediction using public datasets demonstrate the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/1911.01731" target="_blank">arXiv:1911.01731</a> [<a href="http://arxiv.org/pdf/1911.01731" target="_blank">pdf</a>]

<h2>Ground Metric Learning on Graphs. (arXiv:1911.03117v3 [stat.ML] UPDATED)</h2>
<h3>Matthieu Heitz, Nicolas Bonneel, David Coeurjolly, Marco Cuturi, Gabriel Peyr&#xe9;</h3>
<p>Optimal transport (OT) distances between probability distributions are
parameterized by the ground metric they use between observations. Their
relevance for real-life applications strongly hinges on whether that ground
metric parameter is suitably chosen. Selecting it adaptively and
algorithmically from prior knowledge, the so-called ground metric learning GML)
problem, has therefore appeared in various settings. We consider it in this
paper when the learned metric is constrained to be a geodesic distance on a
graph that supports the measures of interest. This imposes a rich structure for
candidate metrics, but also enables far more efficient learning procedures when
compared to a direct optimization over the space of all metric matrices. We use
this setting to tackle an inverse problem stemming from the observation of a
density evolving with time: we seek a graph ground metric such that the OT
interpolation between the starting and ending densities that result from that
ground metric agrees with the observed evolution. This OT dynamic framework is
relevant to model natural phenomena exhibiting displacements of mass, such as
for instance the evolution of the color palette induced by the modification of
lighting and materials.
</p>
<a href="http://arxiv.org/abs/1911.03117" target="_blank">arXiv:1911.03117</a> [<a href="http://arxiv.org/pdf/1911.03117" target="_blank">pdf</a>]

<h2>Quantile Propagation for Wasserstein-Approximate Gaussian Processes. (arXiv:1912.10200v3 [cs.LG] UPDATED)</h2>
<h3>Rui Zhang, Christian J. Walder, Edwin V. Bonilla, Marian-Andrei Rizoiu, Lexing Xie</h3>
<p>Approximate inference techniques are the cornerstone of probabilistic methods
based on Gaussian process priors. Despite this, most work approximately
optimizes standard divergence measures such as the Kullback-Leibler (KL)
divergence, which lack the basic desiderata for the task at hand, while chiefly
offering merely technical convenience. We develop a new approximate inference
method for Gaussian process models which overcomes the technical challenges
arising from abandoning these convenient divergences. Our method---dubbed
Quantile Propagation (QP)---is similar to expectation propagation (EP) but
minimizes the $L_2$ Wasserstein distance (WD) instead of the KL divergence. The
WD exhibits all the required properties of a distance metric, while respecting
the geometry of the underlying sample space. We show that QP matches quantile
functions rather than moments as in EP and has the same mean update but a
smaller variance update than EP, thereby alleviating EP's tendency to
over-estimate posterior variances. Crucially, despite the significant
complexity of dealing with the WD, QP has the same favorable locality property
as EP, and thereby admits an efficient algorithm. Experiments on classification
and Poisson regression show that QP outperforms both EP and variational Bayes.
</p>
<a href="http://arxiv.org/abs/1912.10200" target="_blank">arXiv:1912.10200</a> [<a href="http://arxiv.org/pdf/1912.10200" target="_blank">pdf</a>]

<h2>Minimax Value Interval for Off-Policy Evaluation and Policy Optimization. (arXiv:2002.02081v6 [cs.LG] UPDATED)</h2>
<h3>Nan Jiang, Jiawei Huang</h3>
<p>We study minimax methods for off-policy evaluation (OPE) using value
functions and marginalized importance weights. Despite that they hold promises
of overcoming the exponential variance in traditional importance sampling,
several key problems remain:

(1) They require function approximation and are generally biased. For the
sake of trustworthy OPE, is there anyway to quantify the biases?

(2) They are split into two styles ("weight-learning" vs "value-learning").
Can we unify them?

In this paper we answer both questions positively. By slightly altering the
derivation of previous methods (one from each style; Uehara et al., 2020), we
unify them into a single value interval that comes with a special type of
double robustness: when either the value-function or the importance-weight
class is well specified, the interval is valid and its length quantifies the
misspecification of the other class. Our interval also provides a unified view
of and new insights to some recent methods, and we further explore the
implications of our results on exploration and exploitation in off-policy
policy optimization with insufficient data coverage.
</p>
<a href="http://arxiv.org/abs/2002.02081" target="_blank">arXiv:2002.02081</a> [<a href="http://arxiv.org/pdf/2002.02081" target="_blank">pdf</a>]

<h2>Post-Comparison Mitigation of Demographic Bias in Face Recognition Using Fair Score Normalization. (arXiv:2002.03592v3 [cs.CV] UPDATED)</h2>
<h3>Philipp Terh&#xf6;rst, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper</h3>
<p>Current face recognition systems achieve high progress on several benchmark
tests. Despite this progress, recent works showed that these systems are
strongly biased against demographic sub-groups. Consequently, an easily
integrable solution is needed to reduce the discriminatory effect of these
biased systems. Previous work mainly focused on learning less biased face
representations, which comes at the cost of a strongly degraded overall
recognition performance. In this work, we propose a novel unsupervised fair
score normalization approach that is specifically designed to reduce the effect
of bias in face recognition and subsequently lead to a significant overall
performance boost. Our hypothesis is built on the notation of individual
fairness by designing a normalization approach that leads to treating similar
individuals similarly. Experiments were conducted on three publicly available
datasets captured under controlled and in-the-wild circumstances. Results
demonstrate that our solution reduces demographic biases, e.g. by up to 82.7%
in the case when gender is considered. Moreover, it mitigates the bias more
consistently than existing works. In contrast to previous works, our fair
normalization approach enhances the overall performance by up to 53.2% at false
match rate of 0.001 and up to 82.9% at a false match rate of 0.00001.
Additionally, it is easily integrable into existing recognition systems and not
limited to face biometrics.
</p>
<a href="http://arxiv.org/abs/2002.03592" target="_blank">arXiv:2002.03592</a> [<a href="http://arxiv.org/pdf/2002.03592" target="_blank">pdf</a>]

<h2>Instant recovery of shape from spectrum via latent space connections. (arXiv:2003.06523v4 [cs.CV] UPDATED)</h2>
<h3>Riccardo Marin, Arianna Rampini, Umberto Castellani, Emanuele Rodol&#xe0;, Maks Ovsjanikov, Simone Melzi</h3>
<p>We introduce the first learning-based method for recovering shapes from
Laplacian spectra. Given an auto-encoder, our model takes the form of a
cycle-consistent module to map latent vectors to sequences of eigenvalues. This
module provides an efficient and effective linkage between spectrum and
geometry of a given shape. Our data-driven approach replaces the need for
ad-hoc regularizers required by prior methods, while providing more accurate
results at a fraction of the computational cost. Our learning model applies
without modifications across different dimensions (2D and 3D shapes alike),
representations (meshes, contours and point clouds), as well as across
different shape classes, and admits arbitrary resolution of the input spectrum
without affecting complexity. The increased flexibility allows us to provide a
proxy to differentiable eigendecomposition and to address notoriously difficult
tasks in 3D vision and geometry processing within a unified framework,
including shape generation from spectrum, mesh super-resolution, shape
exploration, style transfer, spectrum estimation from point clouds,
segmentation transfer and point-to-point matching.
</p>
<a href="http://arxiv.org/abs/2003.06523" target="_blank">arXiv:2003.06523</a> [<a href="http://arxiv.org/pdf/2003.06523" target="_blank">pdf</a>]

<h2>Inferring the Material Properties of Granular Media for Robotic Tasks. (arXiv:2003.08032v4 [cs.RO] UPDATED)</h2>
<h3>Carolyn Matl, Yashraj Narang, Ruzena Bajcsy, Fabio Ramos, Dieter Fox</h3>
<p>Granular media (e.g., cereal grains, plastic resin pellets, and pills) are
ubiquitous in robotics-integrated industries, such as agriculture,
manufacturing, and pharmaceutical development. This prevalence mandates the
accurate and efficient simulation of these materials. This work presents a
software and hardware framework that automatically calibrates a fast physics
simulator to accurately simulate granular materials by inferring material
properties from real-world depth images of granular formations (i.e., piles and
rings). Specifically, coefficients of sliding friction, rolling friction, and
restitution of grains are estimated from summary statistics of grain formations
using likelihood-free Bayesian inference. The calibrated simulator accurately
predicts unseen granular formations in both simulation and experiment;
furthermore, simulator predictions are shown to generalize to more complex
tasks, including using a robot to pour grains into a bowl, as well as to create
a desired pattern of piles and rings. Visualizations of the framework and
experiments can be viewed at https://youtu.be/OBvV5h2NMKA
</p>
<a href="http://arxiv.org/abs/2003.08032" target="_blank">arXiv:2003.08032</a> [<a href="http://arxiv.org/pdf/2003.08032" target="_blank">pdf</a>]

<h2>Accelerating Deep Reinforcement Learning With the Aid of Partial Model: Energy-Efficient Predictive Video Streaming. (arXiv:2003.09708v2 [cs.LG] UPDATED)</h2>
<h3>Dong Liu, Jianyu Zhao, Chenyang Yang, Lajos Hanzo</h3>
<p>Predictive power allocation is conceived for energy-efficient video streaming
over mobile networks using deep reinforcement learning. The goal is to minimize
the accumulated energy consumption of each base station over a complete video
streaming session under the constraint that avoids video playback
interruptions. To handle the continuous state and action spaces, we resort to
deep deterministic policy gradient (DDPG) algorithm for solving the formulated
problem. In contrast to previous predictive power allocation policies that
first predict future information with historical data and then optimize the
power allocation based on the predicted information, the proposed policy
operates in an on-line and end-to-end manner. By judiciously designing the
action and state that only depend on slowly-varying average channel gains, we
reduce the signaling overhead between the edge server and the base stations,
and make it easier to learn a good policy. To further avoid playback
interruption throughout the learning process and improve the convergence speed,
we exploit the partially known model of the system dynamics by integrating the
concepts of safety layer, post-decision state, and virtual experiences into the
basic DDPG algorithm. Our simulation results show that the proposed policies
converge to the optimal policy that is derived based on perfect large-scale
channel prediction and outperform the first-predict-then-optimize policy in the
presence of prediction errors. By harnessing the partially known model, the
convergence speed can be dramatically improved.
</p>
<a href="http://arxiv.org/abs/2003.09708" target="_blank">arXiv:2003.09708</a> [<a href="http://arxiv.org/pdf/2003.09708" target="_blank">pdf</a>]

<h2>Robust Single Rotation Averaging. (arXiv:2004.00732v4 [cs.CV] UPDATED)</h2>
<h3>Seong Hun Lee, Javier Civera</h3>
<p>We propose a novel method for single rotation averaging using the Weiszfeld
algorithm. Our contribution is threefold: First, we propose a robust
initialization based on the elementwise median of the input rotation matrices.
Our initial solution is more accurate and robust than the commonly used chordal
$L_2$-mean. Second, we propose an outlier rejection scheme that can be
incorporated in the Weiszfeld algorithm to improve the robustness of $L_1$
rotation averaging. Third, we propose a method for approximating the chordal
$L_1$-mean using the Weiszfeld algorithm. An extensive evaluation shows that
both our method and the state of the art perform equally well with the proposed
outlier rejection scheme, but ours is $2-4$ times faster.
</p>
<a href="http://arxiv.org/abs/2004.00732" target="_blank">arXiv:2004.00732</a> [<a href="http://arxiv.org/pdf/2004.00732" target="_blank">pdf</a>]

<h2>Reinforcement Learning with Augmented Data. (arXiv:2004.14990v5 [cs.LG] UPDATED)</h2>
<h3>Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas</h3>
<p>Learning from visual observations is a fundamental yet challenging problem in
Reinforcement Learning (RL). Although algorithmic advances combined with
convolutional neural networks have proved to be a recipe for success, current
methods are still lacking on two fronts: (a) data-efficiency of learning and
(b) generalization to new environments. To this end, we present Reinforcement
Learning with Augmented Data (RAD), a simple plug-and-play module that can
enhance most RL algorithms. We perform the first extensive study of general
data augmentations for RL on both pixel-based and state-based inputs, and
introduce two new data augmentations - random translate and random amplitude
scale. We show that augmentations such as random translate, crop, color jitter,
patch cutout, random convolutions, and amplitude scale can enable simple RL
algorithms to outperform complex state-of-the-art methods across common
benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and
final performance on the DeepMind Control Suite benchmark for pixel-based
control as well as OpenAI Gym benchmark for state-based control. We further
demonstrate that RAD significantly improves test-time generalization over
existing methods on several OpenAI ProcGen benchmarks. Our RAD module and
training code are available at https://www.github.com/MishaLaskin/rad.
</p>
<a href="http://arxiv.org/abs/2004.14990" target="_blank">arXiv:2004.14990</a> [<a href="http://arxiv.org/pdf/2004.14990" target="_blank">pdf</a>]

<h2>Neural Controlled Differential Equations for Irregular Time Series. (arXiv:2005.08926v2 [cs.LG] UPDATED)</h2>
<h3>Patrick Kidger, James Morrill, James Foster, Terry Lyons</h3>
<p>Neural ordinary differential equations are an attractive option for modelling
temporal dynamics. However, a fundamental issue is that the solution to an
ordinary differential equation is determined by its initial condition, and
there is no mechanism for adjusting the trajectory based on subsequent
observations. Here, we demonstrate how this may be resolved through the
well-understood mathematics of \emph{controlled differential equations}. The
resulting \emph{neural controlled differential equation} model is directly
applicable to the general setting of partially-observed irregularly-sampled
multivariate time series, and (unlike previous work on this problem) it may
utilise memory-efficient adjoint-based backpropagation even across
observations. We demonstrate that our model achieves state-of-the-art
performance against similar (ODE or RNN based) models in empirical studies on a
range of datasets. Finally we provide theoretical results demonstrating
universal approximation, and that our model subsumes alternative ODE models.
</p>
<a href="http://arxiv.org/abs/2005.08926" target="_blank">arXiv:2005.08926</a> [<a href="http://arxiv.org/pdf/2005.08926" target="_blank">pdf</a>]

<h2>Robotics Meets Cosmetic Dermatology: Development of a Novel Vision-Guided System for Skin Photo-Rejuvenation. (arXiv:2005.10462v3 [cs.RO] UPDATED)</h2>
<h3>Muhammad Muddassir, Domingo Gomez, Shujian Chen, Luyin Hu, David Navarro-Alarcon</h3>
<p>In this paper, we present a novel robotic system for skin photo-rejuvenation
procedures, which can uniformly deliver the laser's energy over the skin of the
face. The robotised procedure is performed by a manipulator whose end-effector
is instrumented with a depth sensor, a thermal camera, and a cosmetic laser
generator. To plan the heat stimulating trajectories for the laser, the system
computes the surface model of the face and segments it into seven regions that
are automatically filled with laser shots. We report experimental results with
human subjects to validate the performance of the system. To the best of the
author's knowledge, this is the first time that facial skin rejuvenation has
been automated by robot manipulators.
</p>
<a href="http://arxiv.org/abs/2005.10462" target="_blank">arXiv:2005.10462</a> [<a href="http://arxiv.org/pdf/2005.10462" target="_blank">pdf</a>]

<h2>Hyperparameter optimization with REINFORCE and Transformers. (arXiv:2006.00939v4 [cs.LG] UPDATED)</h2>
<h3>Chepuri Shri Krishna, Ashish Gupta, Swarnim Narayan, Himanshu Rai, Diksha Manchanda</h3>
<p>Reinforcement Learning has yielded promising results for Neural Architecture
Search (NAS). In this paper, we demonstrate how its performance can be improved
by using a simplified Transformer block to model the policy network. The
simplified Transformer uses a 2-stream attention-based mechanism to model
hyper-parameter dependencies while avoiding layer normalization and position
encoding. We posit that this parsimonious design balances model complexity
against expressiveness, making it suitable for discovering optimal
architectures in high-dimensional search spaces with limited exploration
budgets. We demonstrate how the algorithm's performance can be further improved
by a) using an actor-critic style algorithm instead of plain vanilla policy
gradient and b) ensembling Transformer blocks with shared parameters, each
block conditioned on a different auto-regressive factorization order. Our
algorithm works well as both a NAS and generic hyper-parameter optimization
(HPO) algorithm: it outperformed most algorithms on NAS-Bench-101, a public
data-set for benchmarking NAS algorithms. In particular, it outperformed RL
based methods that use alternate architectures to model the policy network,
underlining the value of using attention-based networks in this setting. As a
generic HPO algorithm, it outperformed Random Search in discovering more
accurate multi-layer perceptron model architectures across 2 regression tasks.
We have adhered to guidelines listed in Lindauer and Hutter while designing
experiments and reporting results.
</p>
<a href="http://arxiv.org/abs/2006.00939" target="_blank">arXiv:2006.00939</a> [<a href="http://arxiv.org/pdf/2006.00939" target="_blank">pdf</a>]

<h2>All your loss are belong to Bayes. (arXiv:2006.04633v2 [cs.LG] UPDATED)</h2>
<h3>Christian Walder, Richard Nock</h3>
<p>Loss functions are a cornerstone of machine learning and the starting point
of most algorithms. Statistics and Bayesian decision theory have contributed,
via properness, to elicit over the past decades a wide set of admissible losses
in supervised learning, to which most popular choices belong (logistic, square,
Matsushita, etc.). Rather than making a potentially biased ad hoc choice of the
loss, there has recently been a boost in efforts to fit the loss to the domain
at hand while training the model itself. The key approaches fit a canonical
link, a function which monotonically relates the closed unit interval to R and
can provide a proper loss via integration. In this paper, we rely on a broader
view of proper composite losses and a recent construct from information
geometry, source functions, whose fitting alleviates constraints faced by
canonical links. We introduce a trick on squared Gaussian Processes to obtain a
random process whose paths are compliant source functions with many desirable
properties in the context of link estimation. Experimental results demonstrate
substantial improvements over the state of the art.
</p>
<a href="http://arxiv.org/abs/2006.04633" target="_blank">arXiv:2006.04633</a> [<a href="http://arxiv.org/pdf/2006.04633" target="_blank">pdf</a>]

<h2>Rethinking Importance Weighting for Deep Learning under Distribution Shift. (arXiv:2006.04662v2 [cs.LG] UPDATED)</h2>
<h3>Tongtong Fang, Nan Lu, Gang Niu, Masashi Sugiyama</h3>
<p>Under distribution shift (DS) where the training data distribution differs
from the test one, a powerful technique is importance weighting (IW) which
handles DS in two separate steps: weight estimation (WE) estimates the
test-over-training density ratio and weighted classification (WC) trains the
classifier from weighted training data. However, IW cannot work well on complex
data, since WE is incompatible with deep learning. In this paper, we rethink IW
and theoretically show it suffers from a circular dependency: we need not only
WE for WC, but also WC for WE where a trained deep classifier is used as the
feature extractor (FE). To cut off the dependency, we try to pretrain FE from
unweighted training data, which leads to biased FE. To overcome the bias, we
propose an end-to-end solution dynamic IW that iterates between WE and WC and
combines them in a seamless manner, and hence our WE can also enjoy deep
networks and stochastic optimizers indirectly. Experiments with two
representative types of DS on three popular datasets show that our dynamic IW
compares favorably with state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2006.04662" target="_blank">arXiv:2006.04662</a> [<a href="http://arxiv.org/pdf/2006.04662" target="_blank">pdf</a>]

<h2>In Proximity of ReLU DNN, PWA Function, and Explicit MPC. (arXiv:2006.05001v2 [cs.LG] UPDATED)</h2>
<h3>Saman Fahandezh-Saadi, Masayoshi Tomizuka</h3>
<p>Rectifier (ReLU) deep neural networks (DNN) and their connection with
piecewise affine (PWA) functions is analyzed. The paper is an effort to find
and study the possibility of representing explicit state feedback policy of
model predictive control (MPC) as a ReLU DNN, and vice versa. The complexity
and architecture of DNN has been examined through some theorems and
discussions. An approximate method has been developed for identification of
input-space in ReLU net which results a PWA function over polyhedral regions.
Also, inverse multiparametric linear or quadratic programs (mp-LP or mp-QP) has
been studied which deals with reconstruction of constraints and cost function
given a PWA function.
</p>
<a href="http://arxiv.org/abs/2006.05001" target="_blank">arXiv:2006.05001</a> [<a href="http://arxiv.org/pdf/2006.05001" target="_blank">pdf</a>]

<h2>GAIT-prop: A biologically plausible learning rule derived from backpropagation of error. (arXiv:2006.06438v3 [cs.LG] UPDATED)</h2>
<h3>Nasir Ahmad, Marcel A. J. van Gerven, Luca Ambrogioni</h3>
<p>Traditional backpropagation of error, though a highly successful algorithm
for learning in artificial neural network models, includes features which are
biologically implausible for learning in real neural circuits. An alternative
called target propagation proposes to solve this implausibility by using a
top-down model of neural activity to convert an error at the output of a neural
network into layer-wise and plausible 'targets' for every unit. These targets
can then be used to produce weight updates for network training. However, thus
far, target propagation has been heuristically proposed without demonstrable
equivalence to backpropagation. Here, we derive an exact correspondence between
backpropagation and a modified form of target propagation (GAIT-prop) where the
target is a small perturbation of the forward pass. Specifically,
backpropagation and GAIT-prop give identical updates when synaptic weight
matrices are orthogonal. In a series of simple computer vision experiments, we
show near-identical performance between backpropagation and GAIT-prop with a
soft orthogonality-inducing regularizer.
</p>
<a href="http://arxiv.org/abs/2006.06438" target="_blank">arXiv:2006.06438</a> [<a href="http://arxiv.org/pdf/2006.06438" target="_blank">pdf</a>]

<h2>Stochastic Shortest Path with Adversarially Changing Costs. (arXiv:2006.11561v2 [cs.LG] UPDATED)</h2>
<h3>Aviv Rosenberg, Yishay Mansour</h3>
<p>Stochastic shortest path (SSP) is a well-known problem in planning and
control, in which an agent has to reach a goal state in minimum total expected
cost. In this paper we consider adversarial SSPs that also account for
adversarial changes in the costs over time, while the dynamics (i.e.,
transition function) remains unchanged. Formally, an agent interacts with an
SSP environment for $K$ episodes, the cost function changes arbitrarily between
episodes, and the fixed dynamics are unknown to the agent. We give high
probability regret bounds of $\widetilde O (\sqrt{K})$ assuming all costs are
strictly positive, and $\widetilde O (K^{3/4})$ for the general case. To the
best of our knowledge, we are the first to consider this natural setting of
adversarial SSP and obtain sub-linear regret for it.
</p>
<a href="http://arxiv.org/abs/2006.11561" target="_blank">arXiv:2006.11561</a> [<a href="http://arxiv.org/pdf/2006.11561" target="_blank">pdf</a>]

<h2>Artemis: tight convergence guarantees for bidirectional compression in Federated Learning. (arXiv:2006.14591v2 [cs.LG] UPDATED)</h2>
<h3>Constantin Philippenko, Aymeric Dieuleveut</h3>
<p>We introduce a new algorithm - Artemis - tackling the problem of learning in
a distributed framework with communication constraints. Several workers
(randomly sampled) perform the optimization process using a central server to
aggregate their computation. To alleviate the communication cost, Artemis
compresses the information sent in both directions (from the workers to the
server and conversely) combined with a memory mechanism. It improves on
existing quantized federated learning algorithms that only consider
unidirectional compression (to the server), or use very strong assumptions on
the compression operator, and often do not take into account devices partial
participation. We provide fast rates of convergence (linear up to a threshold)
under weak assumptions on the stochastic gradients (noise's variance bounded
only at optimal point) in non-i.i.d. setting, highlight the impact of memory
for unidirectional and bidirectional compression, analyze Polyak-Ruppert
averaging. We use convergence in distribution to obtain a lower bound of the
asymptotic variance that highlights practical limits of compression. And we
provide experimental results to demonstrate the validity of our analysis.
</p>
<a href="http://arxiv.org/abs/2006.14591" target="_blank">arXiv:2006.14591</a> [<a href="http://arxiv.org/pdf/2006.14591" target="_blank">pdf</a>]

<h2>CoPhy-PGNN: Learning Physics-guided Neural Networks withCompeting Loss Functions for Solving Eigenvalue Problems. (arXiv:2007.01420v4 [cs.LG] UPDATED)</h2>
<h3>Mohannad Elhamod, Jie Bu, Christopher Singh, Matthew Redell, Abantika Ghosh, Viktor Podolskiy, Wei-Cheng Lee, Anuj Karpatne</h3>
<p>Physics-guided Neural Networks (PGNNs) represent an emerging class of neural
networks that are trained using physics-guided (PG) loss functions (capturing
violations in network outputs with known physics), along with the supervision
contained in data. Existing work in PGNNs have demonstrated the efficacy of
adding single PG loss functions in the neural network objectives, using
constant trade-off parameters, to ensure better generalizability. However, in
the presence of multiple physics loss functions with competing gradient
directions, there is a need to adaptively tune the contribution of competing PG
loss functions during the course of training to arrive at generalizable
solutions. We demonstrate the presence of competing PG losses in the generic
neural network problem of solving for the lowest (or highest) eigenvector of a
physics-based eigenvalue equation, common to many scientific problems. We
present a novel approach to handle competing PG losses and demonstrate its
efficacy in learning generalizable solutions in two motivating applications of
quantum mechanics and electromagnetic propagation. All the code and data used
in this work is available at https://github.com/jayroxis/Cophy-PGNN.
</p>
<a href="http://arxiv.org/abs/2007.01420" target="_blank">arXiv:2007.01420</a> [<a href="http://arxiv.org/pdf/2007.01420" target="_blank">pdf</a>]

<h2>Auxiliary Tasks Speed Up Learning PointGoal Navigation. (arXiv:2007.04561v2 [cs.CV] UPDATED)</h2>
<h3>Joel Ye, Dhruv Batra, Erik Wijmans, Abhishek Das</h3>
<p>PointGoal Navigation is an embodied task that requires agents to navigate to
a specified point in an unseen environment. Wijmans et al. showed that this
task is solvable but their method is computationally prohibitive, requiring 2.5
billion frames and 180 GPU-days. In this work, we develop a method to
significantly increase sample and time efficiency in learning PointNav using
self-supervised auxiliary tasks (e.g. predicting the action taken between two
egocentric observations, predicting the distance between two observations from
a trajectory,etc.).We find that naively combining multiple auxiliary tasks
improves sample efficiency,but only provides marginal gains beyond a point. To
overcome this, we use attention to combine representations learnt from
individual auxiliary tasks. Our best agent is 5.5x faster to reach the
performance of the previous state-of-the-art, DD-PPO, at 40M frames, and
improves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is
publicly available at https://github.com/joel99/habitat-pointnav-aux.
</p>
<a href="http://arxiv.org/abs/2007.04561" target="_blank">arXiv:2007.04561</a> [<a href="http://arxiv.org/pdf/2007.04561" target="_blank">pdf</a>]

<h2>DeepHazard: neural network for time-varying risks. (arXiv:2007.13218v2 [stat.ML] UPDATED)</h2>
<h3>Denise Rava, Jelena Bradic</h3>
<p>Prognostic models in survival analysis are aimed at understanding the
relationship between patients' covariates and the distribution of survival
time. Traditionally, semi-parametric models, such as the Cox model, have been
assumed. These often rely on strong proportionality assumptions of the hazard
that might be violated in practice. Moreover, they do not often include
covariate information updated over time. We propose a new flexible method for
survival prediction: DeepHazard, a neural network for time-varying risks. Our
approach is tailored for a wide range of continuous hazards forms, with the
only restriction of being additive in time. A flexible implementation, allowing
different optimization methods, along with any norm penalty, is developed.
Numerical examples illustrate that our approach outperforms existing
state-of-the-art methodology in terms of predictive capability evaluated
through the C-index metric. The same is revealed on the popular real datasets
as METABRIC, GBSG, and ACTG.
</p>
<a href="http://arxiv.org/abs/2007.13218" target="_blank">arXiv:2007.13218</a> [<a href="http://arxiv.org/pdf/2007.13218" target="_blank">pdf</a>]

<h2>FedML: A Research Library and Benchmark for Federated Machine Learning. (arXiv:2007.13518v3 [cs.LG] UPDATED)</h2>
<h3>Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, Salman Avestimehr</h3>
<p>Federated learning (FL) is a rapidly growing research field in machine
learning. However, existing FL libraries cannot adequately support diverse
algorithmic development; inconsistent dataset and model usage make fair
algorithm comparison challenging. In this work, we introduce FedML, an open
research library and benchmark to facilitate FL algorithm development and fair
performance comparison. FedML supports three computing paradigms: on-device
training for edge devices, distributed computing, and single-machine
simulation. FedML also promotes diverse algorithmic research with flexible and
generic API design and comprehensive reference baseline implementations
(optimizer, models, and datasets). We hope FedML could provide an efficient and
reproducible means for developing and evaluating FL algorithms that would
benefit the FL research community. We maintain the source code, documents, and
user community at https://fedml.ai.
</p>
<a href="http://arxiv.org/abs/2007.13518" target="_blank">arXiv:2007.13518</a> [<a href="http://arxiv.org/pdf/2007.13518" target="_blank">pdf</a>]

<h2>Oblique Predictive Clustering Trees. (arXiv:2007.13617v2 [cs.LG] UPDATED)</h2>
<h3>Toma&#x17e; Stepi&#x161;nik, Dragi Kocev</h3>
<p>Predictive clustering trees (PCTs) are a well established generalization of
standard decision trees, which can be used to solve a variety of predictive
modeling tasks, including structured output prediction. Combining them into
ensembles yields state-of-the-art performance. Furthermore, the ensembles of
PCTs can be interpreted by calculating feature importance scores from the
learned models. However, their learning time scales poorly with the
dimensionality of the output space. This is often problematic, especially in
(hierarchical) multi-label classification, where the output can consist of
hundreds of potential labels. Also, learning of PCTs can not exploit the
sparsity of data to improve the computational efficiency, which is common in
both input (molecular fingerprints, bag of words representations) and output
spaces (in multi-label classification, examples are often labeled with only a
fraction of possible labels). In this paper, we propose oblique predictive
clustering trees, capable of addressing these limitations. We design and
implement two methods for learning oblique splits that contain linear
combinations of features in the tests, hence a split corresponds to an
arbitrary hyperplane in the input space. The methods are efficient for high
dimensional data and capable of exploiting sparse data. We experimentally
evaluate the proposed methods on 60 benchmark datasets for 6 predictive
modeling tasks. The results of the experiments show that oblique predictive
clustering trees achieve performance on-par with state-of-the-art methods and
are orders of magnitude faster than standard PCTs. We also show that meaningful
feature importance scores can be extracted from the models learned with the
proposed methods.
</p>
<a href="http://arxiv.org/abs/2007.13617" target="_blank">arXiv:2007.13617</a> [<a href="http://arxiv.org/pdf/2007.13617" target="_blank">pdf</a>]

<h2>Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. (arXiv:2007.14513v4 [cs.LG] UPDATED)</h2>
<h3>Chaoyang He, Murali Annavaram, Salman Avestimehr</h3>
<p>Scaling up the convolutional neural network (CNN) size (e.g., width, depth,
etc.) is known to effectively improve model accuracy. However, the large model
size impedes training on resource-constrained edge devices. For instance,
federated learning (FL) may place undue burden on the compute capability of
edge nodes, even though there is a strong practical need for FL due to its
privacy and confidentiality properties. To address the resource-constrained
reality of edge devices, we reformulate FL as a group knowledge transfer
training algorithm, called FedGKT. FedGKT designs a variant of the alternating
minimization approach to train small CNNs on edge nodes and periodically
transfer their knowledge by knowledge distillation to a large server-side CNN.
FedGKT consolidates several advantages into a single framework: reduced demand
for edge computation, lower communication bandwidth for large CNNs, and
asynchronous training, all while maintaining model accuracy comparable to
FedAvg. We train CNNs designed based on ResNet-56 and ResNet-110 using three
distinct datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D.
variants. Our results show that FedGKT can obtain comparable or even slightly
higher accuracy than FedAvg. More importantly, FedGKT makes edge training
affordable. Compared to the edge training using FedAvg, FedGKT demands 9 to 17
times less computational power (FLOPs) on edge devices and requires 54 to 105
times fewer parameters in the edge CNN. Our source code is released at FedML
(https://fedml.ai).
</p>
<a href="http://arxiv.org/abs/2007.14513" target="_blank">arXiv:2007.14513</a> [<a href="http://arxiv.org/pdf/2007.14513" target="_blank">pdf</a>]

<h2>An Imitation from Observation Approach to Transfer Learning with Dynamics Mismatch. (arXiv:2008.01594v2 [cs.AI] UPDATED)</h2>
<h3>Siddarth Desai, Ishan Durugkar, Haresh Karnan, Garrett Warnell, Josiah Hanna, Peter Stone</h3>
<p>We examine the problem of transferring a policy learned in a source
environment to a target environment with different dynamics, particularly in
the case where it is critical to reduce the amount of interaction with the
target environment during learning. This problem is particularly important in
sim-to-real transfer because simulators inevitably model real-world dynamics
imperfectly. In this paper, we show that one existing solution to this transfer
problem - grounded action transformation - is closely related to the problem of
imitation from observation (IfO): learning behaviors that mimic the
observations of behavior demonstrations. After establishing this relationship,
we hypothesize that recent state-of-the-art approaches from the IfO literature
can be effectively repurposed for grounded transfer learning.To validate our
hypothesis we derive a new algorithm - generative adversarial reinforced action
transformation (GARAT) - based on adversarial imitation from observation
techniques. We run experiments in several domains with mismatched dynamics, and
find that agents trained with GARAT achieve higher returns in the target
environment compared to existing black-box transfer methods
</p>
<a href="http://arxiv.org/abs/2008.01594" target="_blank">arXiv:2008.01594</a> [<a href="http://arxiv.org/pdf/2008.01594" target="_blank">pdf</a>]

<h2>R-MNet: A Perceptual Adversarial Network for Image Inpainting. (arXiv:2008.04621v2 [cs.CV] UPDATED)</h2>
<h3>Jireh Jam, Connah Kendrick, Vincent Drouard, Kevin Walker, Gee-Sern Hsu, Moi Hoon Yap</h3>
<p>Facial image inpainting is a problem that is widely studied, and in recent
years the introduction of Generative Adversarial Networks, has led to
improvements in the field. Unfortunately some issues persists, in particular
when blending the missing pixels with the visible ones. We address the problem
by proposing a Wasserstein GAN combined with a new reverse mask operator,
namely Reverse Masking Network (R-MNet), a perceptual adversarial network for
image inpainting. The reverse mask operator transfers the reverse masked image
to the end of the encoder-decoder network leaving only valid pixels to be
inpainted. Additionally, we propose a new loss function computed in feature
space to target only valid pixels combined with adversarial training. These
then capture data distributions and generate images similar to those in the
training data with achieved realism (realistic and coherent) on the output
images. We evaluate our method on publicly available dataset, and compare with
state-of-the-art methods. We show that our method is able to generalize to
high-resolution inpainting task, and further show more realistic outputs that
are plausible to the human visual system when compared with the
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2008.04621" target="_blank">arXiv:2008.04621</a> [<a href="http://arxiv.org/pdf/2008.04621" target="_blank">pdf</a>]

<h2>Metrics for Exposing the Biases of Content-Style Disentanglement. (arXiv:2008.12378v3 [cs.CV] UPDATED)</h2>
<h3>Xiao Liu, Spyridon Thermos, Gabriele Valvano, Agisilaos Chartsias, Alison O&#x27;Neil, Sotirios A. Tsaftaris</h3>
<p>A recent spate of state-of-the-art semi- and unsupervised solutions for
challenging computer vision tasks encode image "content" into a spatial tensor
and image appearance or "style" into a vector. Most of these solutions use the
term disentangled for their representations and employ different "biases" such
as model design, learning objectives, and data, to achieve good performance in
spatially equivariant tasks (e.g. image-to-image translation). While
considerable effort has been made to measure disentanglement in vector
representations, we have lacked metrics for spatial content and vector style
representations. In this paper, we propose such metrics to characterize the
degree of disentanglement in terms of how (un)correlated and informative the
content and style representations are, and we further examine its relation to
task performance. In particular, we first identify key design choices and
learning constraints on three popular models that employ content-style
disentanglement and derive ablated versions. Secondly, we use our metrics to
ascertain the role of each bias. Our experiments reveal a "sweet spot" between
disentanglement, task performance and latent space interpretability. Our
metrics are not task-dependent; thus, they can help guide either the design of
new future models or the selection of viable models such that this ideal "sweet
spot" is achieved in any task where content-style representations are useful.
</p>
<a href="http://arxiv.org/abs/2008.12378" target="_blank">arXiv:2008.12378</a> [<a href="http://arxiv.org/pdf/2008.12378" target="_blank">pdf</a>]

<h2>SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v3 [cs.CV] UPDATED)</h2>
<h3>Alexander Wang, Mengye Ren, Richard Zemel</h3>
<p>Sketch drawings are an intuitive visual domain that appeals to human
instinct. Previous work has shown that recurrent neural networks are capable of
producing sketch drawings of a single or few classes at a time. In this work we
investigate representations developed by training a generative model to produce
sketches from pixel images across many classes in a sketch domain. We find that
the embeddings learned by this sketching model are informative for visual tasks
and capture some forms of visual understanding. We then use them to exceed
state-of-the-art performance in unsupervised few-shot classification on the
Omniglot and mini-ImageNet benchmarks. We also leverage the generative capacity
of our model to produce high quality sketches of novel classes based on just a
single example.
</p>
<a href="http://arxiv.org/abs/2009.04806" target="_blank">arXiv:2009.04806</a> [<a href="http://arxiv.org/pdf/2009.04806" target="_blank">pdf</a>]

<h2>Synbols: Probing Learning Algorithms with Synthetic Datasets. (arXiv:2009.06415v2 [cs.CV] UPDATED)</h2>
<h3>Alexandre Lacoste, Pau Rodr&#xed;guez, Fr&#xe9;d&#xe9;ric Branchaud-Charron, Parmida Atighehchian, Massimo Caccia, Issam Laradji, Alexandre Drouin, Matt Craddock, Laurent Charlin, David V&#xe1;zquez</h3>
<p>Progress in the field of machine learning has been fueled by the introduction
of benchmark datasets pushing the limits of existing algorithms. Enabling the
design of datasets to test specific properties and failure modes of learning
algorithms is thus a problem of high interest, as it has a direct impact on
innovation in the field. In this sense, we introduce Synbols -- Synthetic
Symbols -- a tool for rapidly generating new datasets with a rich composition
of latent features rendered in low resolution images. Synbols leverages the
large amount of symbols available in the Unicode standard and the wide range of
artistic font provided by the open font community. Our tool's high-level
interface provides a language for rapidly generating new distributions on the
latent features, including various types of textures and occlusions. To
showcase the versatility of Synbols, we use it to dissect the limitations and
flaws in standard learning algorithms in various learning setups including
supervised learning, active learning, out of distribution generalization,
unsupervised representation learning, and object counting.
</p>
<a href="http://arxiv.org/abs/2009.06415" target="_blank">arXiv:2009.06415</a> [<a href="http://arxiv.org/pdf/2009.06415" target="_blank">pdf</a>]

<h2>Weighted Model Counting in FO2 with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula. (arXiv:2009.12237v5 [cs.AI] UPDATED)</h2>
<h3>Sagar Malhotra, Luciano Serafini</h3>
<p>Weighted First Order Model Counting (WFOMC) computes the weighted sum of the
models of a first order theory on a domain of a given finite size. WFOMC has
emerged as a fundamental tool for probabilistic inference. Algorithms for WFOMC
that run in polynomial time w.r.t. the domain size are called lifted inference
algorithms. Such algorithms have been developed for multiple extensions of
FO$^2$(the fragment of First Order Logic with two variables) for the special
case of symmetric weight functions. In this paper, instead of developing a
specific algorithm, we derive a closed form formula for WFOMC in FO$^2$. The
three key advantages of our proposal are: (i) it deals with existential
quantifiers without introducing negative weights; (ii) it easily extends to
FO$^2$ with cardinality constraints and counting quantifiers (aka C$^2$);
finally, (iii) it supports WFOMC for a class of weight functions strictly
larger than symmetric weight functions, which can model count distributions,
without introducing complex or negative weights.
</p>
<a href="http://arxiv.org/abs/2009.12237" target="_blank">arXiv:2009.12237</a> [<a href="http://arxiv.org/pdf/2009.12237" target="_blank">pdf</a>]

<h2>PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v2 [cs.LG] UPDATED)</h2>
<h3>Justin K. Terry, Benjamin Black, Mario Jayakumar, Ananth Hari, Luis Santos, Clemens Dieffendahl, Niall L. Williams, Yashas Lokesh, Ryan Sullivan, Caroline Horsch, Praveen Ravi</h3>
<p>OpenAI's Gym library contains a large, diverse set of environments that are
useful benchmarks in reinforcement learning, under a single elegant Python API
(with tools to develop new compliant environments). The introduction of this
library has proven a watershed moment for the reinforcement learning community,
because it created an accessible set of benchmark environments that everyone
could use(including wrapper important existing libraries), and because a
standardized APIlets RL methods and environments from anywhere be trivially
exchanged. This paper similarly introduces PettingZoo, a library of diverse
sets of multi-agent environments under a single elegant Python API, with tools
to easily make new compliant environments.
</p>
<a href="http://arxiv.org/abs/2009.14471" target="_blank">arXiv:2009.14471</a> [<a href="http://arxiv.org/pdf/2009.14471" target="_blank">pdf</a>]

<h2>Time Matters: Time-Aware LSTMs for Predictive Business Process Monitoring. (arXiv:2010.00889v3 [cs.LG] UPDATED)</h2>
<h3>An Nguyen, Srijeet Chatterjee, Sven Weinzierl, Leo Schwinn, Martin Matzner, Bjoern Eskofier</h3>
<p>Predictive business process monitoring (PBPM) aims to predict future process
behavior during ongoing process executions based on event log data. Especially,
techniques for the next activity and timestamp prediction can help to improve
the performance of operational business processes. Recently, many PBPM
solutions based on deep learning were proposed by researchers. Due to the
sequential nature of event log data, a common choice is to apply recurrent
neural networks with long short-term memory (LSTM) cells. We argue, that the
elapsed time between events is informative. However, current PBPM techniques
mainly use 'vanilla' LSTM cells and hand-crafted time-related control flow
features. To better model the time dependencies between events, we propose a
new PBPM technique based on time-aware LSTM (T-LSTM) cells. T-LSTM cells
incorporate the elapsed time between consecutive events inherently to adjust
the cell memory. Furthermore, we introduce cost-sensitive learning to account
for the common class imbalance in event logs. Our experiments on publicly
available benchmark event logs indicate the effectiveness of the introduced
techniques.
</p>
<a href="http://arxiv.org/abs/2010.00889" target="_blank">arXiv:2010.00889</a> [<a href="http://arxiv.org/pdf/2010.00889" target="_blank">pdf</a>]

<h2>Policy learning in SE(3) action spaces. (arXiv:2010.02798v2 [cs.RO] UPDATED)</h2>
<h3>Dian Wang, Colin Kohler, Robert Platt</h3>
<p>In the spatial action representation, the action space spans the space of
target poses for robot motion commands, i.e. SE(2) or SE(3). This approach has
been used to solve challenging robotic manipulation problems and shows promise.
However, the method is often limited to a three dimensional action space and
short horizon tasks. This paper proposes ASRSE3, a new method for handling
higher dimensional spatial action spaces that transforms an original MDP with
high dimensional action space into a new MDP with reduced action space and
augmented state space. We also propose SDQfD, a variation of DQfD designed for
large action spaces. ASRSE3 and SDQfD are evaluated in the context of a set of
challenging block construction tasks. We show that both methods outperform
standard baselines and can be used in practice on real robotics systems.
</p>
<a href="http://arxiv.org/abs/2010.02798" target="_blank">arXiv:2010.02798</a> [<a href="http://arxiv.org/pdf/2010.02798" target="_blank">pdf</a>]

<h2>The DongNiao International Birds 10000 Dataset. (arXiv:2010.06454v2 [cs.CV] UPDATED)</h2>
<h3>Jian Mei, Hao Dong</h3>
<p>DongNiao International Birds 10000 (DIB-10K) is a challenging image dataset
which has more than 10 thousand different types of birds. It was created to
enable the study of machine learning and also ornithology research. DIB-10K
does not own the copyright of these images. It only provides thumbnails of
images, in a way similar to ImageNet.
</p>
<a href="http://arxiv.org/abs/2010.06454" target="_blank">arXiv:2010.06454</a> [<a href="http://arxiv.org/pdf/2010.06454" target="_blank">pdf</a>]

<h2>CIMON: Towards High-quality Hash Codes. (arXiv:2010.07804v3 [cs.CV] UPDATED)</h2>
<h3>Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Huasong Zhong, Minghua Deng, Jianqiang Huang, Xian-sheng Hua</h3>
<p>Recently, hashing is widely-used in approximate nearest neighbor search for
its storage and computational efficiency. Due to the lack of labeled data in
practice, many studies focus on unsupervised hashing. Most of the unsupervised
hashing methods learn to map images into semantic similarity-preserving hash
codes by constructing local semantic similarity structure from the pre-trained
model as guiding information, i.e., treating each point pair similar if their
distance is small in feature space. However, due to the inefficient
representation ability of the pre-trained model, many false positives and
negatives in local semantic similarity will be introduced and lead to error
propagation during hash code learning. Moreover, most of hashing methods ignore
the basic characteristics of hash codes such as collisions, which will cause
instability of hash codes to disturbance. In this paper, we propose a new
method named Comprehensive sImilarity Mining and cOnsistency learNing (CIMON).
First, we use global constraint learning and similarity statistical
distribution to obtain reliable and smooth guidance. Second, image augmentation
and consistency learning will be introduced to explore both semantic and
contrastive consistency to derive robust hash codes with fewer collisions.
Extensive experiments on several benchmark datasets show that the proposed
method consistently outperforms a wide range of state-of-the-art methods in
both retrieval performance and robustness.
</p>
<a href="http://arxiv.org/abs/2010.07804" target="_blank">arXiv:2010.07804</a> [<a href="http://arxiv.org/pdf/2010.07804" target="_blank">pdf</a>]

<h2>FGAGT: Flow-Guided Adaptive Graph Tracking. (arXiv:2010.09015v2 [cs.CV] UPDATED)</h2>
<h3>Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Xiaoliang Cheng, Kewei Liang</h3>
<p>Most previous tracking methods usually use the optical flow method to
estimate the position of the historical object in the current frame and then
use the linear combination of feature similarity and IOU(Intersection over
Union) to perform association matching near the position. However, the features
used in these methods are not aligned, i.e., the features of the historical
objects are extracted from the historical feature maps, not from the current
frame, even the same object may undergo posture, angle, etc. changes during the
movement, and even light intensity changes. In addition, most methods only use
the appearance information when extracting the feature vector, not the position
relationship, nor the feature information of the historical object, so the
information is not fully utilized. In order to solve the above problems, we
proposed the FGAGT tracker, which uses the optical flow method to predict the
center position of the historical object in the current frame and extract the
feature vector, so that the feature of the historical object can be aligned
with the feature of the object in the current frame. Then these features are
input into the graph neural network, and the global Spatio-temporal position
and appearance information are integrated to update the feature vectors of all
objects. In the training phase, we propose the Balanced MSE LOSS to balance the
sample distribution for data association. Experiments show that our method
reaches the level of state-of-the-art, where the MOTA index exceeds FairMOT by
2.5 points, and CenterTrack by 8.4 points on the MOT17 dataset, exceeds FairMOT
by 1.6 points on the MOT16 dataset. Code will be avaliable.
</p>
<a href="http://arxiv.org/abs/2010.09015" target="_blank">arXiv:2010.09015</a> [<a href="http://arxiv.org/pdf/2010.09015" target="_blank">pdf</a>]

<h2>A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp Detection. (arXiv:2010.10695v3 [cs.RO] UPDATED)</h2>
<h3>Kuang-Yu Jeng, Yueh-Cheng Liu, Zhe Yu Liu, Jen-Wei Wang, Ya-Liang Chang, Hung-Ting Su, Winston H. Hsu</h3>
<p>We proposed an end-to-end grasp detection network, Grasp Detection Network
(GDN), cooperated with a novel coarse-to-fine (C2F) grasp representation design
to detect diverse and accurate 6-DoF grasps based on point clouds. Compared to
previous two-stage approaches which sample and evaluate multiple grasp
candidates, our architecture is at least 20 times faster. It is also 8% and 40%
more accurate in terms of the success rate in single object scenes and the
complete rate in clutter scenes, respectively. Our method shows superior
results among settings with different number of views and input points.
Moreover, we propose a new AP-based metric which considers both rotation and
transition errors, making it a more comprehensive evaluation tool for grasp
detection models.
</p>
<a href="http://arxiv.org/abs/2010.10695" target="_blank">arXiv:2010.10695</a> [<a href="http://arxiv.org/pdf/2010.10695" target="_blank">pdf</a>]

<h2>Concentric mixtures of Mallows models for top-$k$ rankings: sampling and identifiability. (arXiv:2010.14260v2 [stat.ML] UPDATED)</h2>
<h3>Collas Fabien, Irurozki Ekhine</h3>
<p>In this paper, we consider mixtures of two Mallows models for top-$k$
rankings, both with the same location parameter but with different scale
parameters, i.e., a mixture of concentric Mallows models. This situation arises
when we have a heterogeneous population of voters formed by two homogeneous
populations, one of which is a subpopulation of expert voters while the other
includes the non-expert voters. We propose efficient sampling algorithms for
Mallows top-$k$ rankings. We show the identifiability of both components, and
the learnability of their respective parameters in this setting by, first,
bounding the sample complexity for the Borda algorithm with top-$k$ rankings
and second, proposing polynomial time algorithm for the separation of the
rankings in each component. Finally, since the rank aggregation will suffer
from a large amount of noise introduced by the non-expert voters, we adapt the
Borda algorithm to be able to recover the ground truth consensus ranking which
is especially consistent with the expert rankings.
</p>
<a href="http://arxiv.org/abs/2010.14260" target="_blank">arXiv:2010.14260</a> [<a href="http://arxiv.org/pdf/2010.14260" target="_blank">pdf</a>]

<h2>Anomaly detection in average fuel consumption with XAI techniques for dynamic generation of explanations. (arXiv:2010.16051v2 [cs.LG] UPDATED)</h2>
<h3>Alberto Barbado</h3>
<p>In this paper we show a complete process for unsupervised anomaly detection
for the average fuel consumption of fleet vehicles that is able to explain what
variables are affecting the consumption in terms of feature relevance. For
doing that, we combine the anomaly detection with a surrogate model that is
able to provide that feature relevance. For this part, we evaluate both
whitebox models from the literature, as well as novel variations over them, and
blackbox models combined with local posthoc feature relevance techniques. The
evaluation is done using real IoT data belonging to Telef\'onica, and is
measured both in terms of model performance, as well as using Explainable AI
metrics that compare the explanations generated in terms representativeness,
fidelity, stability and contrastiveness. The explanations generate
counterfactual recommendations that show what could have been done to reduce
the average fuel consumption of a vehicle and turn it into an inlier. The
procedure is combined with domain knowledge expressed in business rules, and is
able to adequate the type of explanations depending on the target user profile.
</p>
<a href="http://arxiv.org/abs/2010.16051" target="_blank">arXiv:2010.16051</a> [<a href="http://arxiv.org/pdf/2010.16051" target="_blank">pdf</a>]

<h2>Towards Preference Learning for Autonomous Ground Robot Navigation Tasks. (arXiv:2010.16361v2 [cs.RO] UPDATED)</h2>
<h3>Cory Hayes, Matthew Marge</h3>
<p>We are interested in the design of autonomous robot behaviors that learn the
preferences of users over continued interactions, with the goal of efficiently
executing navigation behaviors in a way that the user expects. In this paper,
we discuss our work in progress to modify a general model for robot navigation
behaviors in an exploration task on a per-user basis using preference-based
reinforcement learning. The novel contribution of this approach is that it
combines reinforcement learning, motion planning, and natural language
processing to allow an autonomous agent to learn from sustained dialogue with a
human teammate as opposed to one-off instructions.
</p>
<a href="http://arxiv.org/abs/2010.16361" target="_blank">arXiv:2010.16361</a> [<a href="http://arxiv.org/pdf/2010.16361" target="_blank">pdf</a>]

<h2>Training EfficientNets at Supercomputer Scale: 83% ImageNet Top-1 Accuracy in One Hour. (arXiv:2011.00071v2 [cs.LG] UPDATED)</h2>
<h3>Arissa Wongpanich, Hieu Pham, James Demmel, Mingxing Tan, Quoc Le, Yang You, Sameer Kumar</h3>
<p>EfficientNets are a family of state-of-the-art image classification models
based on efficiently scaled convolutional neural networks. Currently,
EfficientNets can take on the order of days to train; for example, training an
EfficientNet-B0 model takes 23 hours on a Cloud TPU v2-8 node. In this paper,
we explore techniques to scale up the training of EfficientNets on TPU-v3 Pods
with 2048 cores, motivated by speedups that can be achieved when training at
such scales. We discuss optimizations required to scale training to a batch
size of 65536 on 1024 TPU-v3 cores, such as selecting large batch optimizers
and learning rate schedules as well as utilizing distributed evaluation and
batch normalization techniques. Additionally, we present timing and performance
benchmarks for EfficientNet models trained on the ImageNet dataset in order to
analyze the behavior of EfficientNets at scale. With our optimizations, we are
able to train EfficientNet on ImageNet to an accuracy of 83% in 1 hour and 4
minutes.
</p>
<a href="http://arxiv.org/abs/2011.00071" target="_blank">arXiv:2011.00071</a> [<a href="http://arxiv.org/pdf/2011.00071" target="_blank">pdf</a>]

<h2>PAC Confidence Predictions for Deep Neural Network Classifiers. (arXiv:2011.00716v2 [cs.LG] UPDATED)</h2>
<h3>Sangdon Park, Shuo Li, Osbert Bastani, Insup Lee</h3>
<p>A key challenge for deploying deep neural networks (DNNs) in safety critical
settings is the need to provide rigorous ways to quantify their uncertainty. In
this paper, we propose a novel algorithm for constructing predicted
classification confidences for DNNs that comes with provable correctness
guarantees. Our approach uses Clopper-Pearson confidence intervals for the
Binomial distribution in conjunction with the histogram binning approach to
calibrated prediction. In addition, we demonstrate how our predicted
confidences can be used to enable downstream guarantees in two settings: (i)
fast DNN inference, where we demonstrate how to compose a fast but inaccurate
DNN with an accurate but slow DNN in a rigorous way to improve performance
without sacrificing accuracy, and (ii) safe planning, where we guarantee safety
when using a DNN to predict whether a given action is safe based on visual
observations. In our experiments, we demonstrate that our approach can be used
to provide guarantees for state-of-the-art DNNs.
</p>
<a href="http://arxiv.org/abs/2011.00716" target="_blank">arXiv:2011.00716</a> [<a href="http://arxiv.org/pdf/2011.00716" target="_blank">pdf</a>]

<h2>Estimating County-Level COVID-19 Exponential Growth Rates Using Generalized Random Forests. (arXiv:2011.01219v3 [cs.LG] UPDATED)</h2>
<h3>Zhaowei She, Zilong Wang, Turgay Ayer, Asmae Toumi, Jagpreet Chhatwal</h3>
<p>Rapid and accurate detection of community outbreaks is critical to address
the threat of resurgent waves of COVID-19. A practical challenge in outbreak
detection is balancing accuracy vs. speed. In particular, while estimation
accuracy improves with longer fitting windows, speed degrades. This paper
presents a machine learning framework to balance this tradeoff using
generalized random forests (GRF), and applies it to detect county level
COVID-19 outbreaks. This algorithm chooses an adaptive fitting window size for
each county based on relevant features affecting the disease spread, such as
changes in social distancing policies. Experiment results show that our method
outperforms any non-adaptive window size choices in 7-day ahead COVID-19
outbreak case number predictions.
</p>
<a href="http://arxiv.org/abs/2011.01219" target="_blank">arXiv:2011.01219</a> [<a href="http://arxiv.org/pdf/2011.01219" target="_blank">pdf</a>]

<h2>VEGA: Towards an End-to-End Configurable AutoML Pipeline. (arXiv:2011.01507v3 [cs.CV] UPDATED)</h2>
<h3>Bochao Wang, Hang Xu, Jiajin Zhang, Chen Chen, Xiaozhi Fang, Ning Kang, Lanqing Hong, Wei Zhang, Yong Li, Zhicheng Liu, Zhenguo Li, Wenzhi Liu, Tong Zhang</h3>
<p>Automated Machine Learning (AutoML) is an important industrial solution for
automatic discovery and deployment of the machine learning models. However,
designing an integrated AutoML system faces four great challenges of
configurability, scalability, integrability, and platform diversity. In this
work, we present VEGA, an efficient and comprehensive AutoML framework that is
compatible and optimized for multiple hardware platforms. a) The VEGA pipeline
integrates various modules of AutoML, including Neural Architecture Search
(NAS), Hyperparameter Optimization (HPO), Auto Data Augmentation, Model
Compression, and Fully Train. b) To support a variety of search algorithms and
tasks, we design a novel fine-grained search space and its description language
to enable easy adaptation to different search algorithms and tasks. c) We
abstract the common components of deep learning frameworks into a unified
interface. VEGA can be executed with multiple back-ends and hardwares.
Extensive benchmark experiments on multiple tasks demonstrate that VEGA can
improve the existing AutoML algorithms and discover new high-performance models
against SOTA methods, e.g. the searched DNet model zoo for Ascend 10x faster
than EfficientNet-B5 and 9.2x faster than RegNetX-32GF on ImageNet. VEGA is
open-sourced at https://github.com/huawei-noah/vega.
</p>
<a href="http://arxiv.org/abs/2011.01507" target="_blank">arXiv:2011.01507</a> [<a href="http://arxiv.org/pdf/2011.01507" target="_blank">pdf</a>]

<h2>Uncertainty Quantification of Darcy Flow through Porous Media using Deep Gaussian Process. (arXiv:2011.01647v2 [stat.ML] UPDATED)</h2>
<h3>A. Daneshkhah, O. Chatrabgoun, M. Esmaeilbeigi, T. Sedighi, S. Abolfathi</h3>
<p>A computational method based on the non-linear Gaussian process (GP), known
as deep Gaussian processes (deep GPs) for uncertainty quantification &amp;
propagation in modelling of flow through heterogeneous porous media is
presented. The method is also used for reducing dimensionality of model output
and consequently emulating highly complex relationship between hydrogeological
properties and reduced order fluid velocity field in a tractable manner. Deep
GPs are multi-layer hierarchical generalisations of GPs with multiple,
infinitely wide hidden layers that are very efficient models for deep learning
and modelling of high-dimensional complex systems by tackling the complexity
through several hidden layers connected with non-linear mappings. According to
this approach, the hydrogeological data is modelled as the output of a
multivariate GP whose inputs are governed by another GP such that each single
layer is either a standard GP or the Gaussian process latent variable model. A
variational approximation framework is used so that the posterior distribution
of the model outputs associated to given inputs can be analytically
approximated. In contrast to the other dimensionality reduction, methods that
do not provide any information about the dimensionality of each hidden layer,
the proposed method automatically selects the dimensionality of each hidden
layer and it can be used to propagate uncertainty obtained in each layer across
the hierarchy. Using this, dimensionality of the full input space consists of
both geometrical parameters of modelling domain and stochastic hydrogeological
parameters can be simultaneously reduced without the need for any
simplifications generally being assumed for stochastic modelling of subsurface
flow problems. It allows estimation of the flow statistics with greatly reduced
computational efforts compared to other stochastic approaches such as Monte
Carlo method.
</p>
<a href="http://arxiv.org/abs/2011.01647" target="_blank">arXiv:2011.01647</a> [<a href="http://arxiv.org/pdf/2011.01647" target="_blank">pdf</a>]

<h2>Where am I? SLAM for Mobile Machines on A Smart Working Site. (arXiv:2011.01830v2 [cs.RO] UPDATED)</h2>
<h3>Yusheng Xiang, Dianzhao Li, Tianqing Su, Quan Zhou, Christine Brach, Samuel S. Mao, Marcus Geimer</h3>
<p>The current optimization approaches of construction machinery are mainly
based on internal sensors. However, the decision of a reasonable strategy is
not only determined by its intrinsic signals, but also very strongly by
environmental information, especially the terrain. Due to the dynamically
changing of the construction site and the consequent absence of a high
definition map, the Simultaneous Localization and Mapping (SLAM) offering the
terrain information for construction machines is still challenging. Current
SLAM technologies proposed for mobile machines are strongly dependent on costly
or computationally expensive sensors, such as RTK GPS and cameras, so that
commercial use is rare. In this study, we proposed an affordable SLAM method to
create a multi-layer gird map for the construction site so that the machine can
have the environmental information and be optimized accordingly. Concretely,
after the machine passes by, we can get the local information and record it.
Combining with positioning technology, we then create a map of the interesting
places of the construction site. As a result of our research gathered from
Gazebo, we showed that a suitable layout is the combination of 1 IMU and 2
differential GPS antennas using the unscented Kalman filter, which keeps the
average distance error lower than 2m and the mapping error lower than 1.3% in
the harsh environment. As an outlook, our SLAM technology provides the
cornerstone to activate many efficiency improvement approaches.
</p>
<a href="http://arxiv.org/abs/2011.01830" target="_blank">arXiv:2011.01830</a> [<a href="http://arxiv.org/pdf/2011.01830" target="_blank">pdf</a>]

<h2>Generating Unobserved Alternatives: A Case Study through Super-Resolution and Decompression. (arXiv:2011.01926v2 [cs.LG] UPDATED)</h2>
<h3>Shichong Peng, Ke Li</h3>
<p>We consider problems where multiple predictions can be considered correct,
but only one of them is given as supervision. This setting differs from both
the regression and class-conditional generative modelling settings: in the
former, there is a unique observed output for each input, which is provided as
supervision; in the latter, there are many observed outputs for each input, and
many are provided as supervision. Applying either regression methods and
conditional generative models to the present setting often results in a model
that can only make a single prediction for each input. We explore several
problems that have this property and develop an approach that can generate
multiple high-quality predictions given the same input. As a result, it can be
used to generate high-quality outputs that are different from the observed
output.
</p>
<a href="http://arxiv.org/abs/2011.01926" target="_blank">arXiv:2011.01926</a> [<a href="http://arxiv.org/pdf/2011.01926" target="_blank">pdf</a>]

<h2>Single Image Human Proxemics Estimation for Visual Social Distancing. (arXiv:2011.02018v2 [cs.CV] UPDATED)</h2>
<h3>Maya Aghaei, Matteo Bustreo, Yiming Wang, Gianluca Bailo, Pietro Morerio, Alessio Del Bue</h3>
<p>In this work, we address the problem of estimating the so-called "Social
Distancing" given a single uncalibrated image in unconstrained scenarios. Our
approach proposes a semi-automatic solution to approximate the homography
matrix between the scene ground and image plane. With the estimated homography,
we then leverage an off-the-shelf pose detector to detect body poses on the
image and to reason upon their inter-personal distances using the length of
their body-parts. Inter-personal distances are further locally inspected to
detect possible violations of the social distancing rules. We validate our
proposed method quantitatively and qualitatively against baselines on public
domain datasets for which we provided groundtruth on inter-personal distances.
Besides, we demonstrate the application of our method deployed in a real
testing scenario where statistics on the inter-personal distances are currently
used to improve the safety in a critical environment.
</p>
<a href="http://arxiv.org/abs/2011.02018" target="_blank">arXiv:2011.02018</a> [<a href="http://arxiv.org/pdf/2011.02018" target="_blank">pdf</a>]

<h2>Mcity Data Collection for Automated Vehicles Study. (arXiv:1912.06258v1 [cs.CV] CROSS LISTED)</h2>
<h3>Yiqun Dong, Yuanxin Zhong, Wenbo Yu, Minghan Zhu, Pingping Lu, Yeyang Fang, Jiajun Hong, Huei Peng</h3>
<p>The main goal of this paper is to introduce the data collection effort at
Mcity targeting automated vehicle development. We captured a comprehensive set
of data from a set of perception sensors (Lidars, Radars, Cameras) as well as
vehicle steering/brake/throttle inputs and an RTK unit. Two in-cabin cameras
record the human driver's behaviors for possible future use. The naturalistic
driving on selected open roads is recorded at different time of day and weather
conditions. We also perform designed choreography data collection inside the
Mcity test facility focusing on vehicle to vehicle, and vehicle to vulnerable
road user interactions which is quite unique among existing open-source
datasets. The vehicle platform, data content, tags/labels, and selected
analysis results are shown in this paper.
</p>
<a href="http://arxiv.org/abs/1912.06258" target="_blank">arXiv:1912.06258</a> [<a href="http://arxiv.org/pdf/1912.06258" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning Based Dynamic Route Planning for Minimizing Travel Time. (arXiv:2011.01771v1 [cs.LG] CROSS LISTED)</h2>
<h3>Yuanzhe Geng, Erwu Liu, Rui Wang, Yiming Liu</h3>
<p>Route planning is important in transportation. Existing works focus on
finding the shortest path solution or using metrics such as safety and energy
consumption to determine the planning. It is noted that most of these studies
rely on prior knowledge of road network, which may be not available in certain
situations. In this paper, we design a route planning algorithm based on deep
reinforcement learning (DRL) for pedestrians. We use travel time consumption as
the metric, and plan the route by predicting pedestrian flow in the road
network. We put an agent, which is an intelligent robot, on a virtual map.
Different from previous studies, our approach assumes that the agent does not
need any prior information about road network, but simply relies on the
interaction with the environment. We propose a dynamically adjustable route
planning (DARP) algorithm, where the agent learns strategies through a dueling
deep Q network to avoid congested roads. Simulation results show that the DARP
algorithm saves 52% of the time under congestion condition when compared with
traditional shortest path planning algorithms.
</p>
<a href="http://arxiv.org/abs/2011.01771" target="_blank">arXiv:2011.01771</a> [<a href="http://arxiv.org/pdf/2011.01771" target="_blank">pdf</a>]

