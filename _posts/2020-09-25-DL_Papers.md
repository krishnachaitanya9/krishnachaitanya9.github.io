---
title: Latest Deep Learning Papers
date: 2020-11-30 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (267 Articles)</h1>
<h2>A Recurrent Vision-and-Language BERT for Navigation. (arXiv:2011.13922v1 [cs.CV])</h2>
<h3>Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould</h3>
<p>Accuracy of many visiolinguistic tasks has benefited significantly from the
application of vision-and-language (V&amp;L) BERT. However, its application for the
task of vision-and-language navigation (VLN) remains limited. One reason for
this is the difficulty adapting the BERT architecture to the partially
observable Markov decision process present in VLN, requiring history-dependent
attention and decision making. In this paper we propose a recurrent BERT model
that is time-aware for use in VLN. Specifically, we equip the BERT model with a
recurrent function that maintains cross-modal state information for the agent.
Through extensive experiments on R2R and REVERIE we demonstrate that our model
can replace more complex encoder-decoder models to achieve state-of-the-art
results. Moreover, our approach can be generalised to other transformer-based
architectures, supports pre-training, and is capable of multi-task learning
suggesting the potential to merge a wide range of BERT-like models for other
vision and language tasks.
</p>
<a href="http://arxiv.org/abs/2011.13922" target="_blank">arXiv:2011.13922</a> [<a href="http://arxiv.org/pdf/2011.13922" target="_blank">pdf</a>]

<h2>D-NeRF: Neural Radiance Fields for Dynamic Scenes. (arXiv:2011.13961v1 [cs.CV])</h2>
<h3>Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer</h3>
<p>Neural rendering techniques combining machine learning with geometric
reasoning have arisen as one of the most promising approaches for synthesizing
novel views of a scene from a sparse set of images. Among these, stands out the
Neural radiance fields (NeRF), which trains a deep network to map 5D input
coordinates (representing spatial location and viewing direction) into a volume
density and view-dependent emitted radiance. However, despite achieving an
unprecedented level of photorealism on the generated images, NeRF is only
applicable to static scenes, where the same spatial location can be queried
from different images. In this paper we introduce D-NeRF, a method that extends
neural radiance fields to a dynamic domain, allowing to reconstruct and render
novel images of objects under rigid and non-rigid motions from a \emph{single}
camera moving around the scene. For this purpose we consider time as an
additional input to the system, and split the learning process in two main
stages: one that encodes the scene into a canonical space and another that maps
this canonical representation into the deformed scene at a particular time.
Both mappings are simultaneously learned using fully-connected networks. Once
the networks are trained, D-NeRF can render novel images, controlling both the
camera view and the time variable, and thus, the object movement. We
demonstrate the effectiveness of our approach on scenes with objects under
rigid, articulated and non-rigid motions. Code, model weights and the dynamic
scenes dataset will be released.
</p>
<a href="http://arxiv.org/abs/2011.13961" target="_blank">arXiv:2011.13961</a> [<a href="http://arxiv.org/pdf/2011.13961" target="_blank">pdf</a>]

<h2>Self supervised contrastive learning for digital histopathology. (arXiv:2011.13971v1 [cs.CV])</h2>
<h3>Ozan Ciga, Anne L. Martel, Tony Xu</h3>
<p>Unsupervised learning has been a long-standing goal of machine learning and
is especially important for medical image analysis, where the learning can
compensate for the scarcity of labeled datasets. A promising subclass of
unsupervised learning is self-supervised learning, which aims to learn salient
features using the raw input as the learning signal. In this paper, we use a
contrastive self-supervised learning method Chen et al. (2020a) that achieved
state-of-the-art results on natural-scene images, and apply this method to
digital histopathology by collecting and training on 60 histopathology datasets
without any labels. We find that combining multiple multi-organ datasets with
different types of staining and resolution properties improves the quality of
the learned features. Furthermore, we find drastically subsampling a dataset
(e.g., using ? 1% of the available image patches) does not negatively impact
the learned representations, unlike training on natural-scene images. Linear
classifiers trained on top of the learned features show that networks
pretrained on digital histopathology datasets perform better than ImageNet
pretrained networks, boosting task performances up to 7.5% in accuracy and 8.9%
in F1. These findings may also be useful when applying newer contrastive
techniques to histopathology data. Pretrained PyTorch models are made publicly
available at https://github.com/ozanciga/self-supervised-histopathology.
</p>
<a href="http://arxiv.org/abs/2011.13971" target="_blank">arXiv:2011.13971</a> [<a href="http://arxiv.org/pdf/2011.13971" target="_blank">pdf</a>]

<h2>Reflective-Net: Learning from Explanations. (arXiv:2011.13986v1 [cs.LG])</h2>
<h3>Johannes Schneider, Michalis Vlachos</h3>
<p>Humans possess a remarkable capability to make fast, intuitive decisions, but
also to self-reflect, i.e., to explain to oneself, and to efficiently learn
from explanations by others. This work provides the first steps toward
mimicking this process by capitalizing on the explanations generated based on
existing explanation methods, i.e. Grad-CAM. Learning from explanations
combined with conventional labeled data yields significant improvements for
classification in terms of accuracy and training time.
</p>
<a href="http://arxiv.org/abs/2011.13986" target="_blank">arXiv:2011.13986</a> [<a href="http://arxiv.org/pdf/2011.13986" target="_blank">pdf</a>]

<h2>Reducing Discrimination in Learning Algorithms for Social Good in Sociotechnical Systems. (arXiv:2011.13988v1 [cs.LG])</h2>
<h3>Katelyn Morrison</h3>
<p>Sociotechnical systems within cities are now equipped with machine learning
algorithms in hopes to increase efficiency and functionality by modeling and
predicting trends. Machine learning algorithms have been applied in these
domains to address challenges such as balancing the distribution of bikes
throughout a city and identifying demand hotspots for ride sharing drivers.
However, these algorithms applied to challenges in sociotechnical systems have
exacerbated social inequalities due to previous bias in data sets or the lack
of data from marginalized communities. In this paper, I will address how smart
mobility initiatives in cities use machine learning algorithms to address
challenges. I will also address how these algorithms unintentionally
discriminate against features such as socioeconomic status to motivate the
importance of algorithmic fairness. Using the bike sharing program in
Pittsburgh, PA, I will present a position on how discrimination can be
eliminated from the pipeline using Bayesian Optimization.
</p>
<a href="http://arxiv.org/abs/2011.13988" target="_blank">arXiv:2011.13988</a> [<a href="http://arxiv.org/pdf/2011.13988" target="_blank">pdf</a>]

<h2>Assessing Post-Disaster Damage from Satellite Imagery using Semi-Supervised Learning Techniques. (arXiv:2011.14004v1 [cs.CV])</h2>
<h3>Jihyeon Lee, Joseph Z. Xu, Kihyuk Sohn, Wenhan Lu, David Berthelot, Izzeddin Gur, Pranav Khaitan, Ke-Wei (Fiona) Huang, Kyriacos Koupparis, Bernhard Kowatsch</h3>
<p>To respond to disasters such as earthquakes, wildfires, and armed conflicts,
humanitarian organizations require accurate and timely data in the form of
damage assessments, which indicate what buildings and population centers have
been most affected. Recent research combines machine learning with remote
sensing to automatically extract such information from satellite imagery,
reducing manual labor and turn-around time. A major impediment to using machine
learning methods in real disaster response scenarios is the difficulty of
obtaining a sufficient amount of labeled data to train a model for an unfolding
disaster. This paper shows a novel application of semi-supervised learning
(SSL) to train models for damage assessment with a minimal amount of labeled
data and large amount of unlabeled data. We compare the performance of
state-of-the-art SSL methods, including MixMatch and FixMatch, to a supervised
baseline for the 2010 Haiti earthquake, 2017 Santa Rosa wildfire, and 2016
armed conflict in Syria. We show how models trained with SSL methods can reach
fully supervised performance despite using only a fraction of labeled data and
identify areas for further improvements.
</p>
<a href="http://arxiv.org/abs/2011.14004" target="_blank">arXiv:2011.14004</a> [<a href="http://arxiv.org/pdf/2011.14004" target="_blank">pdf</a>]

<h2>Active Learning in CNNs via Expected Improvement Maximization. (arXiv:2011.14015v1 [cs.LG])</h2>
<h3>Udai G. Nagpal, David A Knowles</h3>
<p>Deep learning models such as Convolutional Neural Networks (CNNs) have
demonstrated high levels of effectiveness in a variety of domains, including
computer vision and more recently, computational biology. However, training
effective models often requires assembling and/or labeling large datasets,
which may be prohibitively time-consuming or costly. Pool-based active learning
techniques have the potential to mitigate these issues, leveraging models
trained on limited data to selectively query unlabeled data points from a pool
in an attempt to expedite the learning process. Here we present "Dropout-based
Expected IMprOvementS" (DEIMOS), a flexible and computationally-efficient
approach to active learning that queries points that are expected to maximize
the model's improvement across a representative sample of points. The proposed
framework enables us to maintain a prediction covariance matrix capturing model
uncertainty, and to dynamically update this matrix in order to generate diverse
batches of points in the batch-mode setting. Our active learning results
demonstrate that DEIMOS outperforms several existing baselines across multiple
regression and classification tasks taken from computer vision and genomics.
</p>
<a href="http://arxiv.org/abs/2011.14015" target="_blank">arXiv:2011.14015</a> [<a href="http://arxiv.org/pdf/2011.14015" target="_blank">pdf</a>]

<h2>Investigating Human Response, Behaviour, and Preference in Joint-Task Interaction. (arXiv:2011.14016v1 [cs.AI])</h2>
<h3>Alan Lindsay, Bart Craenen, Sara Dalzel-Job, Robin L. Hill, Ronald P. A. Petrick</h3>
<p>Human interaction relies on a wide range of signals, including non-verbal
cues. In order to develop effective Explainable Planning (XAIP) agents it is
important that we understand the range and utility of these communication
channels. Our starting point is existing results from joint task interaction
and their study in cognitive science. Our intention is that these lessons can
inform the design of interaction agents -- including those using planning
techniques -- whose behaviour is conditioned on the user's response, including
affective measures of the user (i.e., explicitly incorporating the user's
affective state within the planning model). We have identified several concepts
at the intersection of plan-based agent behaviour and joint task interaction
and have used these to design two agents: one reactive and the other partially
predictive. We have designed an experiment in order to examine human behaviour
and response as they interact with these agents. In this paper we present the
designed study and the key questions that are being investigated. We also
present the results from an empirical analysis where we examined the behaviour
of the two agents for simulated users.
</p>
<a href="http://arxiv.org/abs/2011.14016" target="_blank">arXiv:2011.14016</a> [<a href="http://arxiv.org/pdf/2011.14016" target="_blank">pdf</a>]

<h2>Rethinking Text Segmentation: A Novel Dataset and A Text-Specific Refinement Approach. (arXiv:2011.14021v1 [cs.CV])</h2>
<h3>Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang, Humphrey Shi</h3>
<p>Text segmentation is a prerequisite in many real-world text-related tasks,
e.g., text style transfer, and scene text removal. However, facing the lack of
high-quality datasets and dedicated investigations, this critical prerequisite
has been left as an assumption in many works, and has been largely overlooked
by current research. To bridge this gap, we proposed TextSeg, a large-scale
fine-annotated text dataset with six types of annotations: word- and
character-wise bounding polygons, masks and transcriptions. We also introduce
Text Refinement Network (TexRNet), a novel text segmentation approach that
adapts to the unique properties of text, e.g. non-convex boundary, diverse
texture, etc., which often impose burdens on traditional segmentation models.
In our TexRNet, we propose text specific network designs to address such
challenges, including key features pooling and attention-based similarity
checking. We also introduce trimap and discriminator losses that show
significant improvement on text segmentation. Extensive experiments are carried
out on both our TextSeg dataset and other existing datasets. We demonstrate
that TexRNet consistently improves text segmentation performance by nearly 2%
compared to other state-of-the-art segmentation methods. Our dataset and code
will be made available at
https://github.com/SHI-Labs/Rethinking-Text-Segmentation.
</p>
<a href="http://arxiv.org/abs/2011.14021" target="_blank">arXiv:2011.14021</a> [<a href="http://arxiv.org/pdf/2011.14021" target="_blank">pdf</a>]

<h2>General Multi-label Image Classification with Transformers. (arXiv:2011.14027v1 [cs.CV])</h2>
<h3>Jack Lanchantin, Tianlu Wang, Vicente Ordonez, Yanjun Qi</h3>
<p>Multi-label image classification is the task of predicting a set of labels
corresponding to objects, attributes or other entities present in an image. In
this work we propose the Classification Transformer (C-Tran), a general
framework for multi-label image classification that leverages Transformers to
exploit the complex dependencies among visual features and labels. Our approach
consists of a Transformer encoder trained to predict a set of target labels
given an input set of masked labels, and visual features from a convolutional
neural network. A key ingredient of our method is a label mask training
objective that uses a ternary encoding scheme to represent the state of the
labels as positive, negative, or unknown during training. Our model shows
state-of-the-art performance on challenging datasets such as COCO and Visual
Genome. Moreover, because our model explicitly represents the uncertainty of
labels during training, it is more general by allowing us to produce improved
results for images with partial or extra label annotations during inference. We
demonstrate this additional capability in the COCO, Visual Genome, News500, and
CUB image datasets.
</p>
<a href="http://arxiv.org/abs/2011.14027" target="_blank">arXiv:2011.14027</a> [<a href="http://arxiv.org/pdf/2011.14027" target="_blank">pdf</a>]

<h2>Voting based ensemble improves robustness of defensive models. (arXiv:2011.14031v1 [cs.LG])</h2>
<h3>Devvrit, Minhao Cheng, Cho-Jui Hsieh, Inderjit Dhillon</h3>
<p>Developing robust models against adversarial perturbations has been an active
area of research and many algorithms have been proposed to train individual
robust models. Taking these pretrained robust models, we aim to study whether
it is possible to create an ensemble to further improve robustness. Several
previous attempts tackled this problem by ensembling the soft-label prediction
and have been proved vulnerable based on the latest attack methods. In this
paper, we show that if the robust training loss is diverse enough, a simple
hard-label based voting ensemble can boost the robust error over each
individual model. Furthermore, given a pool of robust models, we develop a
principled way to select which models to ensemble. Finally, to verify the
improved robustness, we conduct extensive experiments to study how to attack a
voting-based ensemble and develop several new white-box attacks. On CIFAR-10
dataset, by ensembling several state-of-the-art pre-trained defense models, our
method can achieve a 59.8% robust accuracy, outperforming all the existing
defensive models without using additional data.
</p>
<a href="http://arxiv.org/abs/2011.14031" target="_blank">arXiv:2011.14031</a> [<a href="http://arxiv.org/pdf/2011.14031" target="_blank">pdf</a>]

<h2>Predicting cardiovascular risk from national administrative databases using a combined survival analysis and deep learning approach. (arXiv:2011.14032v1 [cs.LG])</h2>
<h3>Sebastiano Barbieri, Suneela Mehta, Billy Wu, Chrianna Bharat, Katrina Poppe, Louisa Jorm, Rod Jackson</h3>
<p>AIMS. This study compared the performance of deep learning extensions of
survival analysis models with traditional Cox proportional hazards (CPH) models
for deriving cardiovascular disease (CVD) risk prediction equations in national
health administrative datasets. METHODS. Using individual person linkage of
multiple administrative datasets, we constructed a cohort of all New Zealand
residents aged 30-74 years who interacted with publicly funded health services
during 2012, and identified hospitalisations and deaths from CVD over five
years of follow-up. After excluding people with prior CVD or heart failure,
sex-specific deep learning and CPH models were developed to estimate the risk
of fatal or non-fatal CVD events within five years. The proportion of explained
time-to-event occurrence, calibration, and discrimination were compared between
models across the whole study population and in specific risk groups. FINDINGS.
First CVD events occurred in 61,927 of 2,164,872 people. Among diagnoses and
procedures, the largest 'local' hazard ratios were associated by the deep
learning models with tobacco use in women (2.04, 95%CI: 1.99-2.10) and with
chronic obstructive pulmonary disease with acute lower respiratory infection in
men (1.56, 95%CI: 1.50-1.62). Other identified predictors (e.g. hypertension,
chest pain, diabetes) aligned with current knowledge about CVD risk predictors.
The deep learning models significantly outperformed the CPH models on the basis
of proportion of explained time-to-event occurrence (Royston and Sauerbrei's
R-squared: 0.468 vs. 0.425 in women and 0.383 vs. 0.348 in men), calibration,
and discrimination (all p&lt;0.0001). INTERPRETATION. Deep learning extensions of
survival analysis models can be applied to large health administrative
databases to derive interpretable CVD risk prediction equations that are more
accurate than traditional CPH models.
</p>
<a href="http://arxiv.org/abs/2011.14032" target="_blank">arXiv:2011.14032</a> [<a href="http://arxiv.org/pdf/2011.14032" target="_blank">pdf</a>]

<h2>Improved Optimistic Algorithm For The Multinomial Logit Contextual Bandit. (arXiv:2011.14033v1 [cs.LG])</h2>
<h3>Priyank Agrawal, Vashist Avadhanula, Theja Tulabandhula</h3>
<p>We consider a dynamic assortment selection problem where the goal is to offer
a sequence of assortments of cardinality at most $K$, out of $N$ items, to
minimize the expected cumulative regret (loss of revenue). The feedback is
given by a multinomial logit (MNL) choice model. This sequential decision
making problem is studied under the MNL contextual bandit framework. The
existing algorithms for MNL contexual bandit have frequentist regret guarantees
as $\tilde{\mathrm{O}}(\kappa\sqrt{T})$, where $\kappa$ is an instance
dependent constant. $\kappa$ could be arbitrarily large, e.g. exponentially
dependent on the model parameters, causing the existing regret guarantees to be
substantially loose. We propose an optimistic algorithm with a carefully
designed exploration bonus term and show that it enjoys
$\tilde{\mathrm{O}}(\sqrt{T})$ regret. In our bounds, the $\kappa$ factor only
affects the poly-log term and not the leading term of the regret bounds.
</p>
<a href="http://arxiv.org/abs/2011.14033" target="_blank">arXiv:2011.14033</a> [<a href="http://arxiv.org/pdf/2011.14033" target="_blank">pdf</a>]

<h2>cMinMax: A Fast Algorithm to Find the Corners of an N-dimensionalConvex Polytope. (arXiv:2011.14035v1 [cs.CV])</h2>
<h3>Dimitrios Chamzas, Constantinos Chamzas, Konstantinos Moustakas</h3>
<p>During the last years, the emerging field of Augmented &amp; Virtual Reality
(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop
low cost high-quality AR systems where computing poweris in demand. Feature
points are extensively used in these real-time frame-rate and 3D applications,
thereforeefficient high-speed feature detectors are necessary. Corners are such
special features and often are used as thefirst step in the marker alignment in
Augmented Reality (AR). Corners are also used in image registration
andrecognition, tracking, SLAM, robot path finding and 2D or 3D object
detection and retrieval. Therefore thereis a large number of corner detection
algorithms but most of them are too computationally intensive for use
inreal-time applications of any complexity. Many times the border of the image
is a convex polygon. For thisspecial, but quite common case, we have developed
a specific algorithm, cMinMax. The proposed algorithmis faster, approximately
by a factor of 5 compared to the widely used Harris Corner Detection algorithm.
Inaddition is highly parallelizable. The algorithm is suitable for the fast
registration of markers in augmentedreality systems and in applications where a
computationally efficient real time feature detector is necessary.The algorithm
can also be extended to N-dimensional polyhedrons.
</p>
<a href="http://arxiv.org/abs/2011.14035" target="_blank">arXiv:2011.14035</a> [<a href="http://arxiv.org/pdf/2011.14035" target="_blank">pdf</a>]

<h2>A RGB-D SLAM Algorithm for Indoor Dynamic Scene. (arXiv:2011.14041v1 [cs.RO])</h2>
<h3>Deng Su, Dehong Chong</h3>
<p>Visual slam technology is one of the key technologies for robot to explore
unknown environment independently. Accurate estimation of camera pose based on
visual sensor is the basis of autonomous navigation and positioning. However,
most visual slam algorithms are based on static environment assumption and
cannot estimate accurate camera pose in dynamic environment. In order to solve
this problem, a visual SLAM algorithm for indoor dynamic environment is
proposed. Firstly, some moving objects are eliminated based on the depth
information of RGB-D camera, and the initial camera pose is obtained by
optimizing the luminosity and depth errors, then the moving objects are further
eliminated. and, the initial static background is used for pose estimation
again. After several iterations, the more accurate static background and more
accurate camera pose is obtained. Experimental results show that, compared with
previous research results, the proposed algorithm can achieve higher pose
estimation accuracy in both low dynamic indoor scenes and high dynamic indoor
scenes.
</p>
<a href="http://arxiv.org/abs/2011.14041" target="_blank">arXiv:2011.14041</a> [<a href="http://arxiv.org/pdf/2011.14041" target="_blank">pdf</a>]

<h2>Generalized Adversarial Examples: Attacks and Defenses. (arXiv:2011.14045v1 [cs.LG])</h2>
<h3>Haojing Shen, Sihong Chen, Ran Wang, Xizhao Wang</h3>
<p>Most of the works follow such definition of adversarial example that is
imperceptible to humans but can fool the deep neural networks (DNNs). Some
works find another interesting form of adversarial examples such as one which
is unrecognizable to humans, but DNNs classify it as one class with high
confidence and adversarial patch. Based on this phenomenon, in this paper, from
the perspective of cognition of humans and machines, we propose a new
definition of adversarial examples. We show that imperceptible adversarial
examples, unrecognizable adversarial examples, and adversarial patches are
derivates of generalized adversarial examples. Then, we propose three types of
adversarial attacks based on the generalized definition. Finally, we propose a
defence mechanism that achieves state-of-the-art performance. We construct a
lossy compression function to filter out the redundant features generated by
the network. In this process, the perturbation produced by the attacker will be
filtered out. Therefore, the defence mechanism can effectively improve the
robustness of the model. The experiments show that our attack methods can
effectively generate adversarial examples, and our defence method can
significantly improve the adversarial robustness of DNNs compared with
adversarial training. As far as we know, our defending method achieves the best
performance even though we do not adopt adversarial training.
</p>
<a href="http://arxiv.org/abs/2011.14045" target="_blank">arXiv:2011.14045</a> [<a href="http://arxiv.org/pdf/2011.14045" target="_blank">pdf</a>]

<h2>Learning from Incomplete Data by Simultaneous Training of Neural Networks and Sparse Coding. (arXiv:2011.14047v1 [cs.LG])</h2>
<h3>Cesar F. Caiafa, Ziyao Wang, Jordi Sol&#xe9;-Casals, Qibin Zhao</h3>
<p>Handling correctly incomplete datasets in machine learning is a fundamental
and classical challenge. In this paper, the problem of training a classifier on
a dataset with missing features, and its application to a complete or
incomplete test dataset, is addressed. A supervised learning method is
developed to train a general classifier, such as a logistic regression or a
deep neural network, using only a limited number of features per sample, while
assuming sparse representations of data vectors on an unknown dictionary. The
pattern of missing features is allowed to be different for each input data
instance and can be either random or structured. The proposed method
simultaneously learns the classifier, the dictionary and the corresponding
sparse representation of each input data sample. A theoretical analysis is
provided, comparing this method with the standard imputation approach, which
consists of performing data completion followed by training the classifier with
those reconstructions. Sufficient conditions are identified such that, if it is
possible to train a classifier on incomplete observations so that their
reconstructions are well separated by a hyperplane, then the same classifier
also correctly separates the original (unobserved) data samples. Extensive
simulation results on synthetic and well-known reference datasets are presented
that validate our theoretical findings and demonstrate the effectiveness of the
proposed method compared to traditional data imputation approaches and one
state of the art algorithm.
</p>
<a href="http://arxiv.org/abs/2011.14047" target="_blank">arXiv:2011.14047</a> [<a href="http://arxiv.org/pdf/2011.14047" target="_blank">pdf</a>]

<h2>Is Support Set Diversity Necessary for Meta-Learning?. (arXiv:2011.14048v1 [cs.LG])</h2>
<h3>Amrith Setlur, Oscar Li, Virginia Smith</h3>
<p>Meta-learning is a popular framework for learning with limited data in which
an algorithm is produced by training over multiple few-shot learning tasks. For
classification problems, these tasks are typically constructed by sampling a
small number of support and query examples from a subset of the classes. While
conventional wisdom is that task diversity should improve the performance of
meta-learning, in this work we find evidence to the contrary: we propose a
modification to traditional meta-learning approaches in which we keep the
support sets fixed across tasks, thus reducing task diversity. Surprisingly, we
find that not only does this modification not result in adverse effects, it
almost always improves the performance for a variety of datasets and
meta-learning methods. We also provide several initial analyses to understand
this phenomenon. Our work serves to: (i) more closely investigate the effect of
support set construction for the problem of meta-learning, and (ii) suggest a
simple, general, and competitive baseline for few-shot learning.
</p>
<a href="http://arxiv.org/abs/2011.14048" target="_blank">arXiv:2011.14048</a> [<a href="http://arxiv.org/pdf/2011.14048" target="_blank">pdf</a>]

<h2>Uncertainty-Aware Physically-Guided Proxy Tasks for Unseen Domain Face Anti-spoofing. (arXiv:2011.14054v1 [cs.CV])</h2>
<h3>Junru Wu, Xiang Yu, Buyu Liu, Zhangyang Wang, Manmohan Chandraker</h3>
<p>Face anti-spoofing (FAS) seeks to discriminate genuine faces from fake ones
arising from any type of spoofing attack. Due to the wide varieties of attacks,
it is implausible to obtain training data that spans all attack types. We
propose to leverage physical cues to attain better generalization on unseen
domains. As a specific demonstration, we use physically guided proxy cues such
as depth, reflection, and material to complement our main anti-spoofing (a.k.a
liveness detection) task, with the intuition that genuine faces across domains
have consistent face-like geometry, minimal reflection, and skin material. We
introduce a novel uncertainty-aware attention scheme that independently learns
to weigh the relative contributions of the main and proxy tasks, preventing the
over-confident issue with traditional attention modules. Further, we propose
attribute-assisted hard negative mining to disentangle liveness-irrelevant
features with liveness features during learning. We evaluate extensively on
public benchmarks with intra-dataset and inter-dataset protocols. Our method
achieves the superior performance especially in unseen domain generalization
for FAS.
</p>
<a href="http://arxiv.org/abs/2011.14054" target="_blank">arXiv:2011.14054</a> [<a href="http://arxiv.org/pdf/2011.14054" target="_blank">pdf</a>]

<h2>Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v1 [cs.CV])</h2>
<h3>Zhongzhan Huang, Senwei Liang, Mingfu Liang, Wei He, Haizhao Yang</h3>
<p>Recently, many plug-and-play self-attention modules are proposed to enhance
the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). Previous works lay an emphasis on the
design of attention module for specific functionality, e.g., light-weighted or
task-oriented attention. However, they ignore the importance of where to plug
in the attention module since they connect the modules individually with each
block of the entire CNN backbone for granted, leading to incremental
computational cost and number of parameters with the growth of network depth.
Thus, we propose a framework called Efficient Attention Network (EAN) to
improve the efficiency for the existing attention modules. In EAN, we leverage
the sharing mechanism (Huang et al. 2020) to share the attention module within
the backbone and search where to connect the shared attention module via
reinforcement learning. Finally, we obtain the attention network with sparse
connections between the backbone and modules, while (1) maintaining accuracy
(2) reducing extra parameter increment and (3) accelerating inference.
Extensive experiments on widely-used benchmarks and popular attention networks
show the effectiveness of EAN. Furthermore, we empirically illustrate that our
EAN has the capacity of transferring to other tasks and capturing the
informative features. The code is available at
https://github.com/gbup-group/EAN-efficient-attention-network
</p>
<a href="http://arxiv.org/abs/2011.14058" target="_blank">arXiv:2011.14058</a> [<a href="http://arxiv.org/pdf/2011.14058" target="_blank">pdf</a>]

<h2>On Generalization of Adaptive Methods for Over-parameterized Linear Regression. (arXiv:2011.14066v1 [stat.ML])</h2>
<h3>Vatsal Shah, Soumya Basu, Anastasios Kyrillidis, Sujay Sanghavi</h3>
<p>Over-parameterization and adaptive methods have played a crucial role in the
success of deep learning in the last decade. The widespread use of
over-parameterization has forced us to rethink generalization by bringing forth
new phenomena, such as implicit regularization of optimization algorithms and
double descent with training progression. A series of recent works have started
to shed light on these areas in the quest to understand -- why do neural
networks generalize well? The setting of over-parameterized linear regression
has provided key insights into understanding this mysterious behavior of neural
networks.

In this paper, we aim to characterize the performance of adaptive methods in
the over-parameterized linear regression setting. First, we focus on two
sub-classes of adaptive methods depending on their generalization performance.
For the first class of adaptive methods, the parameter vector remains in the
span of the data and converges to the minimum norm solution like gradient
descent (GD). On the other hand, for the second class of adaptive methods, the
gradient rotation caused by the pre-conditioner matrix results in an in-span
component of the parameter vector that converges to the minimum norm solution
and the out-of-span component that saturates. Our experiments on
over-parameterized linear regression and deep neural networks support this
theory.
</p>
<a href="http://arxiv.org/abs/2011.14066" target="_blank">arXiv:2011.14066</a> [<a href="http://arxiv.org/pdf/2011.14066" target="_blank">pdf</a>]

<h2>Movement Tracks for the Automatic Detection of Fish Behavior in Videos. (arXiv:2011.14070v1 [cs.CV])</h2>
<h3>Declan McIntosh, Tunai Porto Marques, Alexandra Branzan Albu, Rodney Rountree, Fabio De Leo</h3>
<p>Global warming is predicted to profoundly impact ocean ecosystems. Fish
behavior is an important indicator of changes in such marine environments.
Thus, the automatic identification of key fish behavior in videos represents a
much needed tool for marine researchers, enabling them to study climate
change-related phenomena. We offer a dataset of sablefish (Anoplopoma fimbria)
startle behaviors in underwater videos, and investigate the use of deep
learning (DL) methods for behavior detection on it. Our proposed detection
system identifies fish instances using DL-based frameworks, determines
trajectory tracks, derives novel behavior-specific features, and employs Long
Short-Term Memory (LSTM) networks to identify startle behavior in sablefish.
Its performance is studied by comparing it with a state-of-the-art DL-based
video event detector.
</p>
<a href="http://arxiv.org/abs/2011.14070" target="_blank">arXiv:2011.14070</a> [<a href="http://arxiv.org/pdf/2011.14070" target="_blank">pdf</a>]

<h2>A Data-Driven Study of Commonsense Knowledge using the ConceptNet Knowledge Base. (arXiv:2011.14084v1 [cs.AI])</h2>
<h3>Ke Shen, Mayank Kejriwal</h3>
<p>Acquiring commonsense knowledge and reasoning is recognized as an important
frontier in achieving general Artificial Intelligence (AI). Recent research in
the Natural Language Processing (NLP) community has demonstrated significant
progress in this problem setting. Despite this progress, which is mainly on
multiple-choice question answering tasks in limited settings, there is still a
lack of understanding (especially at scale) of the nature of commonsense
knowledge itself. In this paper, we propose and conduct a systematic study to
enable a deeper understanding of commonsense knowledge by doing an empirical
and structural analysis of the ConceptNet knowledge base. ConceptNet is a
freely available knowledge base containing millions of commonsense assertions
presented in natural language. Detailed experimental results on three carefully
designed research questions, using state-of-the-art unsupervised graph
representation learning ('embedding') and clustering techniques, reveal deep
substructures in ConceptNet relations, allowing us to make data-driven and
computational claims about the meaning of phenomena such as 'context' that are
traditionally discussed only in qualitative terms. Furthermore, our methodology
provides a case study in how to use data-science and computational
methodologies for understanding the nature of an everyday (yet complex)
psychological phenomenon that is an essential feature of human intelligence.
</p>
<a href="http://arxiv.org/abs/2011.14084" target="_blank">arXiv:2011.14084</a> [<a href="http://arxiv.org/pdf/2011.14084" target="_blank">pdf</a>]

<h2>Deterministic Certification to Adversarial Attacks via Bernstein Polynomial Approximation. (arXiv:2011.14085v1 [cs.LG])</h2>
<h3>Ching-Chia Kao, Jhe-Bang Ko, Chun-Shien Lu</h3>
<p>Randomized smoothing has established state-of-the-art provable robustness
against $\ell_2$ norm adversarial attacks with high probability. However, the
introduced Gaussian data augmentation causes a severe decrease in natural
accuracy. We come up with a question, "Is it possible to construct a smoothed
classifier without randomization while maintaining natural accuracy?". We find
the answer is definitely yes. We study how to transform any classifier into a
certified robust classifier based on a popular and elegant mathematical tool,
Bernstein polynomial. Our method provides a deterministic algorithm for
decision boundary smoothing. We also introduce a distinctive approach of
norm-independent certified robustness via numerical solutions of nonlinear
systems of equations. Theoretical analyses and experimental results indicate
that our method is promising for classifier smoothing and robustness
certification.
</p>
<a href="http://arxiv.org/abs/2011.14085" target="_blank">arXiv:2011.14085</a> [<a href="http://arxiv.org/pdf/2011.14085" target="_blank">pdf</a>]

<h2>FreezeNet: Full Performance by Reduced Storage Costs. (arXiv:2011.14087v1 [cs.LG])</h2>
<h3>Paul Wimmer, Jens Mehnert, Alexandru Condurache</h3>
<p>Pruning generates sparse networks by setting parameters to zero. In this work
we improve one-shot pruning methods, applied before training, without adding
any additional storage costs while preserving the sparse gradient computations.
The main difference to pruning is that we do not sparsify the network's weights
but learn just a few key parameters and keep the other ones fixed at their
random initialized value. This mechanism is called freezing the parameters.
Those frozen weights can be stored efficiently with a single 32bit random seed
number. The parameters to be frozen are determined one-shot by a single for-
and backward pass applied before training starts. We call the introduced method
FreezeNet. In our experiments we show that FreezeNets achieve good results,
especially for extreme freezing rates. Freezing weights preserves the gradient
flow throughout the network and consequently, FreezeNets train better and have
an increased capacity compared to their pruned counterparts. On the
classification tasks MNIST and CIFAR-10/100 we outperform SNIP, in this setting
the best reported one-shot pruning method, applied before training. On MNIST,
FreezeNet achieves 99.2% performance of the baseline LeNet-5-Caffe
architecture, while compressing the number of trained and stored parameters by
a factor of x 157.
</p>
<a href="http://arxiv.org/abs/2011.14087" target="_blank">arXiv:2011.14087</a> [<a href="http://arxiv.org/pdf/2011.14087" target="_blank">pdf</a>]

<h2>Time-series Change Point Detection with Self-Supervised Contrastive Predictive Coding. (arXiv:2011.14097v1 [cs.LG])</h2>
<h3>Shohreh Deldari, Daniel V. Smith, Hao Xue, Flora D. Salim</h3>
<p>Change Point Detection techniques aim to capture changes in trends and
sequences in time-series data to describe the underlying behaviour of the
system. Detecting changes and anomalies in the web services, the trend of
applications usage can provide valuable insight towards the system, however,
many existing approaches are done in a supervised manner, requiring
well-labelled data. As the amount of data produced and captured by sensors are
growing rapidly, it is getting harder and even impossible to annotate the data.
Therefore, coming up with a self-supervised solution is a necessity these days.
In this work, we propose TSCP a novel self-supervised technique for temporal
change point detection, based on representation learning with Temporal
Convolutional Network (TCN). To the best of our knowledge, our proposed method
is the first method which employs Contrastive Learning for prediction with the
aim change point detection. Through extensive evaluations, we demonstrate that
our method outperforms multiple state-of-the-art change point detection and
anomaly detection baselines, including those adopting either unsupervised or
semi-supervised approach. TSCP is shown to improve both non-Deep learning- and
Deep learning-based methods by 0.28 and 0.12 in terms of average F1-score
across three datasets.
</p>
<a href="http://arxiv.org/abs/2011.14097" target="_blank">arXiv:2011.14097</a> [<a href="http://arxiv.org/pdf/2011.14097" target="_blank">pdf</a>]

<h2>Let's Hope it Works! Inaccurate Supervision of Neural Networks with Incorrect Labels: Application to Epilepsy. (arXiv:2011.14101v1 [cs.CV])</h2>
<h3>Florian Dubost, Erin Hong, Daniel Y Fu, Nandita Bhaskhar, Siyi Tang, Khaled Saab, Daniel Rubin, Jared Dunnmon, Christopher Lee-Messer</h3>
<p>This work describes multiple weak supervision strategies for video processing
with neural networks in the context of epilepsy. To study seizure onset,
researchers have designed automated methods to detect seizures from
electroencephalography (EEG), a modality used for recording electrical brain
activity. However, the EEG signal alone is sometimes not enough for existing
detection methods to discriminate seizure from artifacts having a similar
signal on EEG. For example, such artifacts could be triggered by patting,
rocking or suctioning in the case of neonates. In this article, we addressed
this problem by automatically detecting an example artifact-patting of neonates
-- from continuous video recordings of neonates acquired during clinical
routine. We computed frame-to-frame cross-correlation matrices to isolate
patterns showing repetitive movements indicative of patting. Next, a
convolutional neural network was trained to classify whether these matrices
contained patting events using weak training labels -- noisy labels generated
during normal clinical procedure. The labels were considered weak as they were
sometimes incorrect. We investigated whether networks trained with more
samples, containing more uncertain and weak labels, could achieve a higher
performance. Our results showed that, in the case of patting detection, such
networks could achieve a higher recall and focused on areas of the
cross-correlation matrices that were more meaningful to the task, without
sacrificing precision.
</p>
<a href="http://arxiv.org/abs/2011.14101" target="_blank">arXiv:2011.14101</a> [<a href="http://arxiv.org/pdf/2011.14101" target="_blank">pdf</a>]

<h2>Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs. (arXiv:2011.14107v1 [cs.CV])</h2>
<h3>Hui-Po Wang, Ning Yu, Mario Fritz</h3>
<p>While Generative Adversarial Networks (GANs) show increasing performance and
the level of realism is becoming indistinguishable from natural images, this
also comes with high demands on data and computation. We show that
state-of-the-art GAN models -- such as they are being publicly released by
researchers and industry -- can be used for a range of applications beyond
unconditional image generation. We achieve this by an iterative scheme that
also allows gaining control over the image generation process despite the
highly non-linear latent spaces of the latest GAN models. We demonstrate that
this opens up the possibility to re-use state-of-the-art, difficult to train,
pre-trained GANs with a high level of control even if only black-box access is
granted. Our work also raises concerns and awareness that the use cases of a
published GAN model may well reach beyond the creators' intention, which needs
to be taken into account before a full public release.
</p>
<a href="http://arxiv.org/abs/2011.14107" target="_blank">arXiv:2011.14107</a> [<a href="http://arxiv.org/pdf/2011.14107" target="_blank">pdf</a>]

<h2>Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules. (arXiv:2011.14115v1 [cs.LG])</h2>
<h3>Johannes Klicpera, Shankari Giri, Johannes T. Margraf, Stephan G&#xfc;nnemann</h3>
<p>Many important tasks in chemistry revolve around molecules during reactions.
This requires predictions far from the equilibrium, while most recent work in
machine learning for molecules has been focused on equilibrium or
near-equilibrium states. In this paper we aim to extend this scope in three
ways. First, we propose the DimeNet++ model, which is 8x faster and 10% more
accurate than the original DimeNet on the QM9 benchmark of equilibrium
molecules. Second, we validate DimeNet++ on highly reactive molecules by
developing the challenging COLL dataset, which contains distorted
configurations of small molecules during collisions. Finally, we investigate
ensembling and mean-variance estimation for uncertainty quantification with the
goal of accelerating the exploration of the vast space of non-equilibrium
structures. Our DimeNet++ implementation as well as the COLL dataset are
available online.
</p>
<a href="http://arxiv.org/abs/2011.14115" target="_blank">arXiv:2011.14115</a> [<a href="http://arxiv.org/pdf/2011.14115" target="_blank">pdf</a>]

<h2>Robotic grasp detection using a novel two-stage approach. (arXiv:2011.14123v1 [cs.RO])</h2>
<h3>Zhe Chu, Mengkai Hu, Xiangyu Chen</h3>
<p>Recently, deep learning has been successfully applied to robotic grasp
detection. Based on convolutional neural networks (CNNs), there have been lots
of end-to-end detection approaches. But end-to-end approaches have strict
requirements for the dataset used for training the neural network models and
it's hard to achieve in practical use. Therefore, we proposed a two-stage
approach using particle swarm optimizer (PSO) candidate estimator and CNN to
detect the most likely grasp. Our approach achieved an accuracy of 92.8% on the
Cornell Grasp Dataset, which leaped into the front ranks of the existing
approaches and is able to run at real-time speeds. After a small change of the
approach, we can predict multiple grasps per object in the meantime so that an
object can be grasped in a variety of ways.
</p>
<a href="http://arxiv.org/abs/2011.14123" target="_blank">arXiv:2011.14123</a> [<a href="http://arxiv.org/pdf/2011.14123" target="_blank">pdf</a>]

<h2>Human-Agent Cooperation in Bridge Bidding. (arXiv:2011.14124v1 [cs.AI])</h2>
<h3>Edward Lockhart, Neil Burch, Nolan Bard, Sebastian Borgeaud, Tom Eccles, Lucas Smaira, Ray Smith</h3>
<p>We introduce a human-compatible reinforcement-learning approach to a
cooperative game, making use of a third-party hand-coded human-compatible bot
to generate initial training data and to perform initial evaluation. Our
learning approach consists of imitation learning, search, and policy iteration.
Our trained agents achieve a new state-of-the-art for bridge bidding in three
settings: an agent playing in partnership with a copy of itself; an agent
partnering a pre-existing bot; and an agent partnering a human player.
</p>
<a href="http://arxiv.org/abs/2011.14124" target="_blank">arXiv:2011.14124</a> [<a href="http://arxiv.org/pdf/2011.14124" target="_blank">pdf</a>]

<h2>Risk-Monotonicity via Distributional Robustness. (arXiv:2011.14126v1 [cs.LG])</h2>
<h3>Zakaria Mhammedi, Hisham Husain</h3>
<p>Acquisition of data is a difficult task in most applications of Machine
Learning (ML), and it is only natural that one hopes and expects lower
populating risk (better performance) with increasing data points. It turns out,
somewhat surprisingly, that this is not the case even for the most standard
algorithms such as the Empirical Risk Minimizer (ERM). Non-monotonic behaviour
of the risk and instability in training have manifested and appeared in the
popular deep learning paradigm under the description of double descent. These
problems not only highlight our lack of understanding of learning algorithms
and generalization but rather render our efforts at data acquisition in vain.
It is, therefore, crucial to pursue this concern and provide a characterization
of such behaviour. In this paper, we derive the first consistent and
risk-monotonic algorithms for a general statistical learning setting under weak
assumptions, consequently resolving an open problem (Viering et al. 2019) on
how to avoid non-monotonic behaviour of risk curves. Our algorithms make use of
Distributionally Robust Optimization (DRO) -- a technique that has shown
promise in other complications of deep learning such as adversarial training.
Our work makes a significant contribution to the topic of risk-monotonicity,
which may be key in resolving empirical phenomena such as double descent.
</p>
<a href="http://arxiv.org/abs/2011.14126" target="_blank">arXiv:2011.14126</a> [<a href="http://arxiv.org/pdf/2011.14126" target="_blank">pdf</a>]

<h2>Towards Fast and Light-Weight Restoration of Dark Images. (arXiv:2011.14133v1 [cs.CV])</h2>
<h3>Mohit Lamba, Atul Balaji, Kaushik Mitra</h3>
<p>The ability to capture good quality images in the dark and near-zero lux
conditions has been a long-standing pursuit of the computer vision community.
The seminal work by Chen et al. [5] has especially caused renewed interest in
this area, resulting in methods that build on top of their work in a bid to
improve the reconstruction. However, for practical utility and deployment of
low-light enhancement algorithms on edge devices such as embedded systems,
surveillance cameras, autonomous robots and smartphones, the solution must
respect additional constraints such as limited GPU memory and processing power.
With this in mind, we propose a deep neural network architecture that aims to
strike a balance between the network latency, memory utilization, model
parameters, and reconstruction quality. The key idea is to forbid computations
in the High-Resolution (HR) space and limit them to a Low-Resolution (LR)
space. However, doing the bulk of computations in the LR space causes artifacts
in the restored image. We thus propose Pack and UnPack operations, which allow
us to effectively transit between the HR and LR spaces without incurring much
artifacts in the restored image. We show that we can enhance a full resolution,
2848 x 4256, extremely dark single-image in the ballpark of 3 seconds even on a
CPU. We achieve this with 2 - 7x fewer model parameters, 2 - 3x lower memory
utilization, 5 - 20x speed up and yet maintain a competitive image
reconstruction quality compared to the state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/2011.14133" target="_blank">arXiv:2011.14133</a> [<a href="http://arxiv.org/pdf/2011.14133" target="_blank">pdf</a>]

<h2>Short-Term Load Forecasting using Bi-directional Sequential Models and Feature Engineering for Small Datasets. (arXiv:2011.14137v1 [cs.LG])</h2>
<h3>Abdul Wahab, Muhammad Anas Tahir, Naveed Iqbal, Faisal Shafait, Syed Muhammad Raza Kazmi</h3>
<p>Electricity load forecasting enables the grid operators to optimally
implement the smart grid's most essential features such as demand response and
energy efficiency. Electricity demand profiles can vary drastically from one
region to another on diurnal, seasonal and yearly scale. Hence to devise a load
forecasting technique that can yield the best estimates on diverse datasets,
specially when the training data is limited, is a big challenge. This paper
presents a deep learning architecture for short-term load forecasting based on
bidirectional sequential models in conjunction with feature engineering that
extracts the hand-crafted derived features in order to aid the model for better
learning and predictions. In the proposed architecture, named as Deep Derived
Feature Fusion (DeepDeFF), the raw input and hand-crafted features are trained
at separate levels and then their respective outputs are combined to make the
final prediction. The efficacy of the proposed methodology is evaluated on
datasets from five countries with completely different patterns. The results
demonstrate that the proposed technique is superior to the existing state of
the art.
</p>
<a href="http://arxiv.org/abs/2011.14137" target="_blank">arXiv:2011.14137</a> [<a href="http://arxiv.org/pdf/2011.14137" target="_blank">pdf</a>]

<h2>AdaBins: Depth Estimation using Adaptive Bins. (arXiv:2011.14141v1 [cs.CV])</h2>
<h3>Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka</h3>
<p>We address the problem of estimating a high quality dense depth map from a
single RGB input image. We start out with a baseline encoder-decoder
convolutional neural network architecture and pose the question of how the
global processing of information can help improve overall depth estimation. To
this end, we propose a transformer-based architecture block that divides the
depth range into bins whose center value is estimated adaptively per image. The
final depth values are estimated as linear combinations of the bin centers. We
call our new building block AdaBins. Our results show a decisive improvement
over the state-of-the-art on several popular depth datasets across all metrics.
We also validate the effectiveness of the proposed block with an ablation study
and provide the code and corresponding pre-trained weights of the new
state-of-the-art model.
</p>
<a href="http://arxiv.org/abs/2011.14141" target="_blank">arXiv:2011.14141</a> [<a href="http://arxiv.org/pdf/2011.14141" target="_blank">pdf</a>]

<h2>i3DMM: Deep Implicit 3D Morphable Model of Human Heads. (arXiv:2011.14143v1 [cs.CV])</h2>
<h3>Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt</h3>
<p>We present the first deep implicit 3D morphable model (i3DMM) of full heads.
Unlike earlier morphable face models it not only captures identity-specific
geometry, texture, and expressions of the frontal face, but also models the
entire head, including hair. We collect a new dataset consisting of 64 people
with different expressions and hairstyles to train i3DMM. Our approach has the
following favorable properties: (i) It is the first full head morphable model
that includes hair. (ii) In contrast to mesh-based models it can be trained on
merely rigidly aligned scans, without requiring difficult non-rigid
registration. (iii) We design a novel architecture to decouple the shape model
into an implicit reference shape and a deformation of this reference shape.
With that, dense correspondences between shapes can be learned implicitly. (iv)
This architecture allows us to semantically disentangle the geometry and color
components, as color is learned in the reference space. Geometry is further
disentangled as identity, expressions, and hairstyle, while color is
disentangled as identity and hairstyle components. We show the merits of i3DMM
using ablation studies, comparisons to state-of-the-art models, and
applications such as semantic head editing and texture transfer. We will make
our model publicly available.
</p>
<a href="http://arxiv.org/abs/2011.14143" target="_blank">arXiv:2011.14143</a> [<a href="http://arxiv.org/pdf/2011.14143" target="_blank">pdf</a>]

<h2>Uncertainty Quantification in Deep Learning through Stochastic Maximum Principle. (arXiv:2011.14145v1 [cs.LG])</h2>
<h3>Richard Archibald, Feng Bao, Yanzhao Cao, He Zhang</h3>
<p>We develop a probabilistic machine learning method, which formulates a class
of stochastic neural networks by a stochastic optimal control problem. An
efficient stochastic gradient descent algorithm is introduced under the
stochastic maximum principle framework. Convergence analysis for stochastic
gradient descent optimization and numerical experiments for applications of
stochastic neural networks are carried out to validate our methodology in both
theory and performance.
</p>
<a href="http://arxiv.org/abs/2011.14145" target="_blank">arXiv:2011.14145</a> [<a href="http://arxiv.org/pdf/2011.14145" target="_blank">pdf</a>]

<h2>Batch Normalization with Enhanced Linear Transformation. (arXiv:2011.14150v1 [cs.CV])</h2>
<h3>Yuhui Xu, Lingxi Xie, Cihang Xie, Jieru Mei, Siyuan Qiao, Wei Shen, Hongkai Xiong, Alan Yuille</h3>
<p>Batch normalization (BN) is a fundamental unit in modern deep networks, in
which a linear transformation module was designed for improving BN's
flexibility of fitting complex data distributions. In this paper, we
demonstrate properly enhancing this linear transformation module can
effectively improve the ability of BN. Specifically, rather than using a single
neuron, we propose to additionally consider each neuron's neighborhood for
calculating the outputs of the linear transformation. Our method, named BNET,
can be implemented with 2-3 lines of code in most deep learning libraries.
Despite the simplicity, BNET brings consistent performance gains over a wide
range of backbones and visual benchmarks. Moreover, we verify that BNET
accelerates the convergence of network training and enhances spatial
information by assigning the important neurons with larger weights accordingly.
The code is available at https://github.com/yuhuixu1993/BNET.
</p>
<a href="http://arxiv.org/abs/2011.14150" target="_blank">arXiv:2011.14150</a> [<a href="http://arxiv.org/pdf/2011.14150" target="_blank">pdf</a>]

<h2>Towards Robust Medical Image Segmentation on Small-Scale Data with Incomplete Labels. (arXiv:2011.14164v1 [cs.CV])</h2>
<h3>Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Min Xu, Irina Voiculescu, Eric P. Xing</h3>
<p>The data-driven nature of deep learning models for semantic segmentation
requires a large number of pixel-level annotations. However, large-scale and
fully labeled medical datasets are often unavailable for practical tasks.
Recently, partially supervised methods have been proposed to utilize images
with incomplete labels to mitigate the data scarcity problem in the medical
domain. As an emerging research area, the breakthroughs made by existing
methods rely on either large-scale data or complex model design, which makes
them 1) less practical for certain real-life tasks and 2) less robust for
small-scale data. It is time to step back and think about the robustness of
partially supervised methods and how to maximally utilize small-scale and
partially labeled data for medical image segmentation tasks. To bridge the
methodological gaps in label-efficient deep learning with partial supervision,
we propose RAMP, a simple yet efficient data augmentation framework for
partially supervised medical image segmentation by exploiting the assumption
that patients share anatomical similarities. We systematically evaluate RAMP
and the previous methods in various controlled multi-structure segmentation
tasks. Compared to the mainstream approaches, RAMP consistently improves the
performance of traditional segmentation networks on small-scale partially
labeled data and utilize additional image-wise weak annotations.
</p>
<a href="http://arxiv.org/abs/2011.14164" target="_blank">arXiv:2011.14164</a> [<a href="http://arxiv.org/pdf/2011.14164" target="_blank">pdf</a>]

<h2>E-Pro: Euler Angle and Probabilistic Model for Face Detection and Recognition. (arXiv:2011.14200v1 [cs.CV])</h2>
<h3>Sandesh Ramesh, Manoj Kumar M V, Sanjay H A</h3>
<p>It is human nature to give prime importance to facial appearances. Often, to
look good is to feel good. Also, facial features are unique to every individual
on this planet, which means it is a source of vital information. This work
proposes a framework named E-Pro for the detection and recognition of faces by
taking facial images as inputs. E-Pro has its potential application in various
domains, namely attendance, surveillance, crowd monitoring, biometric-based
authentication etc. E-Pro is developed here as a mobile application that aims
to aid lecturers to mark attendance in a classroom by detecting and recognizing
the faces of students from a picture clicked through the app. E-Pro has been
developed using Google Firebase Face Recognition APIs, which uses Euler Angles,
and Probabilistic Model. E-Pro has been tested on stock images and the
experimental results are promising.
</p>
<a href="http://arxiv.org/abs/2011.14200" target="_blank">arXiv:2011.14200</a> [<a href="http://arxiv.org/pdf/2011.14200" target="_blank">pdf</a>]

<h2>Class-agnostic Object Detection. (arXiv:2011.14204v1 [cs.CV])</h2>
<h3>Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Premkumar Natarajan</h3>
<p>Object detection models perform well at localizing and classifying objects
that they are shown during training. However, due to the difficulty and cost
associated with creating and annotating detection datasets, trained models
detect a limited number of object types with unknown objects treated as
background content. This hinders the adoption of conventional detectors in
real-world applications like large-scale object matching, visual grounding,
visual relation prediction, obstacle detection (where it is more important to
determine the presence and location of objects than to find specific types),
etc. We propose class-agnostic object detection as a new problem that focuses
on detecting objects irrespective of their object-classes. Specifically, the
goal is to predict bounding boxes for all objects in an image but not their
object-classes. The predicted boxes can then be consumed by another system to
perform application-specific classification, retrieval, etc. We propose
training and evaluation protocols for benchmarking class-agnostic detectors to
advance future research in this domain. Finally, we propose (1) baseline
methods and (2) a new adversarial learning framework for class-agnostic
detection that forces the model to exclude class-specific information from
features used for predictions. Experimental results show that adversarial
learning improves class-agnostic detection efficacy.
</p>
<a href="http://arxiv.org/abs/2011.14204" target="_blank">arXiv:2011.14204</a> [<a href="http://arxiv.org/pdf/2011.14204" target="_blank">pdf</a>]

<h2>AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy. (arXiv:2011.14206v1 [cs.RO])</h2>
<h3>Zhenjia Xu, Beichun Qi, Shubham Agrawal, Shuran Song</h3>
<p>This paper aims to improve robots' versatility and adaptability by allowing
them to use a large variety of end-effector tools and quickly adapt to new
tools. We propose AdaGrasp, a method to learn a single grasping policy that
generalizes to novel grippers. By training on a large collection of grippers,
our algorithm is able to acquire generalizable knowledge of how different
grippers should be used in various tasks. Given a visual observation of the
scene and the gripper, AdaGrasp infers the possible grasping poses and their
grasp scores by computing the cross convolution between the shape encodings of
the input gripper and scene. Intuitively, this cross convolution operation can
be considered as an efficient way of exhaustively matching the scene geometry
with gripper geometry under different grasp poses (i.e., translations and
orientations), where a good "match" of 3D geometry will lead to a successful
grasp. We validate our methods in both simulation and real-world environment.
Our experiment shows that AdaGrasp significantly outperforms the existing
multi-gripper grasping policy method, especially when handling cluttered
environments and partial observations. Video is available at
https://youtu.be/MUawdWnQDyQ
</p>
<a href="http://arxiv.org/abs/2011.14206" target="_blank">arXiv:2011.14206</a> [<a href="http://arxiv.org/pdf/2011.14206" target="_blank">pdf</a>]

<h2>Curvature Regularization to Prevent Distortion in Graph Embedding. (arXiv:2011.14211v1 [cs.LG])</h2>
<h3>Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Chunxu Zhang, Bo Yang</h3>
<p>Recent research on graph embedding has achieved success in various
applications. Most graph embedding methods preserve the proximity in a graph
into a manifold in an embedding space. We argue an important but neglected
problem about this proximity-preserving strategy: Graph topology patterns,
while preserved well into an embedding manifold by preserving proximity, may
distort in the ambient embedding Euclidean space, and hence to detect them
becomes difficult for machine learning models. To address the problem, we
propose curvature regularization, to enforce flatness for embedding manifolds,
thereby preventing the distortion. We present a novel angle-based sectional
curvature, termed ABS curvature, and accordingly three kinds of curvature
regularization to induce flat embedding manifolds during graph embedding. We
integrate curvature regularization into five popular proximity-preserving
embedding methods, and empirical results in two applications show significant
improvements on a wide range of open graph datasets.
</p>
<a href="http://arxiv.org/abs/2011.14211" target="_blank">arXiv:2011.14211</a> [<a href="http://arxiv.org/pdf/2011.14211" target="_blank">pdf</a>]

<h2>Truly shift-invariant convolutional neural networks. (arXiv:2011.14214v1 [cs.CV])</h2>
<h3>Anadi Chaman (1), Ivan Dokmani&#x107; (2) ((1) University of Illinois at Urbana-Champaign, (2) University of Basel)</h3>
<p>Thanks to the use of convolution and pooling layers, convolutional neural
networks were for a long time thought to be shift-invariant. However, recent
works have shown that the output of a CNN can change significantly with small
shifts in input: a problem caused by the presence of downsampling (stride)
layers. The existing solutions rely either on data augmentation or on
anti-aliasing, both of which have limitations and neither of which enables
perfect shift invariance. Additionally, the gains obtained from these methods
do not extend to image patterns not seen during training. To address these
challenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling
scheme that allows convolutional neural networks to achieve 100% consistency in
classification performance under shifts, without any loss in accuracy. With APS
the networks exhibit perfect consistency to shifts even before training, making
it the first approach that makes convolutional neural networks truly shift
invariant.
</p>
<a href="http://arxiv.org/abs/2011.14214" target="_blank">arXiv:2011.14214</a> [<a href="http://arxiv.org/pdf/2011.14214" target="_blank">pdf</a>]

<h2>FaceGuard: A Self-Supervised Defense Against Adversarial Face Images. (arXiv:2011.14218v1 [cs.CV])</h2>
<h3>Debayan Deb, Xiaoming Liu, Anil K. Jain</h3>
<p>Prevailing defense mechanisms against adversarial face images tend to overfit
to the adversarial perturbations in the training set and fail to generalize to
unseen adversarial attacks. We propose a new self-supervised adversarial
defense framework, namely FaceGuard, that can automatically detect, localize,
and purify a wide variety of adversarial faces without utilizing pre-computed
adversarial training samples. During training, FaceGuard automatically
synthesizes challenging and diverse adversarial attacks, enabling a classifier
to learn to distinguish them from real faces and a purifier attempts to remove
the adversarial perturbations in the image space. Experimental results on LFW
dataset show that FaceGuard can achieve 99.81% detection accuracy on six unseen
adversarial attack types. In addition, the proposed method can enhance the face
recognition performance of ArcFace from 34.27% TAR @ 0.1% FAR under no defense
to 77.46% TAR @ 0.1% FAR.
</p>
<a href="http://arxiv.org/abs/2011.14218" target="_blank">arXiv:2011.14218</a> [<a href="http://arxiv.org/pdf/2011.14218" target="_blank">pdf</a>]

<h2>Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models. (arXiv:2011.14238v1 [stat.ML])</h2>
<h3>Amy X. Zhang, Le Bao, Michael J. Daniels</h3>
<p>We introduce a novel procedure for obtaining cross-validated predictive
estimates for Bayesian hierarchical regression models (BHRMs). Bayesian
hierarchical models are popular for their ability to model complex dependence
structures and provide probabilistic uncertainty estimates, but can be
computationally expensive to run. Cross-validation (CV) is therefore not a
common practice to evaluate the predictive performance of BHRMs. Our method
circumvents the need to re-run computationally costly estimation methods for
each cross-validation fold and makes CV more feasible for large BHRMs. By
conditioning on the variance-covariance parameters, we shift the CV problem
from probability-based sampling to a simple and familiar optimization problem.
In many cases, this produces estimates which are equivalent to full CV. We
provide theoretical results and demonstrate its efficacy on publicly available
data and in simulations.
</p>
<a href="http://arxiv.org/abs/2011.14238" target="_blank">arXiv:2011.14238</a> [<a href="http://arxiv.org/pdf/2011.14238" target="_blank">pdf</a>]

<h2>HEDRA: A Bio-Inspired Modular Tensegrity Soft Robot With Polyhedral Parallel Modules. (arXiv:2011.14240v1 [cs.RO])</h2>
<h3>Vishal Ramadoss, Keerthi Sagar, Mohamed Sadiq Ikbal, Jesus Hiram Lugo Calles, Matteo Zoppi</h3>
<p>There is a surge of research interest in the field of tensegrity robotics.
Robots developed under this paradigm provide many advantages and have
distinguishing features in terms of structural compliance, dexterity, safety,
and weight reduction. This paper proposes a new robotic mechanism based on
tensegrity ('tension-integrity') robots and reconfigurable modular robots. The
specific actuation schemes for this tensegrity robot with multiple degrees of
freedom are presented. This article describes an easy-to-assemble 350 mm
tensegrity based robot prototype by stacking a series of rigid struts linked
with tensegrity joints that have no direct rigid contact with each other. The
functionality of the proposed robot is validated by the experimental results by
integrating the polyhedral parallel structure as its skeleton and series of
tensegrity joints. The proposed manipulator is capable of reaching bending
angles up to 76 degrees. An adaptive cable driven underactuated robotic gripper
is designed and attached to the tensegrity manipulator for grasping objects in
different shapes, weights, and sizes.
</p>
<a href="http://arxiv.org/abs/2011.14240" target="_blank">arXiv:2011.14240</a> [<a href="http://arxiv.org/pdf/2011.14240" target="_blank">pdf</a>]

<h2>Monte Carlo Tree Search for a single target search game on a 2-D lattice. (arXiv:2011.14246v1 [cs.LG])</h2>
<h3>Elana Kozak, Scott Hottovy</h3>
<p>Monte Carlo Tree Search (MCTS) is a branch of stochastic modeling that
utilizes decision trees for optimization, mostly applied to artificial
intelligence (AI) game players. This project imagines a game in which an AI
player searches for a stationary target within a 2-D lattice. We analyze its
behavior with different target distributions and compare its efficiency to the
Levy Flight Search, a model for animal foraging behavior. In addition to
simulated data analysis we prove two theorems about the convergence of MCTS
when computation constraints neglected.
</p>
<a href="http://arxiv.org/abs/2011.14246" target="_blank">arXiv:2011.14246</a> [<a href="http://arxiv.org/pdf/2011.14246" target="_blank">pdf</a>]

<h2>Importance Weight Estimation and Generalization in Domain Adaptation under Label Shift. (arXiv:2011.14251v1 [cs.LG])</h2>
<h3>Kamyar Azizzadenesheli</h3>
<p>We study generalization under label shift in domain adaptation where the
learner has access to labeled samples from the source domain but unlabeled
samples from the target domain. Prior works deploy label classifiers and
introduce various methods to estimate the importance weights from source to
target domains. They use these estimates in importance weighted empirical risk
minimization to learn classifiers. In this work, we theoretically compare the
prior approaches, relax their strong assumptions, and generalize them from
requiring label classifiers to general functions. This latter generalization
improves the conditioning on the inverse operator of the induced inverse
problems by allowing for broader exploitation of the spectrum of the forward
operator.

The prior works in the study of label shifts are limited to categorical label
spaces. In this work, we propose a series of methods to estimate the importance
weight functions for arbitrary normed label spaces. We introduce a new operator
learning approach between Hilbert spaces defined on labels (rather than
covariates) and show that it induces a perturbed inverse problem of compact
operators. We propose a novel approach to solve the inverse problem in the
presence of perturbation. This analysis has its own independent interest since
such problems commonly arise in partial differential equations and
reinforcement learning.

For both categorical and general normed spaces, we provide concentration
bounds for the proposed estimators. Using the existing generalization analysis
based on Rademacher complexity, R\'enyi divergence, and MDFR lemma in
Azizzadenesheli et al. [2019], we show the generalization property of the
importance weighted empirical risk minimization on the unseen target domain.
</p>
<a href="http://arxiv.org/abs/2011.14251" target="_blank">arXiv:2011.14251</a> [<a href="http://arxiv.org/pdf/2011.14251" target="_blank">pdf</a>]

<h2>Distilled Thompson Sampling: Practical and Efficient Thompson Sampling via Imitation Learning. (arXiv:2011.14266v1 [cs.LG])</h2>
<h3>Hongseok Namkoong, Samuel Daulton, Eytan Bakshy</h3>
<p>Thompson sampling (TS) has emerged as a robust technique for contextual
bandit problems. However, TS requires posterior inference and optimization for
action generation, prohibiting its use in many internet applications where
latency and ease of deployment are of concern. We propose a novel
imitation-learning-based algorithm that distills a TS policy into an explicit
policy representation by performing posterior inference and optimization
offline. The explicit policy representation enables fast online decision-making
and easy deployment in mobile and server-based environments. Our algorithm
iteratively performs offline batch updates to the TS policy and learns a new
imitation policy. Since we update the TS policy with observations collected
under the imitation policy, our algorithm emulates an off-policy version of TS.
Our imitation algorithm guarantees Bayes regret comparable to TS, up to the sum
of single-step imitation errors. We show these imitation errors can be made
arbitrarily small when unlabeled contexts are cheaply available, which is the
case for most large-scale internet applications. Empirically, we show that our
imitation policy achieves comparable regret to TS, while reducing decision-time
latency by over an order of magnitude. Our algorithm is deployed in video
upload systems at Facebook and Instagram and is handling millions of uploads
each day.
</p>
<a href="http://arxiv.org/abs/2011.14266" target="_blank">arXiv:2011.14266</a> [<a href="http://arxiv.org/pdf/2011.14266" target="_blank">pdf</a>]

<h2>Minimax Sample Complexity for Turn-based Stochastic Game. (arXiv:2011.14267v1 [cs.LG])</h2>
<h3>Qiwen Cui, Lin F. Yang</h3>
<p>The empirical success of Multi-agent reinforcement learning is encouraging,
while few theoretical guarantees have been revealed. In this work, we prove
that the plug-in solver approach, probably the most natural reinforcement
learning algorithm, achieves minimax sample complexity for turn-based
stochastic game (TBSG). Specifically, we plan in an empirical TBSG by utilizing
a `simulator' that allows sampling from arbitrary state-action pair. We show
that the empirical Nash equilibrium strategy is an approximate Nash equilibrium
strategy in the true TBSG and give both problem-dependent and
problem-independent bound. We develop absorbing TBSG and reward perturbation
techniques to tackle the complex statistical dependence. The key idea is
artificially introducing a suboptimality gap in TBSG and then the Nash
equilibrium strategy lies in a finite set.
</p>
<a href="http://arxiv.org/abs/2011.14267" target="_blank">arXiv:2011.14267</a> [<a href="http://arxiv.org/pdf/2011.14267" target="_blank">pdf</a>]

<h2>Generalization and Memorization: The Bias Potential Model. (arXiv:2011.14269v1 [stat.ML])</h2>
<h3>Hongkang Yang, E Weinan</h3>
<p>Models for learning probability distributions such as generative models and
density estimators behave quite differently from models for learning functions.
One example is found in the memorization phenomenon, namely the ultimate
convergence to the empirical distribution, that occurs in generative
adversarial networks (GANs). For this reason, the issue of generalization is
more subtle than that for supervised learning. For the bias potential model, we
show that dimension-independent generalization accuracy is achievable if early
stopping is adopted, despite that in the long term, the model either memorizes
the samples or diverges. The generality of our arguments indicates that this
slow deterioration from generalization to memorization might be common to
distribution-learning models in general.
</p>
<a href="http://arxiv.org/abs/2011.14269" target="_blank">arXiv:2011.14269</a> [<a href="http://arxiv.org/pdf/2011.14269" target="_blank">pdf</a>]

<h2>Multi-task GANs for Semantic Segmentation and Depth Completion with Cycle Consistency. (arXiv:2011.14272v1 [cs.CV])</h2>
<h3>Chongzhen Zhang, Yang Tang, Chaoqiang Zhao, Qiyu Sun, Zhencheng Ye, J&#xfc;rgen Kurths</h3>
<p>Semantic segmentation and depth completion are two challenging tasks in scene
understanding, and they are widely used in robotics and autonomous driving.
Although several works are proposed to jointly train these two tasks using some
small modifications, like changing the last layer, the result of one task is
not utilized to improve the performance of the other one despite that there are
some similarities between these two tasks. In this paper, we propose multi-task
generative adversarial networks (Multi-task GANs), which are not only competent
in semantic segmentation and depth completion, but also improve the accuracy of
depth completion through generated semantic images. In addition, we improve the
details of generated semantic images based on CycleGAN by introducing
multi-scale spatial pooling blocks and the structural similarity reconstruction
loss. Furthermore, considering the inner consistency between semantic and
geometric structures, we develop a semantic-guided smoothness loss to improve
depth completion results. Extensive experiments on Cityscapes dataset and KITTI
depth completion benchmark show that the Multi-task GANs are capable of
achieving competitive performance for both semantic segmentation and depth
completion tasks.
</p>
<a href="http://arxiv.org/abs/2011.14272" target="_blank">arXiv:2011.14272</a> [<a href="http://arxiv.org/pdf/2011.14272" target="_blank">pdf</a>]

<h2>UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information. (arXiv:2011.14284v1 [cs.CV])</h2>
<h3>Girisha S, Ujjwal Verma, Manohara Pai M M, Radhika Pai</h3>
<p>Semantic segmentation of aerial videos has been extensively used for decision
making in monitoring environmental changes, urban planning, and disaster
management. The reliability of these decision support systems is dependent on
the accuracy of the video semantic segmentation algorithms. The existing CNN
based video semantic segmentation methods have enhanced the image semantic
segmentation methods by incorporating an additional module such as LSTM or
optical flow for computing temporal dynamics of the video which is a
computational overhead. The proposed research work modifies the CNN
architecture by incorporating temporal information to improve the efficiency of
video semantic segmentation.

In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net)
is proposed for UAV video semantic segmentation. The encoder of the proposed
architecture embeds temporal information for temporally consistent labelling.
The decoder is enhanced by introducing the feature retainer module, which aids
in the accurate localization of the class labels. The proposed UVid-Net
architecture for UAV video semantic segmentation is quantitatively evaluated on
an extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been
observed which is significantly greater than the other state-of-the-art
algorithms. Further, the proposed work produced promising results even for the
pre-trained model of UVid-Net on the urban street scene with fine-tuning the
final layer on UAV aerial videos.
</p>
<a href="http://arxiv.org/abs/2011.14284" target="_blank">arXiv:2011.14284</a> [<a href="http://arxiv.org/pdf/2011.14284" target="_blank">pdf</a>]

<h2>Deeper or Wider Networks of Point Clouds with Self-attention?. (arXiv:2011.14285v1 [cs.CV])</h2>
<h3>Haoxi Ran, Li Lu</h3>
<p>Prevalence of deeper networks driven by self-attention is in stark contrast
to underexplored point-based methods. In this paper, we propose groupwise
self-attention as the basic block to construct our network: SepNet. Our
proposed module can effectively capture both local and global dependencies.
This module computes the features of a group based on the summation of the
weighted features of any point within the group. For convenience, we generalize
groupwise operations to assemble this module. To further facilitate our
networks, we deepen and widen SepNet on the tasks of segmentation and
classification respectively, and verify its practicality. Specifically, SepNet
achieves state-of-the-art for the tasks of classification and segmentation on
most of the datasets. We show empirical evidence that SepNet can obtain extra
accuracy in classification or segmentation from increased width or depth,
respectively.
</p>
<a href="http://arxiv.org/abs/2011.14285" target="_blank">arXiv:2011.14285</a> [<a href="http://arxiv.org/pdf/2011.14285" target="_blank">pdf</a>]

<h2>Learning Affinity-Aware Upsampling for Deep Image Matting. (arXiv:2011.14288v1 [cs.CV])</h2>
<h3>Yutong Dai, Hao Lu, Chunhua Shen</h3>
<p>We show that learning affinity in upsampling provides an effective and
efficient approach to exploit pairwise interactions in deep networks.
Second-order features are commonly used in dense prediction to build adjacent
relations with a learnable module after upsampling such as non-local blocks.
Since upsampling is essential, learning affinity in upsampling can avoid
additional propagation layers, offering the potential for building compact
models. By looking at existing upsampling operators from a unified mathematical
perspective, we generalize them into a second-order form and introduce
Affinity-Aware Upsampling (A2U) where upsampling kernels are generated using a
light-weight lowrank bilinear model and are conditioned on second-order
features. Our upsampling operator can also be extended to downsampling. We
discuss alternative implementations of A2U and verify their effectiveness on
two detail-sensitive tasks: image reconstruction on a toy dataset; and a
largescale image matting task where affinity-based ideas constitute mainstream
matting approaches. In particular, results on the Composition-1k matting
dataset show that A2U achieves a 14% relative improvement in the SAD metric
against a strong baseline with negligible increase of parameters (&lt;0.5%).
Compared with the state-of-the-art matting network, we achieve 8% higher
performance with only 40% model complexity.
</p>
<a href="http://arxiv.org/abs/2011.14288" target="_blank">arXiv:2011.14288</a> [<a href="http://arxiv.org/pdf/2011.14288" target="_blank">pdf</a>]

<h2>Learning geometry-image representation for 3D point cloud generation. (arXiv:2011.14289v1 [cs.CV])</h2>
<h3>Lei Wang, Yuchun Huang, Pengjie Tao, Yaolin Hou, Yuxuan Liu</h3>
<p>We study the problem of generating point clouds of 3D objects. Instead of
discretizing the object into 3D voxels with huge computational cost and
resolution limitations, we propose a novel geometry image based generator (GIG)
to convert the 3D point cloud generation problem to a 2D geometry image
generation problem. Since the geometry image is a completely regular 2D array
that contains the surface points of the 3D object, it leverages both the
regularity of the 2D array and the geodesic neighborhood of the 3D surface.
Thus, one significant benefit of our GIG is that it allows us to directly
generate the 3D point clouds using efficient 2D image generation networks.
Experiments on both rigid and non-rigid 3D object datasets have demonstrated
the promising performance of our method to not only create plausible and novel
3D objects, but also learn a probabilistic latent space that well supports the
shape editing like interpolation and arithmetic.
</p>
<a href="http://arxiv.org/abs/2011.14289" target="_blank">arXiv:2011.14289</a> [<a href="http://arxiv.org/pdf/2011.14289" target="_blank">pdf</a>]

<h2>A method for large diffeomorphic registration via broken geodesics. (arXiv:2011.14298v1 [cs.CV])</h2>
<h3>Alphin J. Thottupattu, Jayanthi Sivaswamy, Venkateswaran P. Krishnan</h3>
<p>Anatomical variabilities seen in longitudinal data or inter-subject data is
usually described by the underlying deformation, captured by non-rigid
registration of these images. Stationary Velocity Field (SVF) based non-rigid
registration algorithms are widely used for registration. SVF based methods
form a metric-free framework which captures a finite dimensional submanifold of
deformations embedded in the infinite dimensional smooth manifold of
diffeomorphisms. However, these methods cover only a limited degree of
deformations. In this paper, we address this limitation and define an
approximate metric space for the manifold of diffeomorphisms $\mathcal{G}$. We
propose a method to break down the large deformation into finite compositions
of small deformations. This results in a broken geodesic path on $\mathcal{G}$
and its length now forms an approximate registration metric. We illustrate the
method using a simple, intensity-based, log-demon implementation. Validation
results of the proposed method show that it can capture large and complex
deformations while producing qualitatively better results than the
state-of-the-art methods. The results also demonstrate that the proposed
registration metric is a good indicator of the degree of deformation.
</p>
<a href="http://arxiv.org/abs/2011.14298" target="_blank">arXiv:2011.14298</a> [<a href="http://arxiv.org/pdf/2011.14298" target="_blank">pdf</a>]

<h2>Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images. (arXiv:2011.14302v1 [cs.CV])</h2>
<h3>Rui Li, Jianlin Su, Chenxi Duan, Shunyi Zheng</h3>
<p>The memory and computation costs of the dot-product attention mechanism
widely used in vision and language tasks increase quadratically with the
spatio-temporal size of the input. Such growth hinders the usage of attention
mechanisms in application scenarios with large inputs. In this Letter, to
remedy this deficiency, we propose a Linear Attention Mechanism (LAM) which is
approximate to dot-product attention with dramatically less memory and
computation costs. The efficient design makes the incorporation between
attention mechanisms and neural networks more flexible and versatile. Based on
the proposed LAM, we refactor the skip connections in the raw U-Net and design
a Multistage Attention ResU-Net (MAResU-Net) for semantic segmentation using
fine-resolution remote sensing images. Experiments conducted on the Vaihingen
dataset demonstrated the effectiveness of our MAResU-Net. Code is available at
https://github.com/lironui/Multistage-Attention-ResU-Net.
</p>
<a href="http://arxiv.org/abs/2011.14302" target="_blank">arXiv:2011.14302</a> [<a href="http://arxiv.org/pdf/2011.14302" target="_blank">pdf</a>]

<h2>Image-based plant disease diagonasis with unsupervised anomaly detection based on reconstructability of colors. (arXiv:2011.14306v1 [cs.CV])</h2>
<h3>Ryoya Katafuchi, Terumasa Tokunaga</h3>
<p>This paper proposes an unsupervised anomaly detection technique for
image-based plant disease diagnosis. A construction of large and openly
available data set on labeled images of healthy and diseased crop plants has
led to growing interest in computer vision techniques for plant disease
diagnosis. Although supervised image classifiers based on deep learning could
be a powerful tool to identify plant diseases, they require huge amount of data
set that have been labeled as healthy and disease. While, data mining
techniques called anomaly detection includes unsupervised approaches that not
require rare samples for training classifiers The proposed method in this study
focuses on the reconstructability of colors on plant images. We expect that a
deep encoder decoder network trained for reconstructing colors of healthy plant
images fails to color symptomatic regions. The main contributions of this work
are as follows: (i) we propose a new image-based plant disease detection
framework utilising a conditional adversarial network called pix2pix,(ii) we
introduce a new anomaly score calculated from CIEDE2000 color difference.
Through experiments using the PlantVillage dataset, we demonstrate that our
method is superior to an existing anomaly detector called AnoGAN for
identifying diseased crop images in terms of accuracy, interpretability and
computational efficiency.
</p>
<a href="http://arxiv.org/abs/2011.14306" target="_blank">arXiv:2011.14306</a> [<a href="http://arxiv.org/pdf/2011.14306" target="_blank">pdf</a>]

<h2>Active Output Selection Strategies for Multiple Learning Regression Models. (arXiv:2011.14307v1 [cs.LG])</h2>
<h3>Adrian Prochaska, Julien Pillas, Bernard B&#xe4;ker</h3>
<p>Active learning shows promise to decrease test bench time for model-based
drivability calibration. This paper presents a new strategy for active output
selection, which suits the needs of calibration tasks. The strategy is actively
learning multiple outputs in the same input space. It chooses the output model
with the highest cross-validation error as leading. The presented method is
applied to three different toy examples with noise in a real world range and to
a benchmark dataset. The results are analyzed and compared to other existing
strategies. In a best case scenario, the presented strategy is able to decrease
the number of points by up to 30% compared to a sequential space-filling design
while outperforming other existing active learning strategies. The results are
promising but also show that the algorithm has to be improved to increase
robustness for noisy environments. Further research will focus on improving the
algorithm and applying it to a real-world example.
</p>
<a href="http://arxiv.org/abs/2011.14307" target="_blank">arXiv:2011.14307</a> [<a href="http://arxiv.org/pdf/2011.14307" target="_blank">pdf</a>]

<h2>BSNet: Bi-Similarity Network for Few-shot Fine-grained Image Classification. (arXiv:2011.14311v1 [cs.CV])</h2>
<h3>Xiaoxu Li, Jijie Wu, Zhuo Sun, Zhanyu Ma, Jie Cao, Jing-Hao Xue</h3>
<p>Few-shot learning for fine-grained image classification has gained recent
attention in computer vision. Among the approaches for few-shot learning, due
to the simplicity and effectiveness, metric-based methods are favorably
state-of-the-art on many tasks. Most of the metric-based methods assume a
single similarity measure and thus obtain a single feature space. However, if
samples can simultaneously be well classified via two distinct similarity
measures, the samples within a class can distribute more compactly in a smaller
feature space, producing more discriminative feature maps. Motivated by this,
we propose a so-called \textit{Bi-Similarity Network} (\textit{BSNet}) that
consists of a single embedding module and a bi-similarity module of two
similarity measures. After the support images and the query images pass through
the convolution-based embedding module, the bi-similarity module learns feature
maps according to two similarity measures of diverse characteristics. In this
way, the model is enabled to learn more discriminative and less
similarity-biased features from few shots of fine-grained images, such that the
model generalization ability can be significantly improved. Through extensive
experiments by slightly modifying established metric/similarity based networks,
we show that the proposed approach produces a substantial improvement on
several fine-grained image benchmark datasets. Codes are available at:
https://github.com/spraise/BSNet
</p>
<a href="http://arxiv.org/abs/2011.14311" target="_blank">arXiv:2011.14311</a> [<a href="http://arxiv.org/pdf/2011.14311" target="_blank">pdf</a>]

<h2>FROCC: Fast Random projection-based One-Class Classification. (arXiv:2011.14317v1 [cs.LG])</h2>
<h3>Arindam Bhattacharya, Sumanth Varambally, Amitabha Bagchi, Srikanta Bedathur</h3>
<p>We present Fast Random projection-based One-Class Classification (FROCC), an
extremely efficient method for one-class classification. Our method is based on
a simple idea of transforming the training data by projecting it onto a set of
random unit vectors that are chosen uniformly and independently from the unit
sphere, and bounding the regions based on separation of the data. FROCC can be
naturally extended with kernels. We theoretically prove that FROCC generalizes
well in the sense that it is stable and has low bias. FROCC achieves up to 3.1
percent points better ROC, with 1.2--67.8x speedup in training and test times
over a range of state-of-the-art benchmarks including the SVM and the deep
learning based models for the OCC task.
</p>
<a href="http://arxiv.org/abs/2011.14317" target="_blank">arXiv:2011.14317</a> [<a href="http://arxiv.org/pdf/2011.14317" target="_blank">pdf</a>]

<h2>ProtoPShare: Prototype Sharing for Interpretable Image Classification and Similarity Discovery. (arXiv:2011.14340v1 [cs.CV])</h2>
<h3>Dawid Rymarczyk, &#x141;ukasz Struski, Jacek Tabor, Bartosz Zieli&#x144;ski</h3>
<p>In this paper, we introduce ProtoPShare, a self-explained method that
incorporates the paradigm of prototypical parts to explain its predictions. The
main novelty of the ProtoPShare is its ability to efficiently share
prototypical parts between the classes thanks to our data-dependent
merge-pruning. Moreover, the prototypes are more consistent and the model is
more robust to image perturbations than the state of the art method ProtoPNet.
We verify our findings on two datasets, the CUB-200-2011 and the Stanford Cars.
</p>
<a href="http://arxiv.org/abs/2011.14340" target="_blank">arXiv:2011.14340</a> [<a href="http://arxiv.org/pdf/2011.14340" target="_blank">pdf</a>]

<h2>Layer Pruning via Fusible Residual Convolutional Block for Deep Neural Networks. (arXiv:2011.14356v1 [cs.CV])</h2>
<h3>Pengtao Xu, Jian Cao, Fanhua Shang, Wenyu Sun, Pu Li</h3>
<p>In order to deploy deep convolutional neural networks (CNNs) on
resource-limited devices, many model pruning methods for filters and weights
have been developed, while only a few to layer pruning. However, compared with
filter pruning and weight pruning, the compact model obtained by layer pruning
has less inference time and run-time memory usage when the same FLOPs and
number of parameters are pruned because of less data moving in memory. In this
paper, we propose a simple layer pruning method using fusible residual
convolutional block (ResConv), which is implemented by inserting shortcut
connection with a trainable information control parameter into a single
convolutional layer. Using ResConv structures in training can improve network
accuracy and train deep plain networks, and adds no additional computation
during inference process because ResConv is fused to be an ordinary
convolutional layer after training. For layer pruning, we convert convolutional
layers of network into ResConv with a layer scaling factor. In the training
process, the L1 regularization is adopted to make the scaling factors sparse,
so that unimportant layers are automatically identified and then removed,
resulting in a model of layer reduction. Our pruning method achieves excellent
performance of compression and acceleration over the state-of-the-arts on
different datasets, and needs no retraining in the case of low pruning rate.
For example, with ResNet-110, we achieve a 65.5%-FLOPs reduction by removing
55.5% of the parameters, with only a small loss of 0.13% in top-1 accuracy on
CIFAR-10.
</p>
<a href="http://arxiv.org/abs/2011.14356" target="_blank">arXiv:2011.14356</a> [<a href="http://arxiv.org/pdf/2011.14356" target="_blank">pdf</a>]

<h2>Exploring Deep 3D Spatial Encodings for Large-Scale 3D Scene Understanding. (arXiv:2011.14358v1 [cs.CV])</h2>
<h3>Saqib Ali Khan, Yilei Shi, Muhammad Shahzad, Xiao Xiang Zhu</h3>
<p>Semantic segmentation of raw 3D point clouds is an essential component in 3D
scene analysis, but it poses several challenges, primarily due to the
non-Euclidean nature of 3D point clouds. Although, several deep learning based
approaches have been proposed to address this task, but almost all of them
emphasized on using the latent (global) feature representations from
traditional convolutional neural networks (CNN), resulting in severe loss of
spatial information, thus failing to model the geometry of the underlying 3D
objects, that plays an important role in remote sensing 3D scenes. In this
letter, we have proposed an alternative approach to overcome the limitations of
CNN based approaches by encoding the spatial features of raw 3D point clouds
into undirected symmetrical graph models. These encodings are then combined
with a high-dimensional feature vector extracted from a traditional CNN into a
localized graph convolution operator that outputs the required 3D segmentation
map. We have performed experiments on two standard benchmark datasets
(including an outdoor aerial remote sensing dataset and an indoor synthetic
dataset). The proposed method achieves on par state-of-the-art accuracy with
improved training time and model stability thus indicating strong potential for
further research towards a generalized state-of-the-art method for 3D scene
understanding.
</p>
<a href="http://arxiv.org/abs/2011.14358" target="_blank">arXiv:2011.14358</a> [<a href="http://arxiv.org/pdf/2011.14358" target="_blank">pdf</a>]

<h2>Optimal Mixture Weights for Off-Policy Evaluation with Multiple Behavior Policies. (arXiv:2011.14359v1 [cs.LG])</h2>
<h3>Jinlin Lai, Lixin Zou, Jiaxing Song</h3>
<p>Off-policy evaluation is a key component of reinforcement learning which
evaluates a target policy with offline data collected from behavior policies.
It is a crucial step towards safe reinforcement learning and has been used in
advertisement, recommender systems and many other applications. In these
applications, sometimes the offline data is collected from multiple behavior
policies. Previous works regard data from different behavior policies equally.
Nevertheless, some behavior policies are better at producing good estimators
while others are not. This paper starts with discussing how to correctly mix
estimators produced by different behavior policies. We propose three ways to
reduce the variance of the mixture estimator when all sub-estimators are
unbiased or asymptotically unbiased. Furthermore, experiments on simulated
recommender systems show that our methods are effective in reducing the
Mean-Square Error of estimation.
</p>
<a href="http://arxiv.org/abs/2011.14359" target="_blank">arXiv:2011.14359</a> [<a href="http://arxiv.org/pdf/2011.14359" target="_blank">pdf</a>]

<h2>A Targeted Universal Attack on Graph Convolutional Network. (arXiv:2011.14365v1 [cs.LG])</h2>
<h3>Jiazhu Dai, Weifeng Zhu, Xiangfeng Luo</h3>
<p>Graph-structured data exist in numerous applications in real life. As a
state-of-the-art graph neural network, the graph convolutional network (GCN)
plays an important role in processing graph-structured data. However, a recent
study reported that GCNs are also vulnerable to adversarial attacks, which
means that GCN models may suffer malicious attacks with unnoticeable
modifications of the data. Among all the adversarial attacks on GCNs, there is
a special kind of attack method called the universal adversarial attack, which
generates a perturbation that can be applied to any sample and causes GCN
models to output incorrect results. Although universal adversarial attacks in
computer vision have been extensively researched, there are few research works
on universal adversarial attacks on graph structured data. In this paper, we
propose a targeted universal adversarial attack against GCNs. Our method
employs a few nodes as the attack nodes. The attack capability of the attack
nodes is enhanced through a small number of fake nodes connected to them.
During an attack, any victim node will be misclassified by the GCN as the
attack node class as long as it is linked to them. The experiments on three
popular datasets show that the average attack success rate of the proposed
attack on any victim node in the graph reaches 83% when using only 3 attack
nodes and 6 fake nodes. We hope that our work will make the community aware of
the threat of this type of attack and raise the attention given to its future
defense.
</p>
<a href="http://arxiv.org/abs/2011.14365" target="_blank">arXiv:2011.14365</a> [<a href="http://arxiv.org/pdf/2011.14365" target="_blank">pdf</a>]

<h2>A smartphone based multi input workflow for non-invasive estimation of haemoglobin levels using machine learning techniques. (arXiv:2011.14370v1 [cs.LG])</h2>
<h3>Sarah, S.Sidhartha Narayan, Irfaan Arif, Hrithwik Shalu, Juned Kadiwala</h3>
<p>We suggest a low cost, non invasive healthcare system that measures
haemoglobin levels in patients and can be used as a preliminary diagnostic test
for anaemia. A combination of image processing, machine learning and deep
learning techniques are employed to develop predictive models to measure
haemoglobin levels. This is achieved through the color analysis of the
fingernail beds, palpebral conjunctiva and tongue of the patients. This
predictive model is then encapsulated in a healthcare application. This
application expedites data collection and facilitates active learning of the
model. It also incorporates personalized calibration of the model for each
patient, assisting in the continual monitoring of the haemoglobin levels of the
patient. Upon validating this framework using data, it can serve as a highly
accurate preliminary diagnostic test for anaemia.
</p>
<a href="http://arxiv.org/abs/2011.14370" target="_blank">arXiv:2011.14370</a> [<a href="http://arxiv.org/pdf/2011.14370" target="_blank">pdf</a>]

<h2>Predicting Regional Locust Swarm Distribution with Recurrent Neural Networks. (arXiv:2011.14371v1 [cs.LG])</h2>
<h3>Hadia Mohmmed Osman Ahmed Samil, Annabelle Martin, Arnav Kumar Jain, Susan Amin, Samira Ebrahimi Kahou</h3>
<p>Locust infestation of some regions in the world, including Africa, Asia and
Middle East has become a concerning issue that can affect the health and the
lives of millions of people. In this respect, there have been attempts to
resolve or reduce the severity of this problem via detection and monitoring of
locust breeding areas using satellites and sensors, or the use of chemicals to
prevent the formation of swarms. However, such methods have not been able to
suppress the emergence and the collective behaviour of locusts. The ability to
predict the location of the locust swarms prior to their formation, on the
other hand, can help people get prepared and tackle the infestation issue more
effectively. Here, we use machine learning to predict the location of locust
swarms using the available data published by the Food and Agriculture
Organization of the United Nations. The data includes the location of the
observed swarms as well as environmental information, including soil moisture
and the density of vegetation. The obtained results show that our proposed
model can successfully, and with reasonable precision, predict the location of
locust swarms, as well as their likely level of damage using a notion of
density.
</p>
<a href="http://arxiv.org/abs/2011.14371" target="_blank">arXiv:2011.14371</a> [<a href="http://arxiv.org/pdf/2011.14371" target="_blank">pdf</a>]

<h2>Constraining Volume Change in Learned Image Registration for Lung CTs. (arXiv:2011.14372v1 [cs.CV])</h2>
<h3>Alessa Hering, Stephanie H&#xe4;ger, Jan Moltz, Nikolas Lessmann, Stefan Heldmann, Bram van Ginneken</h3>
<p>Deep-learning-based registration methods emerged as a fast alternative to
conventional registration methods. However, these methods often still cannot
achieve the same performance as conventional registration methods, because they
are either limited to small deformation or they fail to handle a superposition
of large and small deformations without producing implausible deformation
fields with foldings inside.

In this paper, we identify important strategies of conventional registration
methods for lung registration and successfully developed the deep-learning
counterpart. We employ a Gaussian-pyramid-based multilevel framework that can
solve the image registration optimization in a coarse-to-fine fashion.
Furthermore, we prevent foldings of the deformation field and restrict the
determinant of the Jacobian to physiologically meaningful values by combining a
volume change penalty with a curvature regularizer in the loss function.
Keypoint correspondences are integrated to focus on the alignment of smaller
structures.

We perform an extensive evaluation to assess the accuracy, the robustness,
the plausibility of the estimated deformation fields, and the transferability
of our registration approach. We show that it archives state-of-the-art results
on the COPDGene dataset compared to the challenge winning conventional
registration method with much shorter execution time.
</p>
<a href="http://arxiv.org/abs/2011.14372" target="_blank">arXiv:2011.14372</a> [<a href="http://arxiv.org/pdf/2011.14372" target="_blank">pdf</a>]

<h2>Offline Reinforcement Learning Hands-On. (arXiv:2011.14379v1 [cs.LG])</h2>
<h3>Louis Monier, Jakub Kmec, Alexandre Laterre, Thomas Pierrot, Valentin Courgeau, Olivier Sigaud, Karim Beguir</h3>
<p>Offline Reinforcement Learning (RL) aims to turn large datasets into powerful
decision-making engines without any online interactions with the environment.
This great promise has motivated a large amount of research that hopes to
replicate the success RL has experienced in simulation settings. This work
ambitions to reflect upon these efforts from a practitioner viewpoint. We start
by discussing the dataset properties that we hypothesise can characterise the
type of offline methods that will be the most successful. We then verify these
claims through a set of experiments and designed datasets generated from
environments with both discrete and continuous action spaces. We experimentally
validate that diversity and high-return examples in the data are crucial to the
success of offline RL and show that behavioural cloning remains a strong
contender compared to its contemporaries. Overall, this work stands as a
tutorial to help people build their intuition on today's offline RL methods and
their applicability.
</p>
<a href="http://arxiv.org/abs/2011.14379" target="_blank">arXiv:2011.14379</a> [<a href="http://arxiv.org/pdf/2011.14379" target="_blank">pdf</a>]

<h2>Self-supervised Visual Reinforcement Learning with Object-centric Representations. (arXiv:2011.14381v1 [cs.LG])</h2>
<h3>Andrii Zadaianchuk, Maximilian Seitzer, Georg Martius</h3>
<p>Autonomous agents need large repertoires of skills to act reasonably on new
tasks that they have not seen before. However, acquiring these skills using
only a stream of high-dimensional, unstructured, and unlabeled observations is
a tricky challenge for any autonomous agent. Previous methods have used
variational autoencoders to encode a scene into a low-dimensional vector that
can be used as a goal for an agent to discover new skills. Nevertheless, in
compositional/multi-object environments it is difficult to disentangle all the
factors of variation into such a fixed-length representation of the whole
scene. We propose to use object-centric representations as a modular and
structured observation space, which is learned with a compositional generative
world model. We show that the structure in the representations in combination
with goal-conditioned attention policies helps the autonomous agent to discover
and learn useful skills. These skills can be further combined to address
compositional tasks like the manipulation of several different objects.
</p>
<a href="http://arxiv.org/abs/2011.14381" target="_blank">arXiv:2011.14381</a> [<a href="http://arxiv.org/pdf/2011.14381" target="_blank">pdf</a>]

<h2>There and Back Again: Learning to Simulate Radar Data for Real-World Applications. (arXiv:2011.14389v1 [cs.RO])</h2>
<h3>Rob Weston, Oiwi Parker Jones, Ingmar Posner</h3>
<p>Simulating realistic radar data has the potential to significantly accelerate
the development of data-driven approaches to radar processing. However, it is
fraught with difficulty due to the notoriously complex image formation process.
Here we propose to learn a radar sensor model capable of synthesising faithful
radar observations based on simulated elevation maps. In particular, we adopt
an adversarial approach to learning a forward sensor model from unaligned radar
examples. In addition, modelling the backward model encourages the output to
remain aligned to the world state through a cyclical consistency criterion. The
backward model is further constrained to predict elevation maps from real radar
data that are grounded by partial measurements obtained from corresponding
lidar scans. Both models are trained in a joint optimisation. We demonstrate
the efficacy of our approach by evaluating a down-stream segmentation model
trained purely on simulated data in a real-world deployment. This achieves
performance within four percentage points of the same model trained entirely on
real data.
</p>
<a href="http://arxiv.org/abs/2011.14389" target="_blank">arXiv:2011.14389</a> [<a href="http://arxiv.org/pdf/2011.14389" target="_blank">pdf</a>]

<h2>RGBD-Net: Predicting color and depth images for novel views synthesis. (arXiv:2011.14398v1 [cs.CV])</h2>
<h3>Phong Nguyen, Animesh Karnewar, Lam Huynh, Esa Rahtu, Jiri Matas, Janne Heikkila</h3>
<p>We address the problem of novel view synthesis from an unstructured set of
reference images. A new method called RGBD-Net is proposed to predict the depth
map and the color images at the target pose in a multi-scale manner. The
reference views are warped to the target pose to obtain multi-scale plane sweep
volumes, which are then passed to our first module, a hierarchical depth
regression network which predicts the depth map of the novel view. Second, a
depth-aware generator network refines the warped novel views and renders the
final target image. These two networks can be trained with or without depth
supervision. In experimental evaluation, RGBD-Net not only produces novel views
with higher quality than the previous state-of-the-art methods, but also the
obtained depth maps enable reconstruction of more accurate 3D point clouds than
the existing multi-view stereo methods. The results indicate that RGBD-Net
generalizes well to previously unseen data.
</p>
<a href="http://arxiv.org/abs/2011.14398" target="_blank">arXiv:2011.14398</a> [<a href="http://arxiv.org/pdf/2011.14398" target="_blank">pdf</a>]

<h2>Reconfigurable Cyber-Physical System for Critical Infrastructure Protection in Smart Cities via Smart Video-Surveillance. (arXiv:2011.14416v1 [cs.CV])</h2>
<h3>Juan Isern, Francisco Barranco, Daniel Deniz, Juho Lesonen, Jari Hannuksela, Richard R. Carrillo</h3>
<p>Automated surveillance is essential for the protection of Critical
Infrastructures (CIs) in future Smart Cities. The dynamic environments and
bandwidth requirements demand systems that adapt themselves to react when
events of interest occur. We present a reconfigurable Cyber Physical System for
the protection of CIs using distributed cloud-edge smart video surveillance.
Our local edge nodes perform people detection via Deep Learning. Processing is
embedded in high performance SoCs (System-on-Chip) achieving real-time
performance ($\approx$ 100 fps - frames per second) which enables efficiently
managing video streams of more cameras source at lower frame rate. Cloud server
gathers results from nodes to carry out biometric facial identification,
tracking, and perimeter monitoring. A Quality and Resource Management module
monitors data bandwidth and triggers reconfiguration adapting the transmitted
video resolution. This also enables a flexible use of the network by multiple
cameras while maintaining the accuracy of biometric identification. A
real-world example shows a reduction of $\approx$ 75\% bandwidth use with
respect to the no-reconfiguration scenario.
</p>
<a href="http://arxiv.org/abs/2011.14416" target="_blank">arXiv:2011.14416</a> [<a href="http://arxiv.org/pdf/2011.14416" target="_blank">pdf</a>]

<h2>LABNet: Local Graph Aggregation Network with Class Balanced Loss for Vehicle Re-Identification. (arXiv:2011.14417v1 [cs.CV])</h2>
<h3>Abu Md Niamul Taufique, Andreas Savakis</h3>
<p>Vehicle re-identification is an important computer vision task where the
objective is to identify a specific vehicle among a set of vehicles seen at
various viewpoints. Recent methods based on deep learning utilize a global
average pooling layer after the backbone feature extractor, however, this
ignores any spatial reasoning on the feature map. In this paper, we propose
local graph aggregation on the backbone feature map, to learn associations of
local information and hence improve feature learning as well as reduce the
effects of partial occlusion and background clutter. Our local graph
aggregation network considers spatial regions of the feature map as nodes and
builds a local neighborhood graph that performs local feature aggregation
before the global average pooling layer. We further utilize a batch
normalization layer to improve the system effectiveness. Additionally, we
introduce a class balanced loss to compensate for the imbalance in the sample
distributions found in the most widely used vehicle re-identification datasets.
Finally, we evaluate our method in three popular benchmarks and show that our
approach outperforms many state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.14417" target="_blank">arXiv:2011.14417</a> [<a href="http://arxiv.org/pdf/2011.14417" target="_blank">pdf</a>]

<h2>Improving Neural Network with Uniform Sparse Connectivity. (arXiv:2011.14420v1 [cs.LG])</h2>
<h3>Weijun Luo</h3>
<p>Neural network forms the foundation of deep learning and numerous AI
applications. Classical neural networks are fully connected, expensive to train
and prone to overfitting. Sparse networks tend to have convoluted structure
search, suboptimal performance and limited usage. We proposed the novel uniform
sparse network (USN) with even and sparse connectivity within each layer. USN
has one striking property that its performance is independent of the
substantial topology variation and enormous model space, thus offers a
search-free solution to all above mentioned issues of neural networks. USN
consistently and substantially outperforms the state-of-the-art sparse network
models in prediction accuracy, speed and robustness. It even achieves higher
prediction accuracy than the fully connected network with only 0.55% parameters
and 1/4 computing time and resources. Importantly, USN is conceptually simple
as a natural generalization of fully connected network with multiple
improvements in accuracy, robustness and scalability. USN can replace the
latter in a range of applications, data types and deep learning architectures.
We have made USN open source at https://github.com/datapplab/sparsenet.
</p>
<a href="http://arxiv.org/abs/2011.14420" target="_blank">arXiv:2011.14420</a> [<a href="http://arxiv.org/pdf/2011.14420" target="_blank">pdf</a>]

<h2>Architectural Adversarial Robustness: The Case for Deep Pursuit. (arXiv:2011.14427v1 [cs.LG])</h2>
<h3>George Cazenavette, Calvin Murdock, Simon Lucey</h3>
<p>Despite their unmatched performance, deep neural networks remain susceptible
to targeted attacks by nearly imperceptible levels of adversarial noise. While
the underlying cause of this sensitivity is not well understood, theoretical
analyses can be simplified by reframing each layer of a feed-forward network as
an approximate solution to a sparse coding problem. Iterative solutions using
basis pursuit are theoretically more stable and have improved adversarial
robustness. However, cascading layer-wise pursuit implementations suffer from
error accumulation in deeper networks. In contrast, our new method of deep
pursuit approximates the activations of all layers as a single global
optimization problem, allowing us to consider deeper, real-world architectures
with skip connections such as residual networks. Experimentally, our approach
demonstrates improved robustness to adversarial noise.
</p>
<a href="http://arxiv.org/abs/2011.14427" target="_blank">arXiv:2011.14427</a> [<a href="http://arxiv.org/pdf/2011.14427" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Crowdsourced Urban Delivery: System States Characterization, Heuristics-guided Action Choice, and Rule-Interposing Integration. (arXiv:2011.14430v1 [cs.AI])</h2>
<h3>Tanvir Ahamed, Bo Zou, Nahid Parvez Farazi, Theja Tulabandhula</h3>
<p>This paper investigates the problem of assigning shipping requests to ad hoc
couriers in the context of crowdsourced urban delivery. The shipping requests
are spatially distributed each with a limited time window between the earliest
time for pickup and latest time for delivery. The ad hoc couriers, termed
crowdsourcees, also have limited time availability and carrying capacity. We
propose a new deep reinforcement learning (DRL)-based approach to tackling this
assignment problem. A deep Q network (DQN) algorithm is trained which entails
two salient features of experience replay and target network that enhance the
efficiency, convergence, and stability of DRL training. More importantly, this
paper makes three methodological contributions: 1) presenting a comprehensive
and novel characterization of crowdshipping system states that encompasses
spatial-temporal and capacity information of crowdsourcees and requests; 2)
embedding heuristics that leverage the information offered by the state
representation and are based on intuitive reasoning to guide specific actions
to take, to preserve tractability and enhance efficiency of training; and 3)
integrating rule-interposing to prevent repeated visiting of the same routes
and node sequences during routing improvement, thereby further enhancing the
training efficiency by accelerating learning. The effectiveness of the proposed
approach is demonstrated through extensive numerical analysis. The results show
the benefits brought by the heuristics-guided action choice and
rule-interposing in DRL training, and the superiority of the proposed approach
over existing heuristics in both solution quality, time, and scalability.
Besides the potential to improve the efficiency of crowdshipping operation
planning, the proposed approach also provides a new avenue and generic
framework for other problems in the vehicle routing context.
</p>
<a href="http://arxiv.org/abs/2011.14430" target="_blank">arXiv:2011.14430</a> [<a href="http://arxiv.org/pdf/2011.14430" target="_blank">pdf</a>]

<h2>Scaling *down* Deep Learning. (arXiv:2011.14439v1 [cs.LG])</h2>
<h3>Sam Greydanus</h3>
<p>Though deep learning models have taken on commercial and political relevance,
many aspects of their training and operation remain poorly understood. This has
sparked interest in "science of deep learning" projects, many of which are run
at scale and require enormous amounts of time, money, and electricity. But how
much of this research really needs to occur at scale? In this paper, we
introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to
classic deep learning benchmarks. The training examples are 20 times smaller
than MNIST examples yet they differentiate more clearly between linear,
nonlinear, and convolutional models which attain 32, 68, and 94% accuracy
respectively (these models obtain 94, 99+, and 99+% on MNIST). Then we present
example use cases which include measuring the spatial inductive biases of
lottery tickets, observing deep double descent, and metalearning an activation
function.
</p>
<a href="http://arxiv.org/abs/2011.14439" target="_blank">arXiv:2011.14439</a> [<a href="http://arxiv.org/pdf/2011.14439" target="_blank">pdf</a>]

<h2>Intrinsic Decomposition of Document Images In-the-Wild. (arXiv:2011.14447v1 [cs.CV])</h2>
<h3>Sagnik Das, Hassan Ahmed Sial, Ke Ma, Ramon Baldrich, Maria Vanrell, Dimitris Samaras</h3>
<p>Automatic document content processing is affected by artifacts caused by the
shape of the paper, non-uniform and diverse color of lighting conditions.
Fully-supervised methods on real data are impossible due to the large amount of
data needed. Hence, the current state of the art deep learning models are
trained on fully or partially synthetic images. However, document shadow or
shading removal results still suffer because: (a) prior methods rely on
uniformity of local color statistics, which limit their application on
real-scenarios with complex document shapes and textures and; (b) synthetic or
hybrid datasets with non-realistic, simulated lighting conditions are used to
train the models. In this paper we tackle these problems with our two main
contributions. First, a physically constrained learning-based method that
directly estimates document reflectance based on intrinsic image formation
which generalizes to challenging illumination conditions. Second, a new dataset
that clearly improves previous synthetic ones, by adding a large range of
realistic shading and diverse multi-illuminant conditions, uniquely customized
to deal with documents in-the-wild. The proposed architecture works in a
self-supervised manner where only the synthetic texture is used as a weak
training signal (obviating the need for very costly ground truth with
disentangled versions of shading and reflectance). The proposed approach leads
to a significant generalization of document reflectance estimation in real
scenes with challenging illumination. We extensively evaluate on the real
benchmark datasets available for intrinsic image decomposition and document
shadow removal tasks. Our reflectance estimation scheme, when used as a
pre-processing step of an OCR pipeline, shows a 26% improvement of character
error rate (CER), thus, proving the practical applicability.
</p>
<a href="http://arxiv.org/abs/2011.14447" target="_blank">arXiv:2011.14447</a> [<a href="http://arxiv.org/pdf/2011.14447" target="_blank">pdf</a>]

<h2>Improved Handling of Motion Blur in Online Object Detection. (arXiv:2011.14448v1 [cs.CV])</h2>
<h3>Mohamed Sayed, Gabriel J. Brostow</h3>
<p>We wish to detect specific categories of objects, for online vision systems
that will run in the real world. Object detection is already very challenging.
It is even harder when the images are blurred, from the camera being in a car
or a hand-held phone. Most existing efforts either focused on sharp images,
with easy to label ground truth, or they have treated motion blur as one of
many generic corruptions.

Instead, we focus especially on the details of egomotion induced blur. We
explore five classes of remedies, where each targets different potential causes
for the performance gap between sharp and blurred images. For example, first
deblurring an image changes its human interpretability, but at present, only
partly improves object detection. The other four classes of remedies address
multi-scale texture, out-of-distribution testing, label generation, and
conditioning by blur-type. Surprisingly, we discover that custom label
generation aimed at resolving spatial ambiguity, ahead of all others, markedly
improves object detection. Also, in contrast to findings from classification,
we see a noteworthy boost by conditioning our model on bespoke categories of
motion blur. We validate and cross-breed the different remedies experimentally
on blurred COCO images, producing an easy and practical favorite model with
superior detection rates.
</p>
<a href="http://arxiv.org/abs/2011.14448" target="_blank">arXiv:2011.14448</a> [<a href="http://arxiv.org/pdf/2011.14448" target="_blank">pdf</a>]

<h2>Conditional Link Prediction of Category-Implicit Keypoint Detection. (arXiv:2011.14462v1 [cs.CV])</h2>
<h3>Ellen Yi-Ge, Rui Fan, Zechun Liu, Zhiqiang Shen</h3>
<p>Keypoints of objects reflect their concise abstractions, while the
corresponding connection links (CL) build the skeleton by detecting the
intrinsic relations between keypoints. Existing approaches are typically
computationally-intensive, inapplicable for instances belonging to multiple
classes, and/or infeasible to simultaneously encode connection information. To
address the aforementioned issues, we propose an end-to-end category-implicit
Keypoint and Link Prediction Network (KLPNet), which is the first approach for
simultaneous semantic keypoint detection (for multi-class instances) and CL
rejuvenation. In our KLPNet, a novel Conditional Link Prediction Graph is
proposed for link prediction among keypoints that are contingent on a
predefined category. Furthermore, a Cross-stage Keypoint Localization Module
(CKLM) is introduced to explore feature aggregation for coarse-to-fine keypoint
localization. Comprehensive experiments conducted on three publicly available
benchmarks demonstrate that our KLPNet consistently outperforms all other
state-of-the-art approaches. Furthermore, the experimental results of CL
prediction also show the effectiveness of our KLPNet with respect to occlusion
problems.
</p>
<a href="http://arxiv.org/abs/2011.14462" target="_blank">arXiv:2011.14462</a> [<a href="http://arxiv.org/pdf/2011.14462" target="_blank">pdf</a>]

<h2>Kinetics-Informed Neural Networks. (arXiv:2011.14473v1 [cs.LG])</h2>
<h3>Gabriel S. Gusm&#xe3;o, Adhika P. Retnanto, Shashwati C. da Cunha, Andrew J. Medford</h3>
<p>Chemical kinetics consists of the phenomenological framework for the
disentanglement of reaction mechanisms, optimization of reaction performance
and the rational design of chemical processes. Here, we utilize feed-forward
artificial neural networks as basis functions for the construction of surrogate
models to solve ordinary differential equations (ODEs) that describe
microkinetic models (MKMs). We present an algebraic framework for the
mathematical description and classification of reaction networks, types of
elementary reaction, and chemical species. Under this framework, we demonstrate
that the simultaneous training of neural nets and kinetic model parameters in a
regularized multiobjective optimization setting leads to the solution of the
inverse problem through the estimation of kinetic parameters from synthetic
experimental data. We probe the limits at which kinetic parameters can be
retrieved as a function of knowledge about the chemical system states over
time, and assess the robustness of the methodology with respect to statistical
noise. This surrogate approach to inverse kinetic ODEs can assist in the
elucidation of reaction mechanisms based on transient data.
</p>
<a href="http://arxiv.org/abs/2011.14473" target="_blank">arXiv:2011.14473</a> [<a href="http://arxiv.org/pdf/2011.14473" target="_blank">pdf</a>]

<h2>An artificial consciousness model and its relations with philosophy of mind. (arXiv:2011.14475v1 [cs.AI])</h2>
<h3>Eduardo C. Garrido-Merch&#xe1;n, Martin Molina, Francisco M. Mendoza</h3>
<p>This work seeks to study the beneficial properties that an autonomous agent
can obtain by implementing a cognitive architecture similar to the one of
conscious beings. Along this document, a conscious model of autonomous agent
based in a global workspace architecture is presented. We describe how this
agent is viewed from different perspectives of philosophy of mind, being
inspired by their ideas. The goal of this model is to create autonomous agents
able to navigate within an environment composed of multiple independent
magnitudes, adapting to its surroundings in order to find the best possible
position in base of its inner preferences. The purpose of the model is to test
the effectiveness of many cognitive mechanisms that are incorporated, such as
an attention mechanism for magnitude selection, pos-session of inner feelings
and preferences, usage of a memory system to storage beliefs and past
experiences, and incorporating a global workspace which controls and integrates
information processed by all the subsystem of the model. We show in a large
experiment set how an autonomous agent can benefit from having a cognitive
architecture such as the one described.
</p>
<a href="http://arxiv.org/abs/2011.14475" target="_blank">arXiv:2011.14475</a> [<a href="http://arxiv.org/pdf/2011.14475" target="_blank">pdf</a>]

<h2>What Can Style Transfer and Paintings Do For Model Robustness?. (arXiv:2011.14477v1 [cs.CV])</h2>
<h3>Hubert Lin, Mitchell van Zuijlen, Sylvia C. Pont, Maarten W.A. Wijntjes, Kavita Bala</h3>
<p>A common strategy for improving model robustness is through data
augmentations. Data augmentations encourage models to learn desired
invariances, such as invariance to horizontal flipping or small changes in
color. Recent work has shown that arbitrary style transfer can be used as a
form of data augmentation to encourage invariance to textures by creating
painting-like images from photographs. However, a stylized photograph is not
quite the same as an artist-created painting. Artists depict perceptually
meaningful cues in paintings so that humans can recognize salient components in
scenes, an emphasis which is not enforced in style transfer. Therefore, we
study how style transfer and paintings differ in their impact on model
robustness. First, we investigate the role of paintings as style images for
stylization-based data augmentation. We find that style transfer functions well
even without paintings as style images. Second, we show that learning from
paintings as a form of perceptual data augmentation can improve model
robustness. Finally, we investigate the invariances learned from stylization
and from paintings, and show that models learn different invariances from these
differing forms of data. Our results provide insights into how stylization
improves model robustness, and provide evidence that artist-created paintings
can be a valuable source of data for model robustness.
</p>
<a href="http://arxiv.org/abs/2011.14477" target="_blank">arXiv:2011.14477</a> [<a href="http://arxiv.org/pdf/2011.14477" target="_blank">pdf</a>]

<h2>Annotation-Efficient Untrimmed Video Action Recognition. (arXiv:2011.14478v1 [cs.CV])</h2>
<h3>Yixiong Zou, Shanghang Zhang, Guangyao Chen, Yonghong Tian, Kurt Keutzer, Jos&#xe9; M. F. Moura</h3>
<p>Deep learning has achieved great success in recognizing video actions, but
the collection and annotation of training data are still laborious, which
mainly lies in two aspects: (1) the amount of required annotated data is large;
(2) temporally annotating the location of each action is time-consuming. Works
such as few-shot learning or untrimmed video recognition have been proposed to
handle either one aspect or the other. However, very few existing works can
handle both aspects simultaneously. In this paper, we target a new problem,
Annotation-Efficient Video Recognition, to reduce the requirement of
annotations for both large amount of samples from different classes and the
action locations. Challenges of this problem come from three folds: (1) action
recognition from untrimmed videos, (2) weak supervision, and (3) novel classes
with only a few training samples. To address the first two challenges, we
propose a background pseudo-labeling method based on open-set detection. To
tackle the third challenge, we propose a self-weighted classification mechanism
and a contrastive learning method to separate background and foreground of the
untrimmed videos. Extensive experiments on ActivityNet v1.2 and ActivityNet
v1.3 verify the effectiveness of the proposed methods. Codes will be released
online.
</p>
<a href="http://arxiv.org/abs/2011.14478" target="_blank">arXiv:2011.14478</a> [<a href="http://arxiv.org/pdf/2011.14478" target="_blank">pdf</a>]

<h2>Multi-scale Adaptive Task Attention Network for Few-Shot Learning. (arXiv:2011.14479v1 [cs.CV])</h2>
<h3>Haoxing Chen, Huaxiong Li, Yaohui Li, Chunlin Chen</h3>
<p>The goal of few-shot learning is to classify unseen categories with few
labeled samples. Recently, the low-level information metric-learning based
methods have achieved satisfying performance, since local representations (LRs)
are more consistent between seen and unseen classes. However, most of these
methods deal with each category in the support set independently, which is not
sufficient to measure the relation between features, especially in a certain
task. Moreover, the low-level information-based metric learning method suffers
when dominant objects of different scales exist in a complex background. To
address these issues, this paper proposes a novel Multi-scale Adaptive Task
Attention Network (MATANet) for few-shot learning. Specifically, we first use a
multi-scale feature generator to generate multiple features at different
scales. Then, an adaptive task attention module is proposed to select the most
important LRs among the entire task. Afterwards, a similarity-to-class module
and a fusion layer are utilized to calculate a joint multi-scale similarity
between the query image and the support set. Extensive experiments on popular
benchmarks clearly show the effectiveness of the proposed MATANet compared with
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.14479" target="_blank">arXiv:2011.14479</a> [<a href="http://arxiv.org/pdf/2011.14479" target="_blank">pdf</a>]

<h2>Value Function Based Performance Optimization of Deep Learning Workloads. (arXiv:2011.14486v1 [cs.LG])</h2>
<h3>Benoit Steiner, Chris Cummins, Horace He, Hugh Leather</h3>
<p>As machine learning techniques become ubiquitous, the efficiency of neural
network implementations is becoming correspondingly paramount. Frameworks, such
as Halide and TVM, separate out the algorithmic representation of the network
from the schedule that determines its implementation. Finding good schedules,
however, remains extremely challenging. We model this scheduling problem as a
sequence of optimization choices, and present a new technique to accurately
predict the expected performance of a partial schedule. By leveraging these
predictions we can make these optimization decisions greedily and rapidly
identify an efficient schedule. This enables us to find schedules that improve
the throughput of deep neural networks by 2.6x over Halide and 1.5x over TVM.
Moreover, our technique is two to three orders of magnitude faster than that of
these tools, and completes in seconds instead of hours.
</p>
<a href="http://arxiv.org/abs/2011.14486" target="_blank">arXiv:2011.14486</a> [<a href="http://arxiv.org/pdf/2011.14486" target="_blank">pdf</a>]

<h2>Continuous Transition: Improving Sample Efficiency for Continuous Control Problems via MixUp. (arXiv:2011.14487v1 [cs.RO])</h2>
<h3>Junfan Lin, Zhongzhan Huang, Keze Wang, Xiaodan Liang, Weiwei Chen, Liang Lin</h3>
<p>Although deep reinforcement learning~(RL) has been successfully applied to a
variety of robotic control tasks, it's still challenging to apply it to
real-world tasks, due to the poor sample efficiency. Attempting to overcome
this shortcoming, several works focus on reusing the collected trajectory data
during the training by decomposing them into a set of policy-irrelevant
discrete transitions. However, their improvements are somewhat marginal since
i) the amount of the transitions is usually small, and ii) the value assignment
only happens in the joint states. To address these issues, this paper
introduces a concise yet powerful method to construct \textit{Continuous
Transition}, which exploits the trajectory information by exploiting the
potential transitions along the trajectory. Specifically, we propose to
synthesize new transitions for training by linearly interpolating the
conjunctive transitions. To keep the constructed transitions authentic, we also
develop a discriminator to guide the construction process automatically.
Extensive experiments demonstrate that our proposed method achieves a
significant improvement in sample efficiency on various complex continuous
robotic control problems in MuJoCo and outperforms the advanced model-based /
model-free RL methods.
</p>
<a href="http://arxiv.org/abs/2011.14487" target="_blank">arXiv:2011.14487</a> [<a href="http://arxiv.org/pdf/2011.14487" target="_blank">pdf</a>]

<h2>Sim2SG: Sim-to-Real Scene Graph Generation for Transfer Learning. (arXiv:2011.14488v1 [cs.CV])</h2>
<h3>Aayush Prakash, Shoubhik Debnath, Jean-Francois Lafleche, Eric Cameracci, Gavriel State, Marc T. Law</h3>
<p>Scene graph (SG) generation has been gaining a lot of traction recently.
Current SG generation techniques, however, rely on the availability of
expensive and limited number of labeled datasets. Synthetic data offers a
viable alternative as labels are essentially free. However, neural network
models trained on synthetic data, do not perform well on real data because of
the domain gap. To overcome this challenge, we propose Sim2SG, a scalable
technique for sim-to-real transfer for scene graph generation. Sim2SG addresses
the domain gap by decomposing it into appearance, label and prediction
discrepancies between the two domains. We handle these discrepancies by
introducing pseudo statistic based self-learning and adversarial techniques.
Sim2SG does not require costly supervision from the real-world dataset. Our
experiments demonstrate significant improvements over baselines in reducing the
domain gap both qualitatively and quantitatively. We validate our approach on
toy simulators, as well as realistic simulators evaluated on real-world data.
</p>
<a href="http://arxiv.org/abs/2011.14488" target="_blank">arXiv:2011.14488</a> [<a href="http://arxiv.org/pdf/2011.14488" target="_blank">pdf</a>]

<h2>Soft-Robust Algorithms for Handling Model Misspecification. (arXiv:2011.14495v1 [cs.LG])</h2>
<h3>Elita A. Lobo, Mohammad Ghavamzadeh, Marek Petrik</h3>
<p>In reinforcement learning, robust policies for high-stakes decision-making
problems with limited data are usually computed by optimizing the percentile
criterion, which minimizes the probability of a catastrophic failure.
Unfortunately, such policies are typically overly conservative as the
percentile criterion is non-convex, difficult to optimize, and ignores the mean
performance. To overcome these shortcomings, we study the soft-robust
criterion, which uses risk measures to balance the mean and percentile criteria
better. In this paper, we establish the soft-robust criterion's fundamental
properties, show that it is NP-hard to optimize, and propose and analyze two
algorithms to optimize it approximately. Our theoretical analyses and empirical
evaluations demonstrate that our algorithms compute much less conservative
solutions than the existing approximate methods for optimizing the
percentile-criterion.
</p>
<a href="http://arxiv.org/abs/2011.14495" target="_blank">arXiv:2011.14495</a> [<a href="http://arxiv.org/pdf/2011.14495" target="_blank">pdf</a>]

<h2>Locus: LiDAR-based Place Recognition using Spatiotemporal Higher-Order Pooling. (arXiv:2011.14497v1 [cs.RO])</h2>
<h3>Kavisha Vidanapathirana, Peyman Moghadam, Ben Harwood, Muming Zhao, Sridha Sridharan, Clinton Fookes</h3>
<p>Place Recognition (PR) enables the estimation of a globally consistent map
and trajectory by providing non-local constraints in Simultaneous Localisation
and Mapping (SLAM). This paper presents Locus, a novel place recognition method
using 3D LiDAR point clouds in large-scale environments. We propose a novel
method for extracting and encoding topological and temporal information related
to components in a scene and demonstrate how the inclusion of this auxiliary
information in place description leads to more robust and discriminative scene
representations. Second-order pooling along with a non-linear transform is used
to aggregate these multi-level features to generate a fixed-length global
descriptor, which is invariant to the permutation of input features. The
proposed method outperforms state-of-the-art methods on the KITTI dataset.
Furthermore, Locus is demonstrated to be robust across several challenging
situations such as occlusions and viewpoint changes.
</p>
<a href="http://arxiv.org/abs/2011.14497" target="_blank">arXiv:2011.14497</a> [<a href="http://arxiv.org/pdf/2011.14497" target="_blank">pdf</a>]

<h2>End-to-End Video Instance Segmentation with Transformers. (arXiv:2011.14503v1 [cs.CV])</h2>
<h3>Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia</h3>
<p>Video instance segmentation (VIS) is the task that requires simultaneously
classifying, segmenting and tracking object instances of interest in video.
Recent methods typically develop sophisticated pipelines to tackle this task.
Here, we propose a new video instance segmentation framework built upon
Transformers, termed VisTR, which views the VIS task as a direct end-to-end
parallel sequence decoding/prediction problem. Given a video clip consisting of
multiple image frames as input, VisTR outputs the sequence of masks for each
instance in the video in order directly. At the core is a new, effective
instance sequence matching and segmentation strategy, which supervises and
segments instances at the sequence level as a whole. VisTR frames the instance
segmentation and tracking in the same perspective of similarity learning, thus
considerably simplifying the overall pipeline and is significantly different
from existing approaches. Without bells and whistles, VisTR achieves the
highest speed among all existing VIS models, and achieves the best result among
methods using single model on the YouTube-VIS dataset. For the first time, we
demonstrate a much simpler and faster video instance segmentation framework
built upon Transformers, achieving competitive accuracy. We hope that VisTR can
motivate future research for more video understanding tasks.
</p>
<a href="http://arxiv.org/abs/2011.14503" target="_blank">arXiv:2011.14503</a> [<a href="http://arxiv.org/pdf/2011.14503" target="_blank">pdf</a>]

<h2>Training and Inference for Integer-Based Semantic Segmentation Network. (arXiv:2011.14504v1 [cs.CV])</h2>
<h3>Jiayi Yang, Lei Deng, Yukuan Yang, Yuan Xie, Guoqi Li</h3>
<p>Semantic segmentation has been a major topic in research and industry in
recent years. However, due to the computation complexity of pixel-wise
prediction and backpropagation algorithm, semantic segmentation has been
demanding in computation resources, resulting in slow training and inference
speed and large storage space to store models. Existing schemes that speed up
segmentation network change the network structure and come with noticeable
accuracy degradation. However, neural network quantization can be used to
reduce computation load while maintaining comparable accuracy and original
network structure. Semantic segmentation networks are different from
traditional deep convolutional neural networks (DCNNs) in many ways, and this
topic has not been thoroughly explored in existing works. In this paper, we
propose a new quantization framework for training and inference of segmentation
networks, where parameters and operations are constrained to 8-bit
integer-based values for the first time. Full quantization of the data flow and
the removal of square and root operations in batch normalization give our
framework the ability to perform inference on fixed-point devices. Our proposed
framework is evaluated on mainstream semantic segmentation networks like
FCN-VGG16 and DeepLabv3-ResNet50, achieving comparable accuracy against
floating-point framework on ADE20K dataset and PASCAL VOC 2012 dataset.
</p>
<a href="http://arxiv.org/abs/2011.14504" target="_blank">arXiv:2011.14504</a> [<a href="http://arxiv.org/pdf/2011.14504" target="_blank">pdf</a>]

<h2>Feature Learning in Infinite-Width Neural Networks. (arXiv:2011.14522v1 [cs.LG])</h2>
<h3>Greg Yang, Edward J. Hu</h3>
<p>As its width tends to infinity, a deep neural network's behavior under
gradient descent can become simplified and predictable (e.g. given by the
Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK
parametrization). However, we show that the standard and NTK parametrizations
of a neural network do not admit infinite-width limits that can learn features,
which is crucial for pretraining and transfer learning such as with BERT. We
propose simple modifications to the standard parametrization to allow for
feature learning in the limit. Using the *Tensor Programs* technique, we derive
explicit formulas for such limits. On Word2Vec and few-shot learning on
Omniglot via MAML, two canonical tasks that rely crucially on feature learning,
we compute these limits exactly. We find that they outperform both NTK
baselines and finite-width networks, with the latter approaching the
infinite-width feature learning performance as width increases.

More generally, we classify a natural space of neural network
parametrizations that generalizes standard, NTK, and Mean Field
parametrizations. We show 1) any parametrization in this space either admits
feature learning or has an infinite-width training dynamics given by kernel
gradient descent, but not both; 2) any such infinite-width limit can be
computed using the Tensor Programs technique.
</p>
<a href="http://arxiv.org/abs/2011.14522" target="_blank">arXiv:2011.14522</a> [<a href="http://arxiv.org/pdf/2011.14522" target="_blank">pdf</a>]

<h2>Inter-layer Transition in Neural Architecture Search. (arXiv:2011.14525v1 [cs.CV])</h2>
<h3>Benteng Ma, Jing Zhang, Yong Xia, Dacheng Tao</h3>
<p>Differential Neural Architecture Search (NAS) methods represent the network
architecture as a repetitive proxy directed acyclic graph (DAG) and optimize
the network weights and architecture weights alternatively in a differential
manner. However, existing methods model the architecture weights on each edge
(i.e., a layer in the network) as statistically independent variables, ignoring
the dependency between edges in DAG induced by their directed topological
connections. In this paper, we make the first attempt to investigate such
dependency by proposing a novel Inter-layer Transition NAS method. It casts the
architecture optimization into a sequential decision process where the
dependency between the architecture weights of connected edges is explicitly
modeled. Specifically, edges are divided into inner and outer groups according
to whether or not their predecessor edges are in the same cell. While the
architecture weights of outer edges are optimized independently, those of inner
edges are derived sequentially based on the architecture weights of their
predecessor edges and the learnable transition matrices in an attentive
probability transition manner. Experiments on five benchmarks confirm the value
of modeling inter-layer dependency and demonstrate the proposed method
outperforms state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.14525" target="_blank">arXiv:2011.14525</a> [<a href="http://arxiv.org/pdf/2011.14525" target="_blank">pdf</a>]

<h2>Heuristic Domain Adaptation. (arXiv:2011.14540v1 [cs.CV])</h2>
<h3>Shuhao Cui, Xuan Jin, Shuhui Wang, Yuan He, Qingming Huang</h3>
<p>In visual domain adaptation (DA), separating the domain-specific
characteristics from the domain-invariant representations is an ill-posed
problem. Existing methods apply different kinds of priors or directly minimize
the domain discrepancy to address this problem, which lack flexibility in
handling real-world situations. Another research pipeline expresses the
domain-specific information as a gradual transferring process, which tends to
be suboptimal in accurately removing the domain-specific properties. In this
paper, we address the modeling of domain-invariant and domain-specific
information from the heuristic search perspective. We identify the
characteristics in the existing representations that lead to larger domain
discrepancy as the heuristic representations. With the guidance of heuristic
representations, we formulate a principled framework of Heuristic Domain
Adaptation (HDA) with well-founded theoretical guarantees. To perform HDA, the
cosine similarity scores and independence measurements between domain-invariant
and domain-specific representations are cast into the constraints at the
initial and final states during the learning procedure. Similar to the final
condition of heuristic search, we further derive a constraint enforcing the
final range of heuristic network output to be small. Accordingly, we propose
Heuristic Domain Adaptation Network (HDAN), which explicitly learns the
domain-invariant and domain-specific representations with the above mentioned
constraints. Extensive experiments show that HDAN has exceeded state-of-the-art
on unsupervised DA, multi-source DA and semi-supervised DA. The code is
available at https://github.com/cuishuhao/HDA.
</p>
<a href="http://arxiv.org/abs/2011.14540" target="_blank">arXiv:2011.14540</a> [<a href="http://arxiv.org/pdf/2011.14540" target="_blank">pdf</a>]

<h2>Persistent Reductions in Regularized Loss Minimization for Variable Selection. (arXiv:2011.14549v1 [stat.ML])</h2>
<h3>Amin Jalali</h3>
<p>In the context of regularized loss minimization with polyhedral gauges, we
show that for a broad class of loss functions (possibly non-smooth and
non-convex) and under a simple geometric condition on the input data it is
possible to efficiently identify a subset of features which are guaranteed to
have zero coefficients in all optimal solutions in all problems with loss
functions from said class, before any iterative optimization has been performed
for the original problem. This procedure is standalone, takes only the data as
input, and does not require any calls to the loss function. Therefore, we term
this procedure as a persistent reduction for the aforementioned class of
regularized loss minimization problems. This reduction can be efficiently
implemented via an extreme ray identification subroutine applied to a
polyhedral cone formed from the datapoints. We employ an existing
output-sensitive algorithm for extreme ray identification which makes our
guarantee and algorithm applicable in ultra-high dimensional problems.
</p>
<a href="http://arxiv.org/abs/2011.14549" target="_blank">arXiv:2011.14549</a> [<a href="http://arxiv.org/pdf/2011.14549" target="_blank">pdf</a>]

<h2>A Customizable Dynamic Scenario Modeling and Data Generation Platform for Autonomous Driving. (arXiv:2011.14551v1 [cs.AI])</h2>
<h3>Jay Shenoy, Edward Kim, Xiangyu Yue, Taesung Park, Daniel Fremont, Alberto Sangiovanni-Vincentelli, Sanjit Seshia</h3>
<p>Safely interacting with humans is a significant challenge for autonomous
driving. The performance of this interaction depends on machine learning-based
modules of an autopilot, such as perception, behavior prediction, and planning.
These modules require training datasets with high-quality labels and a diverse
range of realistic dynamic behaviors. Consequently, training such modules to
handle rare scenarios is difficult because they are, by definition, rarely
represented in real-world datasets. Hence, there is a practical need to augment
datasets with synthetic data covering these rare scenarios. In this paper, we
present a platform to model dynamic and interactive scenarios, generate the
scenarios in simulation with different modalities of labeled sensor data, and
collect this information for data augmentation. To our knowledge, this is the
first integrated platform for these tasks specialized to the autonomous driving
domain.
</p>
<a href="http://arxiv.org/abs/2011.14551" target="_blank">arXiv:2011.14551</a> [<a href="http://arxiv.org/pdf/2011.14551" target="_blank">pdf</a>]

<h2>A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models. (arXiv:2011.14554v1 [cs.LG])</h2>
<h3>Jeong-Hoe Ku, JiHun Oh, YoungYoon Lee, Gaurav Pooniwala, SangJeong Lee</h3>
<p>This paper aims to provide a selective survey about knowledge
distillation(KD) framework for researchers and practitioners to take advantage
of it for developing new optimized models in the deep neural network field. To
this end, we give a brief overview of knowledge distillation and some related
works including learning using privileged information(LUPI) and generalized
distillation(GD). Even though knowledge distillation based on the
teacher-student architecture was initially devised as a model compression
technique, it has found versatile applications over various frameworks.

In this paper, we review the characteristics of knowledge distillation from
the hypothesis that the three important ingredients of knowledge distillation
are distilled knowledge and loss,teacher-student paradigm, and the distillation
process. In addition, we survey the versatility of the knowledge distillation
by studying its direct applications and its usage in combination with other
deep learning paradigms. Finally we present some future works in knowledge
distillation including explainable knowledge distillation where the analytical
analysis of the performance gain is studied and the self-supervised learning
which is a hot research topic in deep learning community.
</p>
<a href="http://arxiv.org/abs/2011.14554" target="_blank">arXiv:2011.14554</a> [<a href="http://arxiv.org/pdf/2011.14554" target="_blank">pdf</a>]

<h2>Learnable Motion Coherence for Correspondence Pruning. (arXiv:2011.14563v1 [cs.CV])</h2>
<h3>Yuan Liu, Lingjie Liu, Cheng Lin, Zhen Dong, Wenping Wang</h3>
<p>Motion coherence is an important clue for distinguishing true correspondences
from false ones. Modeling motion coherence on sparse putative correspondences
is challenging due to their sparsity and uneven distributions. Existing works
on motion coherence are sensitive to parameter settings and have difficulty in
dealing with complex motion patterns. In this paper, we introduce a network
called Laplacian Motion Coherence Network (LMCNet) to learn motion coherence
property for correspondence pruning. We propose a novel formulation of fitting
coherent motions with a smooth function on a graph of correspondences and show
that this formulation allows a closed-form solution by graph Laplacian. This
closed-form solution enables us to design a differentiable layer in a learning
framework to capture global motion coherence from putative correspondences. The
global motion coherence is further combined with local coherence extracted by
another local layer to robustly detect inlier correspondences. Experiments
demonstrate that LMCNet has superior performances to the state of the art in
relative camera pose estimation and correspondences pruning of dynamic scenes.
</p>
<a href="http://arxiv.org/abs/2011.14563" target="_blank">arXiv:2011.14563</a> [<a href="http://arxiv.org/pdf/2011.14563" target="_blank">pdf</a>]

<h2>Deep Implicit Templates for 3D Shape Representation. (arXiv:2011.14565v1 [cs.CV])</h2>
<h3>Zerong Zheng, Tao Yu, Qionghai Dai, Yebin Liu</h3>
<p>Deep implicit functions (DIFs), as a kind of 3D shape representation, are
becoming more and more popular in the 3D vision community due to their
compactness and strong representation power. However, unlike polygon mesh-based
templates, it remains a challenge to reason dense correspondences or other
semantic relationships across shapes represented by DIFs, which limits its
applications in texture transfer, shape analysis and so on. To overcome this
limitation and also make DIFs more interpretable, we propose Deep Implicit
Templates, a new 3D shape representation that supports explicit correspondence
reasoning in deep implicit representations. Our key idea is to formulate DIFs
as conditional deformations of a template implicit function. To this end, we
propose Spatial Warping LSTM, which decomposes the conditional spatial
transformation into multiple affine transformations and guarantees
generalization capability. Moreover, the training loss is carefully designed in
order to achieve high reconstruction accuracy while learning a plausible
template with accurate correspondences in an unsupervised manner. Experiments
show that our method can not only learn a common implicit template for a
collection of shapes, but also establish dense correspondences across all the
shapes simultaneously without any supervision.
</p>
<a href="http://arxiv.org/abs/2011.14565" target="_blank">arXiv:2011.14565</a> [<a href="http://arxiv.org/pdf/2011.14565" target="_blank">pdf</a>]

<h2>Gradient Sparsification Can Improve Performance of Differentially-Private Convex Machine Learning. (arXiv:2011.14572v1 [cs.LG])</h2>
<h3>Farhad Farokhi</h3>
<p>We use gradient sparsification to reduce the adverse effect of differential
privacy noise on performance of private machine learning models. To this aim,
we employ compressed sensing and additive Laplace noise to evaluate
differentially-private gradients. Noisy privacy-preserving gradients are used
to perform stochastic gradient descent for training machine learning models.
Sparsification, achieved by setting the smallest gradient entries to zero, can
reduce the convergence speed of the training algorithm. However, by
sparsification and compressed sensing, the dimension of communicated gradient
and the magnitude of additive noise can be reduced. The interplay between these
effects determines whether gradient sparsification improves the performance of
differentially-private machine learning models. We investigate this
analytically in the paper. We prove that, in small-data regime with tight
privacy budget, compression can improve performance of privacy-preserving
machine learning models. However, in big-data regime, compression does not
necessarily improve the performance. Intuitively, this is because the effect of
privacy-preserving noise is minimal in big-data regime and thus improvements
from gradient sparsification cannot compensate for its slower convergence.
</p>
<a href="http://arxiv.org/abs/2011.14572" target="_blank">arXiv:2011.14572</a> [<a href="http://arxiv.org/pdf/2011.14572" target="_blank">pdf</a>]

<h2>DUT:Learning Video Stabilization by Simply Watching Unstable Videos. (arXiv:2011.14574v1 [cs.CV])</h2>
<h3>Yufei Xu, Jing Zhang, Stephen J. Mayban, Dacheng Tao</h3>
<p>We propose a Deep Unsupervised Trajectory-based stabilization framework (DUT)
in this paper\footnote{Our code is available at
https://github.com/Annbless/DUTCode.}. Traditional stabilizers focus on
trajectory-based smoothing, which is controllable but fragile in occluded and
textureless cases regarding the usage of hand-crafted features. On the other
hand, previous deep video stabilizers directly generate stable videos in a
supervised manner without explicit trajectory estimation, which is robust but
less controllable and the appropriate paired data are hard to obtain. To
construct a controllable and robust stabilizer, DUT makes the first attempt to
stabilize unstable videos by explicitly estimating and smoothing trajectories
in an unsupervised deep learning manner, which is composed of a DNN-based
keypoint detector and motion estimator to generate grid-based trajectories, and
a DNN-based trajectory smoother to stabilize videos. We exploit both the nature
of continuity in motion and the consistency of keypoints and grid vertices
before and after stabilization for unsupervised training. Experiment results on
public benchmarks show that DUT outperforms representative state-of-the-art
methods both qualitatively and quantitatively.
</p>
<a href="http://arxiv.org/abs/2011.14574" target="_blank">arXiv:2011.14574</a> [<a href="http://arxiv.org/pdf/2011.14574" target="_blank">pdf</a>]

<h2>Where Should We Begin? A Low-Level Exploration of Weight Initialization Impact on Quantized Behaviour of Deep Neural Networks. (arXiv:2011.14578v1 [cs.LG])</h2>
<h3>Stone Yun, Alexander Wong</h3>
<p>With the proliferation of deep convolutional neural network (CNN) algorithms
for mobile processing, limited precision quantization has become an essential
tool for CNN efficiency. Consequently, various works have sought to design
fixed precision quantization algorithms and quantization-focused optimization
techniques that minimize quantization induced performance degradation. However,
there is little concrete understanding of how various CNN design decisions/best
practices affect quantized inference behaviour. Weight initialization
strategies are often associated with solving issues such as vanishing/exploding
gradients but an often-overlooked aspect is their impact on the final trained
distributions of each layer. We present an in-depth, fine-grained ablation
study of the effect of different weights initializations on the final
distributions of weights and activations of different CNN architectures. The
fine-grained, layerwise analysis enables us to gain deep insights on how
initial weights distributions will affect final accuracy and quantized
behaviour. To our best knowledge, we are the first to perform such a low-level,
in-depth quantitative analysis of weights initialization and its effect on
quantized behaviour.
</p>
<a href="http://arxiv.org/abs/2011.14578" target="_blank">arXiv:2011.14578</a> [<a href="http://arxiv.org/pdf/2011.14578" target="_blank">pdf</a>]

<h2>End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences. (arXiv:2011.14579v1 [cs.CV])</h2>
<h3>Zhijian~Qiao, Zhe~Liu, Chuanzhe~Suo, Huanshu~Wei, Zhuowen~Shen, Hesheng~Wang</h3>
<p>3D Point cloud registration is still a very challenging topic due to the
difficulty in finding the rigid transformation between two point clouds with
partial correspondences, and it's even harder in the absence of any initial
estimation information. In this paper, we present an end-to-end deep-learning
based approach to resolve the point cloud registration problem. Firstly, the
revised LPD-Net is introduced to extract features and aggregate them with the
graph network. Secondly, the self-attention mechanism is utilized to enhance
the structure information in the point cloud and the cross-attention mechanism
is designed to enhance the corresponding information between the two input
point clouds. Based on which, the virtual corresponding points can be generated
by a soft pointer based method, and finally, the point cloud registration
problem can be solved by implementing the SVD method. Comparison results in
ModelNet40 dataset validate that the proposed approach reaches the
state-of-the-art in point cloud registration tasks and experiment resutls in
KITTI dataset validate the effectiveness of the proposed approach in real
applications.
</p>
<a href="http://arxiv.org/abs/2011.14579" target="_blank">arXiv:2011.14579</a> [<a href="http://arxiv.org/pdf/2011.14579" target="_blank">pdf</a>]

<h2>Robust and Private Learning of Halfspaces. (arXiv:2011.14580v1 [cs.LG])</h2>
<h3>Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Thao Nguyen</h3>
<p>In this work, we study the trade-off between differential privacy and
adversarial robustness under L2-perturbations in the context of learning
halfspaces. We prove nearly tight bounds on the sample complexity of robust
private learning of halfspaces for a large regime of parameters. A highlight of
our results is that robust and private learning is harder than robust or
private learning alone. We complement our theoretical analysis with
experimental results on the MNIST and USPS datasets, for a learning algorithm
that is both differentially private and adversarially robust.
</p>
<a href="http://arxiv.org/abs/2011.14580" target="_blank">arXiv:2011.14580</a> [<a href="http://arxiv.org/pdf/2011.14580" target="_blank">pdf</a>]

<h2>ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition. (arXiv:2011.14584v1 [cs.CV])</h2>
<h3>Hsin-Pai Cheng, Feng Liang, Meng Li, Bowen Cheng, Feng Yan, Hai Li, Vikas Chandra, Yiran Chen</h3>
<p>Scale variance among different sizes of body parts and objects is a
challenging problem for visual recognition tasks. Existing works usually design
dedicated backbone or apply Neural architecture Search(NAS) for each task to
tackle this challenge. However, existing works impose significant limitations
on the design or search space. To solve these problems, we present ScaleNAS, a
one-shot learning method for exploring scale-aware representations. ScaleNAS
solves multiple tasks at a time by searching multi-scale feature aggregation.
ScaleNAS adopts a flexible search space that allows an arbitrary number of
blocks and cross-scale feature fusions. To cope with the high search cost
incurred by the flexible space, ScaleNAS employs one-shot learning for
multi-scale supernet driven by grouped sampling and evolutionary search.
Without further retraining, ScaleNet can be directly deployed for different
visual recognition tasks with superior performance. We use ScaleNAS to create
high-resolution models for two different tasks, ScaleNet-P for human pose
estimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S
outperform existing manually crafted and NAS-based methods in both tasks. When
applying ScaleNet-P to bottom-up human pose estimation, it surpasses the
state-of-the-art HigherHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on
COCO test-dev, achieving new state-of-the-art result.
</p>
<a href="http://arxiv.org/abs/2011.14584" target="_blank">arXiv:2011.14584</a> [<a href="http://arxiv.org/pdf/2011.14584" target="_blank">pdf</a>]

<h2>Just One Moment: Inconspicuous One Frame Attack on Deep Action Recognition. (arXiv:2011.14585v1 [cs.CV])</h2>
<h3>Jaehui Hwang, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee</h3>
<p>The video-based action recognition task has been extensively studied in
recent years. In this paper, we study the vulnerability of deep learning-based
action recognition methods against the adversarial attack using a new one frame
attack that adds an inconspicuous perturbation to only a single frame of a
given video clip. We investigate the effectiveness of our one frame attack on
state-of-the-art action recognition models, along with thorough analysis of the
vulnerability in terms of their model structure and perceivability of the
perturbation. Our method shows high fooling rates and produces hardly
perceivable perturbation to human observers, which is evaluated by a subjective
test. In addition, we present a video-agnostic approach that finds a universal
perturbation.
</p>
<a href="http://arxiv.org/abs/2011.14585" target="_blank">arXiv:2011.14585</a> [<a href="http://arxiv.org/pdf/2011.14585" target="_blank">pdf</a>]

<h2>FactorizeNet: Progressive Depth Factorization for Efficient Network Architecture Exploration Under Quantization Constraints. (arXiv:2011.14586v1 [cs.CV])</h2>
<h3>Stone Yun, Alexander Wong</h3>
<p>Depth factorization and quantization have emerged as two of the principal
strategies for designing efficient deep convolutional neural network (CNN)
architectures tailored for low-power inference on the edge. However, there is
still little detailed understanding of how different depth factorization
choices affect the final, trained distributions of each layer in a CNN,
particularly in the situation of quantized weights and activations. In this
study, we introduce a progressive depth factorization strategy for efficient
CNN architecture exploration under quantization constraints. By algorithmically
increasing the granularity of depth factorization in a progressive manner, the
proposed strategy enables a fine-grained, low-level analysis of layer-wise
distributions. Thus enabling the gain of in-depth, layer-level insights on
efficiency-accuracy tradeoffs under fixed-precision quantization. Such a
progressive depth factorization strategy also enables efficient identification
of the optimal depth-factorized macroarchitecture design (which we will refer
to here as FactorizeNet) based on the desired efficiency-accuracy requirements.
</p>
<a href="http://arxiv.org/abs/2011.14586" target="_blank">arXiv:2011.14586</a> [<a href="http://arxiv.org/pdf/2011.14586" target="_blank">pdf</a>]

<h2>Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation. (arXiv:2011.14589v1 [cs.CV])</h2>
<h3>Tianze Gao, Huihui Pan, Huijun Gao</h3>
<p>Monocular 3D object detection is an active research topic due to its
challenging nature and broad applying prospects. In this work, a single-stage
keypoint-based network, named as FADNet, is presented to address the task of
monocular 3D object detection with autonomous driving as the target
application. In contrast to previous keypoint-based methods which adopt
identical layouts for output branches, we propose to divide the output
modalities into different groups according to the estimating difficulty,
whereby different groups are treated differently. To this end, a convolutional
gated recurrent unit (convGRU) is embedded into our network to enable
sequential feature association across the convolutional features in different
groups. The purpose of sequential feature association is to improve the
accuracy of harder estimations under the guidance of easier ones. It is also
observed that such design contributes to the geometric consistency between 2D
and 3D estimations. Another contribution of this work is the strategy of depth
hint augmentation. To provide characterized depth patterns as hints for depth
estimation, a dedicated depth hint module is designed to generate row-wise
features named as depth hints, which are explicitly supervised in a bin-wise
manner. In the training stage, the regression outputs are uniformly encoded to
enable loss disentanglement. The 2D loss term is further adapted to be
depth-aware for improving the detection accuracy of small objects. The
contributions of this work are validated by conducting experiments and ablation
study on the KITTI3D benchmark. Without utilizing depth priors, post
optimization, or other refinement modules, our network achieves the performance
on par with state-of-the-art methods while maintaining a decent running speed.
The code is available at https://github.com/gtzly/FADNet.
</p>
<a href="http://arxiv.org/abs/2011.14589" target="_blank">arXiv:2011.14589</a> [<a href="http://arxiv.org/pdf/2011.14589" target="_blank">pdf</a>]

<h2>Incremental Learning via Rate Reduction. (arXiv:2011.14593v1 [cs.LG])</h2>
<h3>Ziyang Wu, Christina Baek, Chong You, Yi Ma</h3>
<p>Current deep learning architectures suffer from catastrophic forgetting, a
failure to retain knowledge of previously learned classes when incrementally
trained on new classes. The fundamental roadblock faced by deep learning
methods is that deep learning models are optimized as "black boxes," making it
difficult to properly adjust the model parameters to preserve knowledge about
previously seen data. To overcome the problem of catastrophic forgetting, we
propose utilizing an alternative "white box" architecture derived from the
principle of rate reduction, where each layer of the network is explicitly
computed without back propagation. Under this paradigm, we demonstrate that,
given a pre-trained network and new data classes, our approach can provably
construct a new network that emulates joint training with all past and new
classes. Finally, our experiments show that our proposed learning algorithm
observes significantly less decay in classification performance, outperforming
state of the art methods on MNIST and CIFAR-10 by a large margin and justifying
the use of "white box" algorithms for incremental learning even for
sufficiently complex image data.
</p>
<a href="http://arxiv.org/abs/2011.14593" target="_blank">arXiv:2011.14593</a> [<a href="http://arxiv.org/pdf/2011.14593" target="_blank">pdf</a>]

<h2>A CRF-based Framework for Tracklet Inactivation in Online Multi-Object Tracking. (arXiv:2011.14594v1 [cs.CV])</h2>
<h3>Tianze Gao, Huihui Pan, Zidong Wang, Huijun Gao</h3>
<p>Online multi-object tracking (MOT) is an active research topic in the domain
of computer vision. In this paper, a CRF-based framework is put forward to
tackle the tracklet inactivation issues in online MOT problems. We apply the
proposed framework to one of the state-of-the-art online MOT trackers,
Tracktor++. The baseline algorithm for online MOT has the drawback of simple
strategy on tracklet inactivation, which relies merely on tracking hypotheses'
classification scores partitioned by using a fixed threshold. To overcome such
a drawback, a discrete conditional random field (CRF) is developed to exploit
the intra-frame relationship between tracking hypotheses. Separate sets of
feature functions are designed for the unary and binary terms in the CRF so as
to cope with various challenges in practical situations. The hypothesis
filtering and dummy nodes techniques are employed to handle the problem of
varying CRF nodes in the MOT context. In this paper, the inference of CRF is
achieved by using the loopy belief propagation algorithm, and the parameters of
the CRF are determined by utilizing the maximum likelihood estimation method.
Experimental results demonstrate that the developed tracker with our CRF-based
framework outperforms the baseline on the MOT16 and MOT17 datasets. The
extensibility of the proposed method is further validated by an extensive
experiment.
</p>
<a href="http://arxiv.org/abs/2011.14594" target="_blank">arXiv:2011.14594</a> [<a href="http://arxiv.org/pdf/2011.14594" target="_blank">pdf</a>]

<h2>Video Self-Stitching Graph Network for Temporal Action Localization. (arXiv:2011.14598v1 [cs.CV])</h2>
<h3>Chen Zhao, Ali Thabet, Bernard Ghanem</h3>
<p>Temporal action localization (TAL) in videos is a challenging task,
especially due to the large scale variation of actions. In the data, short
actions usually occupy the major proportion, but have the lowest performance
with all current methods. In this paper, we confront the challenge of short
actions and propose a multi-level cross-scale solution dubbed as video
self-stitching graph network (VSGN). We have two key components in VSGN: video
self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we
focus on a short period of a video and magnify it along the temporal dimension
to obtain a larger scale. By our self-stitching approach, we are able to
utilize the original clip and its magnified counterpart in one input sequence
to take advantage of the complementary properties of both scales. The xGPN
component further exploits the cross-scale correlations by a pyramid of
cross-scale graph networks, each containing a hybrid temporal-graph module to
aggregate features from across scales as well as within the same scale. Our
VSGN not only enhances the feature representations, but also generates more
positive anchors for short actions and more short training samples. Experiments
demonstrate that VSGN obviously improves the localization performance of short
actions as well as achieving the state-of-the-art overall performance on
ActivityNet-v1.3, reaching an average mAP of 35.07 %.
</p>
<a href="http://arxiv.org/abs/2011.14598" target="_blank">arXiv:2011.14598</a> [<a href="http://arxiv.org/pdf/2011.14598" target="_blank">pdf</a>]

<h2>REaL: Real-time Face Detection and Recognition Using Euclidean Space and Likelihood Estimation. (arXiv:2011.14603v1 [cs.CV])</h2>
<h3>Sandesh Ramesh, Manoj Kumar M V, K Aditya Shastry</h3>
<p>Detecting and recognizing faces accurately has always been a challenge.
Differentiating facial features, training images, and producing quick results
require a lot of computation. The REaL system we have proposed in this paper
discusses its functioning and ways in which computations can be carried out in
a short period. REaL experiments are carried out on live images and the
recognition rates are promising. The system is also successful in removing
non-human objects from its calculations. The system uses a local database to
store captured images and feeds the neural network frequently. The captured
images are cropped automatically to remove unwanted noise. The system
calculates the Euler angles and the probability of whether the face is smiling,
has its left eye, and right eyes open or not.
</p>
<a href="http://arxiv.org/abs/2011.14603" target="_blank">arXiv:2011.14603</a> [<a href="http://arxiv.org/pdf/2011.14603" target="_blank">pdf</a>]

<h2>Zero-Shot Calibration of Fisheye Cameras. (arXiv:2011.14607v1 [cs.CV])</h2>
<h3>Jae-Yeong Lee</h3>
<p>In this paper, we present a novel zero-shot camera calibration method that
estimates camera parameters with no calibration image. It is common sense that
we need at least one or more pattern images for camera calibration. However,
the proposed method estimates camera parameters from the horizontal and
vertical field of view information of the camera without any image acquisition.
The proposed method is particularly useful for wide-angle or fisheye cameras
that have large image distortion. Image distortion is modeled in the way
fisheye lenses are designed and estimated based on the square pixel assumption
of the image sensors. The calibration accuracy of the proposed method is
evaluated on eight different commercial cameras qualitatively and
quantitatively, and compared with conventional calibration methods. The
experimental results show that the calibration accuracy of the zero-shot method
is comparable to conventional full calibration results. The method can be used
as a practical alternative in real applications where individual calibration is
difficult or impractical, and in most field applications where calibration
accuracy is less critical. Moreover, the estimated camera parameters by the
method can also be used to provide proper initialization of any existing
calibration methods, making them to converge more stably and avoid local
minima.
</p>
<a href="http://arxiv.org/abs/2011.14607" target="_blank">arXiv:2011.14607</a> [<a href="http://arxiv.org/pdf/2011.14607" target="_blank">pdf</a>]

<h2>SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses. (arXiv:2011.14611v1 [cs.CV])</h2>
<h3>Jinlong Fan, Jing Zhang, Dacheng Tao</h3>
<p>Deep learning has demonstrated its power in image rectification by leveraging
the representation capacity of deep neural networks via supervised training
based on a large-scale synthetic dataset. However, the model may overfit the
synthetic images and generalize not well on real-world fisheye images due to
the limited universality of a specific distortion model and the lack of
explicitly modeling the distortion and rectification process. In this paper, we
propose a novel self-supervised image rectification (SIR) method based on an
important insight that the rectified results of distorted images of the same
scene from different lens should be the same. Specifically, we devise a new
network architecture with a shared encoder and several prediction heads, each
of which predicts the distortion parameter of a specific distortion model. We
further leverage a differentiable warping module to generate the rectified
images and re-distorted images from the distortion parameters and exploit the
intra- and inter-model consistency between them during training, thereby
leading to a self-supervised learning scheme without the need for ground-truth
distortion parameters or normal images. Experiments on synthetic dataset and
real-world fisheye images demonstrate that our method achieves comparable or
even better performance than the supervised baseline method and representative
state-of-the-art methods. Self-supervised learning also improves the
universality of distortion models while keeping their self-consistency.
</p>
<a href="http://arxiv.org/abs/2011.14611" target="_blank">arXiv:2011.14611</a> [<a href="http://arxiv.org/pdf/2011.14611" target="_blank">pdf</a>]

<h2>DeepCloth: Neural Garment Representation for Shape and Style Editing. (arXiv:2011.14619v1 [cs.CV])</h2>
<h3>Zhaoqi Su, Tao Yu, Yangang Wang, Yipeng Li, Yebin Liu</h3>
<p>Garment representation, animation and editing is a challenging topic in the
area of computer vision and graphics. Existing methods cannot perform smooth
and reasonable garment transition under different shape styles and topologies.
In this work, we introduce a novel method, termed as DeepCloth, to establish a
unified garment representation framework enabling free and smooth garment style
transition. Our key idea is to represent garment geometry by a "UV-position map
with mask", which potentially allows the description of various garments with
different shapes and topologies. Furthermore, we learn a continuous feature
space mapped from the above UV space, enabling garment shape editing and
transition by controlling the garment features. Finally, we demonstrate
applications of garment animation, reconstruction and editing based on our
neural garment representation and encoding method. To conclude, with the
proposed DeepCloth, we move a step forward on establishing a more flexible and
general 3D garment digitization framework. Experiments demonstrate that our
method can achieve the state-of-the-art garment modeling results compared with
the previous methods.
</p>
<a href="http://arxiv.org/abs/2011.14619" target="_blank">arXiv:2011.14619</a> [<a href="http://arxiv.org/pdf/2011.14619" target="_blank">pdf</a>]

<h2>RegFlow: Probabilistic Flow-based Regression for Future Prediction. (arXiv:2011.14620v1 [cs.LG])</h2>
<h3>Maciej Zi&#x119;ba, Marcin Przewi&#x119;&#x17a;likowski, Marek &#x15a;mieja, Jacek Tabor, Tomasz Trzcinski, Przemys&#x142;aw Spurek</h3>
<p>Predicting future states or actions of a given system remains a fundamental,
yet unsolved challenge of intelligence, especially in the scope of complex and
non-deterministic scenarios, such as modeling behavior of humans. Existing
approaches provide results under strong assumptions concerning unimodality of
future states, or, at best, assuming specific probability distributions that
often poorly fit to real-life conditions. In this work we introduce a robust
and flexible probabilistic framework that allows to model future predictions
with virtually no constrains regarding the modality or underlying probability
distribution. To achieve this goal, we leverage a hypernetwork architecture and
train a continuous normalizing flow model. The resulting method dubbed RegFlow
achieves state-of-the-art results on several benchmark datasets, outperforming
competing approaches by a significant margin.
</p>
<a href="http://arxiv.org/abs/2011.14620" target="_blank">arXiv:2011.14620</a> [<a href="http://arxiv.org/pdf/2011.14620" target="_blank">pdf</a>]

<h2>Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images. (arXiv:2011.14631v1 [cs.CV])</h2>
<h3>Yuemei Zhou, Gaochang Wu, Ying Fu, Kun Li, Yebin Liu</h3>
<p>The combination of various cameras is enriching the way of computational
photography, among which referencebased super-resolution (RefSR) plays a
critical role in multiscale imaging systems. However, existing RefSR approaches
fail to accomplish high-fidelity super-resolution under a large resolution gap,
e.g., 8x upscaling, due to the less consideration of underneath scene
structure. In this paper, we aim to solve the RefSR problem (in actual
multiscale camera systems) inspired by multiplane images (MPI) representation.
Specifically, we propose Cross-MPI, an end-to-end RefSR network composed of a
novel planeaware attention-based MPI mechanism, a multiscale guided upsampling
module as well as a super-resolution (SR) synthesis and fusion module. Instead
of using a direct and exhaustive matching between the cross-scale stereo, the
proposed plane-aware attention mechanism fully utilizes the concealed scene
structure for efficient attention-based correspondence searching. Further
combined with a gentle coarse-to-fine guided upsampling strategy, the proposed
Cross-MPI is able to achieve a robust and accurate detail transmission.
Experimental results on both digital synthesized and optical zoomed cross-scale
data show the Cross- MPI framework can achieve superior performance against the
existing RefSR methods, and is a real fit for actual multiscale camera systems
even with large scale differences.
</p>
<a href="http://arxiv.org/abs/2011.14631" target="_blank">arXiv:2011.14631</a> [<a href="http://arxiv.org/pdf/2011.14631" target="_blank">pdf</a>]

<h2>Optimizing the Neural Architecture of Reinforcement Learning Agents. (arXiv:2011.14632v1 [cs.LG])</h2>
<h3>N. Mazyavkina, S. Moustafa, I. Trofimov, E. Burnaev</h3>
<p>Reinforcement learning (RL) enjoyed significant progress over the last years.
One of the most important steps forward was the wide application of neural
networks. However, architectures of these neural networks are typically
constructed manually. In this work, we study recently proposed neural
architecture search (NAS) methods for optimizing the architecture of RL agents.
We carry out experiments on the Atari benchmark and conclude that modern NAS
methods find architectures of RL agents outperforming a manually selected one.
</p>
<a href="http://arxiv.org/abs/2011.14632" target="_blank">arXiv:2011.14632</a> [<a href="http://arxiv.org/pdf/2011.14632" target="_blank">pdf</a>]

<h2>TSSRGCN: Temporal Spectral Spatial Retrieval Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2011.14638v1 [cs.LG])</h2>
<h3>Xu Chen, Yuanxing Zhang, Lun Du, Zheng Fang, Yi Ren, Kaigui Bian, Kunqing Xie</h3>
<p>Traffic flow forecasting is of great significance for improving the
efficiency of transportation systems and preventing emergencies. Due to the
highly non-linearity and intricate evolutionary patterns of short-term and
long-term traffic flow, existing methods often fail to take full advantage of
spatial-temporal information, especially the various temporal patterns with
different period shifting and the characteristics of road segments. Besides,
the globality representing the absolute value of traffic status indicators and
the locality representing the relative value have not been considered
simultaneously. This paper proposes a neural network model that focuses on the
globality and locality of traffic networks as well as the temporal patterns of
traffic data. The cycle-based dilated deformable convolution block is designed
to capture different time-varying trends on each node accurately. Our model can
extract both global and local spatial information since we combine two graph
convolutional network methods to learn the representations of nodes and edges.
Experiments on two real-world datasets show that the model can scrutinize the
spatial-temporal correlation of traffic data, and its performance is better
than the compared state-of-the-art methods. Further analysis indicates that the
locality and globality of the traffic networks are critical to traffic flow
prediction and the proposed TSSRGCN model can adapt to the various temporal
traffic patterns.
</p>
<a href="http://arxiv.org/abs/2011.14638" target="_blank">arXiv:2011.14638</a> [<a href="http://arxiv.org/pdf/2011.14638" target="_blank">pdf</a>]

<h2>Vehicle Reconstruction and Texture Estimation Using Deep Implicit Semantic Template Mapping. (arXiv:2011.14642v1 [cs.CV])</h2>
<h3>Xiaochen Zhao, Zerong Zheng, Chaonan Ji, Zhenyi Liu, Yirui Luo, Tao Yu, Jinli Suo, Qionghai Dai, Yebin Liu</h3>
<p>We introduce VERTEX, an effective solution to recover 3D shape and intrinsic
texture of vehicles from uncalibrated monocular input in real-world street
environments. To fully utilize the template prior of vehicles, we propose a
novel geometry and texture joint representation, based on implicit semantic
template mapping. Compared to existing representations which infer 3D texture
distribution, our method explicitly constrains the texture distribution on the
2D surface of the template as well as avoids limitations of fixed resolution
and topology. Moreover, by fusing the global and local features together, our
approach is capable to generate consistent and detailed texture in both visible
and invisible areas. We also contribute a new synthetic dataset containing 830
elaborate textured car models labeled with sparse key points and rendered using
Physically Based Rendering (PBRT) system with measured HDRI skymaps to obtain
highly realistic images. Experiments demonstrate the superior performance of
our approach on both testing dataset and in-the-wild images. Furthermore, the
presented technique enables additional applications such as 3D vehicle texture
transfer and material identification.
</p>
<a href="http://arxiv.org/abs/2011.14642" target="_blank">arXiv:2011.14642</a> [<a href="http://arxiv.org/pdf/2011.14642" target="_blank">pdf</a>]

<h2>Feature Space Singularity for Out-of-Distribution Detection. (arXiv:2011.14654v1 [stat.ML])</h2>
<h3>Haiwen Huang, Zhihan Li, Lulu Wang, Sishuo Chen, Bin Dong, Xinyu Zhou</h3>
<p>Out-of-Distribution (OoD) detection is important for building safe artificial
intelligence systems. However, current OoD detection methods still cannot meet
the performance requirements for practical deployment. In this paper, we
propose a simple yet effective algorithm based on a novel observation: in a
trained neural network, OoD samples with bounded norms well concentrate in the
feature space. We call the center of OoD features the Feature Space Singularity
(FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD
samples can be identified by taking a threshold on the FSSD. Our analysis of
the phenomenon reveals why our algorithm works. We demonstrate that our
algorithm achieves state-of-the-art performance on various OoD detection
benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test
data and can be further enhanced by ensembling. These make FSSD a promising
algorithm to be employed in real world. We release our code at
\url{https://github.com/megvii-research/FSSD_OoD_Detection}.
</p>
<a href="http://arxiv.org/abs/2011.14654" target="_blank">arXiv:2011.14654</a> [<a href="http://arxiv.org/pdf/2011.14654" target="_blank">pdf</a>]

<h2>SplitNet: Divide and Co-training. (arXiv:2011.14660v1 [cs.CV])</h2>
<h3>Shuai Zhao, Liguang Zhou, Wenxiao Wang, Deng Cai, Tin Lun Lam, Yangsheng Xu</h3>
<p>The width of a neural network matters since increasing the width will
necessarily increase the model capacity. However, the performance of a network
does not improve linearly with the width and soon gets saturated. To tackle
this problem, we propose to increase the number of networks rather than purely
scaling up the width. To prove it, one large network is divided into several
small ones, and each of these small networks has a fraction of the original
one's parameters. We then train these small networks together and make them see
various views of the same data to learn different and complementary knowledge.
During this co-training process, networks can also learn from each other. As a
result, small networks can achieve better ensemble performance than the large
one with few or no extra parameters or FLOPs. \emph{This reveals that the
number of networks is a new dimension of effective model scaling, besides
depth/width/resolution}. Small networks can also achieve faster inference speed
than the large one by concurrent running on different devices. We validate the
idea -- increasing the number of networks is a new dimension of effective model
scaling -- with different network architectures on common benchmarks through
extensive experiments. The code is available at
\url{https://github.com/mzhaoshuai/SplitNet-Divide-and-Co-training}.
</p>
<a href="http://arxiv.org/abs/2011.14660" target="_blank">arXiv:2011.14660</a> [<a href="http://arxiv.org/pdf/2011.14660" target="_blank">pdf</a>]

<h2>Revisiting Unsupervised Meta-Learning: Amplifying or Compensating for the Characteristics of Few-Shot Tasks. (arXiv:2011.14663v1 [cs.CV])</h2>
<h3>Han-Jia Ye, Lu Han, De-Chuan Zhan</h3>
<p>Meta-learning becomes a practical approach towards few-shot image
classification, where a visual recognition system is constructed with limited
annotated data. Inductive bias such as embedding is learned from a base class
set with ample labeled examples and then generalizes to few-shot tasks with
novel classes. Surprisingly, we find that the base class set labels are not
necessary, and discriminative embeddings could be meta-learned in an
unsupervised manner. Comprehensive analyses indicate two modifications -- the
semi-normalized distance metric and the sufficient sampling -- improves
unsupervised meta-learning (UML) significantly. Based on the modified baseline,
we further amplify or compensate for the characteristic of tasks when training
a UML model. First, mixed embeddings are incorporated to increase the
difficulty of few-shot tasks. Next, we utilize a task-specific embedding
transformation to deal with the specific properties among tasks, maintaining
the generalization ability into the vanilla embeddings. Experiments on few-shot
learning benchmarks verify that our approaches outperform previous UML methods
by a 4-10% performance gap, and embeddings learned with our UML achieve
comparable or even better performance than its supervised variants.
</p>
<a href="http://arxiv.org/abs/2011.14663" target="_blank">arXiv:2011.14663</a> [<a href="http://arxiv.org/pdf/2011.14663" target="_blank">pdf</a>]

<h2>Why Convolutional Networks Learn Oriented Bandpass Filters: Theory and Empirical Support. (arXiv:2011.14665v1 [cs.CV])</h2>
<h3>Isma Hadji, Richard P. Wildes</h3>
<p>It has been repeatedly observed that convolutional architectures when applied
to image understanding tasks learn oriented bandpass filters. A standard
explanation of this result is that these filters reflect the structure of the
images that they have been exposed to during training: Natural images typically
are locally composed of oriented contours at various scales and oriented
bandpass filters are matched to such structure. We offer an alternative
explanation based not on the structure of images, but rather on the structure
of convolutional architectures. In particular, complex exponentials are the
eigenfunctions of convolution. These eigenfunctions are defined globally;
however, convolutional architectures operate locally. To enforce locality, one
can apply a windowing function to the eigenfunctions, which leads to oriented
bandpass filters as the natural operators to be learned with convolutional
architectures. From a representational point of view, these filters allow for a
local systematic way to characterize and operate on an image or other signal.
We offer empirical support for the hypothesis that convolutional networks learn
such filters at all of their convolutional layers. While previous research has
shown evidence of filters having oriented bandpass characteristics at early
layers, ours appears to be the first study to document the predominance of such
filter characteristics at all layers. Previous studies have missed this
observation because they have concentrated on the cumulative compositional
effects of filtering across layers, while we examine the filter characteristics
that are present at each layer.
</p>
<a href="http://arxiv.org/abs/2011.14665" target="_blank">arXiv:2011.14665</a> [<a href="http://arxiv.org/pdf/2011.14665" target="_blank">pdf</a>]

<h2>AFD-Net: Adaptive Fully-Dual Network for Few-Shot Object Detection. (arXiv:2011.14667v1 [cs.CV])</h2>
<h3>Longyao Liu, Bo Ma, Yulin Zhang, Xin Yi, Haozhi Li</h3>
<p>Few-shot object detection (FSOD) aims at learning a detector that can fast
adapt to previously unseen objects with scarce annotated examples, which is
challenging and demanding. Existing methods solve this problem by performing
subtasks of classification and localization utilizing a shared component (e.g.,
RoI head) in a detector, yet few of them take the preference difference in
embedding space of two subtasks into consideration. In this paper, we carefully
analyze the characteristics of FSOD and present that a general few-shot
detector should consider the explicit decomposition of two subtasks, and
leverage information from both of them for enhancing feature representations.
To the end, we propose a simple yet effective Adaptive Fully-Dual Network
(AFD-Net). Specifically, we extend Faster R-CNN by introducing Dual Query
Encoder and Dual Attention Generator for separate feature extraction, and Dual
Aggregator for separate model reweighting. Spontaneously, separate decision
making is achieved with the R-CNN detector. Besides, for the acquisition of
enhanced feature representations, we further introduce Adaptive Fusion
Mechanism to adaptively perform feature fusion suitable for the specific
subtask. Extensive experiments on PASCAL VOC and MS COCO in various settings
show that, our method achieves new state-of-the-art performance by a large
margin, demonstrating its effectiveness and generalization ability.
</p>
<a href="http://arxiv.org/abs/2011.14667" target="_blank">arXiv:2011.14667</a> [<a href="http://arxiv.org/pdf/2011.14667" target="_blank">pdf</a>]

<h2>Where to Explore Next? ExHistCNN for History-aware Autonomous 3D Exploration. (arXiv:2011.14669v1 [cs.CV])</h2>
<h3>Yiming Wang, Alessio Del Bue</h3>
<p>In this work we address the problem of autonomous 3D exploration of an
unknown indoor environment using a depth camera. We cast the problem as the
estimation of the Next Best View (NBV) that maximises the coverage of the
unknown area. We do this by re-formulating NBV estimation as a classification
problem and we propose a novel learning-based metric that encodes both, the
current 3D observation (a depth frame) and the history of the ongoing
reconstruction. One of the major contributions of this work is about
introducing a new representation for the 3D reconstruction history as an
auxiliary utility map which is efficiently coupled with the current depth
observation. With both pieces of information, we train a light-weight CNN,
named ExHistCNN, that estimates the NBV as a set of directions towards which
the depth sensor finds most unexplored areas. We perform extensive evaluation
on both synthetic and real room scans demonstrating that the proposed ExHistCNN
is able to approach the exploration performance of an oracle using the complete
knowledge of the 3D environment.
</p>
<a href="http://arxiv.org/abs/2011.14669" target="_blank">arXiv:2011.14669</a> [<a href="http://arxiv.org/pdf/2011.14669" target="_blank">pdf</a>]

<h2>Meta Batch-Instance Normalization for Generalizable Person Re-Identification. (arXiv:2011.14670v1 [cs.CV])</h2>
<h3>Seokeon Choi, Taekyung Kim, Minki Jeong, Hyoungseob Park, Changick Kim</h3>
<p>Although supervised person re-identification (Re-ID) methods have shown
impressive performance, they suffer from a poor generalization capability on
unseen domains. Therefore, generalizable Re-ID has recently attracted growing
attention. Many existing methods have employed an instance normalization
technique to reduce style variations, but the loss of discriminative
information could not be avoided. In this paper, we propose a novel
generalizable Re-ID framework, named Meta Batch-Instance Normalization
(MetaBIN). Our main idea is to generalize normalization layers by simulating
unsuccessful generalization scenarios beforehand in the meta-learning pipeline.
To this end, we combine learnable batch-instance normalization layers with
meta-learning and investigate the challenging cases caused by both batch and
instance normalization layers. Moreover, we diversify the virtual simulations
via our meta-train loss accompanied by a cyclic inner-updating manner to boost
generalization capability. After all, the MetaBIN framework prevents our model
from overfitting to the given source styles and improves the generalization
capability to unseen domains without additional data augmentation or
complicated network design. Extensive experimental results show that our model
outperforms the state-of-the-art methods on the large-scale domain
generalization Re-ID benchmark.
</p>
<a href="http://arxiv.org/abs/2011.14670" target="_blank">arXiv:2011.14670</a> [<a href="http://arxiv.org/pdf/2011.14670" target="_blank">pdf</a>]

<h2>HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation. (arXiv:2011.14672v1 [cs.CV])</h2>
<h3>Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, Cewu Lu</h3>
<p>Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh
for the human body by estimating several parameters. However, learning the
abstract parameters is a highly non-linear process and suffers from image-model
misalignment, leading to mediocre model performance. In contrast, 3D keypoint
estimation methods combine deep CNN network with the volumetric representation
to achieve pixel-level localization accuracy but may predict unrealistic body
structure. In this paper, we address the above issues by bridging the gap
between body mesh estimation and 3D keypoint estimation. We propose a novel
hybrid inverse kinematics solution (HybrIK). HybrIK directly transforms
accurate 3D joints to relative body-part rotations for 3D body mesh
reconstruction, via the twist-and-swing decomposition. The swing rotation is
analytically solved with 3D joints, and the twist rotation is derived from the
visual cues through the neural network. We show that HybrIK preserves both the
accuracy of 3D pose and the realistic body structure of the parametric human
model, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than
the pure 3D keypoint estimation methods. Without bells and whistles, the
proposed method surpasses the state-of-the-art methods by a large margin on
various 3D human pose and shape benchmarks. As an illustrative example, HybrIK
outperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW
dataset. Our code is available at https://github.com/Jeff-sjtu/HybrIK.
</p>
<a href="http://arxiv.org/abs/2011.14672" target="_blank">arXiv:2011.14672</a> [<a href="http://arxiv.org/pdf/2011.14672" target="_blank">pdf</a>]

<h2>CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild. (arXiv:2011.14679v1 [cs.CV])</h2>
<h3>Bastian Wandt, Marco Rudolph, Petrissa Zell, Helge Rhodin, Bodo Rosenhahn</h3>
<p>Human pose estimation from single images is a challenging problem in computer
vision that requires large amounts of labeled training data to be solved
accurately. Unfortunately, for many human activities (\eg outdoor sports) such
training data does not exist and is hard or even impossible to acquire with
traditional motion capture systems. We propose a self-supervised approach that
learns a single image 3D pose estimator from unlabeled multi-view data. To this
end, we exploit multi-view consistency constraints to disentangle the observed
2D pose into the underlying 3D pose and camera rotation. In contrast to most
existing methods, we do not require calibrated cameras and can therefore learn
from moving cameras. Nevertheless, in the case of a static camera setup, we
present an optional extension to include constant relative camera rotations
over multiple views into our framework. Key to the success are new, unbiased
reconstruction objectives that mix information across views and training
samples. The proposed approach is evaluated on two benchmark datasets
(Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset.
</p>
<a href="http://arxiv.org/abs/2011.14679" target="_blank">arXiv:2011.14679</a> [<a href="http://arxiv.org/pdf/2011.14679" target="_blank">pdf</a>]

<h2>Robust Ultra-wideband Range Error Mitigation with Deep Learning at the Edge. (arXiv:2011.14684v1 [cs.LG])</h2>
<h3>Simone Angarano, Vittorio Mazzia, Francesco Salvetti, Giovanni Fantin, Marcello Chiaberge</h3>
<p>Ultra-wideband (UWB) is the state-of-the-art and most popular technology for
wireless localization. Nevertheless, precise ranging and localization in
non-line-of-sight (NLoS) conditions is still an open research topic. Indeed,
multipath effects, reflections, refractions and complexity of the indoor radio
environment can easily introduce a positive bias in the ranging measurement,
resulting in highly inaccurate and unsatisfactory position estimation. This
article proposes an efficient representation learning methodology that exploits
the latest advancement in deep learning and graph optimization techniques to
achieve effective ranging error mitigation at the edge. Channel Impulse
Response (CIR) signals are directly exploited to extract high semantic features
to estimate corrections in either NLoS or LoS conditions. Extensive
experimentation with different settings and configurations have proved the
effectiveness of our methodology and demonstrated the feasibility of a robust
and low computational power UWB range error mitigation.
</p>
<a href="http://arxiv.org/abs/2011.14684" target="_blank">arXiv:2011.14684</a> [<a href="http://arxiv.org/pdf/2011.14684" target="_blank">pdf</a>]

<h2>Can neural networks learn persistent homology features?. (arXiv:2011.14688v1 [cs.LG])</h2>
<h3>Guido Mont&#xfa;far, Nina Otter, Yuguang Wang</h3>
<p>Topological data analysis uses tools from topology -- the mathematical area
that studies shapes -- to create representations of data. In particular, in
persistent homology, one studies one-parameter families of spaces associated
with data, and persistence diagrams describe the lifetime of topological
invariants, such as connected components or holes, across the one-parameter
family. In many applications, one is interested in working with features
associated with persistence diagrams rather than the diagrams themselves. In
our work, we explore the possibility of learning several types of features
extracted from persistence diagrams using neural networks.
</p>
<a href="http://arxiv.org/abs/2011.14688" target="_blank">arXiv:2011.14688</a> [<a href="http://arxiv.org/pdf/2011.14688" target="_blank">pdf</a>]

<h2>KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization. (arXiv:2011.14691v1 [cs.LG])</h2>
<h3>Het Shah, Avishree Khare, Neelay Shah, Khizir Siddiqui</h3>
<p>In recent years, the growing size of neural networks has led to a vast amount
of research concerning compression techniques to mitigate the drawbacks of such
large sizes. Most of these research works can be categorized into three broad
families : Knowledge Distillation, Pruning, and Quantization. While there has
been steady research in this domain, adoption and commercial usage of the
proposed techniques has not quite progressed at the rate. We present KD-Lib, an
open-source PyTorch based library, which contains state-of-the-art modular
implementations of algorithms from the three families on top of multiple
abstraction layers. KD-Lib is model and algorithm-agnostic, with extended
support for hyperparameter tuning using Optuna and Tensorboard for logging and
monitoring. The library can be found at - https://github.com/SforAiDl/KD_Lib.
</p>
<a href="http://arxiv.org/abs/2011.14691" target="_blank">arXiv:2011.14691</a> [<a href="http://arxiv.org/pdf/2011.14691" target="_blank">pdf</a>]

<h2>Adaptive Compact Attention For Few-shot Video-to-video Translation. (arXiv:2011.14695v1 [cs.CV])</h2>
<h3>Risheng Huang, Li Shen, Xuan Wang, Cheng Lin, Hao-Zhi Huang</h3>
<p>This paper proposes an adaptive compact attention model for few-shot
video-to-video translation. Existing works in this domain only use features
from pixel-wise attention without considering the correlations among multiple
reference images, which leads to heavy computation but limited performance.
Therefore, we introduce a novel adaptive compact attention mechanism to
efficiently extract contextual features jointly from multiple reference images,
of which encoded view-dependent and motion-dependent information can
significantly benefit the synthesis of realistic videos. Our core idea is to
extract compact basis sets from all the reference images as higher-level
representations. To further improve the reliability, in the inference phase, we
also propose a novel method based on the Delaunay Triangulation algorithm to
automatically select the resourceful references according to the input label.
We extensively evaluate our method on a large-scale talking-head video dataset
and a human dancing dataset; the experimental results show the superior
performance of our method for producing photorealistic and temporally
consistent videos, and considerable improvements over the state-of-the-art
method.
</p>
<a href="http://arxiv.org/abs/2011.14695" target="_blank">arXiv:2011.14695</a> [<a href="http://arxiv.org/pdf/2011.14695" target="_blank">pdf</a>]

<h2>On Initial Pools for Deep Active Learning. (arXiv:2011.14696v1 [cs.LG])</h2>
<h3>Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, Vineeth N Balasubramanian</h3>
<p>Active Learning (AL) techniques aim to minimize the training data required to
train a model for a given task. Pool-based AL techniques start with a small
initial labeled pool and then iteratively pick batches of the most informative
samples for labeling. Generally, the initial pool is sampled randomly and
labeled to seed the AL iterations. While recent` studies have focused on
evaluating the robustness of various query functions in AL, little to no
attention has been given to the design of the initial labeled pool. Given the
recent successes of learning representations in self-supervised/unsupervised
ways, we propose to study if an intelligently sampled initial labeled pool can
improve deep AL performance. We will investigate the effect of intelligently
sampled initial labeled pools, including the use of self-supervised and
unsupervised strategies, on deep AL methods. We describe our experimental
details, implementation details, datasets, performance metrics as well as
planned ablation studies in this proposal. If intelligently sampled initial
pools improve AL performance, our work could make a positive contribution to
boosting AL performance with no additional annotation, developing datasets with
lesser annotation cost in general, and promoting further research in the use of
unsupervised learning methods for AL.
</p>
<a href="http://arxiv.org/abs/2011.14696" target="_blank">arXiv:2011.14696</a> [<a href="http://arxiv.org/pdf/2011.14696" target="_blank">pdf</a>]

<h2>BOTD: Bold Outline Text Detector. (arXiv:2011.14714v1 [cs.CV])</h2>
<h3>C. Yang, Z. Xiong, M. Chen, Q. Wang, X. Li</h3>
<p>Recently, text detection for arbitrary shape has attracted more and more
search attention. Although segmentation-based methods, which are not limited by
the text shape, have been studied to improve the performance, the slow
detection speed, complicated post-processing, and text adhesion problem are
still limitations for the practical application. In this paper, we propose a
simple yet effective arbitrary-shape text detector, named Bold Outline Text
Detector (BOTD). It is a novel one-stage detection framework with few
post-processing processes. At the same time, the text adhesion problem can also
be well alleviated. Specifically, BOTD first generates a center mask (CM) for
each text instance, which makes the adhesive text easy to distinguish. Base on
the CM, we further compute the polar minimum distance (PMD) for each text
instance. PMD denotes the shortest distance between the center point of CM and
the outline of the text instance. By dividing the text mask into CM and PMD,
the outline of arbitrary-shape text instance can be obtained by simply
predicting its CM and PMD. Without any bells and whistles, BOTD achieves an
F-measure of 80.1% on CTW1500 with 52 FPS. Note that the post-processing time
only accounts for 9% of the whole inference time. Code and trained models will
be publicly available soon.
</p>
<a href="http://arxiv.org/abs/2011.14714" target="_blank">arXiv:2011.14714</a> [<a href="http://arxiv.org/pdf/2011.14714" target="_blank">pdf</a>]

<h2>Probabilistic Load Forecasting Based on Adaptive Online Learning. (arXiv:2011.14721v1 [cs.LG])</h2>
<h3>Ver&#xf3;nica &#xc1;lvarez, Santiago Mazuelas, Jos&#xe9; A. Lozano</h3>
<p>Load forecasting is crucial for multiple energy management tasks such as
scheduling generation capacity, planning supply and demand, and minimizing
energy trade costs. Such relevance has increased even more in recent years due
to the integration of renewable energies, electric cars, and microgrids.
Conventional load forecasting techniques obtain single-value load forecasts by
exploiting consumption patterns of past load demand. However, such techniques
cannot assess intrinsic uncertainties in load demand, and cannot capture
dynamic changes in consumption patterns. To address these problems, this paper
presents a method for probabilistic load forecasting based on the adaptive
online learning of hidden Markov models. We propose learning and forecasting
techniques with theoretical guarantees, and experimentally assess their
performance in multiple scenarios. In particular, we develop adaptive online
learning techniques that update model parameters recursively, and sequential
prediction techniques that obtain probabilistic forecasts using the most recent
parameters. The performance of the method is evaluated using multiple datasets
corresponding with regions that have different sizes and display assorted
time-varying consumption patterns. The results show that the proposed method
can significantly improve the performance of existing techniques for a wide
range of scenarios.
</p>
<a href="http://arxiv.org/abs/2011.14721" target="_blank">arXiv:2011.14721</a> [<a href="http://arxiv.org/pdf/2011.14721" target="_blank">pdf</a>]

<h2>Dual Geometric Graph Network (DG2N) -- Zero-Shot Refinement for Dense Shape Correspondence. (arXiv:2011.14723v1 [cs.CV])</h2>
<h3>Dvir Ginzburg, Dan Raviv</h3>
<p>We provide a novel new approach for aligning geometric models using a dual
graph structure where local features are mapping probabilities. Alignment of
non-rigid structures is one of the most challenging computer vision tasks due
to the high number of unknowns needed to model the correspondence. We have seen
a leap forward using DNN models in template alignment and functional maps, but
those methods fail for inter-class alignment where nonisometric deformations
exist. Here we propose to rethink this task and use unrolling concepts on a
dual graph structure - one for a forward map and one for a backward map, where
the features are pulled back matching probabilities from the target into the
source. We report state of the art results on stretchable domains alignment in
a rapid and stable solution for meshes and cloud of points.
</p>
<a href="http://arxiv.org/abs/2011.14723" target="_blank">arXiv:2011.14723</a> [<a href="http://arxiv.org/pdf/2011.14723" target="_blank">pdf</a>]

<h2>RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction. (arXiv:2011.14744v1 [cs.CV])</h2>
<h3>Yinyu Nie, Ji Hou, Xiaoguang Han, Matthias Nie&#xdf;ner</h3>
<p>Semantic scene understanding from point clouds is particularly challenging as
the points reflect only a sparse set of the underlying 3D geometry. Previous
works often convert point cloud into regular grids (e.g. voxels or bird-eye
view images), and resort to grid-based convolutions for scene understanding. In
this work, we introduce RfD-Net that jointly detects and reconstructs dense
object surfaces directly from raw point clouds. Instead of representing scenes
with regular grids, our method leverages the sparsity of point cloud data and
focuses on predicting shapes that are recognized with high objectness. With
this design, we decouple the instance reconstruction into global object
localization and local shape prediction. It not only eases the difficulty of
learning 2-D manifold surfaces from sparse 3D space, the point clouds in each
object proposal convey shape details that support implicit function learning to
reconstruct any high-resolution surfaces. Our experiments indicate that
instance detection and reconstruction present complementary effects, where the
shape prediction head shows consistent effects on improving object detection
with modern 3D proposal network backbones. The qualitative and quantitative
evaluations further demonstrate that our approach consistently outperforms the
state-of-the-arts and improves over 11 of mesh IoU in object reconstruction.
</p>
<a href="http://arxiv.org/abs/2011.14744" target="_blank">arXiv:2011.14744</a> [<a href="http://arxiv.org/pdf/2011.14744" target="_blank">pdf</a>]

<h2>A Comprehensive Review on Recent Methods and Challenges of Video Description. (arXiv:2011.14752v1 [cs.CV])</h2>
<h3>Alok Singh, Thoudam Doren Singh, Sivaji Bandyopadhyay</h3>
<p>Video description involves the generation of the natural language description
of actions, events, and objects in the video. There are various applications of
video description by filling the gap between languages and vision for visually
impaired people, generating automatic title suggestion based on content,
browsing of the video based on the content and video-guided machine translation
[86] etc.In the past decade, several works had been done in this field in terms
of approaches/methods for video description, evaluation metrics,and datasets.
For analyzing the progress in the video description task, a comprehensive
survey is needed that covers all the phases of video description approaches
with a special focus on recent deep learning approaches. In this work, we
report a comprehensive survey on the phases of video description approaches,
the dataset for video description, evaluation metrics, open competitions for
motivating the research on the video description, open challenges in this
field, and future research directions. In this survey, we cover the
state-of-the-art approaches proposed for each and every dataset with their pros
and cons. For the growth of this research domain,the availability of numerous
benchmark dataset is a basic need. Further, we categorize all the dataset into
two classes: open domain dataset and domain-specific dataset. From our survey,
we observe that the work in this field is in fast-paced development since the
task of video description falls in the intersection of computer vision and
natural language processing. But still, the work in the video description is
far from saturation stage due to various challenges like the redundancy due to
similar frames which affect the quality of visual features, the availability of
dataset containing more diverse content and availability of an effective
evaluation metric.
</p>
<a href="http://arxiv.org/abs/2011.14752" target="_blank">arXiv:2011.14752</a> [<a href="http://arxiv.org/pdf/2011.14752" target="_blank">pdf</a>]

<h2>Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v1 [cs.CV])</h2>
<h3>Tony Lindeberg</h3>
<p>This article presents a hybrid approach between scale-space theory and deep
learning, where a deep learning architecture is constructed by coupling
parameterized scale-space operations in cascade. By sharing the learnt
parameters between multiple scale channels, and by using the transformation
properties of the scale-space primitives under scaling transformations, the
resulting network becomes provably scale covariant. By in addition performing
max pooling over the multiple scale channels, a resulting network architecture
for image classification also becomes provably scale invariant. We investigate
the performance of such networks on the MNISTLargeScale dataset, which contains
rescaled images from original MNIST over a factor 4 concerning training data
and over a factor of 16 concerning testing data. It is demonstrated that the
resulting approach allows for scale generalization, enabling good performance
for classifying patterns at scales not present in the training data.
</p>
<a href="http://arxiv.org/abs/2011.14759" target="_blank">arXiv:2011.14759</a> [<a href="http://arxiv.org/pdf/2011.14759" target="_blank">pdf</a>]

<h2>How Good MVSNets Are at Depth Fusion. (arXiv:2011.14761v1 [cs.CV])</h2>
<h3>Oleg Voynov, Aleksandr Safin, Savva Ignatyev, Evgeny Burnaev</h3>
<p>We study the effects of the additional input to deep multi-view stereo
methods in the form of low-quality sensor depth. We modify two state-of-the-art
deep multi-view stereo methods for using with the input depth. We show that the
additional input depth may improve the quality of deep multi-view stereo.
</p>
<a href="http://arxiv.org/abs/2011.14761" target="_blank">arXiv:2011.14761</a> [<a href="http://arxiv.org/pdf/2011.14761" target="_blank">pdf</a>]

<h2>Binary Classification: Counterbalancing Class Imbalance by Applying Regression Models in Combination with One-Sided Label Shifts. (arXiv:2011.14764v1 [cs.LG])</h2>
<h3>Peter Bellmann, Heinke Hihn, Daniel A. Braun, Friedhelm Schwenker</h3>
<p>In many real-world pattern recognition scenarios, such as in medical
applications, the corresponding classification tasks can be of an imbalanced
nature. In the current study, we focus on binary, imbalanced classification
tasks, i.e.~binary classification tasks in which one of the two classes is
under-represented (minority class) in comparison to the other class (majority
class). In the literature, many different approaches have been proposed, such
as under- or oversampling, to counter class imbalance. In the current work, we
introduce a novel method, which addresses the issues of class imbalance. To
this end, we first transfer the binary classification task to an equivalent
regression task. Subsequently, we generate a set of negative and positive
target labels, such that the corresponding regression task becomes balanced,
with respect to the redefined target label set. We evaluate our approach on a
number of publicly available data sets in combination with Support Vector
Machines. Moreover, we compare our proposed method to one of the most popular
oversampling techniques (SMOTE). Based on the detailed discussion of the
presented outcomes of our experimental evaluation, we provide promising ideas
for future research directions.
</p>
<a href="http://arxiv.org/abs/2011.14764" target="_blank">arXiv:2011.14764</a> [<a href="http://arxiv.org/pdf/2011.14764" target="_blank">pdf</a>]

<h2>Improved Generalization Bounds for Robust Learning. (arXiv:1810.02180v3 [cs.LG] UPDATED)</h2>
<h3>Idan Attias, Aryeh Kontorovich, Yishay Mansour</h3>
<p>We consider a model of robust learning in an adversarial environment. The
learner gets uncorrupted training data with access to possible corruptions that
may be effected by the adversary during testing. The learner's goal is to build
a robust classifier, which will be tested on future adversarial examples. The
adversary is limited to $k$ possible corruptions for each input. We model the
learner-adversary interaction as a zero-sum game. This model is closely related
to the adversarial examples model of Schmidt et al. (2018); Madry et al.
(2017). Our main results consist of generalization bounds for the binary and
multiclass classification, as well as the real-valued case (regression). For
the binary classification setting, we both tighten the generalization bound of
Feige, Mansour, and Schapire (2015), and are also able to handle infinite
hypothesis classes. The sample complexity is improved from
$\mathcal{O}(\frac{1}{\epsilon^4}\log(\frac{|H|}{\delta}))$ to
$\mathcal{O}\big(\frac{1}{\epsilon^2}(\sqrt{k VC(H)}\log^{\frac{3}{2}+\alpha}(k
VC(H))+\log(\frac{1}{\delta})\big)$ for any $\alpha &gt; 0$. Additionally, we
extend the algorithm and generalization bound from the binary to the multiclass
and real-valued cases. Along the way, we obtain results on fat-shattering
dimension and Rademacher complexity of $k$-fold maxima over function classes;
these may be of independent interest. For binary classification, the algorithm
of Feige et al. (2015) uses a regret minimization algorithm and an ERM oracle
as a black box; we adapt it for the multiclass and regression settings. The
algorithm provides us with near-optimal policies for the players on a given
training sample.
</p>
<a href="http://arxiv.org/abs/1810.02180" target="_blank">arXiv:1810.02180</a> [<a href="http://arxiv.org/pdf/1810.02180" target="_blank">pdf</a>]

<h2>Top-K Off-Policy Correction for a REINFORCE Recommender System. (arXiv:1812.02353v2 [cs.LG] UPDATED)</h2>
<h3>Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, Ed Chi</h3>
<p>Industrial recommender systems deal with extremely large action spaces --
many millions of items to recommend. Moreover, they need to serve billions of
users, who are unique at any point in time, making a complex user state space.
Luckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell
time) are available for learning. Learning from the logged feedback is however
subject to biases caused by only observing feedback on recommendations selected
by the previous versions of the recommender. In this work, we present a general
recipe of addressing such biases in a production top-K recommender system at
Youtube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The
contributions of the paper are: (1) scaling REINFORCE to a production
recommender system with an action space on the orders of millions; (2) applying
off-policy correction to address data biases in learning from logged feedback
collected from multiple behavior policies; (3) proposing a novel top-K
off-policy correction to account for our policy recommending multiple items at
a time; (4) showcasing the value of exploration. We demonstrate the efficacy of
our approaches through a series of simulations and multiple live experiments on
Youtube.
</p>
<a href="http://arxiv.org/abs/1812.02353" target="_blank">arXiv:1812.02353</a> [<a href="http://arxiv.org/pdf/1812.02353" target="_blank">pdf</a>]

<h2>Functional Object-Oriented Network for Manipulation Learning. (arXiv:1902.01537v4 [cs.RO] UPDATED)</h2>
<h3>David Paulius, Yongqiang Huang, Roger Milton, William D. Buchanan, Jeanine Sam, Yu Sun</h3>
<p>This paper presents a novel structured knowledge representation called the
functional object-oriented network (FOON) to model the connectivity of the
functional-related objects and their motions in manipulation tasks. The
graphical model FOON is learned by observing object state change and human
manipulations with the objects. Using a well-trained FOON, robots can decipher
a task goal, seek the correct objects at the desired states on which to
operate, and generate a sequence of proper manipulation motions. The paper
describes FOON's structure and an approach to form a universal FOON with
extracted knowledge from online instructional videos. A graph retrieval
approach is presented to generate manipulation motion sequences from the FOON
to achieve a desired goal, demonstrating the flexibility of FOON in creating a
novel and adaptive means of solving a problem using knowledge gathered from
multiple sources. The results are demonstrated in a simulated environment to
illustrate the motion sequences generated from the FOON to carry out the
desired tasks.
</p>
<a href="http://arxiv.org/abs/1902.01537" target="_blank">arXiv:1902.01537</a> [<a href="http://arxiv.org/pdf/1902.01537" target="_blank">pdf</a>]

<h2>Task Planning with a Weighted Functional Object-Oriented Network. (arXiv:1905.00502v3 [cs.RO] UPDATED)</h2>
<h3>David Paulius, Kelvin Sheng Pei Dong, Yu Sun</h3>
<p>In reality, there is still much to be done for robots to be able to perform
manipulation actions with full autonomy. Complicated manipulation tasks, such
as cooking, may still require a person to perform some actions that are very
risky for a robot to perform. On the other hand, some other actions may be very
risky for a human with physical disabilities to perform. Therefore, it is
necessary to balance the workload of a robot and a human based on their
limitations while minimizing the effort needed from a human in a collaborative
robot (cobot) set-up. This paper proposes a new version of our functional
object-oriented network (FOON) that integrates weights in its functional units
to reflect a robot's chance of successfully executing an action of that
functional unit. The paper also presents a task planning algorithm for the
weighted FOON to allocate manipulation action load to the robot and human to
achieve optimal performance while minimizing human effort. Through a number of
experiments, this paper shows several successful cases in which using the
proposed weighted FOON and the task planning algorithm allow a robot and a
human to successfully complete complicated tasks together with higher success
rates than a robot doing them alone.
</p>
<a href="http://arxiv.org/abs/1905.00502" target="_blank">arXiv:1905.00502</a> [<a href="http://arxiv.org/pdf/1905.00502" target="_blank">pdf</a>]

<h2>A Variational Approach for Learning from Positive and Unlabeled Data. (arXiv:1906.00642v6 [cs.LG] UPDATED)</h2>
<h3>Hui Chen, Fangqing Liu, Yin Wang, Liyue Zhao, Hao Wu</h3>
<p>Learning binary classifiers only from positive and unlabeled (PU) data is an
important and challenging task in many real-world applications, including web
text classification, disease gene identification and fraud detection, where
negative samples are difficult to verify experimentally. Most recent PU
learning methods are developed based on the conventional misclassification risk
of the supervised learning type, and they require to solve the intractable risk
estimation problem by approximating the negative data distribution or the class
prior. In this paper, we introduce a variational principle for PU learning that
allows us to quantitatively evaluate the modeling error of the Bayesian
classifier directly from given data. This leads to a loss function which can be
efficiently calculated without any intermediate step or model, and a
variational learning method can then be employed to optimize the classifier
under general conditions. In addition, the discriminative performance and
numerical stability of the variational PU learning method can be further
improved by incorporating a margin maximizing loss function. We illustrate the
effectiveness of the proposed variational method on a number of benchmark
examples.
</p>
<a href="http://arxiv.org/abs/1906.00642" target="_blank">arXiv:1906.00642</a> [<a href="http://arxiv.org/pdf/1906.00642" target="_blank">pdf</a>]

<h2>DiCENet: Dimension-wise Convolutions for Efficient Networks. (arXiv:1906.03516v3 [cs.CV] UPDATED)</h2>
<h3>Sachin Mehta, Hannaneh Hajishirzi, Mohammad Rastegari</h3>
<p>We introduce a novel and generic convolutional unit, DiCE unit, that is built
using dimension-wise convolutions and dimension-wise fusion. The dimension-wise
convolutions apply light-weight convolutional filtering across each dimension
of the input tensor while dimension-wise fusion efficiently combines these
dimension-wise representations; allowing the DiCE unit to efficiently encode
spatial and channel-wise information contained in the input tensor. The DiCE
unit is simple and can be seamlessly integrated with any architecture to
improve its efficiency and performance. Compared to depth-wise separable
convolutions, the DiCE unit shows significant improvements across different
architectures. When DiCE units are stacked to build the DiCENet model, we
observe significant improvements over state-of-the-art models across various
computer vision tasks including image classification, object detection, and
semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4%
higher accuracy than state-of-the-art manually designed models (e.g.,
MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g.,
object detection) that are often used in resource-constrained devices in
comparison to state-of-the-art separable convolution-based efficient networks,
including neural search-based methods (e.g., MobileNetv3 and MixNet. Our source
code in PyTorch is open-source and is available at
https://github.com/sacmehta/EdgeNets/
</p>
<a href="http://arxiv.org/abs/1906.03516" target="_blank">arXiv:1906.03516</a> [<a href="http://arxiv.org/pdf/1906.03516" target="_blank">pdf</a>]

<h2>Learning Deep Generative Models with Annealed Importance Sampling. (arXiv:1906.04904v3 [stat.ML] UPDATED)</h2>
<h3>Xinqiang Ding, David J. Freedman</h3>
<p>Variational inference (VI) and Markov chain Monte Carlo (MCMC) are two main
approximate approaches for learning deep generative models by maximizing
marginal likelihood. In this paper, we propose using annealed importance
sampling for learning deep generative models. Our proposed approach bridges VI
with MCMC. It generalizes VI methods such as variational auto-encoders and
importance weighted auto-encoders (IWAE) and the MCMC method proposed in
(Hoffman, 2017). It also provides insights into why running multiple short MCMC
chains can help learning deep generative models. Through experiments, we show
that our approach yields better density models than IWAE and can effectively
trade computation for model accuracy without increasing memory cost.
</p>
<a href="http://arxiv.org/abs/1906.04904" target="_blank">arXiv:1906.04904</a> [<a href="http://arxiv.org/pdf/1906.04904" target="_blank">pdf</a>]

<h2>Tensor Analysis with n-Mode Generalized Difference Subspace. (arXiv:1909.01954v2 [cs.LG] UPDATED)</h2>
<h3>Bernardo B. Gatto, Eulanda M. dos Santos, Alessandro L. Koerich, Kazuhiro Fukui, Waldir S. S. Junior</h3>
<p>The increasing use of multiple sensors, which produce a large amount of
multi-dimensional data, requires efficient representation and classification
methods. In this paper, we present a new method for multi-dimensional data
classification that relies on two premises: 1) multi-dimensional data are
usually represented by tensors, since this brings benefits from multilinear
algebra and established tensor factorization methods; and 2) multilinear data
can be described by a subspace of a vector space. The subspace representation
has been employed for pattern-set recognition, and its tensor representation
counterpart is also available in the literature. However, traditional methods
do not use discriminative information of the tensors, degrading the
classification accuracy. In this case, generalized difference subspace (GDS)
provides an enhanced subspace representation by reducing data redundancy and
revealing discriminative structures. Since GDS does not handle tensor data, we
propose a new projection called n-mode GDS, which efficiently handles tensor
data. We also introduce the n-mode Fisher score as a class separability index
and an improved metric based on the geodesic distance for tensor data
similarity. The experimental results on gesture and action recognition show
that the proposed method outperforms methods commonly used in the literature
without relying on pre-trained models or transfer learning.
</p>
<a href="http://arxiv.org/abs/1909.01954" target="_blank">arXiv:1909.01954</a> [<a href="http://arxiv.org/pdf/1909.01954" target="_blank">pdf</a>]

<h2>Interpreting and Improving Adversarial Robustness of Deep Neural Networks with Neuron Sensitivity. (arXiv:1909.06978v3 [cs.CV] UPDATED)</h2>
<h3>Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li</h3>
<p>Deep neural networks (DNNs) are vulnerable to adversarial examples where
inputs with imperceptible perturbations mislead DNNs to incorrect results.
Despite the potential risk they bring, adversarial examples are also valuable
for providing insights into the weakness and blind-spots of DNNs. Thus, the
interpretability of a DNN in the adversarial setting aims to explain the
rationale behind its decision-making process and makes deeper understanding
which results in better practical applications. To address this issue, we try
to explain adversarial robustness for deep models from a new perspective of
neuron sensitivity which is measured by neuron behavior variation intensity
against benign and adversarial examples. In this paper, we first draw the close
connection between adversarial robustness and neuron sensitivities, as
sensitive neurons make the most non-trivial contributions to model predictions
in the adversarial setting. Based on that, we further propose to improve
adversarial robustness by constraining the similarities of sensitive neurons
between benign and adversarial examples which stabilizes the behaviors of
sensitive neurons towards adversarial noises. Moreover, we demonstrate that
state-of-the-art adversarial training methods improve model robustness by
reducing neuron sensitivities which in turn confirms the strong connections
between adversarial robustness and neuron sensitivity as well as the
effectiveness of using sensitive neurons to build robust models. Extensive
experiments on various datasets demonstrate that our algorithm effectively
achieves excellent results.
</p>
<a href="http://arxiv.org/abs/1909.06978" target="_blank">arXiv:1909.06978</a> [<a href="http://arxiv.org/pdf/1909.06978" target="_blank">pdf</a>]

<h2>The Universal Approximation Property. (arXiv:1910.03344v4 [stat.ML] UPDATED)</h2>
<h3>Anastasis Kratsios</h3>
<p>The universal approximation property of various machine learning models is
currently only understood on a case-by-case basis, limiting the rapid
development of new theoretically justified neural network architectures and
blurring our understanding of our current models' potential. This paper works
towards overcoming these challenges by presenting a characterization, a
representation, a construction method, and an existence result, each of which
applies to any universal approximator on most function spaces of practical
interest. Our characterization result is used to describe which activation
functions allow the feed-forward architecture to maintain its universal
approximation capabilities when multiple constraints are imposed on its final
layers and its remaining layers are only sparsely connected. These include a
rescaled and shifted Leaky ReLU activation function but not the ReLU activation
function. Our construction and representation result is used to exhibit a
simple modification of the feed-forward architecture, which can approximate any
continuous function with non-pathological growth, uniformly on the entire
Euclidean input space. This improves the known capabilities of the feed-forward
architecture.
</p>
<a href="http://arxiv.org/abs/1910.03344" target="_blank">arXiv:1910.03344</a> [<a href="http://arxiv.org/pdf/1910.03344" target="_blank">pdf</a>]

<h2>Time2Graph: Revisiting Time Series Modeling with Dynamic Shapelets. (arXiv:1911.04143v2 [cs.LG] UPDATED)</h2>
<h3>Ziqiang Cheng, Yang Yang, Wei Wang, Wenjie Hu, Yueting Zhuang, Guojie Song</h3>
<p>Time series modeling has attracted extensive research efforts; however,
achieving both reliable efficiency and interpretability from a unified model
still remains a challenging problem. Among the literature, shapelets offer
interpretable and explanatory insights in the classification tasks, while most
existing works ignore the differing representative power at different time
slices, as well as (more importantly) the evolution pattern of shapelets. In
this paper, we propose to extract time-aware shapelets by designing a two-level
timing factor. Moreover, we define and construct the shapelet evolution graph,
which captures how shapelets evolve over time and can be incorporated into the
time series embeddings by graph embedding algorithms. To validate whether the
representations obtained in this way can be applied effectively in various
scenarios, we conduct experiments based on three public time series datasets,
and two real-world datasets from different domains. Experimental results
clearly show the improvements achieved by our approach compared with 17
state-of-the-art baselines.
</p>
<a href="http://arxiv.org/abs/1911.04143" target="_blank">arXiv:1911.04143</a> [<a href="http://arxiv.org/pdf/1911.04143" target="_blank">pdf</a>]

<h2>Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem. (arXiv:1911.09539v2 [cs.AI] UPDATED)</h2>
<h3>Andr&#xe9; Hottung, Kevin Tierney</h3>
<p>Learning how to automatically solve optimization problems has the potential
to provide the next big leap in optimization technology. The performance of
automatically learned heuristics on routing problems has been steadily
improving in recent years, but approaches based purely on machine learning are
still outperformed by state-of-the-art optimization methods. To close this
performance gap, we propose a novel large neighborhood search (LNS) framework
for vehicle routing that integrates learned heuristics for generating new
solutions. The learning mechanism is based on a deep neural network with an
attention mechanism and has been especially designed to be integrated into an
LNS search setting. We evaluate our approach on the capacitated vehicle routing
problem (CVRP) and the split delivery vehicle routing problem (SDVRP). On CVRP
instances with up to 297 customers, our approach significantly outperforms an
LNS that uses only handcrafted heuristics and a well-known heuristic from the
literature. Furthermore, we show for the CVRP and the SDVRP that our approach
surpasses the performance of existing machine learning approaches and comes
close to the performance of state-of-the-art optimization approaches.
</p>
<a href="http://arxiv.org/abs/1911.09539" target="_blank">arXiv:1911.09539</a> [<a href="http://arxiv.org/pdf/1911.09539" target="_blank">pdf</a>]

<h2>Gating Revisited: Deep Multi-layer RNNs That Can Be Trained. (arXiv:1911.11033v3 [cs.CV] UPDATED)</h2>
<h3>Mehmet Ozgur Turkoglu, Stefano D&#x27;Aronco, Jan Dirk Wegner, Konrad Schindler</h3>
<p>We propose a new STAckable Recurrent cell (STAR) for recurrent neural
networks (RNNs), which has fewer parameters than widely used LSTM and GRU while
being more robust against vanishing or exploding gradients. Stacking recurrent
units into deep architectures suffers from two major limitations: (i) many
recurrent cells (e.g., LSTMs) are costly in terms of parameters and computation
resources; and (ii) deep RNNs are prone to vanishing or exploding gradients
during training. We investigate the training of multi-layer RNNs and examine
the magnitude of the gradients as they propagate through the network in the
"vertical" direction. We show that, depending on the structure of the basic
recurrent unit, the gradients are systematically attenuated or amplified. Based
on our analysis we design a new type of gated cell that better preserves
gradient magnitude. We validate our design on a large number of sequence
modelling tasks and demonstrate that the proposed STAR cell allows to build and
train deeper recurrent architectures, ultimately leading to improved
performance while being computationally more efficient.
</p>
<a href="http://arxiv.org/abs/1911.11033" target="_blank">arXiv:1911.11033</a> [<a href="http://arxiv.org/pdf/1911.11033" target="_blank">pdf</a>]

<h2>Learning to segment images with classification labels. (arXiv:1912.12533v2 [cs.CV] UPDATED)</h2>
<h3>Ozan Ciga, Anne L. Martel</h3>
<p>Two of the most common tasks in medical imaging are classification and
segmentation. Either task requires labeled data annotated by experts, which is
scarce and expensive to collect. Annotating data for segmentation is generally
considered to be more laborious as the annotator has to draw around the
boundaries of regions of interest, as opposed to assigning image patches a
class label. Furthermore, in tasks such as breast cancer histopathology, any
realistic clinical application often includes working with whole slide images,
whereas most publicly available training data are in the form of image patches,
which are given a class label. We propose an architecture that can alleviate
the requirements for segmentation-level ground truth by making use of
image-level labels to reduce the amount of time spent on data curation. In
addition, this architecture can help unlock the potential of previously
acquired image-level datasets on segmentation tasks by annotating a small
number of regions of interest. In our experiments, we show using only one
segmentation-level annotation per class, we can achieve performance comparable
to a fully annotated dataset.
</p>
<a href="http://arxiv.org/abs/1912.12533" target="_blank">arXiv:1912.12533</a> [<a href="http://arxiv.org/pdf/1912.12533" target="_blank">pdf</a>]

<h2>FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for Semi-supervised Optical Flow Estimation. (arXiv:2001.06171v3 [cs.CV] UPDATED)</h2>
<h3>Xiaolin Song, Yuyang Zhao, Jingyu Yang, Cuiling Lan, Wenjun Zeng</h3>
<p>Optical flow estimation is an important yet challenging problem in the field
of video analytics. The features of different semantics levels/layers of a
convolutional neural network can provide information of different granularity.
To exploit such flexible and comprehensive information, we propose a
semi-supervised Feature Pyramidal Correlation and Residual Reconstruction
Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of
two main modules: pyramid correlation mapping and residual reconstruction. The
pyramid correlation mapping module takes advantage of the multi-scale
correlations of global/local patches by aggregating features of different
scales to form a multi-level cost volume. The residual reconstruction module
aims to reconstruct the sub-band high-frequency residuals of finer optical flow
in each stage. Based on the pyramid correlation mapping, we further propose a
correlation-warping-normalization (CWN) module to efficiently exploit the
correlation dependency. Experiment results show that the proposed scheme
achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and
0.10 in terms of average end-point error (AEE) against competing baseline
methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel
dataset, respectively.
</p>
<a href="http://arxiv.org/abs/2001.06171" target="_blank">arXiv:2001.06171</a> [<a href="http://arxiv.org/pdf/2001.06171" target="_blank">pdf</a>]

<h2>Learning the Hypotheses Space from data: Learning Space and U-curve Property. (arXiv:2001.09532v2 [stat.ML] UPDATED)</h2>
<h3>Diego Marcondes, Adilson Simonis, Junior Barrera</h3>
<p>This paper presents an extension of the classical agnostic PAC learning model
in which learning problems are modelled not only by a Hypothesis Space
$\mathcal{H}$, but also by a Learning Space $\mathbb{L}(\mathcal{H})$, which is
a cover of $\mathcal{H}$, constrained by a VC-dimension property, that is a
suitable domain for Model Selection algorithms. Our main contribution is a data
driven general learning algorithm to perform regularized Model Selection on
$\mathbb{L}(\mathcal{H})$. A remarkable, formally proved, consequence of this
approach are conditions on $\mathbb{L}(\mathcal{H})$ and on the loss function
that lead to estimated out-of-sample error surfaces which are true U-curves on
$\mathbb{L}(\mathcal{H})$ chains, enabling a more efficient search on
$\mathbb{L}(\mathcal{H})$. To our knowledge, this is the first rigorous result
asserting that a non exhaustive search of a family of candidate models can
return an optimal solution. In this new framework, an U-curve optimization
algorithm becomes a natural component of Model Selection, hence of learning
algorithms. The abstract general framework proposed here may have important
implications on modern learning models and on areas such as Neural Architecture
Search.
</p>
<a href="http://arxiv.org/abs/2001.09532" target="_blank">arXiv:2001.09532</a> [<a href="http://arxiv.org/pdf/2001.09532" target="_blank">pdf</a>]

<h2>Coherent and Archimedean choice in general Banach spaces. (arXiv:2002.05461v3 [cs.AI] UPDATED)</h2>
<h3>Gert de Cooman</h3>
<p>I introduce and study a new notion of Archimedeanity for binary and
non-binary choice between options that live in an abstract Banach space,
through a very general class of choice models, called sets of desirable option
sets. In order to also be able to bring horse lottery options into the fold, I
pay special attention to the case where these linear spaces do not include all
`constant' options. I consider the frameworks of conservative inference
associated with Archimedean (and coherent) choice models, and also pay quite a
lot of attention to representation of general (non-binary) choice models in
terms of the simpler, binary ones. The representation theorems proved here
provide an axiomatic characterisation for, amongst many other choice methods,
Levi's E-admissibility and Walley--Sen maximality.
</p>
<a href="http://arxiv.org/abs/2002.05461" target="_blank">arXiv:2002.05461</a> [<a href="http://arxiv.org/pdf/2002.05461" target="_blank">pdf</a>]

<h2>On Learning Sets of Symmetric Elements. (arXiv:2002.08599v4 [cs.LG] UPDATED)</h2>
<h3>Haggai Maron, Or Litany, Gal Chechik, Ethan Fetaya</h3>
<p>Learning from unordered sets is a fundamental learning setup, recently
attracting increasing attention. Research in this area has focused on the case
where elements of the set are represented by feature vectors, and far less
emphasis has been given to the common case where set elements themselves adhere
to their own symmetries. That case is relevant to numerous applications, from
deblurring image bursts to multi-view 3D shape recognition and reconstruction.
In this paper, we present a principled approach to learning sets of general
symmetric elements. We first characterize the space of linear layers that are
equivariant both to element reordering and to the inherent symmetries of
elements, like translation in the case of images. We further show that networks
that are composed of these layers, called Deep Sets for Symmetric Elements
(DSS) layers, are universal approximators of both invariant and equivariant
functions, and that these networks are strictly more expressive than Siamese
networks. DSS layers are also straightforward to implement. Finally, we show
that they improve over existing set-learning architectures in a series of
experiments with images, graphs, and point-clouds.
</p>
<a href="http://arxiv.org/abs/2002.08599" target="_blank">arXiv:2002.08599</a> [<a href="http://arxiv.org/pdf/2002.08599" target="_blank">pdf</a>]

<h2>Learning Certified Individually Fair Representations. (arXiv:2002.10312v2 [cs.LG] UPDATED)</h2>
<h3>Anian Ruoss, Mislav Balunovi&#x107;, Marc Fischer, Martin Vechev</h3>
<p>Fair representation learning provides an effective way of enforcing fairness
constraints without compromising utility for downstream users. A desirable
family of such fairness constraints, each requiring similar treatment for
similar individuals, is known as individual fairness. In this work, we
introduce the first method that enables data consumers to obtain certificates
of individual fairness for existing and new data points. The key idea is to map
similar individuals to close latent representations and leverage this latent
proximity to certify individual fairness. That is, our method enables the data
producer to learn and certify a representation where for a data point all
similar individuals are at $\ell_\infty$-distance at most $\epsilon$, thus
allowing data consumers to certify individual fairness by proving
$\epsilon$-robustness of their classifier. Our experimental evaluation on five
real-world datasets and several fairness constraints demonstrates the
expressivity and scalability of our approach.
</p>
<a href="http://arxiv.org/abs/2002.10312" target="_blank">arXiv:2002.10312</a> [<a href="http://arxiv.org/pdf/2002.10312" target="_blank">pdf</a>]

<h2>Interactive Robot Training for Non-Markov Tasks. (arXiv:2003.02232v2 [cs.RO] UPDATED)</h2>
<h3>Ankit Shah, Samir Wadhwania, Julie Shah</h3>
<p>Defining sound and complete specifications for robots using formal languages
is challenging, while learning formal specifications directly from
demonstrations can lead to over-constrained task policies. In this paper, we
propose a Bayesian interactive robot training framework that allows the robot
to learn from both demonstrations provided by a teacher, and that teacher's
assessments of the robot's task executions. We also present an active learning
approach -- inspired by uncertainty sampling -- to identify the task execution
with the most uncertain degree of acceptability. Through a simulated
experiment, we demonstrate that our active learning approach identifies a
teacher's intended task specification with an equivalent or greater similarity
when compared to an approach that learns purely from demonstrations. Finally,
we demonstrate the efficacy of our approach in a real-world setting through a
user-study based on teaching a robot to set a dinner table.
</p>
<a href="http://arxiv.org/abs/2003.02232" target="_blank">arXiv:2003.02232</a> [<a href="http://arxiv.org/pdf/2003.02232" target="_blank">pdf</a>]

<h2>Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation. (arXiv:2003.13045v2 [cs.CV] UPDATED)</h2>
<h3>Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, Feiyue Huang</h3>
<p>Unsupervised learning of optical flow, which leverages the supervision from
view synthesis, has emerged as a promising alternative to supervised methods.
However, the objective of unsupervised learning is likely to be unreliable in
challenging scenes. In this work, we present a framework to use more reliable
supervision from transformations. It simply twists the general unsupervised
learning pipeline by running another forward pass with transformed data from
augmentation, along with using transformed predictions of original data as the
self-supervision signal. Besides, we further introduce a lightweight network
with multiple frames by a highly-shared flow decoder. Our method consistently
gets a leap of performance on several benchmarks with the best accuracy among
deep unsupervised methods. Also, our method achieves competitive results to
recent fully supervised methods while with much fewer parameters.
</p>
<a href="http://arxiv.org/abs/2003.13045" target="_blank">arXiv:2003.13045</a> [<a href="http://arxiv.org/pdf/2003.13045" target="_blank">pdf</a>]

<h2>SoCal: Selective Oracle Questioning for Consistency-based Active Learning of Cardiac Signals. (arXiv:2004.09557v2 [cs.LG] UPDATED)</h2>
<h3>Dani Kiyasseh, Tingting Zhu, David A. Clifton</h3>
<p>The ubiquity and rate of collection of cardiac signals produce large,
unlabelled datasets. Active learning (AL) can exploit such datasets by
incorporating human annotators (oracles) to improve generalization performance.
However, the over-reliance of existing algorithms on oracles continues to
burden physicians. To minimize this burden, we propose SoCal, a
consistency-based AL framework that dynamically determines whether to request a
label from an oracle or to generate a pseudo-label instead. We show that our
framework decreases the labelling burden while maintaining strong performance,
even in the presence of a noisy oracle.
</p>
<a href="http://arxiv.org/abs/2004.09557" target="_blank">arXiv:2004.09557</a> [<a href="http://arxiv.org/pdf/2004.09557" target="_blank">pdf</a>]

<h2>CLOPS: Continual Learning of Physiological Signals. (arXiv:2004.09578v2 [cs.LG] UPDATED)</h2>
<h3>Dani Kiyasseh, Tingting Zhu, David A. Clifton</h3>
<p>Deep learning algorithms are known to experience destructive interference
when instances violate the assumption of being independent and identically
distributed (i.i.d). This violation, however, is ubiquitous in clinical
settings where data are streamed temporally and from a multitude of
physiological sensors. To overcome this obstacle, we propose CLOPS, a
replay-based continual learning strategy. In three continual learning scenarios
based on three publically-available datasets, we show that CLOPS can outperform
the state-of-the-art methods, GEM and MIR. Moreover, we propose end-to-end
trainable parameters, which we term task-instance parameters, that can be used
to quantify task difficulty and similarity. This quantification yields insights
into both network interpretability and clinical applications, where task
difficulty is poorly quantified.
</p>
<a href="http://arxiv.org/abs/2004.09578" target="_blank">arXiv:2004.09578</a> [<a href="http://arxiv.org/pdf/2004.09578" target="_blank">pdf</a>]

<h2>Boosting Deep Open World Recognition by Clustering. (arXiv:2004.13849v2 [cs.CV] UPDATED)</h2>
<h3>Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bul&#xf2;, Elisa Ricci, Barbara Caputo</h3>
<p>While convolutional neural networks have brought significant advances in
robot vision, their ability is often limited to closed world scenarios, where
the number of semantic concepts to be recognized is determined by the available
training set. Since it is practically impossible to capture all possible
semantic concepts present in the real world in a single training set, we need
to break the closed world assumption, equipping our robot with the capability
to act in an open world. To provide such ability, a robot vision system should
be able to (i) identify whether an instance does not belong to the set of known
categories (i.e. open set recognition), and (ii) extend its knowledge to learn
new classes over time (i.e. incremental learning). In this work, we show how we
can boost the performance of deep open world recognition algorithms by means of
a new loss formulation enforcing a global to local clustering of class-specific
features. In particular, a first loss term, i.e. global clustering, forces the
network to map samples closer to the class centroid they belong to while the
second one, local clustering, shapes the representation space in such a way
that samples of the same class get closer in the representation space while
pushing away neighbours belonging to other classes. Moreover, we propose a
strategy to learn class-specific rejection thresholds, instead of heuristically
estimating a single global threshold, as in previous works. Experiments on
RGB-D Object and Core50 datasets show the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2004.13849" target="_blank">arXiv:2004.13849</a> [<a href="http://arxiv.org/pdf/2004.13849" target="_blank">pdf</a>]

<h2>Zero-Shot Learning and its Applications from Autonomous Vehicles to COVID-19 Diagnosis: A Review. (arXiv:2004.14143v3 [cs.CV] UPDATED)</h2>
<h3>Mahdi Rezaei, Mahsa Shahidi</h3>
<p>The challenge of learning a new concept, object, or a new medical disease
recognition without receiving any examples beforehand is called Zero-Shot
Learning (ZSL). One of the major issues in deep learning based methodologies
such as in Medical Imaging and other real-world applications is the requirement
of large annotated datasets prepared by clinicians or experts to train the
model. ZSL is known for having minimal human intervention by relying only on
previously known or trained concepts plus currently existing auxiliary
information. This makes the ZSL applicable in many real-world scenarios, from
unknown object detection in autonomous vehicles to medical imaging and
unforeseen diseases such as COVID-19 Chest X-Ray (CXR) based diagnosis. We
introduce a novel and broaden solution called Few/one-shot learning, and
present the definition of the ZSL problem as an extreme case of the few-shot
learning. We review over fundamentals and the challenging steps of Zero-Shot
Learning, including state-of-the-art categories of solutions, as well as our
recommended solution, motivations behind each approach, their advantages over
each category to guide both clinicians and AI researchers to proceed with the
best techniques and practices based on their applications. We then review
through different datasets inducing medical and non-medical images, the variety
of splits, and the evaluation protocols proposed so far. Finally, we discuss
the recent applications and future directions of ZSL. We aim to convey a useful
intuition through this paper towards the goal of handling complex learning
tasks more similar to the way humans learn. We mainly focus on two applications
in the current modern yet challenging era: coping with an early and fast
diagnosis of COVID-19 cases, and also encouraging the readers to develop other
similar AI-based automated detection/recognition systems using ZSL.
</p>
<a href="http://arxiv.org/abs/2004.14143" target="_blank">arXiv:2004.14143</a> [<a href="http://arxiv.org/pdf/2004.14143" target="_blank">pdf</a>]

<h2>Generalization Bounds via Information Density and Conditional Information Density. (arXiv:2005.08044v3 [cs.LG] UPDATED)</h2>
<h3>Fredrik Hellstr&#xf6;m, Giuseppe Durisi</h3>
<p>We present a general approach, based on an exponential inequality, to derive
bounds on the generalization error of randomized learning algorithms. Using
this approach, we provide bounds on the average generalization error as well as
bounds on its tail probability, for both the PAC-Bayesian and single-draw
scenarios. Specifically, for the case of subgaussian loss functions, we obtain
novel bounds that depend on the information density between the training data
and the output hypothesis. When suitably weakened, these bounds recover many of
the information-theoretic available bounds in the literature. We also extend
the proposed exponential-inequality approach to the setting recently introduced
by Steinke and Zakynthinou (2020), where the learning algorithm depends on a
randomly selected subset of the available training data. For this setup, we
present bounds for bounded loss functions in terms of the conditional
information density between the output hypothesis and the random variable
determining the subset choice, given all training data. Through our approach,
we recover the average generalization bound presented by Steinke and
Zakynthinou (2020) and extend it to the PAC-Bayesian and single-draw scenarios.
For the single-draw scenario, we also obtain novel bounds in terms of the
conditional $\alpha$-mutual information and the conditional maximal leakage.
</p>
<a href="http://arxiv.org/abs/2005.08044" target="_blank">arXiv:2005.08044</a> [<a href="http://arxiv.org/pdf/2005.08044" target="_blank">pdf</a>]

<h2>Clinical Predictive Models for COVID-19: Systematic Study. (arXiv:2005.08302v2 [cs.LG] UPDATED)</h2>
<h3>Patrick Schwab, August DuMont Sch&#xfc;tte, Benedikt Dietz, Stefan Bauer</h3>
<p>Coronavirus Disease 2019 (COVID-19) is a rapidly emerging respiratory disease
caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Due
to the rapid human-to-human transmission of SARS-CoV-2, many healthcare systems
are at risk of exceeding their healthcare capacities, in particular in terms of
SARS-CoV-2 tests, hospital and intensive care unit (ICU) beds and mechanical
ventilators. Predictive algorithms could potentially ease the strain on
healthcare systems by identifying those who are most likely to receive a
positive SARS-CoV-2 test, be hospitalised or admitted to the ICU. Here, we
study clinical predictive models that estimate, using machine learning and
based on routinely collected clinical data, which patients are likely to
receive a positive SARS-CoV-2 test, require hospitalisation or intensive care.
To evaluate the predictive performance of our models, we perform a
retrospective evaluation on clinical and blood analysis data from a cohort of
5644 patients. Our experimental results indicate that our predictive models
identify (i) patients that test positive for SARS-CoV-2 a priori at a
sensitivity of 75% (95% CI: 67%, 81%) and a specificity of 49% (95% CI: 46%,
51%), (ii) SARS-CoV-2 positive patients that require hospitalisation with 0.92
AUC (95% CI: 0.81, 0.98), and (iii) SARS-CoV-2 positive patients that require
critical care with 0.98 AUC (95% CI: 0.95, 1.00). In addition, we determine
which clinical features are predictive to what degree for each of the
aforementioned clinical tasks. Our results indicate that predictive models
trained on routinely collected clinical data could be used to predict clinical
pathways for COVID-19, and therefore help inform care and prioritise resources.
</p>
<a href="http://arxiv.org/abs/2005.08302" target="_blank">arXiv:2005.08302</a> [<a href="http://arxiv.org/pdf/2005.08302" target="_blank">pdf</a>]

<h2>CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks. (arXiv:2005.09544v2 [cs.CV] UPDATED)</h2>
<h3>Maxim Maximov, Ismail Elezi, Laura Leal-Taix&#xe9;</h3>
<p>The unprecedented increase in the usage of computer vision technology in
society goes hand in hand with an increased concern in data privacy. In many
real-world scenarios like people tracking or action recognition, it is
important to be able to process the data while taking careful consideration in
protecting people's identity. We propose and develop CIAGAN, a model for image
and video anonymization based on conditional generative adversarial networks.
Our model is able to remove the identifying characteristics of faces and bodies
while producing high-quality images and videos that can be used for any
computer vision task, such as detection or tracking. Unlike previous methods,
we have full control over the de-identification (anonymization) procedure,
ensuring both anonymization as well as diversity. We compare our method to
several baselines and achieve state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2005.09544" target="_blank">arXiv:2005.09544</a> [<a href="http://arxiv.org/pdf/2005.09544" target="_blank">pdf</a>]

<h2>CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients. (arXiv:2005.13249v2 [cs.LG] UPDATED)</h2>
<h3>Dani Kiyasseh, Tingting Zhu, David A. Clifton</h3>
<p>The healthcare industry generates troves of unlabelled physiological data.
This data can be exploited via contrastive learning, a self-supervised
pre-training method that encourages representations of instances to be similar
to one another. We propose a family of contrastive learning methods, CLOCS,
that encourages representations across space, time, \textit{and} patients to be
similar to one another. We show that CLOCS consistently outperforms the
state-of-the-art methods, BYOL and SimCLR, when performing a linear evaluation
of, and fine-tuning on, downstream tasks. We also show that CLOCS achieves
strong generalization performance with only 25\% of labelled training data.
Furthermore, our training procedure naturally generates patient-specific
representations that can be used to quantify patient-similarity.
</p>
<a href="http://arxiv.org/abs/2005.13249" target="_blank">arXiv:2005.13249</a> [<a href="http://arxiv.org/pdf/2005.13249" target="_blank">pdf</a>]

<h2>DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution. (arXiv:2006.02334v2 [cs.CV] UPDATED)</h2>
<h3>Siyuan Qiao, Liang-Chieh Chen, Alan Yuille</h3>
<p>Many modern object detectors demonstrate outstanding performances by using
the mechanism of looking and thinking twice. In this paper, we explore this
mechanism in the backbone design for object detection. At the macro level, we
propose Recursive Feature Pyramid, which incorporates extra feedback
connections from Feature Pyramid Networks into the bottom-up backbone layers.
At the micro level, we propose Switchable Atrous Convolution, which convolves
the features with different atrous rates and gathers the results using switch
functions. Combining them results in DetectoRS, which significantly improves
the performances of object detection. On COCO test-dev, DetectoRS achieves
state-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance
segmentation, and 50.0% PQ for panoptic segmentation. The code is made publicly
available.
</p>
<a href="http://arxiv.org/abs/2006.02334" target="_blank">arXiv:2006.02334</a> [<a href="http://arxiv.org/pdf/2006.02334" target="_blank">pdf</a>]

<h2>Are We Hungry for 3D LiDAR Data for Semantic Segmentation? A Survey and Experimental Study. (arXiv:2006.04307v2 [cs.CV] UPDATED)</h2>
<h3>Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, Huijing Zhao</h3>
<p>3D semantic segmentation is a fundamental task for robotic and autonomous
driving applications. Recent works have been focused on using deep learning
techniques, whereas developing fine-annotated 3D LiDAR datasets is extremely
labor intensive and requires professional skills. The performance limitation
caused by insufficient datasets is called data hunger problem. This research
provides a comprehensive survey and experimental study on the question: are we
hungry for 3D LiDAR data for semantic segmentation? The studies are conducted
at three levels. First, a broad review to the main 3D LiDAR datasets is
conducted, followed by a statistical analysis on three representative datasets
to gain an in-depth view on the datasets' size and diversity, which are the
critical factors in learning deep models. Second, a systematic review to the
state-of-the-art 3D semantic segmentation is conducted, followed by experiments
and cross examinations of three representative deep learning methods to find
out how the size and diversity of the datasets affect deep models' performance.
Finally, a systematic survey to the existing efforts to solve the data hunger
problem is conducted on both methodological and dataset's viewpoints, followed
by an insightful discussion of remaining problems and open questions To the
best of our knowledge, this is the first work to analyze the data hunger
problem for 3D semantic segmentation using deep learning techniques that are
addressed in the literature review, statistical analysis, and cross-dataset and
cross-algorithm experiments. We share findings and discussions, which may lead
to potential topics in future works.
</p>
<a href="http://arxiv.org/abs/2006.04307" target="_blank">arXiv:2006.04307</a> [<a href="http://arxiv.org/pdf/2006.04307" target="_blank">pdf</a>]

<h2>Banach Space Representer Theorems for Neural Networks and Ridge Splines. (arXiv:2006.05626v2 [stat.ML] UPDATED)</h2>
<h3>Rahul Parhi, Robert D. Nowak</h3>
<p>We develop a variational framework to understand the properties of the
functions learned by neural networks fit to data. We propose and study a family
of continuous-domain linear inverse problems with total variation-like
regularization in the Radon domain subject to data fitting constraints. We
derive a representer theorem showing that finite-width, single-hidden layer
neural networks are solutions to these inverse problems. We draw on many
techniques from variational spline theory and so we propose the notion of
polynomial ridge splines, which correspond to a single-hidden layer neural
networks with truncated power functions as the activation function. The
representer theorem is reminiscent of the classical reproducing kernel Hilbert
space representer theorem, but we show that the neural network problem is posed
over a non-Hilbertian Banach space. While the learning problems are posed in
the continuous-domain, similar to kernel methods, the problems can be recast as
finite-dimensional neural network training problems. These neural network
training problems have regularizers which are related to the well-known weight
decay and path-norm regularizers. Thus, our result gives insight into
functional characteristics of trained neural networks and also into the design
neural network regularizers. We also show that these regularizers promote
neural network solutions with desirable generalization properties.
</p>
<a href="http://arxiv.org/abs/2006.05626" target="_blank">arXiv:2006.05626</a> [<a href="http://arxiv.org/pdf/2006.05626" target="_blank">pdf</a>]

<h2>Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v2 [cs.CV] UPDATED)</h2>
<h3>Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, Fisher Yu</h3>
<p>Similarity learning has been recognized as an crucial step for object
tracking. However, existing multiple object tracking methods only use sparse
ground truth matching as the training objective, while ignoring the majority of
the informative regions on the images. In this paper, we present Quasi-Dense
Similarity Learning, which densely samples hundreds of region proposals on a
pair of images for contrastive learning. We can naturally combine this
similarity learning with existing detection methods to build Quasi-Dense
Tracking (QDTrack) without turning to displacement regression or motion priors.
We also find that the resulting distinctive feature space admits a simple
nearest neighbor search at the inference time. Despite its simplicity, QDTrack
outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking
benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external
training data. Compared to methods with similar detectors, it boosts almost 10
points of MOTA and significantly decreases the number of ID switches on BDD100K
and Waymo datasets. The code is available at
\url{https://github.com/SysCV/qdtrack}.
</p>
<a href="http://arxiv.org/abs/2006.06664" target="_blank">arXiv:2006.06664</a> [<a href="http://arxiv.org/pdf/2006.06664" target="_blank">pdf</a>]

<h2>Adversarial representation learning for synthetic replacement of private attributes. (arXiv:2006.08039v4 [cs.LG] UPDATED)</h2>
<h3>John Martinsson, Edvin Listo Zec, Daniel Gillblad, Olof Mogren</h3>
<p>Data privacy is an increasingly important aspect of many real-world big data
analytics tasks. Data sources that contain sensitive information may have
immense potential which could be unlocked using privacy enhancing
transformations, but current methods often fail to produce convincing output.
Furthermore, finding the right balance between privacy and utility is often a
tricky trade-off. In this work, we propose a novel approach for data
privatization, which involves two steps: in the first step, it removes the
sensitive information, and in the second step, it replaces this information
with an independent random sample. Our method builds on adversarial
representation learning which ensures strong privacy by training the model to
fool an increasingly strong adversary. While previous methods only aim at
obfuscating the sensitive information, we find that adding new random
information in its place strengthens the provided privacy and provides better
utility at any given level of privacy. The result is an approach that can
provide stronger privatization on image data, and yet be preserving both the
domain and the utility of the inputs, entirely independent of the downstream
task.
</p>
<a href="http://arxiv.org/abs/2006.08039" target="_blank">arXiv:2006.08039</a> [<a href="http://arxiv.org/pdf/2006.08039" target="_blank">pdf</a>]

<h2>FrostNet: Towards Quantization-Aware Network Architecture Search. (arXiv:2006.09679v4 [cs.LG] UPDATED)</h2>
<h3>Taehoon Kim, YoungJoon Yoo, Jihoon Yang</h3>
<p>INT8 quantization has become one of the standard techniques for deploying
convolutional neural networks (CNNs) on edge devices to reduce the memory and
computational resource usages. By analyzing quantized performances of existing
mobile-target network architectures, we can raise an issue regarding the
importance of network architecture for optimal INT8 quantization. In this
paper, we present a new network architecture search (NAS) procedure to find a
network that guarantees both full-precision (FLOAT32) and quantized (INT8)
performances. We first propose critical but straightforward optimization method
which enables quantization-aware training (QAT) : floating-point statistic
assisting (StatAssist) and stochastic gradient boosting (GradBoost). By
integrating the gradient-based NAS with StatAssist and GradBoost, we discovered
a quantization-efficient network building block, Frost bottleneck. Furthermore,
we used Frost bottleneck as the building block for hardware-aware NAS to obtain
quantization-efficient networks, FrostNets, which show improved quantization
performances compared to other mobile-target networks while maintaining
competitive FLOAT32 performance. Our FrostNets achieve higher recognition
accuracy than existing CNNs with comparable latency when quantized, due to
higher latency reduction rate (average 65%).
</p>
<a href="http://arxiv.org/abs/2006.09679" target="_blank">arXiv:2006.09679</a> [<a href="http://arxiv.org/pdf/2006.09679" target="_blank">pdf</a>]

<h2>CoSE: Compositional Stroke Embeddings. (arXiv:2006.09930v2 [cs.LG] UPDATED)</h2>
<h3>Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, Otmar Hilliges</h3>
<p>We present a generative model for complex free-form structures such as
stroke-based drawing tasks. While previous approaches rely on sequence-based
models for drawings of basic objects or handwritten text, we propose a model
that treats drawings as a collection of strokes that can be composed into
complex structures such as diagrams (e.g., flow-charts). At the core of the
approach lies a novel autoencoder that projects variable-length strokes into a
latent space of fixed dimension. This representation space allows a relational
model, operating in latent space, to better capture the relationship between
strokes and to predict subsequent strokes. We demonstrate qualitatively and
quantitatively that our proposed approach is able to model the appearance of
individual strokes, as well as the compositional structure of larger diagram
drawings. Our approach is suitable for interactive use cases such as
auto-completing diagrams. We make code and models publicly available at
https://eth-ait.github.io/cose.
</p>
<a href="http://arxiv.org/abs/2006.09930" target="_blank">arXiv:2006.09930</a> [<a href="http://arxiv.org/pdf/2006.09930" target="_blank">pdf</a>]

<h2>DREAM: Deep Regret minimization with Advantage baselines and Model-free learning. (arXiv:2006.10410v2 [cs.LG] UPDATED)</h2>
<h3>Eric Steinberger, Adam Lerer, Noam Brown</h3>
<p>We introduce DREAM, a deep reinforcement learning algorithm that finds
optimal strategies in imperfect-information games with multiple agents.
Formally, DREAM converges to a Nash Equilibrium in two-player zero-sum games
and to an extensive-form coarse correlated equilibrium in all other games. Our
primary innovation is an effective algorithm that, in contrast to other
regret-based deep learning algorithms, does not require access to a perfect
simulator of the game to achieve good performance. We show that DREAM
empirically achieves state-of-the-art performance among model-free algorithms
in popular benchmark games, and is even competitive with algorithms that do use
a perfect simulator.
</p>
<a href="http://arxiv.org/abs/2006.10410" target="_blank">arXiv:2006.10410</a> [<a href="http://arxiv.org/pdf/2006.10410" target="_blank">pdf</a>]

<h2>Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes. (arXiv:2006.11441v3 [cs.LG] UPDATED)</h2>
<h3>Mengdi Xu, Wenhao Ding, Jiacheng Zhu, Zuxin Liu, Baiming Chen, Ding Zhao</h3>
<p>Continuously learning to solve unseen tasks with limited experience has been
extensively pursued in meta-learning and continual learning, but with
restricted assumptions such as accessible task distributions, independently and
identically distributed tasks, and clear task delineations. However, real-world
physical tasks frequently violate these assumptions, resulting in performance
degradation. This paper proposes a continual online model-based reinforcement
learning approach that does not require pre-training to solve task-agnostic
problems with unknown task boundaries. We maintain a mixture of experts to
handle nonstationarity, and represent each different type of dynamics with a
Gaussian Process to efficiently leverage collected data and expressively model
uncertainty. We propose a transition prior to account for the temporal
dependencies in streaming data and update the mixture online via sequential
variational inference. Our approach reliably handles the task distribution
shift by generating new models for never-before-seen dynamics and reusing old
models for previously seen dynamics. In experiments, our approach outperforms
alternative methods in non-stationary tasks, including classic control with
changing dynamics and decision making in different driving scenarios.
</p>
<a href="http://arxiv.org/abs/2006.11441" target="_blank">arXiv:2006.11441</a> [<a href="http://arxiv.org/pdf/2006.11441" target="_blank">pdf</a>]

<h2>Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors. (arXiv:2006.13205v2 [cs.LG] UPDATED)</h2>
<h3>Karl Pertsch, Oleh Rybkin, Frederik Ebert, Chelsea Finn, Dinesh Jayaraman, Sergey Levine</h3>
<p>The ability to predict and plan into the future is fundamental for agents
acting in the world. To reach a faraway goal, we predict trajectories at
multiple timescales, first devising a coarse plan towards the goal and then
gradually filling in details. In contrast, current learning approaches for
visual prediction and planning fail on long-horizon tasks as they generate
predictions (1) without considering goal information, and (2) at the finest
temporal resolution, one step at a time. In this work we propose a framework
for visual prediction and planning that is able to overcome both of these
limitations. First, we formulate the problem of predicting towards a goal and
propose the corresponding class of latent space goal-conditioned predictors
(GCPs). GCPs significantly improve planning efficiency by constraining the
search space to only those trajectories that reach the goal. Further, we show
how GCPs can be naturally formulated as hierarchical models that, given two
observations, predict an observation between them, and by recursively
subdividing each part of the trajectory generate complete sequences. This
divide-and-conquer strategy is effective at long-term prediction, and enables
us to design an effective hierarchical planning algorithm that optimizes
trajectories in a coarse-to-fine manner. We show that by using both
goal-conditioning and hierarchical prediction, GCPs enable us to solve visual
planning tasks with much longer horizon than previously possible.
</p>
<a href="http://arxiv.org/abs/2006.13205" target="_blank">arXiv:2006.13205</a> [<a href="http://arxiv.org/pdf/2006.13205" target="_blank">pdf</a>]

<h2>Tensor Programs II: Neural Tangent Kernel for Any Architecture. (arXiv:2006.14548v4 [stat.ML] UPDATED)</h2>
<h3>Greg Yang</h3>
<p>We prove that a randomly initialized neural network of *any architecture* has
its Tangent Kernel (NTK) converge to a deterministic limit, as the network
widths tend to infinity. We demonstrate how to calculate this limit. In prior
literature, the heuristic study of neural network gradients often assumes every
weight matrix used in forward propagation is independent from its transpose
used in backpropagation (Schoenholz et al. 2017). This is known as the
*gradient independence assumption (GIA)*. We identify a commonly satisfied
condition, which we call *Simple GIA Check*, such that the NTK limit
calculation based on GIA is correct. Conversely, when Simple GIA Check fails,
we show GIA can result in wrong answers. Our material here presents the NTK
results of Yang (2019a) in a friendly manner and showcases the *tensor
programs* technique for understanding wide neural networks. We provide
reference implementations of infinite-width NTKs of recurrent neural network,
transformer, and batch normalization at https://github.com/thegregyang/NTK4A.
</p>
<a href="http://arxiv.org/abs/2006.14548" target="_blank">arXiv:2006.14548</a> [<a href="http://arxiv.org/pdf/2006.14548" target="_blank">pdf</a>]

<h2>SMPR: Single-Stage Multi-Person Pose Regression. (arXiv:2006.15576v2 [cs.CV] UPDATED)</h2>
<h3>Junqi Lin, Huixin Miao, Junjie Cao, Zhixun Su, Risheng Liu</h3>
<p>Existing multi-person pose estimators can be roughly divided into two-stage
approaches (top-down and bottom-up approaches) and one-stage approaches. The
two-stage methods either suffer high computational redundancy for additional
person detectors or group keypoints heuristically after predicting all the
instance-free keypoints. The recently proposed single-stage methods do not rely
on the above two extra stages but have lower performance than the latest
bottom-up approaches. In this work, a novel single-stage multi-person pose
regression, termed SMPR, is presented. It follows the paradigm of dense
prediction and predicts instance-aware keypoints from every location. Besides
feature aggregation, we propose better strategies to define positive pose
hypotheses for training which all play an important role in dense pose
estimation. The network also learns the scores of estimated poses. The pose
scoring strategy further improves the pose estimation performance by
prioritizing superior poses during non-maximum suppression (NMS). We show that
our method not only outperforms existing single-stage methods and but also be
competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on
the COCO test-dev pose benchmark. Code is available at
https://github.com/cmdi-dlut/SMPR.
</p>
<a href="http://arxiv.org/abs/2006.15576" target="_blank">arXiv:2006.15576</a> [<a href="http://arxiv.org/pdf/2006.15576" target="_blank">pdf</a>]

<h2>Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v4 [cs.CV] UPDATED)</h2>
<h3>Youngwan Lee, Joong-won Hwang, Hyung-Il Kim, Kimin Yun, Yongjin Kwon</h3>
<p>Since many safety-critical systems, such as surgical robots and autonomous
driving cars, are in unstable environments with sensor noise and incomplete
data, it is desirable for object detectors to take into account the confidence
of localization prediction. There are three limitations of the prior
uncertainty estimation methods for anchor-based object detection. 1) They model
the uncertainty based on object properties having different characteristics,
such as location (center point) and scale (width, height). 2) they model a box
offset and ground-truth as Gaussian distribution and Dirac delta distribution,
which leads to the model misspecification problem. Because the Dirac delta
distribution is not exactly represented as Gaussian, i.e., for any $\mu$ and
$\Sigma$. 3) Since anchor-based methods are sensitive to hyper-parameters of
anchor, the localization uncertainty modeling is also sensitive to these
parameters. Therefore, we propose a new localization uncertainty estimation
method called Gaussian-FCOS for anchor-free object detection. Our method
captures the uncertainty based on four directions of box offsets~(left, right,
top, bottom) that have similar properties, which enables to capture which
direction is uncertain and provide a quantitative value in range~[0, 1]. To
this end, we design a new uncertainty loss, negative power log-likelihood loss,
to measure uncertainty by weighting IoU to the likelihood loss, which
alleviates the model misspecification problem. Experiments on COCO datasets
demonstrate that our Gaussian-FCOS reduces false positives and finds more
missing-objects by mitigating over-confidence scores with the estimated
uncertainty. We hope Gaussian-FCOS serves as a crucial component for the
reliability-required task.
</p>
<a href="http://arxiv.org/abs/2006.15607" target="_blank">arXiv:2006.15607</a> [<a href="http://arxiv.org/pdf/2006.15607" target="_blank">pdf</a>]

<h2>Causal Discovery in Physical Systems from Videos. (arXiv:2007.00631v3 [cs.LG] UPDATED)</h2>
<h3>Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox, Animesh Garg</h3>
<p>Causal discovery is at the core of human cognition. It enables us to reason
about the environment and make counterfactual predictions about unseen
scenarios that can vastly differ from our previous experiences. We consider the
task of causal discovery from videos in an end-to-end fashion without
supervision on the ground-truth graph structure. In particular, our goal is to
discover the structural dependencies among environmental and object variables:
inferring the type and strength of interactions that have a causal effect on
the behavior of the dynamical system. Our model consists of (a) a perception
module that extracts a semantically meaningful and temporally consistent
keypoint representation from images, (b) an inference module for determining
the graph distribution induced by the detected keypoints, and (c) a dynamics
module that can predict the future by conditioning on the inferred graph. We
assume access to different configurations and environmental conditions, i.e.,
data from unknown interventions on the underlying system; thus, we can hope to
discover the correct underlying causal graph without explicit interventions. We
evaluate our method in a planar multi-body interaction environment and
scenarios involving fabrics of different shapes like shirts and pants.
Experiments demonstrate that our model can correctly identify the interactions
from a short sequence of images and make long-term future predictions. The
causal structure assumed by the model also allows it to make counterfactual
predictions and extrapolate to systems of unseen interaction graphs or graphs
of various sizes.
</p>
<a href="http://arxiv.org/abs/2007.00631" target="_blank">arXiv:2007.00631</a> [<a href="http://arxiv.org/pdf/2007.00631" target="_blank">pdf</a>]

<h2>Query-Free Adversarial Transfer via Undertrained Surrogates. (arXiv:2007.00806v2 [cs.CV] UPDATED)</h2>
<h3>Chris Miller, Soroush Vosoughi</h3>
<p>Deep neural networks are vulnerable to adversarial examples -- minor
perturbations added to a model's input which cause the model to output an
incorrect prediction. We introduce a new method for improving the efficacy of
adversarial attacks in a black-box setting by undertraining the surrogate model
which the attacks are generated on. Using two datasets and five model
architectures, we show that this method transfers well across architectures and
outperforms state-of-the-art methods by a wide margin. We interpret the
effectiveness of our approach as a function of reduced surrogate model loss
function curvature and increased universal gradient characteristics, and show
that our approach reduces the presence of local loss maxima which hinder
transferability. Our results suggest that finding strong single surrogate
models is a highly effective and simple method for generating transferable
adversarial attacks, and that this method represents a valuable route for
future study in this field.
</p>
<a href="http://arxiv.org/abs/2007.00806" target="_blank">arXiv:2007.00806</a> [<a href="http://arxiv.org/pdf/2007.00806" target="_blank">pdf</a>]

<h2>AutoBayes: Automated Bayesian Graph Exploration for Nuisance-Robust Inference. (arXiv:2007.01255v3 [cs.LG] UPDATED)</h2>
<h3>Andac Demir, Toshiaki Koike-Akino, Ye Wang, Deniz Erdogmus</h3>
<p>Learning data representations that capture task-related features, but are
invariant to nuisance variations remains a key challenge in machine learning.
We introduce an automated Bayesian inference framework, called AutoBayes, that
explores different graphical models linking classifier, encoder, decoder,
estimator and adversarial network blocks to optimize nuisance-invariant machine
learning pipelines. AutoBayes also enables learning disentangled
representations, where the latent variable is split into multiple pieces to
impose various relationships with the nuisance variation and task labels. We
benchmark the framework on several public datasets, and provide analysis of its
capability for subject-transfer learning with/without variational modeling and
adversarial training. We demonstrate a significant performance improvement with
ensemble learning across explored graphical models.
</p>
<a href="http://arxiv.org/abs/2007.01255" target="_blank">arXiv:2007.01255</a> [<a href="http://arxiv.org/pdf/2007.01255" target="_blank">pdf</a>]

<h2>Covariate Distribution Aware Meta-learning. (arXiv:2007.02523v3 [cs.LG] UPDATED)</h2>
<h3>Amrith Setlur, Saket Dingliwal, Barnabas Poczos</h3>
<p>Meta-learning has proven to be successful for few-shot learning across the
regression, classification, and reinforcement learning paradigms. Recent
approaches have adopted Bayesian interpretations to improve gradient-based
meta-learners by quantifying the uncertainty of the post-adaptation estimates.
Most of these works almost completely ignore the latent relationship between
the covariate distribution $(p(x))$ of a task and the corresponding conditional
distribution $p(y|x)$. In this paper, we identify the need to explicitly model
the meta-distribution over the task covariates in a hierarchical Bayesian
framework. We begin by introducing a graphical model that leverages the samples
from the marginal $p(x)$ to better infer the posterior over the optimal
parameters of the conditional distribution $(p(y|x))$ for each task. Based on
this model we propose a computationally feasible meta-learning algorithm by
introducing meaningful relaxations in our final objective. We demonstrate the
gains of our algorithm over initialization based meta-learning baselines on
popular classification benchmarks. Finally, to understand the potential benefit
of modeling task covariates we further evaluate our method on a synthetic
regression dataset.
</p>
<a href="http://arxiv.org/abs/2007.02523" target="_blank">arXiv:2007.02523</a> [<a href="http://arxiv.org/pdf/2007.02523" target="_blank">pdf</a>]

<h2>Dynamic Regret of Convex and Smooth Functions. (arXiv:2007.03479v2 [cs.LG] UPDATED)</h2>
<h3>Peng Zhao, Yu-Jie Zhang, Lijun Zhang, Zhi-Hua Zhou</h3>
<p>We investigate online convex optimization in non-stationary environments and
choose the dynamic regret as the performance measure, defined as the difference
between cumulative loss incurred by the online algorithm and that of any
feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the
path-length that essentially reflects the non-stationarity of environments, the
state-of-the-art dynamic regret is $\mathcal{O}(\sqrt{T(1+P_T)})$. Although
this bound is proved to be minimax optimal for convex functions, in this paper,
we demonstrate that it is possible to further enhance the dynamic regret by
exploiting the smoothness condition. Specifically, we propose novel online
algorithms that are capable of leveraging smoothness and replace the dependence
on $T$ in the dynamic regret by problem-dependent quantities: the variation in
gradients of loss functions, the cumulative loss of the comparator sequence,
and the minimum of the previous two terms. These quantities are at most
$\mathcal{O}(T)$ while could be much smaller in benign environments. Therefore,
our results are adaptive to the intrinsic difficulty of the problem, since the
bounds are tighter than existing results for easy problems and meanwhile
guarantee the same rate in the worst case.
</p>
<a href="http://arxiv.org/abs/2007.03479" target="_blank">arXiv:2007.03479</a> [<a href="http://arxiv.org/pdf/2007.03479" target="_blank">pdf</a>]

<h2>Domain Adaptation for Robust Workload Level Alignment Between Sessions and Subjects using fNIRS. (arXiv:2007.06706v2 [cs.CV] UPDATED)</h2>
<h3>Boyang Lyu, Thao Pham, Giles Blaney, Zachary Haga, Angelo Sassaroli, Sergio Fantini, Shuchin Aeron</h3>
<p>Significance: We demonstrated the potential of using domain adaptation on
functional Near-Infrared Spectroscopy (fNIRS) data to classify different levels
of n-back tasks that involve working memory. Aim: Domain shift in fNIRS data is
a challenge in the workload level alignment across different experiment
sessions and subjects. In order to address this problem, two domain adaptation
approaches -- Gromov-Wasserstein (G-W) and Fused Gromov-Wasserstein (FG-W) were
used. Approach: Specifically, we used labeled data from one session or one
subject to classify trials in another session (within the same subject) or
another subject. We applied G-W for session-by-session alignment and FG-W for
subject-by-subject alignment to fNIRS data acquired during different n-back
task levels. We compared these approaches with three supervised methods:
multi-class Support Vector Machine (SVM), Convolutional Neural Network (CNN),
and Recurrent Neural Network (RNN). Results: In a sample of six subjects, G-W
resulted in an alignment accuracy of 68 $\pm$ 4 % (weighted mean $\pm$ standard
error) for session-by-session alignment, FG-W resulted in an alignment accuracy
of 55 $\pm$ 2 % for subject-by-subject alignment. In each of these cases, 25 %
accuracy represents chance. Alignment accuracy results from both G-W and FG-W
are significantly greater than those from SVM, CNN and RNN. We also showed that
removal of motion artifacts from the fNIRS data plays an important role in
improving alignment performance. Conclusions: Domain adaptation has potential
for session-by-session and subject-by-subject alignment of mental workload by
using fNIRS data.
</p>
<a href="http://arxiv.org/abs/2007.06706" target="_blank">arXiv:2007.06706</a> [<a href="http://arxiv.org/pdf/2007.06706" target="_blank">pdf</a>]

<h2>Early stopping and polynomial smoothing in regression with reproducing kernels. (arXiv:2007.06827v2 [stat.ML] UPDATED)</h2>
<h3>Yaroslav Averyanov, Alain Celisse</h3>
<p>In this paper, we study the problem of early stopping for iterative learning
algorithms in a reproducing kernel Hilbert space (RKHS) in the nonparametric
regression framework. In particular, we work with the gradient descent and
(iterative) kernel ridge regression algorithms. We present a data-driven rule
to perform early stopping without a validation set that is based on the
so-called minimum discrepancy principle. This method enjoys only one assumption
on the regression function: it belongs to a reproducing kernel Hilbert space
(RKHS). The proposed rule is proved to be minimax-optimal over different types
of kernel spaces, including finite-rank and Sobolev smoothness classes. The
proof is derived from the fixed-point analysis of the localized Rademacher
complexities, which is a standard technique for obtaining optimal rates in the
nonparametric regression literature. In addition to that, we present simulation
results on artificial datasets that show the comparable performance of the
designed rule with respect to other stopping rules such as the one determined
by V-fold cross-validation.
</p>
<a href="http://arxiv.org/abs/2007.06827" target="_blank">arXiv:2007.06827</a> [<a href="http://arxiv.org/pdf/2007.06827" target="_blank">pdf</a>]

<h2>Two-Level Adversarial Visual-Semantic Coupling for Generalized Zero-shot Learning. (arXiv:2007.07757v2 [cs.CV] UPDATED)</h2>
<h3>Shivam Chandhok, Vineeth N Balasubramanian</h3>
<p>The performance of generative zero-shot methods mainly depends on the quality
of generated features and how well the model facilitates knowledge transfer
between visual and semantic domains. The quality of generated features is a
direct consequence of the ability of the model to capture the several modes of
the underlying data distribution. To address these issues, we propose a new
two-level joint maximization idea to augment the generative network with an
inference network during training which helps our model capture the several
modes of the data and generate features that better represent the underlying
data distribution. This provides strong cross-modal interaction for effective
transfer of knowledge between visual and semantic domains. Furthermore,
existing methods train the zero-shot classifier either on generate synthetic
image features or latent embeddings produced by leveraging representation
learning. In this work, we unify these paradigms into a single model which in
addition to synthesizing image features, also utilizes the representation
learning capabilities of the inference network to provide discriminative
features for the final zero-shot recognition task. We evaluate our approach on
four benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several
state-of-the-art methods, and show its performance. We also perform ablation
studies to analyze and understand our method more carefully for the Generalized
Zero-shot Learning task.
</p>
<a href="http://arxiv.org/abs/2007.07757" target="_blank">arXiv:2007.07757</a> [<a href="http://arxiv.org/pdf/2007.07757" target="_blank">pdf</a>]

<h2>ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning. (arXiv:2007.07936v2 [cs.CV] UPDATED)</h2>
<h3>Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, Lennart Svensson</h3>
<p>The state of the art in semantic segmentation is steadily increasing in
performance, resulting in more precise and reliable segmentations in many
different applications. However, progress is limited by the cost of generating
labels for training, which sometimes requires hours of manual labor for a
single image. Because of this, semi-supervised methods have been applied to
this task, with varying degrees of success. A key challenge is that common
augmentations used in semi-supervised classification are less effective for
semantic segmentation. We propose a novel data augmentation mechanism called
ClassMix, which generates augmentations by mixing unlabelled samples, by
leveraging on the network's predictions for respecting object boundaries. We
evaluate this augmentation technique on two common semi-supervised semantic
segmentation benchmarks, showing that it attains state-of-the-art results.
Lastly, we also provide extensive ablation studies comparing different design
decisions and training regimes.
</p>
<a href="http://arxiv.org/abs/2007.07936" target="_blank">arXiv:2007.07936</a> [<a href="http://arxiv.org/pdf/2007.07936" target="_blank">pdf</a>]

<h2>DACS: Domain Adaptation via Cross-domain Mixed Sampling. (arXiv:2007.08702v2 [cs.CV] UPDATED)</h2>
<h3>Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, Lennart Svensson</h3>
<p>Semantic segmentation models based on convolutional neural networks have
recently displayed remarkable performance for a multitude of applications.
However, these models typically do not generalize well when applied on new
domains, especially when going from synthetic to real data. In this paper we
address the problem of unsupervised domain adaptation (UDA), which attempts to
train on labelled data from one domain (source domain), and simultaneously
learn from unlabelled data in the domain of interest (target domain). Existing
methods have seen success by training on pseudo-labels for these unlabelled
images. Multiple techniques have been proposed to mitigate low-quality
pseudo-labels arising from the domain shift, with varying degrees of success.
We propose DACS: Domain Adaptation via Cross-domain mixed Sampling, which mixes
images from the two domains along with the corresponding labels and
pseudo-labels. These mixed samples are then trained on, in addition to the
labelled data itself. We demonstrate the effectiveness of our solution by
achieving state-of-the-art results for GTA5 to Cityscapes, a common
synthetic-to-real semantic segmentation benchmark for UDA.
</p>
<a href="http://arxiv.org/abs/2007.08702" target="_blank">arXiv:2007.08702</a> [<a href="http://arxiv.org/pdf/2007.08702" target="_blank">pdf</a>]

<h2>It's LeVAsa not LevioSA! Latent Encodings for Valence-Arousal Structure Alignment. (arXiv:2007.10058v3 [cs.AI] UPDATED)</h2>
<h3>Surabhi S. Nath, Vishaal Udandarao, Jainendra Shukla</h3>
<p>In recent years, great strides have been made in the field of affective
computing. Several models have been developed to represent and quantify
emotions. Two popular ones include (i) categorical models which represent
emotions as discrete labels, and (ii) dimensional models which represent
emotions in a Valence-Arousal (VA) circumplex domain. However, there is no
standard for annotation mapping between the two labelling methods. We build a
novel algorithm for mapping categorical and dimensional model labels using
annotation transfer across affective facial image datasets. Further, we utilize
the transferred annotations to learn rich and interpretable data
representations using a variational autoencoder (VAE). We present "LeVAsa", a
VAE model that learns implicit structure by aligning the latent space with the
VA space. We evaluate the efficacy of LeVAsa by comparing performance with the
Vanilla VAE using quantitative and qualitative analysis on two benchmark
affective image datasets. Our results reveal that LeVAsa achieves high
latent-circumplex alignment which leads to improved downstream categorical
emotion prediction. The work also demonstrates the trade-off between degree of
alignment and quality of reconstructions.
</p>
<a href="http://arxiv.org/abs/2007.10058" target="_blank">arXiv:2007.10058</a> [<a href="http://arxiv.org/pdf/2007.10058" target="_blank">pdf</a>]

<h2>Neural networks with late-phase weights. (arXiv:2007.12927v2 [cs.LG] UPDATED)</h2>
<h3>Johannes von Oswald, Seijin Kobayashi, Jo&#xe3;o Sacramento, Alexander Meulemans, Christian Henning, Benjamin F. Grewe</h3>
<p>The largely successful method of training neural networks is to learn their
weights using some variant of stochastic gradient descent (SGD). Here, we show
that the solutions found by SGD can be further improved by ensembling a subset
of the weights in late stages of learning. At the end of learning, we obtain
back a single model by taking a spatial average in weight space. To avoid
incurring increased computational costs, we investigate a family of
low-dimensional late-phase weight models which interact multiplicatively with
the remaining parameters. Our results show that augmenting standard models with
late-phase weights improves generalization in established benchmarks such as
CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a
theoretical analysis of a noisy quadratic problem which provides a simplified
picture of the late phases of neural network learning.
</p>
<a href="http://arxiv.org/abs/2007.12927" target="_blank">arXiv:2007.12927</a> [<a href="http://arxiv.org/pdf/2007.12927" target="_blank">pdf</a>]

<h2>Efficient Generation of Structured Objects with Constrained Adversarial Networks. (arXiv:2007.13197v2 [cs.LG] UPDATED)</h2>
<h3>Luca Di Liello, Pierfrancesco Ardino, Jacopo Gobbi, Paolo Morettin, Stefano Teso, Andrea Passerini</h3>
<p>Generative Adversarial Networks (GANs) struggle to generate structured
objects like molecules and game maps. The issue is that structured objects must
satisfy hard requirements (e.g., molecules must be chemically valid) that are
difficult to acquire from examples alone. As a remedy, we propose Constrained
Adversarial Networks (CANs), an extension of GANs in which the constraints are
embedded into the model during training. This is achieved by penalizing the
generator proportionally to the mass it allocates to invalid structures. In
contrast to other generative models, CANs support efficient inference of valid
structures (with high probability) and allows to turn on and off the learned
constraints at inference time. CANs handle arbitrary logical constraints and
leverage knowledge compilation techniques to efficiently evaluate the
disagreement between the model and the constraints. Our setup is further
extended to hybrid logical-neural constraints for capturing very complex
constraints, like graph reachability. An extensive empirical analysis shows
that CANs efficiently generate valid structures that are both high-quality and
novel.
</p>
<a href="http://arxiv.org/abs/2007.13197" target="_blank">arXiv:2007.13197</a> [<a href="http://arxiv.org/pdf/2007.13197" target="_blank">pdf</a>]

<h2>RGB-D Salient Object Detection: A Survey. (arXiv:2008.00230v3 [cs.CV] UPDATED)</h2>
<h3>Tao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing Shen, Ling Shao</h3>
<p>Salient object detection (SOD), which simulates the human visual perception
system to locate the most attractive object(s) in a scene, has been widely
applied to various computer vision tasks. Now, with the advent of depth
sensors, depth maps with affluent spatial information that can be beneficial in
boosting the performance of SOD, can easily be captured. Although various RGB-D
based SOD models with promising performance have been proposed over the past
several years, an in-depth understanding of these models and challenges in this
topic remains lacking. In this paper, we provide a comprehensive survey of
RGB-D based SOD models from various perspectives, and review related benchmark
datasets in detail. Further, considering that the light field can also provide
depth maps, we review SOD models and popular benchmark datasets from this
domain as well. Moreover, to investigate the SOD ability of existing models, we
carry out a comprehensive evaluation, as well as attribute-based evaluation of
several representative RGB-D based SOD models. Finally, we discuss several
challenges and open directions of RGB-D based SOD for future research. All
collected models, benchmark datasets, source code links, datasets constructed
for attribute-based evaluation, and codes for evaluation will be made publicly
available at https://github.com/taozh2017/RGBDSODsurvey
</p>
<a href="http://arxiv.org/abs/2008.00230" target="_blank">arXiv:2008.00230</a> [<a href="http://arxiv.org/pdf/2008.00230" target="_blank">pdf</a>]

<h2>Dynamic Discrete Choice Estimation with Partially Observable States and Hidden Dynamics. (arXiv:2008.00500v2 [cs.LG] UPDATED)</h2>
<h3>Yanling Chang, Alfredo Garcia, Zhide Wang</h3>
<p>Dynamic discrete choice models are used to estimate the intertemporal
preferences of an agent as described by a reward function based upon observable
histories of states and implemented actions. However, in many applications,
such as reliability and healthcare, the system state is only partially
observable or hidden (e.g., the level of deterioration of an engine, the
condition of a disease), and the decision maker only has access to information
imperfectly correlated with the true value of the hidden state. In this paper,
we consider the estimation of a dynamic discrete choice model with state
variables and system dynamics hidden to both the agent and the modeler, thus
generalizing the model in Rust(1987) to partially observable cases. We examine
the structural properties of the model and prove that this model is still
identifiable if the cardinality of the state space, the discount factor, the
distribution of random shocks, and the rewards for a given (reference) action
are given. We analyze both theoretically and numerically the potential
mis-specification errors that may be incurred when the Rust's model is
improperly used in partially observable settings. We further apply the model to
a subset of dataset in Rust(1987) for bus engine mileage and replacement
decisions. The results show that our model can improve model fit as measured by
the $\log$-likelihood function by $17.73\%$ and the $\log$-likelihood ratio
test shows that our model statistically outperforms the Rust's model.
Interestingly, our hidden state model also reveals an economically meaningful
route assignment behavior in the dataset which was hitherto ignored, i.e.
routes with lower mileage are assigned to buses believed to be in worse
condition.
</p>
<a href="http://arxiv.org/abs/2008.00500" target="_blank">arXiv:2008.00500</a> [<a href="http://arxiv.org/pdf/2008.00500" target="_blank">pdf</a>]

<h2>Geometric Interpretations of the Normalized Epipolar Error. (arXiv:2008.01254v5 [cs.CV] UPDATED)</h2>
<h3>Seong Hun Lee, Javier Civera</h3>
<p>In this work, we provide geometric interpretations of the normalized epipolar
error. Most notably, we show that it is directly related to the following
quantities: (1) the shortest distance between the two backprojected rays, (2)
the dihedral angle between the two bounding epipolar planes, and (3) the
$L_1$-optimal angular reprojection error.
</p>
<a href="http://arxiv.org/abs/2008.01254" target="_blank">arXiv:2008.01254</a> [<a href="http://arxiv.org/pdf/2008.01254" target="_blank">pdf</a>]

<h2>LoCo: Local Contrastive Representation Learning. (arXiv:2008.01342v2 [cs.LG] UPDATED)</h2>
<h3>Yuwen Xiong, Mengye Ren, Raquel Urtasun</h3>
<p>Deep neural nets typically perform end-to-end backpropagation to learn the
weights, a procedure that creates synchronization constraints in the weight
update step across layers and is not biologically plausible. Recent advances in
unsupervised contrastive representation learning point to the question of
whether a learning algorithm can also be made local, that is, the updates of
lower layers do not directly depend on the computation of upper layers. While
Greedy InfoMax separately learns each block with a local objective, we found
that it consistently hurts readout accuracy in state-of-the-art unsupervised
contrastive learning algorithms, possibly due to the greedy objective as well
as gradient isolation. In this work, we discover that by overlapping local
blocks stacking on top of each other, we effectively increase the decoder depth
and allow upper blocks to implicitly send feedbacks to lower blocks. This
simple design closes the performance gap between local learning and end-to-end
contrastive learning algorithms for the first time. Aside from standard
ImageNet experiments, we also show results on complex downstream tasks such as
object detection and instance segmentation directly using readout features.
</p>
<a href="http://arxiv.org/abs/2008.01342" target="_blank">arXiv:2008.01342</a> [<a href="http://arxiv.org/pdf/2008.01342" target="_blank">pdf</a>]

<h2>Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications. (arXiv:2008.02793v2 [cs.CV] UPDATED)</h2>
<h3>Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, Arun Mallya</h3>
<p>The generative adversarial network (GAN) framework has emerged as a powerful
tool for various image and video synthesis tasks, allowing the synthesis of
visual content in an unconditional or input-conditional manner. It has enabled
the generation of high-resolution photorealistic images and videos, a task that
was challenging or impossible with prior methods. It has also led to the
creation of many new applications in content creation. In this paper, we
provide an overview of GANs with a special focus on algorithms and applications
for visual synthesis. We cover several important techniques to stabilize GAN
training, which has a reputation for being notoriously difficult. We also
discuss its applications to image translation, image processing, video
synthesis, and neural rendering.
</p>
<a href="http://arxiv.org/abs/2008.02793" target="_blank">arXiv:2008.02793</a> [<a href="http://arxiv.org/pdf/2008.02793" target="_blank">pdf</a>]

<h2>Regularization and Normalization For Generative Adversarial Networks: A Survey. (arXiv:2008.08930v2 [cs.LG] UPDATED)</h2>
<h3>Ziqiang Li, Rentuo Tao, Pengfei Xia, Huanhuan Chen, Bin Li</h3>
<p>Generative Adversarial Networks (GANs), a popular generative model, have been
widely applied in different scenarios thanks to the development of deep neural
networks. The proposal of standard GAN is based upon the non-parametric
assumption of the infinite capacity of networks. It is still unknown whether
GANs can generate realistic samples without any prior. Due to excessive
assumptions, many issues need to be addressed in GANs training, such as
non-convergence, mode collapses, gradient disappearance, and the sensitivity of
hyperparameters. As acknowledged, regularization and normalization are common
methods of introducing prior information and can be used for stability training
as well. At present, many regularization and normalization methods are proposed
in GANs.In order to explain these methods in a systematic manner, this paper
summarizes regularization and normalization methods used in GANs and classifies
them into seven groups: Gradient penalty, Norm normalization and
regularization, Jacobian regularization, Layer normalization, Consistency
regularization, Data Augmentation, and Self-supervision. This paper presents
the analysis of these methods and highlights the possible future studies in
this area.
</p>
<a href="http://arxiv.org/abs/2008.08930" target="_blank">arXiv:2008.08930</a> [<a href="http://arxiv.org/pdf/2008.08930" target="_blank">pdf</a>]

<h2>Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v3 [cs.CV] UPDATED)</h2>
<h3>Bharath Comandur, Avinash C. Kak</h3>
<p>We present a novel multi-view training framework and CNN architecture for
combining information from multiple overlapping satellite images and noisy
training labels derived from OpenStreetMap (OSM) to semantically label
buildings and roads across large geographic regions (100 km$^2$). Our approach
to multi-view semantic segmentation yields a 4-7% improvement in the per-class
IoU scores compared to the traditional approaches that use the views
independently of one another. A unique (and, perhaps, surprising) property of
our system is that modifications that are added to the tail-end of the CNN for
learning from the multi-view data can be discarded at the time of inference
with a relatively small penalty in the overall performance. This implies that
the benefits of training using multiple views are absorbed by all the layers of
the network. Additionally, our approach only adds a small overhead in terms of
the GPU-memory consumption even when training with as many as 32 views per
scene. The system we present is end-to-end automated, which facilitates
comparing the classifiers trained directly on true orthophotos vis-a-vis first
training them on the off-nadir images and subsequently translating the
predicted labels to geographical coordinates. With no human supervision, our
IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively
which are better than state-of-the-art approaches that use OSM labels and that
are not completely automated.
</p>
<a href="http://arxiv.org/abs/2008.10271" target="_blank">arXiv:2008.10271</a> [<a href="http://arxiv.org/pdf/2008.10271" target="_blank">pdf</a>]

<h2>DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment in COVID-19 Pandemic. (arXiv:2008.11672v3 [cs.CV] UPDATED)</h2>
<h3>Mahdi Rezaei, Mohsen Azarmi</h3>
<p>Social distancing is a recommended solution by the World Health Organisation
(WHO) to minimise the spread of COVID-19 in public places. The majority of
governments and national health authorities have set the 2-meter physical
distancing as a mandatory safety measure in shopping centres, schools and other
covered areas. In this research, we develop a hybrid Computer Vision and
YOLOv4-based Deep Neural Network model for automated people detection in the
crowd in indoor and outdoor environments using common CCTV security cameras.
The proposed DNN model in combination with an adapted inverse perspective
mapping (IPM) technique and SORT tracking algorithm leads to a robust people
detection and social distancing monitoring. The model has been trained against
two most comprehensive datasets by the time of the research the Microsoft
Common Objects in Context (MS COCO) and Google Open Image datasets. The system
has been evaluated against the Oxford Town Centre dataset with superior
performance compared to three state-of-the-art methods. The evaluation has been
conducted in challenging conditions, including occlusion, partial visibility,
and under lighting variations with the mean average precision of 99.8% and the
real-time speed of 24.1 fps. We also provide an online infection risk
assessment scheme by statistical analysis of the Spatio-temporal data from
people's moving trajectories and the rate of social distancing violations. The
developed model is a generic and accurate people detection and tracking
solution that can be applied in many other fields such as autonomous vehicles,
human action recognition, anomaly detection, sports, crowd analysis, or any
other research areas where the human detection is in the centre of attention.
</p>
<a href="http://arxiv.org/abs/2008.11672" target="_blank">arXiv:2008.11672</a> [<a href="http://arxiv.org/pdf/2008.11672" target="_blank">pdf</a>]

<h2>reval: a Python package to determine best clustering solutions with stability-based relative clustering validation. (arXiv:2009.01077v2 [cs.LG] UPDATED)</h2>
<h3>Isotta Landi, Veronica Mandelli, Michael V. Lombardo</h3>
<p>Determining the best partition for a dataset can be a challenging task
because of 1) the lack of a priori information within an unsupervised learning
framework; and 2) the absence of a unique clustering validation approach to
evaluate clustering solutions. Here we present reval: a Python package that
leverages stability-based relative clustering validation methods to determine
best clustering solutions as the ones that best generalize to unseen data.
Statistical software, both in R and Python, usually rely on internal validation
metrics, such as silhouette, to select the number of clusters that best fits
the data. Meanwhile, open-source software solutions that easily implement
relative clustering techniques are lacking. Internal validation methods exploit
characteristics of the data itself to produce a result, whereas relative
approaches attempt to leverage the unknown underlying distribution of data
points looking for generalizable and replicable results. The implementation of
relative validation methods can further the theory of clustering by enriching
the already available methods that can be used to investigate clustering
results in different situations and for different data distributions. This work
aims at contributing to this effort by developing a stability-based method that
selects the best clustering solution as the one that replicates, via supervised
learning, on unseen subsets of data. The package works with multiple clustering
and classification algorithms, hence allowing both the automatization of the
labeling process and the assessment of the stability of different clustering
mechanisms.
</p>
<a href="http://arxiv.org/abs/2009.01077" target="_blank">arXiv:2009.01077</a> [<a href="http://arxiv.org/pdf/2009.01077" target="_blank">pdf</a>]

<h2>Perceptual Deep Neural Networks: Adversarial Robustness through Input Recreation. (arXiv:2009.01110v4 [cs.CV] UPDATED)</h2>
<h3>Danilo Vasconcellos Vargas, Bingli Liao, Takahiro Kanzaki</h3>
<p>Adversarial examples have shown that albeit highly accurate, models learned
by machines, differently from humans, have many weaknesses. However, humans'
perception is also fundamentally different from machines, because we do not see
the signals which arrive at the retina but a rather complex recreation of them.
In this paper, we explore how machines could recreate the input as well as
investigate the benefits of such an augmented perception. In this regard, we
propose Perceptual Deep Neural Networks ($\varphi$DNN) which also recreate
their own input before further processing. The concept is formalized
mathematically and two variations of it are developed (one based on inpainting
the whole image and the other based on a noisy resized super resolution
recreation). Experiments reveal that $\varphi$DNNs and their adversarial
training variations can increase the robustness substantially, surpassing both
state-of-the-art defenses and pre-processing types of defenses in 100% of the
tests. $\varphi$DNNs are shown to scale well to bigger image sizes, keeping a
similar high accuracy throughout; while the state-of-the-art worsen up to 35%.
Moreover, the recreation process intentionally corrupts the input image.
Interestingly, we show by ablation tests that corrupting the input is, although
counter-intuitive, beneficial. Thus, $\varphi$DNNs reveal that input recreation
has strong benefits for artificial neural networks similar to biological ones,
shedding light into the importance of purposely corrupting the input as well as
pioneering an area of perception models based on GANs and autoencoders for
robust recognition in artificial intelligence.
</p>
<a href="http://arxiv.org/abs/2009.01110" target="_blank">arXiv:2009.01110</a> [<a href="http://arxiv.org/pdf/2009.01110" target="_blank">pdf</a>]

<h2>HPSGD: Hierarchical Parallel SGD With Stale Gradients Featuring. (arXiv:2009.02701v2 [cs.LG] UPDATED)</h2>
<h3>Yuhao Zhou, Qing Ye, Hailun Zhang, Jiancheng Lv</h3>
<p>While distributed training significantly speeds up the training process of
the deep neural network (DNN), the utilization of the cluster is relatively low
due to the time-consuming data synchronizing between workers. To alleviate this
problem, a novel Hierarchical Parallel SGD (HPSGD) strategy is proposed based
on the observation that the data synchronization phase can be paralleled with
the local training phase (i.e., Feed-forward and back-propagation).
Furthermore, an improved model updating method is unitized to remedy the
introduced stale gradients problem, which commits updates to the replica (i.e.,
a temporary model that has the same parameters as the global model) and then
merges the average changes to the global model. Extensive experiments are
conducted to demonstrate that the proposed HPSGD approach substantially boosts
the distributed DNN training, reduces the disturbance of the stale gradients
and achieves better accuracy in given fixed wall-time.
</p>
<a href="http://arxiv.org/abs/2009.02701" target="_blank">arXiv:2009.02701</a> [<a href="http://arxiv.org/pdf/2009.02701" target="_blank">pdf</a>]

<h2>The Importance of Pessimism in Fixed-Dataset Policy Optimization. (arXiv:2009.06799v3 [cs.AI] UPDATED)</h2>
<h3>Jacob Buckman, Carles Gelada, Marc G. Bellemare</h3>
<p>We study worst-case guarantees on the expected return of fixed-dataset policy
optimization algorithms. Our core contribution is a unified conceptual and
mathematical framework for the study of algorithms in this regime. This
analysis reveals that for naive approaches, the possibility of erroneous value
overestimation leads to a difficult-to-satisfy requirement: in order to
guarantee that we select a policy which is near-optimal, we may need the
dataset to be informative of the value of every policy. To avoid this,
algorithms can follow the pessimism principle, which states that we should
choose the policy which acts optimally in the worst possible world. We show why
pessimistic algorithms can achieve good performance even when the dataset is
not informative of every policy, and derive families of algorithms which follow
this principle. These theoretical findings are validated by experiments on a
tabular gridworld, and deep learning experiments on four MinAtar environments.
</p>
<a href="http://arxiv.org/abs/2009.06799" target="_blank">arXiv:2009.06799</a> [<a href="http://arxiv.org/pdf/2009.06799" target="_blank">pdf</a>]

<h2>Driver Anomaly Detection: A Dataset and Contrastive Learning Approach. (arXiv:2009.14660v2 [cs.CV] UPDATED)</h2>
<h3>Okan K&#xf6;p&#xfc;kl&#xfc;, Jiapeng Zheng, Hang Xu, Gerhard Rigoll</h3>
<p>Distracted drivers are more likely to fail to anticipate hazards, which
result in car accidents. Therefore, detecting anomalies in drivers' actions
(i.e., any action deviating from normal driving) contains the utmost importance
to reduce driver-related accidents. However, there are unbounded many anomalous
actions that a driver can do while driving, which leads to an 'open set
recognition' problem. Accordingly, instead of recognizing a set of anomalous
actions that are commonly defined by previous dataset providers, in this work,
we propose a contrastive learning approach to learn a metric to differentiate
normal driving from anomalous driving. For this task, we introduce a new
video-based benchmark, the Driver Anomaly Detection (DAD) dataset, which
contains normal driving videos together with a set of anomalous actions in its
training set. In the test set of the DAD dataset, there are unseen anomalous
actions that still need to be winnowed out from normal driving. Our method
reaches 0.9673 AUC on the test set, demonstrating the effectiveness of the
contrastive learning approach on the anomaly detection task. Our dataset, codes
and pre-trained models are publicly available.
</p>
<a href="http://arxiv.org/abs/2009.14660" target="_blank">arXiv:2009.14660</a> [<a href="http://arxiv.org/pdf/2009.14660" target="_blank">pdf</a>]

<h2>Static and Animated 3D Scene Generation from Free-form Text Descriptions. (arXiv:2010.01549v2 [cs.CV] UPDATED)</h2>
<h3>Faria Huq, Nafees Ahmed, Anindya Iqbal</h3>
<p>Generating coherent and useful image/video scenes from a free-form textual
description is technically a very difficult problem to handle. Textual
description of the same scene can vary greatly from person to person, or
sometimes even for the same person from time to time. As the choice of words
and syntax vary while preparing a textual description, it is challenging for
the system to reliably produce a consistently desirable output from different
forms of language input. The prior works of scene generation have been mostly
confined to rigorous sentence structures of text input which restrict the
freedom of users to write description. In our work, we study a new pipeline
that aims to generate static as well as animated 3D scenes from different types
of free-form textual scene description without any major restriction. In
particular, to keep our study practical and tractable, we focus on a small
subspace of all possible 3D scenes, containing various combinations of cube,
cylinder and sphere. We design a two-stage pipeline. In the first stage, we
encode the free-form text using an encoder-decoder neural architecture. In the
second stage, we generate a 3D scene based on the generated encoding. Our
neural architecture exploits state-of-the-art language model as encoder to
leverage rich contextual encoding and a new multi-head decoder to predict
multiple features of an object in the scene simultaneously. For our
experiments, we generate a large synthetic data-set which contains 13,00,000
and 14,00,000 samples of unique static and animated scene descriptions,
respectively. We achieve 98.427% accuracy on test data set in detecting the 3D
objects features successfully. Our work shows a proof of concept of one
approach towards solving the problem, and we believe with enough training data,
the same pipeline can be expanded to handle even broader set of 3D scene
generation problems.
</p>
<a href="http://arxiv.org/abs/2010.01549" target="_blank">arXiv:2010.01549</a> [<a href="http://arxiv.org/pdf/2010.01549" target="_blank">pdf</a>]

<h2>Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch. (arXiv:2010.01729v4 [cs.CV] UPDATED)</h2>
<h3>Youngeun Kim, Priyadarshini Panda</h3>
<p>Spiking Neural Networks (SNNs) have recently emerged as an alternative to
deep learning owing to sparse, asynchronous and binary event (or spike) driven
processing, that can yield huge energy efficiency benefits on neuromorphic
hardware. However, training high-accuracy and low-latency SNNs from scratch
suffers from non-differentiable nature of a spiking neuron. To address this
training issue in SNNs, we revisit batch normalization and propose a temporal
Batch Normalization Through Time (BNTT) technique. Most prior SNN works till
now have disregarded batch normalization deeming it ineffective for training
temporal SNNs. Different from previous works, our proposed BNTT decouples the
parameters in a BNTT layer along the time axis to capture the temporal dynamics
of spikes. The temporally evolving learnable parameters in BNTT allow a neuron
to control its spike rate through different time-steps, enabling low-latency
and low-energy training from scratch. We conduct experiments on CIFAR-10,
CIFAR-100, Tiny-ImageNet and event-driven DVS-CIFAR10 datasets. BNTT allows us
to train deep SNN architectures from scratch, for the first time, on complex
datasets with just few 25-30 time-steps. We also propose an early exit
algorithm using the distribution of parameters in BNTT to reduce the latency at
inference, that further improves the energy-efficiency.
</p>
<a href="http://arxiv.org/abs/2010.01729" target="_blank">arXiv:2010.01729</a> [<a href="http://arxiv.org/pdf/2010.01729" target="_blank">pdf</a>]

<h2>DEMI: Discriminative Estimator of Mutual Information. (arXiv:2010.01766v2 [cs.LG] UPDATED)</h2>
<h3>Ruizhi Liao, Daniel Moyer, Polina Golland, William M. Wells</h3>
<p>Estimating mutual information between continuous random variables is often
intractable and extremely challenging for high-dimensional data. Recent
progress has leveraged neural networks to optimize variational lower bounds on
mutual information. Although showing promise for this difficult problem, the
variational methods have been theoretically and empirically proven to have
serious statistical limitations: 1) many methods struggle to produce accurate
estimates when the underlying mutual information is either low or high; 2) the
resulting estimators may suffer from high variance. Our approach is based on
training a classifier that provides the probability that a data sample pair is
drawn from the joint distribution rather than from the product of its marginal
distributions. Moreover, we establish a direct connection between mutual
information and the average log odds estimate produced by the classifier on a
test set, leading to a simple and accurate estimator of mutual information. We
show theoretically that our method and other variational approaches are
equivalent when they achieve their optimum, while our method sidesteps the
variational bound. Empirical results demonstrate high accuracy of our approach
and the advantages of our estimator in the context of representation learning.
Our demo is available at https://github.com/RayRuizhiLiao/demi_mi_estimator.
</p>
<a href="http://arxiv.org/abs/2010.01766" target="_blank">arXiv:2010.01766</a> [<a href="http://arxiv.org/pdf/2010.01766" target="_blank">pdf</a>]

<h2>EqCo: Equivalent Rules for Self-supervised Contrastive Learning. (arXiv:2010.01929v2 [cs.CV] UPDATED)</h2>
<h3>Benjin Zhu, Junqiang Huang, Zeming Li, Xiangyu Zhang, Jian Sun</h3>
<p>In this paper, we propose a method, named EqCo (Equivalent Rules for
Contrastive Learning), to make self-supervised learning irrelevant to the
number of negative samples in the contrastive learning framework. Inspired by
the InfoMax principle, we point that the margin term in contrastive loss needs
to be adaptively scaled according to the number of negative pairs in order to
keep steady mutual information bound and gradient magnitude. EqCo bridges the
performance gap among a wide range of negative sample sizes, so that we can use
only a few negative pairs (e.g. 16 per query) to perform self-supervised
contrastive training on large-scale vision datasets like ImageNet, while with
almost no accuracy drop. This is quite a contrast to the widely used large
batch training or memory bank mechanism in current practices. Equipped with
EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on
ImageNet (linear evaluation protocol) while only involves 16 negative pairs per
query instead of 65536, suggesting that large quantities of negative samples
might not be a critical factor in contrastive learning frameworks.
</p>
<a href="http://arxiv.org/abs/2010.01929" target="_blank">arXiv:2010.01929</a> [<a href="http://arxiv.org/pdf/2010.01929" target="_blank">pdf</a>]

<h2>Deformable DETR: Deformable Transformers for End-to-End Object Detection. (arXiv:2010.04159v2 [cs.CV] UPDATED)</h2>
<h3>Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai</h3>
<p>DETR has been recently proposed to eliminate the need for many hand-designed
components in object detection while demonstrating good performance. However,
it suffers from slow convergence and limited feature spatial resolution, due to
the limitation of Transformer attention modules in processing image feature
maps. To mitigate these issues, we proposed Deformable DETR, whose attention
modules only attend to a small set of key sampling points around a reference.
Deformable DETR can achieve better performance than DETR (especially on small
objects) with 10 times less training epochs. Extensive experiments on the COCO
benchmark demonstrate the effectiveness of our approach. Code is released at
https://github.com/fundamentalvision/Deformable-DETR.
</p>
<a href="http://arxiv.org/abs/2010.04159" target="_blank">arXiv:2010.04159</a> [<a href="http://arxiv.org/pdf/2010.04159" target="_blank">pdf</a>]

<h2>Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks. (arXiv:2010.04261v3 [cs.LG] UPDATED)</h2>
<h3>Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, Rong Ge</h3>
<p>Hessian captures important properties of the deep neural network loss
landscape. We observe that eigenvectors and eigenspaces of the layer-wise
Hessian for neural network objective have several interesting structures -- top
eigenspaces for different models have high overlap, and top eigenvectors form
low rank matrices when they are reshaped into the same shape as the
corresponding weight matrix. These structures, as well as the low rank
structure of the Hessian observed in previous studies, can be explained by
approximating the Hessian using Kronecker factorization. Our new understanding
can also explain why some of these structures become weaker when the network is
trained with batch normalization. Finally, we show that the Kronecker
factorization can be combined with PAC-Bayes techniques to get better explicit
generalization bounds.
</p>
<a href="http://arxiv.org/abs/2010.04261" target="_blank">arXiv:2010.04261</a> [<a href="http://arxiv.org/pdf/2010.04261" target="_blank">pdf</a>]

<h2>GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering. (arXiv:2010.04595v2 [cs.CV] UPDATED)</h2>
<h3>Alex Trevithick, Bo Yang</h3>
<p>We present a simple yet powerful implicit neural function that can represent
and render arbitrarily complex 3D scenes in a single network only from 2D
observations. The function models 3D scenes as a general radiance field, which
takes a set of posed 2D images with camera poses and intrinsics as input,
constructs an internal representation for each 3D point of the scene, and
renders the corresponding appearance and geometry of any 3D point viewing from
an arbitrary angle. The key to our approach is to explicitly integrate the
principle of multi-view geometry to obtain the internal representations from
observed 2D views, such that the learned implicit representations empirically
remain multi-view consistent. In addition, we introduce an effective neural
module to learn general features for each pixel in 2D images, allowing the
constructed internal 3D representations to be general as well. Extensive
experiments demonstrate the superiority of our approach.
</p>
<a href="http://arxiv.org/abs/2010.04595" target="_blank">arXiv:2010.04595</a> [<a href="http://arxiv.org/pdf/2010.04595" target="_blank">pdf</a>]

<h2>Maximin Optimization for Binary Regression. (arXiv:2010.05077v3 [cs.LG] UPDATED)</h2>
<h3>Nisan Chiprut, Amir Globerson, Ami Wiesel</h3>
<p>We consider regression problems with binary weights. Such optimization
problems are ubiquitous in quantized learning models and digital communication
systems. A natural approach is to optimize the corresponding Lagrangian using
variants of the gradient ascent-descent method. Such maximin techniques are
still poorly understood even in the concave-convex case. The non-convex binary
constraints may lead to spurious local minima. Interestingly, we prove that
this approach is optimal in linear regression with low noise conditions as well
as robust regression with a small number of outliers. Practically, the method
also performs well in regression with cross entropy loss, as well as non-convex
multi-layer neural networks. Taken together our approach highlights the
potential of saddle-point optimization for learning constrained models.
</p>
<a href="http://arxiv.org/abs/2010.05077" target="_blank">arXiv:2010.05077</a> [<a href="http://arxiv.org/pdf/2010.05077" target="_blank">pdf</a>]

<h2>Data Agnostic RoBERTa-based Natural Language to SQL Query Generation. (arXiv:2010.05243v2 [cs.AI] UPDATED)</h2>
<h3>Debaditya Pal, Harsh Sharma, Kaustubh Chaudhari</h3>
<p>Relational databases are among the most widely used architectures to store
massive amounts of data in the modern world. However, there is a barrier
between these databases and the average user. The user often lacks the
knowledge of a query language such as SQL required to interact with the
database. The NL2SQL task aims at finding deep learning approaches to solve
this problem by converting natural language questions into valid SQL queries.
Given the sensitive nature of some databases and the growing need for data
privacy, we have presented an approach with data privacy at its core. We have
passed RoBERTa embeddings and data-agnostic knowledge vectors into LSTM based
submodels to predict the final query. Although we have not achieved state of
the art results, we have eliminated the need for the table data, right from the
training of the model, and have achieved a test set execution accuracy of
76.7%. By eliminating the table data dependency while training we have created
a model capable of zero shot learning based on the natural language question
and table schema alone.
</p>
<a href="http://arxiv.org/abs/2010.05243" target="_blank">arXiv:2010.05243</a> [<a href="http://arxiv.org/pdf/2010.05243" target="_blank">pdf</a>]

<h2>Measuring Visual Generalization in Continuous Control from Pixels. (arXiv:2010.06740v2 [cs.LG] UPDATED)</h2>
<h3>Jake Grigsby, Yanjun Qi</h3>
<p>Self-supervised learning and data augmentation have significantly reduced the
performance gap between state and image-based reinforcement learning agents in
continuous control tasks. However, it is still unclear whether current
techniques can face a variety of visual conditions required by real-world
environments. We propose a challenging benchmark that tests agents' visual
generalization by adding graphical variety to existing continuous control
domains. Our empirical analysis shows that current methods struggle to
generalize across a diverse set of visual changes, and we examine the specific
factors of variation that make these tasks difficult. We find that data
augmentation techniques outperform self-supervised learning approaches and that
more significant image transformations provide better visual generalization
\footnote{The benchmark and our augmented actor-critic implementation are
open-sourced @ https://github.com/QData/dmc_remastered)
</p>
<a href="http://arxiv.org/abs/2010.06740" target="_blank">arXiv:2010.06740</a> [<a href="http://arxiv.org/pdf/2010.06740" target="_blank">pdf</a>]

<h2>AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. (arXiv:2010.07468v4 [cs.LG] UPDATED)</h2>
<h3>Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, James S. Duncan</h3>
<p>Most popular optimizers for deep learning can be broadly categorized as
adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient
descent (SGD) with momentum). For many models such as convolutional neural
networks (CNNs), adaptive methods typically converge faster but generalize
worse compared to SGD; for complex settings such as generative adversarial
networks (GANs), adaptive methods are typically the default because of their
stability.We propose AdaBelief to simultaneously achieve three goals: fast
convergence as in adaptive methods, good generalization as in SGD, and training
stability. The intuition for AdaBelief is to adapt the stepsize according to
the "belief" in the current gradient direction. Viewing the exponential moving
average (EMA) of the noisy gradient as the prediction of the gradient at the
next time step, if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step; if the observed
gradient is close to the prediction, we trust it and take a large step. We
validate AdaBelief in extensive experiments, showing that it outperforms other
methods with fast convergence and high accuracy on image classification and
language modeling. Specifically, on ImageNet, AdaBelief achieves comparable
accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief
demonstrates high stability and improves the quality of generated samples
compared to a well-tuned Adam optimizer. Code is available at
https://github.com/juntang-zhuang/Adabelief-Optimizer
</p>
<a href="http://arxiv.org/abs/2010.07468" target="_blank">arXiv:2010.07468</a> [<a href="http://arxiv.org/pdf/2010.07468" target="_blank">pdf</a>]

<h2>Distortion-aware Monocular Depth Estimation for Omnidirectional Images. (arXiv:2010.08942v2 [cs.CV] UPDATED)</h2>
<h3>Hong-Xiang Chen, Kunhong Li, Zhiheng Fu, Mengyi Liu, Zonghao Chen, Yulan Guo</h3>
<p>A main challenge for tasks on panorama lies in the distortion of objects
among images. In this work, we propose a Distortion-Aware Monocular
Omnidirectional (DAMO) dense depth estimation network to address this challenge
on indoor panoramas with two steps. First, we introduce a distortion-aware
module to extract calibrated semantic features from omnidirectional images.
Specifically, we exploit deformable convolution to adjust its sampling grids to
geometric variations of distorted objects on panoramas and then utilize a strip
pooling module to sample against horizontal distortion introduced by inverse
gnomonic projection. Second, we further introduce a plug-and-play
spherical-aware weight matrix for our objective function to handle the uneven
distribution of areas projected from a sphere. Experiments on the 360D dataset
show that the proposed method can effectively extract semantic features from
distorted panoramas and alleviate the supervision bias caused by distortion. It
achieves state-of-the-art performance on the 360D dataset with high efficiency.
</p>
<a href="http://arxiv.org/abs/2010.08942" target="_blank">arXiv:2010.08942</a> [<a href="http://arxiv.org/pdf/2010.08942" target="_blank">pdf</a>]

<h2>D2RL: Deep Dense Architectures in Reinforcement Learning. (arXiv:2010.09163v2 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Homanga Bharadhwaj, Aravind Srinivas, Animesh Garg</h3>
<p>While improvements in deep learning architectures have played a crucial role
in improving the state of supervised and unsupervised learning in computer
vision and natural language processing, neural network architecture choices for
reinforcement learning remain relatively under-explored. We take inspiration
from successful architectural choices in computer vision and generative
modelling, and investigate the use of deeper networks and dense connections for
reinforcement learning on a variety of simulated robotic learning benchmark
environments. Our findings reveal that current methods benefit significantly
from dense connections and deeper networks, across a suite of manipulation and
locomotion tasks, for both proprioceptive and image-based observations. We hope
that our results can serve as a strong baseline and further motivate future
research into neural network architectures for reinforcement learning. The
project website with code is at this link
https://sites.google.com/view/d2rl/home.
</p>
<a href="http://arxiv.org/abs/2010.09163" target="_blank">arXiv:2010.09163</a> [<a href="http://arxiv.org/pdf/2010.09163" target="_blank">pdf</a>]

<h2>Autonomous Spot: Long-Range Autonomous Exploration of Extreme Environments with Legged Locomotion. (arXiv:2010.09259v3 [cs.RO] UPDATED)</h2>
<h3>Amanda Bouman, Muhammad Fadhil Ginting, Nikhilesh Alatur, Matteo Palieri, David D. Fan, Thomas Touma, Torkom Pailevanian, Sung-Kyun Kim, Kyohei Otsu, Joel Burdick, Ali-akbar Agha-mohammadi</h3>
<p>This paper serves as one of the first efforts to enable large-scale and
long-duration autonomy using the Boston Dynamics Spot robot. Motivated by
exploring extreme environments, particularly those involved in the DARPA
Subterranean Challenge, this paper pushes the boundaries of the
state-of-practice in enabling legged robotic systems to accomplish real-world
complex missions in relevant scenarios. In particular, we discuss the behaviors
and capabilities which emerge from the integration of the autonomy architecture
NeBula (Networked Belief-aware Perceptual Autonomy) with next-generation
mobility systems. We will discuss the hardware and software challenges, and
solutions in mobility, perception, autonomy, and very briefly, wireless
networking, as well as lessons learned and future directions. We demonstrate
the performance of the proposed solutions on physical systems in real-world
scenarios.
</p>
<a href="http://arxiv.org/abs/2010.09259" target="_blank">arXiv:2010.09259</a> [<a href="http://arxiv.org/pdf/2010.09259" target="_blank">pdf</a>]

<h2>FTBNN: Rethinking Non-linearity for 1-bit CNNs and Going Beyond. (arXiv:2010.09294v3 [cs.CV] UPDATED)</h2>
<h3>Zhuo Su, Linpu Fang, Deke Guo, Duwen Hu, Matti Pietik&#xe4;inen, Li Liu</h3>
<p>Binary neural networks (BNNs), where both weights and activations are
binarized into 1 bit, have been widely studied in recent years due to its great
benefit of highly accelerated computation and substantially reduced memory
footprint that appeal to the development of resource constrained devices. In
contrast to previous methods tending to reduce the quantization error for
training BNN structures, we argue that the binarized convolution process owns
an increasing linearity towards the target of minimizing such error, which in
turn hampers BNN's discriminative ability. In this paper, we re-investigate and
tune proper non-linear modules to fix that contradiction, leading to a strong
baseline which achieves state-of-the-art performance on the large-scale
ImageNet dataset in terms of accuracy and training efficiency. To go further,
we find that the proposed BNN model still has much potential to be compressed
by making a better use of the efficient binary operations, without losing
accuracy. In addition, the limited capacity of the BNN model can also be
increased with the help of group execution. Based on these insights, we are
able to improve the baseline with an additional 4~5% top-1 accuracy gain even
with less computational cost. Our code will be made public at
https://github.com/zhuogege1943/ftbnn.
</p>
<a href="http://arxiv.org/abs/2010.09294" target="_blank">arXiv:2010.09294</a> [<a href="http://arxiv.org/pdf/2010.09294" target="_blank">pdf</a>]

<h2>A combined full-reference image quality assessment method based on convolutional activation maps. (arXiv:2010.09361v2 [cs.CV] UPDATED)</h2>
<h3>Domonkos Varga</h3>
<p>The goal of full-reference image quality assessment (FR-IQA) is to predict
the quality of an image as perceived by human observers with using its
pristine, reference counterpart. In this study, we explore a novel, combined
approach which predicts the perceptual quality of a distorted image by
compiling a feature vector from convolutional activation maps. More
specifically, a reference-distorted image pair is run through a pretrained
convolutional neural network and the activation maps are compared with a
traditional image similarity metric. Subsequently, the resulted feature vector
is mapped onto perceptual quality scores with the help of a trained support
vector regressor. A detailed parameter study is also presented in which the
design choices of the proposed method is reasoned. Furthermore, we study the
relationship between the amount of training images and the prediction
performance. Specifically, it is demonstrated that the proposed method can be
trained with few amount of data to reach high prediction performance. Our best
proposal - ActMapFeat - is compared to the state-of-the-art on six publicly
available benchmark IQA databases, such as KADID-10k, TID2013, TID2008, MDID,
CSIQ, and VCL-FER. Specifically, our method is able to significantly outperform
the state-of-the-art on these benchmark databases.
</p>
<a href="http://arxiv.org/abs/2010.09361" target="_blank">arXiv:2010.09361</a> [<a href="http://arxiv.org/pdf/2010.09361" target="_blank">pdf</a>]

<h2>Cross-Modal Information Maximization for Medical Imaging: CMIM. (arXiv:2010.10593v2 [cs.CV] UPDATED)</h2>
<h3>Tristan Sylvain, Francis Dutil, Tess Berthier, Lisa Di Jorio, Margaux Luck, Devon Hjelm, Yoshua Bengio</h3>
<p>In hospitals, data are siloed to specific information systems that make the
same information available under different modalities such as the different
medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound,
etc.) and their associated radiology reports. This offers unique opportunities
to obtain and use at train-time those multiple views of the same information
that might not always be available at test-time.

In this paper, we propose an innovative framework that makes the most of
available data by learning good representations of a multi-modal input that are
resilient to modality dropping at test-time, using recent advances in mutual
information maximization. By maximizing cross-modal information at train time,
we are able to outperform several state-of-the-art baselines in two different
settings, medical image classification, and segmentation. In particular, our
method is shown to have a strong impact on the inference-time performance of
weaker modalities.
</p>
<a href="http://arxiv.org/abs/2010.10593" target="_blank">arXiv:2010.10593</a> [<a href="http://arxiv.org/pdf/2010.10593" target="_blank">pdf</a>]

<h2>Deep Learning Frameworks for Pavement Distress Classification: A Comparative Analysis. (arXiv:2010.10681v2 [cs.CV] UPDATED)</h2>
<h3>Vishal Mandal, Abdul Rashid Mussah, Yaw Adu-Gyamfi</h3>
<p>Automatic detection and classification of pavement distresses is critical in
timely maintaining and rehabilitating pavement surfaces. With the evolution of
deep learning and high performance computing, the feasibility of vision-based
pavement defect assessments has significantly improved. In this study, the
authors deploy state-of-the-art deep learning algorithms based on different
network backbones to detect and characterize pavement distresses. The influence
of different backbone models such as CSPDarknet53, Hourglass-104 and
EfficientNet were studied to evaluate their classification performance. The
models were trained using 21,041 images captured across urban and rural streets
of Japan, Czech Republic and India. Finally, the models were assessed based on
their ability to predict and classify distresses, and tested using F1 score
obtained from the statistical precision and recall values. The best performing
model achieved an F1 score of 0.58 and 0.57 on two test datasets released by
the IEEE Global Road Damage Detection Challenge. The source code including the
trained models are made available at [1].
</p>
<a href="http://arxiv.org/abs/2010.10681" target="_blank">arXiv:2010.10681</a> [<a href="http://arxiv.org/pdf/2010.10681" target="_blank">pdf</a>]

<h2>Decentralized Deep Learning using Momentum-Accelerated Consensus. (arXiv:2010.11166v2 [cs.LG] UPDATED)</h2>
<h3>Aditya Balu, Zhanhong Jiang, Sin Yong Tan, Chinmay Hedge, Young M Lee, Soumik Sarkar</h3>
<p>We consider the problem of decentralized deep learning where multiple agents
collaborate to learn from a distributed dataset. While there exist several
decentralized deep learning approaches, the majority consider a central
parameter-server topology for aggregating the model parameters from the agents.
However, such a topology may be inapplicable in networked systems such as
ad-hoc mobile networks, field robotics, and power network systems where direct
communication with the central parameter server may be inefficient. In this
context, we propose and analyze a novel decentralized deep learning algorithm
where the agents interact over a fixed communication topology (without a
central server). Our algorithm is based on the heavy-ball acceleration method
used in gradient-based optimization. We propose a novel consensus protocol
where each agent shares with its neighbors its model parameters as well as
gradient-momentum values during the optimization process. We consider both
strongly convex and non-convex objective functions and theoretically analyze
our algorithm's performance. We present several empirical comparisons with
competing decentralized learning methods to demonstrate the efficacy of our
approach under different communication topologies.
</p>
<a href="http://arxiv.org/abs/2010.11166" target="_blank">arXiv:2010.11166</a> [<a href="http://arxiv.org/pdf/2010.11166" target="_blank">pdf</a>]

<h2>Versatile Verification of Tree Ensembles. (arXiv:2010.13880v2 [cs.LG] UPDATED)</h2>
<h3>Laurens Devos, Wannes Meert, Jesse Davis</h3>
<p>Machine learned models often must abide by certain requirements (e.g.,
fairness or legal). This has spurred interested in developing approaches that
can provably verify whether a model satisfies certain properties. This paper
introduces a generic algorithm called Veritas that enables tackling multiple
different verification tasks for tree ensemble models like random forests (RFs)
and gradient boosting decision trees (GBDTs). This generality contrasts with
previous work, which has focused exclusively on either adversarial example
generation or robustness checking. Veritas formulates the verification task as
a generic optimization problem and introduces a novel search space
representation. Veritas offers two key advantages. First, it provides anytime
lower and upper bounds when the optimization problem cannot be solved exactly.
In contrast, many existing methods have focused on exact solutions and are thus
limited by the verification problem being NP-complete. Second, Veritas produces
full (bounded suboptimal) solutions that can be used to generate concrete
examples. We experimentally show that Veritas outperforms the previous state of
the art by (a) generating exact solutions more frequently, (b) producing
tighter bounds when (a) is not possible, and (c) offering orders of magnitude
speed ups. Subsequently, Veritas enables tackling more and larger real-world
verification scenarios.
</p>
<a href="http://arxiv.org/abs/2010.13880" target="_blank">arXiv:2010.13880</a> [<a href="http://arxiv.org/pdf/2010.13880" target="_blank">pdf</a>]

<h2>Sample-Optimal and Efficient Learning of Tree Ising models. (arXiv:2010.14864v2 [cs.LG] UPDATED)</h2>
<h3>Constantinos Daskalakis, Qinxuan Pan</h3>
<p>We show that $n$-variable tree-structured Ising models can be learned
computationally-efficiently to within total variation distance $\epsilon$ from
an optimal $O(n \ln n/\epsilon^2)$ samples, where $O(\cdot)$ hides an absolute
constant which, importantly, does not depend on the model being learned -
neither its tree nor the magnitude of its edge strengths, on which we place no
assumptions. Our guarantees hold, in fact, for the celebrated Chow-Liu [1968]
algorithm, using the plug-in estimator for estimating mutual information. While
this (or any other) algorithm may fail to identify the structure of the
underlying model correctly from a finite sample, we show that it will still
learn a tree-structured model that is $\epsilon$-close to the true one in total
variation distance, a guarantee called "proper learning."

Our guarantees do not follow from known results for the Chow-Liu algorithm
and the ensuing literature on learning graphical models, including a recent
renaissance of algorithms on this learning challenge, which only yield
asymptotic consistency results, or sample-inefficient and/or time-inefficient
algorithms, unless further assumptions are placed on the graphical model, such
as bounds on the "strengths" of the model's edges/hyperedges. While we
establish guarantees for a widely known and simple algorithm, the analysis that
this algorithm succeeds and is sample-optimal is quite complex, requiring a
hierarchical classification of the edges into layers with different
reconstruction guarantees, depending on their strength, combined with delicate
uses of the subadditivity of the squared Hellinger distance over graphical
models to control the error accumulation.
</p>
<a href="http://arxiv.org/abs/2010.14864" target="_blank">arXiv:2010.14864</a> [<a href="http://arxiv.org/pdf/2010.14864" target="_blank">pdf</a>]

<h2>Training Generative Adversarial Networks by Solving Ordinary Differential Equations. (arXiv:2010.15040v2 [stat.ML] UPDATED)</h2>
<h3>Chongli Qin, Yan Wu, Jost Tobias Springenberg, Andrew Brock, Jeff Donahue, Timothy P. Lillicrap, Pushmeet Kohli</h3>
<p>The instability of Generative Adversarial Network (GAN) training has
frequently been attributed to gradient descent. Consequently, recent methods
have aimed to tailor the models and training procedures to stabilise the
discrete updates. In contrast, we study the continuous-time dynamics induced by
GAN training. Both theory and toy experiments suggest that these dynamics are
in fact surprisingly stable. From this perspective, we hypothesise that
instabilities in training GANs arise from the integration error in discretising
the continuous dynamics. We experimentally verify that well-known ODE solvers
(such as Runge-Kutta) can stabilise training - when combined with a regulariser
that controls the integration error. Our approach represents a radical
departure from previous methods which typically use adaptive optimisation and
stabilisation techniques that constrain the functional space (e.g. Spectral
Normalisation). Evaluation on CIFAR-10 and ImageNet shows that our method
outperforms several strong baselines, demonstrating its efficacy.
</p>
<a href="http://arxiv.org/abs/2010.15040" target="_blank">arXiv:2010.15040</a> [<a href="http://arxiv.org/pdf/2010.15040" target="_blank">pdf</a>]

<h2>General Data Analytics with Applications to Visual Information Analysis: A Provable Backward-Compatible Semisimple Paradigm over T-Algebra. (arXiv:2011.00307v4 [cs.CV] UPDATED)</h2>
<h3>Liang Liao, Stephen John Maybank</h3>
<p>We consider a novel backward-compatible paradigm of general data analytics
over a recently-reported semisimple algebra (called t-algebra). We study the
abstract algebraic framework over the t-algebra by representing the elements of
t-algebra by fix-sized multi-way arrays of complex numbers and the algebraic
structure over the t-algebra by a collection of direct-product constituents.
Over the t-algebra, many algorithms, if not all, are generalized in a
straightforward manner using this new semisimple paradigm. To demonstrate the
new paradigm's performance and its backward-compatibility, we generalize some
canonical algorithms for visual pattern analysis. Experiments on public
datasets show that the generalized algorithms compare favorably with their
canonical counterparts.
</p>
<a href="http://arxiv.org/abs/2011.00307" target="_blank">arXiv:2011.00307</a> [<a href="http://arxiv.org/pdf/2011.00307" target="_blank">pdf</a>]

<h2>Adversarial training for predictive tasks: theoretical analysis and limitations in the deterministic case. (arXiv:2011.00835v3 [stat.ML] UPDATED)</h2>
<h3>Thibault Lesieur, J&#xe9;r&#xe9;mie Messud, Issa Hammoud, Hanyuan Peng, C&#xe9;line Lacombe, Paulien Jeunesse</h3>
<p>To train a deep neural network to mimic the outcomes of processing sequences,
a version of Conditional Generalized Adversarial Network (CGAN) can be used. It
has been observed by others that CGAN can help to improve the results even for
deterministic sequences, where only one output is associated with the
processing of a given input. Surprisingly, our CGAN-based tests on
deterministic geophysical processing sequences did not produce a real
improvement compared to the use of an $L_p$ loss; we here propose a first
theoretical explanation why. Our analysis goes from the non-deterministic case
to the deterministic one. It led us to develop an adversarial way to train a
content loss that gave better results on our data.
</p>
<a href="http://arxiv.org/abs/2011.00835" target="_blank">arXiv:2011.00835</a> [<a href="http://arxiv.org/pdf/2011.00835" target="_blank">pdf</a>]

<h2>Robust building footprint extraction from big multi-sensor data using deep competition network. (arXiv:2011.02879v3 [cs.CV] UPDATED)</h2>
<h3>Mehdi Khoshboresh-Masouleh, Mohammad R. Saradjian</h3>
<p>Building footprint extraction (BFE) from multi-sensor data such as optical
images and light detection and ranging (LiDAR) point clouds is widely used in
various fields of remote sensing applications. However, it is still challenging
research topic due to relatively inefficient building extraction techniques
from variety of complex scenes in multi-sensor data. In this study, we develop
and evaluate a deep competition network (DCN) that fuses very high spatial
resolution optical remote sensing images with LiDAR data for robust BFE. DCN is
a deep superpixelwise convolutional encoder-decoder architecture using the
encoder vector quantization with classified structure. DCN consists of five
encoding-decoding blocks with convolutional weights for robust binary
representation (superpixel) learning. DCN is trained and tested in a big
multi-sensor dataset obtained from the state of Indiana in the United States
with multiple building scenes. Comparison results of the accuracy assessment
showed that DCN has competitive BFE performance in comparison with other deep
semantic binary segmentation architectures. Therefore, we conclude that the
proposed model is a suitable solution to the robust BFE from big multi-sensor
data.
</p>
<a href="http://arxiv.org/abs/2011.02879" target="_blank">arXiv:2011.02879</a> [<a href="http://arxiv.org/pdf/2011.02879" target="_blank">pdf</a>]

<h2>Latent Neural Differential Equations for Video Generation. (arXiv:2011.03864v2 [cs.CV] UPDATED)</h2>
<h3>Cade Gordon, Natalie Parde</h3>
<p>Generative Adversarial Networks have recently shown promise for video
generation, building off of the success of image generation while also
addressing a new challenge: time. Although time was analyzed in some early
work, the literature has not adequately grown with temporal modeling
developments. We propose studying the effects of Neural Differential Equations
to model the temporal dynamics of video generation. The paradigm of Neural
Differential Equations presents many theoretical strengths including the first
continuous representation of time within video generation. In order to address
the effects of Neural Differential Equations, we will investigate how changes
in temporal models affect generated video quality.
</p>
<a href="http://arxiv.org/abs/2011.03864" target="_blank">arXiv:2011.03864</a> [<a href="http://arxiv.org/pdf/2011.03864" target="_blank">pdf</a>]

<h2>End-to-end Lane Shape Prediction with Transformers. (arXiv:2011.04233v2 [cs.CV] UPDATED)</h2>
<h3>Ruijin Liu, Zejian Yuan, Tie Liu, Zhiliang Xiong</h3>
<p>Lane detection, the process of identifying lane markings as approximated
curves, is widely used for lane departure warning and adaptive cruise control
in autonomous vehicles. The popular pipeline that solves it in two steps --
feature extraction plus post-processing, while useful, is too inefficient and
flawed in learning the global context and lanes' long and thin structures. To
tackle these issues, we propose an end-to-end method that directly outputs
parameters of a lane shape model, using a network built with a transformer to
learn richer structures and context. The lane shape model is formulated based
on road structures and camera pose, providing physical interpretation for
parameters of network output. The transformer models non-local interactions
with a self-attention mechanism to capture slender structures and global
context. The proposed method is validated on the TuSimple benchmark and shows
state-of-the-art accuracy with the most lightweight model size and fastest
speed. Additionally, our method shows excellent adaptability to a challenging
self-collected lane detection dataset, showing its powerful deployment
potential in real applications. Codes are available at
https://github.com/liuruijin17/LSTR.
</p>
<a href="http://arxiv.org/abs/2011.04233" target="_blank">arXiv:2011.04233</a> [<a href="http://arxiv.org/pdf/2011.04233" target="_blank">pdf</a>]

<h2>Duality-Gated Mutual Condition Network for RGBT Tracking. (arXiv:2011.07188v2 [cs.CV] UPDATED)</h2>
<h3>Andong Lu, Cun Qian, Chenglong Li, Jin Tang, Liang Wang</h3>
<p>Low-quality modalities contain not only a lot of noisy information but also
some discriminative features in RGBT tracking. However, the potentials of
low-quality modalities are not well explored in existing RGBT tracking
algorithms. In this work, we propose a novel duality-gated mutual condition
network to fully exploit the discriminative information of all modalities while
suppressing the effects of data noise. In specific, we design a mutual
condition module, which takes the discriminative information of a modality as
the condition to guide feature learning of target appearance in another
modality. Such module can effectively enhance target representations of all
modalities even in the presence of low-quality modalities. To improve the
quality of conditions and further reduce data noise, we propose a duality-gated
mechanism and integrate it into the mutual condition module. To deal with the
tracking failure caused by sudden camera motion, which often occurs in RGBT
tracking, we design a resampling strategy based on optical flow algorithms. It
does not increase much computational cost since we perform optical flow
calculation only when the model prediction is unreliable and then execute
resampling when the sudden camera motion is detected. Extensive experiments on
four RGBT tracking benchmark datasets show that our method performs favorably
against the state-of-the-art tracking algorithms
</p>
<a href="http://arxiv.org/abs/2011.07188" target="_blank">arXiv:2011.07188</a> [<a href="http://arxiv.org/pdf/2011.07188" target="_blank">pdf</a>]

<h2>Quantifying Sources of Uncertainty in Deep Learning-Based Image Reconstruction. (arXiv:2011.08413v2 [cs.CV] UPDATED)</h2>
<h3>Riccardo Barbano, &#x17d;eljko Kereta, Chen Zhang, Andreas Hauptmann, Simon Arridge, Bangti Jin</h3>
<p>Image reconstruction methods based on deep neural networks have shown
outstanding performance, equalling or exceeding the state-of-the-art results of
conventional approaches, but often do not provide uncertainty information about
the reconstruction. In this work we propose a scalable and efficient framework
to simultaneously quantify aleatoric and epistemic uncertainties in learned
iterative image reconstruction. We build on a Bayesian deep gradient descent
method for quantifying epistemic uncertainty, and incorporate the
heteroscedastic variance of the noise to account for the aleatoric uncertainty.
We show that our method exhibits competitive performance against conventional
benchmarks for computed tomography with both sparse view and limited angle
data. The estimated uncertainty captures the variability in the
reconstructions, caused by the restricted measurement model, and by missing
information, due to the limited angle geometry.
</p>
<a href="http://arxiv.org/abs/2011.08413" target="_blank">arXiv:2011.08413</a> [<a href="http://arxiv.org/pdf/2011.08413" target="_blank">pdf</a>]

<h2>Redesigning the classification layer by randomizing the class representation vectors. (arXiv:2011.08704v2 [cs.LG] UPDATED)</h2>
<h3>Gabi Shalev, Gal-Lev Shalev, Joseph Keshet</h3>
<p>Neural image classification models typically consist of two components. The
first is an image encoder, which is responsible for encoding a given raw image
into a representative vector. The second is the classification component, which
is often implemented by projecting the representative vector onto target class
vectors. The target class vectors, along with the rest of the model parameters,
are estimated so as to minimize the loss function. In this paper, we analyze
how simple design choices for the classification layer affect the learning
dynamics. We show that the standard cross-entropy training implicitly captures
visual similarities between different classes, which might deteriorate accuracy
or even prevents some models from converging. We propose to draw the class
vectors randomly and set them as fixed during training, thus invalidating the
visual similarities encoded in these vectors. We analyze the effects of keeping
the class vectors fixed and show that it can increase the inter-class
separability, intra-class compactness, and the overall model accuracy, while
maintaining the robustness to image corruptions and the generalization of the
learned concepts.
</p>
<a href="http://arxiv.org/abs/2011.08704" target="_blank">arXiv:2011.08704</a> [<a href="http://arxiv.org/pdf/2011.08704" target="_blank">pdf</a>]

<h2>Fault-Aware Robust Control via Adversarial Reinforcement Learning. (arXiv:2011.08728v2 [cs.RO] UPDATED)</h2>
<h3>Fan Yang, Chao Yang, Di Guo, Huaping Liu, Fuchun Sun</h3>
<p>Robots have limited adaptation ability compared to humans and animals in the
case of damage. However, robot damages are prevalent in real-world
applications, especially for robots deployed in extreme environments. The
fragility of robots greatly limits their widespread application. We propose an
adversarial reinforcement learning framework, which significantly increases
robot robustness over joint damage cases in both manipulation tasks and
locomotion tasks. The agent is trained iteratively under the joint damage cases
where it has poor performance. We validate our algorithm on a three-fingered
robot hand and a quadruped robot. Our algorithm can be trained only in
simulation and directly deployed on a real robot without any fine-tuning. It
also demonstrates exceeding success rates over arbitrary joint damage cases.
</p>
<a href="http://arxiv.org/abs/2011.08728" target="_blank">arXiv:2011.08728</a> [<a href="http://arxiv.org/pdf/2011.08728" target="_blank">pdf</a>]

<h2>FROST: Faster and more Robust One-shot Semi-supervised Training. (arXiv:2011.09471v3 [cs.LG] UPDATED)</h2>
<h3>Helena E. Liu, Leslie N. Smith</h3>
<p>Recent advances in one-shot semi-supervised learning have lowered the barrier
for deep learning of new applications. However, the state-of-the-art for
semi-supervised learning is slow to train and the performance is sensitive to
the choices of the labeled data and hyper-parameter values. In this paper, we
present a one-shot semi-supervised learning method that trains up to an order
of magnitude faster and is more robust than state-of-the-art methods.
Specifically, we show that by combining semi-supervised learning with a
one-stage, single network version of self-training, our FROST methodology
trains faster and is more robust to choices for the labeled samples and changes
in hyper-parameters. Our experiments demonstrate FROST's capability to perform
well when the composition of the unlabeled data is unknown; that is when the
unlabeled data contain unequal numbers of each class and can contain
out-of-distribution examples that don't belong to any of the training classes.
High performance, speed of training, and insensitivity to hyper-parameters make
FROST the most practical method for one-shot semi-supervised training. Our code
is available at https://github.com/HelenaELiu/FROST.
</p>
<a href="http://arxiv.org/abs/2011.09471" target="_blank">arXiv:2011.09471</a> [<a href="http://arxiv.org/pdf/2011.09471" target="_blank">pdf</a>]

<h2>Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v4 [cs.CV] UPDATED)</h2>
<h3>Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to remote sensing, where unlabeled data is often
abundant but labeled data is scarce. We first show that due to their different
characteristics, a non-trivial gap persists between contrastive and supervised
learning on standard benchmarks. To close the gap, we propose novel training
methods that exploit the spatiotemporal structure of remote sensing data. We
leverage spatially aligned images over time to construct temporal positive
pairs in contrastive learning and geo-location to design pre-text tasks. Our
experiments show that our proposed method closes the gap between contrastive
and supervised learning on image classification, object detection and semantic
segmentation for remote sensing and other geo-tagged image datasets.
</p>
<a href="http://arxiv.org/abs/2011.09980" target="_blank">arXiv:2011.09980</a> [<a href="http://arxiv.org/pdf/2011.09980" target="_blank">pdf</a>]

<h2>NeuralAnnot: Neural Annotator for in-the-wild Expressive 3D Human Pose and Mesh Training Sets. (arXiv:2011.11232v2 [cs.CV] UPDATED)</h2>
<h3>Gyeongsik Moon, Kyoung Mu Lee</h3>
<p>Recovering expressive 3D human pose and mesh from in-the-wild images is
greatly challenging due to the absence of the training data. Several
optimization-based methods have been used to obtain pseudo-groundtruth (GT) 3D
poses and meshes from GT 2D poses. However, they often produce bad ones with
long running time because their frameworks are optimized on each sample only
using 2D supervisions in a sequential way. To overcome the limitations, we
present NeuralAnnot, a neural annotator that learns to construct in-the-wild
expressive 3D human pose and mesh training sets. Our NeuralAnnot is trained on
a large number of samples by 2D supervisions from a target in-the-wild dataset
and 3D supervisions from auxiliary datasets with GT 3D poses in a parallel way.
We show that our NeuralAnnot produces far better 3D pseudo-GTs with much
shorter running time than the optimization-based methods, and the newly
obtained training set brings great performance gain. The newly obtained
training sets and codes will be publicly available.
</p>
<a href="http://arxiv.org/abs/2011.11232" target="_blank">arXiv:2011.11232</a> [<a href="http://arxiv.org/pdf/2011.11232" target="_blank">pdf</a>]

<h2>Pose2Pose: 3D Positional Pose-Guided 3D Rotational Pose Prediction for Expressive 3D Human Pose and Mesh Estimation. (arXiv:2011.11534v2 [cs.CV] UPDATED)</h2>
<h3>Gyeongsik Moon, Kyoung Mu Lee</h3>
<p>Previous 3D human pose and mesh estimation methods mostly rely on only global
image feature to predict 3D rotations of human joints (i.e., 3D rotational
pose) from an input image. However, local features on the position of human
joints (i.e., positional pose) can provide joint-specific information, which is
essential to understand human articulation. To effectively utilize both local
and global features, we present Pose2Pose, a 3D positional pose-guided 3D
rotational pose prediction network, along with a positional pose-guided pooling
and joint-specific graph convolution. The positional pose-guided pooling
extracts useful joint-specific local and global features. Also, the
joint-specific graph convolution effectively processes the joint-specific
features by learning joint-specific characteristics and different relationships
between different joints. We use Pose2Pose for expressive 3D human pose and
mesh estimation and show that it outperforms all previous part-specific and
expressive methods by a large margin. The codes will be publicly available.
</p>
<a href="http://arxiv.org/abs/2011.11534" target="_blank">arXiv:2011.11534</a> [<a href="http://arxiv.org/pdf/2011.11534" target="_blank">pdf</a>]

<h2>Unsupervised Discovery of Disentangled Manifolds in GANs. (arXiv:2011.11842v2 [cs.CV] UPDATED)</h2>
<h3>Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang</h3>
<p>As recent generative models can generate photo-realistic images, people seek
to understand the mechanism behind the generation process. Interpretable
generation process is beneficial to various image editing applications. In this
work, we propose a framework to discover interpretable directions in the latent
space given arbitrary pre-trained generative adversarial networks. We propose
to learn the transformation from prior one-hot vectors representing different
attributes to the latent space used by pre-trained models. Furthermore, we
apply a centroid loss function to improve consistency and smoothness while
traversing through different directions. We demonstrate the efficacy of the
proposed framework on a wide range of datasets. The discovered direction
vectors are shown to be visually corresponding to various distinct attributes
and thus enable attribute editing.
</p>
<a href="http://arxiv.org/abs/2011.11842" target="_blank">arXiv:2011.11842</a> [<a href="http://arxiv.org/pdf/2011.11842" target="_blank">pdf</a>]

<h2>Is a Green Screen Really Necessary for Real-Time Portrait Matting?. (arXiv:2011.11961v2 [cs.CV] UPDATED)</h2>
<h3>Zhanghan Ke, Kaican Li, Yurou Zhou, Qiuhua Wu, Xiangyu Mao, Qiong Yan, Rynson W.H. Lau</h3>
<p>For portrait matting without the green screen, existing works either require
auxiliary inputs that are costly to obtain or use multiple models that are
computationally expensive. Consequently, they are unavailable in real-time
applications. In contrast, we present a light-weight matting objective
decomposition network (MODNet), which can process portrait matting from a
single input image in real time. The design of MODNet benefits from optimizing
a series of correlated sub-objectives simultaneously via explicit constraints.
Moreover, since trimap-free methods usually suffer from the domain shift
problem in practice, we introduce (1) a self-supervised strategy based on
sub-objectives consistency to adapt MODNet to real-world data and (2) a
one-frame delay trick to smooth the results when applying MODNet to portrait
video sequence.

MODNet is easy to be trained in an end-to-end style. It is much faster than
contemporaneous matting methods and runs at 63 frames per second. On a
carefully designed portrait matting benchmark newly proposed in this work,
MODNet greatly outperforms prior trimap-free methods. More importantly, our
method achieves remarkable results in daily photos and videos. Now, do you
really need a green screen for real-time portrait matting?
</p>
<a href="http://arxiv.org/abs/2011.11961" target="_blank">arXiv:2011.11961</a> [<a href="http://arxiv.org/pdf/2011.11961" target="_blank">pdf</a>]

<h2>RIN: Textured Human Model Recovery and Imitation with a Single Image. (arXiv:2011.12024v2 [cs.CV] UPDATED)</h2>
<h3>Haoxi Ran, Guangfu Wang, Li Lu</h3>
<p>Human imitation has become topical recently, driven by GAN's ability to
disentangle human pose and body content. However, the latest methods hardly
focus on 3D information, and to avoid self-occlusion, a massive amount of input
images are needed. In this paper, we propose RIN, a novel volume-based
framework for reconstructing a textured 3D model from a single picture and
imitating a subject with the generated model. Specifically, to estimate most of
the human texture, we propose a U-Net-like front-to-back translation network.
With both front and back images input, the textured volume recovery module
allows us to color a volumetric human. A sequence of 3D poses then guides the
colored volume via Flowable Disentangle Networks as a volume-to-volume
translation task. To project volumes to a 2D plane during training, we design a
differentiable depth-aware renderer. Our experiments demonstrate that our
volume-based model is adequate for human imitation, and the back view can be
estimated reliably using our network. While prior works based on either 2D pose
or semantic map often fail for the unstable appearance of a human, our
framework can still produce concrete results, which are competitive to those
imagined from multi-view input.
</p>
<a href="http://arxiv.org/abs/2011.12024" target="_blank">arXiv:2011.12024</a> [<a href="http://arxiv.org/pdf/2011.12024" target="_blank">pdf</a>]

<h2>MetaGater: Fast Learning of Conditional Channel Gated Networks via Federated Meta-Learning. (arXiv:2011.12511v2 [cs.LG] UPDATED)</h2>
<h3>Sen Lin, Li Yang, Zhezhi He, Deliang Fan, Junshan Zhang</h3>
<p>While deep learning has achieved phenomenal successes in many AI
applications, its enormous model size and intensive computation requirements
pose a formidable challenge to the deployment in resource-limited nodes. There
has recently been an increasing interest in computationally-efficient learning
methods, e.g., quantization, pruning and channel gating. However, most existing
techniques cannot adapt to different tasks quickly. In this work, we advocate a
holistic approach to jointly train the backbone network and the channel gating
which enables dynamical selection of a subset of filters for more efficient
local computation given the data input. Particularly, we develop a federated
meta-learning approach to jointly learn good meta-initializations for both
backbone networks and gating modules, by making use of the model similarity
across learning tasks on different nodes. In this way, the learnt meta-gating
module effectively captures the important filters of a good meta-backbone
network, based on which a task-specific conditional channel gated network can
be quickly adapted, i.e., through one-step gradient descent, from the
meta-initializations in a two-stage procedure using new samples of that task.
The convergence of the proposed federated meta-learning algorithm is
established under mild conditions. Experimental results corroborate the
effectiveness of our method in comparison to related work.
</p>
<a href="http://arxiv.org/abs/2011.12511" target="_blank">arXiv:2011.12511</a> [<a href="http://arxiv.org/pdf/2011.12511" target="_blank">pdf</a>]

<h2>TLeague: A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning. (arXiv:2011.12895v2 [cs.LG] UPDATED)</h2>
<h3>Peng Sun, Jiechao Xiong, Lei Han, Xinghai Sun, Shuxing Li, Jiawei Xu, Meng Fang, Zhengyou Zhang</h3>
<p>Competitive Self-Play (CSP) based Multi-Agent Reinforcement Learning (MARL)
has shown phenomenal breakthroughs recently. Strong AIs are achieved for
several benchmarks, including Dota 2, Glory of Kings, Quake III, StarCraft II,
to name a few. Despite the success, the MARL training is extremely data
thirsty, requiring typically billions of (if not trillions of) frames be seen
from the environment during training in order for learning a high performance
agent. This poses non-trivial difficulties for researchers or engineers and
prevents the application of MARL to a broader range of real-world problems. To
address this issue, in this manuscript we describe a framework, referred to as
TLeague, that aims at large-scale training and implements several main-stream
CSP-MARL algorithms. The training can be deployed in either a single machine or
a cluster of hybrid machines (CPUs and GPUs), where the standard Kubernetes is
supported in a cloud native manner. TLeague achieves a high throughput and a
reasonable scale-up when performing distributed training. Thanks to the modular
design, it is also easy to extend for solving other multi-agent problems or
implementing and verifying MARL algorithms. We present experiments over
StarCraft II, ViZDoom and Pommerman to show the efficiency and effectiveness of
TLeague. The code is open-sourced and available at
https://github.com/tencent-ailab/tleague_projpage
</p>
<a href="http://arxiv.org/abs/2011.12895" target="_blank">arXiv:2011.12895</a> [<a href="http://arxiv.org/pdf/2011.12895" target="_blank">pdf</a>]

<h2>How to train your conditional GAN: An approach using geometrically structured latent manifolds. (arXiv:2011.13055v2 [cs.CV] UPDATED)</h2>
<h3>Sameera Ramasinghe, Moshiur Farazi, Salman Khan, Nick Barnes, Stephen Gould</h3>
<p>Conditional generative modeling typically requires capturing one-to-many
mappings between the inputs and outputs. However, vanilla conditional GANs
(cGAN) tend to ignore the variations of the latent seeds which results in
mode-collapse. As a solution, recent works have moved towards comparatively
expensive models for generating diverse outputs in a conditional setting. In
this paper, we argue that the limited diversity of the vanilla cGANs is not due
to a lack of capacity, but a result of non-optimal training schemes. We tackle
this problem from a geometrical perspective and propose a novel training
mechanism that increases both the diversity and the visual quality of the
vanilla cGAN. The proposed solution does not demand architectural modifications
and paves the way for more efficient architectures that target conditional
generation in multi-modal spaces. We validate the efficacy of our model against
a diverse set of tasks and show that the proposed solution is generic and
effective across multiple datasets.
</p>
<a href="http://arxiv.org/abs/2011.13055" target="_blank">arXiv:2011.13055</a> [<a href="http://arxiv.org/pdf/2011.13055" target="_blank">pdf</a>]

<h2>Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect. (arXiv:2011.13375v2 [cs.CV] UPDATED)</h2>
<h3>Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee, Earlence Fernandes</h3>
<p>Physical adversarial examples for camera-based computer vision have so far
been achieved through visible artifacts -- a sticker on a Stop sign, colorful
borders around eyeglasses or a 3D printed object with a colorful texture. An
implicit assumption here is that the perturbations must be visible so that a
camera can sense them. By contrast, we contribute a procedure to generate, for
the first time, physical adversarial examples that are invisible to human eyes.
Rather than modifying the victim object with visible artifacts, we modify light
that illuminates the object. We demonstrate how an attacker can craft a
modulated light signal that adversarially illuminates a scene and causes
targeted misclassifications on a state-of-the-art ImageNet deep learning model.
Concretely, we exploit the radiometric rolling shutter effect in commodity
cameras to create precise striping patterns that appear on images. To human
eyes, it appears like the object is illuminated, but the camera creates an
image with stripes that will cause ML models to output the attacker-desired
classification. We conduct a range of simulation and physical experiments with
LEDs, demonstrating targeted attack rates up to 84%.
</p>
<a href="http://arxiv.org/abs/2011.13375" target="_blank">arXiv:2011.13375</a> [<a href="http://arxiv.org/pdf/2011.13375" target="_blank">pdf</a>]

<h2>3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer. (arXiv:2011.13388v2 [cs.CV] UPDATED)</h2>
<h3>Mattia Segu, Margarita Grinvald, Roland Siegwart, Federico Tombari</h3>
<p>Transferring the style from one image onto another is a popular and widely
studied task in computer vision. Yet, learning-based style transfer in the 3D
setting remains a largely unexplored problem. To our knowledge, we propose the
first learning-based generative approach for style transfer between 3D objects.
Our method allows to combine the content and style of a source and target 3D
model to generate a novel shape that resembles in style the target while
retaining the source content. The proposed framework can synthesize new 3D
shapes both in the form of point clouds and meshes. Furthermore, we extend our
technique to implicitly learn the underlying multimodal style distribution of
the individual category domains. By sampling style codes from the learned
distributions, we increase the variety of styles that our model can confer to a
given reference object. Experimental results validate the effectiveness of the
proposed 3D style transfer method on a number of benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.13388" target="_blank">arXiv:2011.13388</a> [<a href="http://arxiv.org/pdf/2011.13388" target="_blank">pdf</a>]

<h2>Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence. (arXiv:2011.13650v2 [cs.CV] UPDATED)</h2>
<h3>Yu Deng, Jiaolong Yang, Xin Tong</h3>
<p>We propose a novel Deformed Implicit Field (DIF) representation for modeling
3D shapes of a category and generating dense correspondences among shapes. With
DIF, a 3D shape is represented by a template implicit field shared across the
category, together with a 3D deformation field and a correction field dedicated
for each shape instance. Shape correspondences can be easily established using
their deformation fields. Our neural network, dubbed DIF-Net, jointly learns a
shape latent space and these fields for 3D objects belonging to a category
without using any correspondence or part label. The learned DIF-Net can also
provides reliable correspondence uncertainty measurement reflecting shape
structure discrepancy. Experiments show that DIF-Net not only produces
high-fidelity 3D shapes but also builds high-quality dense correspondences
across different shapes. We also demonstrate several applications such as
texture transfer and shape editing, where our method achieves compelling
results that cannot be achieved by previous methods.
</p>
<a href="http://arxiv.org/abs/2011.13650" target="_blank">arXiv:2011.13650</a> [<a href="http://arxiv.org/pdf/2011.13650" target="_blank">pdf</a>]

<h2>SFTrack++: A Fast Learnable Spectral Segmentation Approach for Space-Time Consistent Tracking. (arXiv:2011.13843v2 [cs.CV] UPDATED)</h2>
<h3>Elena Burceanu</h3>
<p>We propose an object tracking method, SFTrack++, that smoothly learns to
preserve the tracked object consistency over space and time dimensions by
taking a spectral clustering approach over the graph of pixels from the video,
using a fast 3D filtering formulation for finding the principal eigenvector of
this graph's adjacency matrix. To better capture complex aspects of the tracked
object, we enrich our formulation to multi-channel inputs, which permit
different points of view for the same input. The channel inputs could be, like
in our experiments, the output of multiple tracking methods or other feature
maps. After extracting and combining those feature maps, instead of relying
only on hidden layers representations to predict a good tracking bounding box,
we explicitly learn an intermediate, more refined one, namely the segmentation
map of the tracked object. This prevents the rough common bounding box approach
to introduce noise and distractors in the learning process. We test our method,
SFTrack++, on seven tracking benchmarks: VOT2018, LaSOT, TrackingNet, GOT10k,
NFS, OTB-100, and UAV123.
</p>
<a href="http://arxiv.org/abs/2011.13843" target="_blank">arXiv:2011.13843</a> [<a href="http://arxiv.org/pdf/2011.13843" target="_blank">pdf</a>]

<h2>Discovering Causal Structure with Reproducing-Kernel Hilbert Space $\epsilon$-Machines. (arXiv:2011.14821v1 [cs.LG])</h2>
<h3>Nicolas Brodu, James P. Crutchfield</h3>
<p>We merge computational mechanics' definition of causal states
(predictively-equivalent histories) with reproducing-kernel Hilbert space
(RKHS) representation inference. The result is a widely-applicable method that
infers causal structure directly from observations of a system's behaviors
whether they are over discrete or continuous events or time. A structural
representation -- a finite- or infinite-state kernel $\epsilon$-machine -- is
extracted by a reduced-dimension transform that gives an efficient
representation of causal states and their topology. In this way, the system
dynamics are represented by a stochastic (ordinary or partial) differential
equation that acts on causal states. We introduce an algorithm to estimate the
associated evolution operator. Paralleling the Fokker-Plank equation, it
efficiently evolves causal-state distributions and makes predictions in the
original data space via an RKHS functional mapping. We demonstrate these
techniques, together with their predictive abilities, on discrete-time,
discrete-value infinite Markov-order processes generated by finite-state hidden
Markov models with (i) finite or (ii) uncountably-infinite causal states and
(iii) a continuous-time, continuous-value process generated by a
thermally-driven chaotic flow. The method robustly estimates causal structure
in the presence of varying external and measurement noise levels.
</p>
<a href="http://arxiv.org/abs/2011.14821" target="_blank">arXiv:2011.14821</a> [<a href="http://arxiv.org/pdf/2011.14821" target="_blank">pdf</a>]

<h2>Explaining by Removing: A Unified Framework for Model Explanation. (arXiv:2011.14878v1 [cs.LG])</h2>
<h3>Ian Covert, Scott Lundberg, Su-In Lee</h3>
<p>Researchers have proposed a wide variety of model explanation approaches, but
it remains unclear how most methods are related or when one method is
preferable to another. We establish a new class of methods, removal-based
explanations, that are based on the principle of simulating feature removal to
quantify each feature's influence. These methods vary in several respects, so
we develop a framework that characterizes each method along three dimensions:
1) how the method removes features, 2) what model behavior the method explains,
and 3) how the method summarizes each feature's influence. Our framework
unifies 25 existing methods, including several of the most widely used
approaches (SHAP, LIME, Meaningful Perturbations, permutation tests). This new
class of explanation methods has rich connections that we examine using tools
that have been largely overlooked by the explainability literature. To anchor
removal-based explanations in cognitive psychology, we show that feature
removal is a simple application of subtractive counterfactual reasoning. Ideas
from cooperative game theory shed light on the relationships and trade-offs
among different methods, and we derive conditions under which all removal-based
explanations have information-theoretic interpretations. Through this analysis,
we develop a unified framework that helps practitioners better understand model
explanation tools, and that offers a strong theoretical foundation upon which
future explainability research can build.
</p>
<a href="http://arxiv.org/abs/2011.14878" target="_blank">arXiv:2011.14878</a> [<a href="http://arxiv.org/pdf/2011.14878" target="_blank">pdf</a>]

<h2>A Deep Learning-based Collocation Method for Modeling Unknown PDEs from Sparse Observation. (arXiv:2011.14965v1 [stat.ML])</h2>
<h3>Priyabrata Saha, Saibal Mukhopadhyay</h3>
<p>Deep learning-based modeling of dynamical systems driven by partial
differential equations (PDEs) has become quite popular in recent years.
However, most of the existing deep learning-based methods either assume strong
physics prior, or depend on specific initial and boundary conditions, or
require data in dense regular grid making them inapt for modeling unknown PDEs
from sparsely-observed data. This paper presents a deep learning-based
collocation method for modeling dynamical systems driven by unknown PDEs when
data sites are sparsely distributed. The proposed method is spatial
dimension-independent, geometrically flexible, learns from sparsely-available
data and the learned model does not depend on any specific initial and boundary
conditions. We demonstrate our method in the forecasting task for
two-dimensional wave equation and Burgers-Fisher equation in multiple
geometries with different boundary conditions.
</p>
<a href="http://arxiv.org/abs/2011.14965" target="_blank">arXiv:2011.14965</a> [<a href="http://arxiv.org/pdf/2011.14965" target="_blank">pdf</a>]

<h2>Variance based sensitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes. (arXiv:2011.15001v1 [stat.ML])</h2>
<h3>Morgane Menz, Sylvain Dubreuil, J&#xe9;r&#xf4;me Morio, Christian Gogu, Nathalie Bartoli, Marie Chiron</h3>
<p>Running a reliability analysis on engineering problems involving complex
numerical models can be computationally very expensive, requiring advanced
simulation methods to reduce the overall numerical cost. Gaussian process based
active learning methods for reliability analysis have emerged as a promising
way for reducing this computational cost. The learning phase of these methods
consists in building a Gaussian process surrogate model of the performance
function and using the uncertainty structure of the Gaussian process to enrich
iteratively this surrogate model. For that purpose a learning criterion has to
be defined. Then, the estimation of the probability of failure is typically
obtained by a classification of a population evaluated on the final surrogate
model. Hence, the estimator of the probability of failure holds two different
uncertainty sources related to the surrogate model approximation and to the
sampling based integration technique. In this paper, we propose a methodology
to quantify the sensitivity of the probability of failure estimator to both
uncertainty sources. This analysis also enables to control the whole error
associated to the failure probability estimate and thus provides an accuracy
criterion on the estimation. Thus, an active learning approach integrating this
analysis to reduce the main source of error and stopping when the global
variability is sufficiently low is introduced. The approach is proposed for
both a Monte Carlo based method as well as an importance sampling based method,
seeking to improve the estimation of rare event probabilities. Performance of
the proposed strategy is then assessed on several examples.
</p>
<a href="http://arxiv.org/abs/2011.15001" target="_blank">arXiv:2011.15001</a> [<a href="http://arxiv.org/pdf/2011.15001" target="_blank">pdf</a>]

<h2>RealCause: Realistic Causal Inference Benchmarking. (arXiv:2011.15007v1 [cs.LG])</h2>
<h3>Brady Neal, Chin-Wei Huang, Sunand Raghupathi</h3>
<p>There are many different causal effect estimators in causal inference.
However, it is unclear how to choose between these estimators because there is
no ground-truth for causal effects. A commonly used option is to simulate
synthetic data, where the ground-truth is known. However, the best causal
estimators on synthetic data are unlikely to be the best causal estimators on
realistic data. An ideal benchmark for causal estimators would both (a) yield
ground-truth values of the causal effects and (b) be representative of real
data. Using flexible generative models, we provide a benchmark that both yields
ground-truth and is realistic. Using this benchmark, we evaluate 66 different
causal estimators.
</p>
<a href="http://arxiv.org/abs/2011.15007" target="_blank">arXiv:2011.15007</a> [<a href="http://arxiv.org/pdf/2011.15007" target="_blank">pdf</a>]

<h2>General Invertible Transformations for Flow-based Generative Modeling. (arXiv:2011.15056v1 [cs.LG])</h2>
<h3>Jakub M. Tomczak</h3>
<p>In this paper, we present a new class of invertible transformations. We
indicate that many well-known invertible tranformations in reversible logic and
reversible neural networks could be derived from our proposition. Next, we
propose two new coupling layers that are important building blocks of
flow-based generative models. In the preliminary experiments on toy digit data,
we present how these new coupling layers could be used in Integer Discrete
Flows (IDF), and that they achieve better results than standard coupling layers
used in IDF and RealNVP.
</p>
<a href="http://arxiv.org/abs/2011.15056" target="_blank">arXiv:2011.15056</a> [<a href="http://arxiv.org/pdf/2011.15056" target="_blank">pdf</a>]

<h2>Inductive Biases for Deep Learning of Higher-Level Cognition. (arXiv:2011.15091v1 [cs.LG])</h2>
<h3>Anirudh Goyal, Yoshua Bengio</h3>
<p>A fascinating hypothesis is that human and animal intelligence could be
explained by a few principles (rather than an encyclopedic list of heuristics).
If that hypothesis was correct, we could more easily both understand our own
intelligence and build intelligent machines. Just like in physics, the
principles themselves would not be sufficient to predict the behavior of
complex systems like brains, and substantial computation might be needed to
simulate human-like intelligence. This hypothesis would suggest that studying
the kind of inductive biases that humans and animals exploit could help both
clarify these principles and provide inspiration for AI research and
neuroscience theories. Deep learning already exploits several key inductive
biases, and this work considers a larger list, focusing on those which concern
mostly higher-level and sequential conscious processing. The objective of
clarifying these particular principles is that they could potentially help us
build AI systems benefiting from humans' abilities in terms of flexible
out-of-distribution and systematic generalization, which is currently an area
where a large gap exists between state-of-the-art machine learning and human
intelligence.
</p>
<a href="http://arxiv.org/abs/2011.15091" target="_blank">arXiv:2011.15091</a> [<a href="http://arxiv.org/pdf/2011.15091" target="_blank">pdf</a>]

