---
title: Latest Deep Learning Papers
date: 2020-12-02 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (187 Articles)</h1>
<h2>A Framework and Dataset for Abstract Art Generation via CalligraphyGAN. (arXiv:2012.00744v1 [cs.CV])</h2>
<h3>Jinggang Zhuo, Ling Fan, Harry Jiannan Wang</h3>
<p>With the advancement of deep learning, artificial intelligence (AI) has made
many breakthroughs in recent years and achieved superhuman performance in
various tasks such as object detection, reading comprehension, and video games.
Generative Modeling, such as various Generative Adversarial Networks (GAN)
models, has been applied to generate paintings and music. Research in Natural
Language Processing (NLP) also had a leap forward in 2018 since the release of
the pre-trained contextual neural language models such as BERT and recently
released GPT3. Despite the exciting AI applications aforementioned, AI is still
significantly lagging behind humans in creativity, which is often considered
the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy,
which is a unique form of visual art where the character itself is an aesthetic
painting. We also draw inspirations from paintings of the Abstract
Expressionist movement in the 1940s and 1950s, such as the work by American
painter Franz Kline. In this paper, we present a creative framework based on
Conditional Generative Adversarial Networks and Contextual Neural Language
Model to generate abstract artworks that have intrinsic meaning and aesthetic
value, which is different from the existing work, such as image captioning and
text-to-image generation, where the texts are the descriptions of the images.
In addition, we have publicly released a Chinese calligraphy image dataset and
demonstrate our framework using a prototype system and a user study.
</p>
<a href="http://arxiv.org/abs/2012.00744" target="_blank">arXiv:2012.00744</a> [<a href="http://arxiv.org/pdf/2012.00744" target="_blank">pdf</a>]

<h2>Forecasting Black Sigatoka Infection Risks with Latent Neural ODEs. (arXiv:2012.00752v1 [cs.LG])</h2>
<h3>Yuchen Wang, Matthieu Chan Chee, Ziyad Edher, Minh Duc Hoang, Shion Fujimori, Sornnujah Kathirgamanathan, Jesse Bettencourt</h3>
<p>Black Sigatoka disease severely decreases global banana production, and
climate change aggravates the problem by altering fungal species distributions.
Due to the heavy financial burden of managing this infectious disease, farmers
in developing countries face significant banana crop losses. Though scientists
have produced mathematical models of infectious diseases, adapting these models
to incorporate climate effects is difficult. We present MR. NODE (Multiple
predictoR Neural ODE), a neural network that models the dynamics of black
Sigatoka infection learnt directly from data via Neural Ordinary Differential
Equations. Our method encodes external predictor factors into the latent space
in addition to the variable that we infer, and it can also predict the
infection risk at an arbitrary point in time. Empirically, we demonstrate on
historical climate data that our method has superior generalization performance
on time points up to one month in the future and unseen irregularities. We
believe that our method can be a useful tool to control the spread of black
Sigatoka.
</p>
<a href="http://arxiv.org/abs/2012.00752" target="_blank">arXiv:2012.00752</a> [<a href="http://arxiv.org/pdf/2012.00752" target="_blank">pdf</a>]

<h2>MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. (arXiv:2012.00759v1 [cs.CV])</h2>
<h3>Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen</h3>
<p>We present MaX-DeepLab, the first end-to-end model for panoptic segmentation.
Our approach simplifies the current pipeline that depends heavily on surrogate
sub-tasks and hand-designed components, such as box detection, non-maximum
suppression, thing-stuff merging, etc. Although these sub-tasks are tackled by
area experts, they fail to comprehensively solve the target task. By contrast,
our MaX-DeepLab directly predicts class-labeled masks with a mask transformer,
and is trained with a panoptic quality inspired loss via bipartite matching.
Our mask transformer employs a dual-path architecture that introduces a global
memory path in addition to a CNN path, allowing direct communication with any
CNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the
box-free regime on the challenging COCO dataset, closing the gap between
box-based and box-free methods for the first time. A small variant of
MaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds.
Furthermore, MaX-DeepLab, without test time augmentation, achieves new
state-of-the-art 51.3% PQ on COCO test-dev set.
</p>
<a href="http://arxiv.org/abs/2012.00759" target="_blank">arXiv:2012.00759</a> [<a href="http://arxiv.org/pdf/2012.00759" target="_blank">pdf</a>]

<h2>Dynamic Feature Pyramid Networks for Object Detection. (arXiv:2012.00779v1 [cs.CV])</h2>
<h3>Mingjian Zhu, Kai Han, Changbin Yu, Yunhe Wang</h3>
<p>This paper studies feature pyramid network (FPN), which is a widely used
module for aggregating multi-scale feature information in the object detection
system. The performance gain in most of the existing works is mainly
contributed to the increase of computation burden, especially the floating
number operations (FLOPs). In addition, the multi-scale information within each
layer in FPN has not been well investigated. To this end, we first introduce an
inception FPN in which each layer contains convolution filters with different
kernel sizes to enlarge the receptive field and integrate more useful
information. Moreover, we point out that not all objects need such a
complicated calculation module and propose a new dynamic FPN (DyFPN). Each
layer in the DyFPN consists of multiple branches with different computational
costs. Specifically, the output features of DyFPN will be calculated by using
the adaptively selected branch according to a learnable gating operation.
Therefore, the proposed method can provide a more efficient dynamic inference
for achieving a better trade-off between accuracy and detection performance.
Extensive experiments conducted on benchmarks demonstrate that the proposed
DyFPN significantly improves performance with the optimal allocation of
computation resources. For instance, replacing the FPN with the inception FPN
improves detection accuracy by 1.6 AP using the Faster R-CNN paradigm on COCO
minival, and the DyFPN further reduces about 40% of its FLOPs while maintaining
similar performance.
</p>
<a href="http://arxiv.org/abs/2012.00779" target="_blank">arXiv:2012.00779</a> [<a href="http://arxiv.org/pdf/2012.00779" target="_blank">pdf</a>]

<h2>Refining Deep Generative Models via Wasserstein Gradient Flows. (arXiv:2012.00780v1 [cs.LG])</h2>
<h3>Abdul Fatir Ansari, Ming Liang Ang, Harold Soh</h3>
<p>Deep generative modeling has seen impressive advances in recent years, to the
point where it is now commonplace to see simulated samples (e.g., images) that
closely resemble real-world data. However, generation quality is generally
inconsistent for any given model and can vary dramatically between samples. We
introduce Discriminator Gradient flow (DGflow), a new technique that improves
generated samples via the gradient flow of entropy-regularized f-divergences
between the real and the generated data distributions. The gradient flow takes
the form of a non-linear Fokker-Plank equation, which can be easily simulated
by sampling from the equivalent McKean-Vlasov process. By refining inferior
samples, our technique avoids wasteful sample rejection used by previous
methods (DRS &amp; MH-GAN). Compared to existing works that focus on specific GAN
variants, we show our refinement approach can be applied to GANs with
vector-valued critics and even other deep generative models such as VAEs and
Normalizing Flows. Empirical results on multiple synthetic, image, and text
datasets demonstrate that DGflow leads to significant improvement in the
quality of generated samples for a variety of generative models, outperforming
the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator
Driven Latent Sampling (DDLS) methods.
</p>
<a href="http://arxiv.org/abs/2012.00780" target="_blank">arXiv:2012.00780</a> [<a href="http://arxiv.org/pdf/2012.00780" target="_blank">pdf</a>]

<h2>Pose-based Sign Language Recognition using GCN and BERT. (arXiv:2012.00781v1 [cs.CV])</h2>
<h3>Anirudh Tunga, Sai Vidyaranya Nuthalapati, Juan Wachs</h3>
<p>Sign language recognition (SLR) plays a crucial role in bridging the
communication gap between the hearing and vocally impaired community and the
rest of the society. Word-level sign language recognition (WSLR) is the first
important step towards understanding and interpreting sign language. However,
recognizing signs from videos is a challenging task as the meaning of a word
depends on a combination of subtle body motions, hand configurations, and other
movements. Recent pose-based architectures for WSLR either model both the
spatial and temporal dependencies among the poses in different frames
simultaneously or only model the temporal information without fully utilizing
the spatial information.

We tackle the problem of WSLR using a novel pose-based approach, which
captures spatial and temporal information separately and performs late fusion.
Our proposed architecture explicitly captures the spatial interactions in the
video using a Graph Convolutional Network (GCN). The temporal dependencies
between the frames are captured using Bidirectional Encoder Representations
from Transformers (BERT). Experimental results on WLASL, a standard word-level
sign language recognition dataset show that our model significantly outperforms
the state-of-the-art on pose-based methods by achieving an improvement in the
prediction accuracy by up to 5%.
</p>
<a href="http://arxiv.org/abs/2012.00781" target="_blank">arXiv:2012.00781</a> [<a href="http://arxiv.org/pdf/2012.00781" target="_blank">pdf</a>]

<h2>Adversarial Robustness Across Representation Spaces. (arXiv:2012.00802v1 [cs.CV])</h2>
<h3>Pranjal Awasthi, George Yu, Chun-Sung Ferng, Andrew Tomkins, Da-Cheng Juan</h3>
<p>Adversarial robustness corresponds to the susceptibility of deep neural
networks to imperceptible perturbations made at test time. In the context of
image tasks, many algorithms have been proposed to make neural networks robust
to adversarial perturbations made to the input pixels. These perturbations are
typically measured in an $\ell_p$ norm. However, robustness often holds only
for the specific attack used for training. In this work we extend the above
setting to consider the problem of training of deep neural networks that can be
made simultaneously robust to perturbations applied in multiple natural
representation spaces. For the case of image data, examples include the
standard pixel representation as well as the representation in the discrete
cosine transform~(DCT) basis. We design a theoretically sound algorithm with
formal guarantees for the above problem. Furthermore, our guarantees also hold
when the goal is to require robustness with respect to multiple $\ell_p$ norm
based attacks. We then derive an efficient practical implementation and
demonstrate the effectiveness of our approach on standard datasets for image
classification.
</p>
<a href="http://arxiv.org/abs/2012.00802" target="_blank">arXiv:2012.00802</a> [<a href="http://arxiv.org/pdf/2012.00802" target="_blank">pdf</a>]

<h2>Stochastic Approximation with Markov Noise: Analysis and applications in reinforcement learning. (arXiv:2012.00805v1 [cs.LG])</h2>
<h3>Prasenjit Karmakar</h3>
<p>We present for the first time an asymptotic convergence analysis of two
time-scale stochastic approximation driven by "controlled" Markov noise. In
particular, the faster and slower recursions have non-additive controlled
Markov noise components in addition to martingale difference noise. We analyze
the asymptotic behavior of our framework by relating it to limiting
differential inclusions in both time scales that are defined in terms of the
ergodic occupation measures associated with the controlled Markov processes.
Using a special case of our results, we present a solution to the off-policy
convergence problem for temporal-difference learning with linear function
approximation. We compile several aspects of the dynamics of stochastic
approximation algorithms with Markov iterate-dependent noise when the iterates
are not known to be stable beforehand. We achieve the same by extending the
lock-in probability (i.e. the probability of convergence to a specific
attractor of the limiting o.d.e. given that the iterates are in its domain of
attraction after a sufficiently large number of iterations (say) n_0) framework
to such recursions. We use these results to prove almost sure convergence of
the iterates to the specified attractor when the iterates satisfy an
"asymptotic tightness" condition. This, in turn, is shown to be useful in
analyzing the tracking ability of general "adaptive" algorithms. Finally, we
obtain the first informative error bounds on function approximation for the
policy evaluation algorithm proposed by Basu et al. when the aim is to find the
risk-sensitive cost represented using exponential utility. We show that this
happens due to the absence of difference term in the earlier bound which is
always present in all our bounds when the state space is large.
</p>
<a href="http://arxiv.org/abs/2012.00805" target="_blank">arXiv:2012.00805</a> [<a href="http://arxiv.org/pdf/2012.00805" target="_blank">pdf</a>]

<h2>Open-Ended Multi-Modal Relational Reason for Video Question Answering. (arXiv:2012.00822v1 [cs.AI])</h2>
<h3>Haozheng Luo, Ruiyang Qin</h3>
<p>People with visual impairments urgently need helps, not only on the basic
tasks such as guiding and retrieving objects , but on the advanced tasks like
picturing the new environments. More than a guiding dog, they might want some
devices which are able to provide linguistic interaction. Building on various
research literature, we aim to conduct a research on the interaction between
the robot agent and visual impaired people. The robot agent, applied VQA
techniques, is able to analyze the environment, process and understand the
pronouncing questions, and provide feedback to the human user. In this paper,
we are going to discuss the related questions about this kind of interaction,
the techniques we used in this work, and how we conduct our research.
</p>
<a href="http://arxiv.org/abs/2012.00822" target="_blank">arXiv:2012.00822</a> [<a href="http://arxiv.org/pdf/2012.00822" target="_blank">pdf</a>]

<h2>A Three-Stage Self-Training Framework for Semi-Supervised Semantic Segmentation. (arXiv:2012.00827v1 [cs.CV])</h2>
<h3>Rihuan Ke, Angelica Aviles-Rivero, Saurabh Pandey, Saikumar Reddy, Carola-Bibiane Sch&#xf6;nlieb</h3>
<p>Semantic segmentation has been widely investigated in the community, in which
the state of the art techniques are based on supervised models. Those models
have reported unprecedented performance at the cost of requiring a large set of
high quality segmentation masks. To obtain such annotations is highly expensive
and time consuming, in particular, in semantic segmentation where pixel-level
annotations are required. In this work, we address this problem by proposing a
holistic solution framed as a three-stage self-training framework for
semi-supervised semantic segmentation. The key idea of our technique is the
extraction of the pseudo-masks statistical information to decrease uncertainty
in the predicted probability whilst enforcing segmentation consistency in a
multi-task fashion. We achieve this through a three-stage solution. Firstly, we
train a segmentation network to produce rough pseudo-masks which predicted
probability is highly uncertain. Secondly, we then decrease the uncertainty of
the pseudo-masks using a multi-task model that enforces consistency whilst
exploiting the rich statistical information of the data. We compare our
approach with existing methods for semi-supervised semantic segmentation and
demonstrate its state-of-the-art performance with extensive experiments.
</p>
<a href="http://arxiv.org/abs/2012.00827" target="_blank">arXiv:2012.00827</a> [<a href="http://arxiv.org/pdf/2012.00827" target="_blank">pdf</a>]

<h2>Data Augmentation with norm-VAE for Unsupervised Domain Adaptation. (arXiv:2012.00848v1 [cs.CV])</h2>
<h3>Qian Wang, Fanlin Meng, Toby P. Breckon</h3>
<p>We address the Unsupervised Domain Adaptation (UDA) problem in image
classification from a new perspective. In contrast to most existing works which
either align the data distributions or learn domain-invariant features, we
directly learn a unified classifier for both domains within a high-dimensional
homogeneous feature space without explicit domain adaptation. To this end, we
employ the effective Selective Pseudo-Labelling (SPL) techniques to take
advantage of the unlabelled samples in the target domain. Surprisingly, data
distribution discrepancy across the source and target domains can be well
handled by a computationally simple classifier (e.g., a shallow Multi-Layer
Perceptron) trained in the original feature space. Besides, we propose a novel
generative model norm-VAE to generate synthetic features for the target domain
as a data augmentation strategy to enhance classifier training. Experimental
results on several benchmark datasets demonstrate the pseudo-labelling strategy
itself can lead to comparable performance to many state-of-the-art methods
whilst the use of norm-VAE for feature augmentation can further improve the
performance in most cases. As a result, our proposed methods (i.e. naive-SPL
and norm-VAE-SPL) can achieve new state-of-the-art performance with the average
accuracy of 93.4% and 90.4% on Office-Caltech and ImageCLEF-DA datasets, and
comparable performance on Digits, Office31 and Office-Home datasets with the
average accuracy of 97.2%, 87.6% and 67.9% respectively.
</p>
<a href="http://arxiv.org/abs/2012.00848" target="_blank">arXiv:2012.00848</a> [<a href="http://arxiv.org/pdf/2012.00848" target="_blank">pdf</a>]

<h2>FFD: Fast Feature Detector. (arXiv:2012.00859v1 [cs.CV])</h2>
<h3>Morteza Ghahremani, Yonghuai Liu, Bernard Tiddeman</h3>
<p>Scale-invariance, good localization and robustness to noise and distortions
are the main properties that a local feature detector should possess. Most
existing local feature detectors find excessive unstable feature points that
increase the number of keypoints to be matched and the computational time of
the matching step. In this paper, we show that robust and accurate keypoints
exist in the specific scale-space domain. To this end, we first formulate the
superimposition problem into a mathematical model and then derive a closed-form
solution for multiscale analysis. The model is formulated via
difference-of-Gaussian (DoG) kernels in the continuous scale-space domain, and
it is proved that setting the scale-space pyramid's blurring ratio and
smoothness to 2 and 0.627, respectively, facilitates the detection of reliable
keypoints. For the applicability of the proposed model to discrete images, we
discretize it using the undecimated wavelet transform and the cubic spline
function. Theoretically, the complexity of our method is less than 5\% of that
of the popular baseline Scale Invariant Feature Transform (SIFT). Extensive
experimental results show the superiority of the proposed feature detector over
the existing representative hand-crafted and learning-based techniques in
accuracy and computational time. The code and supplementary materials can be
found at~{\url{https://github.com/mogvision/FFD}}.
</p>
<a href="http://arxiv.org/abs/2012.00859" target="_blank">arXiv:2012.00859</a> [<a href="http://arxiv.org/pdf/2012.00859" target="_blank">pdf</a>]

<h2>Towards Good Practices in Self-supervised Representation Learning. (arXiv:2012.00868v1 [cs.CV])</h2>
<h3>Srikar Appalaraju, Yi Zhu, Yusheng Xie, Istv&#xe1;n Feh&#xe9;rv&#xe1;ri</h3>
<p>Self-supervised representation learning has seen remarkable progress in the
last few years. More recently, contrastive instance learning has shown
impressive results compared to its supervised learning counterparts. However,
even with the ever increased interest in contrastive instance learning, it is
still largely unclear why these methods work so well. In this paper, we aim to
unravel some of the mysteries behind their success, which are the good
practices. Through an extensive empirical analysis, we hope to not only provide
insights but also lay out a set of best practices that led to the success of
recent work in self-supervised representation learning.
</p>
<a href="http://arxiv.org/abs/2012.00868" target="_blank">arXiv:2012.00868</a> [<a href="http://arxiv.org/pdf/2012.00868" target="_blank">pdf</a>]

<h2>A compact sequence encoding scheme for online human activity recognition in HRI applications. (arXiv:2012.00873v1 [cs.CV])</h2>
<h3>Georgios Tsatiris, Kostas Karpouzis, Stefanos Kollias</h3>
<p>Human activity recognition and analysis has always been one of the most
active areas of pattern recognition and machine intelligence, with applications
in various fields, including but not limited to exertion games, surveillance,
sports analytics and healthcare. Especially in Human-Robot Interaction, human
activity understanding plays a crucial role as household robotic assistants are
a trend of the near future. However, state-of-the-art infrastructures that can
support complex machine intelligence tasks are not always available, and may
not be for the average consumer, as robotic hardware is expensive. In this
paper we propose a novel action sequence encoding scheme which efficiently
transforms spatio-temporal action sequences into compact representations, using
Mahalanobis distance-based shape features and the Radon transform. This
representation can be used as input for a lightweight convolutional neural
network. Experiments show that the proposed pipeline, when based on
state-of-the-art human pose estimation techniques, can provide a robust
end-to-end online action recognition scheme, deployable on hardware lacking
extreme computing capabilities.
</p>
<a href="http://arxiv.org/abs/2012.00873" target="_blank">arXiv:2012.00873</a> [<a href="http://arxiv.org/pdf/2012.00873" target="_blank">pdf</a>]

<h2>Diffusion is All You Need for Learning on Surfaces. (arXiv:2012.00888v1 [cs.CV])</h2>
<h3>Nicholas Sharp, Souhaib Attaiki, Keenan Crane, Maks Ovsjanikov</h3>
<p>We introduce a new approach to deep learning on 3D surfaces such as meshes or
point clouds. Our key insight is that a simple learned diffusion layer can
spatially share data in a principled manner, replacing operations like
convolution and pooling which are complicated and expensive on surfaces. The
only other ingredients in our network are a spatial gradient operation, which
uses dot-products of derivatives to encode tangent-invariant filters, and a
multi-layer perceptron applied independently at each point. The resulting
architecture, which we call DiffusionNet, is remarkably simple, efficient, and
scalable. Continuously optimizing for spatial support avoids the need to pick
neighborhood sizes or filter widths a priori, or worry about their impact on
network size/training time. Furthermore, the principled, geometric nature of
these networks makes them agnostic to the underlying representation and
insensitive to discretization. In practice, this means significant robustness
to mesh sampling, and even the ability to train on a mesh and evaluate on a
point cloud. Our experiments demonstrate that these networks achieve
state-of-the-art results for a variety of tasks on both meshes and point
clouds, including surface classification, segmentation, and non-rigid
correspondence.
</p>
<a href="http://arxiv.org/abs/2012.00888" target="_blank">arXiv:2012.00888</a> [<a href="http://arxiv.org/pdf/2012.00888" target="_blank">pdf</a>]

<h2>Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and Algorithms. (arXiv:2012.00889v1 [cs.LG])</h2>
<h3>Aaron J. Snoswell, Surya P. N. Singh, Nan Ye</h3>
<p>We provide new perspectives and inference algorithms for Maximum Entropy
(MaxEnt) Inverse Reinforcement Learning (IRL), which provides a principled
method to find a most non-committal reward function consistent with given
expert demonstrations, among many consistent reward functions.

We first present a generalized MaxEnt formulation based on minimizing a
KL-divergence instead of maximizing an entropy. This improves the previous
heuristic derivation of the MaxEnt IRL model (for stochastic MDPs), allows a
unified view of MaxEnt IRL and Relative Entropy IRL, and leads to a model-free
learning algorithm for the MaxEnt IRL model. Second, a careful review of
existing inference algorithms and implementations showed that they
approximately compute the marginals required for learning the model. We provide
examples to illustrate this, and present an efficient and exact inference
algorithm. Our algorithm can handle variable length demonstrations; in
addition, while a basic version takes time quadratic in the maximum
demonstration length L, an improved version of this algorithm reduces this to
linear using a padding trick.

Experiments show that our exact algorithm improves reward learning as
compared to the approximate ones. Furthermore, our algorithm scales up to a
large, real-world dataset involving driver behaviour forecasting. We provide an
optimized implementation compatible with the OpenAI Gym interface. Our new
insight and algorithms could possibly lead to further interest and exploration
of the original MaxEnt IRL model.
</p>
<a href="http://arxiv.org/abs/2012.00889" target="_blank">arXiv:2012.00889</a> [<a href="http://arxiv.org/pdf/2012.00889" target="_blank">pdf</a>]

<h2>Displacement-Invariant Cost Computation for Efficient Stereo Matching. (arXiv:2012.00899v1 [cs.CV])</h2>
<h3>Yiran Zhong, Charles Loop, Wonmin Byeon, Stan Birchfield, Yuchao Dai, Kaihao Zhang, Alexey Kamenev, Thomas Breuel, Hongdong Li, Jan Kautz</h3>
<p>Although deep learning-based methods have dominated stereo matching
leaderboards by yielding unprecedented disparity accuracy, their inference time
is typically slow, on the order of seconds for a pair of 540p images. The main
reason is that the leading methods employ time-consuming 3D convolutions
applied to a 4D feature volume. A common way to speed up the computation is to
downsample the feature volume, but this loses high-frequency details. To
overcome these challenges, we propose a \emph{displacement-invariant cost
computation module} to compute the matching costs without needing a 4D feature
volume. Rather, costs are computed by applying the same 2D convolution network
on each disparity-shifted feature map pair independently. Unlike previous 2D
convolution-based methods that simply perform context mapping between inputs
and disparity maps, our proposed approach learns to match features between the
two images. We also propose an entropy-based refinement strategy to refine the
computed disparity map, which further improves speed by avoiding the need to
compute a second disparity map on the right image. Extensive experiments on
standard datasets (SceneFlow, KITTI, ETH3D, and Middlebury) demonstrate that
our method achieves competitive accuracy with much less inference time. On
typical image sizes, our method processes over 100 FPS on a desktop GPU, making
our method suitable for time-critical applications such as autonomous driving.
We also show that our approach generalizes well to unseen datasets,
outperforming 4D-volumetric methods.
</p>
<a href="http://arxiv.org/abs/2012.00899" target="_blank">arXiv:2012.00899</a> [<a href="http://arxiv.org/pdf/2012.00899" target="_blank">pdf</a>]

<h2>Deep Multi-Fidelity Active Learning of High-dimensional Outputs. (arXiv:2012.00901v1 [cs.LG])</h2>
<h3>Shibo Li, Robert M. Kirby, Shandian Zhe</h3>
<p>Many applications, such as in physical simulation and engineering design,
demand we estimate functions with high-dimensional outputs. The training
examples can be collected with different fidelities to allow a cost/accuracy
trade-off. In this paper, we consider the active learning task that identifies
both the fidelity and input to query new training examples so as to achieve the
best benefit-cost ratio. To this end, we propose DMFAL, a Deep Multi-Fidelity
Active Learning approach. We first develop a deep neural network-based
multi-fidelity model for learning with high-dimensional outputs, which can
flexibly, efficiently capture all kinds of complex relationships across the
outputs and fidelities to improve prediction. We then propose a mutual
information-based acquisition function that extends the predictive entropy
principle. To overcome the computational challenges caused by large output
dimensions, we use multi-variate Delta's method and moment-matching to estimate
the output posterior, and Weinstein-Aronszajn identity to calculate and
optimize the acquisition function. The computation is tractable, reliable and
efficient. We show the advantage of our method in several applications of
computational physics and engineering design.
</p>
<a href="http://arxiv.org/abs/2012.00901" target="_blank">arXiv:2012.00901</a> [<a href="http://arxiv.org/pdf/2012.00901" target="_blank">pdf</a>]

<h2>ReMP: Rectified Metric Propagation for Few-Shot Learning. (arXiv:2012.00904v1 [cs.CV])</h2>
<h3>Yang Zhao, Chunyuan Li, Ping Yu, Changyou Chen</h3>
<p>Few-shot learning features the capability of generalizing from a few
examples. In this paper, we first identify that a discriminative feature space,
namely a rectified metric space, that is learned to maintain the metric
consistency from training to testing, is an essential component to the success
of metric-based few-shot learning. Numerous analyses indicate that a simple
modification of the objective can yield substantial performance gains. The
resulting approach, called rectified metric propagation (ReMP), further
optimizes an attentive prototype propagation network, and applies a repulsive
force to make confident predictions. Extensive experiments demonstrate that the
proposed ReMP is effective and efficient, and outperforms the state of the arts
on various standard few-shot learning datasets.
</p>
<a href="http://arxiv.org/abs/2012.00904" target="_blank">arXiv:2012.00904</a> [<a href="http://arxiv.org/pdf/2012.00904" target="_blank">pdf</a>]

<h2>Towards Imperceptible Adversarial Image Patches Based on Network Explanations. (arXiv:2012.00909v1 [cs.CV])</h2>
<h3>Yaguan Qian, Jiamin Wang, Bin Wang, Zhaoquan Gu, Xiang Ling, Chunming Wu</h3>
<p>The vulnerability of deep neural networks (DNNs) for adversarial examples
have attracted more attention. Many algorithms are proposed to craft powerful
adversarial examples. However, these algorithms modifying the global or local
region of pixels without taking into account network explanations. Hence, the
perturbations are redundancy and easily detected by human eyes. In this paper,
we propose a novel method to generate local region perturbations. The main idea
is to find the contributing feature regions (CFRs) of images based on network
explanations for perturbations. Due to the network explanations, the
perturbations added to the CFRs are more effective than other regions. In our
method, a soft mask matrix is designed to represent the CFRs for finely
characterizing the contributions of each pixel. Based on this soft mask, we
develop a new objective function with inverse temperature to search for optimal
perturbations in CFRs. Extensive experiments are conducted on CIFAR-10 and
ILSVRC2012, which demonstrate the effectiveness, including attack success rate,
imperceptibility,and transferability.
</p>
<a href="http://arxiv.org/abs/2012.00909" target="_blank">arXiv:2012.00909</a> [<a href="http://arxiv.org/pdf/2012.00909" target="_blank">pdf</a>]

<h2>MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection. (arXiv:2012.00914v1 [cs.CV])</h2>
<h3>Kellie Corona (1), Katie Osterdahl (1), Roderic Collins (1), Anthony Hoogs (1) ((1) Kitware, Inc.)</h3>
<p>We present the Multiview Extended Video with Activities (MEVA) dataset, a new
and very-large-scale dataset for human activity recognition. Existing security
datasets either focus on activity counts by aggregating public video
disseminated due to its content, which typically excludes same-scene background
video, or they achieve persistence by observing public areas and thus cannot
control for activity content. Our dataset is over 9300 hours of untrimmed,
continuous video, scripted to include diverse, simultaneous activities, along
with spontaneous background activity. We have annotated 144 hours for 37
activity types, marking bounding boxes of actors and props. Our collection
observed approximately 100 actors performing scripted scenarios and spontaneous
background activity over a three-week period at an access-controlled venue,
collecting in multiple modalities with overlapping and non-overlapping indoor
and outdoor viewpoints. The resulting data includes video from 38 RGB and
thermal IR cameras, 42 hours of UAV footage, as well as GPS locations for the
actors. 122 hours of annotation are sequestered in support of the NIST Activity
in Extended Video (ActEV) challenge; the other 22 hours of annotation and the
corresponding video are available on our website, along with an additional 306
hours of ground camera data, 4.6 hours of UAV data, and 9.6 hours of GPS logs.
Additional derived data includes camera models geo-registering the outdoor
cameras and a dense 3D point cloud model of the outdoor scene. The data was
collected with IRB oversight and approval and released under a CC-BY-4.0
license.
</p>
<a href="http://arxiv.org/abs/2012.00914" target="_blank">arXiv:2012.00914</a> [<a href="http://arxiv.org/pdf/2012.00914" target="_blank">pdf</a>]

<h2>CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. (arXiv:2012.00924v1 [cs.CV])</h2>
<h3>Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, Cewu Lu</h3>
<p>Estimating hand-object (HO) pose during interaction has been brought
remarkable growth in virtue of deep learning methods. Modeling the contact
between the hand and object properly is the key to construct a plausible grasp.
Yet, previous works usually focus on jointly estimating HO pose but not fully
explore the physical contact preserved in grasping. In this paper, we present
an explicit contact representation, Contact Potential Field (CPF) that models
each hand-object contact as a spring-mass system. Then we can refine a natural
grasp by minimizing the elastic energy w.r.t those systems. To recover CPF, we
also propose a learning-fitting hybrid framework named MIHO. Extensive
experiments on two public benchmarks have shown that our method can achieve
state-of-the-art in several reconstruction metrics, and allow us to produce
more physically plausible HO pose even when the ground-truth exhibits severe
interpenetration or disjointedness. Our code is available at
https://github.com/lixiny/CPF .
</p>
<a href="http://arxiv.org/abs/2012.00924" target="_blank">arXiv:2012.00924</a> [<a href="http://arxiv.org/pdf/2012.00924" target="_blank">pdf</a>]

<h2>SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning. (arXiv:2012.00925v1 [cs.LG])</h2>
<h3>Zhuowei Wang, Jing Jiang, Bo Han, Lei Feng, Bo An, Gang Niu, Guodong Long</h3>
<p>Deep learning with noisy labels is a challenging task. Recent prominent
methods that build on a specific sample selection (SS) strategy and a specific
semi-supervised learning (SSL) model achieved state-of-the-art performance.
Intuitively, better performance could be achieved if stronger SS strategies and
SSL models are employed. Following this intuition, one might easily derive
various effective noisy-label learning methods using different combinations of
SS strategies and SSL models, which is, however, reinventing the wheel in
essence. To prevent this problem, we propose SemiNLL, a versatile framework
that combines SS strategies and SSL models in an end-to-end manner. Our
framework can absorb various SS strategies and SSL backbones, utilizing their
power to achieve promising performance. We also instantiate our framework with
different combinations, which set the new state of the art on
benchmark-simulated and real-world datasets with noisy labels.
</p>
<a href="http://arxiv.org/abs/2012.00925" target="_blank">arXiv:2012.00925</a> [<a href="http://arxiv.org/pdf/2012.00925" target="_blank">pdf</a>]

<h2>pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis. (arXiv:2012.00926v1 [cs.CV])</h2>
<h3>Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein</h3>
<p>We have witnessed rapid progress on 3D-aware image synthesis, leveraging
recent advances in generative visual models and neural rendering. Existing
approaches however fall short in two ways: first, they may lack an underlying
3D representation or rely on view-inconsistent rendering, hence synthesizing
images that are not multi-view consistent; second, they often depend upon
representation network architectures that are not expressive enough, and their
results thus lack in image quality. We propose a novel generative model, named
Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for
high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural
representations with periodic activation functions and volumetric rendering to
represent scenes as view-consistent 3D representations with fine detail. The
proposed approach obtains state-of-the-art results for 3D-aware image synthesis
with multiple real and synthetic datasets.
</p>
<a href="http://arxiv.org/abs/2012.00926" target="_blank">arXiv:2012.00926</a> [<a href="http://arxiv.org/pdf/2012.00926" target="_blank">pdf</a>]

<h2>Extended T: Learning with Mixed Closed-set and Open-set Noisy Labels. (arXiv:2012.00932v1 [cs.LG])</h2>
<h3>Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Jiankang Deng, Jiatong Li, Yinian Mao</h3>
<p>The label noise transition matrix $T$, reflecting the probabilities that true
labels flip into noisy ones, is of vital importance to model label noise and
design statistically consistent classifiers. The traditional transition matrix
is limited to model closed-set label noise, where noisy training data has true
class labels within the noisy label set. It is unfitted to employ such a
transition matrix to model open-set label noise, where some true class labels
are outside the noisy label set. Thus when considering a more realistic
situation, i.e., both closed-set and open-set label noise occurs, existing
methods will undesirably give biased solutions. Besides, the traditional
transition matrix is limited to model instance-independent label noise, which
may not perform well in practice. In this paper, we focus on learning under the
mixed closed-set and open-set label noise. We address the aforementioned issues
by extending the traditional transition matrix to be able to model mixed label
noise, and further to the cluster-dependent transition matrix to better
approximate the instance-dependent label noise in real-world applications. We
term the proposed transition matrix as the cluster-dependent extended
transition matrix. An unbiased estimator (i.e., extended $T$-estimator) has
been designed to estimate the cluster-dependent extended transition matrix by
only exploiting the noisy data. Comprehensive synthetic and real experiments
validate that our method can better model the mixed label noise, following its
more robust performance than the prior state-of-the-art label-noise learning
methods.
</p>
<a href="http://arxiv.org/abs/2012.00932" target="_blank">arXiv:2012.00932</a> [<a href="http://arxiv.org/pdf/2012.00932" target="_blank">pdf</a>]

<h2>Improving Accuracy of Binary Neural Networks using Unbalanced Activation Distribution. (arXiv:2012.00938v1 [cs.LG])</h2>
<h3>Hyungjun Kim, Jihoon Park, Changhun Lee, Jae-Joon Kim</h3>
<p>Binarization of neural network models is considered as one of the promising
methods to deploy deep neural network models on resource-constrained
environments such as mobile devices. However, Binary Neural Networks (BNNs)
tend to suffer from severe accuracy degradation compared to the full-precision
counterpart model. Several techniques were proposed to improve the accuracy of
BNNs. One of the approaches is to balance the distribution of binary
activations so that the amount of information in the binary activations becomes
maximum. Based on extensive analysis, in stark contrast to previous work, we
argue that unbalanced activation distribution can actually improve the accuracy
of BNNs. We also show that adjusting the threshold values of binary activation
functions results in the unbalanced distribution of the binary activation,
which increases the accuracy of BNN models. Experimental results show that the
accuracy of previous BNN models (e.g. XNOR-Net and Bi-Real-Net) can be improved
by simply shifting the threshold values of binary activation functions without
requiring any other modification.
</p>
<a href="http://arxiv.org/abs/2012.00938" target="_blank">arXiv:2012.00938</a> [<a href="http://arxiv.org/pdf/2012.00938" target="_blank">pdf</a>]

<h2>Tensor Completion via Few-shot Convolutional Sparse Coding. (arXiv:2012.00944v1 [cs.CV])</h2>
<h3>Zhebin Wu, Chuan Chen, Cong Liu, Zibin Zheng</h3>
<p>Tensor data often suffer from missing value problem due to the complex
high-dimensional structure while acquiring them. To complete the missing
information, lots of Low-Rank Tensor Completion (LRTC) methods have been
proposed, most of which depend on the low-rank property of tensor data. In this
way, the low-rank component of the original data could be recovered roughly.
However, the shortcoming is that the detail information can not be fully
recovered. On the contrary, in the field of signal processing, Convolutional
Sparse Coding (CSC) can provide a good representation of the high-frequency
component of the image, which is generally associated with the detail component
of the data. Nevertheless, CSC can not handle the low-frequency component well.
To this end, we propose a novel method, LRTC-CSC, which adopts CSC as a
supplementary regularization for LRTC to capture the high-frequency components.
Therefore, LRTC-CSC can not only solve the missing value problem but also
recover the details. Moreover, LRTC-CSC can be trained with small samples due
to the sparsity characteristic of CSC. Extensive experiments show the
effectiveness of LRTC-CSC, and quantitative evaluation indicates that the
performance of our model is superior to state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.00944" target="_blank">arXiv:2012.00944</a> [<a href="http://arxiv.org/pdf/2012.00944" target="_blank">pdf</a>]

<h2>Two-Stage Single Image Reflection Removal with Reflection-Aware Guidance. (arXiv:2012.00945v1 [cs.CV])</h2>
<h3>Yu Li, Ming Liu, Yaling Yi, Qince Li, Dongwei Ren, Wangmeng Zuo</h3>
<p>Removing undesired reflection from an image captured through a glass surface
is a very challenging problem with many practical application scenarios. For
improving reflection removal, cascaded deep models have been usually adopted to
estimate the transmission in a progressive manner. However, most existing
methods are still limited in exploiting the result in prior stage for guiding
transmission estimation. In this paper, we present a novel two-stage network
with reflection-aware guidance (RAGNet) for single image reflection removal
(SIRR). To be specific, the reflection layer is firstly estimated due to that
it generally is much simpler and is relatively easier to estimate.
Reflectionaware guidance (RAG) module is then elaborated for better exploiting
the estimated reflection in predicting transmission layer. By incorporating
feature maps from the estimated reflection and observation, RAG can be used (i)
to mitigate the effect of reflection from the observation, and (ii) to generate
mask in partial convolution for mitigating the effect of deviating from linear
combination hypothesis. A dedicated mask loss is further presented for
reconciling the contributions of encoder and decoder features. Experiments on
five commonly used datasets demonstrate the quantitative and qualitative
superiority of our RAGNet in comparison to the state-of-the-art SIRR methods.
The source code and pre-trained model are available at
https://github.com/liyucs/RAGNet.
</p>
<a href="http://arxiv.org/abs/2012.00945" target="_blank">arXiv:2012.00945</a> [<a href="http://arxiv.org/pdf/2012.00945" target="_blank">pdf</a>]

<h2>Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in Large Scenes. (arXiv:2012.00946v1 [cs.CV])</h2>
<h3>Qi Zhang, Antoni B. Chan</h3>
<p>Crowd counting in single-view images has achieved outstanding performance on
existing counting datasets. However, single-view counting is not applicable to
large and wide scenes (e.g., public parks, long subway platforms, or event
spaces) because a single camera cannot capture the whole scene in adequate
detail for counting, e.g., when the scene is too large to fit into the
field-of-view of the camera, too long so that the resolution is too low on
faraway crowds, or when there are too many large objects that occlude large
portions of the crowd. Therefore, to solve the wide-area counting task requires
multiple cameras with overlapping fields-of-view. In this paper, we propose a
deep neural network framework for multi-view crowd counting, which fuses
information from multiple camera views to predict a scene-level density map on
the ground-plane of the 3D world. We consider three versions of the fusion
framework: the late fusion model fuses camera-view density map; the naive early
fusion model fuses camera-view feature maps; and the multi-view multi-scale
early fusion model ensures that features aligned to the same ground-plane point
have consistent scales. A rotation selection module further ensures consistent
rotation alignment of the features. We test our 3 fusion models on 3 multi-view
counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view
counting dataset containing a crowded street intersection. Our methods achieve
state-of-the-art results compared to other multi-view counting baselines.
</p>
<a href="http://arxiv.org/abs/2012.00946" target="_blank">arXiv:2012.00946</a> [<a href="http://arxiv.org/pdf/2012.00946" target="_blank">pdf</a>]

<h2>The Geometry and Kinematics of the Matrix Lie Group $SE_K(3)$. (arXiv:2012.00950v1 [cs.RO])</h2>
<h3>Yarong Luo, Mengyuan Wang, Chi Guo</h3>
<p>Currently state estimation is very important for the robotics, and the
uncertainty representation based Lie group is natural for the state estimation
problem. It is necessary to exploit the geometry and kinematic of matrix Lie
group sufficiently. Therefore, this note gives a detailed derivation of the
recently proposed matrix Lie group $SE_K(3)$ for the first time, our results
extend the results in Barfoot \cite{barfoot2017state}. Then we describe the
situations where this group is suitable for state representation. We also have
developed code based on Matlab framework for quickly implementing and testing.
</p>
<a href="http://arxiv.org/abs/2012.00950" target="_blank">arXiv:2012.00950</a> [<a href="http://arxiv.org/pdf/2012.00950" target="_blank">pdf</a>]

<h2>Ship Detection: Parameter Server Variant. (arXiv:2012.00953v1 [cs.CV])</h2>
<h3>Benjamin Smith</h3>
<p>Deep learning ship detection in satellite optical imagery suffers from false
positive occurrences with clouds, landmasses, and man-made objects that
interfere with correct classification of ships, typically limiting class
accuracy scores to 88\%. This work explores the tensions between customization
strategies, class accuracy rates, training times, and costs in cloud based
solutions. We demonstrate how a custom U-Net can achieve 92\% class accuracy
over a validation dataset and 68\% over a target dataset with 90\% confidence.
We also compare a single node architecture with a parameter server variant
whose workers act as a boosting mechanism. The parameter server variant
outperforms class accuracy on the target dataset reaching 73\% class accuracy
compared to the best single node approach. A comparative investigation on the
systematic performance of the single node and parameter server variant
architectures is discussed with support from empirical findings.
</p>
<a href="http://arxiv.org/abs/2012.00953" target="_blank">arXiv:2012.00953</a> [<a href="http://arxiv.org/pdf/2012.00953" target="_blank">pdf</a>]

<h2>PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization. (arXiv:2012.00972v1 [cs.CV])</h2>
<h3>Guangming Wang, Xinrui Wu, Zhe Liu, Hesheng Wang</h3>
<p>A novel 3D point cloud learning model for deep LiDAR odometry, named
PWCLO-Net, using hierarchical embedding mask optimization is proposed in this
paper. In this model, the Pyramid, Warping, and Cost volume (PWC) structure for
the LiDAR Odometry task is built to hierarchically refine the estimated pose in
a coarse-to-fine approach. An attentive cost volume is built to associate two
point clouds and obtain the embedding motion information. Then, a novel
trainable embedding mask is proposed to weight the cost volume of all points to
the overall pose information and filter outlier points. The estimated current
pose is used to warp the first point cloud to bridge the distance to the second
point cloud, and then the cost volume of the residual motion is built. At the
same time, the embedding mask is optimized hierarchically from coarse to fine
to obtain more accurate filtering information for pose refinement. The pose
warp-refinement process is repeatedly used to make the pose estimation more
robust for outliers. The superior performance and effectiveness of our LiDAR
odometry model are demonstrated on the KITTI odometry dataset. Our method
outperforms all recent learning-based methods and outperforms the
geometry-based approach, LOAM with mapping optimization, on most sequences of
the KITTI odometry dataset.
</p>
<a href="http://arxiv.org/abs/2012.00972" target="_blank">arXiv:2012.00972</a> [<a href="http://arxiv.org/pdf/2012.00972" target="_blank">pdf</a>]

<h2>Learning Vector Quantized Shape Code for Amodal Blastomere Instance Segmentation. (arXiv:2012.00985v1 [cs.CV])</h2>
<h3>Won-Dong Jang, Donglai Wei, Xingxuan Zhang, Brian Leahy, Helen Yang, James Tompkin, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister</h3>
<p>Blastomere instance segmentation is important for analyzing embryos'
abnormality. To measure the accurate shapes and sizes of blastomeres, their
amodal segmentation is necessary. Amodal instance segmentation aims to recover
the complete silhouette of an object even when the object is not fully visible.
For each detected object, previous methods directly regress the target mask
from input features. However, images of an object under different amounts of
occlusion should have the same amodal mask output, which makes it harder to
train the regression model. To alleviate the problem, we propose to classify
input features into intermediate shape codes and recover complete object shapes
from them. First, we pre-train the Vector Quantized Variational Autoencoder
(VQ-VAE) model to learn these discrete shape codes from ground truth amodal
masks. Then, we incorporate the VQ-VAE model into the amodal instance
segmentation pipeline with an additional refinement module. We also detect an
occlusion map to integrate occlusion information with a backbone feature. As
such, our network faithfully detects bounding boxes of amodal objects. On an
internal embryo cell image benchmark, the proposed method outperforms previous
state-of-the-art methods. To show generalizability, we show segmentation
results on the public KINS natural image benchmark. To examine the learned
shape codes and model design choices, we perform ablation studies on a
synthetic dataset of simple overlaid shapes. Our method would enable accurate
measurement of blastomeres in in vitro fertilization (IVF) clinics, which
potentially can increase IVF success rate.
</p>
<a href="http://arxiv.org/abs/2012.00985" target="_blank">arXiv:2012.00985</a> [<a href="http://arxiv.org/pdf/2012.00985" target="_blank">pdf</a>]

<h2>PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds. (arXiv:2012.00987v1 [cs.CV])</h2>
<h3>Yi Wei, Ziyi Wang, Yongming Rao, Jiwen Lu, Jie Zhou</h3>
<p>In this paper, we propose Point-Voxel Recurrent All-Pairs Field Transforms
(PV-RAFT) to estimate scene flow from point clouds. All-pairs correlations play
important roles in scene flow estimation task. However, since point clouds are
irregular and unordered, it is challenging to efficiently extract features from
all-pairs fields in 3D space. To tackle this problem, we present point-voxel
correlation fields, which captures both local and long-range dependencies of
point pairs. To capture point-based correlations, we adopt K-Nearest Neighbors
search that preserves fine-grained information in the local region. By
voxelizing point clouds in a multi-scale manner, a pyramid correlation voxels
are constructed to model long-range correspondences. Integrating two types of
correlations, our PV-RAFT makes use of all-pairs relations to handle both small
and large displacements. We evaluate the proposed method on both synthetic
dataset FlyingThings3D and real scenes dataset KITTI. Experimental results show
that PV-RAFT surpasses state-of-the-art methods by remarkable margins.
</p>
<a href="http://arxiv.org/abs/2012.00987" target="_blank">arXiv:2012.00987</a> [<a href="http://arxiv.org/pdf/2012.00987" target="_blank">pdf</a>]

<h2>On the Error Resistance of Hinge Loss Minimization. (arXiv:2012.00989v1 [cs.LG])</h2>
<h3>Kunal Talwar</h3>
<p>Commonly used classification algorithms in machine learning, such as support
vector machines, minimize a convex surrogate loss on training examples. In
practice, these algorithms are surprisingly robust to errors in the training
data. In this work, we identify a set of conditions on the data under which
such surrogate loss minimization algorithms provably learn the correct
classifier. This allows us to establish, in a unified framework, the robustness
of these algorithms under various models on data as well as error. In
particular, we show that if the data is linearly classifiable with a slightly
non-trivial margin (i.e. a margin at least $C/\sqrt{d}$ for $d$-dimensional
unit vectors), and the class-conditional distributions are near isotropic and
logconcave, then surrogate loss minimization has negligible error on the
uncorrupted data even when a constant fraction of examples are adversarially
mislabeled.
</p>
<a href="http://arxiv.org/abs/2012.00989" target="_blank">arXiv:2012.00989</a> [<a href="http://arxiv.org/pdf/2012.00989" target="_blank">pdf</a>]

<h2>Partially Shared Semi-supervised Deep Matrix Factorization with Multi-view Data. (arXiv:2012.00993v1 [cs.LG])</h2>
<h3>Haonan Huang, Naiyao Liang, Wei Yan, Zuyuan Yang, Weijun Sun</h3>
<p>Since many real-world data can be described from multiple views, multi-view
learning has attracted considerable attention. Various methods have been
proposed and successfully applied to multi-view learning, typically based on
matrix factorization models. Recently, it is extended to the deep structure to
exploit the hierarchical information of multi-view data, but the view-specific
features and the label information are seldom considered. To address these
concerns, we present a partially shared semi-supervised deep matrix
factorization model (PSDMF). By integrating the partially shared deep
decomposition structure, graph regularization and the semi-supervised
regression model, PSDMF can learn a compact and discriminative representation
through eliminating the effects of uncorrelated information. In addition, we
develop an efficient iterative updating algorithm for PSDMF. Extensive
experiments on five benchmark datasets demonstrate that PSDMF can achieve
better performance than the state-of-the-art multi-view learning approaches.
The MATLAB source code is available at
https://github.com/libertyhhn/PartiallySharedDMF.
</p>
<a href="http://arxiv.org/abs/2012.00993" target="_blank">arXiv:2012.00993</a> [<a href="http://arxiv.org/pdf/2012.00993" target="_blank">pdf</a>]

<h2>An Once-for-All Budgeted Pruning Framework for ConvNets Considering Input Resolution. (arXiv:2012.00996v1 [cs.CV])</h2>
<h3>Wenyu Sun, Jian Cao, Pengtao Xu, Xiangcheng Liu, Pu Li</h3>
<p>We propose an efficient once-for-all budgeted pruning framework (OFARPruning)
to find many compact network structures close to winner tickets in the early
training stage considering the effect of input resolution during the pruning
process. In structure searching stage, we utilize cosine similarity to measure
the similarity of the pruning mask to get high-quality network structures with
low energy and time consumption. After structure searching stage, our proposed
method randomly sample the compact structures with different pruning rates and
input resolution to achieve joint optimization. Ultimately, we can obtain a
cohort of compact networks adaptive to various resolution to meet dynamic FLOPs
constraints on different edge devices with only once training. The experiments
based on image classification and object detection show that OFARPruning has a
higher accuracy than the once-for-all compression methods such as US-Net and
MutualNet (1-2% better with less FLOPs), and achieve the same even higher
accuracy as the conventional pruning methods (72.6% vs. 70.5% on MobileNetv2
under 170 MFLOPs) with much higher efficiency.
</p>
<a href="http://arxiv.org/abs/2012.00996" target="_blank">arXiv:2012.00996</a> [<a href="http://arxiv.org/pdf/2012.00996" target="_blank">pdf</a>]

<h2>q-SNE: Visualizing Data using q-Gaussian Distributed Stochastic Neighbor Embedding. (arXiv:2012.00999v1 [cs.CV])</h2>
<h3>Motoshi Abe, Junichi Miyao, Takio Kurita</h3>
<p>The dimensionality reduction has been widely introduced to use the
high-dimensional data for regression, classification, feature analysis, and
visualization. As the one technique of dimensionality reduction, a stochastic
neighbor embedding (SNE) was introduced. The SNE leads powerful results to
visualize high-dimensional data by considering the similarity between the local
Gaussian distributions of high and low-dimensional space. To improve the SNE, a
t-distributed stochastic neighbor embedding (t-SNE) was also introduced. To
visualize high-dimensional data, the t-SNE leads to more powerful and flexible
visualization on 2 or 3-dimensional mapping than the SNE by using a
t-distribution as the distribution of low-dimensional data. Recently, Uniform
manifold approximation and projection (UMAP) is proposed as a dimensionality
reduction technique. We present a novel technique called a q-Gaussian
distributed stochastic neighbor embedding (q-SNE). The q-SNE leads to more
powerful and flexible visualization on 2 or 3-dimensional mapping than the
t-SNE and the SNE by using a q-Gaussian distribution as the distribution of
low-dimensional data. The q-Gaussian distribution includes the Gaussian
distribution and the t-distribution as the special cases with q=1.0 and q=2.0.
Therefore, the q-SNE can also express the t-SNE and the SNE by changing the
parameter q, and this makes it possible to find the best visualization by
choosing the parameter q. We show the performance of q-SNE as visualization on
2-dimensional mapping and classification by k-Nearest Neighbors (k-NN)
classifier in embedded space compared with SNE, t-SNE, and UMAP by using the
datasets MNIST, COIL-20, OlivettiFaces, FashionMNIST, and Glove.
</p>
<a href="http://arxiv.org/abs/2012.00999" target="_blank">arXiv:2012.00999</a> [<a href="http://arxiv.org/pdf/2012.00999" target="_blank">pdf</a>]

<h2>Artist, Style And Year Classification Using Face Recognition And Clustering With Convolutional Neural Networks. (arXiv:2012.01009v1 [cs.CV])</h2>
<h3>Doruk Pancaroglu</h3>
<p>Artist, year and style classification of fine-art paintings are generally
achieved using standard image classification methods, image segmentation, or
more recently, convolutional neural networks (CNNs). This works aims to use
newly developed face recognition methods such as FaceNet that use CNNs to
cluster fine-art paintings using the extracted faces in the paintings, which
are found abundantly. A dataset consisting of over 80,000 paintings from over
1000 artists is chosen, and three separate face recognition and clustering
tasks are performed. The produced clusters are analyzed by the file names of
the paintings and the clusters are named by their majority artist, year range,
and style. The clusters are further analyzed and their performance metrics are
calculated. The study shows promising results as the artist, year, and styles
are clustered with an accuracy of 58.8, 63.7, and 81.3 percent, while the
clusters have an average purity of 63.1, 72.4, and 85.9 percent.
</p>
<a href="http://arxiv.org/abs/2012.01009" target="_blank">arXiv:2012.01009</a> [<a href="http://arxiv.org/pdf/2012.01009" target="_blank">pdf</a>]

<h2>Driving-Policy Adaptive Safeguard for Autonomous Vehicles Using Reinforcement Learning. (arXiv:2012.01010v1 [cs.RO])</h2>
<h3>Zhong Cao, Shaobing Xu, Songan Zhang, Huei Peng, Diange Yang</h3>
<p>Safeguard functions such as those provided by advanced emergency braking
(AEB) can provide another layer of safety for autonomous vehicles (AV). A smart
safeguard function should adapt the activation conditions to the driving
policy, to avoid unnecessary interventions as well as improve vehicle safety.
This paper proposes a driving-policy adaptive safeguard (DPAS) design,
including a collision avoidance strategy and an activation function. The
collision avoidance strategy is designed in a reinforcement learning framework,
obtained by Monte-Carlo Tree Search (MCTS). It can learn from past collisions
and manipulate both braking and steering in stochastic traffics. The
driving-policy adaptive activation function should dynamically assess current
driving policy risk and kick in when an urgent threat is detected. To generate
this activation function, MCTS' exploration and rollout modules are designed to
fully evaluate the AV's current driving policy, and then explore other safer
actions. In this study, the DPAS is validated with two typical highway-driving
policies. The results are obtained through and 90,000 times in the stochastic
and aggressive simulated traffic. The results are calibrated by naturalistic
driving data and show that the proposed safeguard reduces the collision rate
significantly without introducing more interventions, compared with the
state-based benchmark safeguards. In summary, the proposed safeguard leverages
the learning-based method in stochastic and emergent scenarios and imposes
minimal influence on the driving policy.
</p>
<a href="http://arxiv.org/abs/2012.01010" target="_blank">arXiv:2012.01010</a> [<a href="http://arxiv.org/pdf/2012.01010" target="_blank">pdf</a>]

<h2>Information Theory in Density Destructors. (arXiv:2012.01012v1 [stat.ML])</h2>
<h3>J. Emmanuel Johnson, Valero Laparra, Gustau Camps-Valls, Raul Santos-Rodr&#xed;guez, Jes&#xfa;s Malo</h3>
<p>Density destructors are differentiable and invertible transforms that map
multivariate PDFs of arbitrary structure (low entropy) into non-structured PDFs
(maximum entropy). Multivariate Gaussianization and multivariate equalization
are specific examples of this family, which break down the complexity of the
original PDF through a set of elementary transforms that progressively remove
the structure of the data. We demonstrate how this property of density
destructive flows is connected to classical information theory, and how density
destructors can be used to get more accurate estimates of information theoretic
quantities. Experiments with total correlation and mutual information
inmultivariate sets illustrate the ability of density destructors compared to
competing methods. These results suggest that information theoretic measures
may be an alternative optimization criteria when learning density destructive
flows.
</p>
<a href="http://arxiv.org/abs/2012.01012" target="_blank">arXiv:2012.01012</a> [<a href="http://arxiv.org/pdf/2012.01012" target="_blank">pdf</a>]

<h2>CORRIDRONE: Corridors for Drones, An Adaptive On-Demand Multi-Lane Design and Testbed. (arXiv:2012.01019v1 [cs.RO])</h2>
<h3>Lima Agnel Tony, Ashwini Ratnoo, Debasish Ghose</h3>
<p>In this article, a novel drone skyway framework called CORRIDRONE is
proposed. As the name suggests, this represents virtual air corridors for
point-to-point safe passage of multiple drones. The corridors are not permanent
but can be set up on demand. A few such scenarios could be those in
warehouse/factory floors, package delivery, shore-to-ship delivery, border
patrol, etc. Several factors play major roles in the planning and design of
such aerial passages. The proposed framework includes many novel features which
aid safe and efficient integration of UAVs into the airspace with already
available technologies. A several kilometres long test bed is proposed to be
set-up at the 1500 acres Challekere campus of Indian Institute of Science, in
the state of Karnataka, to design and test the infrastructure required for
CORRIDRONE.
</p>
<a href="http://arxiv.org/abs/2012.01019" target="_blank">arXiv:2012.01019</a> [<a href="http://arxiv.org/pdf/2012.01019" target="_blank">pdf</a>]

<h2>Proceedings of NeurIPS 2019 Workshop on Artificial Intelligence for Humanitarian Assistance and Disaster Response. (arXiv:2012.01022v1 [cs.AI])</h2>
<h3>Ritwik Gupta, Eric T. Heim</h3>
<p>This is the proceedings of the 1st AI + HADR workshop which was held in
Vancouver, Canada on December 13, 2019 as part of the Neural Information
Processing Systems conference.
</p>
<a href="http://arxiv.org/abs/2012.01022" target="_blank">arXiv:2012.01022</a> [<a href="http://arxiv.org/pdf/2012.01022" target="_blank">pdf</a>]

<h2>Leveraging Neural Network Gradients within Trajectory Optimization for Proactive Human-Robot Interactions. (arXiv:2012.01027v1 [cs.RO])</h2>
<h3>Simon Schaefer, Karen Leung, Boris Ivanovic, Marco Pavone</h3>
<p>To achieve seamless human-robot interactions, robots need to intimately
reason about complex interaction dynamics and future human behaviors within
their motion planning process. However, there is a disconnect between
state-of-the-art neural network-based human behavior models and robot motion
planners -- either the behavior models are limited in their consideration of
downstream planning or a simplified behavior model is used to ensure
tractability of the planning problem. In this work, we present a framework that
fuses together the interpretability and flexibility of trajectory optimization
(TO) with the predictive power of state-of-the-art human trajectory prediction
models. In particular, we leverage gradient information from data-driven
prediction models to explicitly reason about human-robot interaction dynamics
within a gradient-based TO problem. We demonstrate the efficacy of our approach
in a multi-agent scenario whereby a robot is required to safely and efficiently
navigate through a crowd of up to ten pedestrians. We compare against a variety
of planning methods, and show that by explicitly accounting for interaction
dynamics within the planner, our method offers safer and more efficient
behaviors, even yielding proactive and nuanced behaviors such as waiting for a
pedestrian to pass before moving.
</p>
<a href="http://arxiv.org/abs/2012.01027" target="_blank">arXiv:2012.01027</a> [<a href="http://arxiv.org/pdf/2012.01027" target="_blank">pdf</a>]

<h2>MAAD-Face: A Massively Annotated Attribute Dataset for Face Images. (arXiv:2012.01030v1 [cs.CV])</h2>
<h3>Philipp Terh&#xf6;rst, Daniel F&#xe4;hrmann, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper</h3>
<p>Soft-biometrics play an important role in face biometrics and related fields
since these might lead to biased performances, threatens the user's privacy, or
are valuable for commercial aspects. Current face databases are specifically
constructed for the development of face recognition applications. Consequently,
these databases contain large amount of face images but lack in the number of
attribute annotations and the overall annotation correctness. In this work, we
propose MAADFace, a new face annotations database that is characterized by the
large number of its high-quality attribute annotations. MAADFace is build on
the VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals.
Using a novel annotation transfer-pipeline that allows an accurate
label-transfer from multiple source-datasets to a target-dataset, MAAD-Face
consists of 123.9M attribute annotations of 47 different binary attributes.
Consequently, it provides 15 and 137 times more attribute labels than CelebA
and LFW. Our investigation on the annotation quality by three human evaluators
demonstrated the superiority of the MAAD-Face annotations over existing
databases. Additionally, we make use of the large amount of high-quality
annotations from MAAD-Face to study the viability of soft-biometrics for
recognition, providing insights about which attributes support genuine and
imposter decisions. The MAAD-Face annotations dataset is publicly available.
</p>
<a href="http://arxiv.org/abs/2012.01030" target="_blank">arXiv:2012.01030</a> [<a href="http://arxiv.org/pdf/2012.01030" target="_blank">pdf</a>]

<h2>Biomedical Knowledge Graph Refinement with Embedding and Logic Rules. (arXiv:2012.01031v1 [cs.AI])</h2>
<h3>Sendong Zhao, Bing Qin, Ting Liu, Fei Wang</h3>
<p>Currently, there is a rapidly increasing need for high-quality biomedical
knowledge graphs (BioKG) that provide direct and precise biomedical knowledge.
In the context of COVID-19, this issue is even more necessary to be
highlighted. However, most BioKG construction inevitably includes numerous
conflicts and noises deriving from incorrect knowledge descriptions in
literature and defective information extraction techniques. Many studies have
demonstrated that reasoning upon the knowledge graph is effective in
eliminating such conflicts and noises. This paper proposes a method BioGRER to
improve the BioKG's quality, which comprehensively combines the knowledge graph
embedding and logic rules that support and negate triplets in the BioKG. In the
proposed model, the BioKG refinement problem is formulated as the probability
estimation for triplets in the BioKG. We employ the variational EM algorithm to
optimize knowledge graph embedding and logic rule inference alternately. In
this way, our model could combine efforts from both the knowledge graph
embedding and logic rules, leading to better results than using them alone. We
evaluate our model over a COVID-19 knowledge graph and obtain competitive
results.
</p>
<a href="http://arxiv.org/abs/2012.01031" target="_blank">arXiv:2012.01031</a> [<a href="http://arxiv.org/pdf/2012.01031" target="_blank">pdf</a>]

<h2>Fast Automatic Feature Selection for Multi-Period Sliding Window Aggregate in Time Series. (arXiv:2012.01037v1 [cs.LG])</h2>
<h3>Rui An, Xingtian Shi, Baohan Xu</h3>
<p>As one of the most well-known artificial feature sampler, the sliding window
is widely used in scenarios where spatial and temporal information exists, such
as computer vision, natural language process, data stream, and time series.
Among which time series is common in many scenarios like credit card payment,
user behavior, and sensors. General feature selection for features extracted by
sliding window aggregate calls for time-consuming iteration to generate
features, and then traditional feature selection methods are employed to rank
them. The decision of key parameter, i.e. the period of sliding windows,
depends on the domain knowledge and calls for trivial. Currently, there is no
automatic method to handle the sliding window aggregate features selection. As
the time consumption of feature generation with different periods and sliding
windows is huge, it is very hard to enumerate them all and then select them.

In this paper, we propose a general framework using Markov Chain to solve
this problem. This framework is very efficient and has high accuracy, such that
it is able to perform feature selection on a variety of features and period
options. We show the detail by 2 common sliding windows and 3 types of
aggregation operators. And it is easy to extend more sliding windows and
aggregation operators in this framework by employing existing theory about
Markov Chain.
</p>
<a href="http://arxiv.org/abs/2012.01037" target="_blank">arXiv:2012.01037</a> [<a href="http://arxiv.org/pdf/2012.01037" target="_blank">pdf</a>]

<h2>A Photogrammetry-based Framework to Facilitate Image-based Modeling and Automatic Camera Tracking. (arXiv:2012.01044v1 [cs.CV])</h2>
<h3>Sebastian Bullinger, Christoph Bodensteiner, Michael Arens</h3>
<p>We propose a framework that extends Blender to exploit Structure from Motion
(SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks
such as sculpting or camera and motion tracking. Applying SfM allows us to
determine camera motions without manually defining feature tracks or
calibrating the cameras used to capture the image data. With MVS we are able to
automatically compute dense scene models, which is not feasible with the
built-in tools of Blender. Currently, our framework supports several
state-of-the-art SfM and MVS pipelines. The modular system design enables us to
integrate further approaches without additional effort. The framework is
publicly available as an open source software package.
</p>
<a href="http://arxiv.org/abs/2012.01044" target="_blank">arXiv:2012.01044</a> [<a href="http://arxiv.org/pdf/2012.01044" target="_blank">pdf</a>]

<h2>Learning Universal Shape Dictionary for Realtime Instance Segmentation. (arXiv:2012.01050v1 [cs.CV])</h2>
<h3>Tutian Tang, Wenqiang Xu, Ruolin Ye, Lixin Yang, Cewu Lu</h3>
<p>We present a novel explicit shape representation for instance segmentation.

Based on how to model the object shape, current instance segmentation systems
can be divided into two categories, implicit and explicit models. The implicit
methods, which represent the object mask/contour by intractable network
parameters, and produce it through pixel-wise classification, are predominant.
However, the explicit methods, which parameterize the shape with simple and
explainable models, are less explored. Since the operations to generate the
final shape are light-weighted, the explicit methods have a clear speed
advantage over implicit methods, which is crucial for real-world applications.
The proposed USD-Seg adopts a linear model, sparse coding with dictionary, for
object shapes.

First, it learns a dictionary from a large collection of shape datasets,
making any shape being able to be decomposed into a linear combination through
the dictionary.

Hence the name "Universal Shape Dictionary".

Then it adds a simple shape vector regression head to ordinary object
detector, giving the detector segmentation ability with minimal overhead.

For quantitative evaluation, we use both average precision (AP) and the
proposed Efficiency of AP (AP$_E$) metric, which intends to also measure the
computational consumption of the framework to cater to the requirements of
real-world applications. We report experimental results on the challenging COCO
dataset, in which our single model on a single Titan Xp GPU achieves 35.8 AP
and 27.8 AP$_E$ at 65 fps with YOLOv4 as base detector, 34.1 AP and 28.6 AP$_E$
at 12 fps with FCOS as base detector.
</p>
<a href="http://arxiv.org/abs/2012.01050" target="_blank">arXiv:2012.01050</a> [<a href="http://arxiv.org/pdf/2012.01050" target="_blank">pdf</a>]

<h2>A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v1 [cs.CV])</h2>
<h3>Quentin Paletta, Joan Lasenby</h3>
<p>Improving irradiance forecasting is critical to further increase the share of
solar in the energy mix. On a short time scale, fish-eye cameras on the ground
are used to capture cloud displacements causing the local variability of the
electricity production. As most of the solar radiation comes directly from the
Sun, current forecasting approaches use its position in the image as a
reference to interpret the cloud cover dynamics. However, existing Sun tracking
methods rely on external data and a calibration of the camera, which requires
access to the device. To address these limitations, this study introduces an
image-based Sun tracking algorithm to localise the Sun in the image when it is
visible and interpolate its daily trajectory from past observations. We
validate the method on a set of sky images collected over a year at SIRTA's
lab. Experimental results show that the proposed method provides robust smooth
Sun trajectories with a mean absolute error below 1% of the image size.
</p>
<a href="http://arxiv.org/abs/2012.01059" target="_blank">arXiv:2012.01059</a> [<a href="http://arxiv.org/pdf/2012.01059" target="_blank">pdf</a>]

<h2>About contrastive unsupervised representation learning for classification and its convergence. (arXiv:2012.01064v1 [cs.LG])</h2>
<h3>Ibrahim Merad, Yiyang Yu, Emmanuel Bacry, St&#xe9;phane Ga&#xef;ffas</h3>
<p>Contrastive representation learning has been recently proved to be very
efficient for self-supervised training. These methods have been successfully
used to train encoders which perform comparably to supervised training on
downstream classification tasks. A few works have started to build a
theoretical framework around contrastive learning in which guarantees for its
performance can be proven. We provide extensions of these results to training
with multiple negative samples and for multiway classification. Furthermore, we
provide convergence guarantees for the minimization of the contrastive training
error with gradient descent of an overparametrized deep neural encoder, and
provide some numerical experiments that complement our theoretical findings
</p>
<a href="http://arxiv.org/abs/2012.01064" target="_blank">arXiv:2012.01064</a> [<a href="http://arxiv.org/pdf/2012.01064" target="_blank">pdf</a>]

<h2>FIT: a Fast and Accurate Framework for Solving Medical Inquiring and Diagnosing Tasks. (arXiv:2012.01065v1 [cs.LG])</h2>
<h3>Weijie He, Xiaohao Mao, Chao Ma, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Ting Chen</h3>
<p>Automatic self-diagnosis provides low-cost and accessible healthcare via an
agent that queries the patient and makes predictions about possible diseases.
From a machine learning perspective, symptom-based self-diagnosis can be viewed
as a sequential feature selection and classification problem. Reinforcement
learning methods have shown good performance in this task but often suffer from
large search spaces and costly training. To address these problems, we propose
a competitive framework, called FIT, which uses an information-theoretic reward
to determine what data to collect next. FIT improves over previous
information-based approaches by using a multimodal variational autoencoder
(MVAE) model and a two-step sampling strategy for disease prediction.
Furthermore, we propose novel methods to substantially reduce the computational
cost of FIT to a level that is acceptable for practical online self-diagnosis.
Our results in two simulated datasets show that FIT can effectively deal with
large search space problems, outperforming existing baselines. Moreover, using
two medical datasets, we show that FIT is a competitive alternative in
real-world settings.
</p>
<a href="http://arxiv.org/abs/2012.01065" target="_blank">arXiv:2012.01065</a> [<a href="http://arxiv.org/pdf/2012.01065" target="_blank">pdf</a>]

<h2>Tagging Real-World Scenarios for the Assessment of Autonomous Vehicles. (arXiv:2012.01081v1 [cs.RO])</h2>
<h3>Erwin de Gelder, Olaf Op den Camp</h3>
<p>The development of Autonomous Vehicles (AVs) has made significant progress in
the last years. An essential aspect in the development of AVs is the assessment
of quality and performance aspects of the AVs, such as safety, comfort, and
efficiency. Among other methods, a scenario-based approach has been proposed.
With scenario-based testing, the AV is subjected to a collection of scenarios
that represent real-world situations.

The collection of scenarios needs to cover the variety of what an AV can
encounter in real traffic. As a result, many different scenarios are
considered, that are grouped into so-called scenario categories. We propose a
method for defining the scenario categories using a system of tags, where each
tag describes a particular characteristic of a scenario category.

There is a balance between having generic scenario categories - very specific
set of scenarios, while for another system one might be interested in a set of
scenarios with a high variety. To accommodate this, tags are structured in
trees. The different layers of the trees can be regarded as different
abstraction levels.

Next to presenting the method for describing scenario categories using tags,
we will illustrate the method by showing applicable trees of tags using
concrete examples in the Singapore traffic system. Trees of tags are shown for
the vehicle under test, the dynamic environment (e.g., the other road users),
the static environment (e.g., the road layout), and the environmental
conditions (weather and lighting conditions). Few examples are presented to
illustrate the proposed method for defining the scenario categories using tags.
</p>
<a href="http://arxiv.org/abs/2012.01081" target="_blank">arXiv:2012.01081</a> [<a href="http://arxiv.org/pdf/2012.01081" target="_blank">pdf</a>]

<h2>Aligning Hyperbolic Representations: an Optimal Transport-based approach. (arXiv:2012.01089v1 [stat.ML])</h2>
<h3>Andr&#xe9;s Hoyos-Idrobo</h3>
<p>Hyperbolic-spaces are better suited to represent data with underlying
hierarchical relationships, e.g., tree-like data. However, it is often
necessary to incorporate, through alignment, different but related
representations meaningfully. This aligning is an important class of machine
learning problems, with applications as ontology matching and cross-lingual
alignment. Optimal transport (OT)-based approaches are a natural choice to
tackle the alignment problem as they aim to find a transformation of the source
dataset to match a target dataset, subject to some distribution constraints.
This work proposes a novel approach based on OT of embeddings on the Poincar\'e
model of hyperbolic spaces. Our method relies on the gyrobarycenter mapping on
M\"obius gyrovector spaces. As a result of this formalism, we derive extensions
to some existing Euclidean methods of OT-based domain adaptation to their
hyperbolic counterparts. Empirically, we show that both Euclidean and
hyperbolic methods have similar performances in the context of retrieval.
</p>
<a href="http://arxiv.org/abs/2012.01089" target="_blank">arXiv:2012.01089</a> [<a href="http://arxiv.org/pdf/2012.01089" target="_blank">pdf</a>]

<h2>PlueckerNet: Learn to Register 3D Line Reconstructions. (arXiv:2012.01096v1 [cs.CV])</h2>
<h3>Liu Liu, Hongdong Li, Haodong Yao, Ruyi Zha</h3>
<p>Aligning two partially-overlapped 3D line reconstructions in Euclidean space
is challenging, as we need to simultaneously solve correspondences and relative
pose between line reconstructions. This paper proposes a neural network based
method and it has three modules connected in sequence: (i) a Multilayer
Perceptron (MLP) based network takes Pluecker representations of lines as
inputs, to extract discriminative line-wise features and matchabilities (how
likely each line is going to have a match), (ii) an Optimal Transport (OT)
layer takes two-view line-wise features and matchabilities as inputs to
estimate a 2D joint probability matrix, with each item describes the matchness
of a line pair, and (iii) line pairs with Top-K matching probabilities are fed
to a 2-line minimal solver in a RANSAC framework to estimate a six
Degree-of-Freedom (6-DoF) rigid transformation. Experiments on both indoor and
outdoor datasets show that the registration (rotation and translation)
precision of our method outperforms baselines significantly.
</p>
<a href="http://arxiv.org/abs/2012.01096" target="_blank">arXiv:2012.01096</a> [<a href="http://arxiv.org/pdf/2012.01096" target="_blank">pdf</a>]

<h2>Self-correcting Q-Learning. (arXiv:2012.01100v1 [cs.LG])</h2>
<h3>Rong Zhu, Mattia Rigotti</h3>
<p>The Q-learning algorithm is known to be affected by the maximization bias,
i.e. the systematic overestimation of action values, an important issue that
has recently received renewed attention. Double Q-learning has been proposed as
an efficient algorithm to mitigate this bias. However, this comes at the price
of an underestimation of action values, in addition to increased memory
requirements and a slower convergence. In this paper, we introduce a new way to
address the maximization bias in the form of a "self-correcting algorithm" for
approximating the maximum of an expected value. Our method balances the
overestimation of the single estimator used in conventional Q-learning and the
underestimation of the double estimator used in Double Q-learning. Applying
this strategy to Q-learning results in Self-correcting Q-learning. We show
theoretically that this new algorithm enjoys the same convergence guarantees as
Q-learning while being more accurate. Empirically, it performs better than
Double Q-learning in domains with rewards of high variance, and it even attains
faster convergence than Q-learning in domains with rewards of zero or low
variance. These advantages transfer to a Deep Q Network implementation that we
call Self-correcting DQN and which outperforms regular DQN and Double DQN on
several tasks in the Atari 2600 domain.
</p>
<a href="http://arxiv.org/abs/2012.01100" target="_blank">arXiv:2012.01100</a> [<a href="http://arxiv.org/pdf/2012.01100" target="_blank">pdf</a>]

<h2>Multi-Objective Optimization of the Textile Manufacturing Process Using Deep-Q-Network Based Multi-Agent Reinforcement Learning. (arXiv:2012.01101v1 [cs.AI])</h2>
<h3>Zhenglei He, Kim Phuc Tran (GEMTEX), Sebastien Thomassey, Xianyi Zeng, Jie Xu, Changhai Yi</h3>
<p>Multi-objective optimization of the textile manufacturing process is an
increasing challenge because of the growing complexity involved in the
development of the textile industry. The use of intelligent techniques has been
often discussed in this domain, although a significant improvement from certain
successful applications has been reported, the traditional methods failed to
work with high-as well as human intervention. Upon which, this paper proposed a
multi-agent reinforcement learning (MARL) framework to transform the
optimization process into a stochastic game and introduced the deep Q-networks
algorithm to train the multiple agents. A utilitarian selection mechanism was
employed in the stochastic game, which (-greedy policy) in each state to avoid
the interruption of multiple equilibria and achieve the correlated equilibrium
optimal solutions of the optimizing process. The case study result reflects
that the proposed MARL system is possible to achieve the optimal solutions for
the textile ozonation process and it performs better than the traditional
approaches.
</p>
<a href="http://arxiv.org/abs/2012.01101" target="_blank">arXiv:2012.01101</a> [<a href="http://arxiv.org/pdf/2012.01101" target="_blank">pdf</a>]

<h2>Efficient Depth Completion Using Learned Bases. (arXiv:2012.01110v1 [cs.CV])</h2>
<h3>Yiran Zhong, Yuchao Dai, Hongdong Li</h3>
<p>In this paper, we propose a new global geometry constraint for depth
completion. By assuming depth maps often lay on low dimensional subspaces, a
dense depth map can be approximated by a weighted sum of full-resolution
principal depth bases. The principal components of depth fields can be learned
from natural depth maps. The given sparse depth points are served as a data
term to constrain the weighting process. When the input depth points are too
sparse, the recovered dense depth maps are often over smoothed. To address this
issue, we add a colour-guided auto-regression model as another regularization
term. It assumes the reconstructed depth maps should share the same nonlocal
similarity in the accompanying colour image. Our colour-guided PCA depth
completion method has closed-form solutions, thus can be efficiently solved and
is significantly more accurate than PCA only method. Extensive experiments on
KITTI and Middlebury datasets demonstrate the superior performance of our
proposed method.
</p>
<a href="http://arxiv.org/abs/2012.01110" target="_blank">arXiv:2012.01110</a> [<a href="http://arxiv.org/pdf/2012.01110" target="_blank">pdf</a>]

<h2>Parallel Scheduling Self-attention Mechanism: Generalization and Optimization. (arXiv:2012.01114v1 [cs.LG])</h2>
<h3>Mingfei Yu, Masahiro Fujita</h3>
<p>Over the past few years, self-attention is shining in the field of deep
learning, especially in the domain of natural language processing(NLP). Its
impressive effectiveness, along with ubiquitous implementations, have aroused
our interest in efficiently scheduling the data-flow of corresponding
computations onto architectures with many computing units to realize parallel
computing. In this paper, based on the theory of self-attention mechanism and
state-of-the-art realization of self-attention in language models, we propose a
general scheduling algorithm, which is derived from the optimum scheduling for
small instances solved by a satisfiability checking(SAT) solver, to parallelize
typical computations of self-attention. Strategies for further optimization on
skipping redundant computations are put forward as well, with which reductions
of almost 25% and 50% of the original computations are respectively achieved
for two widely-adopted application schemes of self-attention. With the proposed
optimization adopted, we have correspondingly come up with another two
scheduling algorithms. The proposed algorithms are applicable regardless of
problem sizes, as long as the number of input vectors is divisible to the
number of computing units available in the architecture. Due to the complexity
of proving the correctness of the algorithms mathematically for general cases,
we have conducted experiments to reveal their validity, together with the
superior quality of the solutions provided by which, by solving SAT problems
for particular instances.
</p>
<a href="http://arxiv.org/abs/2012.01114" target="_blank">arXiv:2012.01114</a> [<a href="http://arxiv.org/pdf/2012.01114" target="_blank">pdf</a>]

<h2>Neural Teleportation. (arXiv:2012.01118v1 [cs.LG])</h2>
<h3>Marco Armenta, Thierry Judge, Nathan Painchaud, Youssef Skandarani, Carl Lemaire, Gabriel Gibeau Sanchez, Philippe Spino, Pierre-Marc Jodoin</h3>
<p>In this paper, we explore a process called neural teleportation, a
mathematical consequence of applying quiver representation theory to neural
networks. Neural teleportation "teleports" a network to a new position in the
weight space, while leaving its function unchanged. This concept generalizes
the notion of positive scale invariance of ReLU networks to any network with
any activation functions and any architecture. In this paper, we shed light on
surprising and counter-intuitive consequences neural teleportation has on the
loss landscape. In particular, we show that teleportation can be used to
explore loss level curves, that it changes the loss landscape, sharpens global
minima and boosts back-propagated gradients. From these observations, we
demonstrate that teleportation accelerates training when used during
initialization regardless of the model, its activation function, the loss
function, and the training data. Our results can be reproduced with the code
available here: https://github.com/vitalab/neuralteleportation.
</p>
<a href="http://arxiv.org/abs/2012.01118" target="_blank">arXiv:2012.01118</a> [<a href="http://arxiv.org/pdf/2012.01118" target="_blank">pdf</a>]

<h2>Algebraically-Informed Deep Networks (AIDN): A Deep Learning Approach to Represent Algebraic Structures. (arXiv:2012.01141v1 [cs.LG])</h2>
<h3>Mustafa Hajij, Ghada Zamzmi, Matthew Dawson, Greg Muller</h3>
<p>One of the central problems in the interface of deep learning and mathematics
is that of building learning systems that can automatically uncover underlying
mathematical laws from observed data. In this work, we make one step towards
building a bridge between algebraic structures and deep learning, and introduce
\textbf{AIDN}, \textit{Algebraically-Informed Deep Networks}. \textbf{AIDN} is
a deep learning algorithm to represent any finitely-presented algebraic object
with a set of deep neural networks. The deep networks obtained via
\textbf{AIDN} are \textit{algebraically-informed} in the sense that they
satisfy the algebraic relations of the presentation of the algebraic structure
that serves as the input to the algorithm. Our proposed network can robustly
compute linear and non-linear representations of most finitely-presented
algebraic structures such as groups, associative algebras, and Lie algebras. We
evaluate our proposed approach and demonstrate its applicability to algebraic
and geometric objects that are significant in low-dimensional topology. In
particular, we study solutions for the Yang-Baxter equations and their
applications on braid groups. Further, we study the representations of the
Temperley-Lieb algebra. Finally, we show, using the Reshetikhin-Turaev
construction, how our proposed deep learning approach can be utilized to
construct new link invariants. We believe the proposed approach would tread a
path toward a promising future research in deep learning applied to algebraic
and geometric structures.
</p>
<a href="http://arxiv.org/abs/2012.01141" target="_blank">arXiv:2012.01141</a> [<a href="http://arxiv.org/pdf/2012.01141" target="_blank">pdf</a>]

<h2>Single-Shot Freestyle Dance Reenactment. (arXiv:2012.01158v1 [cs.CV])</h2>
<h3>Oran Gafni, Oron Ashual, Lior Wolf</h3>
<p>The task of motion transfer between a source dancer and a target person is a
special case of the pose transfer problem, in which the target person changes
their pose in accordance with the motions of the dancer.

In this work, we propose a novel method that can reanimate a single image by
arbitrary video sequences, unseen during training. The method combines three
networks: (i) a segmentation-mapping network, (ii) a realistic frame-rendering
network, and (iii) a face refinement network. By separating this task into
three stages, we are able to attain a novel sequence of realistic frames,
capturing natural motion and appearance. Our method obtains significantly
better visual quality than previous methods and is able to animate diverse body
types and appearances, which are captured in challenging poses, as shown in the
experiments and supplementary video.
</p>
<a href="http://arxiv.org/abs/2012.01158" target="_blank">arXiv:2012.01158</a> [<a href="http://arxiv.org/pdf/2012.01158" target="_blank">pdf</a>]

<h2>Improving Interpretability in Medical Imaging Diagnosis using Adversarial Training. (arXiv:2012.01166v1 [cs.LG])</h2>
<h3>Andrei Margeloiu, Nikola Simidjievski, Mateja Jamnik, Adrian Weller</h3>
<p>We investigate the influence of adversarial training on the interpretability
of convolutional neural networks (CNNs), specifically applied to diagnosing
skin cancer. We show that gradient-based saliency maps of adversarially trained
CNNs are significantly sharper and more visually coherent than those of
standardly trained CNNs. Furthermore, we show that adversarially trained
networks highlight regions with significant color variation within the lesion,
a common characteristic of melanoma. We find that fine-tuning a robust network
with a small learning rate further improves saliency maps' sharpness. Lastly,
we provide preliminary work suggesting that robustifying the first layers to
extract robust low-level features leads to visually coherent explanations.
</p>
<a href="http://arxiv.org/abs/2012.01166" target="_blank">arXiv:2012.01166</a> [<a href="http://arxiv.org/pdf/2012.01166" target="_blank">pdf</a>]

<h2>Sparse Convolutions on Continuous Domains for Point Cloud and Event Stream Networks. (arXiv:2012.01170v1 [cs.CV])</h2>
<h3>Dominic Jack, Frederic Maire, Simon Denman, Anders Eriksson</h3>
<p>Image convolutions have been a cornerstone of a great number of deep learning
advances in computer vision. The research community is yet to settle on an
equivalent operator for sparse, unstructured continuous data like point clouds
and event streams however. We present an elegant sparse matrix-based
interpretation of the convolution operator for these cases, which is consistent
with the mathematical definition of convolution and efficient during training.
On benchmark point cloud classification problems we demonstrate networks built
with these operations can train an order of magnitude or more faster than top
existing methods, whilst maintaining comparable accuracy and requiring a tiny
fraction of the memory. We also apply our operator to event stream processing,
achieving state-of-the-art results on multiple tasks with streams of hundreds
of thousands of events.
</p>
<a href="http://arxiv.org/abs/2012.01170" target="_blank">arXiv:2012.01170</a> [<a href="http://arxiv.org/pdf/2012.01170" target="_blank">pdf</a>]

<h2>Assessing the Influencing Factors on the Accuracy of Underage Facial Age Estimation. (arXiv:2012.01179v1 [cs.CV])</h2>
<h3>Felix Anda, Brett A. Becker, David Lillis, Nhien-An Le-Khac, Mark Scanlon</h3>
<p>Swift response to the detection of endangered minors is an ongoing concern
for law enforcement. Many child-focused investigations hinge on digital
evidence discovery and analysis. Automated age estimation techniques are needed
to aid in these investigations to expedite this evidence discovery process, and
decrease investigator exposure to traumatic material. Automated techniques also
show promise in decreasing the overflowing backlog of evidence obtained from
increasing numbers of devices and online services. A lack of sufficient
training data combined with natural human variance has been long hindering
accurate automated age estimation -- especially for underage subjects. This
paper presented a comprehensive evaluation of the performance of two cloud age
estimation services (Amazon Web Service's Rekognition service and Microsoft
Azure's Face API) against a dataset of over 21,800 underage subjects. The
objective of this work is to evaluate the influence that certain human
biometric factors, facial expressions, and image quality (i.e. blur, noise,
exposure and resolution) have on the outcome of automated age estimation
services. A thorough evaluation allows us to identify the most influential
factors to be overcome in future age estimation systems.
</p>
<a href="http://arxiv.org/abs/2012.01179" target="_blank">arXiv:2012.01179</a> [<a href="http://arxiv.org/pdf/2012.01179" target="_blank">pdf</a>]

<h2>Classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology. (arXiv:2012.01189v1 [cs.CV])</h2>
<h3>Adriana Borowa, Dawid Rymarczyk, Dorota Ocho&#x144;ska, Monika Brzychczy-W&#x142;och, Bartosz Zieli&#x144;ski</h3>
<p>In this work, we analyze if it is possible to distinguish between different
clones of the same bacteria species (Klebsiella pneumoniae) based only on
microscopic images. It is a challenging task, previously considered impossible
due to the high clones similarity. For this purpose, we apply a multi-step
algorithm with attention-based multiple instance learning. Except for obtaining
accuracy at the level of 0.9, we introduce extensive interpretability based on
CellProfiler and persistence homology, increasing the understandability and
trust in the model.
</p>
<a href="http://arxiv.org/abs/2012.01189" target="_blank">arXiv:2012.01189</a> [<a href="http://arxiv.org/pdf/2012.01189" target="_blank">pdf</a>]

<h2>Meta-Cognition-Based Simple And Effective Approach To Object Detection. (arXiv:2012.01201v1 [cs.CV])</h2>
<h3>Sannidhi P Kumar, Chandan Gautam, Suresh Sundaram</h3>
<p>Recently, many researchers have attempted to improve deep learning-based
object detection models, both in terms of accuracy and operational speeds.
However, frequently, there is a trade-off between speed and accuracy of such
models, which encumbers their use in practical applications such as autonomous
navigation. In this paper, we explore a meta-cognitive learning strategy for
object detection to improve generalization ability while at the same time
maintaining detection speed. The meta-cognitive method selectively samples the
object instances in the training dataset to reduce overfitting. We use YOLO v3
Tiny as a base model for the work and evaluate the performance using the MS
COCO dataset. The experimental results indicate an improvement in absolute
precision of 2.6% (minimum), and 4.4% (maximum), with no overhead to inference
time.
</p>
<a href="http://arxiv.org/abs/2012.01201" target="_blank">arXiv:2012.01201</a> [<a href="http://arxiv.org/pdf/2012.01201" target="_blank">pdf</a>]

<h2>Learning Delaunay Surface Elements for Mesh Reconstruction. (arXiv:2012.01203v1 [cs.CV])</h2>
<h3>Marie-Julie Rakotosaona, Paul Guerrero, Noam Aigerman, Niloy Mitra, Maks Ovsjanikov</h3>
<p>We present a method for reconstructing triangle meshes from point clouds.
Existing learning-based methods for mesh reconstruction mostly generate
triangles individually, making it hard to create manifold meshes. We leverage
the properties of 2D Delaunay triangulations to construct a mesh from manifold
surface elements. Our method first estimates local geodesic neighborhoods
around each point. We then perform a 2D projection of these neighborhoods using
a learned logarithmic map. A Delaunay triangulation in this 2D domain is
guaranteed to produce a manifold patch, which we call a Delaunay surface
element. We synchronize the local 2D projections of neighboring elements to
maximize the manifoldness of the reconstructed mesh. Our results show that we
achieve better overall manifoldness of our reconstructed meshes than current
methods to reconstruct meshes with arbitrary topology.
</p>
<a href="http://arxiv.org/abs/2012.01203" target="_blank">arXiv:2012.01203</a> [<a href="http://arxiv.org/pdf/2012.01203" target="_blank">pdf</a>]

<h2>Unsupervised Neural Domain Adaptation for Document Image Binarization. (arXiv:2012.01204v1 [cs.CV])</h2>
<h3>Francisco J. Castellanos, Antonio-Javier Gallego, Jorge Calvo-Zaragoza</h3>
<p>Binarization is a well-known image processing task, whose objective is to
separate the foreground of an image from the background. One of the many tasks
for which it is useful is that of preprocessing document images in order to
identify relevant information, such as text or symbols. The wide variety of
document types, typologies, alphabets, and formats makes binarization
challenging, and there are, therefore, multiple proposals with which to solve
this problem, from classical manually-adjusted methods, to more recent
approaches based on machine learning. The latter techniques require a large
amount of training data in order to obtain good results; however, labeling a
portion of each existing collection of documents is not feasible in practice.
This is a common problem in supervised learning, which can be addressed by
using the so-called Domain Adaptation (DA) techniques. These techniques take
advantage of the knowledge learned in one domain, for which labeled data are
available, to apply it to other domains for which there are no labeled data.
This paper proposes a method that combines neural networks and DA in order to
carry out unsupervised document binarization. However, when both the source and
target domains are very similar, this adaptation could be detrimental. Our
methodology, therefore, first measures the similarity between domains in an
innovative manner in order to determine whether or not it is appropriate to
apply the adaptation process. The results reported in the experimentation, when
evaluating up to 20 possible combinations among five different domains, show
that our proposal successfully deals with the binarization of new document
domains without the need for labeled data.
</p>
<a href="http://arxiv.org/abs/2012.01204" target="_blank">arXiv:2012.01204</a> [<a href="http://arxiv.org/pdf/2012.01204" target="_blank">pdf</a>]

<h2>VisEvol: Visual Analytics to Support Hyperparameter Search through Evolutionary Optimization. (arXiv:2012.01205v1 [cs.LG])</h2>
<h3>Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas Kerren</h3>
<p>During the training phase of machine learning (ML) models, it is usually
necessary to configure several hyperparameters. This process is computationally
intensive and requires an extensive search to infer the best hyperparameter set
for the given problem. The challenge is exacerbated by the fact that most ML
models are complex internally, and training involves trial-and-error processes
that could remarkably affect the predictive result. Moreover, each
hyperparameter of an ML algorithm is potentially intertwined with the others,
and changing it might result in unforeseeable impacts on the remaining
hyperparameters. Evolutionary optimization is a promising method to try and
address those issues. According to this method, performant models are stored,
while the remainder are improved through crossover and mutation processes
inspired by genetic algorithms. We present VisEvol, a visual analytics tool
that supports interactive exploration of hyperparameters and intervention in
this evolutionary procedure. In summary, our proposed tool helps the user to
generate new models through evolution and eventually explore powerful
hyperparameter combinations in diverse regions of the extensive hyperparameter
space. The outcome is a voting ensemble (with equal rights) that boosts the
final predictive performance. The utility and applicability of VisEvol are
demonstrated with two use cases and interviews with ML experts who evaluated
the effectiveness of the tool.
</p>
<a href="http://arxiv.org/abs/2012.01205" target="_blank">arXiv:2012.01205</a> [<a href="http://arxiv.org/pdf/2012.01205" target="_blank">pdf</a>]

<h2>Target Reaching Behaviour for Unfreezing the Robot in a Semi-Static and Crowded Environment. (arXiv:2012.01206v1 [cs.RO])</h2>
<h3>Arturo Cruz-Maya</h3>
<p>Robot navigation in human semi-static and crowded environments can lead to
the freezing problem, where the robot can not move due to the presence of
humans standing on its path and no other path is available. Classical
approaches of robot navigation do not provide a solution for this problem. In
such situations, the robot could interact with the humans in order to clear its
path instead of considering them as unanimated obstacles. In this work, we
propose a robot behavior for a wheeled humanoid robot that complains with
social norms for clearing its path when the robot is frozen due to the presence
of humans. The behavior consists of two modules: 1) A detection module, which
make use of the Yolo v3 algorithm trained to detect human hands and human arms.
2) A gesture module, which make use of a policy trained in simulation using the
Proximal Policy Optimization algorithm. Orchestration of the two models is done
using the ROS framework.
</p>
<a href="http://arxiv.org/abs/2012.01206" target="_blank">arXiv:2012.01206</a> [<a href="http://arxiv.org/pdf/2012.01206" target="_blank">pdf</a>]

<h2>Learning Spatial Attention for Face Super-Resolution. (arXiv:2012.01211v1 [cs.CV])</h2>
<h3>Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, Kwan-Yee K. Wong</h3>
<p>General image super-resolution techniques have difficulties in recovering
detailed face structures when applying to low resolution face images. Recent
deep learning based methods tailored for face images have achieved improved
performance by jointly trained with additional task such as face parsing and
landmark prediction. However, multi-task learning requires extra manually
labeled data. Besides, most of the existing works can only generate relatively
low resolution face images (e.g., $128\times128$), and their applications are
therefore limited. In this paper, we introduce a novel SPatial Attention
Residual Network (SPARNet) built on our newly proposed Face Attention Units
(FAUs) for face super-resolution. Specifically, we introduce a spatial
attention mechanism to the vanilla residual blocks. This enables the
convolutional layers to adaptively bootstrap features related to the key face
structures and pay less attention to those less feature-rich regions. This
makes the training more effective and efficient as the key face structures only
account for a very small portion of the face image. Visualization of the
attention maps shows that our spatial attention network can capture the key
face structures well even for very low resolution faces (e.g., $16\times16$).
Quantitative comparisons on various kinds of metrics (including PSNR, SSIM,
identity similarity, and landmark detection) demonstrate the superiority of our
method over current state-of-the-arts. We further extend SPARNet with
multi-scale discriminators, named as SPARNetHD, to produce high resolution
results (i.e., $512\times512$). We show that SPARNetHD trained with synthetic
data cannot only produce high quality and high resolution outputs for
synthetically degraded face images, but also show good generalization ability
to real world low quality face images. Codes are available at
\url{https://github.com/chaofengc/Face-SPARNet}.
</p>
<a href="http://arxiv.org/abs/2012.01211" target="_blank">arXiv:2012.01211</a> [<a href="http://arxiv.org/pdf/2012.01211" target="_blank">pdf</a>]

<h2>Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning. (arXiv:2012.01227v1 [cs.LG])</h2>
<h3>Taehyeong Kim, Injune Hwang, Hyundo Lee, Hyunseo Kim, Won-Seok Choi, Byoung-Tak Zhang</h3>
<p>Active learning is widely used to reduce labeling effort and training time by
repeatedly querying only the most beneficial samples from unlabeled data. In
real-world problems where data cannot be stored indefinitely due to limited
storage or privacy issues, the query selection and the model update should be
performed as soon as a new data sample is observed. Various online active
learning methods have been studied to deal with these challenges; however,
there are difficulties in selecting representative query samples and updating
the model efficiently. In this study, we propose Message Passing Adaptive
Resonance Theory (MPART) for online active semi-supervised learning. The
proposed model learns the distribution and topology of the input data online.
It then infers the class of unlabeled data and selects informative and
representative samples through message passing between nodes on the topological
graph. MPART queries the beneficial samples on-the-fly in stream-based
selective sampling scenarios, and continuously improve the classification model
using both labeled and unlabeled data. We evaluate our model on visual (MNIST,
SVHN, CIFAR-10) and audio (NSynth) datasets with comparable query selection
strategies and frequencies, showing that MPART significantly outperforms the
competitive models in online active learning environments.
</p>
<a href="http://arxiv.org/abs/2012.01227" target="_blank">arXiv:2012.01227</a> [<a href="http://arxiv.org/pdf/2012.01227" target="_blank">pdf</a>]

<h2>Curiosity-driven 3D Scene Structure from Single-image Self-supervision. (arXiv:2012.01230v1 [cs.CV])</h2>
<h3>David Griffiths, Jan Boehm, Tobias Ritschel</h3>
<p>Previous work has demonstrated learning isolated 3D objects (voxel grids,
point clouds, meshes, etc.) from 2D-only self-supervision. We here set out to
extend this to entire 3D scenes made out of multiple objects, including their
location, orientation and type, and the scenes illumination. Once learned, we
can map arbitrary 2D images to 3D scene structure. We analyze why
analysis-by-synthesis-like losses for supervision of 3D scene structure using
differentiable rendering is not practical, as it almost always gets stuck in
local minima of visual ambiguities. This can be overcome by a novel form of
training: we use an additional network to steer the optimization itself to
explore the full gamut of possible solutions i.e. to be curious, and hence, to
resolve those ambiguities and find workable minima. The resulting system
converts 2D images of different virtual or real images into complete 3D scenes,
learned only from 2D images of those scenes.
</p>
<a href="http://arxiv.org/abs/2012.01230" target="_blank">arXiv:2012.01230</a> [<a href="http://arxiv.org/pdf/2012.01230" target="_blank">pdf</a>]

<h2>Policy Supervectors: General Characterization of Agents by their Behaviour. (arXiv:2012.01244v1 [cs.AI])</h2>
<h3>Anssi Kanervisto, Tomi Kinnunen, Ville Hautam&#xe4;ki</h3>
<p>By studying the underlying policies of decision-making agents, we can learn
about their shortcomings and potentially improve them. Traditionally, this has
been done either by examining the agent's implementation, its behaviour while
it is being executed, its performance with a reward/fitness function or by
visualizing the density of states the agent visits. However, these methods fail
to describe the policy's behaviour in complex, high-dimensional environments or
do not scale to thousands of policies, which is required when studying training
algorithms. We propose policy supervectors for characterizing agents by the
distribution of states they visit, adopting successful techniques from the area
of speech technology. Policy supervectors can characterize policies regardless
of their design philosophy (e.g. rule-based vs. neural networks) and scale to
thousands of policies on a single workstation machine. We demonstrate method's
applicability by studying the evolution of policies during reinforcement
learning, evolutionary training and imitation learning, providing insight on
e.g. how the search space of evolutionary algorithms is also reflected in
agent's behaviour, not just in the parameters.
</p>
<a href="http://arxiv.org/abs/2012.01244" target="_blank">arXiv:2012.01244</a> [<a href="http://arxiv.org/pdf/2012.01244" target="_blank">pdf</a>]

<h2>Vision-based flocking in outdoor environments. (arXiv:2012.01245v1 [cs.RO])</h2>
<h3>Fabian Schilling, Fabrizio Schiano, Dario Floreano</h3>
<p>Deployment of drone swarms usually relies on inter-agent communication or
visual markers that are mounted on the vehicles to simplify their mutual
detection. This letter proposes a vision-based detection and tracking algorithm
that enables groups of drones to navigate without communication or visual
markers. We employ a convolutional neural network to detect and localize nearby
agents onboard the quadcopters in real-time. Rather than manually labeling a
dataset, we automatically annotate images to train the neural network using
background subtraction by systematically flying a quadcopter in front of a
static camera. We use a multi-agent state tracker to estimate the relative
positions and velocities of nearby agents, which are subsequently fed to a
flocking algorithm for high-level control. The drones are equipped with
multiple cameras to provide omnidirectional visual inputs. The camera setup
ensures the safety of the flock by avoiding blind spots regardless of the agent
configuration. We evaluate the approach with a group of three real quadcopters
that are controlled using the proposed vision-based flocking algorithm. The
results show that the drones can safely navigate in an outdoor environment
despite substantial background clutter and difficult lighting conditions.
</p>
<a href="http://arxiv.org/abs/2012.01245" target="_blank">arXiv:2012.01245</a> [<a href="http://arxiv.org/pdf/2012.01245" target="_blank">pdf</a>]

<h2>Chair Segments: A Compact Benchmark for the Study of Object Segmentation. (arXiv:2012.01250v1 [cs.CV])</h2>
<h3>Leticia Pinto-Alva, Ian K. Torres, Rosangel Garcia, Ziyan Yang, Vicente Ordonez</h3>
<p>Over the years, datasets and benchmarks have had an outsized influence on the
design of novel algorithms. In this paper, we introduce ChairSegments, a novel
and compact semi-synthetic dataset for object segmentation. We also show
empirical findings in transfer learning that mirror recent findings for image
classification. We particularly show that models that are fine-tuned from a
pretrained set of weights lie in the same basin of the optimization landscape.
ChairSegments consists of a diverse set of prototypical images of chairs with
transparent backgrounds composited into a diverse array of backgrounds. We aim
for ChairSegments to be the equivalent of the CIFAR-10 dataset but for quickly
designing and iterating over novel model architectures for segmentation. On
Chair Segments, a U-Net model can be trained to full convergence in only thirty
minutes using a single GPU. Finally, while this dataset is semi-synthetic, it
can be a useful proxy for real data, leading to state-of-the-art accuracy on
the Object Discovery dataset when used as a source of pretraining.
</p>
<a href="http://arxiv.org/abs/2012.01250" target="_blank">arXiv:2012.01250</a> [<a href="http://arxiv.org/pdf/2012.01250" target="_blank">pdf</a>]

<h2>Partial Gromov-Wasserstein Learning for Partial Graph Matching. (arXiv:2012.01252v1 [cs.LG])</h2>
<h3>Weijie Liu, Chao Zhang, Jiahao Xie, Zebang Shen, Hui Qian, Nenggan Zheng</h3>
<p>Graph matching finds the correspondence of nodes across two graphs and is a
basic task in graph-based machine learning. Numerous existing methods match
every node in one graph to one node in the other graph whereas two graphs
usually overlap partially in many \realworld{} applications. In this paper, a
partial Gromov-Wasserstein learning framework is proposed for partially
matching two graphs, which fuses the partial Gromov-Wasserstein distance and
the partial Wasserstein distance as the objective and updates the partial
transport map and the node embedding in an alternating fashion. The proposed
framework transports a fraction of the probability mass and matches node pairs
with high relative similarities across the two graphs. Incorporating an
embedding learning method, heterogeneous graphs can also be matched. Numerical
experiments on both synthetic and \realworld{} graphs demonstrate that our
framework can improve the F1 score by at least $20\%$ and often much more.
</p>
<a href="http://arxiv.org/abs/2012.01252" target="_blank">arXiv:2012.01252</a> [<a href="http://arxiv.org/pdf/2012.01252" target="_blank">pdf</a>]

<h2>Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing. (arXiv:2012.01271v1 [cs.CV])</h2>
<h3>Taewook Kim, Yonghyun Kim</h3>
<p>Face anti-spoofing aims to prevent false authentications of face recognition
systems by distinguishing whether an image is originated from a human face or a
spoof medium. We propose a novel method called Doubly Adversarial Suppression
Network (DASN) for domain-agnostic face anti-spoofing; DASN improves the
generalization ability to unseen domains by learning to effectively suppress
spoof-irrelevant factors (SiFs) (e.g., camera sensors, illuminations). To
achieve our goal, we introduce two types of adversarial learning schemes. In
the first adversarial learning scheme, multiple SiFs are suppressed by
deploying multiple discrimination heads that are trained against an encoder. In
the second adversarial learning scheme, each of the discrimination heads is
also adversarially trained to suppress a spoof factor, and the group of the
secondary spoof classifier and the encoder aims to intensify the spoof factor
by overcoming the suppression. We evaluate the proposed method on four public
benchmark datasets, and achieve remarkable evaluation results. The results
demonstrate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.01271" target="_blank">arXiv:2012.01271</a> [<a href="http://arxiv.org/pdf/2012.01271" target="_blank">pdf</a>]

<h2>Regularization and False Alarms Quantification: Two Sides of the Explainability Coin. (arXiv:2012.01273v1 [cs.LG])</h2>
<h3>Nima Safaei, Pooria Assadi</h3>
<p>Regularization is a well-established technique in machine learning (ML) to
achieve an optimal bias-variance trade-off which in turn reduces model
complexity and enhances explainability. To this end, some hyper-parameters must
be tuned, enabling the ML model to accurately fit the unseen data as well as
the seen data. In this article, the authors argue that the regularization of
hyper-parameters and quantification of costs and risks of false alarms are in
reality two sides of the same coin, explainability. Incorrect or non-existent
estimation of either quantities undermines the measurability of the economic
value of using ML, to the extent that might make it practically useless.
</p>
<a href="http://arxiv.org/abs/2012.01273" target="_blank">arXiv:2012.01273</a> [<a href="http://arxiv.org/pdf/2012.01273" target="_blank">pdf</a>]

<h2>How Robust are Randomized Smoothing based Defenses to Data Poisoning?. (arXiv:2012.01274v1 [cs.LG])</h2>
<h3>Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Jihun Hamm</h3>
<p>The prediction of certifiably robust classifiers remains constant around a
neighborhood of a point, making them resilient to test-time attacks with a
guarantee. In this work, we present a previously unrecognized threat to robust
machine learning models that highlights the importance of training-data quality
in achieving high certified robustness. Specifically, we propose a novel
bilevel optimization based data poisoning attack that degrades the robustness
guarantees of certifiably robust classifiers. Unlike other data poisoning
attacks that reduce the accuracy of the poisoned models on a small set of
target points, our attack reduces the average certified radius of an entire
target class in the dataset. Moreover, our attack is effective even when the
victim trains the models from scratch using state-of-the-art robust training
methods such as Gaussian data augmentation\cite{cohen2019certified},
MACER\cite{zhai2020macer}, and SmoothAdv\cite{salman2019provably}. To make the
attack harder to detect we use clean-label poisoning points with imperceptibly
small distortions. The effectiveness of the proposed method is evaluated by
poisoning MNIST and CIFAR10 datasets and training deep neural networks using
the previously mentioned robust training methods and certifying their
robustness using randomized smoothing. For the models trained with these robust
training methods our attack points reduce the average certified radius of the
target class by more than 30% and are transferable to models with different
architectures and models trained with different robust training methods.
</p>
<a href="http://arxiv.org/abs/2012.01274" target="_blank">arXiv:2012.01274</a> [<a href="http://arxiv.org/pdf/2012.01274" target="_blank">pdf</a>]

<h2>Pareto Deterministic Policy Gradients and Its Application in 5G Massive MIMO Networks. (arXiv:2012.01279v1 [cs.LG])</h2>
<h3>Zhou Zhou, Yan Xin, Hao Chen, Charlie Zhang, Lingjia Liu</h3>
<p>In this paper, we consider jointly optimizing cell load balance and network
throughput via a reinforcement learning (RL) approach, where inter-cell
handover (i.e., user association assignment) and massive MIMO antenna tilting
are configured as the RL policy to learn. Our rationale behind using RL is to
circumvent the challenges of analytically modeling user mobility and network
dynamics. To accomplish this joint optimization, we integrate vector rewards
into the RL value network and conduct RL action via a separate policy network.
We name this method as Pareto deterministic policy gradients (PDPG). It is an
actor-critic, model-free and deterministic policy algorithm which can handle
the coupling objectives with the following two merits: 1) It solves the
optimization via leveraging the degree of freedom of vector reward as opposed
to choosing handcrafted scalar-reward; 2) Cross-validation over multiple
policies can be significantly reduced. Accordingly, the RL enabled network
behaves in a self-organized way: It learns out the underlying user mobility
through measurement history to proactively operate handover and antenna tilt
without environment assumptions. Our numerical evaluation demonstrates that the
introduced RL method outperforms scalar-reward based approaches. Meanwhile, to
be self-contained, an ideal static optimization based brute-force search solver
is included as a benchmark. The comparison shows that the RL approach performs
as well as this ideal strategy, though the former one is constrained with
limited environment observations and lower action frequency, whereas the latter
ones have full access to the user mobility. The convergence of our introduced
approach is also tested under different user mobility environment based on our
measurement data from a real scenario.
</p>
<a href="http://arxiv.org/abs/2012.01279" target="_blank">arXiv:2012.01279</a> [<a href="http://arxiv.org/pdf/2012.01279" target="_blank">pdf</a>]

<h2>Are Gradient-based Saliency Maps Useful in Deep Reinforcement Learning?. (arXiv:2012.01281v1 [cs.LG])</h2>
<h3>Matthias Rosynski, Frank Kirchner, Matias Valdenegro-Toro</h3>
<p>Deep Reinforcement Learning (DRL) connects the classic Reinforcement Learning
algorithms with Deep Neural Networks. A problem in DRL is that CNNs are
black-boxes and it is hard to understand the decision-making process of agents.
In order to be able to use RL agents in highly dangerous environments for
humans and machines, the developer needs a debugging tool to assure that the
agent does what is expected. Currently, rewards are primarily used to interpret
how well an agent is learning. However, this can lead to deceptive conclusions
if the agent receives more rewards by memorizing a policy and not learning to
respond to the environment. In this work, it is shown that this problem can be
recognized with the help of gradient visualization techniques. This work brings
some of the best-known visualization methods from the field of image
classification to the area of Deep Reinforcement Learning. Furthermore, two new
visualization techniques have been developed, one of which provides
particularly good results. It is being proven to what extent the algorithms can
be used in the area of Reinforcement learning. Also, the question arises on how
well the DRL algorithms can be visualized across different environments with
varying visualization techniques.
</p>
<a href="http://arxiv.org/abs/2012.01281" target="_blank">arXiv:2012.01281</a> [<a href="http://arxiv.org/pdf/2012.01281" target="_blank">pdf</a>]

<h2>The Self-Simplifying Machine: Exploiting the Structure of Piecewise Linear Neural Networks to Create Interpretable Models. (arXiv:2012.01293v1 [cs.LG])</h2>
<h3>William Knauth</h3>
<p>Today, it is more important than ever before for users to have trust in the
models they use. As Machine Learning models fall under increased regulatory
scrutiny and begin to see more applications in high-stakes situations, it
becomes critical to explain our models. Piecewise Linear Neural Networks (PLNN)
with the ReLU activation function have quickly become extremely popular models
due to many appealing properties; however, they still present many challenges
in the areas of robustness and interpretation. To this end, we introduce novel
methodology toward simplification and increased interpretability of Piecewise
Linear Neural Networks for classification tasks. Our methods include the use of
a trained, deep network to produce a well-performing, single-hidden-layer
network without further stochastic training, in addition to an algorithm to
reduce flat networks to a smaller, more interpretable size with minimal loss in
performance. On these methods, we conduct preliminary studies of model
performance, as well as a case study on Wells Fargo's Home Lending dataset,
together with visual model interpretation.
</p>
<a href="http://arxiv.org/abs/2012.01293" target="_blank">arXiv:2012.01293</a> [<a href="http://arxiv.org/pdf/2012.01293" target="_blank">pdf</a>]

<h2>Safe Reinforcement Learning for Antenna Tilt Optimisation using Shielding and Multiple Baselines. (arXiv:2012.01296v1 [cs.LG])</h2>
<h3>Saman Feghhi, Erik Aumayr, Filippo Vannella, Ezeddin Al Hakim, Grigorios Iakovidis</h3>
<p>Safe interaction with the environment is one of the most challenging aspects
of Reinforcement Learning (RL) when applied to real-world problems. This is
particularly important when unsafe actions have a high or irreversible negative
impact on the environment. In the context of network management operations,
Remote Electrical Tilt (RET) optimisation is a safety-critical application in
which exploratory modifications of antenna tilt angles of Base Stations (BSs)
can cause significant performance degradation in the network. In this paper, we
propose a modular Safe Reinforcement Learning (SRL) architecture which is then
used to address the RET optimisation in cellular networks. In this approach, a
safety shield continuously benchmarks the performance of RL agents against safe
baselines, and determines safe antenna tilt updates to be performed on the
network. Our results demonstrate improved performance of the SRL agent over the
baseline while ensuring the safety of the performed actions.
</p>
<a href="http://arxiv.org/abs/2012.01296" target="_blank">arXiv:2012.01296</a> [<a href="http://arxiv.org/pdf/2012.01296" target="_blank">pdf</a>]

<h2>Complex Coordinate-Based Meta-Analysis with Probabilistic Programming. (arXiv:2012.01303v1 [cs.AI])</h2>
<h3>Valentin Iovene (NEUROSPIN, PARIETAL), Gaston Zanitti (NEUROSPIN, PARIETAL), Demian Wassermann (NEUROSPIN, PARIETAL)</h3>
<p>With the growing number of published functional magnetic resonance imaging
(fMRI) studies, meta-analysis databases and models have become an integral part
of brain mapping research. Coordinate-based meta-analysis (CBMA) databases are
built by automatically extracting both coordinates of reported peak activations
and term associations using natural language processing (NLP) techniques.
Solving term-based queries on these databases make it possible to obtain
statistical maps of the brain related to specific cognitive processes. However,
with tools like Neurosynth, only singleterm queries lead to statistically
reliable results. When solving richer queries, too few studies from the
database contribute to the statistical estimations. We design a probabilistic
domain-specific language (DSL) standing on Datalog and one of its probabilistic
extensions, CP-Logic, for expressing and solving rich logic-based queries. We
encode a CBMA database into a probabilistic program. Using the joint
distribution of its Bayesian network translation, we show that solutions of
queries on this program compute the right probability distributions of voxel
activations. We explain how recent lifted query processing algorithms make it
possible to scale to the size of large neuroimaging data, where state of the
art knowledge compilation (KC) techniques fail to solve queries fast enough for
practical applications. Finally, we introduce a method for relating studies to
terms probabilistically, leading to better solutions for conjunctive queries on
smaller databases. We demonstrate results for two-term conjunctive queries,
both on simulated meta-analysis databases and on the widely-used Neurosynth
database.
</p>
<a href="http://arxiv.org/abs/2012.01303" target="_blank">arXiv:2012.01303</a> [<a href="http://arxiv.org/pdf/2012.01303" target="_blank">pdf</a>]

<h2>Top-1 CORSMAL Challenge 2020 Submission: Filling Mass Estimation Using Multi-modal Observations of Human-robot Handovers. (arXiv:2012.01311v1 [cs.CV])</h2>
<h3>Vladimir Iashin, Francesca Palermo, G&#xf6;khan Solak, Claudio Coppola</h3>
<p>Human-robot object handover is a key skill for the future of human-robot
collaboration. CORSMAL 2020 Challenge focuses on the perception part of this
problem: the robot needs to estimate the filling mass of a container held by a
human. Although there are powerful methods in image processing and audio
processing individually, answering such a problem requires processing data from
multiple sensors together. The appearance of the container, the sound of the
filling, and the depth data provide essential information. We propose a
multi-modal method to predict three key indicators of the filling mass: filling
type, filling level, and container capacity. These indicators are then combined
to estimate the filling mass of a container. Our method obtained Top-1 overall
performance among all submissions to CORSMAL 2020 Challenge on both public and
private subsets while showing no evidence of overfitting. Our source code is
publicly available: https://github.com/v-iashin/CORSMAL
</p>
<a href="http://arxiv.org/abs/2012.01311" target="_blank">arXiv:2012.01311</a> [<a href="http://arxiv.org/pdf/2012.01311" target="_blank">pdf</a>]

<h2>Improved Contrastive Divergence Training of Energy Based Models. (arXiv:2012.01316v1 [cs.LG])</h2>
<h3>Yilun Du, Shuang Li, Joshua Tenenbaum, Igor Mordatch</h3>
<p>We propose several different techniques to improve contrastive divergence
training of energy-based models (EBMs). We first show that a gradient term
neglected in the popular contrastive divergence formulation is both tractable
to estimate and is important to avoid training instabilities in previous
models. We further highlight how data augmentation, multi-scale processing, and
reservoir sampling can be used to improve model robustness and generation
quality. Thirdly, we empirically evaluate stability of model architectures and
show improved performance on a host of benchmarks and use cases, such as image
generation, OOD detection, and compositional generation.
</p>
<a href="http://arxiv.org/abs/2012.01316" target="_blank">arXiv:2012.01316</a> [<a href="http://arxiv.org/pdf/2012.01316" target="_blank">pdf</a>]

<h2>Siamese Basis Function Networks for Defect Classification. (arXiv:2012.01338v1 [cs.CV])</h2>
<h3>Tobias Schlagenhauf, Faruk Yildirim, Benedikt Br&#xfc;ckner, J&#xfc;rgen Fleischer</h3>
<p>Defect classification on metallic surfaces is considered a critical issue
since substantial quantities of steel and other metals are processed by the
manufacturing industry on a daily basis. The authors propose a new approach
where they introduce the usage of so called Siamese Kernels in a Basis Function
Network to create the Siamese Basis Function Network (SBF-Network). The
underlying idea is to classify by comparison using similarity scores. This
classification is reinforced through efficient deep learning based feature
extraction methods. First, a center image is assigned to each Siamese Kernel.
The Kernels are then trained to generate encodings in a way that enables them
to distinguish their center from other images in the dataset. Using this
approach the authors created some kind of class-awareness inside the Siamese
Kernels. To classify a given image, each Siamese Kernel generates a feature
vector for its center as well as the given image. These vectors represent
encodings of the respective images in a lower-dimensional space. The distance
between each pair of encodings is then computed using the cosine distance
together with radial basis functions. The distances are fed into a multilayer
neural network to perform the classification. With this approach the authors
achieved outstanding results on the state of the art NEU surface defect
dataset.
</p>
<a href="http://arxiv.org/abs/2012.01338" target="_blank">arXiv:2012.01338</a> [<a href="http://arxiv.org/pdf/2012.01338" target="_blank">pdf</a>]

<h2>Cross-modal Retrieval and Synthesis (X-MRS): Closing the modality gap in shared subspace. (arXiv:2012.01345v1 [cs.CV])</h2>
<h3>Ricardo Guerrero, Hai Xuan Pham, Vladimir Pavlovic</h3>
<p>Computational food analysis (CFA), a broad set of methods that attempt to
automate food understanding, naturally requires analysis of multi-modal
evidence of a particular food or dish, e.g. images, recipe text, preparation
video, nutrition labels, etc. A key to making CFA possible is multi-modal
shared subspace learning, which in turn can be used for cross-modal retrieval
and/or synthesis, particularly, between food images and their corresponding
textual recipes. In this work we propose a simple yet novel architecture for
shared subspace learning, which is used to tackle the food image-to-recipe
retrieval problem. Our proposed method employs an effective transformer based
multilingual recipe encoder coupled with a traditional image embedding
architecture. Experimental analysis on the public Recipe1M dataset shows that
the subspace learned via the proposed method outperforms the current
state-of-the-arts (SoTA) in food retrieval by a large margin, obtaining
recall@1 of 0.64. Furthermore, in order to demonstrate the representational
power of the learned subspace, we propose a generative food image synthesis
model conditioned on the embeddings of recipes. Synthesized images can
effectively reproduce the visual appearance of paired samples, achieving R@1 of
0.68 in the image-to-recipe retrieval experiment, thus effectively capturing
the semantics of the textual recipe.
</p>
<a href="http://arxiv.org/abs/2012.01345" target="_blank">arXiv:2012.01345</a> [<a href="http://arxiv.org/pdf/2012.01345" target="_blank">pdf</a>]

<h2>Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep Reinforcement Learning And Machine Teaching. (arXiv:2012.01356v1 [cs.RO])</h2>
<h3>Aleksei Gonnochenko, Aleksandr Semochkin, Dmitry Egorov, Dmitrii Statovoy, Seyedhassan Zabihifar, Aleksey Postnikov, Elena Seliverstova, Ali Zaidi, Jayson Stemmler, Kevin Limkrailassiri</h3>
<p>Given the laborious difficulty of moving heavy bags of physical currency in
the cash center of the bank, there is a large demand for training and deploying
safe autonomous systems capable of conducting such tasks in a collaborative
workspace. However, the deformable properties of the bag along with the large
quantity of rigid-body coins contained within it, significantly increases the
challenges of bag detection, grasping and manipulation by a robotic gripper and
arm. In this paper, we apply deep reinforcement learning and machine learning
techniques to the task of controlling a collaborative robot to automate the
unloading of coin bags from a trolley. To accomplish the task-specific process
of gripping flexible materials like coin bags where the center of the mass
changes during manipulation, a special gripper was implemented in simulation
and designed in physical hardware. Leveraging a depth camera and object
detection using deep learning, a bag detection and pose estimation has been
done for choosing the optimal point of grasping. An intelligent approach based
on deep reinforcement learning has been introduced to propose the best
configuration of the robot end-effector to maximize successful grasping. A
boosted motion planning is utilized to increase the speed of motion planning
during robot operation. Real-world trials with the proposed pipeline have
demonstrated success rates over 96\% in a real-world setting.
</p>
<a href="http://arxiv.org/abs/2012.01356" target="_blank">arXiv:2012.01356</a> [<a href="http://arxiv.org/pdf/2012.01356" target="_blank">pdf</a>]

<h2>DA2: Deep Attention Adapter for Memory-EfficientOn-Device Multi-Domain Learning. (arXiv:2012.01362v1 [cs.CV])</h2>
<h3>Li Yang, Adnan Siraj Rakin, Deliang Fan</h3>
<p>Nowadays, one practical limitation of deep neural network (DNN) is its high
degree of specialization to a single task or domain (e.g. one visual domain).
It motivates researchers to develop algorithms that can adapt DNN model to
multiple domains sequentially, meanwhile still performing well on the past
domains, which is known as multi-domain learning. Conventional methods only
focus on improving accuracy with minimal parameter update, while ignoring high
computing and memory usage during training, which makes it impossible to deploy
into more and more widely used resource-limited edge devices, like mobile
phone, IoT, embedded systems, etc. During our study, we observe that memory
used for activation storage is the bottleneck that largely limits the training
time and cost on edge devices. To reduce training memory usage, while keeping
the domain adaption accuracy performance, in this work, we propose Deep
Attention Adaptor, a novel on-device multi-domain learning method, aiming to
achieve domain adaption on resource-limited edge devices in both fast and
memory-efficient manner. During on-device training, DA2 freezes the weights of
pre-trained backbone model to reduce the training memory consumption (i.e., no
need to store activation features during backward propagation). Furthermore, to
improve the adaption accuracy performance, we propose to improve the model
capacity by learning a light-weight memory-efficient residual attention adaptor
module. We validate DA2 on multiple datasets against state-of-the-art methods,
which shows good improvement in both accuracy and training cost. Finally, we
demonstrate the algorithm's efficiency on NIVDIA Jetson Nano tiny GPU, proving
the proposed DA2 reduces the on-device memory consumption by 19-37x during
training in comparison to the baseline methods.
</p>
<a href="http://arxiv.org/abs/2012.01362" target="_blank">arXiv:2012.01362</a> [<a href="http://arxiv.org/pdf/2012.01362" target="_blank">pdf</a>]

<h2>DERAIL: Diagnostic Environments for Reward And Imitation Learning. (arXiv:2012.01365v1 [cs.LG])</h2>
<h3>Pedro Freire, Adam Gleave, Sam Toyer, Stuart Russell</h3>
<p>The objective of many real-world tasks is complex and difficult to
procedurally specify. This makes it necessary to use reward or imitation
learning algorithms to infer a reward or policy directly from human data.
Existing benchmarks for these algorithms focus on realism, testing in complex
environments. Unfortunately, these benchmarks are slow, unreliable and cannot
isolate failures. As a complementary approach, we develop a suite of simple
diagnostic tasks that test individual facets of algorithm performance in
isolation. We evaluate a range of common reward and imitation learning
algorithms on our tasks. Our results confirm that algorithm performance is
highly sensitive to implementation details. Moreover, in a case-study into a
popular preference-based reward learning implementation, we illustrate how the
suite can pinpoint design flaws and rapidly evaluate candidate solutions. The
environments are available at https://github.com/HumanCompatibleAI/seals .
</p>
<a href="http://arxiv.org/abs/2012.01365" target="_blank">arXiv:2012.01365</a> [<a href="http://arxiv.org/pdf/2012.01365" target="_blank">pdf</a>]

<h2>Improving Solution Quality of Bounded Max-Sum Algorithm to Solve DCOPs involving Hard and Soft Constraints. (arXiv:2012.01369v1 [cs.AI])</h2>
<h3>Md. Musfiqur Rahman, Mashrur Rashik, Md. Mamun-or-Rashid, Md. Mosaddek Khan</h3>
<p>Bounded Max-Sum (BMS) is a message-passing algorithm that provides
approximation solution to a specific form of de-centralized coordination
problems, namely Distributed Constrained Optimization Problems (DCOPs). In
particular, BMS algorithm is able to solve problems of this type having large
search space at the expense of low computational cost. Notably, the traditional
DCOP formulation does not consider those constraints that must be
satisfied(also known as hard constraints), rather it concentrates only on soft
constraints. Hence, although the presence of both types of constraints are
observed in a number of real-world applications, the BMS algorithm does not
actively capitalize on the hard constraints. To address this issue, we tailor
BMS in such a way that can deal with DCOPs having both type constraints. In so
doing, our approach improves the solution quality of the algorithm. The
empirical results exhibit a marked improvement in the quality of the solutions
of large DCOPs.
</p>
<a href="http://arxiv.org/abs/2012.01369" target="_blank">arXiv:2012.01369</a> [<a href="http://arxiv.org/pdf/2012.01369" target="_blank">pdf</a>]

<h2>Cross-Descriptor Visual Localization and Mapping. (arXiv:2012.01377v1 [cs.CV])</h2>
<h3>Mihai Dusmanu, Ondrej Miksik, Johannes L. Sch&#xf6;nberger, Marc Pollefeys</h3>
<p>Visual localization and mapping is the key technology underlying the majority
of Mixed Reality and robotics systems. Most state-of-the-art approaches rely on
local features to establish correspondences between images. In this paper, we
present three novel scenarios for localization and mapping which require the
continuous update of feature representations and the ability to match across
different feature types. While localization and mapping is a fundamental
computer vision problem, the traditional setup treats it as a single-shot
process using the same local image features throughout the evolution of a map.
This assumes the whole process is repeated from scratch whenever the underlying
features are changed. However, reiterating it is typically impossible in
practice, because raw images are often not stored and re-building the maps
could lead to loss of the attached digital content. To overcome the limitations
of current approaches, we present the first principled solution to
cross-descriptor localization and mapping. Our data-driven approach is agnostic
to the feature descriptor type, has low computational requirements, and scales
linearly with the number of description algorithms. Extensive experiments
demonstrate the effectiveness of our approach on state-of-the-art benchmarks
for a variety of handcrafted and learned features.
</p>
<a href="http://arxiv.org/abs/2012.01377" target="_blank">arXiv:2012.01377</a> [<a href="http://arxiv.org/pdf/2012.01377" target="_blank">pdf</a>]

<h2>Deep Graph Neural Networks with Shallow Subgraph Samplers. (arXiv:2012.01380v1 [cs.LG])</h2>
<h3>Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Rajgopal Kannan, Viktor Prasanna, Long Jin, Andrey Malevich, Ren Chen</h3>
<p>While Graph Neural Networks (GNNs) are powerful models for learning
representations on graphs, most state-of-the-art models do not have significant
accuracy gain beyond two to three layers. Deep GNNs fundamentally need to
address: 1). expressivity challenge due to oversmoothing, and 2). computation
challenge due to neighborhood explosion. We propose a simple "deep GNN, shallow
sampler" design principle to improve both the GNN accuracy and efficiency -- to
generate representation of a target node, we use a deep GNN to pass messages
only within a shallow, localized subgraph. A properly sampled subgraph may
exclude irrelevant or even noisy nodes, and still preserve the critical
neighbor features and graph structures. The deep GNN then smooths the
informative local signals to enhance feature learning, rather than
oversmoothing the global graph signals into just "white noise". We
theoretically justify why the combination of deep GNNs with shallow samplers
yields the best learning performance. We then propose various sampling
algorithms and neural architecture extensions to achieve good empirical
results. Experiments on five large graphs show that our models achieve
significantly higher accuracy and efficiency, compared with state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2012.01380" target="_blank">arXiv:2012.01380</a> [<a href="http://arxiv.org/pdf/2012.01380" target="_blank">pdf</a>]

<h2>A Self-Supervised Feature Map Augmentation (FMA) Loss and Combined Augmentations Finetuning to Efficiently Improve the Robustness of CNNs. (arXiv:2012.01386v1 [cs.CV])</h2>
<h3>Nikhil Kapoor, Chun Yuan, Jonas L&#xf6;hdefink, Roland Zimmermann, Serin Varghese, Fabian H&#xfc;ger, Nico Schmidt, Peter Schlicht, Tim Fingscheidt</h3>
<p>Deep neural networks are often not robust to semantically-irrelevant changes
in the input. In this work we address the issue of robustness of
state-of-the-art deep convolutional neural networks (CNNs) against commonly
occurring distortions in the input such as photometric changes, or the addition
of blur and noise. These changes in the input are often accounted for during
training in the form of data augmentation. We have two major contributions:
First, we propose a new regularization loss called feature-map augmentation
(FMA) loss which can be used during finetuning to make a model robust to
several distortions in the input. Second, we propose a new combined
augmentations (CA) finetuning strategy, that results in a single model that is
robust to several augmentation types at the same time in a data-efficient
manner. We use the CA strategy to improve an existing state-of-the-art method
called stability training (ST). Using CA, on an image classification task with
distorted images, we achieve an accuracy improvement of on average 8.94% with
FMA and 8.86% with ST absolute on CIFAR-10 and 8.04% with FMA and 8.27% with ST
absolute on ImageNet, compared to 1.98% and 2.12%, respectively, with the well
known data augmentation method, while keeping the clean baseline performance.
</p>
<a href="http://arxiv.org/abs/2012.01386" target="_blank">arXiv:2012.01386</a> [<a href="http://arxiv.org/pdf/2012.01386" target="_blank">pdf</a>]

<h2>Fine-grained activity recognition for assembly videos. (arXiv:2012.01392v1 [cs.CV])</h2>
<h3>Jonathan D. Jones, Cathryn Cortesa, Amy Shelton, Barbara Landau, Sanjeev Khudanpur, Gregory D. Hager</h3>
<p>In this paper we address the task of recognizing assembly actions as a
structure (e.g. a piece of furniture or a toy block tower) is built up from a
set of primitive objects. Recognizing the full range of assembly actions
requires perception at a level of spatial detail that has not been attempted in
the action recognition literature to date. We extend the fine-grained activity
recognition setting to address the task of assembly action recognition in its
full generality by unifying assembly actions and kinematic structures within a
single framework. We use this framework to develop a general method for
recognizing assembly actions from observation sequences, along with observation
features that take advantage of a spatial assembly's special structure.
Finally, we evaluate our method empirically on two application-driven data
sources: (1) An IKEA furniture-assembly dataset, and (2) A block-building
dataset. On the first, our system recognizes assembly actions with an average
framewise accuracy of 70% and an average normalized edit distance of 10%. On
the second, which requires fine-grained geometric reasoning to distinguish
between assemblies, our system attains an average normalized edit distance of
23% -- a relative improvement of 69% over prior work.
</p>
<a href="http://arxiv.org/abs/2012.01392" target="_blank">arXiv:2012.01392</a> [<a href="http://arxiv.org/pdf/2012.01392" target="_blank">pdf</a>]

<h2>Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER. (arXiv:2012.01399v1 [cs.LG])</h2>
<h3>Markus Holzleitner, Lukas Gruber, Jos&#xe9; Arjona-Medina, Johannes Brandstetter, Sepp Hochreiter</h3>
<p>We prove under commonly used assumptions the convergence of actor-critic
reinforcement learning algorithms, which simultaneously learn a policy
function, the actor, and a value function, the critic. Both functions can be
deep neural networks of arbitrary complexity. Our framework allows showing
convergence of the well known Proximal Policy Optimization (PPO) and of the
recently introduced RUDDER. For the convergence proof we employ recently
introduced techniques from the two time-scale stochastic approximation theory.
Our results are valid for actor-critic methods that use episodic samples and
that have a policy that becomes more greedy during learning. Previous
convergence proofs assume linear function approximation, cannot treat episodic
examples, or do not consider that policies become greedy. The latter is
relevant since optimal policies are typically deterministic.
</p>
<a href="http://arxiv.org/abs/2012.01399" target="_blank">arXiv:2012.01399</a> [<a href="http://arxiv.org/pdf/2012.01399" target="_blank">pdf</a>]

<h2>Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization. (arXiv:2012.01405v1 [cs.CV])</h2>
<h3>Long Zhao, Yuxiao Wang, Jiaping Zhao, Liangzhe Yuan, Jennifer J. Sun, Florian Schroff, Hartwig Adam, Xi Peng, Dimitris Metaxas, Ting Liu</h3>
<p>We introduce a novel representation learning method to disentangle
pose-dependent as well as view-dependent factors from 2D human poses. The
method trains a network using cross-view mutual information maximization
(CV-MIM) which maximizes mutual information of the same pose performed from
different viewpoints in a contrastive learning manner. We further propose two
regularization terms to ensure disentanglement and smoothness of the learned
representations. The resulting pose representations can be used for cross-view
action recognition. To evaluate the power of the learned representations, in
addition to the conventional fully-supervised action recognition settings, we
introduce a novel task called single-shot cross-view action recognition. This
task trains models with actions from only one single viewpoint while models are
evaluated on poses captured from all possible viewpoints. We evaluate the
learned representations on standard benchmarks for action recognition, and show
that (i) CV-MIM performs competitively compared with the state-of-the-art
models in the fully-supervised scenarios; (ii) CV-MIM outperforms other
competing methods by a large margin in the single-shot cross-view setting;
(iii) and the learned representations can significantly boost the performance
when reducing the amount of supervised training data.
</p>
<a href="http://arxiv.org/abs/2012.01405" target="_blank">arXiv:2012.01405</a> [<a href="http://arxiv.org/pdf/2012.01405" target="_blank">pdf</a>]

<h2>Ontological Smart Contracts in OASIS: Ontology for Agents, Systems, and Integration of Services. (arXiv:2012.01410v1 [cs.AI])</h2>
<h3>Domenico Cantone, Carmelo Fabio Longo, Marianna Nicolosi-Asmundo, Daniele Francesco Santamaria, Corrado Santoro</h3>
<p>In this contribution we extend an ontology for modelling agents and their
interactions, called Ontology for Agents, Systems, and Integration of Services
(in short, OASIS), with conditionals and ontological smart contracts (in short,
OSCs). OSCs are ontological representations of smart contracts that allow to
establish responsibilities and authorizations among agents and set agreements,
whereas conditionals allow one to restrict and limit agent interactions, define
activation mechanisms that trigger agent actions, and define constraints and
contract terms on OSCs. Conditionals and OSCs, as defined in OASIS, are applied
to extend with ontological capabilities digital public ledgers such as the
blockchain and smart contracts implemented on it. We will also sketch the
architecture of a framework based on the OASIS definition of OSCs that exploits
the Ethereum platform and the Interplanetary File System.
</p>
<a href="http://arxiv.org/abs/2012.01410" target="_blank">arXiv:2012.01410</a> [<a href="http://arxiv.org/pdf/2012.01410" target="_blank">pdf</a>]

<h2>PatchmatchNet: Learned Multi-View Patchmatch Stereo. (arXiv:2012.01411v1 [cs.CV])</h2>
<h3>Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, Marc Pollefeys</h3>
<p>We present PatchmatchNet, a novel and learnable cascade formulation of
Patchmatch for high-resolution multi-view stereo. With high computation speed
and low memory requirement, PatchmatchNet can process higher resolution imagery
and is more suited to run on resource limited devices than competitors that
employ 3D cost volume regularization. For the first time we introduce an
iterative multi-scale Patchmatch in an end-to-end trainable architecture and
improve the Patchmatch core algorithm with a novel and learned adaptive
propagation and evaluation scheme for each iteration. Extensive experiments
show a very competitive performance and generalization for our method on DTU,
Tanks &amp; Temples and ETH3D, but at a significantly higher efficiency than all
existing top-performing models: at least two and a half times faster than
state-of-the-art methods with twice less memory usage.
</p>
<a href="http://arxiv.org/abs/2012.01411" target="_blank">arXiv:2012.01411</a> [<a href="http://arxiv.org/pdf/2012.01411" target="_blank">pdf</a>]

<h2>A Few Guidelines for Incremental Few-Shot Segmentation. (arXiv:2012.01415v1 [cs.CV])</h2>
<h3>Fabio Cermelli, Massimiliano Mancini, Yongqin Xian, Zeynep Akata, Barbara Caputo</h3>
<p>Reducing the amount of supervision required by neural networks is especially
important in the context of semantic segmentation, where collecting dense
pixel-level annotations is particularly expensive. In this paper, we address
this problem from a new perspective: Incremental Few-Shot Segmentation. In
particular, given a pretrained segmentation model and few images containing
novel classes, our goal is to learn to segment novel classes while retaining
the ability to segment previously seen ones. In this context, we discover,
against all beliefs, that fine-tuning the whole architecture with these few
images is not only meaningful, but also very effective. We show how the main
problems of end-to-end training in this scenario are i) the drift of the
batch-normalization statistics toward novel classes that we can fix with batch
renormalization and ii) the forgetting of old classes, that we can fix with
regularization strategies. We summarize our findings with five guidelines that
together consistently lead to the state of the art on the COCO and Pascal-VOC
2012 datasets, with different number of images per class and even with multiple
learning episodes.
</p>
<a href="http://arxiv.org/abs/2012.01415" target="_blank">arXiv:2012.01415</a> [<a href="http://arxiv.org/pdf/2012.01415" target="_blank">pdf</a>]

<h2>The duality structure gradient descent algorithm: analysis and applications to neural networks. (arXiv:1708.00523v7 [cs.LG] UPDATED)</h2>
<h3>Thomas Flynn</h3>
<p>The training of deep neural networks is typically carried out using some form
of gradient descent, often with great success. However, existing non-asymptotic
analyses of first-order optimization algorithms typically employ a gradient
smoothness assumption that is too strong to be applicable in the case of deep
neural networks. To address this, we propose an algorithm named duality
structure gradient descent (DSGD) that is amenable to non-asymptotic
performance analysis, under mild assumptions on the training set and network
architecture. The algorithm can be viewed as a form of layer-wise coordinate
descent, where at each iteration the algorithm chooses one layer of the network
to update. The decision of what layer to update is done in a greedy fashion,
based on a rigorous lower bound on the improvement of the objective function
for each choice of layer. In the analysis, we bound the time required to reach
approximate stationary points, in both the deterministic and stochastic
settings. The convergence is measured in terms of a parameter-dependent family
of norms that is derived from the network architecture and designed to confirm
a smoothness-like property on the gradient of the training loss function. We
empirically demonstrate the effectiveness of DSGD in several neural network
training scenarios.
</p>
<a href="http://arxiv.org/abs/1708.00523" target="_blank">arXiv:1708.00523</a> [<a href="http://arxiv.org/pdf/1708.00523" target="_blank">pdf</a>]

<h2>Autoencoding any Data through Kernel Autoencoders. (arXiv:1805.11028v3 [stat.ML] UPDATED)</h2>
<h3>Pierre Laforgue, Stephan Cl&#xe9;men&#xe7;on, Florence d&#x27;Alch&#xe9;-Buc</h3>
<p>This paper investigates a novel algorithmic approach to data representation
based on kernel methods. Assuming that the observations lie in a Hilbert space
X, the introduced Kernel Autoencoder (KAE) is the composition of mappings from
vector-valued Reproducing Kernel Hilbert Spaces (vv-RKHSs) that minimizes the
expected reconstruction error. Beyond a first extension of the autoencoding
scheme to possibly infinite dimensional Hilbert spaces, KAE further allows to
autoencode any kind of data by choosing X to be itself a RKHS. A theoretical
analysis of the model is carried out, providing a generalization bound, and
shedding light on its connection with Kernel Principal Component Analysis. The
proposed algorithms are then detailed at length: they crucially rely on the
form taken by the minimizers, revealed by a dedicated Representer Theorem.
Finally, numerical experiments on both simulated data and real labeled graphs
(molecules) provide empirical evidence of the KAE performances.
</p>
<a href="http://arxiv.org/abs/1805.11028" target="_blank">arXiv:1805.11028</a> [<a href="http://arxiv.org/pdf/1805.11028" target="_blank">pdf</a>]

<h2>Explaining Machine Learning Models using Entropic Variable Projection. (arXiv:1810.07924v5 [stat.ML] UPDATED)</h2>
<h3>Fran&#xe7;ois Bachoc (IMT), Fabrice Gamboa (IMT), Max Halford (IMT, IRIT), Jean-Michel Loubes (IMT), Laurent Risser (IMT)</h3>
<p>In this paper, we present a new explainability formalism designed to explain
how each input variable of a test set impacts the predictions of machine
learning models. Hence, we propose a group explainability formalism for trained
machine learning decision rules, based on their response to the variability of
the input variables distribution. In order to emphasize the impact of each
input variable, this formalism uses an information theory framework that
quantifies the influence of all input-output observations based on entropic
projections. This is thus the first unified and model agnostic formalism
enabling data scientists to interpret the dependence between the input
variables, their impact on the prediction errors, and their influence on the
output predictions. Convergence rates of the entropic projections are provided
in the large sample case. Most importantly, we prove that computing an
explanation in our framework has a low algorithmic complexity, making it
scalable to real-life large datasets. We illustrate our strategy by explaining
complex decision rules learned by using XGBoost, Random Forest or Deep Neural
Network classifiers on various datasets such as Adult income, MNIST and CelebA.
We finally make clear its differences with the explainability strategies
\textit{LIME} and \textit{SHAP}, that are based on single observations. Results
can be reproduced by using the freely distributed Python toolbox
https://gems-ai.com}.
</p>
<a href="http://arxiv.org/abs/1810.07924" target="_blank">arXiv:1810.07924</a> [<a href="http://arxiv.org/pdf/1810.07924" target="_blank">pdf</a>]

<h2>Statistical Characteristics of Deep Representations: An Empirical Investigation. (arXiv:1811.03666v2 [cs.LG] UPDATED)</h2>
<h3>Daeyoung Choi, Kyungeun Lee, Duhun Hwang, Wonjong Rhee</h3>
<p>In this study, the effects of eight representation regularization methods are
investigated, including two newly developed rank regularizers (RR). The
investigation shows that the statistical characteristics of representations
such as correlation, sparsity, and rank can be manipulated as intended, during
training. Furthermore, it is possible to improve the baseline performance
simply by trying all the representation regularizers and fine-tuning the
strength of their effects. In contrast to performance improvement, no
consistent relationship between performance and statistical characteristics was
observable. The results indicate that manipulation of statistical
characteristics can be helpful for improving performance, but only indirectly
through its influence on learning dynamics or its tuning effects.
</p>
<a href="http://arxiv.org/abs/1811.03666" target="_blank">arXiv:1811.03666</a> [<a href="http://arxiv.org/pdf/1811.03666" target="_blank">pdf</a>]

<h2>Morphological Network: How Far Can We Go with Morphological Neurons?. (arXiv:1901.00109v3 [cs.LG] UPDATED)</h2>
<h3>Ranjan Mondal, Soumendu Sundar Mukherjee, Sanchayan Santra, Bhabatosh Chanda</h3>
<p>In recent years, the idea of using morphological operations as networks has
received much attention. Mathematical morphology provides very efficient and
useful image processing and image analysis tools based on basic operators like
dilation and erosion, defined in terms of kernels. Many other morphological
operations are built up using the dilation and erosion operations. Although the
learning of structuring elements such as dilation or erosion using the
backpropagation algorithm is not new, the order and the way these morphological
operations are used is not standard. In this paper, we have theoretically
analyzed the use of morphological operations for processing 1D feature vectors
and shown that this gets extended to the 2D case in a simple manner. Our
theoretical results show that a morphological block represents a sum of hinge
functions. Hinge functions are used in many places for classification and
regression tasks (Breiman (1993)). We have also proved a universal
approximation theorem -- a stack of two morphological blocks can approximate
any continuous function over arbitrary compact sets. To experimentally validate
the efficacy of this network in real-life applications, we have evaluated its
performance on satellite image classification datasets since morphological
operations are very sensitive to geometrical shapes and structures. We have
also shown results on a few tasks like segmentation of blood vessels from
fundus images, segmentation of lungs from chest x-ray and image dehazing. The
results are encouraging and further establishes the potential of morphological
networks.
</p>
<a href="http://arxiv.org/abs/1901.00109" target="_blank">arXiv:1901.00109</a> [<a href="http://arxiv.org/pdf/1901.00109" target="_blank">pdf</a>]

<h2>Hierarchical Multi-task Deep Neural Network Architecture for End-to-End Driving. (arXiv:1902.03466v2 [cs.LG] UPDATED)</h2>
<h3>Jose Solomon, Francois Charette</h3>
<p>A novel hierarchical Deep Neural Network (DNN) model is presented to address
the task of end-to-end driving. The model consists of a master classifier
network which determines the driving task required from an input stereo image
and directs said image to one of a set of subservient network regression models
that perform inference and output a steering command. These subservient
networks are designed and trained for a specific driving task: straightaway,
swerve maneuver, tight turn, gradual turn, and chicane. Using this modular
network strategy allows for two primary advantages: an overall reduction in the
amount of data required to train the complete system, and for model tailoring
where more complex models can be used for more challenging tasks while
simplified networks can handle more mundane tasks. It is this latter facet of
the model that makes the approach attractive to a number of applications beyond
the current vehicle steering strategy.
</p>
<a href="http://arxiv.org/abs/1902.03466" target="_blank">arXiv:1902.03466</a> [<a href="http://arxiv.org/pdf/1902.03466" target="_blank">pdf</a>]

<h2>GCOMB: Learning Budget-constrained Combinatorial Algorithms over Billion-sized Graphs. (arXiv:1903.03332v4 [cs.LG] UPDATED)</h2>
<h3>Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, Ambuj Singh</h3>
<p>There has been an increased interest in discovering heuristics for
combinatorial problems on graphs through machine learning. While existing
techniques have primarily focused on obtaining high-quality solutions,
scalability to billion-sized graphs has not been adequately addressed. In
addition, the impact of budget-constraint, which is necessary for many
practical scenarios, remains to be studied. In this paper, we propose a
framework called GCOMB to bridge these gaps. GCOMB trains a Graph Convolutional
Network (GCN) using a novel probabilistic greedy mechanism to predict the
quality of a node. To further facilitate the combinatorial nature of the
problem, GCOMB utilizes a Q-learning framework, which is made efficient through
importance sampling. We perform extensive experiments on real graphs to
benchmark the efficiency and efficacy of GCOMB. Our results establish that
GCOMB is 100 times faster and marginally better in quality than
state-of-the-art algorithms for learning combinatorial algorithms.
Additionally, a case-study on the practical combinatorial problem of Influence
Maximization (IM) shows GCOMB is 150 times faster than the specialized IM
algorithm IMM with similar quality.
</p>
<a href="http://arxiv.org/abs/1903.03332" target="_blank">arXiv:1903.03332</a> [<a href="http://arxiv.org/pdf/1903.03332" target="_blank">pdf</a>]

<h2>Weight Map Layer for Noise and Adversarial Attack Robustness. (arXiv:1905.00568v2 [cs.LG] UPDATED)</h2>
<h3>Mohammed Amer, Tom&#xe1;s Maul</h3>
<p>Convolutional neural networks (CNNs) are known for their good performance and
generalization in vision-related tasks and have become state-of-the-art in both
application and research-based domains. However, just like other neural network
models, they suffer from a susceptibility to noise and adversarial attacks. An
adversarial defence aims at reducing a neural network's susceptibility to
adversarial attacks through learning or architectural modifications. We propose
the weight map layer (WM) as a generic architectural addition to CNNs and show
that it can increase their robustness to noise and adversarial attacks. We
further explain that the enhanced robustness of the two WM variants results
from the adaptive activation-variance amplification exhibited by the layer. We
show that the WM layer can be integrated into scaled up models to increase
their noise and adversarial attack robustness, while achieving comparable
accuracy levels across different datasets.
</p>
<a href="http://arxiv.org/abs/1905.00568" target="_blank">arXiv:1905.00568</a> [<a href="http://arxiv.org/pdf/1905.00568" target="_blank">pdf</a>]

<h2>Mixed pooling of seasonality for time series forecasting: An application to pallet transport data. (arXiv:1908.05339v2 [stat.ML] UPDATED)</h2>
<h3>Hyunji Moon, Bomi Song, Hyeonseop Lee</h3>
<p>Multiple seasonal patterns play a key role in time series forecasting,
especially for business time series where seasonal effects are often dramatic.
Previous approaches including Fourier decomposition, exponential smoothing, and
seasonal autoregressive integrated moving average (SARIMA) models do not
reflect the distinct characteristics of each period in seasonal patterns. We
propose a mixed hierarchical seasonality (MHS) model. Intermediate parameters
for each seasonal period are first estimated, and a mixture of intermediate
parameters is taken. This results in a model that automatically learns the
relative importance of each seasonality and addresses the interactions between
them. The model is implemented with Stan, a probabilistic language, and was
compared with three existing models on a real-world dataset of pallet transport
from a logistic network. Our new model achieved considerable improvements in
terms of out of sample prediction error (MAPE) and predictive density (ELPD)
compared to complete pooling, Fourier decomposition, and SARIMA model.
</p>
<a href="http://arxiv.org/abs/1908.05339" target="_blank">arXiv:1908.05339</a> [<a href="http://arxiv.org/pdf/1908.05339" target="_blank">pdf</a>]

<h2>Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative Adversarial Networks. (arXiv:1909.02215v3 [cs.CV] UPDATED)</h2>
<h3>Baris Gecer, Alexander Lattas, Stylianos Ploumpis, Jiankang Deng, Athanasios Papaioannou, Stylianos Moschoglou, Stefanos Zafeiriou</h3>
<p>Generating realistic 3D faces is of high importance for computer graphics and
computer vision applications. Generally, research on 3D face generation
revolves around linear statistical models of the facial surface. Nevertheless,
these models cannot represent faithfully either the facial texture or the
normals of the face, which are very crucial for photo-realistic face synthesis.
Recently, it was demonstrated that Generative Adversarial Networks (GANs) can
be used for generating high-quality textures of faces. Nevertheless, the
generation process either omits the geometry and normals, or independent
processes are used to produce 3D shape information. In this paper, we present
the first methodology that generates high-quality texture, shape, and normals
jointly, which can be used for photo-realistic synthesis. To do so, we propose
a novel GAN that can generate data from different modalities while exploiting
their correlations. Furthermore, we demonstrate how we can condition the
generation on the expression and create faces with various facial expressions.
The qualitative results shown in this paper are compressed due to size
limitations, full-resolution results and the accompanying video can be found in
the supplementary documents. The code and models are available at the project
page: https://github.com/barisgecer/TBGAN.
</p>
<a href="http://arxiv.org/abs/1909.02215" target="_blank">arXiv:1909.02215</a> [<a href="http://arxiv.org/pdf/1909.02215" target="_blank">pdf</a>]

<h2>Blind Network Revenue Management and Bandits with Knapsacks under Limited Switches. (arXiv:1911.01067v4 [cs.LG] UPDATED)</h2>
<h3>David Simchi-Levi, Yunzong Xu, Jinglong Zhao</h3>
<p>Our work is motivated by a common business constraint in online markets.
While firms respect the advantages of dynamic pricing and price
experimentation, they must limit the number of price changes (i.e., switches)
to be within some budget due to various practical reasons. We study both the
classical price-based network revenue management problem in the
distributionally-unknown setup, and the bandits with knapsacks problem. In
these problems, a decision-maker (without prior knowledge of the environment)
has finite initial inventory of multiple resources to allocate over a finite
time horizon. Beyond the classical resource constraints, we introduce an
additional switching constraint to these problems, which restricts the total
number of times that the decision-maker makes switches between actions to be
within a fixed switching budget. For such problems, we show matching upper and
lower bounds on the optimal regret, and propose computationally-efficient
limited-switch algorithms that achieve the optimal regret. Our work reveals a
surprising result: the optimal regret rate is completely characterized by a
piecewise-constant function of the switching budget, which further depends on
the number of resource constraints -- to the best of our knowledge, this is the
first time the number of resources constraints is shown to play a fundamental
role in determining the statistical complexity of online learning problems. We
conduct computational experiments to examine the performance of our algorithms
on a numerical setup that is widely used in the literature. Compared with
benchmark algorithms from the literature, our proposed algorithms achieve
promising performance with clear advantages on the number of incurred switches.
Practically, firms can benefit from our study and improve their learning and
decision-making performance when they simultaneously face resource and
switching constraints.
</p>
<a href="http://arxiv.org/abs/1911.01067" target="_blank">arXiv:1911.01067</a> [<a href="http://arxiv.org/pdf/1911.01067" target="_blank">pdf</a>]

<h2>A Programmable Approach to Neural Network Compression. (arXiv:1911.02497v2 [cs.LG] UPDATED)</h2>
<h3>Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan</h3>
<p>Deep neural networks (DNNs) frequently contain far more weights, represented
at a higher precision, than are required for the specific task which they are
trained to perform. Consequently, they can often be compressed using techniques
such as weight pruning and quantization that reduce both the model size and
inference time without appreciable loss in accuracy. However, finding the best
compression strategy and corresponding target sparsity for a given DNN,
hardware platform, and optimization objective currently requires expensive,
frequently manual, trial-and-error experimentation. In this paper, we introduce
a programmable system for model compression called Condensa. Users
programmatically compose simple operators, in Python, to build more complex and
practically interesting compression strategies. Given a strategy and
user-provided objective (such as minimization of running time), Condensa uses a
novel Bayesian optimization-based algorithm to automatically infer desirable
sparsities. Our experiments on four real-world DNNs demonstrate memory
footprint and hardware runtime throughput improvements of 188x and 2.59x,
respectively, using at most ten samples per search. We have released a
reference implementation of Condensa at https://github.com/NVlabs/condensa.
</p>
<a href="http://arxiv.org/abs/1911.02497" target="_blank">arXiv:1911.02497</a> [<a href="http://arxiv.org/pdf/1911.02497" target="_blank">pdf</a>]

<h2>Convolutional STN for Weakly Supervised Object Localization. (arXiv:1912.01522v2 [cs.CV] UPDATED)</h2>
<h3>Akhil Meethal, Marco Pedersoli, Soufiane Belharbi, Eric Granger</h3>
<p>Weakly supervised object localization is a challenging task in which the
object of interest should be localized while learning its appearance.
State-of-the-art methods recycle the architecture of a standard CNN by using
the activation maps of the last layer for localizing the object. While this
approach is simple and works relatively well, object localization relies on
different features than classification, thus, a specialized localization
mechanism is required during training to improve performance. In this paper, we
propose a convolutional, multi-scale spatial localization network that provides
accurate localization for the object of interest. Experimental results on
CUB-200-2011 and ImageNet datasets show that our proposed approach provides
competitive performance for weakly supervised localization.
</p>
<a href="http://arxiv.org/abs/1912.01522" target="_blank">arXiv:1912.01522</a> [<a href="http://arxiv.org/pdf/1912.01522" target="_blank">pdf</a>]

<h2>Deep-Learning Estimation of Band Gap with the Reading-Periodic-Table Method and Periodic Convolution Layer. (arXiv:1912.05916v3 [cs.LG] UPDATED)</h2>
<h3>Tomohiko Konno</h3>
<p>We verified that the deep learning method named reading periodic table
introduced by ref. Deep Learning Model for Finding New Superconductors, which
utilizes deep learning to read the periodic table and the laws of the elements,
is applicable not only for superconductors, for which the method was originally
applied but also for other problems of materials by demonstrating band gap
estimations. We then extended the method to learn the laws better by directly
learning the cylindrical periodicity between the right- and left-most columns
in the periodic table at the learning representation level, that is, by
considering the left- and right-most columns to be adjacent to each other.
Thus, while the original method handles the table as is, the extended method
treats the periodic table as if its two edges are connected. This is achieved
using novel layers named periodic convolution layers, which can handle inputs
exhibiting periodicity and may be applied to other problems related to computer
vision, time series, and so on for data that possess some periodicity. In the
reading periodic table method, no material feature or descriptor is required as
input. We demonstrated two types of deep learning estimation: methods to
estimate the existence of a bandgap, and methods to estimate the value of the
bandgap given when the existence of the bandgap in the materials is known.
Finally, we discuss the limitations of the dataset and model evaluation method.
We may be unable to distinguish good models based on the random train-test
split scheme; thus, we must prepare an appropriate dataset where the training
and test data are temporally separate. The code and data are open.
</p>
<a href="http://arxiv.org/abs/1912.05916" target="_blank">arXiv:1912.05916</a> [<a href="http://arxiv.org/pdf/1912.05916" target="_blank">pdf</a>]

<h2>Joint Learning of Generative Translator and Classifier for Visually Similar Classes. (arXiv:1912.06994v2 [cs.CV] UPDATED)</h2>
<h3>ByungIn Yoo, Tristan Sylvain, Yoshua Bengio, Junmo Kim</h3>
<p>In this paper, we propose a Generative Translation Classification Network
(GTCN) for improving visual classification accuracy in settings where classes
are visually similar and data is scarce. For this purpose, we propose joint
learning from a scratch to train a classifier and a generative stochastic
translation network end-to-end. The translation network is used to perform
on-line data augmentation across classes, whereas previous works have mostly
involved domain adaptation. To help the model further benefit from this
data-augmentation, we introduce an adaptive fade-in loss and a quadruplet loss.
We perform experiments on multiple datasets to demonstrate the proposed
method's performance in varied settings. Of particular interest, training on
40% of the dataset is enough for our model to surpass the performance of
baselines trained on the full dataset. When our architecture is trained on the
full dataset, we achieve comparable performance with state-of-the-art methods
despite using a light-weight architecture.
</p>
<a href="http://arxiv.org/abs/1912.06994" target="_blank">arXiv:1912.06994</a> [<a href="http://arxiv.org/pdf/1912.06994" target="_blank">pdf</a>]

<h2>Depth Completion Using a View-constrained Deep Prior. (arXiv:2001.07791v3 [cs.CV] UPDATED)</h2>
<h3>Pallabi Ghosh, Vibhav Vineet, Larry S. Davis, Abhinav Shrivastava, Sudipta Sinha, Neel Joshi</h3>
<p>Recent work has shown that the structure of convolutional neural networks
(CNNs) induces a strong prior that favors natural images. This prior, known as
a deep image prior (DIP), is an effective regularizer in inverse problems such
as image denoising and inpainting. We extend the concept of the DIP to depth
images. Given color images and noisy and incomplete target depth maps, we
optimize a randomly-initialized CNN model to reconstruct a depth map restored
by virtue of using the CNN network structure as a prior combined with a
view-constrained photo-consistency loss. This loss is computed using images
from a geometrically calibrated camera from nearby viewpoints. We apply this
deep depth prior for inpainting and refining incomplete and noisy depth maps
within both binocular and multi-view stereo pipelines. Our quantitative and
qualitative evaluation shows that our refined depth maps are more accurate and
complete, and after fusion, produces dense 3D models of higher quality.
</p>
<a href="http://arxiv.org/abs/2001.07791" target="_blank">arXiv:2001.07791</a> [<a href="http://arxiv.org/pdf/2001.07791" target="_blank">pdf</a>]

<h2>Deep Moment Matching Kernel for Multi-source Gaussian Processes. (arXiv:2002.02826v2 [cs.LG] UPDATED)</h2>
<h3>Chi-Ken Lu, Patrick Shafto</h3>
<p>Human learners have the ability to solve new tasks efficiently if previous
knowledge is relevant, which has motivated research into few-shot learning and
transfer learning. We formalize the integration of relevant knowledge as
multi-source regression in which the target function is inferred using Gaussian
Process (GP) with the deep moment matching (DMM) kernel. We obtain a
non-stationary DMM kernel from prior relevant data by analytically calculating
the covariance of the target function. We interpret the data-informed DMM
kernel, which serves as prior for target function, as: (1) a refined similarity
determined by squared distance in the latent space and (2) as propagating
uncertainty measured in RKHS defined by the posterior covariance from the prior
learning. In comparison with the autoregressive models, variational DGP models
and others, results show GP regression with the DMM kernels is effective when
applying to the standard synthetic and real-world multi-fidelity data sets.
</p>
<a href="http://arxiv.org/abs/2002.02826" target="_blank">arXiv:2002.02826</a> [<a href="http://arxiv.org/pdf/2002.02826" target="_blank">pdf</a>]

<h2>t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections. (arXiv:2002.06910v4 [cs.LG] UPDATED)</h2>
<h3>Angelos Chatzimparmpas, Rafael M. Martins, Andreas Kerren</h3>
<p>t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of
multidimensional data has proven to be a popular approach, with successful
applications in a wide range of domains. Despite their usefulness, t-SNE
projections can be hard to interpret or even misleading, which hurts the
trustworthiness of the results. Understanding the details of t-SNE itself and
the reasons behind specific patterns in its output may be a daunting task,
especially for non-experts in dimensionality reduction. In this work, we
present t-viSNE, an interactive tool for the visual exploration of t-SNE
projections that enables analysts to inspect different aspects of their
accuracy and meaning, such as the effects of hyper-parameters, distance and
neighborhood preservation, densities and costs of specific neighborhoods, and
the correlations between dimensions and visual patterns. We propose a coherent,
accessible, and well-integrated collection of different views for the
visualization of t-SNE projections. The applicability and usability of t-viSNE
are demonstrated through hypothetical usage scenarios with real data sets.
Finally, we present the results of a user study where the tool's effectiveness
was evaluated. By bringing to light information that would normally be lost
after running t-SNE, we hope to support analysts in using t-SNE and making its
results better understandable.
</p>
<a href="http://arxiv.org/abs/2002.06910" target="_blank">arXiv:2002.06910</a> [<a href="http://arxiv.org/pdf/2002.06910" target="_blank">pdf</a>]

<h2>Analyzing Neural Networks Based on Random Graphs. (arXiv:2002.08104v3 [cs.LG] UPDATED)</h2>
<h3>Romuald A. Janik, Aleksandra Nowak</h3>
<p>We perform a massive evaluation of neural networks with architectures
corresponding to random graphs of various types. We investigate various
structural and numerical properties of the graphs in relation to neural network
test accuracy. We find that none of the classical numerical graph invariants by
itself allows to single out the best networks. Consequently, we introduce a new
numerical graph characteristic that selects a set of quasi-1-dimensional
graphs, which are a majority among the best performing networks. We also find
that networks with primarily short-range connections perform better than
networks which allow for many long-range connections. Moreover, many resolution
reducing pathways are beneficial. We provide a dataset of 1020 graphs and the
test accuracies of their corresponding neural networks at
https://github.com/rmldj/random-graph-nn-paper
</p>
<a href="http://arxiv.org/abs/2002.08104" target="_blank">arXiv:2002.08104</a> [<a href="http://arxiv.org/pdf/2002.08104" target="_blank">pdf</a>]

<h2>BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images. (arXiv:2002.08988v4 [cs.CV] UPDATED)</h2>
<h3>Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, Niloy Mitra</h3>
<p>We present BlockGAN, an image generative model that learns object-aware 3D
scene representations directly from unlabelled 2D images. Current work on scene
representation learning either ignores scene background or treats the whole
scene as one object. Meanwhile, work that considers scene compositionality
treats scene objects only as image patches or 2D layers with alpha maps.
Inspired by the computer graphics pipeline, we design BlockGAN to learn to
first generate 3D features of background and foreground objects, then combine
them into 3D features for the wholes cene, and finally render them into
realistic images. This allows BlockGAN to reason over occlusion and interaction
between objects' appearance, such as shadow and lighting, and provides control
over each object's 3D pose and identity, while maintaining image realism.
BlockGAN is trained end-to-end, using only unlabelled single images, without
the need for 3D geometry, pose labels, object masks, or multiple views of the
same scene. Our experiments show that using explicit 3D features to represent
objects allows BlockGAN to learn disentangled representations both in terms of
objects (foreground and background) and their properties (pose and identity).
</p>
<a href="http://arxiv.org/abs/2002.08988" target="_blank">arXiv:2002.08988</a> [<a href="http://arxiv.org/pdf/2002.08988" target="_blank">pdf</a>]

<h2>Adaptive Federated Optimization. (arXiv:2003.00295v3 [cs.LG] UPDATED)</h2>
<h3>Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone&#x10d;n&#xfd;, Sanjiv Kumar, H. Brendan McMahan</h3>
<p>Federated learning is a distributed machine learning paradigm in which a
large number of clients coordinate with a central server to learn a model
without sharing their own training data. Standard federated optimization
methods such as Federated Averaging (FedAvg) are often difficult to tune and
exhibit unfavorable convergence behavior. In non-federated settings, adaptive
optimization methods have had notable success in combating such issues. In this
work, we propose federated versions of adaptive optimizers, including Adagrad,
Adam, and Yogi, and analyze their convergence in the presence of heterogeneous
data for general non-convex settings. Our results highlight the interplay
between client heterogeneity and communication efficiency. We also perform
extensive experiments on these methods and show that the use of adaptive
optimizers can significantly improve the performance of federated learning.
</p>
<a href="http://arxiv.org/abs/2003.00295" target="_blank">arXiv:2003.00295</a> [<a href="http://arxiv.org/pdf/2003.00295" target="_blank">pdf</a>]

<h2>Maximal Causes for Exponential Family Observables. (arXiv:2003.02214v2 [cs.LG] UPDATED)</h2>
<h3>S. Hamid Mousavi, Jakob Drefs, Florian Hirschberger, J&#xf6;rg L&#xfc;cke</h3>
<p>Latent variable models represent observed variables as parameterized
functions of a set of latent variables. Examples are factor analysis or
probabilistic sparse coding which assume weighted linear summations to
determine the mean of Gaussian distribution for the observables. However, in
many cases observables do not follow a normal distribution, and a linear
summation of latents is often at odds with non-Gaussian observables (e.g.,
means of the Bernoulli distribution have to lie in the unit interval).
Furthermore, the assumption of a linear summation model may (for many types of
data) not be closely aligned with the true data generation process even for
Gaussian observables. Alternative superposition models (i.e., alternative links
between latents and observables) have therefore been investigated repeatedly.
Here we show that using the maximization instead of summation to link latents
to observables allows for the derivation of a very general and concise set of
parameter update equations. Concretely, we derive a set of update equations
that has the same functional form for all distributions of the exponential
family. Our results consequently provide directly applicable learning equations
for commonly as well as for unusually distributed data. We numerically verify
our analytical results assuming standard Gaussian, Gamma, Poisson, Bernoulli
and Exponential distributions. We point to some potential applications by
providing different experiments on the learning of variance structure, noise
type estimation, and denoising.
</p>
<a href="http://arxiv.org/abs/2003.02214" target="_blank">arXiv:2003.02214</a> [<a href="http://arxiv.org/pdf/2003.02214" target="_blank">pdf</a>]

<h2>BayesFlow: Learning complex stochastic models with invertible neural networks. (arXiv:2003.06281v4 [stat.ML] UPDATED)</h2>
<h3>Stefan T. Radev, Ulf K. Mertens, Andreass Voss, Lynton Ardizzone, Ullrich K&#xf6;the</h3>
<p>Estimating the parameters of mathematical models is a common problem in
almost all branches of science. However, this problem can prove notably
difficult when processes and model descriptions become increasingly complex and
an explicit likelihood function is not available. With this work, we propose a
novel method for globally amortized Bayesian inference based on invertible
neural networks which we call BayesFlow. The method uses simulation to learn a
global estimator for the probabilistic mapping from observed data to underlying
model parameters. A neural network pre-trained in this way can then, without
additional training or optimization, infer full posteriors on arbitrary many
real datasets involving the same model family. In addition, our method
incorporates a summary network trained to embed the observed data into
maximally informative summary statistics. Learning summary statistics from data
makes the method applicable to modeling scenarios where standard inference
techniques with hand-crafted summary statistics fail. We demonstrate the
utility of BayesFlow on challenging intractable models from population
dynamics, epidemiology, cognitive science and ecology. We argue that BayesFlow
provides a general framework for building amortized Bayesian parameter
estimation machines for any forward model from which data can be simulated.
</p>
<a href="http://arxiv.org/abs/2003.06281" target="_blank">arXiv:2003.06281</a> [<a href="http://arxiv.org/pdf/2003.06281" target="_blank">pdf</a>]

<h2>Rethinking Object Detection in Retail Stores. (arXiv:2003.08230v2 [cs.CV] UPDATED)</h2>
<h3>Yuanqiang Cai, Longyin Wen, Libo Zhang, Dawei Du, Weiqiang Wang</h3>
<p>The convention standard for object detection uses a bounding box to represent
each individual object instance. However, it is not practical in the
industry-relevant applications in the context of warehouses due to severe
occlusions among groups of instances of the same categories. In this paper, we
propose a new task, ie, simultaneously object localization and counting,
abbreviated as Locount, which requires algorithms to localize groups of objects
of interest with the number of instances. However, there does not exist a
dataset or benchmark designed for such a task. To this end, we collect a
large-scale object localization and counting dataset with rich annotations in
retail stores, which consists of 50,394 images with more than 1.9 million
object instances in 140 categories. Together with this dataset, we provide a
new evaluation protocol and divide the training and testing subsets to fairly
evaluate the performance of algorithms for Locount, developing a new benchmark
for the Locount task. Moreover, we present a cascaded localization and counting
network as a strong baseline, which gradually classifies and regresses the
bounding boxes of objects with the predicted numbers of instances enclosed in
the bounding boxes, trained in an end-to-end manner. Extensive experiments are
conducted on the proposed dataset to demonstrate its significance and the
analysis discussions on failure cases are provided to indicate future
directions. Dataset is available at
https://isrc.iscas.ac.cn/gitlab/research/locount-dataset.
</p>
<a href="http://arxiv.org/abs/2003.08230" target="_blank">arXiv:2003.08230</a> [<a href="http://arxiv.org/pdf/2003.08230" target="_blank">pdf</a>]

<h2>SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning. (arXiv:2004.11246v2 [cs.CV] UPDATED)</h2>
<h3>Ignacio Serna, Aythami Morales, Julian Fierrez, Manuel Cebrian, Nick Obradovich, Iyad Rahwan</h3>
<p>We propose a discrimination-aware learning method to improve both accuracy
and fairness of biased face recognition algorithms. The most popular face
recognition benchmarks assume a distribution of subjects without paying much
attention to their demographic attributes. In this work, we perform a
comprehensive discrimination-aware experimentation of deep learning-based face
recognition. We also propose a general formulation of algorithmic
discrimination with application to face biometrics. The experiments include
tree popular face recognition models and three public databases composed of
64,000 identities from different demographic groups characterized by gender and
ethnicity. We experimentally show that learning processes based on the most
used face databases have led to popular pre-trained deep face models that
present a strong algorithmic discrimination. We finally propose a
discrimination-aware learning method, Sensitive Loss, based on the popular
triplet loss function and a sensitive triplet generator. Our approach works as
an add-on to pre-trained networks and is used to improve their performance in
terms of average accuracy and fairness. The method shows results comparable to
state-of-the-art de-biasing networks and represents a step forward to prevent
discriminatory effects by automatic systems.
</p>
<a href="http://arxiv.org/abs/2004.11246" target="_blank">arXiv:2004.11246</a> [<a href="http://arxiv.org/pdf/2004.11246" target="_blank">pdf</a>]

<h2>Addressing Artificial Intelligence Bias in Retinal Disease Diagnostics. (arXiv:2004.13515v4 [cs.AI] UPDATED)</h2>
<h3>Philippe Burlina, Neil Joshi, William Paul, Katia D. Pacheco, Neil M. Bressler</h3>
<p>This study evaluated generative methods to potentially mitigate AI bias when
diagnosing diabetic retinopathy (DR) resulting from training data imbalance, or
domain generalization which occurs when deep learning systems (DLS) face
concepts at test/inference time they were not initially trained on. The public
domain Kaggle-EyePACS dataset (88,692 fundi and 44,346 individuals, originally
diverse for ethnicity) was modified by adding clinician-annotated labels and
constructing an artificial scenario of data imbalance and domain generalization
by disallowing training (but not testing) exemplars for images of retinas with
DR warranting referral (DR-referable) and from darker-skin individuals, who
presumably have greater concentration of melanin within uveal melanocytes, on
average, contributing to retinal image pigmentation. A traditional/baseline
diagnostic DLS was compared against new DLSs that would use training data
augmented via generative models for debiasing. Accuracy (95% confidence
intervals [CI]) of the baseline diagnostics DLS for fundus images of
lighter-skin individuals was 73.0% (66.9%, 79.2%) vs. darker-skin of 60.5%
(53.5%, 67.3%), demonstrating bias/disparity (delta=12.5%) (Welch t-test
t=2.670, P=.008) in AI performance across protected subpopulations. Using novel
generative methods for addressing missing subpopulation training data
(DR-referable darker-skin) achieved instead accuracy, for lighter-skin, of
72.0% (65.8%, 78.2%), and for darker-skin, of 71.5% (65.2%,77.8%),
demonstrating closer parity (delta=0.5%) in accuracy across subpopulations
(Welch t-test t=0.111, P=.912). Findings illustrate how data imbalance and
domain generalization can lead to disparity of accuracy across subpopulations,
and show that novel generative methods of synthetic fundus images may play a
role for debiasing AI.
</p>
<a href="http://arxiv.org/abs/2004.13515" target="_blank">arXiv:2004.13515</a> [<a href="http://arxiv.org/pdf/2004.13515" target="_blank">pdf</a>]

<h2>Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context. (arXiv:2005.00589v2 [cs.CV] UPDATED)</h2>
<h3>Xinyi Zheng, Doug Burdick, Lucian Popa, Xu Zhong, Nancy Xin Ru Wang</h3>
<p>Documents are often used for knowledge sharing and preservation in business
and science, within which are tables that capture most of the critical data.
Unfortunately, most documents are stored and distributed as PDF or scanned
images, which fail to preserve logical table structure. Recent vision-based
deep learning approaches have been proposed to address this gap, but most still
cannot achieve state-of-the-art results. We present Global Table Extractor
(GTE), a vision-guided systematic framework for joint table detection and cell
structured recognition, which could be built on top of any object detection
model. With GTE-Table, we invent a new penalty based on the natural cell
containment constraint of tables to train our table network aided by cell
location predictions. GTE-Cell is a new hierarchical cell detection network
that leverages table styles. Further, we design a method to automatically label
table and cell structure in existing documents to cheaply create a large corpus
of training and test data. We use this to enhance PubTabNet with cell labels
and create FinTabNet, real-world and complex scientific and financial datasets
with detailed table structure annotations to help train and test structure
recognition. Our framework surpasses previous state-of-the-art results on the
ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell
structure recognition with a significant 5.8% improvement in the full table
extraction system. Further experiments demonstrate a greater than 45%
improvement in cell structure recognition when compared to a vanilla RetinaNet
object detection model in our new out-of-domain FinTabNet.
</p>
<a href="http://arxiv.org/abs/2005.00589" target="_blank">arXiv:2005.00589</a> [<a href="http://arxiv.org/pdf/2005.00589" target="_blank">pdf</a>]

<h2>Robust Non-Linear Matrix Factorization for Dictionary Learning, Denoising, and Clustering. (arXiv:2005.01317v2 [cs.LG] UPDATED)</h2>
<h3>Jicong Fan, Chengrun Yang, Madeleine Udell</h3>
<p>Low dimensional nonlinear structure abounds in datasets across computer
vision and machine learning. Kernelized matrix factorization techniques have
recently been proposed to learn these nonlinear structures for denoising,
classification, dictionary learning, and missing data imputation, by observing
that the image of the matrix in a sufficiently large feature space is low-rank.
However, these nonlinear methods fail in the presence of sparse noise or
outliers. In this work, we propose a new robust nonlinear factorization method
called Robust Non-Linear Matrix Factorization (RNLMF). RNLMF constructs a
dictionary for the data space by factoring a kernelized feature space; a noisy
matrix can then be decomposed as the sum of a sparse noise matrix and a clean
data matrix that lies in a low dimensional nonlinear manifold. RNLMF is robust
to sparse noise and outliers and scales to matrices with thousands of rows and
columns. Empirically, RNLMF achieves noticeable improvements over baseline
methods in denoising and clustering.
</p>
<a href="http://arxiv.org/abs/2005.01317" target="_blank">arXiv:2005.01317</a> [<a href="http://arxiv.org/pdf/2005.01317" target="_blank">pdf</a>]

<h2>StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics. (arXiv:2005.01575v8 [cs.LG] UPDATED)</h2>
<h3>Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas Kerren</h3>
<p>In machine learning (ML), ensemble methods such as bagging, boosting, and
stacking are widely-established approaches that regularly achieve top-notch
predictive performance. Stacking (also called "stacked generalization") is an
ensemble method that combines heterogeneous base models, arranged in at least
one layer, and then employs another metamodel to summarize the predictions of
those models. Although it may be a highly-effective approach for increasing the
predictive performance of ML, generating a stack of models from scratch can be
a cumbersome trial-and-error process. This challenge stems from the enormous
space of available solutions, with different sets of data instances and
features that could be used for training, several algorithms to choose from,
and instantiations of these algorithms using diverse parameters (i.e., models)
that perform differently according to various metrics. In this work, we present
a knowledge generation model, which supports ensemble learning with the use of
visualization, and a visual analytics system for stacked generalization. Our
system, StackGenVis, assists users in dynamically adapting performance metrics,
managing data instances, selecting the most important features for a given data
set, choosing a set of top-performant and diverse algorithms, and measuring the
predictive performance. In consequence, our proposed tool helps users to decide
between distinct models and to reduce the complexity of the resulting stack by
removing overpromising and underperforming models. The applicability and
effectiveness of StackGenVis are demonstrated with two use cases: a real-world
healthcare data set and a collection of data related to sentiment/stance
detection in texts. Finally, the tool has been evaluated through interviews
with three ML experts.
</p>
<a href="http://arxiv.org/abs/2005.01575" target="_blank">arXiv:2005.01575</a> [<a href="http://arxiv.org/pdf/2005.01575" target="_blank">pdf</a>]

<h2>Text Recognition in the Wild: A Survey. (arXiv:2005.03492v2 [cs.CV] UPDATED)</h2>
<h3>Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, Tianwei Wang</h3>
<p>The history of text can be traced back over thousands of years. Rich and
precise semantic information carried by text is important in a wide range of
vision-based application scenarios. Therefore, text recognition in natural
scenes has been an active research field in computer vision and pattern
recognition. In recent years, with the rise and development of deep learning,
numerous methods have shown promising in terms of innovation, practicality, and
efficiency. This paper aims to (1) summarize the fundamental problems and the
state-of-the-art associated with scene text recognition; (2) introduce new
insights and ideas; (3) provide a comprehensive review of publicly available
resources; (4) point out directions for future work. In summary, this
literature review attempts to present the entire picture of the field of scene
text recognition. It provides a comprehensive reference for people entering
this field, and could be helpful to inspire future research. Related resources
are available at our Github repository:
https://github.com/HCIILAB/Scene-Text-Recognition.
</p>
<a href="http://arxiv.org/abs/2005.03492" target="_blank">arXiv:2005.03492</a> [<a href="http://arxiv.org/pdf/2005.03492" target="_blank">pdf</a>]

<h2>Recovery and Generalization in Over-Realized Dictionary Learning. (arXiv:2006.06179v2 [cs.LG] UPDATED)</h2>
<h3>Jeremias Sulam, Chong You, Zhihui Zhu</h3>
<p>In over two decades of research, the field of dictionary learning has
gathered a large collection of successful applications, and theoretical
guarantees for model recovery are known only whenever optimization is carried
out in the same model class as that of the underlying dictionary. This work
characterizes the surprising phenomenon that dictionary recovery can be
facilitated by searching over the space of larger over-realized models. This
observation is general and independent of the specific dictionary learning
algorithm used. We thoroughly demonstrate this observation in practice and
provide an analysis of this phenomenon by tying recovery measures to
generalization bounds. In particular, we show that model recovery can be
upper-bounded by the empirical risk, a model-dependent quantity and the
generalization gap, reflecting our empirical findings. We further show that an
efficient and provably correct distillation approach can be employed to recover
the correct atoms from the over-realized model. As a result, our meta-algorithm
provides dictionary estimates with consistently better recovery of the
ground-truth model.
</p>
<a href="http://arxiv.org/abs/2006.06179" target="_blank">arXiv:2006.06179</a> [<a href="http://arxiv.org/pdf/2006.06179" target="_blank">pdf</a>]

<h2>Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond. (arXiv:2006.06525v2 [cs.CV] UPDATED)</h2>
<h3>Wenhao Wang, Fang Zhao, Shengcai Liao, Ling Shao</h3>
<p>Unsupervised domain adaptation (UDA) for person re-identification is
challenging because of the huge gap between the source and target domain. A
typical self-training method is to use pseudo-labels generated by clustering
algorithms to iteratively optimize the model on the target domain. However, a
drawback to this is that noisy pseudo-labels generally cause trouble in
learning. To address this problem, a mutual learning method by dual networks
has been developed to produce reliable soft labels. However, as the two neural
networks gradually converge, their complementarity is weakened and they likely
become biased towards the same kind of noise. This paper proposes a novel
light-weight module, the Attentive WaveBlock (AWB), which can be integrated
into the dual networks of mutual learning to enhance the complementarity and
further depress noise in the pseudo-labels. Specifically, we first introduce a
parameter-free module, the WaveBlock, which creates a difference between
features learned by two networks by waving blocks of feature maps differently.
Then, an attention mechanism is leveraged to enlarge the difference created and
discover more complementary features. Furthermore, two kinds of combination
strategies, i.e. pre-attention and post-attention, are explored. Experiments
demonstrate that the proposed method achieves state-of-the-art performance with
significant improvements on multiple UDA person re-identification tasks. We
also prove the generality of the proposed method by applying it to vehicle
re-identification and image classification tasks. Our codes and models are
available at https://github.com/WangWenhao0716/Attentive-WaveBlock.
</p>
<a href="http://arxiv.org/abs/2006.06525" target="_blank">arXiv:2006.06525</a> [<a href="http://arxiv.org/pdf/2006.06525" target="_blank">pdf</a>]

<h2>Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings. (arXiv:2006.06634v2 [cs.CV] UPDATED)</h2>
<h3>Mihai Dusmanu, Johannes L. Sch&#xf6;nberger, Sudipta N. Sinha, Marc Pollefeys</h3>
<p>Many computer vision systems require users to upload image features to the
cloud for processing and storage. Such image features can be exploited to
recover sensitive information about the scene or subjects, e.g., by
reconstructing the appearance of the original image. To address this privacy
concern, we propose a new privacy-preserving feature representation. The core
idea of our work is to drop constraints from each feature descriptor by
embedding it within an affine subspace containing the original feature as well
as adversarial feature samples. Feature matching on the privacy-preserving
representation is enabled based on the notion of subspace-to-subspace distance.
We experimentally demonstrate the effectiveness of our method and its high
practical relevance for the applications of visual localization and mapping as
well as face authentication. Compared to the original features, our approach
has only marginal impact on performance but makes it significantly more
difficult for an adversary to recover private information.
</p>
<a href="http://arxiv.org/abs/2006.06634" target="_blank">arXiv:2006.06634</a> [<a href="http://arxiv.org/pdf/2006.06634" target="_blank">pdf</a>]

<h2>Data Augmentation for Graph Neural Networks. (arXiv:2006.06830v2 [cs.LG] UPDATED)</h2>
<h3>Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, Neil Shah</h3>
<p>Data augmentation has been widely used to improve generalizability of machine
learning models. However, comparatively little work studies data augmentation
for graphs. This is largely due to the complex, non-Euclidean structure of
graphs, which limits possible manipulation operations. Augmentation operations
commonly used in vision and language have no analogs for graphs. Our work
studies graph data augmentation for graph neural networks (GNNs) in the context
of improving semi-supervised node-classification. We discuss practical and
theoretical motivations, considerations and strategies for graph data
augmentation. Our work shows that neural edge predictors can effectively encode
class-homophilic structure to promote intra-class edges and demote inter-class
edges in given graph structure, and our main contribution introduces the GAug
graph data augmentation framework, which leverages these insights to improve
performance in GNN-based node classification via edge prediction. Extensive
experiments on multiple benchmarks show that augmentation via GAug improves
performance across GNN architectures and datasets.
</p>
<a href="http://arxiv.org/abs/2006.06830" target="_blank">arXiv:2006.06830</a> [<a href="http://arxiv.org/pdf/2006.06830" target="_blank">pdf</a>]

<h2>Layer-wise Learning of Kernel Dependence Networks. (arXiv:2006.08539v3 [stat.ML] UPDATED)</h2>
<h3>Chieh Wu, Aria Masoomi, Arthur Gretton, Jennifer Dy</h3>
<p>Due to recent debate over the biological plausibility of backpropagation
(BP), finding an alternative network optimization strategy has become an active
area of interest. We design a new type of kernel network, that is solved
greedily, to theoretically answer several questions of interest. First, if BP
is difficult to simulate in the brain, are there instead \textit{trivial
network weights} (requiring minimum computation) that allow a greedily trained
network to classify any pattern. Second, can a greedily trained network
converge to a kernel? What kernel will it converge to? Third, is this trivial
solution optimal? How is the optimal solution related to generalization?
Lastly, can we theoretically identify the network width and depth without a
grid search? We prove that the kernel embedding is the trivial solution that
compels the greedy procedure to converge to a kernel with Universal property.
Yet, this trivial solution is not even optimal. By obtaining the optimal
solution spectrally, it provides insight into the generalization of the network
while informing us of the network width and depth.
</p>
<a href="http://arxiv.org/abs/2006.08539" target="_blank">arXiv:2006.08539</a> [<a href="http://arxiv.org/pdf/2006.08539" target="_blank">pdf</a>]

<h2>Maximum Roaming Multi-Task Learning. (arXiv:2006.09762v2 [cs.CV] UPDATED)</h2>
<h3>Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, Maria A. Zuluaga</h3>
<p>Multi-task learning has gained popularity due to the advantages it provides
with respect to resource usage and performance. Nonetheless, the joint
optimization of parameters with respect to multiple tasks remains an active
research topic. Sub-partitioning the parameters between different tasks has
proven to be an efficient way to relax the optimization constraints over the
shared weights, may the partitions be disjoint or overlapping. However, one
drawback of this approach is that it can weaken the inductive bias generally
set up by the joint task optimization. In this work, we present a novel way to
partition the parameter space without weakening the inductive bias.
Specifically, we propose Maximum Roaming, a method inspired by dropout that
randomly varies the parameter partitioning, while forcing them to visit as many
tasks as possible at a regulated frequency, so that the network fully adapts to
each update. We study the properties of our method through experiments on a
variety of visual multi-task data sets. Experimental results suggest that the
regularization brought by roaming has more impact on performance than usual
partitioning optimization strategies. The overall method is flexible, easily
applicable, provides superior regularization and consistently achieves improved
performances compared to recent multi-task learning formulations.
</p>
<a href="http://arxiv.org/abs/2006.09762" target="_blank">arXiv:2006.09762</a> [<a href="http://arxiv.org/pdf/2006.09762" target="_blank">pdf</a>]

<h2>Shapeshifter Networks: Decoupling Layers from Parameters for Scalable and Effective Deep Learning. (arXiv:2006.10598v2 [cs.LG] UPDATED)</h2>
<h3>Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate Saenko</h3>
<p>Fitting a model into GPU memory during training is an increasing concern as
models continue to grow. To address this issue, we present Shapeshifter
Networks (SSNs), a flexible neural network framework that decouples layers from
model weights, enabling us to implement any neural network with an arbitrary
number of parameters. In SSNs each layer obtains weights from a parameter store
that decides where and how to allocate parameters to layers. This can result in
sharing parameters across layers even when they have different sizes or perform
different operations. SSNs do not require any modifications to a model's loss
function or architecture, making them easy to use. Our approach can create
parameter efficient networks by using a relatively small number of weights, or
can improve a model's performance by adding additional model capacity during
training without affecting the computational resources required at test time.
We evaluate SSNs using seven network architectures across diverse tasks that
include image classification, bidirectional image-sentence retrieval, and
phrase grounding, creating high performing models even when using as little as
1% of the parameters.
</p>
<a href="http://arxiv.org/abs/2006.10598" target="_blank">arXiv:2006.10598</a> [<a href="http://arxiv.org/pdf/2006.10598" target="_blank">pdf</a>]

<h2>Supervision Accelerates Pre-training in Contrastive Semi-Supervised Learning of Visual Representations. (arXiv:2006.10803v2 [cs.LG] UPDATED)</h2>
<h3>Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, Michael Rabbat</h3>
<p>We investigate a strategy for improving the efficiency of contrastive
learning of visual representations by leveraging a small amount of supervised
information during pre-training. We propose a semi-supervised loss, SuNCEt,
based on noise-contrastive estimation and neighbourhood component analysis,
that aims to distinguish examples of different classes in addition to the
self-supervised instance-wise pretext tasks. On ImageNet, we find that SuNCEt
can be used to match the semi-supervised learning accuracy of previous
contrastive approaches while using less than half the amount of pre-training
and compute. Our main insight is that leveraging even a small amount of labeled
data during pre-training, and not only during fine-tuning, provides an
important signal that can significantly accelerate contrastive learning of
visual representations. Our code is available online at
github.com/facebookresearch/suncet.
</p>
<a href="http://arxiv.org/abs/2006.10803" target="_blank">arXiv:2006.10803</a> [<a href="http://arxiv.org/pdf/2006.10803" target="_blank">pdf</a>]

<h2>Gaussian Process Regression with Local Explanation. (arXiv:2007.01669v3 [cs.LG] UPDATED)</h2>
<h3>Yuya Yoshikawa, Tomoharu Iwata</h3>
<p>Gaussian process regression (GPR) is a fundamental model used in machine
learning. Owing to its accurate prediction with uncertainty and versatility in
handling various data structures via kernels, GPR has been successfully used in
various applications. However, in GPR, how the features of an input contribute
to its prediction cannot be interpreted. Herein, we propose GPR with local
explanation, which reveals the feature contributions to the prediction of each
sample, while maintaining the predictive performance of GPR. In the proposed
model, both the prediction and explanation for each sample are performed using
an easy-to-interpret locally linear model. The weight vector of the locally
linear model is assumed to be generated from multivariate Gaussian process
priors. The hyperparameters of the proposed models are estimated by maximizing
the marginal likelihood. For a new test sample, the proposed model can predict
the values of its target variable and weight vector, as well as their
uncertainties, in a closed form. Experimental results on various benchmark
datasets verify that the proposed model can achieve predictive performance
comparable to those of GPR and superior to that of existing interpretable
models, and can achieve higher interpretability than them, both quantitatively
and qualitatively.
</p>
<a href="http://arxiv.org/abs/2007.01669" target="_blank">arXiv:2007.01669</a> [<a href="http://arxiv.org/pdf/2007.01669" target="_blank">pdf</a>]

<h2>Patch-wise Attack for Fooling Deep Neural Network. (arXiv:2007.06765v3 [cs.CV] UPDATED)</h2>
<h3>Lianli Gao, Qilong Zhang, Jingkuan Song, Xianglong Liu, Heng Tao Shen</h3>
<p>By adding human-imperceptible noise to clean images, the resultant
adversarial examples can fool other unknown models. Features of a pixel
extracted by deep neural networks (DNNs) are influenced by its surrounding
regions, and different DNNs generally focus on different discriminative regions
in recognition. Motivated by this, we propose a patch-wise iterative algorithm
-- a black-box attack towards mainstream normally trained and defense models,
which differs from the existing attack methods manipulating pixel-wise noise.
In this way, without sacrificing the performance of white-box attack, our
adversarial examples can have strong transferability. Specifically, we
introduce an amplification factor to the step size in each iteration, and one
pixel's overall gradient overflowing the $\epsilon$-constraint is properly
assigned to its surrounding regions by a project kernel. Our method can be
generally integrated to any gradient-based attack methods. Compared with the
current state-of-the-art attacks, we significantly improve the success rate by
9.2\% for defense models and 3.7\% for normally trained models on average. Our
code is available at
\url{https://github.com/qilong-zhang/Patch-wise-iterative-attack}
</p>
<a href="http://arxiv.org/abs/2007.06765" target="_blank">arXiv:2007.06765</a> [<a href="http://arxiv.org/pdf/2007.06765" target="_blank">pdf</a>]

<h2>Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows. (arXiv:2007.10973v2 [cs.CV] UPDATED)</h2>
<h3>Kunal Gupta, Manmohan Chandraker</h3>
<p>Meshes are important representations of physical 3D entities in the virtual
world. Applications like rendering, simulations and 3D printing require meshes
to be manifold so that they can interact with the world like the real objects
they represent. Prior methods generate meshes with great geometric accuracy but
poor manifoldness. In this work, we propose Neural Mesh Flow (NMF) to generate
two-manifold meshes for genus-0 shapes. Specifically, NMF is a shape
auto-encoder consisting of several Neural Ordinary Differential Equation
(NODE)[1] blocks that learn accurate mesh geometry by progressively deforming a
spherical mesh. Training NMF is simpler compared to state-of-the-art methods
since it does not require any explicit mesh-based regularization. Our
experiments demonstrate that NMF facilitates several applications such as
single-view mesh reconstruction, global shape parameterization, texture
mapping, shape deformation and correspondence. Importantly, we demonstrate that
manifold meshes generated using NMF are better-suited for physically-based
rendering and simulation. Code and data are released.
</p>
<a href="http://arxiv.org/abs/2007.10973" target="_blank">arXiv:2007.10973</a> [<a href="http://arxiv.org/pdf/2007.10973" target="_blank">pdf</a>]

<h2>Generative Classifiers as a Basis for Trustworthy Image Classification. (arXiv:2007.15036v2 [cs.CV] UPDATED)</h2>
<h3>Radek Mackowiak, Lynton Ardizzone, Ullrich K&#xf6;the, Carsten Rother</h3>
<p>With the maturing of deep learning systems, trustworthiness is becoming
increasingly important for model assessment. We understand trustworthiness as
the combination of explainability and robustness. Generative classifiers (GCs)
are a promising class of models that are said to naturally accomplish these
qualities. However, this has mostly been demonstrated on simple datasets such
as MNIST and CIFAR in the past. In this work, we firstly develop an
architecture and training scheme that allows GCs to operate on a more relevant
level of complexity for practical computer vision, namely the ImageNet
challenge. Secondly, we demonstrate the immense potential of GCs for
trustworthy image classification. Explainability and some aspects of robustness
are vastly improved compared to feed-forward models, even when the GCs are just
applied naively. While not all trustworthiness problems are solved completely,
we observe that GCs are a highly promising basis for further algorithms and
modifications. We release our trained model for download in the hope that it
serves as a starting point for other generative classification tasks, in much
the same way as pretrained ResNet architectures do for discriminative
classification.
</p>
<a href="http://arxiv.org/abs/2007.15036" target="_blank">arXiv:2007.15036</a> [<a href="http://arxiv.org/pdf/2007.15036" target="_blank">pdf</a>]

<h2>A Multi-Task Learning Approach for Human Activity Segmentation and Ergonomics Risk Assessment. (arXiv:2008.03014v2 [cs.CV] UPDATED)</h2>
<h3>Behnoosh Parsa, Ashis G. Banerjee</h3>
<p>We propose a new approach to Human Activity Evaluation (HAE) in long videos
using graph-based multi-task modeling. Previous works in activity evaluation
either directly compute a metric using a detected skeleton or use the scene
information to regress the activity score. These approaches are insufficient
for accurate activity assessment since they only compute an average score over
a clip, and do not consider the correlation between the joints and body
dynamics. Moreover, they are highly scene-dependent which makes the
generalizability of these methods questionable. We propose a novel multi-task
framework for HAE that utilizes a Graph Convolutional Network backbone to embed
the interconnections between human joints in the features. In this framework,
we solve the Human Activity Segmentation (HAS) problem as an auxiliary task to
improve activity assessment. The HAS head is powered by an Encoder-Decoder
Temporal Convolutional Network to semantically segment long videos into
distinct activity classes, whereas, HAE uses a Long-Short-Term-Memory-based
architecture. We evaluate our method on the UW-IOM and TUM Kitchen datasets and
discuss the success and failure cases in these two datasets.
</p>
<a href="http://arxiv.org/abs/2008.03014" target="_blank">arXiv:2008.03014</a> [<a href="http://arxiv.org/pdf/2008.03014" target="_blank">pdf</a>]

<h2>Spatiotemporal Contrastive Video Representation Learning. (arXiv:2008.03800v3 [cs.CV] UPDATED)</h2>
<h3>Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, Yin Cui</h3>
<p>We present a self-supervised Contrastive Video Representation Learning (CVRL)
method to learn spatiotemporal visual representations from unlabeled videos.
Our representations are learned using a contrastive loss, where two augmented
clips from the same short video are pulled together in the embedding space,
while clips from different videos are pushed away. We study what makes for good
data augmentation for video self-supervised learning and find both spatial and
temporal information are crucial. We carefully design data augmentations
involving spatial and temporal cues. Concretely, we propose a temporally
consistent spatial augmentation method to impose strong spatial augmentations
on each frame of the video while maintaining the temporal consistency across
frames. We also propose a sampling-based temporal augmentation method to avoid
overly enforcing invariance on the clips that are distant in a video. On the
Kinetics-600 dataset, a linear classifier trained on the representations
learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50)
backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR
unsupervised pre-training by 18.8% using the same inflated R3D-50. The
performance of CVRL can be further improved to 72.6% with a larger R3D-50
(4$\times$ filters) backbone, significantly closing the gap between
unsupervised and supervised video representation learning.
</p>
<a href="http://arxiv.org/abs/2008.03800" target="_blank">arXiv:2008.03800</a> [<a href="http://arxiv.org/pdf/2008.03800" target="_blank">pdf</a>]

<h2>Deep learning for photoacoustic imaging: a survey. (arXiv:2008.04221v4 [cs.CV] UPDATED)</h2>
<h3>Changchun Yang, Hengrong Lan, Feng Gao, Fei Gao</h3>
<p>Machine learning has been developed dramatically and witnessed a lot of
applications in various fields over the past few years. This boom originated in
2009, when a new model emerged, that is, the deep artificial neural network,
which began to surpass other established mature models on some important
benchmarks. Later, it was widely used in academia and industry. Ranging from
image analysis to natural language processing, it fully exerted its magic and
now become the state-of-the-art machine learning models. Deep neural networks
have great potential in medical imaging technology, medical data analysis,
medical diagnosis and other healthcare issues, and is promoted in both
pre-clinical and even clinical stages. In this review, we performed an overview
of some new developments and challenges in the application of machine learning
to medical image analysis, with a special focus on deep learning in
photoacoustic imaging. The aim of this review is threefold: (i) introducing
deep learning with some important basics, (ii) reviewing recent works that
apply deep learning in the entire ecological chain of photoacoustic imaging,
from image reconstruction to disease diagnosis, (iii) providing some open
source materials and other resources for researchers interested in applying
deep learning to photoacoustic imaging.
</p>
<a href="http://arxiv.org/abs/2008.04221" target="_blank">arXiv:2008.04221</a> [<a href="http://arxiv.org/pdf/2008.04221" target="_blank">pdf</a>]

<h2>Deep Model-Based Reinforcement Learning for High-Dimensional Problems, a Survey. (arXiv:2008.05598v2 [cs.LG] UPDATED)</h2>
<h3>Aske Plaat, Walter Kosters, Mike Preuss</h3>
<p>Deep reinforcement learning has shown remarkable success in the past few
years. Highly complex sequential decision making problems have been solved in
tasks such as game playing and robotics. Unfortunately, the sample complexity
of most deep reinforcement learning methods is high, precluding their use in
some important applications. Model-based reinforcement learning creates an
explicit model of the environment dynamics to reduce the need for environment
samples. Current deep learning methods use high-capacity networks to solve
high-dimensional problems. Unfortunately, high-capacity models typically
require many samples, negating the potential benefit of lower sample complexity
in model-based methods. A challenge for deep model-based methods is therefore
to achieve high predictive power while maintaining low sample complexity. In
recent years, many model-based methods have been introduced to address this
challenge. In this paper, we survey the contemporary model-based landscape.
First we discuss definitions and relations to other fields. We propose a
taxonomy based on three approaches: using explicit planning on given
transitions, using explicit planning on learned transitions, and end-to-end
learning of both planning and transitions. We use these approaches to organize
a comprehensive overview of important recent developments such as latent
models. We describe methods and benchmarks, and we suggest directions for
future work for each of the approaches. Among promising research directions are
curriculum learning, uncertainty modeling, and use of latent models for
transfer learning.
</p>
<a href="http://arxiv.org/abs/2008.05598" target="_blank">arXiv:2008.05598</a> [<a href="http://arxiv.org/pdf/2008.05598" target="_blank">pdf</a>]

<h2>Minimal Adversarial Examples for Deep Learning on 3D Point Clouds. (arXiv:2008.12066v2 [cs.CV] UPDATED)</h2>
<h3>Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung</h3>
<p>With recent developments of convolutional neural networks, deep learning for
3D point clouds has shown significant progress in various 3D scene
understanding tasks, e.g., object recognition, object detection. In a
safety-critical environment, it is however not well understood how such deep
learning models are vulnerable to adversarial examples. In this work, we
explore adversarial attacks for point cloud-based neural networks. We propose a
general formulation for adversarial point cloud generation via $\ell_0$-norm
optimisation. Our method generates adversarial examples by attacking the
classification ability of the point cloud-based networks while considering the
perceptibility of the examples and ensuring the minimum level of point
manipulations. The proposed method is general and can be realised in different
attack strategies. Experimental results show that our method achieves the
state-of-the-art performance with higher than 89\% and 90\% of attack success
on synthetic and real-world data respectively, while manipulating only about
4\% of the total points.
</p>
<a href="http://arxiv.org/abs/2008.12066" target="_blank">arXiv:2008.12066</a> [<a href="http://arxiv.org/pdf/2008.12066" target="_blank">pdf</a>]

<h2>NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size. (arXiv:2009.00437v4 [cs.LG] UPDATED)</h2>
<h3>Xuanyi Dong, Lu Liu, Katarzyna Musial, Bogdan Gabrys</h3>
<p>Neural architecture search (NAS) has attracted a lot of attention and has
been illustrated to bring tangible benefits in a large number of applications
in the past few years. Architecture topology and architecture size have been
regarded as two of the most important aspects for the performance of deep
learning models and the community has spawned lots of searching algorithms for
both aspects of the neural architectures. However, the performance gain from
these searching algorithms is achieved under different search spaces and
training setups. This makes the overall performance of the algorithms to some
extent incomparable and the improvement from a sub-module of the searching
model unclear. In this paper, we propose NATS-Bench, a unified benchmark on
searching for both topology and size, for (almost) any up-to-date NAS
algorithm. NATS-Bench includes the search space of 15,625 neural cell
candidates for architecture topology and 32,768 for architecture size on three
datasets. We analyse the validity of our benchmark in terms of various criteria
and performance comparison of all candidates in the search space. We also show
the versatility of NATS-Bench by benchmarking 13 recent state-of-the-art NAS
algorithms on it. All logs and diagnostic information trained using the same
setup for each candidate are provided. This facilitates a much larger community
of researchers to focus on developing better NAS algorithms in a more
comparable and computationally cost friendly environment. All codes are
publicly available at: https://xuanyidong.com/assets/projects/NATS-Bench .
</p>
<a href="http://arxiv.org/abs/2009.00437" target="_blank">arXiv:2009.00437</a> [<a href="http://arxiv.org/pdf/2009.00437" target="_blank">pdf</a>]

<h2>On the implementation of a global optimization method for mixed-variable problems. (arXiv:2009.02183v2 [cs.LG] UPDATED)</h2>
<h3>Giacomo Nannicini</h3>
<p>We describe the optimization algorithm implemented in the open-source
derivative-free solver RBFOpt. The algorithm is based on the radial basis
function method of Gutmann and the metric stochastic response surface method of
Regis and Shoemaker. We propose several modifications aimed at generalizing and
improving these two algorithms: (i) the use of an extended space to represent
categorical variables in unary encoding; (ii) a refinement phase to locally
improve a candidate solution; (iii) interpolation models without the
unisolvence condition, to both help deal with categorical variables, and
initiate the optimization before a uniquely determined model is possible; (iv)
a master-worker framework to allow asynchronous objective function evaluations
in parallel. Numerical experiments show the effectiveness of these ideas.
</p>
<a href="http://arxiv.org/abs/2009.02183" target="_blank">arXiv:2009.02183</a> [<a href="http://arxiv.org/pdf/2009.02183" target="_blank">pdf</a>]

<h2>Addressing Cold Start in Recommender Systems with Hierarchical Graph Neural Networks. (arXiv:2009.03455v2 [cs.LG] UPDATED)</h2>
<h3>Ivan Maksimov, Rodrigo Rivera-Castro, Evgeny Burnaev</h3>
<p>Recommender systems have become an essential instrument in a wide range of
industries to personalize the user experience. A significant issue that has
captured both researchers' and industry experts' attention is the cold start
problem for new items. In this work, we present a graph neural network
recommender system using item hierarchy graphs and a bespoke architecture to
handle the cold start case for items. The experimental study on multiple
datasets and millions of users and interactions indicates that our method
achieves better forecasting quality than the state-of-the-art with a comparable
computational time.
</p>
<a href="http://arxiv.org/abs/2009.03455" target="_blank">arXiv:2009.03455</a> [<a href="http://arxiv.org/pdf/2009.03455" target="_blank">pdf</a>]

<h2>Highly Accurate CNN Inference Using Approximate Activation Functions over Homomorphic Encryption. (arXiv:2009.03727v2 [cs.LG] UPDATED)</h2>
<h3>Takumi Ishiyama, Takuya Suzuki, Hayato Yamana</h3>
<p>In the big data era, cloud-based machine learning as a service (MLaaS) has
attracted considerable attention. However, when handling sensitive data, such
as financial and medical data, a privacy issue emerges, because the cloud
server can access clients' raw data. A common method of handling sensitive data
in the cloud uses homomorphic encryption, which allows computation over
encrypted data without decryption. Previous research usually adopted a
low-degree polynomial mapping function, such as the square function, for data
classification. However, this technique results in low classification accuracy.
In this study, we seek to improve the classification accuracy for inference
processing in a convolutional neural network (CNN) while using homomorphic
encryption. We adopt an activation function that approximates Google's Swish
activation function while using a fourth-order polynomial. We also adopt batch
normalization to normalize the inputs for the Swish function to fit the input
range to minimize the error. We implemented CNN inference labeling over
homomorphic encryption using the Microsoft's Simple Encrypted Arithmetic
Library for the Cheon-Kim-Kim-Song (CKKS) scheme. The experimental evaluations
confirmed classification accuracies of 99.22% and 80.48% for MNIST and
CIFAR-10, respectively, which entails 0.04% and 4.11% improvements,
respectively, over previous methods.
</p>
<a href="http://arxiv.org/abs/2009.03727" target="_blank">arXiv:2009.03727</a> [<a href="http://arxiv.org/pdf/2009.03727" target="_blank">pdf</a>]

<h2>FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition. (arXiv:2009.07838v2 [cs.CV] UPDATED)</h2>
<h3>Tom&#xe1;&#x161; Sixta, Julio C. S. Jacques Junior, Pau Buch-Cardona, Neil M. Robertson, Eduard Vazquez, Sergio Escalera</h3>
<p>This work summarizes the 2020 ChaLearn Looking at People Fair Face
Recognition and Analysis Challenge and provides a description of the
top-winning solutions and analysis of the results. The aim of the challenge was
to evaluate accuracy and bias in gender and skin colour of submitted algorithms
on the task of 1:1 face verification in the presence of other confounding
attributes. Participants were evaluated using an in-the-wild dataset based on
reannotated IJB-C, further enriched by 12.5K new images and additional labels.
The dataset is not balanced, which simulates a real world scenario where
AI-based models supposed to present fair outcomes are trained and evaluated on
imbalanced data. The challenge attracted 151 participants, who made more than
1.8K submissions in total. The final phase of the challenge attracted 36 active
teams out of which 10 exceeded 0.999 AUC-ROC while achieving very low scores in
the proposed bias metrics. Common strategies by the participants were face
pre-processing, homogenization of data distributions, the use of bias aware
loss functions and ensemble models. The analysis of top-10 teams shows higher
false positive rates (and lower false negative rates) for females with dark
skin tone as well as the potential of eyeglasses and young age to increase the
false positive rates too.
</p>
<a href="http://arxiv.org/abs/2009.07838" target="_blank">arXiv:2009.07838</a> [<a href="http://arxiv.org/pdf/2009.07838" target="_blank">pdf</a>]

<h2>Some Remarks on Replicated Simulated Annealing. (arXiv:2009.14702v2 [cs.LG] UPDATED)</h2>
<h3>Vincent Gripon, Matthias L&#xf6;we, Franck Vermet</h3>
<p>Recently authors have introduced the idea of training discrete weights neural
networks using a mix between classical simulated annealing and a replica ansatz
known from the statistical physics literature. Among other points, they claim
their method is able to find robust configurations. In this paper, we analyze
this so-called "replicated simulated annealing" algorithm. In particular, we
explicit criteria to guarantee its convergence, and study when it successfully
samples from configurations. We also perform experiments using synthetic and
real data bases.
</p>
<a href="http://arxiv.org/abs/2009.14702" target="_blank">arXiv:2009.14702</a> [<a href="http://arxiv.org/pdf/2009.14702" target="_blank">pdf</a>]

<h2>Understanding Self-supervised Learning with Dual Deep Networks. (arXiv:2010.00578v5 [cs.LG] UPDATED)</h2>
<h3>Yuandong Tian, Lantao Yu, Xinlei Chen, Surya Ganguli</h3>
<p>We propose a novel theoretical framework to understand self-supervised
learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR,
BYOL). First, we prove that in each SGD update of SimCLR with various loss
functions (simple contrastive loss, soft Triplet loss and InfoNCE loss), the
weights at each layer are updated by a \emph{covariance operator} that
specifically amplifies initial random selectivities that vary across data
samples but survive averages over data augmentations. We show this leads to the
emergence of hierarchical features, if the input data are generated from a
hierarchical latent tree model. With the same framework, we also show
analytically that in BYOL, the combination of BatchNorm and a predictor network
creates an implicit contrastive term, acting as an approximate covariance
operator. Additionally, for linear architectures we derive exact solutions for
BYOL that provide conceptual insights into how BYOL can learn useful
non-collapsed representations without any contrastive terms that separate
negative pairs. Extensive ablation studies justify our theoretical findings.
</p>
<a href="http://arxiv.org/abs/2010.00578" target="_blank">arXiv:2010.00578</a> [<a href="http://arxiv.org/pdf/2010.00578" target="_blank">pdf</a>]

<h2>Invariant Representation Learning for Infant Pose Estimation with Small Data. (arXiv:2010.06100v3 [cs.CV] UPDATED)</h2>
<h3>Xiaofei Huang, Nihang Fu, Shuangjun Liu, Kathan Vyas, Amirreza Farnoosh, Sarah Ostadabbas</h3>
<p>With the increasing maturity of the human pose estimation domain, its
applications have become more and more broaden. Yet, the state-of-the-art pose
estimation models performance degrades significantly in the applications that
include novel subjects or poses, such as infants with their unique movements.
Infant motion analysis is a topic with critical importance in early
developmental studies. However, models trained on large-scale adult pose
datasets are barely successful in estimating infant poses due to the
significant differences in their body ratio and the versatility of poses they
can take compared to adults. Moreover, the privacy and security considerations
hinder the availability of adequate infant images required for training of a
robust pose inference model from scratch. Here, we present an invariant
representation learning strategy that allows us to augment the limited
available real infant pose data by incorporating the knowledge from the
adjacent domains of adult poses as well as synthetic infant models. We
introduce a multi-stage training strategy to gradually transfer these knowledge
into our fine-tuned domain-adapted infant pose (FiDIP) estimation model. In
developing FiDIP, we also built and publicly released a synthetic and real
infant pose (SyRIP) dataset with small yet diverse real infant images as well
as generated synthetic infant data. We demonstrated that our FiDIP model
outperforms state-of-the-art human pose estimation model for the infant pose
estimation, with the mean average precision (AP) as high as 90.1.
</p>
<a href="http://arxiv.org/abs/2010.06100" target="_blank">arXiv:2010.06100</a> [<a href="http://arxiv.org/pdf/2010.06100" target="_blank">pdf</a>]

<h2>Boosting One-Point Derivative-Free Online Optimization via Residual Feedback. (arXiv:2010.07378v2 [cs.LG] UPDATED)</h2>
<h3>Yan Zhang, Yi Zhou, Kaiyi Ji, Michael M. Zavlanos</h3>
<p>Zeroth-order optimization (ZO) typically relies on two-point feedback to
estimate the unknown gradient of the objective function. Nevertheless,
two-point feedback can not be used for online optimization of time-varying
objective functions, where only a single query of the function value is
possible at each time step. In this work, we propose a new one-point feedback
method for online optimization that estimates the objective function gradient
using the residual between two feedback points at consecutive time instants.
Moreover, we develop regret bounds for ZO with residual feedback for both
convex and nonconvex online optimization problems. Specifically, for both
deterministic and stochastic problems and for both Lipschitz and smooth
objective functions, we show that using residual feedback can produce gradient
estimates with much smaller variance compared to conventional one-point
feedback methods. As a result, our regret bounds are much tighter compared to
existing regret bounds for ZO with conventional one-point feedback, which
suggests that ZO with residual feedback can better track the optimizer of
online optimization problems. Additionally, our regret bounds rely on weaker
assumptions than those used in conventional one-point feedback methods.
Numerical experiments show that ZO with residual feedback significantly
outperforms existing one-point feedback methods also in practice.
</p>
<a href="http://arxiv.org/abs/2010.07378" target="_blank">arXiv:2010.07378</a> [<a href="http://arxiv.org/pdf/2010.07378" target="_blank">pdf</a>]

<h2>A Survey on Churn Analysis. (arXiv:2010.13119v3 [cs.LG] UPDATED)</h2>
<h3>Jaehuyn Ahn</h3>
<p>In this paper, I present churn prediction techniques that have been released
so far. Churn prediction is used in the fields of Internet services, games,
insurance, and management. However, since it has been used intensively to
increase the predictability of various industry/academic fields, there is a big
difference in its definition and utilization. In this paper, I collected the
definitions of churn used in the fields of business administration, marketing,
IT, telecommunications, newspapers, insurance and psychology, and described
their differences. Based on this, I classified and explained churn loss,
feature engineering, and prediction models. Our study can be used to select the
definition of churn and its associated models suitable for the service field
that researchers are most interested in by integrating fragmented churn studies
in industry/academic fields.
</p>
<a href="http://arxiv.org/abs/2010.13119" target="_blank">arXiv:2010.13119</a> [<a href="http://arxiv.org/pdf/2010.13119" target="_blank">pdf</a>]

<h2>On Graph Neural Networks versus Graph-Augmented MLPs. (arXiv:2010.15116v2 [cs.LG] UPDATED)</h2>
<h3>Lei Chen, Zhengdao Chen, Joan Bruna</h3>
<p>From the perspective of expressive power, this work compares multi-layer
Graph Neural Networks (GNNs) with a simplified alternative that we call
Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first augments node
features with certain multi-hop operators on the graph and then applies an MLP
in a node-wise fashion. From the perspective of graph isomorphism testing, we
show both theoretically and numerically that GA-MLPs with suitable operators
can distinguish almost all non-isomorphic graphs, just like the
Weifeiler-Lehman (WL) test. However, by viewing them as node-level functions
and examining the equivalence classes they induce on rooted graphs, we prove a
separation in expressive power between GA-MLPs and GNNs that grows
exponentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count
the number of attributed walks. We also demonstrate via community detection
experiments that GA-MLPs can be limited by their choice of operator family, as
compared to GNNs with higher flexibility in learning.
</p>
<a href="http://arxiv.org/abs/2010.15116" target="_blank">arXiv:2010.15116</a> [<a href="http://arxiv.org/pdf/2010.15116" target="_blank">pdf</a>]

<h2>Explanation generation for anomaly detection models applied to the fuel consumption of a vehicle fleet. (arXiv:2010.16051v3 [cs.LG] UPDATED)</h2>
<h3>Alberto Barbado, &#xd3;scar Corcho</h3>
<p>In this paper we show a complete process for unsupervised anomaly detection
for the fuel consumption of a vehicle fleet, that is able to explain which
variables affect the consumption in terms of feature relevance. We combine
anomaly detection with a surrogate model that is able to provide that feature
relevance. For these surrogate models, we evaluate both whitebox ones from the
literature, as well as novel variations over them, and blackbox models combined
with local posthoc feature relevance techniques.

The evaluation is done using real IoT data, and is measured both in terms of
model performance, as well as using Explainable AI metrics that compare the
explanations generated in terms representativeness, fidelity, stability and
contrastiveness. This provides a complete evaluation both in terms of
predictive power and XAI.

The explanations generate counterfactual recommendations that show what could
have been done to reduce the fuel consumption of a vehicle and turn it into an
inlier. The procedure is combined with domain knowledge expressed in business
rules, and is able to adequate the type of explanations depending on the target
user profile.
</p>
<a href="http://arxiv.org/abs/2010.16051" target="_blank">arXiv:2010.16051</a> [<a href="http://arxiv.org/pdf/2010.16051" target="_blank">pdf</a>]

<h2>LIFI: Towards Linguistically Informed Frame Interpolation. (arXiv:2010.16078v5 [cs.CV] UPDATED)</h2>
<h3>Aradhya Neeraj Mathur, Devansh Batra, Yaman Kumar, Rajiv Ratn Shah, Roger Zimmermann</h3>
<p>In this work, we explore a new problem of frame interpolation for speech
videos. Such content today forms the major form of online communication. We try
to solve this problem by using several deep learning video generation
algorithms to generate the missing frames. We also provide examples where
computer vision models despite showing high performance on conventional
non-linguistic metrics fail to accurately produce faithful interpolation of
speech. With this motivation, we provide a new set of linguistically-informed
metrics specifically targeted to the problem of speech videos interpolation. We
also release several datasets to test computer vision video generation models
of their speech understanding.
</p>
<a href="http://arxiv.org/abs/2010.16078" target="_blank">arXiv:2010.16078</a> [<a href="http://arxiv.org/pdf/2010.16078" target="_blank">pdf</a>]

<h2>DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion. (arXiv:2011.03984v2 [cs.LG] UPDATED)</h2>
<h3>Zhen Han, Yunpu Ma, Peng Chen, Volker Tresp</h3>
<p>There has recently been increasing interest in learning representations of
temporal knowledge graphs (KGs), which record the dynamic relationships between
entities over time. Temporal KGs often exhibit multiple simultaneous
non-Euclidean structures, such as hierarchical and cyclic structures. However,
existing embedding approaches for temporal KGs typically learn entity
representations and their dynamic evolution in the Euclidean space, which might
not capture such intrinsic structures very well. To this end, we propose Dy-
ERNIE, a non-Euclidean embedding approach that learns evolving entity
representations in a product of Riemannian manifolds, where the composed spaces
are estimated from the sectional curvatures of underlying data. Product
manifolds enable our approach to better reflect a wide variety of geometric
structures on temporal KGs. Besides, to capture the evolutionary dynamics of
temporal KGs, we let the entity representations evolve according to a velocity
vector defined in the tangent space at each timestamp. We analyze in detail the
contribution of geometric spaces to representation learning of temporal KGs and
evaluate our model on temporal knowledge graph completion tasks. Extensive
experiments on three real-world datasets demonstrate significantly improved
performance, indicating that the dynamics of multi-relational graph data can be
more properly modeled by the evolution of embeddings on Riemannian manifolds.
</p>
<a href="http://arxiv.org/abs/2011.03984" target="_blank">arXiv:2011.03984</a> [<a href="http://arxiv.org/pdf/2011.03984" target="_blank">pdf</a>]

<h2>Real-time object detection method based on improved YOLOv4-tiny. (arXiv:2011.04244v2 [cs.CV] UPDATED)</h2>
<h3>Zicong Jiang, Liquan Zhao, Shuaiyang Li, Yanfei Jia</h3>
<p>The "You only look once v4"(YOLOv4) is one type of object detection methods
in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network
structure and reduce parameters, which makes it be suitable for developing on
the mobile and embedded devices. To improve the real-time of object detection,
a fast object detection method is proposed based on YOLOv4-tiny. It firstly
uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules
in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs
an auxiliary residual network block to extract more feature information of
object to reduce detection error. In the design of auxiliary network, two
consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract
global features, and channel attention and spatial attention are also used to
extract more effective information. In the end, it merges the auxiliary network
and backbone network to construct the whole network structure of improved
YOLOv4-tiny. Simulation results show that the proposed method has faster object
detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of
average precision as the YOLOv4-tiny. It is more suitable for real-time object
detection.
</p>
<a href="http://arxiv.org/abs/2011.04244" target="_blank">arXiv:2011.04244</a> [<a href="http://arxiv.org/pdf/2011.04244" target="_blank">pdf</a>]

<h2>GANMEX: One-vs-One Attributions using GAN-based Model Explainability. (arXiv:2011.06015v2 [cs.LG] UPDATED)</h2>
<h3>Sheng-Min Shih, Pin-Ju Tien, Zohar Karnin</h3>
<p>Attribution methods have been shown as promising approaches for identifying
key features that led to learned model predictions. While most existing
attribution methods rely on a baseline input for performing feature
perturbations, limited research has been conducted to address the baseline
selection issues. Poor choices of baselines limit the ability of one-vs-one
explanations for multi-class classifiers, which means the attribution methods
were not able to explain why an input belongs to its original class but not the
other specified target class. Achieving one-vs-one explanation is crucial when
certain classes are more similar than others, e.g. two bird types among
multiple animals, by focusing on key differentiating features rather than
shared features across classes. In this paper, we present GANMEX, a novel
approach applying Generative Adversarial Networks (GAN) by incorporating the
to-be-explained classifier as part of the adversarial networks. Our approach
effectively selects the baseline as the closest realistic sample belong to the
target class, which allows attribution methods to provide true one-vs-one
explanations. We showed that GANMEX baselines improved the saliency maps and
led to stronger performance on perturbation-based evaluation metrics over the
existing baselines. Existing attribution results are known for being
insensitive to model randomization, and we demonstrated that GANMEX baselines
led to better outcome under the cascading randomization of the model.
</p>
<a href="http://arxiv.org/abs/2011.06015" target="_blank">arXiv:2011.06015</a> [<a href="http://arxiv.org/pdf/2011.06015" target="_blank">pdf</a>]

<h2>Fairness and Robustness in Invariant Learning: A Case Study in Toxicity Classification. (arXiv:2011.06485v2 [cs.LG] UPDATED)</h2>
<h3>Robert Adragna, Elliot Creager, David Madras, Richard Zemel</h3>
<p>Robustness is of central importance in machine learning and has given rise to
the fields of domain generalization and invariant learning, which are concerned
with improving performance on a test distribution distinct from but related to
the training distribution. In light of recent work suggesting an intimate
connection between fairness and robustness, we investigate whether algorithms
from robust ML can be used to improve the fairness of classifiers that are
trained on biased data and tested on unbiased data. We apply Invariant Risk
Minimization (IRM), a domain generalization algorithm that employs a causal
discovery inspired method to find robust predictors, to the task of fairly
predicting the toxicity of internet comments. We show that IRM achieves better
out-of-distribution accuracy and fairness than Empirical Risk Minimization
(ERM) methods, and analyze both the difficulties that arise when applying IRM
in practice and the conditions under which IRM will likely be effective in this
scenario. We hope that this work will inspire further studies of how robust
machine learning methods relate to algorithmic fairness.
</p>
<a href="http://arxiv.org/abs/2011.06485" target="_blank">arXiv:2011.06485</a> [<a href="http://arxiv.org/pdf/2011.06485" target="_blank">pdf</a>]

<h2>Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with Airlight and Scattering Coefficient Estimation. (arXiv:2011.09114v2 [cs.CV] UPDATED)</h2>
<h3>Yuki Fujimura, Motoharu Sonogashira, Masaaki Iiyama</h3>
<p>We propose a learning-based multi-view stereo (MVS) method in scattering
media, such as fog or smoke, with a novel cost volume, called the dehazing cost
volume. Images captured in scattering media are degraded due to light
scattering and attenuation caused by suspended particles. This degradation
depends on scene depth; thus, it is difficult for traditional MVS methods to
evaluate photometric consistency because the depth is unknown before
three-dimensional (3D) reconstruction. The dehazing cost volume can solve this
chicken-and-egg problem of depth estimation and image restoration by computing
the scattering effect using swept planes in the cost volume. We also propose a
method of estimating scattering parameters, such as airlight, and a scattering
coefficient, which are required for our dehazing cost volume. The output depth
of a network with our dehazing cost volume can be regarded as a function of
these parameters; thus, they are geometrically optimized with a sparse 3D point
cloud obtained at a structure-from-motion step. Experimental results on
synthesized hazy images indicate the effectiveness of our dehazing cost volume
against the ordinary cost volume regarding scattering media. We also
demonstrated the applicability of our dehazing cost volume to real foggy
scenes.
</p>
<a href="http://arxiv.org/abs/2011.09114" target="_blank">arXiv:2011.09114</a> [<a href="http://arxiv.org/pdf/2011.09114" target="_blank">pdf</a>]

<h2>EWareNet: Emotion Aware Human Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation. (arXiv:2011.09438v3 [cs.RO] UPDATED)</h2>
<h3>Venkatraman Narayanan, Bala Murali Manoghar, Rama Prashanth RV, Aniket Bera</h3>
<p>We present EWareNet, a novel intent-aware social robot navigation algorithm
among pedestrians. Our approach predicts the trajectory-based pedestrian intent
from historical gaits, which is then used for intent-guided navigation taking
into account social and proxemic constraints. To predict pedestrian intent, we
propose a transformer-based model that works on a commodity RGB-D camera
mounted onto a moving robot. Our intent prediction routine is integrated into a
mapless navigation scheme and makes no assumptions about the environment of
pedestrian motion. Our navigation scheme consists of a novel obstacle profile
representation methodology that is dynamically adjusted based on the pedestrian
pose, intent, and emotion. The navigation scheme is based on a reinforcement
learning algorithm that takes into consideration human intent and robot's
impact on human intent, in addition to the environmental configuration. We
outperform current state-of-art algorithms for intent prediction from 3D gaits.
</p>
<a href="http://arxiv.org/abs/2011.09438" target="_blank">arXiv:2011.09438</a> [<a href="http://arxiv.org/pdf/2011.09438" target="_blank">pdf</a>]

<h2>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v3 [cs.LG] UPDATED)</h2>
<h3>Hao-Zhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, Wei Chen</h3>
<p>Conventional unsupervised multi-source domain adaptation (UMDA) methods
assume all source domains can be accessed directly. This neglects the
privacy-preserving policy, that is, all the data and computations must be kept
decentralized. There exists three problems in this scenario: (1) Minimizing the
domain distance requires the pairwise calculation of the data from source and
target domains, which is not accessible. (2) The communication cost and privacy
security limit the application of UMDA methods (e.g., the domain adversarial
training). (3) Since users have no authority to check the data quality, the
irrelevant or malicious source domains are more likely to appear, which causes
negative transfer. In this study, we propose a privacy-preserving UMDA paradigm
named Knowledge Distillation based Decentralized Domain Adaptation (KD3A),
which performs domain adaptation through the knowledge distillation on models
from different source domains. KD3A solves the above problems with three
components: (1) A multi-source knowledge distillation method named Knowledge
Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting
strategy named Consensus Focus to identify both the malicious and irrelevant
domains. (3) A decentralized optimization strategy for domain distance named
BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is
robust to the negative transfer and brings a 100x reduction of communication
cost compared with other decentralized UMDA methods. Moreover, our KD3A
significantly outperforms state-of-the-art UMDA approaches.
</p>
<a href="http://arxiv.org/abs/2011.09757" target="_blank">arXiv:2011.09757</a> [<a href="http://arxiv.org/pdf/2011.09757" target="_blank">pdf</a>]

<h2>Accelerating Probabilistic Volumetric Mapping using Ray-Tracing Graphics Hardware. (arXiv:2011.10348v2 [cs.RO] UPDATED)</h2>
<h3>Heajung Min, Kyung Min Han, Young J. Kim</h3>
<p>Probabilistic volumetric mapping (PVM) represents a 3D environmental map for
an autonomous robotic navigational task. A popular implementation such as
Octomap is widely used in the robotics community for such a purpose. The
Octomap relies on octree to represent a PVM and its main bottleneck lies in
massive ray-shooting to determine the occupancy of the underlying volumetric
voxel grids. In this paper, we propose GPU-based ray shooting to drastically
improve the ray shooting performance in Octomap. Our main idea is based on the
use of recent ray-tracing RTX GPU, mainly designed for real-time
photo-realistic computer graphics and the accompanying graphics API, known as
DXR. Our ray-shooting first maps leaf-level voxels in the given octree to a set
of axis-aligned bounding boxes (AABBs) and employ massively parallel ray
shooting on them using GPUs to find free and occupied voxels. These are fed
back into CPU to update the voxel occupancy and restructure the octree. In our
experiments, we have observed more than three-orders-of-magnitude performance
improvement in terms of ray shooting using ray-tracing RTX GPU over a
state-of-the-art Octomap CPU implementation, where the benchmarking
environments consist of more than 77K points and 25K~34K voxel grids.
</p>
<a href="http://arxiv.org/abs/2011.10348" target="_blank">arXiv:2011.10348</a> [<a href="http://arxiv.org/pdf/2011.10348" target="_blank">pdf</a>]

<h2>SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations. (arXiv:2011.10684v2 [cs.LG] UPDATED)</h2>
<h3>Hao-Zhe Feng, Kezhi Kong, Minghao Chen, Tianye Zhang, Minfeng Zhu, Wei Chen</h3>
<p>Semi-supervised variational autoencoders (VAEs) have obtained strong results,
but have also encountered the challenge that good ELBO values do not always
imply accurate inference results. In this paper, we investigate and propose two
causes of this problem: (1) The ELBO objective cannot utilize the label
information directly. (2) A bottleneck value exists and continuing to optimize
ELBO after this value will not improve inference accuracy. On the basis of the
experiment results, we propose SHOT-VAE to address these problems without
introducing additional prior knowledge. The SHOT-VAE offers two contributions:
(1) A new ELBO approximation named smooth-ELBO that integrates the label
predictive loss into ELBO. (2) An approximation based on optimal interpolation
that breaks the ELBO value bottleneck by reducing the margin between ELBO and
the data likelihood. The SHOT-VAE achieves good performance with a 25.30% error
rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on
CIFAR-10 with 4k labels.
</p>
<a href="http://arxiv.org/abs/2011.10684" target="_blank">arXiv:2011.10684</a> [<a href="http://arxiv.org/pdf/2011.10684" target="_blank">pdf</a>]

<h2>The Emerging Trends of Multi-Label Learning. (arXiv:2011.11197v2 [cs.LG] UPDATED)</h2>
<h3>Weiwei Liu, Xiaobo Shen, Haobo Wang, Ivor W. Tsang</h3>
<p>Exabytes of data are generated daily by humans, leading to the growing need
for new efforts in dealing with the grand challenges for multi-label learning
brought by big data. For example, extreme multi-label classification is an
active and rapidly growing research area that deals with classification tasks
with an extremely large number of classes or labels; utilizing massive data
with limited supervision to build a multi-label classification model becomes
valuable for practical applications, etc. Besides these, there are tremendous
efforts on how to harvest the strong learning capability of deep learning to
better capture the label dependencies in multi-label learning, which is the key
for deep learning to address real-world classification tasks. However, it is
noted that there has been a lack of systemic studies that focus explicitly on
analyzing the emerging trends and new challenges of multi-label learning in the
era of big data. It is imperative to call for a comprehensive survey to fulfill
this mission and delineate future research directions and new applications.
</p>
<a href="http://arxiv.org/abs/2011.11197" target="_blank">arXiv:2011.11197</a> [<a href="http://arxiv.org/pdf/2011.11197" target="_blank">pdf</a>]

<h2>Stochastic Motion Planning under Partial Observability for Mobile Robots with Continuous Range Measurements. (arXiv:2011.11836v2 [cs.RO] UPDATED)</h2>
<h3>Ke Sun, Brent Schlotfeldt, George Pappas, Vijay Kumar</h3>
<p>In this paper, we address the problem of stochastic motion planning under
partial observability, more specifically, how to navigate a mobile robot
equipped with continuous range sensors such as LIDAR. In contrast to many
existing robotic motion planning methods, we explicitly consider the
uncertainty of the robot state by modeling the system as a POMDP. Recent work
on general purpose POMDP solvers is typically limited to discrete observation
spaces, and does not readily apply to the proposed problem due to the
continuous measurements from LIDAR. In this work, we build upon an existing
Monte Carlo Tree Search method, POMCP, and propose a new algorithm POMCP++. Our
algorithm can handle continuous observation spaces with a novel measurement
selection strategy. The POMCP++ algorithm overcomes over-optimism in the value
estimation of a rollout policy by removing the implicit perfect state
assumption at the rollout phase. We validate POMCP++ in theory by proving it is
a Monte Carlo Tree Search algorithm. Through comparisons with other methods
that can also be applied to the proposed problem, we show that POMCP++ yields
significantly higher success rate and total reward.
</p>
<a href="http://arxiv.org/abs/2011.11836" target="_blank">arXiv:2011.11836</a> [<a href="http://arxiv.org/pdf/2011.11836" target="_blank">pdf</a>]

<h2>GMOT-40: A Benchmark for Generic Multiple Object Tracking. (arXiv:2011.11858v2 [cs.CV] UPDATED)</h2>
<h3>Hexin Bai, Wensheng Cheng, Peng Chu, Juehuan Liu, Kai Zhang, Haibin Ling</h3>
<p>Multiple Object Tracking (MOT) has witnessed remarkable advances in recent
years. However, existing studies dominantly request prior knowledge of the
tracking target, and hence may not generalize well to unseen categories. In
contrast, Generic Multiple Object Tracking (GMOT), which requires little prior
information about the target, is largely under-explored. In this paper, we make
contributions to boost the study of GMOT in three aspects. First, we construct
the first public GMOT dataset, dubbed GMOT-40, which contains 40 carefully
annotated sequences evenly distributed among 10 object categories. In addition,
two tracking protocols are adopted to evaluate different characteristics of
tracking algorithms. Second, by noting the lack of devoted tracking algorithms,
we have designed a series of baseline GMOT algorithms. Third, we perform a
thorough evaluation on GMOT-40, involving popular MOT algorithms (with
necessary modifications) and the proposed baselines. We will release the
GMOT-40 benchmark, the evaluation results, as well as the baseline algorithm to
the public upon the publication of the paper.
</p>
<a href="http://arxiv.org/abs/2011.11858" target="_blank">arXiv:2011.11858</a> [<a href="http://arxiv.org/pdf/2011.11858" target="_blank">pdf</a>]

<h2>TinaFace: Strong but Simple Baseline for Face Detection. (arXiv:2011.13183v2 [cs.CV] UPDATED)</h2>
<h3>Yanjia Zhu, Hongxiang Cai, Shuhan Zhang, Chenhao Wang, Yichao Xiong</h3>
<p>Face detection has received intensive attention in recent years. Many works
present lots of special methods for face detection from different perspectives
like model architecture, data augmentation, label assignment and etc., which
make the overall algorithm and system become more and more complex. In this
paper, we point out that \textbf{there is no gap between face detection and
generic object detection}. Then we provide a strong but simple baseline method
to deal with face detection named TinaFace. We use ResNet-50 \cite{he2016deep}
as backbone, and all modules and techniques in TinaFace are constructed on
existing modules, easily implemented and based on generic object detection. On
the hard test set of the most popular and challenging face detection benchmark
WIDER FACE \cite{yang2016wider}, with single-model and single-scale, our
TinaFace achieves 92.1\% average precision (AP), which exceeds most of the
recent face detectors with larger backbone. And after using test time
augmentation (TTA), our TinaFace outperforms the current state-of-the-art
method and achieves 92.4\% AP. The code will be available at
\url{https://github.com/Media-Smart/vedadet}.
</p>
<a href="http://arxiv.org/abs/2011.13183" target="_blank">arXiv:2011.13183</a> [<a href="http://arxiv.org/pdf/2011.13183" target="_blank">pdf</a>]

<h2>Analyzing Unaligned Multimodal Sequence via Graph Convolution and Graph Pooling Fusion. (arXiv:2011.13572v2 [cs.AI] UPDATED)</h2>
<h3>Sijie Mai, Songlong Xing, Jiaxuan He, Ying Zeng, Haifeng Hu</h3>
<p>In this paper, we study the task of multimodal sequence analysis which aims
to draw inferences from visual, language and acoustic sequences. A majority of
existing works generally focus on aligned fusion, mostly at word level, of the
three modalities to accomplish this task, which is impractical in real-world
scenarios. To overcome this issue, we seek to address the task of multimodal
sequence analysis on unaligned modality sequences which is still relatively
underexplored and also more challenging. Recurrent neural network (RNN) and its
variants are widely used in multimodal sequence analysis, but they are
susceptible to the issues of gradient vanishing/explosion and high time
complexity due to its recurrent nature. Therefore, we propose a novel model,
termed Multimodal Graph, to investigate the effectiveness of graph neural
networks (GNN) on modeling multimodal sequential data. The graph-based
structure enables parallel computation in time dimension and can learn longer
temporal dependency in long unaligned sequences. Specifically, our Multimodal
Graph is hierarchically structured to cater to two stages, i.e., intra- and
inter-modal dynamics learning. For the first stage, a graph convolutional
network is employed for each modality to learn intra-modal dynamics. In the
second stage, given that the multimodal sequences are unaligned, the commonly
considered word-level fusion does not pertain. To this end, we devise a graph
pooling fusion network to automatically learn the associations between various
nodes from different modalities. Additionally, we define multiple ways to
construct the adjacency matrix for sequential data. Experimental results
suggest that our graph-based model reaches state-of-the-art performance on two
benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.13572" target="_blank">arXiv:2011.13572</a> [<a href="http://arxiv.org/pdf/2011.13572" target="_blank">pdf</a>]

<h2>Time-series Change Point Detection with Self-Supervised Contrastive Predictive Coding. (arXiv:2011.14097v2 [cs.LG] UPDATED)</h2>
<h3>Shohreh Deldari, Daniel V. Smith, Hao Xue, Flora D. Salim</h3>
<p>Change Point Detection techniques aim to capture changes in trends and
sequences in time-series data to describe the underlying behaviour of the
system. Detecting changes and anomalies in the web services, the trend of
applications usage can provide valuable insight towards the system, however,
many existing approaches are done in a supervised manner, requiring
well-labelled data. As the amount of data produced and captured by sensors are
growing rapidly, it is getting harder and even impossible to annotate the data.
Therefore, coming up with a self-supervised solution is a necessity these days.
In this work, we propose TSCP2 a novel self-supervised technique for temporal
change point detection, based on representation learning with Temporal
Convolutional Network (TCN). To the best of our knowledge, our proposed method
is the first method which employs Contrastive Learning for prediction with the
aim change point detection. Through extensive evaluations, we demonstrate that
our method outperforms multiple state-of-the-art change point detection and
anomaly detection baselines, including those adopting either unsupervised or
semi-supervised approach. TSCP2 is shown to improve both non-Deep learning- and
Deep learning-based methods by 0.28 and 0.12 in terms of average F1-score
across three datasets.
</p>
<a href="http://arxiv.org/abs/2011.14097" target="_blank">arXiv:2011.14097</a> [<a href="http://arxiv.org/pdf/2011.14097" target="_blank">pdf</a>]

<h2>Improving Neural Network with Uniform Sparse Connectivity. (arXiv:2011.14420v2 [cs.LG] UPDATED)</h2>
<h3>Weijun Luo</h3>
<p>Neural network forms the foundation of deep learning and numerous AI
applications. Classical neural networks are fully connected, expensive to train
and prone to overfitting. Sparse networks tend to have convoluted structure
search, suboptimal performance and limited usage. We proposed the novel uniform
sparse network (USN) with even and sparse connectivity within each layer. USN
has one striking property that its performance is independent of the
substantial topology variation and enormous model space, thus offers a
search-free solution to all above mentioned issues of neural networks. USN
consistently and substantially outperforms the state-of-the-art sparse network
models in prediction accuracy, speed and robustness. It even achieves higher
prediction accuracy than the fully connected network with only 0.55% parameters
and 1/4 computing time and resources. Importantly, USN is conceptually simple
as a natural generalization of fully connected network with multiple
improvements in accuracy, robustness and scalability. USN can replace the
latter in a range of applications, data types and deep learning architectures.
We have made USN open source at https://github.com/datapplab/sparsenet.
</p>
<a href="http://arxiv.org/abs/2011.14420" target="_blank">arXiv:2011.14420</a> [<a href="http://arxiv.org/pdf/2011.14420" target="_blank">pdf</a>]

<h2>Scaling down Deep Learning. (arXiv:2011.14439v2 [cs.LG] UPDATED)</h2>
<h3>Sam Greydanus</h3>
<p>Though deep learning models have taken on commercial and political relevance,
many aspects of their training and operation remain poorly understood. This has
sparked interest in "science of deep learning" projects, many of which are run
at scale and require enormous amounts of time, money, and electricity. But how
much of this research really needs to occur at scale? In this paper, we
introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to
classic deep learning benchmarks. The training examples are 20 times smaller
than MNIST examples yet they differentiate more clearly between linear,
nonlinear, and convolutional models which attain 32, 68, and 94% accuracy
respectively (these models obtain 94, 99+, and 99+% on MNIST). Then we present
example use cases which include measuring the spatial inductive biases of
lottery tickets, observing deep double descent, and metalearning an activation
function.
</p>
<a href="http://arxiv.org/abs/2011.14439" target="_blank">arXiv:2011.14439</a> [<a href="http://arxiv.org/pdf/2011.14439" target="_blank">pdf</a>]

<h2>Gradient Sparsification Can Improve Performance of Differentially-Private Convex Machine Learning. (arXiv:2011.14572v2 [cs.LG] UPDATED)</h2>
<h3>Farhad Farokhi</h3>
<p>We use gradient sparsification to reduce the adverse effect of differential
privacy noise on performance of private machine learning models. To this aim,
we employ compressed sensing and additive Laplace noise to evaluate
differentially-private gradients. Noisy privacy-preserving gradients are used
to perform stochastic gradient descent for training machine learning models.
Sparsification, achieved by setting the smallest gradient entries to zero, can
reduce the convergence speed of the training algorithm. However, by
sparsification and compressed sensing, the dimension of communicated gradient
and the magnitude of additive noise can be reduced. The interplay between these
effects determines whether gradient sparsification improves the performance of
differentially-private machine learning models. We investigate this
analytically in the paper. We prove that, for small privacy budgets,
compression can improve performance of privacy-preserving machine learning
models. However, for large privacy budgets, compression does not necessarily
improve the performance. Intuitively, this is because the effect of
privacy-preserving noise is minimal in large privacy budget regime and thus
improvements from gradient sparsification cannot compensate for its slower
convergence.
</p>
<a href="http://arxiv.org/abs/2011.14572" target="_blank">arXiv:2011.14572</a> [<a href="http://arxiv.org/pdf/2011.14572" target="_blank">pdf</a>]

<h2>Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation. (arXiv:2011.14589v3 [cs.CV] UPDATED)</h2>
<h3>Tianze Gao, Huihui Pan, Huijun Gao</h3>
<p>Monocular 3D object detection is a promising research topic for the
intelligent perception systems of autonomous driving. In this work, a
single-stage keypoint-based network, named as FADNet, is presented to address
the task of monocular 3D object detection. In contrast to previous
keypoint-based methods which adopt identical layouts for output branches, we
propose to divide the output modalities into different groups according to the
estimating difficulty, whereby different groups are treated differently by
sequential feature association. Another contribution of this work is the
strategy of depth hint augmentation. To provide characterized depth patterns as
hints for depth estimation, a dedicated depth hint module is designed to
generate row-wise features named as depth hints, which are explicitly
supervised in a bin-wise manner. In the training stage, the regression outputs
are uniformly encoded to enable loss disentanglement. The 2D loss term is
further adapted to be depth-aware for improving the detection accuracy of small
objects. The contributions of this work are validated by conducting experiments
and ablation study on the KITTI benchmark. Without utilizing depth priors, post
optimization, or other refinement modules, our network performs competitively
against state-of-the-art methods while maintaining a decent running speed.
</p>
<a href="http://arxiv.org/abs/2011.14589" target="_blank">arXiv:2011.14589</a> [<a href="http://arxiv.org/pdf/2011.14589" target="_blank">pdf</a>]

<h2>Combinatorial Learning of Graph Edit Distance via Dynamic Embedding. (arXiv:2011.15039v2 [cs.LG] UPDATED)</h2>
<h3>Runzhong Wang, Tianqi Zhang, Tianshu Yu, Junchi Yan, Xiaokang Yang</h3>
<p>Graph Edit Distance (GED) is a popular similarity measurement for pairwise
graphs and it also refers to the recovery of the edit path from the source
graph to the target graph. Traditional A* algorithm suffers scalability issues
due to its exhaustive nature, whose search heuristics heavily rely on human
prior knowledge. This paper presents a hybrid approach by combing the
interpretability of traditional search-based techniques for producing the edit
path, as well as the efficiency and adaptivity of deep embedding models to
achieve a cost-effective GED solver. Inspired by dynamic programming,
node-level embedding is designated in a dynamic reuse fashion and suboptimal
branches are encouraged to be pruned. To this end, our method can be readily
integrated into A* procedure in a dynamic fashion, as well as significantly
reduce the computational burden with a learned heuristic. Experimental results
on different graph datasets show that our approach can remarkably ease the
search process of A* without sacrificing much accuracy. To our best knowledge,
this work is also the first deep learning-based GED method for recovering the
edit path.
</p>
<a href="http://arxiv.org/abs/2011.15039" target="_blank">arXiv:2011.15039</a> [<a href="http://arxiv.org/pdf/2011.15039" target="_blank">pdf</a>]

<h2>Solvable Model for Inheriting the Regularization through Knowledge Distillation. (arXiv:2012.00194v2 [cs.LG] UPDATED)</h2>
<h3>Luca Saglietti, Lenka Zdeborov&#xe1;</h3>
<p>In recent years the empirical success of transfer learning with neural
networks has stimulated an increasing interest in obtaining a theoretical
understanding of its core properties. Knowledge distillation where a smaller
neural network is trained using the outputs of a larger neural network is a
particularly interesting case of transfer learning. In the present work, we
introduce a statistical physics framework that allows an analytic
characterization of the properties of knowledge distillation (KD) in shallow
neural networks. Focusing the analysis on a solvable model that exhibits a
non-trivial generalization gap, we investigate the effectiveness of KD. We are
able to show that, through KD, the regularization properties of the larger
teacher model can be inherited by the smaller student and that the yielded
generalization performance is closely linked to and limited by the optimality
of the teacher. Finally, we analyze the double descent phenomenology that can
arise in the considered KD setting.
</p>
<a href="http://arxiv.org/abs/2012.00194" target="_blank">arXiv:2012.00194</a> [<a href="http://arxiv.org/pdf/2012.00194" target="_blank">pdf</a>]

<h2>FairFaceGAN: Fairness-aware Facial Image-to-Image Translation. (arXiv:2012.00282v2 [cs.CV] UPDATED)</h2>
<h3>Sunhee Hwang, Sungho Park, Dohyung Kim, Mirae Do, Hyeran Byun</h3>
<p>In this paper, we introduce FairFaceGAN, a fairness-aware facial
Image-to-Image translation model, mitigating the problem of unwanted
translation in protected attributes (e.g., gender, age, race) during facial
attributes editing. Unlike existing models, FairFaceGAN learns fair
representations with two separate latents - one related to the target
attributes to translate, and the other unrelated to them. This strategy enables
FairFaceGAN to separate the information about protected attributes and that of
target attributes. It also prevents unwanted translation in protected
attributes while target attributes editing. To evaluate the degree of fairness,
we perform two types of experiments on CelebA dataset. First, we compare the
fairness-aware classification performances when augmenting data by existing
image translation methods and FairFaceGAN respectively. Moreover, we propose a
new fairness metric, namely Frechet Protected Attribute Distance (FPAD), which
measures how well protected attributes are preserved. Experimental results
demonstrate that FairFaceGAN shows consistent improvements in terms of fairness
over the existing image translation models. Further, we also evaluate image
translation performances, where FairFaceGAN shows competitive results, compared
to those of existing methods.
</p>
<a href="http://arxiv.org/abs/2012.00282" target="_blank">arXiv:2012.00282</a> [<a href="http://arxiv.org/pdf/2012.00282" target="_blank">pdf</a>]

<h2>Procedure for the Safety Assessment of an Autonomous Vehicle Using Real-World Scenarios. (arXiv:2012.00643v2 [cs.RO] UPDATED)</h2>
<h3>Erwin de Gelder, Olaf Op den Camp</h3>
<p>The development of Autonomous Vehicles (AVs) has made significant progress in
the last years. An important aspect in the development of AVs is the assessment
of their safety. New approaches need to be worked out. Among these, real-world
scenario-based assessment is widely supported by many players in the automotive
field. Scenario-based assessment allows for using virtual simulation tools in
addition to physical tests, such as on a test track, proving ground, or public
road.

We propose a procedure for real-world scenario-based road-approval assessment
considering three stakeholders: the applicant, the assessor, and the (road or
vehicle) authority. The challenges are as follows. Firstly, the tests need to
be tailored to the operational design domain (ODD) and dynamic driving task
(DDT) description of the AV. Secondly, it is assumed that the applicant does
not want to disclose all of the detailed test results because of proprietary or
confidential information contained in these results. Thirdly, due to the
complex ODD and DDT, many test scenarios are required to obtain sufficient
confidence in the assessment of the AV. Consequently, it is assumed that due to
limited resources, it is infeasible for the assessor to conduct all (physical)
tests.

We propose a systematic approach for determining the tests that are based on
the requirements set by the authority and the AV's ODD and DDT description,
such that the tests are tailored to the applicable ODD and DDT. Each test comes
with metrics that enables the applicant to provide a performance rating of the
AV for each of the tests. By only providing a performance rating for each test,
the applicant does not need to disclose the details of the test results. In our
proposed procedure, the assessor only conducts a limited number of tests. The
main purpose of these tests is to verify the fidelity of the results provided
by the applicant.
</p>
<a href="http://arxiv.org/abs/2012.00643" target="_blank">arXiv:2012.00643</a> [<a href="http://arxiv.org/pdf/2012.00643" target="_blank">pdf</a>]

<h2>Approximate Kernel PCA Using Random Features: Computational vs. Statistical Trade-off. (arXiv:1706.06296v3 [stat.ML] UPDATED)</h2>
<h3>Bharath Sriperumbudur, Nicholas Sterge</h3>
<p>Kernel methods are powerful learning methodologies that provide a simple way
to construct nonlinear algorithms from linear ones. Despite their popularity,
they suffer from poor scalability in big data scenarios. Various approximation
methods, including random feature approximation, have been proposed to
alleviate the problem. However, the statistical consistency of most of these
approximate kernel methods is not well understood except for kernel ridge
regression wherein it has been shown that the random feature approximation is
not only computationally efficient but also statistically consistent with a
minimax optimal rate of convergence. In this paper, we investigate the efficacy
of random feature approximation in the context of kernel principal component
analysis (KPCA) by studying the trade-off between computational and statistical
behaviors of approximate KPCA. We show that the approximate KPCA is both
computationally and statistically efficient compared to KPCA in terms of the
error associated with reconstructing a kernel function based on its projection
onto the corresponding eigenspaces. The analysis hinges on Bernstein-type
inequalities for the operator and Hilbert-Schmidt norms of a self-adjoint
Hilbert-Schmidt operator-valued U-statistics, which is of independent interest.
</p>
<a href="http://arxiv.org/abs/1706.06296" target="_blank">arXiv:1706.06296</a> [<a href="http://arxiv.org/pdf/1706.06296" target="_blank">pdf</a>]

