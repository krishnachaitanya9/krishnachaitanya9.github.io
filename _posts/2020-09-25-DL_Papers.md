---
title: Latest Deep Learning Papers
date: 2020-12-16 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (173 Articles)</h1>
<h2>End-to-end Generative Floor-plan and Layout with Attributes and Relation Graph. (arXiv:2012.08514v1 [cs.CV])</h2>
<h3>Xinhan Di, Pengqian Yu, Danfeng Yang, Hong Zhu, Changyu Sun, YinDong Liu</h3>
<p>In this paper, we propose an end-end model for producing furniture layout for
interior scene synthesis from the random vector. This proposed model is aimed
to support professional interior designers to produce the interior decoration
solutions more quickly. The proposed model combines a conditional floor-plan
module of the room, a conditional graphical floor-plan module of the room and a
conditional layout module. As compared with the prior work on scene synthesis,
our proposed three modules enhance the ability of auto-layout generation given
the dimensional category of the room. We conduct our experiments on the
proposed real-world interior layout dataset that contains $191208$ designs from
the professional designers. Our numerical results demonstrate that the proposed
model yields higher-quality layouts in comparison with the state-of-the-art
model. The dataset and code are released
\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}
</p>
<a href="http://arxiv.org/abs/2012.08514" target="_blank">arXiv:2012.08514</a> [<a href="http://arxiv.org/pdf/2012.08514" target="_blank">pdf</a>]

<h2>Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection. (arXiv:2012.08548v1 [cs.CV])</h2>
<h3>Jingru Tan, Xin Lu, Gang Zhang, Changqing Yin, Quanquan Li</h3>
<p>Recently proposed decoupled training methods emerge as a dominant paradigm
for long-tailed object detection. But they require an extra fine-tuning stage,
and the disjointed optimization of representation and classifier might lead to
suboptimal results. However, end-to-end training methods, like equalization
loss (EQL), still perform worse than decoupled training methods. In this paper,
we reveal the main issue in long-tailed object detection is the imbalanced
gradients between positives and negatives, and find that EQL does not solve it
well. To address the problem of imbalanced gradients, we introduce a new
version of equalization loss, called equalization loss v2 (EQL v2), a novel
gradient guided reweighing mechanism that re-balances the training process for
each category independently and equally. Extensive experiments are performed on
the challenging LVIS benchmark. EQL v2 outperforms origin EQL by about 4 points
overall AP with 14-18 points improvements on the rare categories. More
importantly, It also surpasses decoupled training methods. Without further
tuning for the Open Images dataset, EQL v2 improves EQL by 6.3 points AP,
showing strong generalization ability. Codes will be released at
https://github.com/tztztztztz/eqlv2
</p>
<a href="http://arxiv.org/abs/2012.08548" target="_blank">arXiv:2012.08548</a> [<a href="http://arxiv.org/pdf/2012.08548" target="_blank">pdf</a>]

<h2>Robust Optimal Classification Trees under Noisy Labels. (arXiv:2012.08560v1 [cs.LG])</h2>
<h3>V&#xed;ctor Blanco, Alberto Jap&#xf3;n, Justo Puerto</h3>
<p>In this paper we propose a novel methodology to construct Optimal
Classification Trees that takes into account that noisy labels may occur in the
training sample. Our approach rests on two main elements: (1) the splitting
rules for the classification trees are designed to maximize the separation
margin between classes applying the paradigm of SVM; and (2) some of the labels
of the training sample are allowed to be changed during the construction of the
tree trying to detect the label noise. Both features are considered and
integrated together to design the resulting Optimal Classification Tree. We
present a Mixed Integer Non Linear Programming formulation for the problem,
suitable to be solved using any of the available off-the-shelf solvers. The
model is analyzed and tested on a battery of standard datasets taken from UCI
Machine Learning repository, showing the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2012.08560" target="_blank">arXiv:2012.08560</a> [<a href="http://arxiv.org/pdf/2012.08560" target="_blank">pdf</a>]

<h2>Grounding Artificial Intelligence in the Origins of Human Behavior. (arXiv:2012.08564v1 [cs.AI])</h2>
<h3>Eleni Nisioti, Cl&#xe9;ment Moulin-Frier</h3>
<p>Recent advances in Artificial Intelligence (AI) have revived the quest for
agents able to acquire an open-ended repertoire of skills. However, although
this ability is fundamentally related to the characteristics of human
intelligence, research in this field rarely considers the processes that may
have guided the emergence of complex cognitive capacities during the evolution
of the species.

Research in Human Behavioral Ecology (HBE) seeks to understand how the
behaviors characterizing human nature can be conceived as adaptive responses to
major changes in the structure of our ecological niche. In this paper, we
propose a framework highlighting the role of environmental complexity in
open-ended skill acquisition, grounded in major hypotheses from HBE and recent
contributions in Reinforcement learning (RL). We use this framework to
highlight fundamental links between the two disciplines, as well as to identify
feedback loops that bootstrap ecological complexity and create promising
research directions for AI researchers.
</p>
<a href="http://arxiv.org/abs/2012.08564" target="_blank">arXiv:2012.08564</a> [<a href="http://arxiv.org/pdf/2012.08564" target="_blank">pdf</a>]

<h2>Personalized Federated Learning with First Order Model Optimization. (arXiv:2012.08565v1 [cs.LG])</h2>
<h3>Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, Jose M. Alvarez</h3>
<p>While federated learning traditionally aims to train a single global model
across decentralized local datasets, one model may not always be ideal for all
participating clients. Here we propose an alternative, where each client only
federates with other relevant clients to obtain a stronger model per
client-specific objectives. To achieve this personalization, rather than
computing a single model average with constant weights for the entire
federation as in traditional FL, we efficiently calculate optimal weighted
model combinations for each client, based on figuring out how much a client can
benefit from another's model. We do not assume knowledge of any underlying data
distributions or client similarities, and allow each client to optimize for
arbitrary target distributions of interest, enabling greater flexibility for
personalization. We evaluate and characterize our method on a variety of
federated settings, datasets, and degrees of local data heterogeneity. Our
method outperforms existing alternatives, while also enabling new features for
personalized FL such as transfer outside of local data distributions.
</p>
<a href="http://arxiv.org/abs/2012.08565" target="_blank">arXiv:2012.08565</a> [<a href="http://arxiv.org/pdf/2012.08565" target="_blank">pdf</a>]

<h2>Attentional Local Contrast Networks for Infrared Small Target Detection. (arXiv:2012.08573v1 [cs.CV])</h2>
<h3>Yimian Dai, Yiquan Wu, Fei Zhou, Kobus Barnard</h3>
<p>To mitigate the issue of minimal intrinsic features for pure data-driven
methods, in this paper, we propose a novel model-driven deep network for
infrared small target detection, which combines discriminative networks and
conventional model-driven methods to make use of both labeled data and the
domain knowledge. By designing a feature map cyclic shift scheme, we modularize
a conventional local contrast measure method as a depth-wise parameterless
nonlinear feature refinement layer in an end-to-end network, which encodes
relatively long-range contextual interactions with clear physical
interpretability. To highlight and preserve the small target features, we also
exploit a bottom-up attentional modulation integrating the smaller scale subtle
details of low-level features into high-level features of deeper layers. We
conduct detailed ablation studies with varying network depths to empirically
verify the effectiveness and efficiency of the design of each component in our
network architecture. We also compare the performance of our network against
other model-driven methods and deep networks on the open SIRST dataset as well.
The results suggest that our network yields a performance boost over its
competitors. Our code, trained models, and results are available online.
</p>
<a href="http://arxiv.org/abs/2012.08573" target="_blank">arXiv:2012.08573</a> [<a href="http://arxiv.org/pdf/2012.08573" target="_blank">pdf</a>]

<h2>FoggySight: A Scheme for Facial Lookup Privacy. (arXiv:2012.08588v1 [cs.CV])</h2>
<h3>Ivan Evtimov, Pascal Sturmfels, Tadayoshi Kohno</h3>
<p>Advances in deep learning algorithms have enabled better-than-human
performance on face recognition tasks. In parallel, private companies have been
scraping social media and other public websites that tie photos to identities
and have built up large databases of labeled face images. Searches in these
databases are now being offered as a service to law enforcement and others and
carry a multitude of privacy risks for social media users. In this work, we
tackle the problem of providing privacy from such face recognition systems. We
propose and evaluate FoggySight, a solution that applies lessons learned from
the adversarial examples literature to modify facial photos in a
privacy-preserving manner before they are uploaded to social media.
FoggySight's core feature is a community protection strategy where users acting
as protectors of privacy for others upload decoy photos generated by
adversarial machine learning algorithms. We explore different settings for this
scheme and find that it does enable protection of facial privacy -- including
against a facial recognition service with unknown internals.
</p>
<a href="http://arxiv.org/abs/2012.08588" target="_blank">arXiv:2012.08588</a> [<a href="http://arxiv.org/pdf/2012.08588" target="_blank">pdf</a>]

<h2>Semantic Annotation for Tabular Data. (arXiv:2012.08594v1 [cs.AI])</h2>
<h3>Udayan Khurana, Sainyam Galhotra</h3>
<p>Detecting semantic concept of columns in tabular data is of particular
interest to many applications ranging from data integration, cleaning, search
to feature engineering and model building in machine learning. Recently,
several works have proposed supervised learning-based or heuristic
pattern-based approaches to semantic type annotation. Both have shortcomings
that prevent them from generalizing over a large number of concepts or
examples. Many neural network based methods also present scalability issues.
Additionally, none of the known methods works well for numerical data. We
propose $C^2$, a column to concept mapper that is based on a maximum likelihood
estimation approach through ensembles. It is able to effectively utilize vast
amounts of, albeit somewhat noisy, openly available table corpora in addition
to two popular knowledge graphs to perform effective and efficient concept
prediction for structured data. We demonstrate the effectiveness of $C^2$ over
available techniques on 9 datasets, the most comprehensive comparison on this
topic so far.
</p>
<a href="http://arxiv.org/abs/2012.08594" target="_blank">arXiv:2012.08594</a> [<a href="http://arxiv.org/pdf/2012.08594" target="_blank">pdf</a>]

<h2>Energy-Constrained Delivery of Goods with Drones Under Varying Wind Conditions. (arXiv:2012.08602v1 [cs.RO])</h2>
<h3>Francesco Betti Sorbelli, Federico Cor&#xe8;, Sajal K. Das, Cristina M. Pinotti</h3>
<p>In this paper, we study the feasibility of sending drones to deliver goods
from a depot to a customer by solving what we call the Mission-Feasibility
Problem (MFP). Due to payload constraints, the drone can serve only one
customer at a time. To this end, we propose a novel framework based on
time-dependent cost graphs to properly model the MFP and tackle the delivery
dynamics. When the drone moves in the delivery area, the global wind may change
thereby affecting the drone's energy consumption, which in turn can increase or
decrease. This issue is addressed by designing three algorithms, namely: (i)
compute the route of minimum energy once, at the beginning of the mission, (ii)
dynamically reconsider the most convenient trip towards the destination, and
(iii) dynamically select only the best local choice. We evaluate the
performance of our algorithms on both synthetic and real-world data. The
changes in the drone's energy consumption are reflected by changes in the cost
of the edges of the graphs. The algorithms receive the new costs every time the
drone flies over a new vertex, and they have no full knowledge in advance of
the weights. We compare them in terms of the percentage of missions that are
completed with success (the drone delivers the goods and comes back to the
depot), with delivered (the drone delivers the goods but cannot come back to
the depot), and with failure (the drone neither delivers the goods nor comes
back to the depot).
</p>
<a href="http://arxiv.org/abs/2012.08602" target="_blank">arXiv:2012.08602</a> [<a href="http://arxiv.org/pdf/2012.08602" target="_blank">pdf</a>]

<h2>Multi-modal AsynDGAN: Learn From Distributed Medical Image Data without Sharing Private Information. (arXiv:2012.08604v1 [cs.LG])</h2>
<h3>Qi Chang, Zhennan Yan, Lohendran Baskaran, Hui Qu, Yikai Zhang, Tong Zhang, Shaoting Zhang, Dimitris N. Metaxas</h3>
<p>As deep learning technologies advance, increasingly more data is necessary to
generate general and robust models for various tasks. In the medical domain,
however, large-scale and multi-parties data training and analyses are
infeasible due to the privacy and data security concerns. In this paper, we
propose an extendable and elastic learning framework to preserve privacy and
security while enabling collaborative learning with efficient communication.
The proposed framework is named distributed Asynchronized Discriminator
Generative Adversarial Networks (AsynDGAN), which consists of a centralized
generator and multiple distributed discriminators. The advantages of our
proposed framework are five-fold: 1) the central generator could learn the real
data distribution from multiple datasets implicitly without sharing the image
data; 2) the framework is applicable for single-modality or multi-modality
data; 3) the learned generator can be used to synthesize samples for
down-stream learning tasks to achieve close-to-real performance as using actual
samples collected from multiple data centers; 4) the synthetic samples can also
be used to augment data or complete missing modalities for one single data
center; 5) the learning process is more efficient and requires lower bandwidth
than other distributed deep learning methods.
</p>
<a href="http://arxiv.org/abs/2012.08604" target="_blank">arXiv:2012.08604</a> [<a href="http://arxiv.org/pdf/2012.08604" target="_blank">pdf</a>]

<h2>Pose Error Reduction for Focus Enhancement in Thermal Synthetic Aperture Visualization. (arXiv:2012.08606v1 [cs.CV])</h2>
<h3>Indrajit Kurmi, David C. Schedl, Oliver Bimber</h3>
<p>Airborne optical sectioning, an effective aerial synthetic aperture imaging
technique for revealing artifacts occluded by forests, requires precise
measurements of drone poses. In this article we present a new approach for
reducing pose estimation errors beyond the possibilities of conventional
Perspective-n-Point solutions by considering the underlying optimization as a
focusing problem. We present an efficient image integration technique, which
also reduces the parameter search space to achieve realistic processing times,
and improves the quality of resulting synthetic integral images.
</p>
<a href="http://arxiv.org/abs/2012.08606" target="_blank">arXiv:2012.08606</a> [<a href="http://arxiv.org/pdf/2012.08606" target="_blank">pdf</a>]

<h2>BeBold: Exploration Beyond the Boundary of Explored Regions. (arXiv:2012.08621v1 [cs.LG])</h2>
<h3>Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E. Gonzalez, Yuandong Tian</h3>
<p>Efficient exploration under sparse rewards remains a key challenge in deep
reinforcement learning. To guide exploration, previous work makes extensive use
of intrinsic reward (IR). There are many heuristics for IR, including
visitation counts, curiosity, and state-difference. In this paper, we analyze
the pros and cons of each method and propose the regulated difference of
inverse visitation counts as a simple but effective criterion for IR. The
criterion helps the agent explore Beyond the Boundary of explored regions and
mitigates common issues in count-based methods, such as short-sightedness and
detachment. The resulting method, BeBold, solves the 12 most challenging
procedurally-generated tasks in MiniGrid with just 120M environment steps,
without any curriculum learning. In comparison, the previous SoTA only solves
50% of the tasks. BeBold also achieves SoTA on multiple tasks in NetHack, a
popular rogue-like game that contains more challenging procedurally-generated
environments.
</p>
<a href="http://arxiv.org/abs/2012.08621" target="_blank">arXiv:2012.08621</a> [<a href="http://arxiv.org/pdf/2012.08621" target="_blank">pdf</a>]

<h2>Smart Mobility Ontology: Current Trends and Future Directions. (arXiv:2012.08622v1 [cs.AI])</h2>
<h3>Ali Yazdizadeh, Bilal Farooq</h3>
<p>Ontology is the explicit and formal representation of the concepts in a
domain and relations among them. Transportation science is a wide domain
dealing with mobility over various complex and interconnected transportation
systems, such as land, aviation, and maritime transport, and can take
considerable advantage from ontology development. While several studies can be
found in the recent literature, there exists a large potential to improve and
develop a comprehensive smart mobility ontology. The current chapter aims to
present different aspects of ontology development in general, such as ontology
development methods, languages, tools, and software. Subsequently, it presents
the currently available mobility-related ontologies developed across different
domains, such as transportation, smart cities, goods mobility, sensors. Current
gaps in the available ontologies are identified, and future directions
regarding ontology development are proposed that can incorporate the
forthcoming autonomous and connected vehicles, mobility as a service (MaaS),
and other disruptive transportation technologies and services.
</p>
<a href="http://arxiv.org/abs/2012.08622" target="_blank">arXiv:2012.08622</a> [<a href="http://arxiv.org/pdf/2012.08622" target="_blank">pdf</a>]

<h2>Post-Hurricane Damage Assessment Using Satellite Imagery and Geolocation Features. (arXiv:2012.08624v1 [cs.CV])</h2>
<h3>Quoc Dung Cao, Youngjun Choe</h3>
<p>Gaining timely and reliable situation awareness after hazard events such as a
hurricane is crucial to emergency managers and first responders. One effective
way to achieve that goal is through damage assessment. Recently, disaster
researchers have been utilizing imagery captured through satellites or drones
to quantify the number of flooded/damaged buildings. In this paper, we propose
a mixed data approach, which leverages publicly available satellite imagery and
geolocation features of the affected area to identify damaged buildings after a
hurricane. The method demonstrated significant improvement from performing a
similar task using only imagery features, based on a case study of Hurricane
Harvey affecting Greater Houston area in 2017. This result opens door to a wide
range of possibilities to unify the advancement in computer vision algorithms
such as convolutional neural networks and traditional methods in damage
assessment, for example, using flood depth or bare-earth topology. In this
work, a creative choice of the geolocation features was made to provide extra
information to the imagery features, but it is up to the users to decide which
other features can be included to model the physical behavior of the events,
depending on their domain knowledge and the type of disaster. The dataset
curated in this work is made openly available (DOI: 10.17603/ds2-3cca-f398).
</p>
<a href="http://arxiv.org/abs/2012.08624" target="_blank">arXiv:2012.08624</a> [<a href="http://arxiv.org/pdf/2012.08624" target="_blank">pdf</a>]

<h2>Learning Prediction Intervals for Model Performance. (arXiv:2012.08625v1 [cs.LG])</h2>
<h3>Benjamin Elder, Matthew Arnold, Anupama Murthi, Jiri Navratil</h3>
<p>Understanding model performance on unlabeled data is a fundamental challenge
of developing, deploying, and maintaining AI systems. Model performance is
typically evaluated using test sets or periodic manual quality assessments,
both of which require laborious manual data labeling. Automated performance
prediction techniques aim to mitigate this burden, but potential inaccuracy and
a lack of trust in their predictions has prevented their widespread adoption.
We address this core problem of performance prediction uncertainty with a
method to compute prediction intervals for model performance. Our methodology
uses transfer learning to train an uncertainty model to estimate the
uncertainty of model performance predictions. We evaluate our approach across a
wide range of drift conditions and show substantial improvement over
competitive baselines. We believe this result makes prediction intervals, and
performance prediction in general, significantly more practical for real-world
use.
</p>
<a href="http://arxiv.org/abs/2012.08625" target="_blank">arXiv:2012.08625</a> [<a href="http://arxiv.org/pdf/2012.08625" target="_blank">pdf</a>]

<h2>Open Problems in Cooperative AI. (arXiv:2012.08630v1 [cs.AI])</h2>
<h3>Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo, Kate Larson, Thore Graepel</h3>
<p>Problems of cooperation--in which agents seek ways to jointly improve their
welfare--are ubiquitous and important. They can be found at scales ranging from
our daily routines--such as driving on highways, scheduling meetings, and
working collaboratively--to our global challenges--such as peace, commerce, and
pandemic preparedness. Arguably, the success of the human species is rooted in
our ability to cooperate. Since machines powered by artificial intelligence are
playing an ever greater role in our lives, it will be important to equip them
with the capabilities necessary to cooperate and to foster cooperation.

We see an opportunity for the field of artificial intelligence to explicitly
focus effort on this class of problems, which we term Cooperative AI. The
objective of this research would be to study the many aspects of the problems
of cooperation and to innovate in AI to contribute to solving these problems.
Central goals include building machine agents with the capabilities needed for
cooperation, building tools to foster cooperation in populations of (machine
and/or human) agents, and otherwise conducting AI research for insight relevant
to problems of cooperation. This research integrates ongoing work on
multi-agent systems, game theory and social choice, human-machine interaction
and alignment, natural-language processing, and the construction of social
tools and platforms. However, Cooperative AI is not the union of these existing
areas, but rather an independent bet about the productivity of specific kinds
of conversations that involve these and other areas. We see opportunity to more
explicitly focus on the problem of cooperation, to construct unified theory and
vocabulary, and to build bridges with adjacent communities working on
cooperation, including in the natural, social, and behavioural sciences.
</p>
<a href="http://arxiv.org/abs/2012.08630" target="_blank">arXiv:2012.08630</a> [<a href="http://arxiv.org/pdf/2012.08630" target="_blank">pdf</a>]

<h2>Multi-Modal Anomaly Detection for Unstructured and Uncertain Environments. (arXiv:2012.08637v1 [cs.RO])</h2>
<h3>Tianchen Ji, Sri Theja Vuppala, Girish Chowdhary, Katherine Driggs-Campbell</h3>
<p>To achieve high-levels of autonomy, modern robots require the ability to
detect and recover from anomalies and failures with minimal human supervision.
Multi-modal sensor signals could provide more information for such anomaly
detection tasks; however, the fusion of high-dimensional and heterogeneous
sensor modalities remains a challenging problem. We propose a deep learning
neural network: supervised variational autoencoder (SVAE), for failure
identification in unstructured and uncertain environments. Our model leverages
the representational power of VAE to extract robust features from
high-dimensional inputs for supervised learning tasks. The training objective
unifies the generative model and the discriminative model, thus making the
learning a one-stage procedure. Our experiments on real field robot data
demonstrate superior failure identification performance than baseline methods,
and that our model learns interpretable representations. Videos of our results
are available on our website:
https://sites.google.com/illinois.edu/supervised-vae .
</p>
<a href="http://arxiv.org/abs/2012.08637" target="_blank">arXiv:2012.08637</a> [<a href="http://arxiv.org/pdf/2012.08637" target="_blank">pdf</a>]

<h2>Sparsity-driven Digital Terrain Model Extraction. (arXiv:2012.08639v1 [cs.CV])</h2>
<h3>Fatih Nar, Erdal Yilmaz, Gustau Camps-Valls</h3>
<p>We here introduce an automatic Digital Terrain Model (DTM) extraction method.
The proposed sparsity-driven DTM extractor (SD-DTM) takes a high-resolution
Digital Surface Model (DSM) as an input and constructs a high-resolution DTM
using the variational framework. To obtain an accurate DTM, an iterative
approach is proposed for the minimization of the target variational cost
function. Accuracy of the SD-DTM is shown in a real-world DSM data set. We show
the efficiency and effectiveness of the approach both visually and
quantitatively via residual plots in illustrative terrain types.
</p>
<a href="http://arxiv.org/abs/2012.08639" target="_blank">arXiv:2012.08639</a> [<a href="http://arxiv.org/pdf/2012.08639" target="_blank">pdf</a>]

<h2>Spectral band selection for vegetation properties retrieval using Gaussian processes regression. (arXiv:2012.08640v1 [cs.CV])</h2>
<h3>Jochem Verrelst, Juan Pablo Rivera, Anatoly Gitelson, Jesus Delegido, Jos&#xe9; Moreno, Gustau Camps-Valls</h3>
<p>With current and upcoming imaging spectrometers, automated band analysis
techniques are needed to enable efficient identification of most informative
bands to facilitate optimized processing of spectral data into estimates of
biophysical variables. This paper introduces an automated spectral band
analysis tool (BAT) based on Gaussian processes regression (GPR) for the
spectral analysis of vegetation properties. The GPR-BAT procedure sequentially
backwards removes the least contributing band in the regression model for a
given variable until only one band is kept. GPR-BAT is implemented within the
framework of the free ARTMO's MLRA (machine learning regression algorithms)
toolbox, which is dedicated to the transforming of optical remote sensing
images into biophysical products. GPR-BAT allows (1) to identify the most
informative bands in relating spectral data to a biophysical variable, and (2)
to find the least number of bands that preserve optimized accurate predictions.
This study concludes that a wise band selection of hyperspectral data is
strictly required for optimal vegetation properties mapping.
</p>
<a href="http://arxiv.org/abs/2012.08640" target="_blank">arXiv:2012.08640</a> [<a href="http://arxiv.org/pdf/2012.08640" target="_blank">pdf</a>]

<h2>A grid-point detection method based on U-net for a structured light system. (arXiv:2012.08641v1 [cs.CV])</h2>
<h3>Dieuthuy Pham, Minhtuan Ha, Changyan Xiao</h3>
<p>Accurate detection of the feature points of the projected pattern plays an
extremely important role in one-shot 3D reconstruction systems, especially for
the ones using a grid pattern. To solve this problem, this paper proposes a
grid-point detection method based on U-net. A specific dataset is designed that
includes the images captured with the two-shot imaging method and the ones
acquired with the one-shot imaging method. Among them, the images in the first
group after labeled as the ground truth images and the images captured at the
same pose with the one-shot method are cut into small patches with the size of
64x64 pixels then feed to the training set. The remaining of the images in the
second group is the test set. The experimental results show that our method can
achieve a better detecting performance with higher accuracy in comparison with
the previous methods.
</p>
<a href="http://arxiv.org/abs/2012.08641" target="_blank">arXiv:2012.08641</a> [<a href="http://arxiv.org/pdf/2012.08641" target="_blank">pdf</a>]

<h2>Does the dataset meet your expectations? Explaining sample representation in image data. (arXiv:2012.08642v1 [cs.CV])</h2>
<h3>Dhasarathy Parthasarathy, Anton Johansson</h3>
<p>Since the behavior of a neural network model is adversely affected by a lack
of diversity in training data, we present a method that identifies and explains
such deficiencies. When a dataset is labeled, we note that annotations alone
are capable of providing a human interpretable summary of sample diversity.
This allows explaining any lack of diversity as the mismatch found when
comparing the \textit{actual} distribution of annotations in the dataset with
an \textit{expected} distribution of annotations, specified manually to capture
essential label diversity. While, in many practical cases, labeling (samples
$\rightarrow$ annotations) is expensive, its inverse, simulation (annotations
$\rightarrow$ samples) can be cheaper. By mapping the expected distribution of
annotations into test samples using parametric simulation, we present a method
that explains sample representation using the mismatch in diversity between
simulated and collected data. We then apply the method to examine a dataset of
geometric shapes to qualitatively and quantitatively explain sample
representation in terms of comprehensible aspects such as size, position, and
pixel brightness.
</p>
<a href="http://arxiv.org/abs/2012.08642" target="_blank">arXiv:2012.08642</a> [<a href="http://arxiv.org/pdf/2012.08642" target="_blank">pdf</a>]

<h2>Enabling Collaborative Video Sensing at the Edge through Convolutional Sharing. (arXiv:2012.08643v1 [cs.CV])</h2>
<h3>Kasthuri Jayarajah, Dhanuja Wanniarachchige, Archan Misra</h3>
<p>While Deep Neural Network (DNN) models have provided remarkable advances in
machine vision capabilities, their high computational complexity and model
sizes present a formidable roadblock to deployment in AIoT-based sensing
applications. In this paper, we propose a novel paradigm by which peer nodes in
a network can collaborate to improve their accuracy on person detection, an
exemplar machine vision task. The proposed methodology requires no re-training
of the DNNs and incurs minimal processing latency as it extracts scene
summaries from the collaborators and injects back into DNNs of the reference
cameras, on-the-fly. Early results show promise with improvements in recall as
high as 10% with a single collaborator, on benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.08643" target="_blank">arXiv:2012.08643</a> [<a href="http://arxiv.org/pdf/2012.08643" target="_blank">pdf</a>]

<h2>Online Learning Demands in Max-min Fairness. (arXiv:2012.08648v1 [stat.ML])</h2>
<h3>Kirthevasan Kandasamy, Gur-Eyal Sela, Joseph E Gonzalez, Michael I Jordan, Ion Stoica</h3>
<p>We describe mechanisms for the allocation of a scarce resource among multiple
users in a way that is efficient, fair, and strategy-proof, but when users do
not know their resource requirements. The mechanism is repeated for multiple
rounds and a user's requirements can change on each round. At the end of each
round, users provide feedback about the allocation they received, enabling the
mechanism to learn user preferences over time. Such situations are common in
the shared usage of a compute cluster among many users in an organisation,
where all teams may not precisely know the amount of resources needed to
execute their jobs. By understating their requirements, users will receive less
than they need and consequently not achieve their goals. By overstating them,
they may siphon away precious resources that could be useful to others in the
organisation. We formalise this task of online learning in fair division via
notions of efficiency, fairness, and strategy-proofness applicable to this
setting, and study this problem under three types of feedback: when the users'
observations are deterministic, when they are stochastic and follow a
parametric model, and when they are stochastic and nonparametric. We derive
mechanisms inspired by the classical max-min fairness procedure that achieve
these requisites, and quantify the extent to which they are achieved via
asymptotic rates. We corroborate these insights with an experimental evaluation
on synthetic problems and a web-serving task.
</p>
<a href="http://arxiv.org/abs/2012.08648" target="_blank">arXiv:2012.08648</a> [<a href="http://arxiv.org/pdf/2012.08648" target="_blank">pdf</a>]

<h2>Accelerating Distributed Online Meta-Learning via Multi-Agent Collaboration under Limited Communication. (arXiv:2012.08660v1 [cs.LG])</h2>
<h3>Sen Lin, Mehmet Dedeoglu, Junshan Zhang</h3>
<p>Thanks to the fast learning capability of a new task with small datasets,
online meta-learning has become an appealing technique for enabling edge
computing in the IoT ecosystems. Nevertheless, to learn a good meta-model for
within-task fast adaptation, a single agent alone has to learn over many tasks,
inevitably leading to the cold-start problem. Seeing that in a multi-agent
network the learning tasks across different agents often share some model
similarity, a fundamental question to ask is "Is it possible to accelerate the
online meta-learning at each agent via limited communication and if yes how
much benefit can be achieved?" To answer this, we propose a multi-agent online
meta-learning framework and treat it as an equivalent two-level nested online
convex optimization (OCO) problem. By characterizing the upper bound of the
agent-task-averaged regret, we show that the performance ceiling of the
multi-agent online meta-learning heavily depends on how much an agent can
benefit from distributed network-level OCO via limited communication, which
however remains unclear. To tackle this challenge, we further study a
distributed online gradient descent algorithm with gradient tracking where
agents collaboratively track the global gradient through only one communication
step per iteration, and it results in $O(\sqrt{T/N})$ for the average regret
per agent, i.e., a factor of $\sqrt{1/N}$ speedup compared with the optimal
single-agent regret $O(\sqrt{T})$ after $T$ iterations, where $N$ is the number
of agents. Building on this sharp performance speedup, we next develop a
multi-agent online meta-learning algorithm and show that it can achieve the
optimal task-average regret at a faster rate of $O(1/\sqrt{NT})$ via limited
communication, compared to single-agent online meta-learning. Extensive
experiments corroborate the theoretic results.
</p>
<a href="http://arxiv.org/abs/2012.08660" target="_blank">arXiv:2012.08660</a> [<a href="http://arxiv.org/pdf/2012.08660" target="_blank">pdf</a>]

<h2>Automated system to measure Tandem Gait to assess executive functions in children. (arXiv:2012.08662v1 [cs.CV])</h2>
<h3>Mohammad Zaki Zadeh, Ashwin Ramesh Babu, Ashish Jaiswal, Maria Kyrarini, Fillia Makedon</h3>
<p>As mobile technologies have become ubiquitous in recent years, computer-based
cognitive tests have become more popular and efficient. In this work, we focus
on assessing motor function in children by analyzing their gait movements.
Although there has been a lot of research on designing automated assessment
systems for gait analysis, most of these efforts use obtrusive wearable sensors
for measuring body movements. We have devised a computer vision-based
assessment system that only requires a camera which makes it easier to employ
in school or home environments. A dataset has been created with 27 children
performing the test. Furthermore in order to improve the accuracy of the
system, a deep learning based model was pre-trained on NTU-RGB+D 120 dataset
and then it was fine-tuned on our gait dataset. The results highlight the
efficacy of proposed work for automating the assessment of children's
performances by achieving 76.61% classification accuracy.
</p>
<a href="http://arxiv.org/abs/2012.08662" target="_blank">arXiv:2012.08662</a> [<a href="http://arxiv.org/pdf/2012.08662" target="_blank">pdf</a>]

<h2>Mitigating bias in calibration error estimation. (arXiv:2012.08668v1 [cs.LG])</h2>
<h3>Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, Michael C. Mozer</h3>
<p>Building reliable machine learning systems requires that we correctly
understand their level of confidence. Calibration focuses on measuring the
degree of accuracy in a model's confidence and most research in calibration
focuses on techniques to improve an empirical estimate of calibration error,
ECE_bin. Using simulation, we show that ECE_bin can systematically
underestimate or overestimate the true calibration error depending on the
nature of model miscalibration, the size of the evaluation data set, and the
number of bins. Critically, ECE_bin is more strongly biased for perfectly
calibrated models. We propose a simple alternative calibration error metric,
ECE_sweep, in which the number of bins is chosen to be as large as possible
while preserving monotonicity in the calibration function. Evaluating our
measure on distributions fit to neural network confidence scores on CIFAR-10,
CIFAR-100, and ImageNet, we show that ECE_sweep produces a less biased
estimator of calibration error and therefore should be used by any researcher
wishing to evaluate the calibration of models trained on similar datasets.
</p>
<a href="http://arxiv.org/abs/2012.08668" target="_blank">arXiv:2012.08668</a> [<a href="http://arxiv.org/pdf/2012.08668" target="_blank">pdf</a>]

<h2>A Closer Look at the Robustness of Vision-and-Language Pre-trained Models. (arXiv:2012.08673v1 [cs.CV])</h2>
<h3>Linjie Li, Zhe Gan, Jingjing Liu</h3>
<p>Large-scale pre-trained multimodal transformers, such as ViLBERT and UNITER,
have propelled the state of the art in vision-and-language (V+L) research to a
new level. Although achieving impressive performance on standard tasks, to
date, it still remains unclear how robust these pre-trained models are. To
investigate, we conduct a host of thorough evaluations on existing pre-trained
models over 4 different types of V+L specific model robustness: (i) Linguistic
Variation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv)
Answer Distribution Shift. Interestingly, by standard model finetuning,
pre-trained V+L models already exhibit better robustness than many
task-specific state-of-the-art methods. To further enhance model robustness, we
propose Mango, a generic and efficient approach that learns a Multimodal
Adversarial Noise GeneratOr in the embedding space to fool pre-trained V+L
models. Differing from previous studies focused on one specific type of
robustness, Mango is task-agnostic, and enables universal performance lift for
pre-trained models over diverse tasks designed to evaluate broad aspects of
robustness. Comprehensive experiments demonstrate that Mango achieves new state
of the art on 7 out of 9 robustness benchmarks, surpassing existing methods by
a significant margin. As the first comprehensive study on V+L robustness, this
work puts robustness of pre-trained models into sharper focus, pointing new
directions for future study.
</p>
<a href="http://arxiv.org/abs/2012.08673" target="_blank">arXiv:2012.08673</a> [<a href="http://arxiv.org/pdf/2012.08673" target="_blank">pdf</a>]

<h2>Wasserstein Contrastive Representation Distillation. (arXiv:2012.08674v1 [cs.LG])</h2>
<h3>Liqun Chen, Zhe Gan, Dong Wang, Jingjing Liu, Ricardo Henao, Lawrence Carin</h3>
<p>The primary goal of knowledge distillation (KD) is to encapsulate the
information of a model learned from a teacher network into a student network,
with the latter being more compact than the former. Existing work, e.g., using
Kullback-Leibler divergence for distillation, may fail to capture important
structural knowledge in the teacher network and often lacks the ability for
feature generalization, particularly in situations when teacher and student are
built to address different classification tasks. We propose Wasserstein
Contrastive Representation Distillation (WCoRD), which leverages both primal
and dual forms of Wasserstein distance for KD. The dual form is used for global
knowledge transfer, yielding a contrastive learning objective that maximizes
the lower bound of mutual information between the teacher and the student
networks. The primal form is used for local contrastive knowledge transfer
within a mini-batch, effectively matching the distributions of features between
the teacher and the student networks. Experiments demonstrate that the proposed
WCoRD method outperforms state-of-the-art approaches on privileged information
distillation, model compression and cross-modal transfer.
</p>
<a href="http://arxiv.org/abs/2012.08674" target="_blank">arXiv:2012.08674</a> [<a href="http://arxiv.org/pdf/2012.08674" target="_blank">pdf</a>]

<h2>Policy Manifold Search for Improving Diversity-based Neuroevolution. (arXiv:2012.08676v1 [cs.LG])</h2>
<h3>Nemanja Rakicevic, Antoine Cully, Petar Kormushev</h3>
<p>Diversity-based approaches have recently gained popularity as an alternative
paradigm to performance-based policy search. A popular approach from this
family, Quality-Diversity (QD), maintains a collection of high-performing
policies separated in the diversity-metric space, defined based on policies'
rollout behaviours. When policies are parameterised as neural networks, i.e.
Neuroevolution, QD tends to not scale well with parameter space dimensionality.
Our hypothesis is that there exists a low-dimensional manifold embedded in the
policy parameter space, containing a high density of diverse and feasible
policies. We propose a novel approach to diversity-based policy search via
Neuroevolution, that leverages learned latent representations of the policy
parameters which capture the local structure of the data. Our approach
iteratively collects policies according to the QD framework, in order to (i)
build a collection of diverse policies, (ii) use it to learn a latent
representation of the policy parameters, (iii) perform policy search in the
learned latent space. We use the Jacobian of the inverse transformation
(i.e.reconstruction function) to guide the search in the latent space. This
ensures that the generated samples remain in the high-density regions of the
original space, after reconstruction. We evaluate our contributions on three
continuous control tasks in simulated environments, and compare to
diversity-based baselines. The findings suggest that our approach yields a more
efficient and robust policy search process.
</p>
<a href="http://arxiv.org/abs/2012.08676" target="_blank">arXiv:2012.08676</a> [<a href="http://arxiv.org/pdf/2012.08676" target="_blank">pdf</a>]

<h2>Inexact-ADMM based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v1 [cs.LG])</h2>
<h3>Sheng Yue, Ju Ren, Jiang Xin, Sen Lin, Junshan Zhang</h3>
<p>In order to meet the requirements for safety and latency in many IoT
applications, intelligent decisions must be made right here right now at the
network edge, calling for edge intelligence. To facilitate fast edge learning,
this work advocates a platform-aided federated meta-learning architecture,
where a set of edge nodes joint force to learn a meta-model (i.e., model
initialization for adaptation in a new learning task) by exploiting the
similarity among edge nodes as well as the cloud knowledge transfer. The
federated meta-learning problem is cast as a regularized optimization problem,
using Bregman Divergence between the edge model and the pre-trained model as
the regularization. We then devise an inexact alternating direction method of
multiplier (ADMM) based Hessian-free federated meta-learning algorithm, called
ADMM-FedMeta, with inexact Hessian estimation. Further, we analyze the
convergence properties and the rapid adaptation performance of ADMM-FedMeta for
the general non-convex case. The theoretical results show that under mild
conditions, ADMM-FedMeta converges to an $\epsilon$-approximate first-order
stationary point after at most $\mathcal{O}(1/\epsilon^2)$ communication
rounds. Extensive experimental studies on benchmark datasets demonstrate the
effectiveness and efficiency of ADMM-FedMeta, and showcase that ADMM-FedMeta
outperforms the existing baselines.
</p>
<a href="http://arxiv.org/abs/2012.08677" target="_blank">arXiv:2012.08677</a> [<a href="http://arxiv.org/pdf/2012.08677" target="_blank">pdf</a>]

<h2>Training an Emotion Detection Classifier using Frames from a Mobile Therapeutic Game for Children with Developmental Disorders. (arXiv:2012.08678v1 [cs.CV])</h2>
<h3>Peter Washington, Haik Kalantarian, Jack Kent, Arman Husic, Aaron Kline, Emilie Leblanc, Cathy Hou, Cezmi Mutlu, Kaitlyn Dunlap, Yordan Penev, Maya Varma, Nate Stockham, Brianna Chrisman, Kelley Paskov, Min Woo Sun, Jae-Yoon Jung, Catalin Voss, Nick Haber, Dennis P. Wall</h3>
<p>Automated emotion classification could aid those who struggle to recognize
emotion, including children with developmental behavioral conditions such as
autism. However, most computer vision emotion models are trained on adult
affect and therefore underperform on child faces. In this study, we designed a
strategy to gamify the collection and the labeling of child affect data in an
effort to boost the performance of automatic child emotion detection to a level
closer to what will be needed for translational digital healthcare. We
leveraged our therapeutic smartphone game, GuessWhat, which was designed in
large part for children with developmental and behavioral conditions, to gamify
the secure collection of video data of children expressing a variety of
emotions prompted by the game. Through a secure web interface gamifying the
human labeling effort, we gathered and labeled 2,155 videos, 39,968 emotion
frames, and 106,001 labels on all images. With this drastically expanded
pediatric emotion centric database (&gt;30x larger than existing public pediatric
affect datasets), we trained a pediatric emotion classification convolutional
neural network (CNN) classifier of happy, sad, surprised, fearful, angry,
disgust, and neutral expressions in children. The classifier achieved 66.9%
balanced accuracy and 67.4% F1-score on the entirety of CAFE as well as 79.1%
balanced accuracy and 78.0% F1-score on CAFE Subset A, a subset containing at
least 60% human agreement on emotions labels. This performance is at least 10%
higher than all previously published classifiers, the best of which reached
56.% balanced accuracy even when combining "anger" and "disgust" into a single
class. This work validates that mobile games designed for pediatric therapies
can generate high volumes of domain-relevant datasets to train state of the art
classifiers to perform tasks highly relevant to precision health efforts.
</p>
<a href="http://arxiv.org/abs/2012.08678" target="_blank">arXiv:2012.08678</a> [<a href="http://arxiv.org/pdf/2012.08678" target="_blank">pdf</a>]

<h2>StrokeGAN: Reducing Mode Collapse in Chinese Font Generation via Stroke Encoding. (arXiv:2012.08687v1 [cs.CV])</h2>
<h3>Jinshan Zeng, Qi Chen, Yunxin Liu, Mingwen Wang, Yuan Yao</h3>
<p>The generation of stylish Chinese fonts is an important problem involved in
many applications. Most of existing generation methods are based on the deep
generative models, particularly, the generative adversarial networks (GAN)
based models. However, these deep generative models may suffer from the mode
collapse issue, which significantly degrades the diversity and quality of
generated results. In this paper, we introduce a one-bit stroke encoding to
capture the key mode information of Chinese characters and then incorporate it
into CycleGAN, a popular deep generative model for Chinese font generation. As
a result we propose an efficient method called StrokeGAN, mainly motivated by
the observation that the stroke encoding contains amount of mode information of
Chinese characters. In order to reconstruct the one-bit stroke encoding of the
associated generated characters, we introduce a stroke-encoding reconstruction
loss imposed on the discriminator. Equipped with such one-bit stroke encoding
and stroke-encoding reconstruction loss, the mode collapse issue of CycleGAN
can be significantly alleviated, with an improved preservation of strokes and
diversity of generated characters. The effectiveness of StrokeGAN is
demonstrated by a series of generation tasks over nine datasets with different
fonts. The numerical results demonstrate that StrokeGAN generally outperforms
the state-of-the-art methods in terms of content and recognition accuracies, as
well as certain stroke error, and also generates more realistic characters.
</p>
<a href="http://arxiv.org/abs/2012.08687" target="_blank">arXiv:2012.08687</a> [<a href="http://arxiv.org/pdf/2012.08687" target="_blank">pdf</a>]

<h2>Domain Adaptive Object Detection via Feature Separation and Alignment. (arXiv:2012.08689v1 [cs.CV])</h2>
<h3>Chengyang Liang, Zixiang Zhao, Junmin Liu, Jiangshe Zhang</h3>
<p>Recently, adversarial-based domain adaptive object detection (DAOD) methods
have been developed rapidly. However, there are two issues that need to be
resolved urgently. Firstly, numerous methods reduce the distributional shifts
only by aligning all the feature between the source and target domain, while
ignoring the private information of each domain. Secondly, DAOD should consider
the feature alignment on object existing regions in images. But redundancy of
the region proposals and background noise could reduce the domain
transferability. Therefore, we establish a Feature Separation and Alignment
Network (FSANet) which consists of a gray-scale feature separation (GSFS)
module, a local-global feature alignment (LGFA) module and a
region-instance-level alignment (RILA) module. The GSFS module decomposes the
distractive/shared information which is useless/useful for detection by a
dual-stream framework, to focus on intrinsic feature of objects and resolve the
first issue. Then, LGFA and RILA modules reduce the distributional shifts of
the multi-level features. Notably, scale-space filtering is exploited to
implement adaptive searching for regions to be aligned, and instance-level
features in each region are refined to reduce redundancy and noise mentioned in
the second issue. Various experiments on multiple benchmark datasets prove that
our FSANet achieves better performance on the target domain detection and
surpasses the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.08689" target="_blank">arXiv:2012.08689</a> [<a href="http://arxiv.org/pdf/2012.08689" target="_blank">pdf</a>]

<h2>Two-Stage Copy-Move Forgery Detection with Self Deep Matching and Proposal SuperGlue. (arXiv:2012.08697v1 [cs.CV])</h2>
<h3>Yaqi Liu, Chao Xia, Xiaobin Zhu, Shengwei Xu</h3>
<p>Copy-move forgery detection identifies a tampered image by detecting pasted
and source regions in the same image. In this paper, we propose a novel
two-stage framework specially for copy-move forgery detection. The first stage
is a backbone self deep matching network, and the second stage is named as
Proposal SuperGlue. In the first stage, atrous convolution and skip matching
are incorporated to enrich spatial information and leverage hierarchical
features. Spatial attention is built on self-correlation to reinforce the
ability to find appearance similar regions. In the second stage, Proposal
SuperGlue is proposed to remove false-alarmed regions and remedy incomplete
regions. Specifically, a proposal selection strategy is designed to enclose
highly suspected regions based on proposal generation and backbone score maps.
Then, pairwise matching is conducted among candidate proposals by deep learning
based keypoint extraction and matching, i.e., SuperPoint and SuperGlue.
Integrated score map generation and refinement methods are designed to
integrate results of both stages and obtain optimized results. Our two-stage
framework unifies end-to-end deep matching and keypoint matching by obtaining
highly suspected proposals, and opens a new gate for deep learning research in
copy-move forgery detection. Experiments on publicly available datasets
demonstrate the effectiveness of our two-stage framework.
</p>
<a href="http://arxiv.org/abs/2012.08697" target="_blank">arXiv:2012.08697</a> [<a href="http://arxiv.org/pdf/2012.08697" target="_blank">pdf</a>]

<h2>Edge Entropy as an Indicator of the Effectiveness of GNNs over CNNs for Node Classification. (arXiv:2012.08698v1 [cs.LG])</h2>
<h3>Lavender Yao Jiang, John Shi, Mark Cheung, Oren Wright, Jos&#xe9; M.F. Moura</h3>
<p>Graph neural networks (GNNs) extend convolutional neural networks (CNNs) to
graph-based data. A question that arises is how much performance improvement
does the underlying graph structure in the GNN provide over the CNN (that
ignores this graph structure). To address this question, we introduce edge
entropy and evaluate how good an indicator it is for possible performance
improvement of GNNs over CNNs. Our results on node classification with
synthetic and real datasets show that lower values of edge entropy predict
larger expected performance gains of GNNs over CNNs, and, conversely, higher
edge entropy leads to expected smaller improvement gains.
</p>
<a href="http://arxiv.org/abs/2012.08698" target="_blank">arXiv:2012.08698</a> [<a href="http://arxiv.org/pdf/2012.08698" target="_blank">pdf</a>]

<h2>Natural grasp intention recognition based on gaze fixation in human-robot interaction. (arXiv:2012.08703v1 [cs.RO])</h2>
<h3>Bo Yang, Jian Huang, Xiaolong Li, Xinxing Chen, Caihua Xiong, Yasuhisa Hasegawa</h3>
<p>Eye movement is closely related to limb actions, so it can be used to infer
movement intentions. More importantly, in some cases, eye movement is the only
way for paralyzed and impaired patients with severe movement disorders to
communicate and interact with the environment. Despite this, eye-tracking
technology still has very limited application scenarios as an intention
recognition method. The goal of this paper is to achieve a natural
fixation-based grasping intention recognition method, with which a user with
hand movement disorders can intuitively express what tasks he/she wants to do
by directly looking at the object of interest. Toward this goal, we design
experiments to study the relationships of fixations in different tasks. We
propose some quantitative features from these relationships and analyze them
statistically. Then we design a natural method for grasping intention
recognition. The experimental results prove that the accuracy of the proposed
method for the grasping intention recognition exceeds 89\% on the training
objects. When this method is extendedly applied to objects not included in the
training set, the average accuracy exceeds 85\%. The grasping experiment in the
actual environment verifies the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.08703" target="_blank">arXiv:2012.08703</a> [<a href="http://arxiv.org/pdf/2012.08703" target="_blank">pdf</a>]

<h2>Sequential Attacks on Kalman Filter-based Forward Collision Warning Systems. (arXiv:2012.08704v1 [cs.RO])</h2>
<h3>Yuzhe Ma, Jon Sharp, Ruizhe Wang, Earlence Fernandes, Xiaojin Zhu</h3>
<p>Kalman Filter (KF) is widely used in various domains to perform sequential
learning or variable estimation. In the context of autonomous vehicles, KF
constitutes the core component of many Advanced Driver Assistance Systems
(ADAS), such as Forward Collision Warning (FCW). It tracks the states
(distance, velocity etc.) of relevant traffic objects based on sensor
measurements. The tracking output of KF is often fed into downstream logic to
produce alerts, which will then be used by human drivers to make driving
decisions in near-collision scenarios. In this paper, we study adversarial
attacks on KF as part of the more complex machine-human hybrid system of
Forward Collision Warning. Our attack goal is to negatively affect human
braking decisions by causing KF to output incorrect state estimations that lead
to false or delayed alerts. We accomplish this by sequentially manipulating
measure ments fed into the KF, and propose a novel Model Predictive Control
(MPC) approach to compute the optimal manipulation. Via experiments conducted
in a simulated driving environment, we show that the attacker is able to
successfully change FCW alert signals through planned manipulation over
measurements prior to the desired target time. These results demonstrate that
our attack can stealthily mislead a distracted human driver and cause vehicle
collisions.
</p>
<a href="http://arxiv.org/abs/2012.08704" target="_blank">arXiv:2012.08704</a> [<a href="http://arxiv.org/pdf/2012.08704" target="_blank">pdf</a>]

<h2>SID-NISM: A Self-supervised Low-light Image Enhancement Framework. (arXiv:2012.08707v1 [cs.CV])</h2>
<h3>Lijun Zhang, Xiao Liu, Erik Learned-Miller, Hui Guan</h3>
<p>When capturing images in low-light conditions, the images often suffer from
low visibility, which not only degrades the visual aesthetics of images, but
also significantly degenerates the performance of many computer vision
algorithms. In this paper, we propose a self-supervised low-light image
enhancement framework (SID-NISM), which consists of two components, a
Self-supervised Image Decomposition Network (SID-Net) and a Nonlinear
Illumination Saturation Mapping function (NISM). As a self-supervised network,
SID-Net could decompose the given low-light image into its reflectance,
illumination and noise directly without any prior training or reference image,
which distinguishes it from existing supervised-learning methods greatly. Then,
the decomposed illumination map will be enhanced by NISM. Having the restored
illumination map, the enhancement can be achieved accordingly. Experiments on
several public challenging low-light image datasets reveal that the images
enhanced by SID-NISM are more natural and have less unexpected artifacts.
</p>
<a href="http://arxiv.org/abs/2012.08707" target="_blank">arXiv:2012.08707</a> [<a href="http://arxiv.org/pdf/2012.08707" target="_blank">pdf</a>]

<h2>AIST: An Interpretable Attention-based Deep Learning Model for Crime Prediction. (arXiv:2012.08713v1 [cs.LG])</h2>
<h3>Yeasir Rayhan, Tanzima Hashem</h3>
<p>Accuracy and interpretability are two essential properties for a crime
prediction model. Because of the adverse effects that the crimes can have on
human life, economy and safety, we need a model that can predict future
occurrence of crime as accurately as possible so that early steps can be taken
to avoid the crime. On the other hand, an interpretable model reveals the
reason behind a model's prediction, ensures its transparency and allows us to
plan the crime prevention steps accordingly. The key challenge in developing
the model is to capture the non-linear spatial dependency and temporal patterns
of a specific crime category while keeping the underlying structure of the
model interpretable. In this paper, we develop AIST, an Attention-based
Interpretable Spatio Temporal Network for crime prediction. AIST models the
dynamic spatio-temporal correlations for a crime category based on past crime
occurrences, external features (e.g., traffic flow and point of interest (POI)
information) and recurring trends of crime. Extensive experiments show the
superiority of our model in terms of both accuracy and interpretability using
real datasets.
</p>
<a href="http://arxiv.org/abs/2012.08713" target="_blank">arXiv:2012.08713</a> [<a href="http://arxiv.org/pdf/2012.08713" target="_blank">pdf</a>]

<h2>A Deep Graph Neural Networks Architecture Design: From Global Pyramid-like Shrinkage Skeleton to Local Topology Link Rewiring. (arXiv:2012.08717v1 [cs.LG])</h2>
<h3>Gege Zhang</h3>
<p>Expressivity plays a fundamental role in evaluating deep neural networks, and
it is closely related to understanding the limit of performance improvement. In
this paper, we propose a three-pipeline training framework based on critical
expressivity, including global model contraction, weight evolution, and link's
weight rewiring. Specifically, we propose a pyramidal-like skeleton to overcome
the saddle points that affect information transfer. Then we analyze the reason
for the modularity (clustering) phenomenon in network topology and use it to
rewire potential erroneous weighted links. We conduct numerical experiments on
node classification and the results confirm that the proposed training
framework leads to a significantly improved performance in terms of fast
convergence and robustness to potential erroneous weighted links. The
architecture design on GNNs, in turn, verifies the expressivity of GNNs from
dynamics and topological space aspects and provides useful guidelines in
designing more efficient neural networks.
</p>
<a href="http://arxiv.org/abs/2012.08717" target="_blank">arXiv:2012.08717</a> [<a href="http://arxiv.org/pdf/2012.08717" target="_blank">pdf</a>]

<h2>Deep Learning to Segment Pelvic Bones: Large-scale CT Datasets and Baseline Models. (arXiv:2012.08721v1 [cs.CV])</h2>
<h3>Pengbo Liu, Hu Han, Yuanqi Du, Heqin Zhu, Yinhao Li, Feng Gu, Honghu Xiao, Jun Li, Chunpeng Zhao, Li Xiao, Xinbao Wu, S.Kevin Zhou</h3>
<p>Purpose: Pelvic bone segmentation in CT has always been an essential step in
clinical diagnosis and surgery planning of pelvic bone diseases. Existing
methods for pelvic bone segmentation are either hand-crafted or semi-automatic
and achieve limited accuracy when dealing with image appearance variations due
to the multi-site domain shift, the presence of contrasted vessels, coprolith
and chyme, bone fractures, low dose, metal artifacts, etc. Due to the lack of a
large-scale pelvic CT dataset with annotations, deep learning methods are not
fully explored. Methods: In this paper, we aim to bridge the data gap by
curating a large pelvic CT dataset pooled from multiple sources and different
manufacturers, including 1, 184 CT volumes and over 320, 000 slices with
different resolutions and a variety of the above-mentioned appearance
variations. Then we propose for the first time, to the best of our knowledge,
to learn a deep multi-class network for segmenting lumbar spine, sacrum, left
hip, and right hip, from multiple-domain images simultaneously to obtain more
effective and robust feature representations. Finally, we introduce a
post-processing tool based on the signed distance function (SDF) to eliminate
false predictions while retaining correctly predicted bone fragments. Results:
Extensive experiments on our dataset demonstrate the effectiveness of our
automatic method, achieving an average Dice of 0.987 for a metal-free volume.
SDF post-processor yields a decrease of 10.5% in hausdorff distance by
maintaining important bone fragments in post-processing phase. Conclusion: We
believe this large-scale dataset will promote the development of the whole
community and plan to open source the images, annotations, codes, and trained
baseline models at this URL1.
</p>
<a href="http://arxiv.org/abs/2012.08721" target="_blank">arXiv:2012.08721</a> [<a href="http://arxiv.org/pdf/2012.08721" target="_blank">pdf</a>]

<h2>Exacerbating Algorithmic Bias through Fairness Attacks. (arXiv:2012.08723v1 [cs.LG])</h2>
<h3>Ninareh Mehrabi, Muhammad Naveed, Fred Morstatter, Aram Galstyan</h3>
<p>Algorithmic fairness has attracted significant attention in recent years,
with many quantitative measures suggested for characterizing the fairness of
different machine learning algorithms. Despite this interest, the robustness of
those fairness measures with respect to an intentional adversarial attack has
not been properly addressed. Indeed, most adversarial machine learning has
focused on the impact of malicious attacks on the accuracy of the system,
without any regard to the system's fairness. We propose new types of data
poisoning attacks where an adversary intentionally targets the fairness of a
system. Specifically, we propose two families of attacks that target fairness
measures. In the anchoring attack, we skew the decision boundary by placing
poisoned points near specific target points to bias the outcome. In the
influence attack on fairness, we aim to maximize the covariance between the
sensitive attributes and the decision outcome and affect the fairness of the
model. We conduct extensive experiments that indicate the effectiveness of our
proposed attacks.
</p>
<a href="http://arxiv.org/abs/2012.08723" target="_blank">arXiv:2012.08723</a> [<a href="http://arxiv.org/pdf/2012.08723" target="_blank">pdf</a>]

<h2>Event-based Motion Segmentation with Spatio-Temporal Graph Cuts. (arXiv:2012.08730v1 [cs.CV])</h2>
<h3>Yi Zhou, Guillermo Gallego, Xiuyuan Lu, Siqi Liu, Shaojie Shen</h3>
<p>Identifying independently moving objects is an essential task for dynamic
scene understanding. However, traditional cameras used in dynamic scenes may
suffer from motion blur or exposure artifacts due to their sampling principle.
By contrast, event-based cameras are novel bio-inspired sensors that offer
advantages to overcome such limitations. They report pixel-wise intensity
changes asynchronously, which enables them to acquire visual information at
exactly the same rate as the scene dynamics. We have developed a method to
identify independently moving objects acquired with an event-based camera,
i.e., to solve the event-based motion segmentation problem. This paper
describes how to formulate the problem as a weakly-constrained multi-model
fitting one via energy minimization, and how to jointly solve its two
subproblems -- event-cluster assignment (labeling) and motion model fitting --
in an iterative manner, by exploiting the spatio-temporal structure of input
events in the form of a space-time graph. Experiments on available datasets
demonstrate the versatility of the method in scenes with different motion
patterns and number of moving objects. The evaluation shows that the method
performs on par or better than the state of the art without having to
predetermine the number of expected moving objects.
</p>
<a href="http://arxiv.org/abs/2012.08730" target="_blank">arXiv:2012.08730</a> [<a href="http://arxiv.org/pdf/2012.08730" target="_blank">pdf</a>]

<h2>Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification. (arXiv:2012.08733v1 [cs.CV])</h2>
<h3>Kecheng Zheng, Cuiling Lan, Wenjun Zeng, Zhizheng Zhan, Zheng-Jun Zha</h3>
<p>Many unsupervised domain adaptive (UDA) person re-identification (ReID)
approaches combine clustering-based pseudo-label prediction with feature
fine-tuning. However, because of domain gap, the pseudo-labels are not always
reliable and there are noisy/incorrect labels. This would mislead the feature
representation learning and deteriorate the performance. In this paper, we
propose to estimate and exploit the credibility of the assigned pseudo-label of
each sample to alleviate the influence of noisy labels, by suppressing the
contribution of noisy samples. We build our baseline framework using the mean
teacher method together with an additional contrastive loss. We have observed
that a sample with a wrong pseudo-label through clustering in general has a
weaker consistency between the output of the mean teacher model and the student
model. Based on this finding, we propose to exploit the uncertainty (measured
by consistency levels) to evaluate the reliability of the pseudo-label of a
sample and incorporate the uncertainty to re-weight its contribution within
various ReID losses, including the identity (ID) classification loss per
sample, the triplet loss, and the contrastive loss. Our uncertainty-guided
optimization brings significant improvement and achieves the state-of-the-art
performance on benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.08733" target="_blank">arXiv:2012.08733</a> [<a href="http://arxiv.org/pdf/2012.08733" target="_blank">pdf</a>]

<h2>Hierarchical Graph Capsule Network. (arXiv:2012.08734v1 [cs.LG])</h2>
<h3>Jinyu Yang, Peilin Zhao, Yu Rong, Chaochao Yan, Chunyuan Li, Hehuan Ma, Junzhou Huang</h3>
<p>Graph Neural Networks (GNNs) draw their strength from explicitly modeling the
topological information of structured data. However, existing GNNs suffer from
limited capability in capturing the hierarchical graph representation which
plays an important role in graph classification. In this paper, we innovatively
propose hierarchical graph capsule network (HGCN) that can jointly learn node
embeddings and extract graph hierarchies. Specifically, disentangled graph
capsules are established by identifying heterogeneous factors underlying each
node, such that their instantiation parameters represent different properties
of the same entity. To learn the hierarchical representation, HGCN
characterizes the part-whole relationship between lower-level capsules (part)
and higher-level capsules (whole) by explicitly considering the structure
information among the parts. Experimental studies demonstrate the effectiveness
of HGCN and the contribution of each component.
</p>
<a href="http://arxiv.org/abs/2012.08734" target="_blank">arXiv:2012.08734</a> [<a href="http://arxiv.org/pdf/2012.08734" target="_blank">pdf</a>]

<h2>Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks. (arXiv:2012.08740v1 [cs.LG])</h2>
<h3>Yuhang Yao, Carlee Joe-Wong</h3>
<p>We study the problem of clustering nodes in a dynamic graph, where the
connections between nodes and nodes' cluster memberships may change over time,
e.g., due to community migration. We first propose a dynamic stochastic block
model that captures these changes, and a simple decay-based clustering
algorithm that clusters nodes based on weighted connections between them, where
the weight decreases at a fixed rate over time. This decay rate can then be
interpreted as signifying the importance of including historical connection
information in the clustering. However, the optimal decay rate may differ for
clusters with different rates of turnover. We characterize the optimal decay
rate for each cluster and propose a clustering method that achieves almost
exact recovery of the true clusters. We then demonstrate the efficacy of our
clustering algorithm with optimized decay rates on simulated graph data.
Recurrent neural networks (RNNs), a popular algorithm for sequence learning,
use a similar decay-based method, and we use this insight to propose two new
RNN-GCN (graph convolutional network) architectures for semi-supervised graph
clustering. We finally demonstrate that the proposed architectures perform well
on real data compared to state-of-the-art graph clustering algorithms.
</p>
<a href="http://arxiv.org/abs/2012.08740" target="_blank">arXiv:2012.08740</a> [<a href="http://arxiv.org/pdf/2012.08740" target="_blank">pdf</a>]

<h2>Provable Benefits of Overparameterization in Model Compression: From Double Descent to Pruning Neural Networks. (arXiv:2012.08749v1 [cs.LG])</h2>
<h3>Xiangyu Chang, Yingcong Li, Samet Oymak, Christos Thrampoulidis</h3>
<p>Deep networks are typically trained with many more parameters than the size
of the training dataset. Recent empirical evidence indicates that the practice
of overparameterization not only benefits training large models, but also
assists - perhaps counterintuitively - building lightweight models.
Specifically, it suggests that overparameterization benefits model pruning /
sparsification. This paper sheds light on these empirical findings by
theoretically characterizing the high-dimensional asymptotics of model pruning
in the overparameterized regime. The theory presented addresses the following
core question: "should one train a small model from the beginning, or first
train a large model and then prune?". We analytically identify regimes in
which, even if the location of the most informative features is known, we are
better off fitting a large model and then pruning rather than simply training
with the known informative features. This leads to a new double descent in the
training of sparse models: growing the original model, while preserving the
target sparsity, improves the test accuracy as one moves beyond the
overparameterization threshold. Our analysis further reveals the benefit of
retraining by relating it to feature correlations. We find that the above
phenomena are already present in linear and random-features models. Our
technical approach advances the toolset of high-dimensional analysis and
precisely characterizes the asymptotic distribution of over-parameterized
least-squares. The intuition gained by analytically studying simpler models is
numerically verified on neural networks.
</p>
<a href="http://arxiv.org/abs/2012.08749" target="_blank">arXiv:2012.08749</a> [<a href="http://arxiv.org/pdf/2012.08749" target="_blank">pdf</a>]

<h2>Difficulty in estimating visual information from randomly sampled images. (arXiv:2012.08751v1 [cs.CV])</h2>
<h3>Masaki Kitayama, Hitoshi Kiya</h3>
<p>In this paper, we evaluate dimensionality reduction methods in terms of
difficulty in estimating visual information on original images from
dimensionally reduced ones. Recently, dimensionality reduction has been
receiving attention as the process of not only reducing the number of random
variables, but also protecting visual information for privacy-preserving
machine learning. For such a reason, difficulty in estimating visual
information is discussed. In particular, the random sampling method that was
proposed for privacy-preserving machine learning, is compared with typical
dimensionality reduction methods. In an image classification experiment, the
random sampling method is demonstrated not only to have high difficulty, but
also to be comparable to other dimensionality reduction methods, while
maintaining the property of spatial information invariant.
</p>
<a href="http://arxiv.org/abs/2012.08751" target="_blank">arXiv:2012.08751</a> [<a href="http://arxiv.org/pdf/2012.08751" target="_blank">pdf</a>]

<h2>Graph Neural Networks: Taxonomy, Advances and Trends. (arXiv:2012.08752v1 [cs.LG])</h2>
<h3>Yu Zhou, Haixia Zheng, Xin Huang</h3>
<p>Graph neural networks provide a powerful toolkit for embedding real-world
graphs into low-dimensional spaces according to specific tasks. Up to now,
there have been several surveys on this topic. However, they usually lay
emphasis on different angles so that the readers can not see a panorama of the
graph neural networks. This survey aims to overcome this limitation, and
provide a comprehensive review on the graph neural networks. First of all, we
provide a novel taxonomy for the graph neural networks, and then refer to up to
400 relevant literatures to show the panorama of the graph neural networks. All
of them are classified into the corresponding categories. In order to drive the
graph neural networks into a new stage, we summarize four future research
directions so as to overcome the facing challenges. It is expected that more
and more scholars can understand and exploit the graph neural networks, and use
them in their research community.
</p>
<a href="http://arxiv.org/abs/2012.08752" target="_blank">arXiv:2012.08752</a> [<a href="http://arxiv.org/pdf/2012.08752" target="_blank">pdf</a>]

<h2>Revisiting 3D Context Modeling with Supervised Pre-training for Universal Lesion Detection in CT Slices. (arXiv:2012.08770v1 [cs.CV])</h2>
<h3>Shu Zhang, Jincheng Xu, Yu-Chun Chen, Jiechao Ma, Zihao Li, Yizhou Wang, Yizhou Yu</h3>
<p>Universal lesion detection from computed tomography (CT) slices is important
for comprehensive disease screening. Since each lesion can locate in multiple
adjacent slices, 3D context modeling is of great significance for developing
automated lesion detection algorithms. In this work, we propose a Modified
Pseudo-3D Feature Pyramid Network (MP3D FPN) that leverages depthwise separable
convolutional filters and a group transform module (GTM) to efficiently extract
3D context enhanced 2D features for universal lesion detection in CT slices. To
facilitate faster convergence, a novel 3D network pre-training method is
derived using solely large-scale 2D object detection dataset in the natural
image domain. We demonstrate that with the novel pre-training method, the
proposed MP3D FPN achieves state-of-the-art detection performance on the
DeepLesion dataset (3.48% absolute improvement in the sensitivity of FPs@0.5),
significantly surpassing the baseline method by up to 6.06% (in MAP@0.5) which
adopts 2D convolution for 3D context modeling. Moreover, the proposed 3D
pre-trained weights can potentially be used to boost the performance of other
3D medical image analysis tasks.
</p>
<a href="http://arxiv.org/abs/2012.08770" target="_blank">arXiv:2012.08770</a> [<a href="http://arxiv.org/pdf/2012.08770" target="_blank">pdf</a>]

<h2>Analysing the Direction of Emotional Influence in Nonverbal Dyadic Communication: A Facial-Expression Study. (arXiv:2012.08780v1 [cs.CV])</h2>
<h3>Maha Shadaydeh, Lea Mueller, Dana Schneider, Martin Thuemmel, Thomas Kessler, Joachim Denzler</h3>
<p>Identifying the direction of emotional influence in a dyadic dialogue is of
increasing interest in the psychological sciences with applications in
psychotherapy, analysis of political interactions, or interpersonal conflict
behavior. Facial expressions are widely described as being automatic and thus
hard to overtly influence. As such, they are a perfect measure for a better
understanding of unintentional behavior cues about social-emotional cognitive
processes. With this view, this study is concerned with the analysis of the
direction of emotional influence in dyadic dialogue based on facial expressions
only. We exploit computer vision capabilities along with causal inference
theory for quantitative verification of hypotheses on the direction of
emotional influence, i.e., causal effect relationships, in dyadic dialogues. We
address two main issues. First, in a dyadic dialogue, emotional influence
occurs over transient time intervals and with intensity and direction that are
variant over time. To this end, we propose a relevant interval selection
approach that we use prior to causal inference to identify those transient
intervals where causal inference should be applied. Second, we propose to use
fine-grained facial expressions that are present when strong distinct facial
emotions are not visible. To specify the direction of influence, we apply the
concept of Granger causality to the time series of facial expressions over
selected relevant intervals. We tested our approach on newly, experimentally
obtained data. Based on the quantitative verification of hypotheses on the
direction of emotional influence, we were able to show that the proposed
approach is most promising to reveal the causal effect pattern in various
instructed interaction conditions.
</p>
<a href="http://arxiv.org/abs/2012.08780" target="_blank">arXiv:2012.08780</a> [<a href="http://arxiv.org/pdf/2012.08780" target="_blank">pdf</a>]

<h2>On $O( \max \{n_1, n_2 \}\log ( \max \{ n_1, n_2 \} n_3) )$ Sample Entries for $n_1 \times n_2 \times n_3$ Tensor Completion via Unitary Transformation. (arXiv:2012.08784v1 [stat.ML])</h2>
<h3>Guang-Jing Song, Michael K. Ng, Xiongjun Zhang</h3>
<p>One of the key problems in tensor completion is the number of uniformly
random sample entries required for recovery guarantee. The main aim of this
paper is to study $n_1 \times n_2 \times n_3$ third-order tensor completion and
investigate into incoherence conditions of $n_3$ low-rank $n_1$-by-$n_2$ matrix
slices under the transformed tensor singular value decomposition where the
unitary transformation is applied along $n_3$-dimension. We show that such
low-rank tensors can be recovered exactly with high probability when the number
of randomly observed entries is of order $O( r\max \{n_1, n_2 \} \log ( \max \{
n_1, n_2 \} n_3))$, where $r$ is the sum of the ranks of these $n_3$ matrix
slices in the transformed tensor. By utilizing synthetic data and imaging data
sets, we demonstrate that the theoretical result can be obtained under valid
incoherence conditions, and the tensor completion performance of the proposed
method is also better than that of existing methods in terms of sample sizes
requirement.
</p>
<a href="http://arxiv.org/abs/2012.08784" target="_blank">arXiv:2012.08784</a> [<a href="http://arxiv.org/pdf/2012.08784" target="_blank">pdf</a>]

<h2>MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. (arXiv:2012.08791v1 [cs.LG])</h2>
<h3>Angus Dempster, Daniel F. Schmidt, Geoffrey I. Webb</h3>
<p>Until recently, the most accurate methods for time series classification were
limited by high computational complexity. ROCKET achieves state-of-the-art
accuracy with a fraction of the computational expense of most existing methods
by transforming input time series using random convolutional kernels, and using
the transformed features to train a linear classifier. We reformulate ROCKET
into a new method, MINIROCKET, making it up to 75 times faster on larger
datasets, and making it almost deterministic (and optionally, with additional
computational expense, fully deterministic), while maintaining essentially the
same accuracy. Using this method, it is possible to train and test a classifier
on all of 109 datasets from the UCR archive to state-of-the-art accuracy in
less than 10 minutes. MINIROCKET is significantly faster than any other method
of comparable accuracy (including ROCKET), and significantly more accurate than
any other method of even roughly-similar computational expense. As such, we
suggest that MINIROCKET should now be considered and used as the default
variant of ROCKET.
</p>
<a href="http://arxiv.org/abs/2012.08791" target="_blank">arXiv:2012.08791</a> [<a href="http://arxiv.org/pdf/2012.08791" target="_blank">pdf</a>]

<h2>Study on the Large Batch Size Training of Neural Networks Based on the Second Order Gradient. (arXiv:2012.08795v1 [cs.LG])</h2>
<h3>Fengli Gao, Huicai Zhong</h3>
<p>Large batch size training in deep neural networks (DNNs) possesses a
well-known 'generalization gap' that remarkably induces generalization
performance degradation. However, it remains unclear how varying batch size
affects the structure of a NN. Here, we combine theory with experiments to
explore the evolution of the basic structural properties, including gradient,
parameter update step length, and loss update step length of NNs under varying
batch sizes. We provide new guidance to improve generalization, which is
further verified by two designed methods involving discarding small-loss
samples and scheduling batch size. A curvature-based learning rate (CBLR)
algorithm is proposed to better fit the curvature variation, a sensitive factor
affecting large batch size training, across layers in a NN. As an approximation
of CBLR, the median-curvature LR (MCLR) algorithm is found to gain comparable
performance to Layer-wise Adaptive Rate Scaling (LARS) algorithm. Our
theoretical results and algorithm offer geometry-based explanations to the
existing studies. Furthermore, we demonstrate that the layer wise LR
algorithms, for example LARS, can be regarded as special instances of CBLR.
Finally, we deduce a theoretical geometric picture of large batch size
training, and show that all the network parameters tend to center on their
related minima.
</p>
<a href="http://arxiv.org/abs/2012.08795" target="_blank">arXiv:2012.08795</a> [<a href="http://arxiv.org/pdf/2012.08795" target="_blank">pdf</a>]

<h2>Latent Space Conditioning on Generative Adversarial Networks. (arXiv:2012.08803v1 [cs.CV])</h2>
<h3>Ricard Durall, Kalun Ho, Franz-Josef Pfreundt, Janis Keuper</h3>
<p>Generative adversarial networks are the state of the art approach towards
learned synthetic image generation. Although early successes were mostly
unsupervised, bit by bit, this trend has been superseded by approaches based on
labelled data. These supervised methods allow a much finer-grained control of
the output image, offering more flexibility and stability. Nevertheless, the
main drawback of such models is the necessity of annotated data. In this work,
we introduce an novel framework that benefits from two popular learning
techniques, adversarial training and representation learning, and takes a step
towards unsupervised conditional GANs. In particular, our approach exploits the
structure of a latent space (learned by the representation learning) and
employs it to condition the generative model. In this way, we break the
traditional dependency between condition and label, substituting the latter by
unsupervised features coming from the latent space. Finally, we show that this
new technique is able to produce samples on demand keeping the quality of its
supervised counterpart.
</p>
<a href="http://arxiv.org/abs/2012.08803" target="_blank">arXiv:2012.08803</a> [<a href="http://arxiv.org/pdf/2012.08803" target="_blank">pdf</a>]

<h2>Temporal Graph Modeling for Skeleton-based Action Recognition. (arXiv:2012.08804v1 [cs.CV])</h2>
<h3>Jianan Li, Xuemei Xie, Zhifu Zhao, Yuhan Cao, Qingzhe Pan, Guangming Shi</h3>
<p>Graph Convolutional Networks (GCNs), which model skeleton data as graphs,
have obtained remarkable performance for skeleton-based action recognition.
Particularly, the temporal dynamic of skeleton sequence conveys significant
information in the recognition task. For temporal dynamic modeling, GCN-based
methods only stack multi-layer 1D local convolutions to extract temporal
relations between adjacent time steps. With the repeat of a lot of local
convolutions, the key temporal information with non-adjacent temporal distance
may be ignored due to the information dilution. Therefore, these methods still
remain unclear how to fully explore temporal dynamic of skeleton sequence. In
this paper, we propose a Temporal Enhanced Graph Convolutional Network (TE-GCN)
to tackle this limitation. The proposed TE-GCN constructs temporal relation
graph to capture complex temporal dynamic. Specifically, the constructed
temporal relation graph explicitly builds connections between semantically
related temporal features to model temporal relations between both adjacent and
non-adjacent time steps. Meanwhile, to further explore the sufficient temporal
dynamic, multi-head mechanism is designed to investigate multi-kinds of
temporal relations. Extensive experiments are performed on two widely used
large-scale datasets, NTU-60 RGB+D and NTU-120 RGB+D. And experimental results
show that the proposed model achieves the state-of-the-art performance by
making contribution to temporal modeling for action recognition.
</p>
<a href="http://arxiv.org/abs/2012.08804" target="_blank">arXiv:2012.08804</a> [<a href="http://arxiv.org/pdf/2012.08804" target="_blank">pdf</a>]

<h2>More Industry-friendly: Federated Learning with High Efficient Design. (arXiv:2012.08809v1 [cs.LG])</h2>
<h3>Dingwei Li, Qinglong Chang, Lixue Pang, Yanfang Zhang, Xudong Sun, Jikun Ding, Liang Zhang</h3>
<p>Although many achievements have been made since Google threw out the paradigm
of federated learning (FL), there still exists much room for researchers to
optimize its efficiency. In this paper, we propose a high efficient FL method
equipped with the double head design aiming for personalization optimization
over non-IID dataset, and the gradual model sharing design for communication
saving. Experimental results show that, our method has more stable accuracy
performance and better communication efficient across various data
distributions than other state of art methods (SOTAs), makes it more
industry-friendly.
</p>
<a href="http://arxiv.org/abs/2012.08809" target="_blank">arXiv:2012.08809</a> [<a href="http://arxiv.org/pdf/2012.08809" target="_blank">pdf</a>]

<h2>Continuous Gesture Recognition from sEMG Sensor Data with Recurrent Neural Networks and Adversarial Domain Adaptation. (arXiv:2012.08816v1 [cs.LG])</h2>
<h3>Ivan Sosin, Daniel Kudenko, Aleksei Shpilman</h3>
<p>Movement control of artificial limbs has made big advances in recent years.
New sensor and control technology enhanced the functionality and usefulness of
artificial limbs to the point that complex movements, such as grasping, can be
performed to a limited extent. To date, the most successful results were
achieved by applying recurrent neural networks (RNNs). However, in the domain
of artificial hands, experiments so far were limited to non-mobile wrists,
which significantly reduces the functionality of such prostheses. In this
paper, for the first time, we present empirical results on gesture recognition
with both mobile and non-mobile wrists. Furthermore, we demonstrate that
recurrent neural networks with simple recurrent units (SRU) outperform regular
RNNs in both cases in terms of gesture recognition accuracy, on data acquired
by an arm band sensing electromagnetic signals from arm muscles (via surface
electromyography or sEMG). Finally, we show that adding domain adaptation
techniques to continuous gesture recognition with RNN improves the transfer
ability between subjects, where a limb controller trained on data from one
person is used for another person.
</p>
<a href="http://arxiv.org/abs/2012.08816" target="_blank">arXiv:2012.08816</a> [<a href="http://arxiv.org/pdf/2012.08816" target="_blank">pdf</a>]

<h2>A comparative evaluation of machine learning methods for robot navigation through human crowds. (arXiv:2012.08822v1 [cs.RO])</h2>
<h3>Anastasia Gaydashenko, Daniel Kudenko, Aleksei Shpilman</h3>
<p>Robot navigation through crowds poses a difficult challenge to AI systems,
since the methods should result in fast and efficient movement but at the same
time are not allowed to compromise safety. Most approaches to date were focused
on the combination of pathfinding algorithms with machine learning for
pedestrian walking prediction. More recently, reinforcement learning techniques
have been proposed in the research literature. In this paper, we perform a
comparative evaluation of pathfinding/prediction and reinforcement learning
approaches on a crowd movement dataset collected from surveillance videos taken
at Grand Central Station in New York. The results demonstrate the strong
superiority of state-of-the-art reinforcement learning approaches over
pathfinding with state-of-the-art behaviour prediction techniques.
</p>
<a href="http://arxiv.org/abs/2012.08822" target="_blank">arXiv:2012.08822</a> [<a href="http://arxiv.org/pdf/2012.08822" target="_blank">pdf</a>]

<h2>Learning to Run with Potential-Based Reward Shaping and Demonstrations from Video Data. (arXiv:2012.08824v1 [cs.LG])</h2>
<h3>Aleksandra Malysheva, Daniel Kudenko, Aleksei Shpilman</h3>
<p>Learning to produce efficient movement behaviour for humanoid robots from
scratch is a hard problem, as has been illustrated by the "Learning to run"
competition at NIPS 2017. The goal of this competition was to train a
two-legged model of a humanoid body to run in a simulated race course with
maximum speed. All submissions took a tabula rasa approach to reinforcement
learning (RL) and were able to produce relatively fast, but not optimal running
behaviour. In this paper, we demonstrate how data from videos of human running
(e.g. taken from YouTube) can be used to shape the reward of the humanoid
learning agent to speed up the learning and produce a better result.
Specifically, we are using the positions of key body parts at regular time
intervals to define a potential function for potential-based reward shaping
(PBRS). Since PBRS does not change the optimal policy, this approach allows the
RL agent to overcome sub-optimalities in the human movements that are shown in
the videos.

We present experiments in which we combine selected techniques from the top
ten approaches from the NIPS competition with further optimizations to create
an high-performing agent as a baseline. We then demonstrate how video-based
reward shaping improves the performance further, resulting in an RL agent that
runs twice as fast as the baseline in 12 hours of training. We furthermore show
that our approach can overcome sub-optimal running behaviour in videos, with
the learned policy significantly outperforming that of the running agent from
the video.
</p>
<a href="http://arxiv.org/abs/2012.08824" target="_blank">arXiv:2012.08824</a> [<a href="http://arxiv.org/pdf/2012.08824" target="_blank">pdf</a>]

<h2>Using noise resilience for ranking generalization of deep neural networks. (arXiv:2012.08854v1 [cs.LG])</h2>
<h3>Depen Morwani, Rahul Vashisht, Harish G. Ramaswamy</h3>
<p>Recent papers have shown that sufficiently overparameterized neural networks
can perfectly fit even random labels. Thus, it is crucial to understand the
underlying reason behind the generalization performance of a network on
real-world data. In this work, we propose several measures to predict the
generalization error of a network given the training data and its parameters.
Using one of these measures, based on noise resilience of the network, we
secured 5th position in the predicting generalization in deep learning (PGDL)
competition at NeurIPS 2020.
</p>
<a href="http://arxiv.org/abs/2012.08854" target="_blank">arXiv:2012.08854</a> [<a href="http://arxiv.org/pdf/2012.08854" target="_blank">pdf</a>]

<h2>Time-Aware Tensor Decomposition for Missing Entry Prediction. (arXiv:2012.08855v1 [cs.LG])</h2>
<h3>Dawon Ahn, Jun-Gi Jang, U Kang</h3>
<p>Given a time-evolving tensor with missing entries, how can we effectively
factorize it for precisely predicting the missing entries? Tensor factorization
has been extensively utilized for analyzing various multi-dimensional
real-world data. However, existing models for tensor factorization have
disregarded the temporal property for tensor factorization while most
real-world data are closely related to time. Moreover, they do not address
accuracy degradation due to the sparsity of time slices. The essential problems
of how to exploit the temporal property for tensor decomposition and consider
the sparsity of time slices remain unresolved. In this paper, we propose TATD
(Time-Aware Tensor Decomposition), a novel tensor decomposition method for
real-world temporal tensors. TATD is designed to exploit temporal dependency
and time-varying sparsity of real-world temporal tensors. We propose a new
smoothing regularization with Gaussian kernel for modeling time dependency.
Moreover, we improve the performance of TATD by considering time-varying
sparsity. We design an alternating optimization scheme suitable for temporal
tensor factorization with our smoothing regularization. Extensive experiments
show that TATD provides the state-of-the-art accuracy for decomposing temporal
tensors.
</p>
<a href="http://arxiv.org/abs/2012.08855" target="_blank">arXiv:2012.08855</a> [<a href="http://arxiv.org/pdf/2012.08855" target="_blank">pdf</a>]

<h2>L\'evy walks derived from a Bayesian decision-making model in non-stationary environments. (arXiv:2012.08858v1 [cs.AI])</h2>
<h3>Shuji Shinohara, Nobuhito Manome, Yoshihiro Nakajima, Yukio Pegio Gunji, Toru Moriyama, Hiroshi Okamoto, Shunji Mitsuyoshi, Ung-il Chung</h3>
<p>L\'evy walks are found in the migratory behaviour patterns of various
organisms, and the reason for this phenomenon has been much discussed. We use
simulations to demonstrate that learning causes the changes in confidence level
during decision-making in non-stationary environments, and results in
L\'evy-walk-like patterns. One inference algorithm involving confidence is
Bayesian inference. We propose an algorithm that introduces the effects of
learning and forgetting into Bayesian inference, and simulate an imitation game
in which two decision-making agents incorporating the algorithm estimate each
other's internal models from their opponent's observational data. For
forgetting without learning, agent confidence levels remained low due to a lack
of information on the counterpart and Brownian walks occurred for a wide range
of forgetting rates. Conversely, when learning was introduced, high confidence
levels occasionally occurred even at high forgetting rates, and Brownian walks
universally became L\'evy walks through a mixture of high- and low-confidence
states.
</p>
<a href="http://arxiv.org/abs/2012.08858" target="_blank">arXiv:2012.08858</a> [<a href="http://arxiv.org/pdf/2012.08858" target="_blank">pdf</a>]

<h2>Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces. (arXiv:2012.08859v1 [cs.LG])</h2>
<h3>Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mariani, Dushyant Mehta, Chris Lott, Tijmen Blankevoort</h3>
<p>This work presents DONNA (Distilling Optimal Neural Network Architectures), a
novel pipeline for rapid neural architecture search and search space
exploration, targeting multiple different hardware platforms and user
scenarios. In DONNA, a search consists of three phases. First, an accuracy
predictor is built for a diverse search space using blockwise knowledge
distillation. This predictor enables searching across diverse
macro-architectural network parameters such as layer types, attention
mechanisms, and channel widths, as well as across micro-architectural
parameters such as block repeats, kernel sizes, and expansion rates. Second, a
rapid evolutionary search phase finds a Pareto-optimal set of architectures in
terms of accuracy and latency for any scenario using the predictor and
on-device measurements. Third, Pareto-optimal models can be quickly finetuned
to full accuracy. With this approach, DONNA finds architectures that outperform
the state of the art. In ImageNet classification, architectures found by DONNA
are 20% faster than EfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU at
similar accuracy and 10% faster with 0.5% higher accuracy than MobileNetV2-1.4x
on a Samsung S20 smartphone. In addition to neural architecture search, DONNA
is used for search-space exploration and hardware-aware model compression.
</p>
<a href="http://arxiv.org/abs/2012.08859" target="_blank">arXiv:2012.08859</a> [<a href="http://arxiv.org/pdf/2012.08859" target="_blank">pdf</a>]

<h2>On The Verification of Neural ODEs with Stochastic Guarantees. (arXiv:2012.08863v1 [cs.LG])</h2>
<h3>Sophie Gruenbacher, Ramin Hasani, Mathias Lechner, Jacek Cyranka, Scott A. Smolka, Radu Grosu</h3>
<p>We show that Neural ODEs, an emerging class of time-continuous neural
networks, can be verified by solving a set of global-optimization problems. For
this purpose, we introduce Stochastic Lagrangian Reachability (SLR), an
abstraction-based technique for constructing a tight Reachtube (an
over-approximation of the set of reachable states over a given time-horizon),
and provide stochastic guarantees in the form of confidence intervals for the
Reachtube bounds. SLR inherently avoids the infamous wrapping effect
(accumulation of over-approximation errors) by performing local optimization
steps to expand safe regions instead of repeatedly forward-propagating them as
is done by deterministic reachability methods. To enable fast local
optimizations, we introduce a novel forward-mode adjoint sensitivity method to
compute gradients without the need for backpropagation. Finally, we establish
asymptotic and non-asymptotic convergence rates for SLR.
</p>
<a href="http://arxiv.org/abs/2012.08863" target="_blank">arXiv:2012.08863</a> [<a href="http://arxiv.org/pdf/2012.08863" target="_blank">pdf</a>]

<h2>Using Spatio-temporal Deep Learning for Forecasting Demand and Supply-demand Gap in Ride-hailing System with Anonymized Spatial Adjacency Information. (arXiv:2012.08868v1 [cs.LG])</h2>
<h3>M. H. Rahman, S. M. Rifaat</h3>
<p>To reduce passenger waiting time and driver search friction, ride-hailing
companies need to accurately forecast spatio-temporal demand and supply-demand
gap. However, due to spatio-temporal dependencies pertaining to demand and
supply-demand gap in a ride-hailing system, making accurate forecasts for both
demand and supply-demand gap is a difficult task. Furthermore, due to
confidentiality and privacy issues, ride-hailing data are sometimes released to
the researchers by removing spatial adjacency information of the zones, which
hinders the detection of spatio-temporal dependencies. To that end, a novel
spatio-temporal deep learning architecture is proposed in this paper for
forecasting demand and supply-demand gap in a ride-hailing system with
anonymized spatial adjacency information, which integrates feature importance
layer with a spatio-temporal deep learning architecture containing
one-dimensional convolutional neural network (CNN) and zone-distributed
independently recurrent neural network (IndRNN). The developed architecture is
tested with real-world datasets of Didi Chuxing, which shows that our models
based on the proposed architecture can outperform conventional time-series
models (e.g., ARIMA) and machine learning models (e.g., gradient boosting
machine, distributed random forest, generalized linear model, artificial neural
network). Additionally, the feature importance layer provides an interpretation
of the model by revealing the contribution of the input features utilized in
prediction.
</p>
<a href="http://arxiv.org/abs/2012.08868" target="_blank">arXiv:2012.08868</a> [<a href="http://arxiv.org/pdf/2012.08868" target="_blank">pdf</a>]

<h2>Solving the Travelling Thief Problem based on Item Selection Weight and Reverse Order Allocation. (arXiv:2012.08888v1 [cs.AI])</h2>
<h3>Lei Yang, Zitong Zhang, Xiaotian Jia, Peipei Kang, Wensheng Zhang, Dongya Wang</h3>
<p>The Travelling Thief Problem (TTP) is a challenging combinatorial
optimization problem that attracts many scholars. The TTP interconnects two
well-known NP-hard problems: the Travelling Salesman Problem (TSP) and the 0-1
Knapsack Problem (KP). Increasingly algorithms have been proposed for solving
this novel problem that combines two interdependent sub-problems. In this
paper, TTP is investigated theoretically and empirically. An algorithm based on
the score value calculated by our proposed formulation in picking items and
sorting items in the reverse order in the light of the scoring value is
proposed to solve the problem. Different approaches for solving the TTP are
compared and analyzed; the experimental investigations suggest that our
proposed approach is very efficient in meeting or beating current
state-of-the-art heuristic solutions on a comprehensive set of benchmark TTP
instances.
</p>
<a href="http://arxiv.org/abs/2012.08888" target="_blank">arXiv:2012.08888</a> [<a href="http://arxiv.org/pdf/2012.08888" target="_blank">pdf</a>]

<h2>Self-Supervised Person Detection in 2D Range Data using a Calibrated Camera. (arXiv:2012.08890v1 [cs.CV])</h2>
<h3>Dan Jia, Mats Steinweg, Alexander Hermans, Bastian Leibe</h3>
<p>Deep learning is the essential building block of state-of-the-art person
detectors in 2D range data. However, only a few annotated datasets are
available for training and testing these deep networks, potentially limiting
their performance when deployed in new environments or with different LiDAR
models. We propose a method, which uses bounding boxes from an image-based
detector (e.g. Faster R-CNN) on a calibrated camera to automatically generate
training labels (called pseudo-labels) for 2D LiDAR-based person detectors.
Through experiments on the JackRabbot dataset with two detector models, DROW3
and DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned
with pseudo-labels, outperform detectors trained using manual annotations from
a different dataset. Combined with robust training techniques, the
self-supervised detectors reach a performance close to the ones trained using
manual annotations. Our method is an effective way to improve person detectors
during deployment without any additional labeling effort, and we release our
source code to support relevant robotic applications.
</p>
<a href="http://arxiv.org/abs/2012.08890" target="_blank">arXiv:2012.08890</a> [<a href="http://arxiv.org/pdf/2012.08890" target="_blank">pdf</a>]

<h2>EffMoP: Efficient Motion Planning Based on Heuristic-Guided Motion Primitives Pruning and Path Optimization With Sparse-Banded Structure. (arXiv:2012.08892v1 [cs.RO])</h2>
<h3>Jian Wen, Xuebo Zhang, Haiming Gao, Jing Yuan, Yongchun Fang</h3>
<p>To solve the autonomous navigation problem in complex environments, an
efficient motion planning approach called EffMoP is presented in this paper.
Considering the challenges from large-scale, partially unknown complex
environments, a three-layer motion planning framework is elaborately designed,
including global path planning, local path optimization, and time-optimal
velocity planning. Compared with existing approaches, the novelty of this work
is twofold: 1) a heuristic-guided pruning strategy of motion primitives is
newly designed and fully integrated into the search-based global path planner
to improve the computational efficiency of graph search, and 2) a novel
soft-constrained local path optimization approach is proposed, wherein the
sparse-banded system structure of the underlying optimization problem is fully
exploited to efficiently solve the problem. We validate the safety, smoothness,
flexibility, and efficiency of EffMoP in various complex simulation scenarios
and challenging real-world tasks. It is shown that the computational efficiency
is improved by 66.21% in the global planning stage and the motion efficiency of
the robot is improved by 22.87% compared with the recent quintic B\'{e}zier
curve-based state space sampling approach.
</p>
<a href="http://arxiv.org/abs/2012.08892" target="_blank">arXiv:2012.08892</a> [<a href="http://arxiv.org/pdf/2012.08892" target="_blank">pdf</a>]

<h2>ReINTEL: A Multimodal Data Challenge for Responsible Information Identification on Social Network Sites. (arXiv:2012.08895v1 [cs.LG])</h2>
<h3>Duc-Trong Le, Xuan-Son Vu, Nhu-Dung To, Huu-Quang Nguyen, Thuy-Trinh Nguyen, Linh Le, Anh-Tuan Nguyen, Minh-Duc Hoang, Nghia Le, Huyen Nguyen, Hoang D. Nguyen</h3>
<p>This paper reports on the ReINTEL Shared Task for Responsible Information
Identification on social network sites, which is hosted at the seventh annual
workshop on Vietnamese Language and Speech Processing (VLSP 2020). Given a
piece of news with respective textual, visual content and metadata,
participants are required to classify whether the news is `reliable' or
`unreliable'. In order to generate a fair benchmark, we introduce a novel
human-annotated dataset of over 10,000 news collected from a social network in
Vietnam. All models will be evaluated in terms of AUC-ROC score, a typical
evaluation metric for classification. The competition was run on the Codalab
platform. Within two months, the challenge has attracted over 60 participants
and recorded nearly 1,000 submission entries.
</p>
<a href="http://arxiv.org/abs/2012.08895" target="_blank">arXiv:2012.08895</a> [<a href="http://arxiv.org/pdf/2012.08895" target="_blank">pdf</a>]

<h2>A connection between the pattern classification problem and the General Linear Model for statistical inference. (arXiv:2012.08903v1 [stat.ML])</h2>
<h3>Juan Manuel Gorriz, SIPBA group, John Suckling</h3>
<p>A connection between the General Linear Model (GLM) in combination with
classical statistical inference and the machine learning (MLE)-based inference
is described in this paper. Firstly, the estimation of the GLM parameters is
expressed as a Linear Regression Model (LRM) of an indicator matrix, that is,
in terms of the inverse problem of regressing the observations. In other words,
both approaches, i.e. GLM and LRM, apply to different domains, the observation
and the label domains, and are linked by a normalization value at the
least-squares solution. Subsequently, from this relationship we derive a
statistical test based on a more refined predictive algorithm, i.e. the
(non)linear Support Vector Machine (SVM) that maximizes the class margin of
separation, within a permutation analysis. The MLE-based inference employs a
residual score and includes the upper bound to compute a better estimation of
the actual (real) error. Experimental results demonstrate how the parameter
estimations derived from each model resulted in different classification
performances in the equivalent inverse problem. Moreover, using real data the
aforementioned predictive algorithms within permutation tests, including such
model-free estimators, are able to provide a good trade-off between type I
error and statistical power.
</p>
<a href="http://arxiv.org/abs/2012.08903" target="_blank">arXiv:2012.08903</a> [<a href="http://arxiv.org/pdf/2012.08903" target="_blank">pdf</a>]

<h2>Multi-Task Learning in Diffractive Deep Neural Networks via Hardware-Software Co-design. (arXiv:2012.08906v1 [cs.LG])</h2>
<h3>Yingjie Li, Ruiyang Chen, Berardi Sensale Rodriguez, Weilu Gao, Cunxi Yu</h3>
<p>Deep neural networks (DNNs) have substantial computational requirements,
which greatly limit their performance in resource-constrained environments.
Recently, there are increasing efforts on optical neural networks and optical
computing based DNNs hardware, which bring significant advantages for deep
learning systems in terms of their power efficiency, parallelism and
computational speed. Among them, free-space diffractive deep neural networks
(D$^2$NNs) based on the light diffraction, feature millions of neurons in each
layer interconnected with neurons in neighboring layers. However, due to the
challenge of implementing reconfigurability, deploying different DNNs
algorithms requires re-building and duplicating the physical diffractive
systems, which significantly degrades the hardware efficiency in practical
application scenarios. Thus, this work proposes a novel hardware-software
co-design method that enables robust and noise-resilient Multi-task Learning in
D$^2$2NNs. Our experimental results demonstrate significant improvements in
versatility and hardware efficiency, and also demonstrate the robustness of
proposed multi-task D$^2$NN architecture under wide noise ranges of all system
components. In addition, we propose a domain-specific regularization algorithm
for training the proposed multi-task architecture, which can be used to
flexibly adjust the desired performance for each task.
</p>
<a href="http://arxiv.org/abs/2012.08906" target="_blank">arXiv:2012.08906</a> [<a href="http://arxiv.org/pdf/2012.08906" target="_blank">pdf</a>]

<h2>Communicative Message Passing for Inductive Relation Reasoning. (arXiv:2012.08911v1 [cs.AI])</h2>
<h3>Sijie Mai, Shuangjia Zheng, Yuedong Yang, Haifeng Hu</h3>
<p>Relation prediction for knowledge graphs aims at predicting missing
relationships between entities. Despite the importance of inductive relation
prediction, most previous works are limited to a transductive setting and
cannot process previously unseen entities. The recent proposed subgraph-based
relation reasoning models provided alternatives to predict links from the
subgraph structure surrounding a candidate triplet inductively. However, we
observe that these methods often neglect the directed nature of the extracted
subgraph and weaken the role of relation information in the subgraph modeling.
As a result, they fail to effectively handle the asymmetric/anti-symmetric
triplets and produce insufficient embeddings for the target triplets. To this
end, we introduce a \textbf{C}\textbf{o}mmunicative \textbf{M}essage
\textbf{P}assing neural network for \textbf{I}nductive re\textbf{L}ation
r\textbf{E}asoning, \textbf{CoMPILE}, that reasons over local directed subgraph
structures and has a vigorous inductive bias to process entity-independent
semantic relations. In contrast to existing models, CoMPILE strengthens the
message interactions between edges and entitles through a communicative kernel
and enables a sufficient flow of relation information. Moreover, we demonstrate
that CoMPILE can naturally handle asymmetric/anti-symmetric relations without
the need for explosively increasing the number of model parameters by
extracting the directed enclosing subgraphs. Extensive experiments show
substantial performance gains in comparison to state-of-the-art methods on
commonly used benchmark datasets with variant inductive settings.
</p>
<a href="http://arxiv.org/abs/2012.08911" target="_blank">arXiv:2012.08911</a> [<a href="http://arxiv.org/pdf/2012.08911" target="_blank">pdf</a>]

<h2>Clustering Ensemble Meets Low-rank Tensor Approximation. (arXiv:2012.08916v1 [cs.LG])</h2>
<h3>Yuheng Jia, Hui Liu, Junhui Hou, Qingfu Zhang</h3>
<p>This paper explores the problem of clustering ensemble, which aims to combine
multiple base clusterings to produce better performance than that of the
individual one. The existing clustering ensemble methods generally construct a
co-association matrix, which indicates the pairwise similarity between samples,
as the weighted linear combination of the connective matrices from different
base clusterings, and the resulting co-association matrix is then adopted as
the input of an off-the-shelf clustering algorithm, e.g., spectral clustering.
However, the co-association matrix may be dominated by poor base clusterings,
resulting in inferior performance. In this paper, we propose a novel low-rank
tensor approximation-based method to solve the problem from a global
perspective. Specifically, by inspecting whether two samples are clustered to
an identical cluster under different base clusterings, we derive a
coherent-link matrix, which contains limited but highly reliable relationships
between samples. We then stack the coherent-link matrix and the co-association
matrix to form a three-dimensional tensor, the low-rankness property of which
is further explored to propagate the information of the coherent-link matrix to
the co-association matrix, producing a refined co-association matrix. We
formulate the proposed method as a convex constrained optimization problem and
solve it efficiently. Experimental results over 7 benchmark data sets show that
the proposed model achieves a breakthrough in clustering performance, compared
with 12 state-of-the-art methods. To the best of our knowledge, this is the
first work to explore the potential of low-rank tensor on clustering ensemble,
which is fundamentally different from previous approaches.
</p>
<a href="http://arxiv.org/abs/2012.08916" target="_blank">arXiv:2012.08916</a> [<a href="http://arxiv.org/pdf/2012.08916" target="_blank">pdf</a>]

<h2>Unsupervised Image Segmentation using Mutual Mean-Teaching. (arXiv:2012.08922v1 [cs.CV])</h2>
<h3>Zhichao Wu, Lei Guo, Hao Zhang, Dan Xu</h3>
<p>Unsupervised image segmentation aims at assigning the pixels with similar
feature into a same cluster without annotation, which is an important task in
computer vision. Due to lack of prior knowledge, most of existing model usually
need to be trained several times to obtain suitable results. To address this
problem, we propose an unsupervised image segmentation model based on the
Mutual Mean-Teaching (MMT) framework to produce more stable results. In
addition, since the labels of pixels from two model are not matched, a label
alignment algorithm based on the Hungarian algorithm is proposed to match the
cluster labels. Experimental results demonstrate that the proposed model is
able to segment various types of images and achieves better performance than
the existing methods.
</p>
<a href="http://arxiv.org/abs/2012.08922" target="_blank">arXiv:2012.08922</a> [<a href="http://arxiv.org/pdf/2012.08922" target="_blank">pdf</a>]

<h2>FuseVis: Interpreting neural networks for image fusion using per-pixel saliency visualization. (arXiv:2012.08932v1 [cs.CV])</h2>
<h3>Nishant Kumar, Stefan Gumhold</h3>
<p>Image fusion helps in merging two or more images to construct a more
informative single fused image. Recently, unsupervised learning based
convolutional neural networks (CNN) have been utilized for different types of
image fusion tasks such as medical image fusion, infrared-visible image fusion
for autonomous driving as well as multi-focus and multi-exposure image fusion
for satellite imagery. However, it is challenging to analyze the reliability of
these CNNs for the image fusion tasks since no groundtruth is available. This
led to the use of a wide variety of model architectures and optimization
functions yielding quite different fusion results. Additionally, due to the
highly opaque nature of such neural networks, it is difficult to explain the
internal mechanics behind its fusion results. To overcome these challenges, we
present a novel real-time visualization tool, named FuseVis, with which the
end-user can compute per-pixel saliency maps that examine the influence of the
input image pixels on each pixel of the fused image. We trained several image
fusion based CNNs on medical image pairs and then using our FuseVis tool, we
performed case studies on a specific clinical application by interpreting the
saliency maps from each of the fusion methods. We specifically visualized the
relative influence of each input image on the predictions of the fused image
and showed that some of the evaluated image fusion methods are better suited
for the specific clinical application. To the best of our knowledge, currently,
there is no approach for visual analysis of neural networks for image fusion.
Therefore, this work opens up a new research direction to improve the
interpretability of deep fusion networks. The FuseVis tool can also be adapted
in other deep neural network based image processing applications to make them
interpretable.
</p>
<a href="http://arxiv.org/abs/2012.08932" target="_blank">arXiv:2012.08932</a> [<a href="http://arxiv.org/pdf/2012.08932" target="_blank">pdf</a>]

<h2>Copyspace: Where to Write on Images?. (arXiv:2012.08933v1 [cs.CV])</h2>
<h3>Jessica M. Lundin, Michael Sollami, Brian Lonsdorf, Alan Ross, Owen Schoppe, David Woodward, S&#xf6;nke Rohde</h3>
<p>The placement of text over an image is an important part of producing
high-quality visual designs. Automating this work by determining appropriate
position, orientation, and style for textual elements requires understanding
the contents of the background image. We refer to the search for aesthetic
parameters of text rendered over images as "copyspace detection", noting that
this task is distinct from foreground-background separation. We have developed
solutions using one and two stage object detection methodologies trained on an
expertly labeled data. This workshop will examine such algorithms for copyspace
detection and demonstrate their application in generative design models and
pipelines such as Einstein Designer.
</p>
<a href="http://arxiv.org/abs/2012.08933" target="_blank">arXiv:2012.08933</a> [<a href="http://arxiv.org/pdf/2012.08933" target="_blank">pdf</a>]

<h2>SAfE: Self-Attention Based Unsupervised Road Safety Classification in Hazardous Environments. (arXiv:2012.08939v1 [cs.CV])</h2>
<h3>Divya Kothandaraman, Rohan Chandra, Dinesh Manocha</h3>
<p>We present a novel approach SAfE that can identify parts of an outdoor scene
that are safe for driving, based on attention models. Our formulation is
designed for hazardous weather conditions that can impair the visibility of
human drivers as well as autonomous vehicles, increasing the risk of accidents.
Our approach is unsupervised and uses domain adaptation, with entropy
minimization and attention transfer discriminators, to leverage the large
amounts of labeled data corresponding to clear weather conditions. Our
attention transfer discriminator uses attention maps from the clear weather
image to help the network learn relevant regions to attend to, on the images
from the hazardous weather dataset. We conduct experiments on CityScapes
simulated datasets depicting various weather conditions such as rain, fog and
snow under different intensities, and additionally on Berkeley Deep Drive. Our
result show that using attention models improves the standard unsupervised
domain adaptation performance by 29.29%. Furthermore, we also compare with
unsupervised domain adaptation methods and show an improvement of at least
12.02% (mIoU) over the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2012.08939" target="_blank">arXiv:2012.08939</a> [<a href="http://arxiv.org/pdf/2012.08939" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning of Graph Matching. (arXiv:2012.08950v1 [cs.CV])</h2>
<h3>Chang Liu, Runzhong Wang, Zetian Jiang, Junchi Yan</h3>
<p>Graph matching under node and pairwise constraints has been a building block
in areas from combinatorial optimization, machine learning to computer vision,
for effective structural representation and association. We present a
reinforcement learning solver that seeks the node correspondence between two
graphs, whereby the node embedding model on the association graph is learned to
sequentially find the node-to-node matching. Our method differs from the
previous deep graph matching model in the sense that they are focused on the
front-end feature and affinity function learning while our method aims to learn
the backend decision making given the affinity objective function whatever
obtained by learning or not. Such an objective function maximization setting
naturally fits with the reinforcement learning mechanism, of which the learning
procedure is label-free. Besides, the model is not restricted to a fixed number
of nodes for matching. These features make it more suitable for practical
usage. Extensive experimental results on both synthetic datasets, natural
images, and QAPLIB showcase the superior performance regarding both matching
accuracy and efficiency. To our best knowledge, this is the first deep
reinforcement learning solver for graph matching.
</p>
<a href="http://arxiv.org/abs/2012.08950" target="_blank">arXiv:2012.08950</a> [<a href="http://arxiv.org/pdf/2012.08950" target="_blank">pdf</a>]

<h2>SimuGAN: Unsupervised forward modeling and optimal design of a LIDAR Camera. (arXiv:2012.08951v1 [cs.CV])</h2>
<h3>Nir Diamant, Tal Mund, Ohad Menashe, Aviad Zabatani, Alex M. Bronstein</h3>
<p>Energy-saving LIDAR camera for short distances estimates an object's distance
using temporally intensity-coded laser light pulses and calculates the maximum
correlation with the back-scattered pulse.

Though on low power, the backs-scattered pulse is noisy and unstable, which
leads to inaccurate and unreliable depth estimation.

To address this problem, we use GANs (Generative Adversarial Networks), which
are two neural networks that can learn complicated class distributions through
an adversarial process. We learn the LIDAR camera's hidden properties and
behavior, creating a novel, fully unsupervised forward model that simulates the
camera. Then, we use the model's differentiability to explore the camera
parameter space and optimize those parameters in terms of depth, accuracy, and
stability. To achieve this goal, we also propose a new custom loss function
designated to the back-scattered code distribution's weaknesses and its
circular behavior. The results are demonstrated on both synthetic and real
data.
</p>
<a href="http://arxiv.org/abs/2012.08951" target="_blank">arXiv:2012.08951</a> [<a href="http://arxiv.org/pdf/2012.08951" target="_blank">pdf</a>]

<h2>Personalized Step Counting Using Wearable Sensors: A Domain Adapted LSTM Network Approach. (arXiv:2012.08975v1 [cs.LG])</h2>
<h3>Arvind Pillai, Halsey Lea, Faisal Khan, Glynn Dennis</h3>
<p>Activity monitors are widely used to measure various physical activities (PA)
as an indicator of mobility, fitness and general health. Similarly, real-time
monitoring of longitudinal trends in step count has significant clinical
potential as a personalized measure of disease related changes in daily
activity. However, inconsistent step count accuracy across vendors, body
locations, and individual gait differences limits clinical utility. The
tri-axial accelerometer inside PA monitors can be exploited to improve step
count accuracy across devices and individuals. In this study, we hypothesize:
(1) raw tri-axial sensor data can be modeled to create reliable and accurate
step count, and (2) a generalized step count model can then be efficiently
adapted to each unique gait pattern using very little new data. Firstly,
open-source raw sensor data was used to construct a long short term memory
(LSTM) deep neural network to model step count. Then we generated a new, fully
independent data set using a different device and different subjects. Finally,
a small amount of subject-specific data was domain adapted to produce
personalized models with high individualized step count accuracy. These results
suggest models trained using large freely available datasets can be adapted to
patient populations where large historical data sets are rare.
</p>
<a href="http://arxiv.org/abs/2012.08975" target="_blank">arXiv:2012.08975</a> [<a href="http://arxiv.org/pdf/2012.08975" target="_blank">pdf</a>]

<h2>C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal Consistent Motion Transfer. (arXiv:2012.08976v1 [cs.CV])</h2>
<h3>Dongxu Wei, Xiaowei Xu, Haibin Shen, Kejie Huang</h3>
<p>Human video motion transfer (HVMT) aims to synthesize videos that one person
imitates other persons' actions. Although existing GAN-based HVMT methods have
achieved great success, they either fail to preserve appearance details due to
the loss of spatial consistency between synthesized and exemplary images, or
generate incoherent video results due to the lack of temporal consistency among
video frames. In this paper, we propose Coarse-to-Fine Flow Warping Network
(C2F-FWN) for spatial-temporal consistent HVMT. Particularly, C2F-FWN utilizes
coarse-to-fine flow warping and Layout-Constrained Deformable Convolution
(LC-DConv) to improve spatial consistency, and employs Flow Temporal
Consistency (FTC) Loss to enhance temporal consistency. In addition, provided
with multi-source appearance inputs, C2F-FWN can support appearance attribute
editing with great flexibility and efficiency. Besides public datasets, we also
collected a large-scale HVMT dataset named SoloDance for evaluation. Extensive
experiments conducted on our SoloDance dataset and the iPER dataset show that
our approach outperforms state-of-art HVMT methods in terms of both spatial and
temporal consistency. Source code and the SoloDance dataset are available at
https://github.com/wswdx/C2F-FWN.
</p>
<a href="http://arxiv.org/abs/2012.08976" target="_blank">arXiv:2012.08976</a> [<a href="http://arxiv.org/pdf/2012.08976" target="_blank">pdf</a>]

<h2>Visually Grounding Instruction for History-Dependent Manipulation. (arXiv:2012.08977v1 [cs.RO])</h2>
<h3>Hyemin Ahn, Obin Kwon, Kyoungdo Kim, Dongheui Lee, Songhwai Oh</h3>
<p>This paper emphasizes the importance of robot's ability to refer its task
history, when it executes a series of pick-and-place manipulations by following
text instructions given one by one. The advantage of referring the manipulation
history can be categorized into two folds: (1) the instructions omitting
details or using co-referential expressions can be interpreted, and (2) the
visual information of objects occluded by previous manipulations can be
inferred. For this challenge, we introduce the task of history-dependent
manipulation which is to visually ground a series of text instructions for
proper manipulations depending on the task history. We also suggest a relevant
dataset and a methodology based on the deep neural network, and show that our
network trained with a synthetic dataset can be applied to the real world based
on images transferred into synthetic-style based on the CycleGAN.
</p>
<a href="http://arxiv.org/abs/2012.08977" target="_blank">arXiv:2012.08977</a> [<a href="http://arxiv.org/pdf/2012.08977" target="_blank">pdf</a>]

<h2>Batch-Constrained Distributional Reinforcement Learning for Session-based Recommendation. (arXiv:2012.08984v1 [cs.LG])</h2>
<h3>Diksha Garg, Priyanka Gupta, Pankaj Malhotra, Lovekesh Vig, Gautam Shroff</h3>
<p>Most of the existing deep reinforcement learning (RL) approaches for
session-based recommendations either rely on costly online interactions with
real users, or rely on potentially biased rule-based or data-driven
user-behavior models for learning. In this work, we instead focus on learning
recommendation policies in the pure batch or offline setting, i.e. learning
policies solely from offline historical interaction logs or batch data
generated from an unknown and sub-optimal behavior policy, without further
access to data from the real-world or user-behavior models. We propose BCD4Rec:
Batch-Constrained Distributional RL for Session-based Recommendations. BCD4Rec
builds upon the recent advances in batch (offline) RL and distributional RL to
learn from offline logs while dealing with the intrinsically stochastic nature
of rewards from the users due to varied latent interest preferences
(environments). We demonstrate that BCD4Rec significantly improves upon the
behavior policy as well as strong RL and non-RL baselines in the batch setting
in terms of standard performance metrics like Click Through Rates or Buy Rates.
Other useful properties of BCD4Rec include: i. recommending items from the
correct latent categories indicating better value estimates despite large
action space (of the order of number of items), and ii. overcoming popularity
bias in clicked or bought items typically present in the offline logs.
</p>
<a href="http://arxiv.org/abs/2012.08984" target="_blank">arXiv:2012.08984</a> [<a href="http://arxiv.org/pdf/2012.08984" target="_blank">pdf</a>]

<h2>Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale. (arXiv:2012.09004v1 [cs.CV])</h2>
<h3>Zhengyan Tong, Xuanhong Chen, Bingbing Ni, Xiaohang Wang</h3>
<p>We propose a novel image-to-pencil translation method that could not only
generate high-quality pencil sketches but also offer the drawing process.
Existing pencil sketch algorithms are based on texture rendering rather than
the direct imitation of strokes, making them unable to show the drawing process
but only a final result. To address this challenge, we first establish a pencil
stroke imitation mechanism. Next, we develop a framework with three branches to
guide stroke drawing: the first branch guides the direction of the strokes, the
second branch determines the shade of the strokes, and the third branch
enhances the details further. Under this framework's guidance, we can produce a
pencil sketch by drawing one stroke every time. Our method is fully
interpretable. Comparison with existing pencil drawing algorithms shows that
our method is superior to others in terms of texture quality, style, and user
evaluation.
</p>
<a href="http://arxiv.org/abs/2012.09004" target="_blank">arXiv:2012.09004</a> [<a href="http://arxiv.org/pdf/2012.09004" target="_blank">pdf</a>]

<h2>I3DOL: Incremental 3D Object Learning without Catastrophic Forgetting. (arXiv:2012.09014v1 [cs.CV])</h2>
<h3>Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, Lichen Wang</h3>
<p>3D object classification has attracted appealing attentions in academic
researches and industrial applications. However, most existing methods need to
access the training data of past 3D object classes when facing the common
real-world scenario: new classes of 3D objects arrive in a sequence. Moreover,
the performance of advanced approaches degrades dramatically for past learned
classes (i.e., catastrophic forgetting), due to the irregular and redundant
geometric structures of 3D point cloud data. To address these challenges, we
propose a new Incremental 3D Object Learning (i.e., I3DOL) model, which is the
first exploration to learn new classes of 3D object continually. Specifically,
an adaptive-geometric centroid module is designed to construct discriminative
local geometric structures, which can better characterize the irregular point
cloud representation for 3D object. Afterwards, to prevent the catastrophic
forgetting brought by redundant geometric information, a geometric-aware
attention mechanism is developed to quantify the contributions of local
geometric structures, and explore unique 3D geometric characteristics with high
contributions for classes incremental learning. Meanwhile, a score fairness
compensation strategy is proposed to further alleviate the catastrophic
forgetting caused by unbalanced data between past and new classes of 3D object,
by compensating biased prediction for new classes in the validation phase.
Experiments on 3D representative datasets validate the superiority of our I3DOL
framework.
</p>
<a href="http://arxiv.org/abs/2012.09014" target="_blank">arXiv:2012.09014</a> [<a href="http://arxiv.org/pdf/2012.09014" target="_blank">pdf</a>]

<h2>AdjointBackMap: Reconstructing Effective Decision Hypersurfaces from CNN Layers Using Adjoint Operators. (arXiv:2012.09020v1 [cs.CV])</h2>
<h3>Qing Wan, Yoonsuck Choe</h3>
<p>There are several effective methods in explaining the inner workings of
convolutional neural networks (CNNs). However, in general, finding the inverse
of the function performed by CNNs as a whole is an ill-posed problem. In this
paper, we propose a method based on adjoint operators to reconstruct, given an
arbitrary unit in the CNN (except for the first convolutional layer), its
effective hypersurface in the input space that replicates that unit's decision
surface conditioned on a particular input image. Our results show that the
hypersurface reconstructed this way, when multiplied by the original input
image, would give nearly exact output value of that unit. We find that the CNN
unit's decision surface is largely conditioned on the input, and this may
explain why adversarial inputs can effectively deceive CNNs.
</p>
<a href="http://arxiv.org/abs/2012.09020" target="_blank">arXiv:2012.09020</a> [<a href="http://arxiv.org/pdf/2012.09020" target="_blank">pdf</a>]

<h2>CompositeTasking: Understanding Images by Spatial Composition of Tasks. (arXiv:2012.09030v1 [cs.CV])</h2>
<h3>Nikola Popovic, Danda Pani Paudel, Thomas Probst, Guolei Sun, Luc Van Gool</h3>
<p>We define the concept of CompositeTasking as the fusion of multiple,
spatially distributed tasks, for various aspects of image understanding.
Learning to perform spatially distributed tasks is motivated by the frequent
availability of only sparse labels across tasks, and the desire for a compact
multi-tasking network. To facilitate CompositeTasking, we introduce a novel
task conditioning model -- a single encoder-decoder network that performs
multiple, spatially varying tasks at once. The proposed network takes a pair of
an image and a set of pixel-wise dense tasks as inputs, and makes the task
related predictions for each pixel, which includes the decision of applying
which task where. As to the latter, we learn the composition of tasks that
needs to be performed according to some CompositeTasking rules. It not only
offers us a compact network for multi-tasking, but also allows for
task-editing. The strength of the proposed method is demonstrated by only
having to supply sparse supervision per task. The obtained results are on par
with our baselines that use dense supervision and a multi-headed multi-tasking
design. The source code will be made publicly available at
www.github.com/nikola3794/composite-tasking .
</p>
<a href="http://arxiv.org/abs/2012.09030" target="_blank">arXiv:2012.09030</a> [<a href="http://arxiv.org/pdf/2012.09030" target="_blank">pdf</a>]

<h2>Improved StyleGAN Embedding: Where are the Good Latents?. (arXiv:2012.09036v1 [cs.CV])</h2>
<h3>Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka</h3>
<p>StyleGAN is able to produce photorealistic images almost indistinguishable
from real ones. Embedding images into the StyleGAN latent space is not a
trivial task due to the reconstruction quality and editing quality trade-off.
In this paper, we first introduce a new normalized space to analyze the
diversity and the quality of the reconstructed latent codes. This space can
help answer the question of where good latent codes are located in latent
space. Second, we propose a framework to analyze the quality of different
embedding algorithms. Third, we propose an improved embedding algorithm based
on our analysis. We compare our results with the current state-of-the-art
methods and achieve a better trade-off between reconstruction quality and
editing quality.
</p>
<a href="http://arxiv.org/abs/2012.09036" target="_blank">arXiv:2012.09036</a> [<a href="http://arxiv.org/pdf/2012.09036" target="_blank">pdf</a>]

<h2>Copula-based synthetic data generation for machine learning emulators in weather and climate: application to a simple radiation model. (arXiv:2012.09037v1 [cs.LG])</h2>
<h3>David Meyer, Thomas Nagler, Robin J. Hogan</h3>
<p>Can we improve machine learning (ML) emulators with synthetic data? The use
of real data for training ML models is often the cause of major limitations.
For example, real data may be (a) only representative of a subset of situations
and domains, (b) expensive to source, (c) limited to specific individuals due
to licensing restrictions. Although the use of synthetic data is becoming
increasingly popular in computer vision, the training of ML emulators in
weather and climate still relies on the use of real data datasets. Here we
investigate whether the use of copula-based synthetically-augmented datasets
improves the prediction of ML emulators for estimating the downwelling longwave
radiation. Results show that bulk errors are cut by up to 75 % for the mean
bias error (from 0.08 to -0.02 W m$^{-2}$ and by up to 62 % (from 1.17 to 0.44
W m$^{-2}$) for the mean absolute error, thus showing potential for improving
the generalization of future ML emulators.
</p>
<a href="http://arxiv.org/abs/2012.09037" target="_blank">arXiv:2012.09037</a> [<a href="http://arxiv.org/pdf/2012.09037" target="_blank">pdf</a>]

<h2>Knowledge Graphs in Manufacturing and Production: A Systematic Literature Review. (arXiv:2012.09049v1 [cs.AI])</h2>
<h3>Georg Buchgeher, David Gabauer, Jorge Martinez-Gil, Lisa Ehrlinger</h3>
<p>Knowledge graphs in manufacturing and production aim to make production lines
more efficient and flexible with higher quality output. This makes knowledge
graphs attractive for companies to reach Industry 4.0 goals. However, existing
research in the field is quite preliminary, and more research effort on
analyzing how knowledge graphs can be applied in the field of manufacturing and
production is needed. Therefore, we have conducted a systematic literature
review as an attempt to characterize the state-of-the-art in this field, i.e.,
by identifying exiting research and by identifying gaps and opportunities for
further research. To do that, we have focused on finding the primary studies in
the existing literature, which were classified and analyzed according to four
criteria: bibliometric key facts, research type facets, knowledge graph
characteristics, and application scenarios. Besides, an evaluation of the
primary studies has also been carried out to gain deeper insights in terms of
methodology, empirical evidence, and relevance. As a result, we can offer a
complete picture of the domain, which includes such interesting aspects as the
fact that knowledge fusion is currently the main use case for knowledge graphs,
that empirical research and industrial application are still missing to a large
extent, that graph embeddings are not fully exploited, and that technical
literature is fast-growing but seems to be still far from its peak.
</p>
<a href="http://arxiv.org/abs/2012.09049" target="_blank">arXiv:2012.09049</a> [<a href="http://arxiv.org/pdf/2012.09049" target="_blank">pdf</a>]

<h2>Scalable and Safe Multi-Agent Motion Planning with Nonlinear Dynamics and Bounded Disturbances. (arXiv:2012.09052v1 [cs.RO])</h2>
<h3>Jingkai Chen, Jiaoyang Li, Chuchu Fan, Brian Williams</h3>
<p>We present a scalable and effective multi-agent safe motion planner that
enables a group of agents to move to their desired locations while avoiding
collisions with obstacles and other agents, with the presence of rich
obstacles, high-dimensional, nonlinear, nonholonomic dynamics, actuation
limits, and disturbances. We address this problem by finding a piecewise linear
path for each agent such that the actual trajectories following these paths are
guaranteed to satisfy the reach-and-avoid requirement. We show that the spatial
tracking error of the actual trajectories of the controlled agents can be
pre-computed for any qualified path that considers the minimum duration of each
path segment due to actuation limits. Using these bounds, we find a
collision-free path for each agent by solving Mixed Integer-Linear Programs and
coordinate agents by using the priority-based search. We demonstrate our method
by benchmarking in 2D and 3D scenarios with ground vehicles and quadrotors,
respectively, and show improvements over the solving time and the solution
quality compared to two state-of-the-art multi-agent motion planners.
</p>
<a href="http://arxiv.org/abs/2012.09052" target="_blank">arXiv:2012.09052</a> [<a href="http://arxiv.org/pdf/2012.09052" target="_blank">pdf</a>]

<h2>Towards Recognizing New Semantic Concepts in New Visual Domains. (arXiv:2012.09058v1 [cs.CV])</h2>
<h3>Massimiliano Mancini</h3>
<p>Deep learning models heavily rely on large scale annotated datasets for
training. Unfortunately, datasets cannot capture the infinite variability of
the real world, thus neural networks are inherently limited by the restricted
visual and semantic information contained in their training set. In this
thesis, we argue that it is crucial to design deep architectures that can
operate in previously unseen visual domains and recognize novel semantic
concepts. In the first part of the thesis, we describe different solutions to
enable deep models to generalize to new visual domains, by transferring
knowledge from a labeled source domain(s) to a domain (target) where no labeled
data are available. We will show how variants of batch-normalization (BN) can
be applied to different scenarios, from domain adaptation when source and
target are mixtures of multiple latent domains, to domain generalization,
continuous domain adaptation, and predictive domain adaptation, where
information about the target domain is available only in the form of metadata.
In the second part of the thesis, we show how to extend the knowledge of a
pretrained deep model to new semantic concepts, without access to the original
training set. We address the scenarios of sequential multi-task learning, using
transformed task-specific binary masks, open-world recognition, with end-to-end
training and enforced clustering, and incremental class learning in semantic
segmentation, where we highlight and address the problem of the semantic shift
of the background class. In the final part, we tackle a more challenging
problem: given images of multiple domains and semantic categories (with their
attributes), how to build a model that recognizes images of unseen concepts in
unseen domains? We also propose an approach based on domain and semantic mixing
of inputs and features, which is a first, promising step towards solving this
problem.
</p>
<a href="http://arxiv.org/abs/2012.09058" target="_blank">arXiv:2012.09058</a> [<a href="http://arxiv.org/pdf/2012.09058" target="_blank">pdf</a>]

<h2>Joint Generative and Contrastive Learning for Unsupervised Person Re-identification. (arXiv:2012.09071v1 [cs.CV])</h2>
<h3>Hao Chen, Yaohui Wang, Benoit Lagadec, Antitza Dantcheva, Francois Bremond</h3>
<p>Annotating identity labels in large-scale datasets is a labour-intensive
work, which strongly limits the scalability of person re-identification (ReID)
in the real world. Unsupervised ReID addresses this issue by learning
representations directly from unlabeled images. Recent self-supervised
contrastive learning provides an effective approach for unsupervised
representation learning. In this paper, we incorporate a Generative Adversarial
Network (GAN) and contrastive learning into one joint training framework. While
the GAN provides online data augmentation for contrastive learning, the
contrastive module learns view-invariant features for generation. In this
context, we propose a mesh-based novel view generator. Specifically, mesh
projections serve as references towards generating novel views of a person. In
addition, we propose a view-invariant loss to facilitate contrastive learning
between original and generated views. Deviating from previous GAN-based
unsupervised ReID methods involving domain adaptation, we do not rely on a
labeled source dataset, which makes our method more flexible. Extensive
experimental results show that our method significantly outperforms
state-of-the-art methods under both, fully unsupervised and unsupervised domain
adaptive settings on several large scale ReID datsets.
</p>
<a href="http://arxiv.org/abs/2012.09071" target="_blank">arXiv:2012.09071</a> [<a href="http://arxiv.org/pdf/2012.09071" target="_blank">pdf</a>]

<h2>Investigating ADR mechanisms with knowledge graph mining and explainable AI. (arXiv:2012.09077v1 [cs.LG])</h2>
<h3>Emmanuel Bresso, Pierre Monnin, C&#xe9;dric Bousquet, Fran&#xe7;ois-Elie Calvier, Ndeye-Coumba Ndiaye, Nadine Petitpain, Malika Sma&#xef;l-Tabbone, Adrien Coulet</h3>
<p>Adverse Drug Reactions (ADRs) are characterized within randomized clinical
trials and postmarketing pharmacovigilance, but their molecular mechanism
remains unknown in most cases. Aside from clinical trials, many elements of
knowledge about drug ingredients are available in open-access knowledge graphs.
In addition, drug classifications that label drugs as either causative or not
for several ADRs, have been established. We propose to mine knowledge graphs
for identifying biomolecular features that may enable reproducing automatically
expert classifications that distinguish drug causative or not for a given type
of ADR. In an explainable AI perspective, we explore simple classification
techniques such as Decision Trees and Classification Rules because they provide
human-readable models, which explain the classification itself, but may also
provide elements of explanation for molecular mechanisms behind ADRs. In
summary, we mine a knowledge graph for features; we train classifiers at
distinguishing, drugs associated or not with ADRs; we isolate features that are
both efficient in reproducing expert classifications and interpretable by
experts (i.e., Gene Ontology terms, drug targets, or pathway names); and we
manually evaluate how they may be explanatory. Extracted features reproduce
with a good fidelity classifications of drugs causative or not for DILI and
SCAR. Experts fully agreed that 73% and 38% of the most discriminative features
are possibly explanatory for DILI and SCAR, respectively; and partially agreed
(2/3) for 90% and 77% of them. Knowledge graphs provide diverse features to
enable simple and explainable models to distinguish between drugs that are
causative or not for ADRs. In addition to explaining classifications, most
discriminative features appear to be good candidates for investigating ADR
mechanisms further.
</p>
<a href="http://arxiv.org/abs/2012.09077" target="_blank">arXiv:2012.09077</a> [<a href="http://arxiv.org/pdf/2012.09077" target="_blank">pdf</a>]

<h2>Sample-Efficient Reinforcement Learning via Counterfactual-Based Data Augmentation. (arXiv:2012.09092v1 [cs.LG])</h2>
<h3>Chaochao Lu, Biwei Huang, Ke Wang, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Kun Zhang, Bernhard Sch&#xf6;lkopf</h3>
<p>Reinforcement learning (RL) algorithms usually require a substantial amount
of interaction data and perform well only for specific tasks in a fixed
environment. In some scenarios such as healthcare, however, usually only few
records are available for each patient, and patients may show different
responses to the same treatment, impeding the application of current RL
algorithms to learn optimal policies. To address the issues of mechanism
heterogeneity and related data scarcity, we propose a data-efficient RL
algorithm that exploits structural causal models (SCMs) to model the state
dynamics, which are estimated by leveraging both commonalities and differences
across subjects. The learned SCM enables us to counterfactually reason what
would have happened had another treatment been taken. It helps avoid real
(possibly risky) exploration and mitigates the issue that limited experiences
lead to biased policies. We propose counterfactual RL algorithms to learn both
population-level and individual-level policies. We show that counterfactual
outcomes are identifiable under mild conditions and that Q- learning on the
counterfactual-based augmented data set converges to the optimal value
function. Experimental results on synthetic and real-world data demonstrate the
efficacy of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2012.09092" target="_blank">arXiv:2012.09092</a> [<a href="http://arxiv.org/pdf/2012.09092" target="_blank">pdf</a>]

<h2>FedADC: Accelerated Federated Learning with Drift Control. (arXiv:2012.09102v1 [cs.LG])</h2>
<h3>Emre Ozfatura, Kerem Ozfatura, Deniz Gunduz</h3>
<p>Federated learning (FL) has become de facto framework for collaborative
learning among edge devices with privacy concern. The core of the FL strategy
is the use of stochastic gradient descent (SGD) in a distributed manner. Large
scale implementation of FL brings new challenges, such as the incorporation of
acceleration techniques designed for SGD into the distributed setting, and
mitigation of the drift problem due to non-homogeneous distribution of local
datasets. These two problems have been separately studied in the literature;
whereas, in this paper, we show that it is possible to address both problems
using a single strategy without any major alteration to the FL framework, or
introducing additional computation and communication load. To achieve this
goal, we propose FedADC, which is an accelerated FL algorithm with drift
control. We empirically illustrate the advantages of FedADC.
</p>
<a href="http://arxiv.org/abs/2012.09102" target="_blank">arXiv:2012.09102</a> [<a href="http://arxiv.org/pdf/2012.09102" target="_blank">pdf</a>]

<h2>Predicting Generalization in Deep Learning via Metric Learning -- PGDL Shared task. (arXiv:2012.09117v1 [cs.LG])</h2>
<h3>Sebastian Me&#x17e;nar, Bla&#x17e; &#x160;krlj</h3>
<p>The competition "Predicting Generalization in Deep Learning (PGDL)" aims to
provide a platform for rigorous study of generalization of deep learning models
and offer insight into the progress of understanding and explaining these
models. This report presents the solution that was submitted by the user
\emph{smeznar} which achieved the eight place in the competition. In the
proposed approach, we create simple metrics and find their best combination
with automatic testing on the provided dataset, exploring how combinations of
various properties of the input neural network architectures can be used for
the prediction of their generalization.
</p>
<a href="http://arxiv.org/abs/2012.09117" target="_blank">arXiv:2012.09117</a> [<a href="http://arxiv.org/pdf/2012.09117" target="_blank">pdf</a>]

<h2>Planning From Pixels in Atari With Learned Symbolic Representations. (arXiv:2012.09126v1 [cs.AI])</h2>
<h3>Andrea Dittadi, Frederik K. Drachmann, Thomas Bolander</h3>
<p>Width-based planning methods have been shown to yield state-of-the-art
performance in the Atari 2600 domain using pixel input. One successful
approach, RolloutIW, represents states with the B-PROST boolean feature set. An
augmented version of RolloutIW, $\pi$-IW, shows that learned features can be
competitive with handcrafted ones for width-based search. In this paper, we
leverage variational autoencoders (VAEs) to learn features directly from pixels
in a principled manner, and without supervision. The inference model of the
trained VAEs extracts boolean features from pixels, and RolloutIW plans with
these features. The resulting combination outperforms the original RolloutIW
and human professional play on Atari 2600 and drastically reduces the size of
the feature set.
</p>
<a href="http://arxiv.org/abs/2012.09126" target="_blank">arXiv:2012.09126</a> [<a href="http://arxiv.org/pdf/2012.09126" target="_blank">pdf</a>]

<h2>Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning. (arXiv:2012.09156v1 [cs.LG])</h2>
<h3>Nathan O. Lambert, Albert Wilcox, Howard Zhang, Kristofer S. J. Pister, Roberto Calandra</h3>
<p>Accurately predicting the dynamics of robotic systems is crucial for
model-based control and reinforcement learning. The most common way to estimate
dynamics is by fitting a one-step ahead prediction model and using it to
recursively propagate the predicted state distribution over long horizons.
Unfortunately, this approach is known to compound even small prediction errors,
making long-term predictions inaccurate. In this paper, we propose a new
parametrization to supervised learning on state-action data to stably predict
at longer horizons -- that we call a trajectory-based model. This
trajectory-based model takes an initial state, a future time index, and control
parameters as inputs, and predicts the state at the future time. Our results in
simulated and experimental robotic tasks show that our trajectory-based models
yield significantly more accurate long term predictions, improved sample
efficiency, and ability to predict task reward.
</p>
<a href="http://arxiv.org/abs/2012.09156" target="_blank">arXiv:2012.09156</a> [<a href="http://arxiv.org/pdf/2012.09156" target="_blank">pdf</a>]

<h2>DECOR-GAN: 3D Shape Detailization by Conditional Refinement. (arXiv:2012.09159v1 [cs.CV])</h2>
<h3>Zhiqin Chen, Vladimir Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, Siddhartha Chaudhuri</h3>
<p>We introduce a deep generative network for 3D shape detailization, akin to
stylization with the style being geometric details. We address the challenge of
creating large varieties of high-resolution and detailed 3D geometry from a
small set of exemplars by treating the problem as that of geometric detail
transfer. Given a low-resolution coarse voxel shape, our network refines it,
via voxel upsampling, into a higher-resolution shape enriched with geometric
details. The output shape preserves the overall structure (or content) of the
input, while its detail generation is conditioned on an input "style code"
corresponding to a detailed exemplar. Our 3D detailization via conditional
refinement is realized by a generative adversarial network, coined DECOR-GAN.
The network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D
PatchGAN discriminator to enforce local patches of the generated model to be
similar to those in the training detailed shapes. During testing, a style code
is fed into the generator to condition the refinement. We demonstrate that our
method can refine a coarse shape into a variety of detailed shapes with
different styles. The generated results are evaluated in terms of content
preservation, plausibility, and diversity. Comprehensive ablation studies are
conducted to validate our network designs.
</p>
<a href="http://arxiv.org/abs/2012.09159" target="_blank">arXiv:2012.09159</a> [<a href="http://arxiv.org/pdf/2012.09159" target="_blank">pdf</a>]

<h2>Learning Continuous Image Representation with Local Implicit Image Function. (arXiv:2012.09161v1 [cs.CV])</h2>
<h3>Yinbo Chen, Sifei Liu, Xiaolong Wang</h3>
<p>How to represent an image? While the visual world is presented in a
continuous manner, machines store and see the images in a discrete way with 2D
arrays of pixels. In this paper, we seek to learn a continuous representation
for images. Inspired by the recent progress in 3D reconstruction with implicit
function, we propose Local Implicit Image Function (LIIF), which takes an image
coordinate and the 2D deep features around the coordinate as inputs, predicts
the RGB value at a given coordinate as an output. Since the coordinates are
continuous, LIIF can be presented in an arbitrary resolution. To generate the
continuous representation for pixel-based images, we train an encoder and LIIF
representation via a self-supervised task with super-resolution. The learned
continuous representation can be presented in arbitrary resolution even
extrapolate to $\times 30$ higher resolution, where the training tasks are not
provided. We further show that LIIF representation builds a bridge between
discrete and continuous representation in 2D, it naturally supports the
learning tasks with size-varied image ground-truths and significantly
outperforms the method with resizing the ground-truths. Our project page with
code is at https://yinboc.github.io/liif/ .
</p>
<a href="http://arxiv.org/abs/2012.09161" target="_blank">arXiv:2012.09161</a> [<a href="http://arxiv.org/pdf/2012.09161" target="_blank">pdf</a>]

<h2>Point Transformer. (arXiv:2012.09164v1 [cs.CV])</h2>
<h3>Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun</h3>
<p>Self-attention networks have revolutionized natural language processing and
are making impressive strides in image analysis tasks such as image
classification and object detection. Inspired by this success, we investigate
the application of self-attention networks to 3D point cloud processing. We
design self-attention layers for point clouds and use these to construct
self-attention networks for tasks such as semantic scene segmentation, object
part segmentation, and object classification. Our Point Transformer design
improves upon prior work across domains and tasks. For example, on the
challenging S3DIS dataset for large-scale semantic scene segmentation, the
Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the
strongest prior model by 3.3 absolute percentage points and crossing the 70%
mIoU threshold for the first time.
</p>
<a href="http://arxiv.org/abs/2012.09164" target="_blank">arXiv:2012.09164</a> [<a href="http://arxiv.org/pdf/2012.09164" target="_blank">pdf</a>]

<h2>Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v1 [cs.CV])</h2>
<h3>Ji Hou, Benjamin Graham, Matthias Nie&#xdf;ner, Saining Xie</h3>
<p>The rapid progress in 3D scene understanding has come with growing demand for
data; however, collecting and annotating 3D scenes (e.g. point clouds) are
notoriously hard. For example, the number of scenes (e.g. indoor rooms) that
can be accessed and scanned might be limited; even given sufficient data,
acquiring 3D labels (e.g. instance masks) requires intensive human labor. In
this paper, we explore data-efficient learning for 3D point cloud. As a first
step towards this direction, we propose Contrastive Scene Contexts, a 3D
pre-training method that makes use of both point-level correspondences and
spatial contexts in a scene. Our method achieves state-of-the-art results on a
suite of benchmarks where training data or labels are scarce. Our study reveals
that exhaustive labelling of 3D point clouds might be unnecessary; and
remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%
(instance segmentation) and 96% (semantic segmentation) of the baseline
performance that uses full annotations.
</p>
<a href="http://arxiv.org/abs/2012.09165" target="_blank">arXiv:2012.09165</a> [<a href="http://arxiv.org/pdf/2012.09165" target="_blank">pdf</a>]

<h2>Resilient Monotone Sequential Maximization. (arXiv:1803.07954v4 [stat.ML] UPDATED)</h2>
<h3>Vasileios Tzoumas, Ali Jadbabaie, George J. Pappas</h3>
<p>Applications in machine learning, optimization, and control require the
sequential selection of a few system elements, such as sensors, data, or
actuators, to optimize the system performance across multiple time steps.
However, in failure-prone and adversarial environments, sensors get attacked,
data get deleted, and actuators fail. Thence, traditional sequential design
paradigms become insufficient and, in contrast, resilient sequential designs
that adapt against system-wide attacks, deletions, or failures become
important. In general, resilient sequential design problems are computationally
hard. Also, even though they often involve objective functions that are
monotone and (possibly) submodular, no scalable approximation algorithms are
known for their solution. In this paper, we provide the first scalable
algorithm, that achieves the following characteristics: system-wide resiliency,
i.e., the algorithm is valid for any number of denial-of-service attacks,
deletions, or failures; adaptiveness, i.e., at each time step, the algorithm
selects system elements based on the history of inflicted attacks, deletions,
or failures; and provable approximation performance, i.e., the algorithm
guarantees for monotone objective functions a solution close to the optimal. We
quantify the algorithm's approximation performance using a notion of curvature
for monotone (not necessarily submodular) set functions. Finally, we support
our theoretical analyses with simulated experiments, by considering a
control-aware sensor scheduling scenario, namely, sensing-constrained robot
navigation.
</p>
<a href="http://arxiv.org/abs/1803.07954" target="_blank">arXiv:1803.07954</a> [<a href="http://arxiv.org/pdf/1803.07954" target="_blank">pdf</a>]

<h2>An Attentive Survey of Attention Models. (arXiv:1904.02874v2 [cs.LG] UPDATED)</h2>
<h3>Sneha Chaudhari, Varun Mithal, Gungor Polatkan, Rohan Ramanath</h3>
<p>Attention Model has now become an important concept in neural networks that
has been researched within diverse application domains. This survey provides a
structured and comprehensive overview of the developments in modeling
attention. In particular, we propose a taxonomy which groups existing
techniques into coherent categories. We review salient neural architectures in
which attention has been incorporated, and discuss applications in which
modeling attention has shown a significant impact. Finally, we also describe
how attention has been used to improve the interpretability of neural networks.
We hope this survey will provide a succinct introduction to attention models
and guide practitioners while developing approaches for their applications.
</p>
<a href="http://arxiv.org/abs/1904.02874" target="_blank">arXiv:1904.02874</a> [<a href="http://arxiv.org/pdf/1904.02874" target="_blank">pdf</a>]

<h2>Specular- and Diffuse-reflection-based Face Spoofing Detection for Mobile Devices. (arXiv:1907.12400v5 [cs.CV] UPDATED)</h2>
<h3>Akinori F. Ebihara, Kazuyuki Sakurai, Hitoshi Imaoka</h3>
<p>In light of the rising demand for biometric-authentication systems,
preventing face spoofing attacks is a critical issue for the safe deployment of
face recognition systems. Here, we propose an efficient face presentation
attack detection (PAD) algorithm that requires minimal hardware and only a
small database, making it suitable for resource-constrained devices such as
mobile phones. Utilizing one monocular visible light camera, the proposed
algorithm takes two facial photos, one taken with a flash, the other without a
flash. The proposed $SpecDiff$ descriptor is constructed by leveraging two
types of reflection: (i) specular reflections from the iris region that have a
specific intensity distribution depending on liveness, and (ii) diffuse
reflections from the entire face region that represents the 3D structure of a
subject's face. Classifiers trained with $SpecDiff$ descriptor outperforms
other flash-based PAD algorithms on both an in-house database and on publicly
available NUAA, Replay-Attack, and SiW databases. Moreover, the proposed
algorithm achieves statistically significantly better accuracy to that of an
end-to-end, deep neural network classifier, while being approximately six-times
faster execution speed. The code is publicly available at
https://github.com/Akinori-F-Ebihara/SpecDiff-spoofing-detector.
</p>
<a href="http://arxiv.org/abs/1907.12400" target="_blank">arXiv:1907.12400</a> [<a href="http://arxiv.org/pdf/1907.12400" target="_blank">pdf</a>]

<h2>SIRUS: Stable and Interpretable RUle Set for Classification. (arXiv:1908.06852v5 [stat.ML] UPDATED)</h2>
<h3>Cl&#xe9;ment B&#xe9;nard (LPSM (UMR\_8001)), G&#xe9;rard Biau (LPSM (UMR\_8001)), S&#xe9;bastien da Veiga, Erwan Scornet (CMAP)</h3>
<p>State-of-the-art learning algorithms, such as random forests or neural
networks, are often qualified as "black-boxes" because of the high number and
complexity of operations involved in their prediction mechanism. This lack of
interpretability is a strong limitation for applications involving critical
decisions, typically the analysis of production processes in the manufacturing
industry. In such critical contexts, models have to be interpretable, i.e.,
simple, stable, and predictive. To address this issue, we design SIRUS (Stable
and Interpretable RUle Set), a new classification algorithm based on random
forests, which takes the form of a short list of rules. While simple models are
usually unstable with respect to data perturbation, SIRUS achieves a remarkable
stability improvement over cutting-edge methods. Furthermore, SIRUS inherits a
predictive accuracy close to random forests, combined with the simplicity of
decision trees. These properties are assessed both from a theoretical and
empirical point of view, through extensive numerical experiments based on our
R/C++ software implementation sirus available from CRAN.
</p>
<a href="http://arxiv.org/abs/1908.06852" target="_blank">arXiv:1908.06852</a> [<a href="http://arxiv.org/pdf/1908.06852" target="_blank">pdf</a>]

<h2>Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control. (arXiv:1908.07423v2 [cs.CV] UPDATED)</h2>
<h3>Yu-Wei Chao, Jimei Yang, Weifeng Chen, Jia Deng</h3>
<p>Recent progress on physics-based character animation has shown impressive
breakthroughs on human motion synthesis, through imitating motion capture data
via deep reinforcement learning. However, results have mostly been demonstrated
on imitating a single distinct motion pattern, and do not generalize to
interactive tasks that require flexible motion patterns due to varying
human-object spatial configurations. To bridge this gap, we focus on one class
of interactive tasks -- sitting onto a chair. We propose a hierarchical
reinforcement learning framework which relies on a collection of subtask
controllers trained to imitate simple, reusable mocap motions, and a meta
controller trained to execute the subtasks properly to complete the main task.
We experimentally demonstrate the strength of our approach over different
non-hierarchical and hierarchical baselines. We also show that our approach can
be applied to motion prediction given an image input. A supplementary video can
be found at https://youtu.be/3CeN0OGz2cA.
</p>
<a href="http://arxiv.org/abs/1908.07423" target="_blank">arXiv:1908.07423</a> [<a href="http://arxiv.org/pdf/1908.07423" target="_blank">pdf</a>]

<h2>A review on ranking problems in statistical learning. (arXiv:1909.02998v3 [cs.LG] UPDATED)</h2>
<h3>Tino Werner</h3>
<p>Ranking problems, also known as preference learning problems, define a widely
spread class of statistical learning problems with many applications, including
fraud detection, document ranking, medicine, credit risk screening, image
ranking or media memorability. In this article, we systematically review
different types of instance ranking problems, i.e., ranking problems that
require the prediction of an order of the response variables, and the
corresponding loss functions resp. goodness criteria. We discuss the
difficulties when trying to optimize those criteria. As for a detailed and
comprehensive overview of existing machine learning techniques to solve such
ranking problems, we systemize existing techniques and recapitulate the
corresponding optimization problems in a unified notation. We also discuss to
which of the ranking problems the respective algorithms are tailored and
identify their strengths and limitations. Computational aspects and open
research problems are also considered.
</p>
<a href="http://arxiv.org/abs/1909.02998" target="_blank">arXiv:1909.02998</a> [<a href="http://arxiv.org/pdf/1909.02998" target="_blank">pdf</a>]

<h2>Cascade Size Distributions: Why They Matter and How to Compute Them Efficiently. (arXiv:1909.05416v2 [cs.AI] UPDATED)</h2>
<h3>Rebekka Burkholz, John Quackenbush</h3>
<p>Cascade models are central to understanding, predicting, and controlling
epidemic spreading and information propagation. Related optimization, including
influence maximization, model parameter inference, or the development of
vaccination strategies, relies heavily on sampling from a model. This is either
inefficient or inaccurate. As alternative, we present an efficient message
passing algorithm that computes the probability distribution of the cascade
size for the Independent Cascade Model on weighted directed networks and
generalizations. Our approach is exact on trees but can be applied to any
network topology. It approximates locally tree-like networks well, scales to
large networks, and can lead to surprisingly good performance on more dense
networks, as we also exemplify on real world data.
</p>
<a href="http://arxiv.org/abs/1909.05416" target="_blank">arXiv:1909.05416</a> [<a href="http://arxiv.org/pdf/1909.05416" target="_blank">pdf</a>]

<h2>Federated Learning with Unbiased Gradient Aggregation and Controllable Meta Updating. (arXiv:1910.08234v3 [cs.LG] UPDATED)</h2>
<h3>Xin Yao, Tianchi Huang, Rui-Xiao Zhang, Ruiyu Li, Lifeng Sun</h3>
<p>Federated learning (FL) aims to train machine learning models in the
decentralized system consisting of an enormous amount of smart edge devices.
Federated averaging (FedAvg), the fundamental algorithm in FL settings,
proposes on-device training and model aggregation to avoid the potential heavy
communication costs and privacy concerns brought by transmitting raw data.
However, through theoretical analysis we argue that 1) the multiple steps of
local updating will result in gradient biases and 2) there is an inconsistency
between the expected target distribution and the optimization objectives
following the training paradigm in FedAvg. To tackle these problems, we first
propose an unbiased gradient aggregation algorithm with the keep-trace gradient
descent and the gradient evaluation strategy. Then we introduce an additional
controllable meta updating procedure with a small set of data samples,
indicating the expected target distribution, to provide a clear and consistent
optimization objective. Both the two improvements are model- and task-agnostic
and can be applied individually or together. Experimental results demonstrate
that the proposed methods are faster in convergence and achieve higher accuracy
with different network architectures in various FL settings.
</p>
<a href="http://arxiv.org/abs/1910.08234" target="_blank">arXiv:1910.08234</a> [<a href="http://arxiv.org/pdf/1910.08234" target="_blank">pdf</a>]

<h2>Measuring Diversity in Heterogeneous Information Networks. (arXiv:2001.01296v3 [cs.AI] UPDATED)</h2>
<h3>Pedro Ramaciotti Morales, Robin Lamarche-Perrin, Raphael Fournier-S&#x27;niehotta, Remy Poulain, Lionel Tabourier, Fabien Tarissan</h3>
<p>Diversity is a concept relevant to numerous domains of research varying from
ecology, to information theory, and to economics, to cite a few. It is a notion
that is steadily gaining attention in the information retrieval, network
analysis, and artificial neural networks communities. While the use of
diversity measures in network-structured data counts a growing number of
applications, no clear and comprehensive description is available for the
different ways in which diversities can be measured. In this article, we
develop a formal framework for the application of a large family of diversity
measures to heterogeneous information networks (HINs), a flexible, widely-used
network data formalism. This extends the application of diversity measures,
from systems of classifications and apportionments, to more complex relations
that can be better modeled by networks. In doing so, we not only provide an
effective organization of multiple practices from different domains, but also
unearth new observables in systems modeled by heterogeneous information
networks. We illustrate the pertinence of our approach by developing different
applications related to various domains concerned by both diversity and
networks. In particular, we illustrate the usefulness of these new proposed
observables in the domains of recommender systems and social media studies,
among other fields.
</p>
<a href="http://arxiv.org/abs/2001.01296" target="_blank">arXiv:2001.01296</a> [<a href="http://arxiv.org/pdf/2001.01296" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Active Human Pose Estimation. (arXiv:2001.02024v2 [cs.CV] UPDATED)</h2>
<h3>Erik G&#xe4;rtner, Aleksis Pirinen, Cristian Sminchisescu</h3>
<p>Most 3d human pose estimation methods assume that input -- be it images of a
scene collected from one or several viewpoints, or from a video -- is given.
Consequently, they focus on estimates leveraging prior knowledge and
measurement by fusing information spatially and/or temporally, whenever
available. In this paper we address the problem of an active observer with
freedom to move and explore the scene spatially -- in `time-freeze' mode --
and/or temporally, by selecting informative viewpoints that improve its
estimation accuracy. Towards this end, we introduce Pose-DRL, a fully trainable
deep reinforcement learning-based active pose estimation architecture which
learns to select appropriate views, in space and time, to feed an underlying
monocular pose estimator. We evaluate our model using single- and multi-target
estimators with strong result in both settings. Our system further learns
automatic stopping conditions in time and transition functions to the next
temporal processing step in videos. In extensive experiments with the Panoptic
multi-view setup, and for complex scenes containing multiple people, we show
that our model learns to select viewpoints that yield significantly more
accurate pose estimates compared to strong multi-view baselines.
</p>
<a href="http://arxiv.org/abs/2001.02024" target="_blank">arXiv:2001.02024</a> [<a href="http://arxiv.org/pdf/2001.02024" target="_blank">pdf</a>]

<h2>On the Trend-corrected Variant of Adaptive Stochastic Optimization Methods. (arXiv:2001.06130v2 [cs.LG] UPDATED)</h2>
<h3>Bingxin Zhou, Xuebin Zheng, Junbin Gao</h3>
<p>Adam-type optimizers, as a class of adaptive moment estimation methods with
the exponential moving average scheme, have been successfully used in many
applications of deep learning. Such methods are appealing due to the capability
on large-scale sparse datasets with high computational efficiency. In this
paper, we present a new framework for Adam-type methods with the trend
information when updating the parameters with the adaptive step size and
gradients. The additional terms in the algorithm promise an efficient movement
on the complex cost surface, and thus the loss would converge more rapidly. We
show empirically the importance of adding the trend component, where our
framework outperforms the conventional Adam and AMSGrad methods constantly on
the classical models with several real-world datasets.
</p>
<a href="http://arxiv.org/abs/2001.06130" target="_blank">arXiv:2001.06130</a> [<a href="http://arxiv.org/pdf/2001.06130" target="_blank">pdf</a>]

<h2>Bayes-TrEx: a Bayesian Sampling Approach to Model Transparency by Example. (arXiv:2002.10248v4 [cs.LG] UPDATED)</h2>
<h3>Serena Booth, Yilun Zhou, Ankit Shah, Julie Shah</h3>
<p>Post-hoc explanation methods are gaining popularity for interpreting,
understanding, and debugging neural networks. Most analyses using such methods
explain decisions in response to inputs drawn from the test set. However, the
test set may have few examples that trigger some model behaviors, such as
high-confidence failures or ambiguous classifications. To address these
challenges, we introduce a flexible model inspection framework: Bayes-TrEx.
Given a data distribution, Bayes-TrEx finds in-distribution examples with a
specified prediction confidence. We demonstrate several use cases of
Bayes-TrEx, including revealing highly confident (mis)classifications,
visualizing class boundaries via ambiguous examples, understanding novel-class
extrapolation behavior, and exposing neural network overconfidence. We use
Bayes-TrEx to study classifiers trained on CLEVR, MNIST, and Fashion-MNIST, and
we show that this framework enables more flexible holistic model analysis than
just inspecting the test set. Code is available at
https://github.com/serenabooth/Bayes-TrEx.
</p>
<a href="http://arxiv.org/abs/2002.10248" target="_blank">arXiv:2002.10248</a> [<a href="http://arxiv.org/pdf/2002.10248" target="_blank">pdf</a>]

<h2>GAN Compression: Efficient Architectures for Interactive Conditional GANs. (arXiv:2003.08936v3 [cs.CV] UPDATED)</h2>
<h3>Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, Song Han</h3>
<p>Conditional Generative Adversarial Networks (cGANs) have enabled controllable
image synthesis for many computer vision and graphics applications. However,
recent cGANs are 1-2 orders of magnitude more computationally-intensive than
modern recognition CNNs. For example, Gau-GAN consumes 281G MACs per image,
compared to 0.44GMACs for MobileNet-v3, making it difficult for interactive
deployment. In this work, we propose a general-purpose compression framework
for reducing the inference time and model size of the generator in cGANs.
Directly applying existing CNNs compression methods yields poor performance due
to the difficulty of GAN training and the differences in generator
architectures. We address these challenges in two ways. First, to stabilize the
GAN training, we transfer knowledge of multiple intermediate representations of
the original model to its compressed model, and unify unpaired and paired
learning. Second, instead of reusing existing CNN designs, our method
automatically finds efficient architectures via neural architecture search
(NAS). To accelerate the search process, we decouple the model training and
architecture search via weight sharing. Experiments demonstrate the
effectiveness of our method across different supervision settings (paired and
unpaired), model architectures, and learning methods (e.g., pix2pix, GauGAN,
CycleGAN). Without losing image quality, we reduce the computation of CycleGAN
by more than 20x and GauGAN by 9x, paving the way for interactive image
synthesis. The code and demo are publicly available.
</p>
<a href="http://arxiv.org/abs/2003.08936" target="_blank">arXiv:2003.08936</a> [<a href="http://arxiv.org/pdf/2003.08936" target="_blank">pdf</a>]

<h2>Towards Reusable Network Components by Learning Compatible Representations. (arXiv:2004.03898v3 [cs.LG] UPDATED)</h2>
<h3>Michael Gygli, Jasper Uijlings, Vittorio Ferrari</h3>
<p>This paper proposes to make a first step towards compatible and hence
reusable network components. Rather than training networks for different tasks
independently, we adapt the training process to produce network components that
are compatible across tasks. In particular, we split a network into two
components, a features extractor and a target task head, and propose various
approaches to accomplish compatibility between them. We systematically analyse
these approaches on the task of image classification on standard datasets. We
demonstrate that we can produce components which are directly compatible
without any fine-tuning or compromising accuracy on the original tasks.
Afterwards, we demonstrate the use of compatible components on three
applications: Unsupervised domain adaptation, transferring classifiers across
feature extractors with different architectures, and increasing the
computational efficiency of transfer learning.
</p>
<a href="http://arxiv.org/abs/2004.03898" target="_blank">arXiv:2004.03898</a> [<a href="http://arxiv.org/pdf/2004.03898" target="_blank">pdf</a>]

<h2>Image Quality Assessment: Unifying Structure and Texture Similarity. (arXiv:2004.07728v3 [cs.CV] UPDATED)</h2>
<h3>Keyan Ding, Kede Ma, Shiqi Wang, Eero P. Simoncelli</h3>
<p>Objective measures of image quality generally operate by comparing pixels of
a "degraded" image to those of the original. Relative to human observers, these
measures are overly sensitive to resampling of texture regions (e.g., replacing
one patch of grass with another). Here, we develop the first full-reference
image quality model with explicit tolerance to texture resampling. Using a
convolutional neural network, we construct an injective and differentiable
function that transforms images to multi-scale overcomplete representations. We
demonstrate empirically that the spatial averages of the feature maps in this
representation capture texture appearance, in that they provide a set of
sufficient statistical constraints to synthesize a wide variety of texture
patterns. We then describe an image quality method that combines correlations
of these spatial averages ("texture similarity") with correlations of the
feature maps ("structure similarity"). The parameters of the proposed measure
are jointly optimized to match human ratings of image quality, while minimizing
the reported distances between subimages cropped from the same texture images.
Experiments show that the optimized method explains human perceptual scores,
both on conventional image quality databases, as well as on texture databases.
The measure also offers competitive performance on related tasks such as
texture classification and retrieval. Finally, we show that our method is
relatively insensitive to geometric transformations (e.g., translation and
dilation), without use of any specialized training or data augmentation. Code
is available at https://github.com/dingkeyan93/DISTS.
</p>
<a href="http://arxiv.org/abs/2004.07728" target="_blank">arXiv:2004.07728</a> [<a href="http://arxiv.org/pdf/2004.07728" target="_blank">pdf</a>]

<h2>Mechanism Design with Bandit Feedback. (arXiv:2004.08924v3 [stat.ML] UPDATED)</h2>
<h3>Kirthevasan Kandasamy, Joseph E. Gonzalez, Michael I. Jordan, Ion Stoica</h3>
<p>We study a multi-round welfare-maximising mechanism design problem in
instances where agents do not know their values. On each round, a mechanism
assigns an allocation each to a set of agents and charges them a price; then
the agents provide (stochastic) feedback to the mechanism for the allocation
they received. This is motivated by applications in cloud markets and online
advertising where an agent may know her value for an allocation only after
experiencing it. Therefore, the mechanism needs to explore different
allocations for each agent, while simultaneously attempting to find the
socially optimal set of allocations. Our focus is on truthful and individually
rational mechanisms which imitate the classical VCG mechanism in the long run.
To that end, we define three notions of regret for the welfare, the individual
utilities of each agent and that of the mechanism. We show that these three
terms are interdependent via an $\Omega(T^{\frac{2}{3}})$ lower bound for the
maximum of these three terms after $T$ rounds of allocations, and describe a
family of anytime algorithms which achieve this rate. Our framework provides
flexibility to control the pricing scheme so as to trade-off between the agent
and seller regrets, and additionally to control the degree of truthfulness and
individual rationality.
</p>
<a href="http://arxiv.org/abs/2004.08924" target="_blank">arXiv:2004.08924</a> [<a href="http://arxiv.org/pdf/2004.08924" target="_blank">pdf</a>]

<h2>Towards Feature Space Adversarial Attack. (arXiv:2004.12385v2 [cs.LG] UPDATED)</h2>
<h3>Qiuling Xu, Guanhong Tao, Siyuan Cheng, Xiangyu Zhang</h3>
<p>We propose a new adversarial attack to Deep Neural Networks for image
classification. Different from most existing attacks that directly perturb
input pixels, our attack focuses on perturbing abstract features, more
specifically, features that denote styles, including interpretable styles such
as vivid colors and sharp outlines, and uninterpretable ones. It induces model
misclassfication by injecting imperceptible style changes through an
optimization procedure. We show that our attack can generate adversarial
samples that are more natural-looking than the state-of-the-art unbounded
attacks. The experiment also supports that existing pixel-space adversarial
attack detection and defense techniques can hardly ensure robustness in the
style related feature space.
</p>
<a href="http://arxiv.org/abs/2004.12385" target="_blank">arXiv:2004.12385</a> [<a href="http://arxiv.org/pdf/2004.12385" target="_blank">pdf</a>]

<h2>Fuzzy c-Means Clustering for Persistence Diagrams. (arXiv:2006.02796v4 [cs.LG] UPDATED)</h2>
<h3>Thomas Davies, Jack Aspinall, Bryan Wilder, Long Tran-Thanh</h3>
<p>Persistence diagrams concisely represent the topology of a point cloud whilst
having strong theoretical guarantees. Most current approaches to integrating
topological information into machine learning implicitly map persistence
diagrams to a Hilbert space, resulting in deformation of the underlying metric
structure whilst also generally requiring prior knowledge about the true
topology of the space. In this paper we give an algorithm for Fuzzy c-Means
(FCM) clustering directly on the space of persistence diagrams, enabling
unsupervised learning that automatically captures the topological structure of
data, with no prior knowledge or additional processing of persistence diagrams.
We prove the same convergence guarantees as traditional FCM clustering: every
convergent subsequence of iterates tends to a local minimum or saddle point. We
end by presenting experiments where our fuzzy topological clustering algorithm
allows for unsupervised top-k candidate selection in settings where (i) the
properties of persistence diagrams make them the natural choice over geometric
equivalents, and (ii) the probabilistic membership values let us rank
candidates in settings where verifying candidate suitability is expensive:
lattice structure classification in materials science and pre-trained model
selection in machine learning.
</p>
<a href="http://arxiv.org/abs/2006.02796" target="_blank">arXiv:2006.02796</a> [<a href="http://arxiv.org/pdf/2006.02796" target="_blank">pdf</a>]

<h2>Automated Design Space Exploration for optimised Deployment of DNN on Arm Cortex-A CPUs. (arXiv:2006.05181v2 [cs.LG] UPDATED)</h2>
<h3>Miguel de Prado, Andrew Mundy, Rabia Saeed, Maurizio Denna, Nuria Pazos, Luca Benini</h3>
<p>The spread of deep learning on embedded devices has prompted the development
of numerous methods to optimise the deployment of deep neural networks (DNN).
Works have mainly focused on: i) efficient DNN architectures, ii) network
optimisation techniques such as pruning and quantisation, iii) optimised
algorithms to speed up the execution of the most computational intensive layers
and, iv) dedicated hardware to accelerate the data flow and computation.
However, there is a lack of research on cross-level optimisation as the space
of approaches becomes too large to test and obtain a globally optimised
solution. Thus, leading to suboptimal deployment in terms of latency, accuracy,
and memory. In this work, we first detail and analyse the methods to improve
the deployment of DNNs across the different levels of software optimisation.
Building on this knowledge, we present an automated exploration framework to
ease the deployment of DNNs. The framework relies on a Reinforcement Learning
search that, combined with a deep learning inference framework, automatically
explores the design space and learns an optimised solution that speeds up the
performance and reduces the memory on embedded CPU platforms. Thus, we present
a set of results for state-of-the-art DNNs on a range of Arm Cortex-A CPU
platforms achieving up to 4x improvement in performance and over 2x reduction
in memory with negligible loss in accuracy with respect to the BLAS
floating-point implementation.
</p>
<a href="http://arxiv.org/abs/2006.05181" target="_blank">arXiv:2006.05181</a> [<a href="http://arxiv.org/pdf/2006.05181" target="_blank">pdf</a>]

<h2>Standardised convolutional filtering for radiomics. (arXiv:2006.05470v2 [cs.CV] UPDATED)</h2>
<h3>Adrien Depeursinge, Vincent Andrearczyk, Philip Whybra, Joost van Griethuysen, Henning M&#xfc;ller, Roger Schaer, Martin Valli&#xe8;res, Alex Zwanenburg (for the Image Biomarker Standardisation Initiative)</h3>
<p>The Image Biomarker Standardisation Initiative (IBSI) aims to improve
reproducibility of radiomics studies by standardising the computational process
of extracting image biomarkers (features) from images. We have previously
established reference values for 169 commonly used features, created a standard
radiomics image processing scheme, and developed reporting guidelines for
radiomic studies. However, several aspects are not standardised.

Here we present a preliminary version of a reference manual on the use of
convolutional image filters in radiomics. Filters, such as wavelets or
Laplacian of Gaussian filters, play an important part in emphasising specific
image characteristics such as edges and blobs. Features derived from filter
response maps have been found to be poorly reproducible. This reference manual
forms the basis of ongoing work on standardising convolutional filters in
radiomics, and will be updated as this work progresses.
</p>
<a href="http://arxiv.org/abs/2006.05470" target="_blank">arXiv:2006.05470</a> [<a href="http://arxiv.org/pdf/2006.05470" target="_blank">pdf</a>]

<h2>Backdoor Attacks on Federated Meta-Learning. (arXiv:2006.07026v2 [cs.LG] UPDATED)</h2>
<h3>Chien-Lun Chen, Leana Golubchik, Marco Paolieri</h3>
<p>Federated learning allows multiple users to collaboratively train a shared
classification model while preserving data privacy. This approach, where model
updates are aggregated by a central server, was shown to be vulnerable to
poisoning backdoor attacks: a malicious user can alter the shared model to
arbitrarily classify specific inputs from a given class. In this paper, we
analyze the effects of backdoor attacks on federated meta-learning, where users
train a model that can be adapted to different sets of output classes using
only a few examples. While the ability to adapt could, in principle, make
federated learning frameworks more robust to backdoor attacks (when new
training examples are benign), we find that even 1-shot~attacks can be very
successful and persist after additional training. To address these
vulnerabilities, we propose a defense mechanism inspired by matching networks,
where the class of an input is predicted from the similarity of its features
with a support set of labeled examples. By removing the decision logic from the
model shared with the federation, success and persistence of backdoor attacks
are greatly reduced.
</p>
<a href="http://arxiv.org/abs/2006.07026" target="_blank">arXiv:2006.07026</a> [<a href="http://arxiv.org/pdf/2006.07026" target="_blank">pdf</a>]

<h2>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. (arXiv:2006.10782v2 [cs.LG] UPDATED)</h2>
<h3>Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, Max Tegmark</h3>
<p>We present an improved method for symbolic regression that seeks to fit data
to formulas that are Pareto-optimal, in the sense of having the best accuracy
for a given complexity. It improves on the previous state-of-the-art by
typically being orders of magnitude more robust toward noise and bad data, and
also by discovering many formulas that stumped previous methods. We develop a
method for discovering generalized symmetries (arbitrary modularity in the
computational graph of a formula) from gradient properties of a neural network
fit. We use normalizing flows to generalize our symbolic regression method to
probability distributions from which we only have samples, and employ
statistical hypothesis testing to accelerate robust brute-force search.
</p>
<a href="http://arxiv.org/abs/2006.10782" target="_blank">arXiv:2006.10782</a> [<a href="http://arxiv.org/pdf/2006.10782" target="_blank">pdf</a>]

<h2>Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data. (arXiv:2006.10833v2 [cs.LG] UPDATED)</h2>
<h3>Sindy L&#xf6;we, David Madras, Richard Zemel, Max Welling</h3>
<p>Standard causal discovery methods must fit a new model whenever they
encounter samples from a new underlying causal graph. However, these samples
often share relevant information - for instance, the dynamics describing the
effects of causal relations - which is lost when following this approach. We
propose Amortized Causal Discovery, a novel framework that leverages such
shared dynamics to learn to infer causal relations from time-series data. This
enables us to train a single, amortized model that infers causal relations
across samples with different underlying causal graphs, and thus makes use of
the information that is shared. We demonstrate experimentally that this
approach, implemented as a variational model, leads to significant improvements
in causal discovery performance, and show how it can be extended to perform
well under hidden confounding.
</p>
<a href="http://arxiv.org/abs/2006.10833" target="_blank">arXiv:2006.10833</a> [<a href="http://arxiv.org/pdf/2006.10833" target="_blank">pdf</a>]

<h2>FNA++: Fast Network Adaptation via Parameter Remapping and Architecture Search. (arXiv:2006.12986v2 [cs.CV] UPDATED)</h2>
<h3>Jiemin Fang, Yuzhu Sun, Qian Zhang, Kangjian Peng, Yuan Li, Wenyu Liu, Xinggang Wang</h3>
<p>Deep neural networks achieve remarkable performance in many computer vision
tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection
approaches reuse neural network architectures designed for image classification
as the backbone, commonly pre-trained on ImageNet. However, performance gains
can be achieved by designing network architectures specifically for detection
and segmentation, as shown by recent neural architecture search (NAS) research
for detection and segmentation. One major challenge though is that ImageNet
pre-training of the search space representation (a.k.a. super network) or the
searched networks incurs huge computational cost. In this paper, we propose a
Fast Network Adaptation (FNA++) method, which can adapt both the architecture
and parameters of a seed network (e.g. an ImageNet pre-trained network) to
become a network with different depths, widths, or kernel sizes via a parameter
remapping technique, making it possible to use NAS for segmentation and
detection tasks a lot more efficiently. In our experiments, we apply FNA++ on
MobileNetV2 to obtain new networks for semantic segmentation, object detection,
and human pose estimation that clearly outperform existing networks designed
both manually and by NAS. We also implement FNA++ on ResNets and NAS networks,
which demonstrates a great generalization ability. The total computation cost
of FNA++ is significantly less than SOTA segmentation and detection NAS
approaches: 1737x less than DPC, 6.8x less than Auto-DeepLab, and 8.0x less
than DetNAS. A series of ablation studies are performed to demonstrate the
effectiveness, and detailed analysis is provided for more insights into the
working mechanism. Codes are available at https://github.com/JaminFong/FNA.
</p>
<a href="http://arxiv.org/abs/2006.12986" target="_blank">arXiv:2006.12986</a> [<a href="http://arxiv.org/pdf/2006.12986" target="_blank">pdf</a>]

<h2>A Theoretical Framework for Target Propagation. (arXiv:2006.14331v4 [cs.LG] UPDATED)</h2>
<h3>Alexander Meulemans, Francesco S. Carzaniga, Johan A.K. Suykens, Jo&#xe3;o Sacramento, Benjamin F. Grewe</h3>
<p>The success of deep learning, a brain-inspired form of AI, has sparked
interest in understanding how the brain could similarly learn across multiple
layers of neurons. However, the majority of biologically-plausible learning
algorithms have not yet reached the performance of backpropagation (BP), nor
are they built on strong theoretical foundations. Here, we analyze target
propagation (TP), a popular but not yet fully understood alternative to BP,
from the standpoint of mathematical optimization. Our theory shows that TP is
closely related to Gauss-Newton optimization and thus substantially differs
from BP. Furthermore, our analysis reveals a fundamental limitation of
difference target propagation (DTP), a well-known variant of TP, in the
realistic scenario of non-invertible neural networks. We provide a first
solution to this problem through a novel reconstruction loss that improves
feedback weight training, while simultaneously introducing architectural
flexibility by allowing for direct feedback connections from the output to each
hidden layer. Our theory is corroborated by experimental results that show
significant improvements in performance and in the alignment of forward weight
updates with loss gradients, compared to DTP.
</p>
<a href="http://arxiv.org/abs/2006.14331" target="_blank">arXiv:2006.14331</a> [<a href="http://arxiv.org/pdf/2006.14331" target="_blank">pdf</a>]

<h2>Deep Bayesian Quadrature Policy Optimization. (arXiv:2006.15637v3 [cs.LG] UPDATED)</h2>
<h3>Akella Ravi Tej, Kamyar Azizzadenesheli, Mohammad Ghavamzadeh, Anima Anandkumar, Yisong Yue</h3>
<p>We study the problem of obtaining accurate policy gradient estimates using a
finite number of samples. Monte-Carlo methods have been the default choice for
policy gradient estimation, despite suffering from high variance in the
gradient estimates. On the other hand, more sample efficient alternatives like
Bayesian quadrature methods have received little attention due to their high
computational complexity. In this work, we propose deep Bayesian quadrature
policy gradient (DBQPG), a computationally efficient high-dimensional
generalization of Bayesian quadrature, for policy gradient estimation. We show
that DBQPG can substitute Monte-Carlo estimation in policy gradient methods,
and demonstrate its effectiveness on a set of continuous control benchmarks. In
comparison to Monte-Carlo estimation, DBQPG provides (i) more accurate gradient
estimates with a significantly lower variance, (ii) a consistent improvement in
the sample complexity and average return for several deep policy gradient
algorithms, and, (iii) the uncertainty in gradient estimation that can be
incorporated to further improve the performance.
</p>
<a href="http://arxiv.org/abs/2006.15637" target="_blank">arXiv:2006.15637</a> [<a href="http://arxiv.org/pdf/2006.15637" target="_blank">pdf</a>]

<h2>Learning Goals from Failure. (arXiv:2006.15657v2 [cs.CV] UPDATED)</h2>
<h3>Dave Epstein, Carl Vondrick</h3>
<p>We introduce a framework that predicts the goals behind observable human
action in video. Motivated by evidence in developmental psychology, we leverage
video of unintentional action to learn video representations of goals without
direct supervision. Our approach models videos as contextual trajectories that
represent both low-level motion and high-level action features. Experiments and
visualizations show our trained model is able to predict the underlying goals
in video of unintentional action. We also propose a method to "automatically
correct" unintentional action by leveraging gradient signals of our model to
adjust latent trajectories. Although the model is trained with minimal
supervision, it is competitive with or outperforms baselines trained on large
(supervised) datasets of successfully executed goals, showing that observing
unintentional action is crucial to learning about goals in video. Project page:
https://aha.cs.columbia.edu/
</p>
<a href="http://arxiv.org/abs/2006.15657" target="_blank">arXiv:2006.15657</a> [<a href="http://arxiv.org/pdf/2006.15657" target="_blank">pdf</a>]

<h2>GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis. (arXiv:2007.02442v3 [cs.CV] UPDATED)</h2>
<h3>Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger</h3>
<p>While 2D generative adversarial networks have enabled high-resolution image
synthesis, they largely lack an understanding of the 3D world and the image
formation process. Thus, they do not provide precise control over camera
viewpoint or object pose. To address this problem, several recent approaches
leverage intermediate voxel-based representations in combination with
differentiable rendering. However, existing methods either produce low image
resolution or fall short in disentangling camera and scene properties, e.g.,
the object identity may vary with the viewpoint. In this paper, we propose a
generative model for radiance fields which have recently proven successful for
novel view synthesis of a single scene. In contrast to voxel-based
representations, radiance fields are not confined to a coarse discretization of
the 3D space, yet allow for disentangling camera and scene properties while
degrading gracefully in the presence of reconstruction ambiguity. By
introducing a multi-scale patch-based discriminator, we demonstrate synthesis
of high-resolution images while training our model from unposed 2D images
alone. We systematically analyze our approach on several challenging synthetic
and real-world datasets. Our experiments reveal that radiance fields are a
powerful representation for generative image synthesis, leading to 3D
consistent models that render with high fidelity.
</p>
<a href="http://arxiv.org/abs/2007.02442" target="_blank">arXiv:2007.02442</a> [<a href="http://arxiv.org/pdf/2007.02442" target="_blank">pdf</a>]

<h2>An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization. (arXiv:2007.05670v2 [stat.ML] UPDATED)</h2>
<h3>Yimin Huang, Yujun Li, Hanrong Ye, Zhenguo Li, Zhihua Zhang</h3>
<p>The evaluation of hyperparameters, neural architectures, or data augmentation
policies becomes a critical model selection problem in advanced deep learning
with a large hyperparameter search space. In this paper, we propose an
efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the
scenario of hyperparameter search evaluation. It evaluates the potential of
hyperparameters by the sub-samples of observations and is theoretically proved
to be optimal under the criterion of cumulative regret. We further combine SS
with Bayesian Optimization and develop a novel hyperparameter optimization
algorithm called BOSS. Empirical studies validate our theoretical arguments of
SS and demonstrate the superior performance of BOSS on a number of
applications, including Neural Architecture Search (NAS), Data Augmentation
(DA), Object Detection (OD), and Reinforcement Learning (RL).
</p>
<a href="http://arxiv.org/abs/2007.05670" target="_blank">arXiv:2007.05670</a> [<a href="http://arxiv.org/pdf/2007.05670" target="_blank">pdf</a>]

<h2>Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network. (arXiv:2007.05674v3 [cs.AI] UPDATED)</h2>
<h3>Matthew C. Fontaine, Ruilin Liu, Ahmed Khalifa, Jignesh Modi, Julian Togelius, Amy K. Hoover, Stefanos Nikolaidis</h3>
<p>Generative adversarial networks (GANs) are now a ubiquitous approach to
procedurally generating video game levels. While GAN generated levels are
stylistically similar to human-authored examples, human designers often want to
explore the generative design space of GANs to extract interesting levels.
However, human designers find latent vectors opaque and would rather explore
along dimensions the designer specifies, such as number of enemies or
obstacles. We propose using state-of-the-art quality diversity algorithms
designed to optimize continuous spaces, i.e. MAP-Elites with a directional
variation operator and Covariance Matrix Adaptation MAP-Elites, to efficiently
explore the latent space of a GAN to extract levels that vary across a set of
specified gameplay measures. In the benchmark domain of Super Mario Bros, we
demonstrate how designers may specify gameplay measures to our system and
extract high-quality (playable) levels with a diverse range of level mechanics,
while still maintaining stylistic similarity to human authored examples. An
online user study shows how the different mechanics of the automatically
generated levels affect subjective ratings of their perceived difficulty and
appearance.
</p>
<a href="http://arxiv.org/abs/2007.05674" target="_blank">arXiv:2007.05674</a> [<a href="http://arxiv.org/pdf/2007.05674" target="_blank">pdf</a>]

<h2>Self-Tuning Bandits over Unknown Covariate-Shifts. (arXiv:2007.08584v3 [stat.ML] UPDATED)</h2>
<h3>Joseph Suk, Samory Kpotufe</h3>
<p>Bandits with covariates, a.k.a. contextual bandits, address situations where
optimal actions (or arms) at a given time $t$, depend on a context $x_t$, e.g.,
a new patient's medical history, a consumer's past purchases. While it is
understood that the distribution of contexts might change over time, e.g., due
to seasonalities, or deployment to new environments, the bulk of studies
concern the most adversarial such changes, resulting in regret bounds that are
often worst-case in nature.

Covariate-shift on the other hand has been considered in classification as a
middle-ground formalism that can capture mild to relatively severe changes in
distributions. We consider nonparametric bandits under such middle-ground
scenarios, and derive new regret bounds that tightly capture a continuum of
changes in context distribution. Furthermore, we show that these rates can be
adaptively attained without knowledge of the time of shift nor the amount of
shift.
</p>
<a href="http://arxiv.org/abs/2007.08584" target="_blank">arXiv:2007.08584</a> [<a href="http://arxiv.org/pdf/2007.08584" target="_blank">pdf</a>]

<h2>Adversarial Training Reduces Information and Improves Transferability. (arXiv:2007.11259v4 [cs.LG] UPDATED)</h2>
<h3>Matteo Terzi, Alessandro Achille, Marco Maggipinto, Gian Antonio Susto</h3>
<p>Recent results show that features of adversarially trained networks for
classification, in addition to being robust, enable desirable properties such
as invertibility. The latter property may seem counter-intuitive as it is
widely accepted by the community that classification models should only capture
the minimal information (features) required for the task. Motivated by this
discrepancy, we investigate the dual relationship between Adversarial Training
and Information Theory. We show that the Adversarial Training can improve
linear transferability to new tasks, from which arises a new trade-off between
transferability of representations and accuracy on the source task. We validate
our results employing robust networks trained on CIFAR-10, CIFAR-100 and
ImageNet on several datasets. Moreover, we show that Adversarial Training
reduces Fisher information of representations about the input and of the
weights about the task, and we provide a theoretical argument which explains
the invertibility of deterministic networks without violating the principle of
minimality. Finally, we leverage our theoretical insights to remarkably improve
the quality of reconstructed images through inversion.
</p>
<a href="http://arxiv.org/abs/2007.11259" target="_blank">arXiv:2007.11259</a> [<a href="http://arxiv.org/pdf/2007.11259" target="_blank">pdf</a>]

<h2>Accuracy and stability of solar variable selection comparison under complicated dependence structures. (arXiv:2007.15614v2 [stat.ML] UPDATED)</h2>
<h3>Ning Xu, Timothy C.G. Fisher, Jian Hong</h3>
<p>In this paper we focus on the empirical variable-selection peformance of
subsample-ordered least angle regression (Solar) -- a novel ultrahigh
dimensional redesign of lasso -- on the empirical data with complicated
dependence structures and, hence, severe multicollinearity and grouping effect
issues. Previous researches show that Solar largely alleviates several known
high-dimensional issues with least-angle regression and $\mathcal{L}_1$
shrinkage. Also, With the same computation load, solar yields substantiali
mprovements over two lasso solvers (least-angle regression for lasso and
coordinate-descent) in terms of the sparsity (37-64\% reduction in the average
number of selected variables), stability and accuracy of variable selection.
Simulations also demonstrate that solar enhances the robustness of variable
selection to different settings of the irrepresentable condition and to
variations in the dependence structures assumed in regression analysis. To
confirm that the improvements are also available for empirical researches, we
choose the prostate cancer data and the Sydney house price data and apply two
lasso solvers, elastic net and Solar on them for comparison. The results shows
that (i) lasso is affected by the grouping effect and randomly drop variables
with high correlations, resulting unreliable and uninterpretable results; (ii)
elastic net is more robust to grouping effect; however, it completely lose
variable-selection sparsity when the dependence structure of the data is
complicated; (iii) solar demonstrates its superior robustness to complicated
dependence structures and grouping effect, returning variable-selection results
with better stability and sparsity. The code can be found at
https://github.com/isaac2math/solar_application
</p>
<a href="http://arxiv.org/abs/2007.15614" target="_blank">arXiv:2007.15614</a> [<a href="http://arxiv.org/pdf/2007.15614" target="_blank">pdf</a>]

<h2>Instrument variable detection with graph learning : an application to high dimensional GIS-census data for house pricing. (arXiv:2007.15769v2 [stat.ML] UPDATED)</h2>
<h3>Ning Xu, Timothy C.G. Fisher, Jian Hong</h3>
<p>Endogeneity bias and instrument variable validation have always been
important topics in statistics and econometrics. In the era of big data, such
issues typically combine with dimensionality issues and, hence, require even
more attention. In this paper, we merge two well-known tools from machine
learning and biostatistics---variable selection algorithms and probablistic
graphs---to estimate house prices and the corresponding causal structure using
2010 data on Sydney. The estimation uses a 200-gigabyte ultrahigh dimensional
database consisting of local school data, GIS information, census data, house
characteristics and other socio-economic records. Using "big data", we show
that it is possible to perform a data-driven instrument selection efficiently
and purge out the invalid instruments. Our approach improves the sparsity of
variable selection, stability and robustness in the presence of high
dimensionality, complicated causal structures and the consequent
multicollinearity, and recovers a sparse and intuitive causal structure. The
approach also reveals an efficiency and effectiveness in endogeneity detection,
instrument validation, weak instrument pruning and the selection of valid
instruments. From the perspective of machine learning, the estimation results
both align with and confirms the facts of Sydney house market, the classical
economic theories and the previous findings of simultaneous equations modeling.
Moreover, the estimation results are consistent with and supported by classical
econometric tools such as two-stage least square regression and different
instrument tests. All the code may be found at
\url{https://github.com/isaac2math/solar_graph_learning}.
</p>
<a href="http://arxiv.org/abs/2007.15769" target="_blank">arXiv:2007.15769</a> [<a href="http://arxiv.org/pdf/2007.15769" target="_blank">pdf</a>]

<h2>Lenient Regret for Multi-Armed Bandits. (arXiv:2008.03959v3 [cs.LG] UPDATED)</h2>
<h3>Nadav Merlis, Shie Mannor</h3>
<p>We consider the Multi-Armed Bandit (MAB) problem, where an agent sequentially
chooses actions and observes rewards for the actions it took. While the
majority of algorithms try to minimize the regret, i.e., the cumulative
difference between the reward of the best action and the agent's action, this
criterion might lead to undesirable results. For example, in large problems, or
when the interaction with the environment is brief, finding an optimal arm is
infeasible, and regret-minimizing algorithms tend to over-explore. To overcome
this issue, algorithms for such settings should instead focus on playing
near-optimal arms. To this end, we suggest a new, more lenient, regret
criterion that ignores suboptimality gaps smaller than some $\epsilon$. We then
present a variant of the Thompson Sampling (TS) algorithm, called
$\epsilon$-TS, and prove its asymptotic optimality in terms of the lenient
regret. Importantly, we show that when the mean of the optimal arm is high
enough, the lenient regret of $\epsilon$-TS is bounded by a constant. Finally,
we show that $\epsilon$-TS can be applied to improve the performance when the
agent knows a lower bound of the suboptimality gaps.
</p>
<a href="http://arxiv.org/abs/2008.03959" target="_blank">arXiv:2008.03959</a> [<a href="http://arxiv.org/pdf/2008.03959" target="_blank">pdf</a>]

<h2>Inverse Reinforcement Learning with Natural Language Goals. (arXiv:2008.06924v3 [cs.LG] UPDATED)</h2>
<h3>Li Zhou, Kevin Small</h3>
<p>Humans generally use natural language to communicate task requirements to
each other. Ideally, natural language should also be usable for communicating
goals to autonomous machines (e.g., robots) to minimize friction in task
specification. However, understanding and mapping natural language goals to
sequences of states and actions is challenging. Specifically, existing work
along these lines has encountered difficulty in generalizing learned policies
to new natural language goals and environments. In this paper, we propose a
novel adversarial inverse reinforcement learning algorithm to learn a
language-conditioned policy and reward function. To improve generalization of
the learned policy and reward function, we use a variational goal generator to
relabel trajectories and sample diverse goals during training. Our algorithm
outperforms multiple baselines by a large margin on a vision-based natural
language instruction following dataset (Room-2-Room), demonstrating a promising
advance in enabling the use of natural language instructions in specifying
agent goals.
</p>
<a href="http://arxiv.org/abs/2008.06924" target="_blank">arXiv:2008.06924</a> [<a href="http://arxiv.org/pdf/2008.06924" target="_blank">pdf</a>]

<h2>DNN2LR: Interpretation-inspired Feature Crossing for Real-world Tabular Data. (arXiv:2008.09775v4 [cs.LG] UPDATED)</h2>
<h3>Zhaocheng Liu, Qiang Liu, Haoli Zhang, Yuntian Chen</h3>
<p>For sake of reliability, it is necessary for models in real-world
applications to be both powerful and globally interpretable. Simple
classifiers, e.g., Logistic Regression (LR), are globally interpretable, but
not powerful enough to model complex nonlinear interactions among features in
tabular data. Meanwhile, Deep Neural Networks (DNNs) have shown great
effectiveness for modeling tabular data, but is not globally interpretable. In
this work, we find local piece-wise interpretations in DNN of a specific
feature are usually inconsistent in different samples, which is caused by
feature interactions in the hidden layers. Accordingly, we can design an
automatic feature crossing method to find feature interactions in DNN, and use
them as cross features in LR. We give definition of the interpretation
inconsistency in DNN, based on which a novel feature crossing method called
DNN2LR is proposed. Extensive experiments have been conducted on four public
datasets and two real-world datasets. The final model, i.e., a LR model
empowered with cross features, generated by DNN2LR can outperform the complex
DNN model, as well as several state-of-the-art feature crossing methods. The
experimental results strongly verify the effectiveness and efficiency of
DNN2LR, especially on real-world datasets with large numbers of feature fields.
</p>
<a href="http://arxiv.org/abs/2008.09775" target="_blank">arXiv:2008.09775</a> [<a href="http://arxiv.org/pdf/2008.09775" target="_blank">pdf</a>]

<h2>Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion. (arXiv:2009.05757v3 [cs.CV] UPDATED)</h2>
<h3>Jinpeng Wang, Yuting Gao, Ke Li, Jianguo Hu, Xinyang Jiang, Xiaowei Guo, Rongrong Ji, Xing Sun</h3>
<p>One significant factor we expect the video representation learning to
capture, especially in contrast with the image representation learning, is the
object motion. However, we found that in the current mainstream video datasets,
some action categories are highly related with the scene where the action
happens, making the model tend to degrade to a solution where only the scene
information is encoded. For example, a trained model may predict a video as
playing football simply because it sees the field, neglecting that the subject
is dancing as a cheerleader on the field. This is against our original
intention towards the video representation learning and may bring scene bias on
different dataset that can not be ignored. In order to tackle this problem, we
propose to decouple the scene and the motion (DSM) with two simple operations,
so that the model attention towards the motion information is better paid.
Specifically, we construct a positive clip and a negative clip for each video.
Compared to the original video, the positive/negative is
motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance
and Temporal Local Disturbance. Our objective is to pull the positive closer
while pushing the negative farther to the original clip in the latent space. In
this way, the impact of the scene is weakened while the temporal sensitivity of
the network is further enhanced. We conduct experiments on two tasks with
various backbones and different pre-training datasets, and find that our method
surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards
action recognition task on the UCF101 and HMDB51 datasets respectively using
the same backbone.
</p>
<a href="http://arxiv.org/abs/2009.05757" target="_blank">arXiv:2009.05757</a> [<a href="http://arxiv.org/pdf/2009.05757" target="_blank">pdf</a>]

<h2>Collaborative Group Learning. (arXiv:2009.07712v3 [cs.LG] UPDATED)</h2>
<h3>Shaoxiong Feng, Hongshen Chen, Xuancheng Ren, Zhuoye Ding, Kan Li, Xu Sun</h3>
<p>Collaborative learning has successfully applied knowledge transfer to guide a
pool of small student networks towards robust local minima. However, previous
approaches typically struggle with drastically aggravated student
homogenization when the number of students rises. In this paper, we propose
Collaborative Group Learning, an efficient framework that aims to diversify the
feature representation and conduct an effective regularization. Intuitively,
similar to the human group study mechanism, we induce students to learn and
exchange different parts of course knowledge as collaborative groups. First,
each student is established by randomly routing on a modular neural network,
which facilitates flexible knowledge communication between students due to
random levels of representation sharing and branching. Second, to resist the
student homogenization, students first compose diverse feature sets by
exploiting the inductive bias from sub-sets of training data, and then
aggregate and distill different complementary knowledge by imitating a random
sub-group of students at each time step. Overall, the above mechanisms are
beneficial for maximizing the student population to further improve the model
generalization without sacrificing computational efficiency. Empirical
evaluations on both image and text tasks indicate that our method significantly
outperforms various state-of-the-art collaborative approaches whilst enhancing
computational efficiency.
</p>
<a href="http://arxiv.org/abs/2009.07712" target="_blank">arXiv:2009.07712</a> [<a href="http://arxiv.org/pdf/2009.07712" target="_blank">pdf</a>]

<h2>DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2009.08753v2 [cs.CV] UPDATED)</h2>
<h3>Yan Hong, Li Niu, Jianfu Zhang, Jing Liang, Liqing Zhang</h3>
<p>Learning to generate new images for a novel category based on only a few
images, named as few-shot image generation, has attracted increasing research
interest. Several state-of-the-art works have yielded impressive results, but
the diversity is still limited. In this work, we propose a novel Delta
Generative Adversarial Network (DeltaGAN), which consists of a reconstruction
subnetwork and a generation subnetwork. The reconstruction subnetwork captures
intra-category transformation, i.e., "delta", between same-category pairs. The
generation subnetwork generates sample-specific "delta" for an input image,
which is combined with this input image to generate a new image within the same
category. Besides, an adversarial delta matching loss is designed to link the
above two subnetworks together. Extensive experiments on five few-shot image
datasets demonstrate the effectiveness of our proposed method.
</p>
<a href="http://arxiv.org/abs/2009.08753" target="_blank">arXiv:2009.08753</a> [<a href="http://arxiv.org/pdf/2009.08753" target="_blank">pdf</a>]

<h2>Few-shot Font Generation with Localized Style Representations and Factorization. (arXiv:2009.11042v2 [cs.CV] UPDATED)</h2>
<h3>Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim</h3>
<p>Automatic few-shot font generation is a practical and widely studied problem
because manual designs are expensive and sensitive to the expertise of
designers. Existing few-shot font generation methods aim to learn to
disentangle the style and content element from a few reference glyphs, and
mainly focus on a universal style representation for each font style. However,
such approach limits the model in representing diverse local styles, and thus
makes it unsuitable to the most complicated letter system, e.g., Chinese, whose
characters consist of a varying number of components (often called "radical")
with a highly complex structure. In this paper, we propose a novel font
generation method by learning localized styles, namely component-wise style
representations, instead of universal styles. The proposed style
representations enable us to synthesize complex local details in text designs.
However, learning component-wise styles solely from reference glyphs is
infeasible in the few-shot font generation scenario, when a target script has a
large number of components, e.g., over 200 for Chinese. To reduce the number of
reference glyphs, we simplify component-wise styles by a product of component
factor and style factor, inspired by low-rank matrix factorization. Thanks to
the combination of strong representation and a compact factorization strategy,
our method shows remarkably better few-shot font generation results (with only
8 reference glyph images) than other state-of-the-arts, without utilizing
strong locality supervision, e.g., location of each component, skeleton, or
strokes. The source code is available at https://github.com/clovaai/lffont.
</p>
<a href="http://arxiv.org/abs/2009.11042" target="_blank">arXiv:2009.11042</a> [<a href="http://arxiv.org/pdf/2009.11042" target="_blank">pdf</a>]

<h2>CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq. (arXiv:2009.11403v2 [cs.AI] UPDATED)</h2>
<h3>Koundinya Vajjha, Avraham Shinnar, Vasily Pestun, Barry Trager, Nathan Fulton</h3>
<p>Reinforcement learning algorithms solve sequential decision-making problems
in probabilistic environments by optimizing for long-term reward. The desire to
use reinforcement learning in safety-critical settings inspires a recent line
of work on formally constrained reinforcement learning; however, these methods
place the implementation of the learning algorithm in their Trusted Computing
Base. The crucial correctness property of these implementations is a guarantee
that the learning algorithm converges to an optimal policy. This paper begins
the work of closing this gap by developing a Coq formalization of two canonical
reinforcement learning algorithms: value and policy iteration for finite state
Markov decision processes. The central results are a formalization of Bellman's
optimality principle and its proof, which uses a contraction property of
Bellman optimality operator to establish that a sequence converges in the
infinite horizon limit. The CertRL development exemplifies how the Giry monad
and mechanized metric coinduction streamline optimality proofs for
reinforcement learning algorithms. The CertRL library provides a general
framework for proving properties about Markov decision processes and
reinforcement learning algorithms, paving the way for further work on
formalization of reinforcement learning algorithms.
</p>
<a href="http://arxiv.org/abs/2009.11403" target="_blank">arXiv:2009.11403</a> [<a href="http://arxiv.org/pdf/2009.11403" target="_blank">pdf</a>]

<h2>Near-Optimal MNL Bandits Under Risk Criteria. (arXiv:2009.12511v2 [cs.LG] UPDATED)</h2>
<h3>Guangyu Xi, Chao Tao, Yuan Zhou</h3>
<p>We study MNL bandits, which is a variant of the traditional multi-armed
bandit problem, under risk criteria. Unlike the ordinary expected revenue, risk
criteria are more general goals widely used in industries and bussiness. We
design algorithms for a broad class of risk criteria, including but not limited
to the well-known conditional value-at-risk, Sharpe ratio and entropy risk, and
prove that they suffer a near-optimal regret. As a complement, we also conduct
experiments with both synthetic and real data to show the empirical performance
of our proposed algorithms.
</p>
<a href="http://arxiv.org/abs/2009.12511" target="_blank">arXiv:2009.12511</a> [<a href="http://arxiv.org/pdf/2009.12511" target="_blank">pdf</a>]

<h2>Complexity and Algorithms for Exploiting Quantal Opponents in Large Two-Player Games. (arXiv:2009.14521v2 [cs.AI] UPDATED)</h2>
<h3>David Milec, Jakub &#x10c;ern&#xfd;, Viliam Lis&#xfd;, Bo An</h3>
<p>Solution concepts of traditional game theory assume entirely rational
players; therefore, their ability to exploit subrational opponents is limited.
One type of subrationality that describes human behavior well is the quantal
response. While there exist algorithms for computing solutions against quantal
opponents, they either do not scale or may provide strategies that are even
worse than the entirely-rational Nash strategies. This paper aims to analyze
and propose scalable algorithms for computing effective and robust strategies
against a quantal opponent in normal-form and extensive-form games. Our
contributions are: (1) we define two different solution concepts related to
exploiting quantal opponents and analyze their properties; (2) we prove that
computing these solutions is computationally hard; (3) therefore, we evaluate
several heuristic approximations based on scalable counterfactual regret
minimization (CFR); and (4) we identify a CFR variant that exploits the bounded
opponents better than the previously used variants while being less exploitable
by the worst-case perfectly-rational opponent.
</p>
<a href="http://arxiv.org/abs/2009.14521" target="_blank">arXiv:2009.14521</a> [<a href="http://arxiv.org/pdf/2009.14521" target="_blank">pdf</a>]

<h2>Learning Rewards from Linguistic Feedback. (arXiv:2009.14715v2 [cs.AI] UPDATED)</h2>
<h3>Theodore R. Sumers, Mark K. Ho, Robert D. Hawkins, Karthik Narasimhan, Thomas L. Griffiths</h3>
<p>We explore unconstrained natural language feedback as a learning signal for
artificial agents. Humans use rich and varied language to teach, yet most prior
work on interactive learning from language assumes a particular form of input
(e.g., commands). We propose a general framework which does not make this
assumption, using aspect-based sentiment analysis to decompose feedback into
sentiment about the features of a Markov decision process. We then perform an
analogue of inverse reinforcement learning, regressing the sentiment on the
features to infer the teacher's latent reward function. To evaluate our
approach, we first collect a corpus of teaching behavior in a cooperative task
where both teacher and learner are human. We implement three artificial
learners: sentiment-based "literal" and "pragmatic" models, and an inference
network trained end-to-end to predict latent rewards. We then repeat our
initial experiment and pair them with human teachers. All three successfully
learn from interactive human feedback. The sentiment models outperform the
inference network, with the "pragmatic" model approaching human performance.
Our work thus provides insight into the information structure of naturalistic
linguistic feedback as well as methods to leverage it for reinforcement
learning.
</p>
<a href="http://arxiv.org/abs/2009.14715" target="_blank">arXiv:2009.14715</a> [<a href="http://arxiv.org/pdf/2009.14715" target="_blank">pdf</a>]

<h2>Early Bird: Loop Closures from Opposing Viewpoints for Perceptually-Aliased Indoor Environments. (arXiv:2010.01421v2 [cs.CV] UPDATED)</h2>
<h3>Satyajit Tourani, Dhagash Desai, Udit Singh Parihar, Sourav Garg, Ravi Kiran Sarvadevabhatla, K. Madhava Krishna</h3>
<p>Significant advances have been made recently in Visual Place Recognition
(VPR), feature correspondence, and localization due to the proliferation of
deep-learning-based methods. However, existing approaches tend to address,
partially or fully, only one of two key challenges: viewpoint change and
perceptual aliasing. In this paper, we present novel research that
simultaneously addresses both challenges by combining deep-learned features
with geometric transformations based on reasonable domain assumptions about
navigation on a ground-plane, whilst also removing the requirement for
specialized hardware setup (e.g. lighting, downwards facing cameras). In
particular, our integration of VPR with SLAM by leveraging the robustness of
deep-learned features and our homography-based extreme viewpoint invariance
significantly boosts the performance of VPR, feature correspondence, and pose
graph submodules of the SLAM pipeline. For the first time, we demonstrate a
localization system capable of state-of-the-art performance despite perceptual
aliasing and extreme 180-degree-rotated viewpoint change in a range of
real-world and simulated experiments. Our system is able to achieve early loop
closures that prevent significant drifts in SLAM trajectories. We also compare
extensively several deep architectures for VPR and descriptor matching. We also
show that superior place recognition and descriptor matching across opposite
views results in a similar performance gain in back-end pose graph
optimization.
</p>
<a href="http://arxiv.org/abs/2010.01421" target="_blank">arXiv:2010.01421</a> [<a href="http://arxiv.org/pdf/2010.01421" target="_blank">pdf</a>]

<h2>Meta-Aggregation Networks for Class-Incremental Learning. (arXiv:2010.05063v2 [cs.CV] UPDATED)</h2>
<h3>Yaoyao Liu, Bernt Schiele, Qianru Sun</h3>
<p>Class-Incremental Learning (CIL) aims to learn a classification model with
the number of classes increasing phase-by-phase. An inherent problem in CIL is
the stability-plasticity dilemma between the learning of old and new classes,
i.e., high-plasticity models easily forget old classes but high-stability
models are weak to learn new classes. We alleviate this issue by proposing a
novel network architecture called Meta-Aggregation Networks (MANets) in which
we explicitly build two residual blocks at each residual level (taking ResNet
as the baseline architecture): a stable block and a plastic block. We aggregate
the output feature maps from these two blocks and then feed the results to the
next-level blocks. We meta-learn the aggregation weights in order to
dynamically optimize and balance between the two types of blocks, i.e.,
inherently between stability and plasticity. We conduct extensive experiments
on three CIL benchmarks: CIFAR-100, ImageNet-Subset, and ImageNet, and show
that many existing CIL methods can be straightforwardly incorporated on the
architecture of MANets to boost their performances.
</p>
<a href="http://arxiv.org/abs/2010.05063" target="_blank">arXiv:2010.05063</a> [<a href="http://arxiv.org/pdf/2010.05063" target="_blank">pdf</a>]

<h2>Thinking Fast and Slow in AI. (arXiv:2010.06002v2 [cs.AI] UPDATED)</h2>
<h3>Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, Biplav Srivastava</h3>
<p>This paper proposes a research direction to advance AI which draws
inspiration from cognitive theories of human decision making. The premise is
that if we gain insights about the causes of some human capabilities that are
still lacking in AI (for instance, adaptability, generalizability, common
sense, and causal reasoning), we may obtain similar capabilities in an AI
system by embedding these causal components. We hope that the high-level
description of our vision included in this paper, as well as the several
research questions that we propose to consider, can stimulate the AI research
community to define, try and evaluate new methodologies, frameworks, and
evaluation metrics, in the spirit of achieving a better understanding of both
human and machine intelligence.
</p>
<a href="http://arxiv.org/abs/2010.06002" target="_blank">arXiv:2010.06002</a> [<a href="http://arxiv.org/pdf/2010.06002" target="_blank">pdf</a>]

<h2>A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels. (arXiv:2010.10952v3 [cs.LG] UPDATED)</h2>
<h3>Leon Lang, Maurice Weiler</h3>
<p>Group equivariant convolutional networks (GCNNs) endow classical
convolutional networks with additional symmetry priors, which can lead to a
considerably improved performance. Recent advances in the theoretical
description of GCNNs revealed that such models can generally be understood as
performing convolutions with G-steerable kernels, that is, kernels that satisfy
an equivariance constraint themselves. While the G-steerability constraint has
been derived, it has to date only been solved for specific use cases - a
general characterization of G-steerable kernel spaces is still missing. This
work provides such a characterization for the practically relevant case of G
being any compact group. Our investigation is motivated by a striking analogy
between the constraints underlying steerable kernels on the one hand and
spherical tensor operators from quantum mechanics on the other hand. By
generalizing the famous Wigner-Eckart theorem for spherical tensor operators,
we prove that steerable kernel spaces are fully understood and parameterized in
terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan
coefficients, and 3) harmonic basis functions on homogeneous spaces.
</p>
<a href="http://arxiv.org/abs/2010.10952" target="_blank">arXiv:2010.10952</a> [<a href="http://arxiv.org/pdf/2010.10952" target="_blank">pdf</a>]

<h2>Regularised Least-Squares Regression with Infinite-Dimensional Output Space. (arXiv:2010.10973v4 [stat.ML] UPDATED)</h2>
<h3>Junhyunng Park, Krikamol Muandet</h3>
<p>We present some learning theory results on vector-valued reproducing kernel
Hilbert space (RKHS) regression, where the input space is allowed to be
non-compact and the output space is a (possibly infinite-dimensional) Hilbert
space.
</p>
<a href="http://arxiv.org/abs/2010.10973" target="_blank">arXiv:2010.10973</a> [<a href="http://arxiv.org/pdf/2010.10973" target="_blank">pdf</a>]

<h2>On the Relationship Between KR Approaches for Explainable Planning. (arXiv:2011.09006v3 [cs.AI] UPDATED)</h2>
<h3>Stylianos Loukas Vasileiou, William Yeoh, Tran Cao Son</h3>
<p>In this paper, we build upon notions from knowledge representation and
reasoning (KR) to expand a preliminary logic-based framework that characterizes
the model reconciliation problem for explainable planning. We also provide a
detailed exposition on the relationship between similar KR techniques, such as
abductive explanations and belief change, and their applicability to
explainable planning.
</p>
<a href="http://arxiv.org/abs/2011.09006" target="_blank">arXiv:2011.09006</a> [<a href="http://arxiv.org/pdf/2011.09006" target="_blank">pdf</a>]

<h2>APAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding. (arXiv:2011.11545v3 [cs.AI] UPDATED)</h2>
<h3>Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang, Xinwen Wang, Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, Zhenyu Guo, Junkui Li</h3>
<p>Limited by the time complexity of querying k-hop neighbors in a graph
database, most graph algorithms cannot be deployed online and execute
millisecond-level inference. This problem dramatically limits the potential of
applying graph algorithms in certain areas, such as financial fraud detection.
Therefore, we propose Asynchronous Propagation Attention Network, an
asynchronous continuous time dynamic graph algorithm for real-time temporal
graph embedding. Traditional graph models usually execute two serial
operations: first graph computation and then model inference. We decouple model
inference and graph computation step so that the heavy graph query operations
will not damage the speed of model inference. Extensive experiments demonstrate
that the proposed method can achieve competitive performance and 8.7 times
inference speed improvement in the meantime.
</p>
<a href="http://arxiv.org/abs/2011.11545" target="_blank">arXiv:2011.11545</a> [<a href="http://arxiv.org/pdf/2011.11545" target="_blank">pdf</a>]

<h2>Implicit bias of deep linear networks in the large learning rate phase. (arXiv:2011.12547v2 [cs.LG] UPDATED)</h2>
<h3>Wei Huang, Weitao Du, Richard Yi Da Xu, Chunrui Liu</h3>
<p>Most theoretical studies explaining the regularization effect in deep
learning have only focused on gradient descent with a sufficient small learning
rate or even gradient flow (infinitesimal learning rate). Such researches,
however, have neglected a reasonably large learning rate applied in most
practical applications. In this work, we characterize the implicit bias effect
of deep linear networks for binary classification using the logistic loss in
the large learning rate regime, inspired by the seminal work by Lewkowycz et
al. [26] in a regression setting with squared loss. They found a learning rate
regime with a large stepsize named the catapult phase, where the loss grows at
the early stage of training and eventually converges to a minimum that is
flatter than those found in the small learning rate regime. We claim that
depending on the separation conditions of data, the gradient descent iterates
will converge to a flatter minimum in the catapult phase. We rigorously prove
this claim under the assumption of degenerate data by overcoming the difficulty
of the non-constant Hessian of logistic loss and further characterize the
behavior of loss and Hessian for non-separable data. Finally, we demonstrate
that flatter minima in the space spanned by non-separable data along with the
learning rate in the catapult phase can lead to better generalization
empirically.
</p>
<a href="http://arxiv.org/abs/2011.12547" target="_blank">arXiv:2011.12547</a> [<a href="http://arxiv.org/pdf/2011.12547" target="_blank">pdf</a>]

<h2>Risk-Monotonicity via Distributional Robustness. (arXiv:2011.14126v2 [cs.LG] UPDATED)</h2>
<h3>Zakaria Mhammedi, Hisham Husain</h3>
<p>Acquisition of data is a difficult task in most applications of Machine
Learning (ML), and it is only natural that one hopes and expects lower
populating risk (better performance) with increasing data points. It turns out,
somewhat surprisingly, that this is not the case even for the most standard
algorithms such as the Empirical Risk Minimizer (ERM). Non-monotonic behaviour
of the risk and instability in training have manifested and appeared in the
popular deep learning paradigm under the description of double descent. These
problems not only highlight our lack of understanding of learning algorithms
and generalization but rather render our efforts at data acquisition in vain.
It is, therefore, crucial to pursue this concern and provide a characterization
of such behaviour. In this paper, we derive the first consistent and
risk-monotonic algorithms for a general statistical learning setting under weak
assumptions, consequently resolving an open problem (Viering et al. 2019) on
how to avoid non-monotonic behaviour of risk curves. Our algorithms make use of
Distributionally Robust Optimization (DRO) -- a technique that has shown
promise in other complications of deep learning such as adversarial training.
Our work makes a significant contribution to the topic of risk-monotonicity,
which may be key in resolving empirical phenomena such as double descent.
</p>
<a href="http://arxiv.org/abs/2011.14126" target="_blank">arXiv:2011.14126</a> [<a href="http://arxiv.org/pdf/2011.14126" target="_blank">pdf</a>]

<h2>One-Pixel Attack Deceives Automatic Detection of Breast Cancer. (arXiv:2012.00517v2 [cs.CV] UPDATED)</h2>
<h3>Joni Korpihalkola, Tuomo Sipola, Samir Puuska, Tero Kokkonen</h3>
<p>In this article we demonstrate that a state-of-the-art machine learning model
predicting whether a whole slide image contains mitosis can be fooled by
changing just a single pixel in the input image. Computer vision and machine
learning can be used to automate various tasks in cancer diagnostic and
detection. If an attacker can manipulate the automated processing, the results
can be devastating and in the worst case lead to wrong diagnostic and
treatments. In this research one-pixel attack is demonstrated in a real-life
scenario with a real tumor dataset. The results indicate that a minor one-pixel
modification of a whole slide image under analysis can affect the diagnosis.
The attack poses a threat from the cyber security perspective: the one-pixel
method can be used as an attack vector by a motivated attacker.
</p>
<a href="http://arxiv.org/abs/2012.00517" target="_blank">arXiv:2012.00517</a> [<a href="http://arxiv.org/pdf/2012.00517" target="_blank">pdf</a>]

<h2>Active Learning: Problem Settings and Recent Developments. (arXiv:2012.04225v2 [cs.LG] UPDATED)</h2>
<h3>Hideitsu Hino</h3>
<p>In supervised learning, acquiring labeled training data for a predictive
model can be very costly, but acquiring a large amount of unlabeled data is
often quite easy. Active learning is a method of obtaining predictive models
with high precision at a limited cost through the adaptive selection of samples
for labeling. This paper explains the basic problem settings of active learning
and recent research trends. In particular, research on learning acquisition
functions to select samples from the data for labeling, theoretical work on
active learning algorithms, and stopping criteria for sequential data
acquisition are highlighted. Application examples for material development and
measurement are introduced.
</p>
<a href="http://arxiv.org/abs/2012.04225" target="_blank">arXiv:2012.04225</a> [<a href="http://arxiv.org/pdf/2012.04225" target="_blank">pdf</a>]

<h2>The Counterfactual NESS Definition of Causation. (arXiv:2012.05123v2 [cs.AI] UPDATED)</h2>
<h3>Sander Beckers</h3>
<p>In previous work with Joost Vennekens I proposed a definition of actual
causation that is based on certain plausible principles, thereby allowing the
debate on causation to shift away from its heavy focus on examples towards a
more systematic analysis. This paper contributes to that analysis in two ways.
First, I show that our definition is in fact a formalization of Wright's famous
NESS definition of causation combined with a counterfactual difference-making
condition. This means that our definition integrates two highly influential
approaches to causation that are claimed to stand in opposition to each other.
Second, I modify our definition to offer a substantial improvement: I weaken
the difference-making condition in such a way that it avoids the problematic
analysis of cases of preemption. The resulting Counterfactual NESS definition
of causation forms a natural compromise between counterfactual approaches and
the NESS approach.
</p>
<a href="http://arxiv.org/abs/2012.05123" target="_blank">arXiv:2012.05123</a> [<a href="http://arxiv.org/pdf/2012.05123" target="_blank">pdf</a>]

<h2>Fully-Automated Liver Tumor Localization and Characterization from Multi-Phase MR Volumes Using Key-Slice ROI Parsing: A Physician-Inspired Approach. (arXiv:2012.06964v2 [cs.CV] UPDATED)</h2>
<h3>Bolin Lai, Xiaoyu Bai, Yuhsuan Wu, Xiao-Yun Zhou, Peng Wang, Jinzheng Cai, Yuankai Huo, Lingyun Huang, Yong Xia, Jing Xiao, Le Lu, Heping Hu, Adam Harrison</h3>
<p>Using radiological scans to identify liver tumors is crucial for proper
patient treatment. This is highly challenging, as top radiologists only achieve
F1 scores of roughly 80% (hepatocellular carcinoma (HCC) vs. others) with only
moderate inter-rater agreement, even when using multi-phase magnetic resonance
(MR) imagery. Thus, there is great impetus for computer-aided diagnosis (CAD)
solutions. A critical challengeis to reliably parse a 3D MR volume to localize
diagnosable regions of interest (ROI). In this paper, we break down this
problem using a key-slice parser (KSP), which emulates physician workflows by
first identifying key slices and then localize their corresponding key ROIs.
Because performance demands are so extreme, (not to miss any key ROI),our KSP
integrates complementary modules--top-down classification-plus-detection (CPD)
and bottom-up localization-by-over-segmentation(LBOS). The CPD uses a
curve-parsing and detection confidence to re-weight classifier confidences. The
LBOS uses over-segmentation to flag CPD failure cases and provides its own
ROIs. For scalability, LBOS is only weakly trained on pseudo-masks using a new
distance-aware Tversky loss. We evaluate our approach on the largest
multi-phase MR liver lesion test dataset to date (430 biopsy-confirmed
patients). Experiments demonstrate that our KSP can localize diagnosable ROIs
with high reliability (85% patients have an average overlap of &gt;= 40% with the
ground truth). Moreover, we achieve an HCC vs. others F1 score of 0.804,
providing a fully-automated CAD solution comparable with top human physicians.
</p>
<a href="http://arxiv.org/abs/2012.06964" target="_blank">arXiv:2012.06964</a> [<a href="http://arxiv.org/pdf/2012.06964" target="_blank">pdf</a>]

<h2>Predicting Generalization in Deep Learning via Local Measures of Distortion. (arXiv:2012.06969v2 [stat.ML] UPDATED)</h2>
<h3>Abhejit Rajagopal, Vamshi C. Madala, Shivkumar Chandrasekaran, Peder E. Z. Larson</h3>
<p>We study generalization in deep learning by appealing to complexity measures
originally developed in approximation and information theory. While these
concepts are challenged by the high-dimensional and data-defined nature of deep
learning, we show that simple vector quantization approaches such as PCA, GMMs,
and SVMs capture their spirit when applied layer-wise to deep extracted
features giving rise to relatively inexpensive complexity measures that
correlate well with generalization performance. We discuss our results in 2020
NeurIPS PGDL challenge.
</p>
<a href="http://arxiv.org/abs/2012.06969" target="_blank">arXiv:2012.06969</a> [<a href="http://arxiv.org/pdf/2012.06969" target="_blank">pdf</a>]

<h2>A One-Size-Fits-All Solution to Conservative Bandit Problems. (arXiv:2012.07341v3 [cs.LG] UPDATED)</h2>
<h3>Yihan Du, Siwei Wang, Longbo Huang</h3>
<p>In this paper, we study a family of conservative bandit problems (CBPs) with
sample-path reward constraints, i.e., the learner's reward performance must be
at least as well as a given baseline at any time. We propose a
One-Size-Fits-All solution to CBPs and present its applications to three
encompassed problems, i.e. conservative multi-armed bandits (CMAB),
conservative linear bandits (CLB) and conservative contextual combinatorial
bandits (CCCB). Different from previous works which consider high probability
constraints on the expected reward, we focus on a sample-path constraint on the
actually received reward, and achieve better theoretical guarantees
($T$-independent additive regrets instead of $T$-dependent) and empirical
performance. Furthermore, we extend the results and consider a novel
conservative mean-variance bandit problem (MV-CBP), which measures the learning
performance with both the expected reward and variability. For this extended
problem, we provide a novel algorithm with $O(1/T)$ normalized additive regrets
($T$-independent in the cumulative form) and validate this result through
empirical evaluation.
</p>
<a href="http://arxiv.org/abs/2012.07341" target="_blank">arXiv:2012.07341</a> [<a href="http://arxiv.org/pdf/2012.07341" target="_blank">pdf</a>]

<h2>Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings. (arXiv:2012.07412v2 [cs.LG] UPDATED)</h2>
<h3>Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, David Wipf</h3>
<p>Cycle-consistent training is widely used for jointly learning a forward and
inverse mapping between two domains of interest without the cumbersome
requirement of collecting matched pairs within each domain. In this regard, the
implicit assumption is that there exists (at least approximately) a
ground-truth bijection such that a given input from either domain can be
accurately reconstructed from successive application of the respective
mappings. But in many applications no such bijection can be expected to exist
and large reconstruction errors can compromise the success of cycle-consistent
training. As one important instance of this limitation, we consider
practically-relevant situations where there exists a many-to-one or surjective
mapping between domains. To address this regime, we develop a conditional
variational autoencoder (CVAE) approach that can be viewed as converting
surjective mappings to implicit bijections whereby reconstruction errors in
both directions can be minimized, and as a natural byproduct, realistic output
diversity can be obtained in the one-to-many direction. As theoretical
motivation, we analyze a simplified scenario whereby minima of the proposed
CVAE-based energy function align with the recovery of ground-truth surjective
mappings. On the empirical side, we consider a synthetic image dataset with
known ground-truth, as well as a real-world application involving natural
language generation from knowledge graphs and vice versa, a prototypical
surjective case. For the latter, our CVAE pipeline can capture such many-to-one
mappings during cycle training while promoting textural diversity for
graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT
</p>
<a href="http://arxiv.org/abs/2012.07412" target="_blank">arXiv:2012.07412</a> [<a href="http://arxiv.org/pdf/2012.07412" target="_blank">pdf</a>]

<h2>Squirrel: A Switching Hyperparameter Optimizer. (arXiv:2012.08180v2 [cs.LG] UPDATED)</h2>
<h3>Noor Awad, Gresa Shala, Difan Deng, Neeratyoy Mallik, Matthias Feurer, Katharina Eggensperger, Andre&#x27; Biedenkapp, Diederick Vermetten, Hao Wang, Carola Doerr, Marius Lindauer, Frank Hutter</h3>
<p>In this short note, we describe our submission to the NeurIPS 2020 BBO
challenge. Motivated by the fact that different optimizers work well on
different problems, our approach switches between different optimizers. Since
the team names on the competition's leaderboard were randomly generated
"alliteration nicknames", consisting of an adjective and an animal with the
same initial letter, we called our approach the Switching Squirrel, or here,
short, Squirrel.
</p>
<a href="http://arxiv.org/abs/2012.08180" target="_blank">arXiv:2012.08180</a> [<a href="http://arxiv.org/pdf/2012.08180" target="_blank">pdf</a>]

<h2>Seeing Behind Objects for 3D Multi-Object Tracking in RGB-D Sequences. (arXiv:2012.08197v2 [cs.CV] UPDATED)</h2>
<h3>Norman M&#xfc;ller, Yu-Shiang Wong, Niloy J. Mitra, Angela Dai, Matthias Nie&#xdf;ner</h3>
<p>Multi-object tracking from RGB-D video sequences is a challenging problem due
to the combination of changing viewpoints, motion, and occlusions over time. We
observe that having the complete geometry of objects aids in their tracking,
and thus propose to jointly infer the complete geometry of objects as well as
track them, for rigidly moving objects over time. Our key insight is that
inferring the complete geometry of the objects significantly helps in tracking.
By hallucinating unseen regions of objects, we can obtain additional
correspondences between the same instance, thus providing robust tracking even
under strong change of appearance. From a sequence of RGB-D frames, we detect
objects in each frame and learn to predict their complete object geometry as
well as a dense correspondence mapping into a canonical space. This allows us
to derive 6DoF poses for the objects in each frame, along with their
correspondence between frames, providing robust object tracking across the
RGB-D sequence. Experiments on both synthetic and real-world RGB-D data
demonstrate that we achieve state-of-the-art performance on dynamic object
tracking. Furthermore, we show that our object completion significantly helps
tracking, providing an improvement of $6.5\%$ in mean MOTA.
</p>
<a href="http://arxiv.org/abs/2012.08197" target="_blank">arXiv:2012.08197</a> [<a href="http://arxiv.org/pdf/2012.08197" target="_blank">pdf</a>]

<h2>Canny-VO: Visual Odometry with RGB-D Cameras based on Geometric 3D-2D Edge Alignment. (arXiv:2012.08228v2 [cs.CV] UPDATED)</h2>
<h3>Yi Zhou, Hongdong Li, Laurent Kneip</h3>
<p>The present paper reviews the classical problem of free-form curve
registration and applies it to an efficient RGBD visual odometry system called
Canny-VO, as it efficiently tracks all Canny edge features extracted from the
images. Two replacements for the distance transformation commonly used in edge
registration are proposed: Approximate Nearest Neighbour Fields and Oriented
Nearest Neighbour Fields. 3D2D edge alignment benefits from these alternative
formulations in terms of both efficiency and accuracy. It removes the need for
the more computationally demanding paradigms of datato-model registration,
bilinear interpolation, and sub-gradient computation. To ensure robustness of
the system in the presence of outliers and sensor noise, the registration is
formulated as a maximum a posteriori problem, and the resulting weighted least
squares objective is solved by the iteratively re-weighted least squares
method. A variety of robust weight functions are investigated and the optimal
choice is made based on the statistics of the residual errors. Efficiency is
furthermore boosted by an adaptively sampled definition of the nearest
neighbour fields. Extensive evaluations on public SLAM benchmark sequences
demonstrate state-of-the-art performance and an advantage over classical
Euclidean distance fields.
</p>
<a href="http://arxiv.org/abs/2012.08228" target="_blank">arXiv:2012.08228</a> [<a href="http://arxiv.org/pdf/2012.08228" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Global Factors in Deep Generative Models. (arXiv:2012.08234v2 [cs.LG] UPDATED)</h2>
<h3>Ignacio Peis, Pablo M. Olmos, Antonio Art&#xe9;s-Rodr&#xed;guez</h3>
<p>We present a novel deep generative model based on non i.i.d. variational
autoencoders that captures global dependencies among observations in a fully
unsupervised fashion. In contrast to the recent semi-supervised alternatives
for global modeling in deep generative models, our approach combines a mixture
model in the local or data-dependent space and a global Gaussian latent
variable, which lead us to obtain three particular insights. First, the induced
latent global space captures interpretable disentangled representations with no
user-defined regularization in the evidence lower bound (as in $\beta$-VAE and
its generalizations). Second, we show that the model performs domain alignment
to find correlations and interpolate between different databases. Finally, we
study the ability of the global space to discriminate between groups of
observations with non-trivial underlying structures, such as face images with
shared attributes or defined sequences of digits images.
</p>
<a href="http://arxiv.org/abs/2012.08234" target="_blank">arXiv:2012.08234</a> [<a href="http://arxiv.org/pdf/2012.08234" target="_blank">pdf</a>]

<h2>Bayes Meets Entailment and Prediction: Commonsense Reasoning with Non-monotonicity, Paraconsistency and Predictive Accuracy. (arXiv:2012.08479v2 [cs.AI] UPDATED)</h2>
<h3>Hiroyuki Kido, Keishi Okamoto</h3>
<p>The recent success of Bayesian methods in neuroscience and artificial
intelligence gives rise to the hypothesis that the brain is a Bayesian machine.
Since logic and learning are both practices of the human brain, it leads to
another hypothesis that there is a Bayesian interpretation underlying both
logical reasoning and machine learning. In this paper, we introduce a
generative model of logical consequence relations. It formalises the process of
how the truth value of a sentence is probabilistically generated from the
probability distribution over states of the world. We show that the generative
model characterises a classical consequence relation, paraconsistent
consequence relation and nonmonotonic consequence relation. In particular, the
generative model gives a new consequence relation that outperforms them in
reasoning with inconsistent knowledge. We also show that the generative model
gives a new classification algorithm that outperforms several representative
algorithms in predictive accuracy and complexity on the Kaggle Titanic dataset.
</p>
<a href="http://arxiv.org/abs/2012.08479" target="_blank">arXiv:2012.08479</a> [<a href="http://arxiv.org/pdf/2012.08479" target="_blank">pdf</a>]

<h2>Amazon SageMaker Autopilot: a white box AutoML solution at scale. (arXiv:2012.08483v2 [cs.LG] UPDATED)</h2>
<h3>Piali Das, Valerio Perrone, Nikita Ivkin, Tanya Bansal, Zohar Karnin, Huibin Shen, Iaroslav Shcherbatyi, Yotam Elor, Wilton Wu, Aida Zolic, Thibaut Lienart, Alex Tang, Amr Ahmed, Jean Baptiste Faddoul, Rodolphe Jenatton, Fela Winkelmolen, Philip Gautier, Leo Dirac, Andre Perunicic, Miroslav Miladinovic, Giovanni Zappella, C&#xe9;dric Archambeau, Matthias Seeger, Bhaskar Dutt, Laurence Rouesnel</h3>
<p>AutoML systems provide a black-box solution to machine learning problems by
selecting the right way of processing features, choosing an algorithm and
tuning the hyperparameters of the entire pipeline. Although these systems
perform well on many datasets, there is still a non-negligible number of
datasets for which the one-shot solution produced by each particular system
would provide sub-par performance. In this paper, we present Amazon SageMaker
Autopilot: a fully managed system providing an automated ML solution that can
be modified when needed. Given a tabular dataset and the target column name,
Autopilot identifies the problem type, analyzes the data and produces a diverse
set of complete ML pipelines including feature preprocessing and ML algorithms,
which are tuned to generate a leaderboard of candidate models. In the scenario
where the performance is not satisfactory, a data scientist is able to view and
edit the proposed ML pipelines in order to infuse their expertise and business
knowledge without having to revert to a fully manual solution. This paper
describes the different components of Autopilot, emphasizing the infrastructure
choices that allow scalability, high quality models, editable ML pipelines,
consumption of artifacts of offline meta-learning, and a convenient integration
with the entire SageMaker suite allowing these trained models to be used in a
production setting.
</p>
<a href="http://arxiv.org/abs/2012.08483" target="_blank">arXiv:2012.08483</a> [<a href="http://arxiv.org/pdf/2012.08483" target="_blank">pdf</a>]

<h2>A Unified Model for the Two-stage Offline-then-Online Resource Allocation. (arXiv:2012.06845v1 [cs.AI] CROSS LISTED)</h2>
<h3>Yifan Xu, Pan Xu, Jianping Pan, Jun Tao</h3>
<p>With the popularity of the Internet, traditional offline resource allocation
has evolved into a new form, called online resource allocation. It features the
online arrivals of agents in the system and the real-time decision-making
requirement upon the arrival of each online agent. Both offline and online
resource allocation have wide applications in various real-world matching
markets ranging from ridesharing to crowdsourcing. There are some emerging
applications such as rebalancing in bike sharing and trip-vehicle dispatching
in ridesharing, which involve a two-stage resource allocation process. The
process consists of an offline phase and another sequential online phase, and
both phases compete for the same set of resources. In this paper, we propose a
unified model which incorporates both offline and online resource allocation
into a single framework. Our model assumes non-uniform and known arrival
distributions for online agents in the second online phase, which can be
learned from historical data. We propose a parameterized linear programming
(LP)-based algorithm, which is shown to be at most a constant factor of $1/4$
from the optimal. Experimental results on the real dataset show that our
LP-based approaches outperform the LP-agnostic heuristics in terms of
robustness and effectiveness.
</p>
<a href="http://arxiv.org/abs/2012.06845" target="_blank">arXiv:2012.06845</a> [<a href="http://arxiv.org/pdf/2012.06845" target="_blank">pdf</a>]

<h2>Trading the System Efficiency for the Income Equality of Drivers in Rideshare. (arXiv:2012.06850v1 [cs.AI] CROSS LISTED)</h2>
<h3>Yifan Xu, Pan Xu</h3>
<p>Several scientific studies have reported the existence of the income gap
among rideshare drivers based on demographic factors such as gender, age, race,
etc. In this paper, we study the income inequality among rideshare drivers due
to discriminative cancellations from riders, and the tradeoff between the
income inequality (called fairness objective) with the system efficiency
(called profit objective). We proposed an online bipartite-matching model where
riders are assumed to arrive sequentially following a distribution known in
advance. The highlight of our model is the concept of acceptance rate between
any pair of driver-rider types, where types are defined based on demographic
factors. Specially, we assume each rider can accept or cancel the driver
assigned to her, each occurs with a certain probability which reflects the
acceptance degree from the rider type towards the driver type. We construct a
bi-objective linear program as a valid benchmark and propose two LP-based
parameterized online algorithms. Rigorous online competitive ratio analysis is
offered to demonstrate the flexibility and efficiency of our online algorithms
in balancing the two conflicting goals, promotions of fairness and profit.
Experimental results on a real-world dataset are provided as well, which
confirm our theoretical predictions.
</p>
<a href="http://arxiv.org/abs/2012.06850" target="_blank">arXiv:2012.06850</a> [<a href="http://arxiv.org/pdf/2012.06850" target="_blank">pdf</a>]

