---
title: Latest Deep Learning Papers
date: 2020-12-14 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (291 Articles)</h1>
<h2>Probabilistic Conditional System Invariant Generation with Bayesian Inference. (arXiv:2012.06615v1 [cs.RO])</h2>
<h3>Meriel Stein, Sebastian Elbaum, Lu Feng, Shili Sheng</h3>
<p>Invariants are a set of properties over program attributes that are expected
to be true during the execution of a program. Since developing those invariants
manually can be costly and challenging, there are a myriad of approaches that
support automated mining of likely invariants from sources such as program
traces. Existing approaches, however, are not equipped to capture the rich
states that condition the behavior of autonomous mobile robots, or to manage
the uncertainty associated with many variables in these systems. This means
that valuable invariants that appear only under specific states remain
uncovered. In this work we introduce an approach to infer conditional
probabilistic invariants to assist in the characterization of the behavior of
such rich stateful, stochastic systems. These probabilistic invariants can
encode a family of conditional patterns, are generated using Bayesian inference
to leverage observed trace data against priors gleaned from previous experience
and expert knowledge, and are ranked based on their surprise value and
information content. Our studies on two semi-autonomous mobile robotic systems
show how the proposed approach is able to generate valuable and previously
hidden stateful invariants.
</p>
<a href="http://arxiv.org/abs/2012.06615" target="_blank">arXiv:2012.06615</a> [<a href="http://arxiv.org/pdf/2012.06615" target="_blank">pdf</a>]

<h2>Street-view Panoramic Video Synthesis from a Single Satellite Image. (arXiv:2012.06628v1 [cs.CV])</h2>
<h3>Zuoyue Li, Zhaopeng Cui, Martin R. Oswald</h3>
<p>We present a novel method for synthesizing both temporally and geometrically
consistent street-view panoramic video from a given single satellite image and
camera trajectory. Existing cross-view synthesis approaches focus more on
images, while video synthesis in such a case has not yet received enough
attention. Single image synthesis approaches are not well suited for video
synthesis since they lack temporal consistency which is a crucial property of
videos. To this end, our approach explicitly creates a 3D point cloud
representation of the scene and maintains dense 3D-2D correspondences across
frames that reflect the geometric scene configuration inferred from the
satellite view. We implement a cascaded network architecture with two hourglass
modules for successive coarse and fine generation for colorizing the point
cloud from the semantics and per-class latent vectors. By leveraging computed
correspondences, the produced street-view video frames adhere to the 3D
geometric scene structure and maintain temporal consistency. Qualitative and
quantitative experiments demonstrate superior results compared to other
state-of-the-art cross-view synthesis approaches that either lack temporal or
geometric consistency. To the best of our knowledge, our work is the first work
to synthesize cross-view images to video.
</p>
<a href="http://arxiv.org/abs/2012.06628" target="_blank">arXiv:2012.06628</a> [<a href="http://arxiv.org/pdf/2012.06628" target="_blank">pdf</a>]

<h2>Regularizing Action Policies for Smooth Control with Reinforcement Learning. (arXiv:2012.06644v1 [cs.RO])</h2>
<h3>Siddharth Mysore, Bassel Mabsout, Renato Mancuso, Kate Saenko</h3>
<p>A critical problem with the practical utility of controllers trained with
deep Reinforcement Learning (RL) is the notable lack of smoothness in the
actions learned by the RL policies. This trend often presents itself in the
form of control signal oscillation and can result in poor control, high power
consumption, and undue system wear. We introduce Conditioning for Action Policy
Smoothness (CAPS), an effective yet intuitive regularization on action
policies, which offers consistent improvement in the smoothness of the learned
state-to-action mappings of neural network controllers, reflected in the
elimination of high-frequency components in the control signal. Tested on a
real system, improvements in controller smoothness on a quadrotor drone
resulted in an almost 80% reduction in power consumption while consistently
training flight-worthy controllers. Project website: this http URL
</p>
<a href="http://arxiv.org/abs/2012.06644" target="_blank">arXiv:2012.06644</a> [<a href="http://arxiv.org/pdf/2012.06644" target="_blank">pdf</a>]

<h2>D$^2$IM-Net: Learning Detail Disentangled Implicit Fields from Single Images. (arXiv:2012.06650v1 [cs.CV])</h2>
<h3>Manyi Li, Hao Zhang</h3>
<p>We present the first single-view 3D reconstruction network aimed at
recovering geometric details from an input image which encompass both
topological shape structures and surface features. Our key idea is to train the
network to learn a detail disentangled reconstruction consisting of two
functions, one implicit field representing the coarse 3D shape and the other
capturing the details. Given an input image, our network, coined D$^2$IM-Net,
encodes it into global and local features which are respectively fed into two
decoders. The base decoder uses the global features to reconstruct a coarse
implicit field, while the detail decoder reconstructs, from the local features,
two displacement maps, defined over the front and back sides of the captured
object. The final 3D reconstruction is a fusion between the base shape and the
displacement maps, with three losses enforcing the recovery of coarse shape,
overall structure, and surface details via a novel Laplacian term.
</p>
<a href="http://arxiv.org/abs/2012.06650" target="_blank">arXiv:2012.06650</a> [<a href="http://arxiv.org/pdf/2012.06650" target="_blank">pdf</a>]

<h2>Epistemic Logic of Know-Who. (arXiv:2012.06651v1 [cs.AI])</h2>
<h3>Sophia Epstein, Pavel Naumov</h3>
<p>The paper suggests a definition of "know who" as a modality using
Grove-Halpern semantics of names. It also introduces a logical system that
describes the interplay between modalities "knows who", "knows", and "for all
agents". The main technical result is a completeness theorem for the proposed
system.
</p>
<a href="http://arxiv.org/abs/2012.06651" target="_blank">arXiv:2012.06651</a> [<a href="http://arxiv.org/pdf/2012.06651" target="_blank">pdf</a>]

<h2>How to Train your Quadrotor: A Framework for Consistently Smooth and Responsive Flight Control via Reinforcement Learning. (arXiv:2012.06656v1 [cs.RO])</h2>
<h3>Siddharth Mysore, Bassel Mabsout, Kate Saenko, Renato Mancuso</h3>
<p>We focus on the problem of reliably training Reinforcement Learning (RL)
models (agents) for stable low-level control in embedded systems and test our
methods on a high-performance, custom-built quadrotor platform. A common but
often under-studied problem in developing RL agents for continuous control is
that the control policies developed are not always smooth. This lack of
smoothness can be a major problem when learning controllers %intended for
deployment on real hardware as it can result in control instability and
hardware failure. Issues of noisy control are further accentuated when training
RL agents in simulation due to simulators ultimately being imperfect
representations of reality - what is known as the reality gap. To combat issues
of instability in RL agents, we propose a systematic framework,
`REinforcement-based transferable Agents through Learning' (RE+AL), for
designing simulated training environments which preserve the quality of trained
agents when transferred to real platforms. RE+AL is an evolution of the
Neuroflight infrastructure detailed in technical reports prepared by members of
our research group. Neuroflight is a state-of-the-art framework for training RL
agents for low-level attitude control. RE+AL improves and completes Neuroflight
by solving a number of important limitations that hindered the deployment of
Neuroflight to real hardware. We benchmark RE+AL on the NF1 racing quadrotor
developed as part of Neuroflight. We demonstrate that RE+AL significantly
mitigates the previously observed issues of smoothness in RL agents.
Additionally, RE+AL is shown to consistently train agents that are
flight-capable and with minimal degradation in controller quality upon
transfer. RE+AL agents also learn to perform better than a tuned PID
controller, with better tracking errors, smoother control and reduced power
consumption.
</p>
<a href="http://arxiv.org/abs/2012.06656" target="_blank">arXiv:2012.06656</a> [<a href="http://arxiv.org/pdf/2012.06656" target="_blank">pdf</a>]

<h2>Protective Policy Transfer. (arXiv:2012.06662v1 [cs.RO])</h2>
<h3>Wenhao Yu, C. Karen Liu, Greg Turk</h3>
<p>Being able to transfer existing skills to new situations is a key capability
when training robots to operate in unpredictable real-world environments. A
successful transfer algorithm should not only minimize the number of samples
that the robot needs to collect in the new environment, but also prevent the
robot from damaging itself or the surrounding environment during the transfer
process. In this work, we introduce a policy transfer algorithm for adapting
robot motor skills to novel scenarios while minimizing serious failures. Our
algorithm trains two control policies in the training environment: a task
policy that is optimized to complete the task of interest, and a protective
policy that is dedicated to keep the robot from unsafe events (e.g. falling to
the ground). To decide which policy to use during execution, we learn a safety
estimator model in the training environment that estimates a continuous safety
level of the robot. When used with a set of thresholds, the safety estimator
becomes a classifier for switching between the protective policy and the task
policy. We evaluate our approach on four simulated robot locomotion problems
and a 2D navigation problem and show that our method can achieve successful
transfer to notably different environments while taking the robot's safety into
consideration.
</p>
<a href="http://arxiv.org/abs/2012.06662" target="_blank">arXiv:2012.06662</a> [<a href="http://arxiv.org/pdf/2012.06662" target="_blank">pdf</a>]

<h2>Avoiding The Double Descent Phenomenon of Random Feature Models Using Hybrid Regularization. (arXiv:2012.06667v1 [cs.LG])</h2>
<h3>Kelvin Kan, James G Nagy, Lars Ruthotto</h3>
<p>We demonstrate the ability of hybrid regularization methods to automatically
avoid the double descent phenomenon arising in the training of random feature
models (RFM). The hallmark feature of the double descent phenomenon is a spike
in the regularization gap at the interpolation threshold, i.e. when the number
of features in the RFM equals the number of training samples. To close this
gap, the hybrid method considered in our paper combines the respective
strengths of the two most common forms of regularization: early stopping and
weight decay. The scheme does not require hyperparameter tuning as it
automatically selects the stopping iteration and weight decay hyperparameter by
using generalized cross-validation (GCV). This also avoids the necessity of a
dedicated validation set. While the benefits of hybrid methods have been
well-documented for ill-posed inverse problems, our work presents the first use
case in machine learning. To expose the need for regularization and motivate
hybrid methods, we perform detailed numerical experiments inspired by image
classification. In those examples, the hybrid scheme successfully avoids the
double descent phenomenon and yields RFMs whose generalization is comparable
with classical regularization approaches whose hyperparameters are tuned
optimally using the test data. We provide our MATLAB codes for implementing the
numerical experiments in this paper at https://github.com/EmoryMLIP/HybridRFM.
</p>
<a href="http://arxiv.org/abs/2012.06667" target="_blank">arXiv:2012.06667</a> [<a href="http://arxiv.org/pdf/2012.06667" target="_blank">pdf</a>]

<h2>Adaptive Histogram-Based Gradient Boosted Trees for Federated Learning. (arXiv:2012.06670v1 [cs.LG])</h2>
<h3>Yuya Jeremy Ong, Yi Zhou, Nathalie Baracaldo, Heiko Ludwig</h3>
<p>Federated Learning (FL) is an approach to collaboratively train a model
across multiple parties without sharing data between parties or an aggregator.
It is used both in the consumer domain to protect personal data as well as in
enterprise settings, where dealing with data domicile regulation and the
pragmatics of data silos are the main drivers. While gradient boosted tree
implementations such as XGBoost have been very successful for many use cases,
its federated learning adaptations tend to be very slow due to using
cryptographic and privacy methods and have not experienced widespread use. We
propose the Party-Adaptive XGBoost (PAX) for federated learning, a novel
implementation of gradient boosting which utilizes a party adaptive histogram
aggregation method, without the need for data encryption. It constructs a
surrogate representation of the data distribution for finding splits of the
decision tree. Our experimental results demonstrate strong model performance,
especially on non-IID distributions, and significantly faster training run-time
across different data sets than existing federated implementations. This
approach makes the use of gradient boosted trees practical in enterprise
federated learning.
</p>
<a href="http://arxiv.org/abs/2012.06670" target="_blank">arXiv:2012.06670</a> [<a href="http://arxiv.org/pdf/2012.06670" target="_blank">pdf</a>]

<h2>TabTransformer: Tabular Data Modeling Using Contextual Embeddings. (arXiv:2012.06678v1 [cs.LG])</h2>
<h3>Xin Huang, Ashish Khetan, Milan Cvitkovic, Zohar Karnin</h3>
<p>We propose TabTransformer, a novel deep tabular data modeling architecture
for supervised and semi-supervised learning. The TabTransformer is built upon
self-attention based Transformers. The Transformer layers transform the
embeddings of categorical features into robust contextual embeddings to achieve
higher prediction accuracy. Through extensive experiments on fifteen publicly
available datasets, we show that the TabTransformer outperforms the
state-of-the-art deep learning methods for tabular data by at least 1.0% on
mean AUC, and matches the performance of tree-based ensemble models.
Furthermore, we demonstrate that the contextual embeddings learned from
TabTransformer are highly robust against both missing and noisy data features,
and provide better interpretability. Lastly, for the semi-supervised setting we
develop an unsupervised pre-training procedure to learn data-driven contextual
embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art
methods.
</p>
<a href="http://arxiv.org/abs/2012.06678" target="_blank">arXiv:2012.06678</a> [<a href="http://arxiv.org/pdf/2012.06678" target="_blank">pdf</a>]

<h2>Convolutional LSTM Neural Networks for Modeling Wildland Fire Dynamics. (arXiv:2012.06679v1 [cs.LG])</h2>
<h3>John Burge, Matthew Bonanni, Matthias Ihme, Lily Hu</h3>
<p>As the climate changes, the severity of wildland fires is expected to worsen.
Understanding, controlling and mitigating these fires requires building models
to accurately capture the fire-propagation dynamics. Supervised machine
learning techniques provide a potential approach for developing such models.
The objective of this study is to evaluate the feasibility of using the
Convolutional Long Short-Term Memory (ConvLSTM) recurrent neural network (RNN)
to model the dynamics of wildland fire propagation. The model is trained on
simulated wildfire data generated by a cellular automaton percolation model.
Four simulated datasets are analyzed, each with increasing degrees of
complexity. The simplest dataset includes a constant wind direction as a single
confounding factor, whereas the most complex dataset includes dynamic wind,
complex terrain, spatially varying moisture content and realistic vegetation
density distributions. We examine how effectively the ConvLSTM can capture the
fire-spread dynamics over consecutive time steps using classification and
regression metrics. It is shown that these ConvLSTMs are capable of capturing
local fire transmission events, as well as the overall fire dynamics, such as
the rate at which the fire spreads. Finally, we demonstrate that ConvLSTMs
outperform non-temporal Convolutional Neural Networks(CNNs), particularly on
the most difficult dataset.
</p>
<a href="http://arxiv.org/abs/2012.06679" target="_blank">arXiv:2012.06679</a> [<a href="http://arxiv.org/pdf/2012.06679" target="_blank">pdf</a>]

<h2>Faster Policy Learning with Continuous-Time Gradients. (arXiv:2012.06684v1 [cs.LG])</h2>
<h3>Samuel Ainsworth, Kendall Lowrey, John Thickstun, Zaid Harchaoui, Siddhartha Srinivasa</h3>
<p>We study the estimation of policy gradients for continuous-time systems with
known dynamics. By reframing policy learning in continuous-time, we show that
it is possible construct a more efficient and accurate gradient estimator. The
standard back-propagation through time estimator (BPTT) computes exact
gradients for a crude discretization of the continuous-time system. In
contrast, we approximate continuous-time gradients in the original system. With
the explicit goal of estimating continuous-time gradients, we are able to
discretize adaptively and construct a more efficient policy gradient estimator
which we call the Continuous-Time Policy Gradient (CTPG). We show that
replacing BPTT policy gradients with more efficient CTPG estimates results in
faster and more robust learning in a variety of control tasks and simulators.
</p>
<a href="http://arxiv.org/abs/2012.06684" target="_blank">arXiv:2012.06684</a> [<a href="http://arxiv.org/pdf/2012.06684" target="_blank">pdf</a>]

<h2>Computing Machinery and Knowledge. (arXiv:2012.06686v1 [cs.AI])</h2>
<h3>Raymond Anneborg</h3>
<p>The purpose of this paper is to discuss the possibilities for computing
machinery, or AI agents, to know and to possess knowledge. This is done mainly
from a virtue epistemology perspective and definition of knowledge. However,
this inquiry also shed light on the human condition, what it means for a human
to know, and to possess knowledge. The paper argues that it is possible for an
AI agent to know and examines this from both current state-of-the-art in
artificial intelligence as well as from the perspective of what the future AI
development might bring in terms of superintelligent AI agents.
</p>
<a href="http://arxiv.org/abs/2012.06686" target="_blank">arXiv:2012.06686</a> [<a href="http://arxiv.org/pdf/2012.06686" target="_blank">pdf</a>]

<h2>Parameter Estimation with Dense and Convolutional Neural Networks Applied to the FitzHugh-Nagumo ODE. (arXiv:2012.06691v1 [stat.ML])</h2>
<h3>Johann Rudi, Julie Bessac, Amanda Lenzi</h3>
<p>Machine learning algorithms have been successfully used to approximate
nonlinear maps under weak assumptions on the structure and properties of the
maps. We present deep neural networks using dense and convolutional layers to
solve an inverse problem, where we seek to estimate parameters in a
FitzHugh-Nagumo model, which consists of a nonlinear system of ordinary
differential equations (ODEs). We employ the neural networks to approximate
reconstruction maps for model parameter estimation from observational data,
where the data comes from the solution of the ODE and takes the form of a time
series representing dynamically spiking membrane potential of a (biological)
neuron. We target this dynamical model because of the computational challenges
it poses in an inference setting, namely, having a highly nonlinear and
nonconvex data misfit term and permitting only weakly informative priors on
parameters. These challenges cause traditional optimization to fail and
alternative algorithms to exhibit large computational costs. We quantify the
predictability of model parameters obtained from the neural networks with
statistical metrics and investigate the effects of network architectures and
presence of noise in observational data. Our results demonstrate that deep
neural networks are capable of very accurately estimating parameters in
dynamical models from observational data.
</p>
<a href="http://arxiv.org/abs/2012.06691" target="_blank">arXiv:2012.06691</a> [<a href="http://arxiv.org/pdf/2012.06691" target="_blank">pdf</a>]

<h2>Learning Representations from Temporally Smooth Data. (arXiv:2012.06694v1 [cs.LG])</h2>
<h3>Shima Rahimi Moghaddam, Fanjun Bu, Christopher J. Honey</h3>
<p>Events in the real world are correlated across nearby points in time, and we
must learn from this temporally smooth data. However, when neural networks are
trained to categorize or reconstruct single items, the common practice is to
randomize the order of training items. What are the effects of temporally
smooth training data on the efficiency of learning? We first tested the effects
of smoothness in training data on incremental learning in feedforward nets and
found that smoother data slowed learning. Moreover, sampling so as to minimize
temporal smoothness produced more efficient learning than sampling randomly. If
smoothness generally impairs incremental learning, then how can networks be
modified to benefit from smoothness in the training data? We hypothesized that
two simple brain-inspired mechanisms, leaky memory in activation units and
memory-gating, could enable networks to rapidly extract useful representations
from smooth data. Across all levels of data smoothness, these brain-inspired
architectures achieved more efficient category learning than feedforward
networks. This advantage persisted, even when leaky memory networks with gating
were trained on smooth data and tested on randomly-ordered data. Finally, we
investigated how these brain-inspired mechanisms altered the internal
representations learned by the networks. We found that networks with
multi-scale leaky memory and memory-gating could learn internal representations
that un-mixed data sources which vary on fast and slow timescales across
training samples. Altogether, we identified simple mechanisms enabling neural
networks to learn more quickly from temporally smooth data, and to generate
internal representations that separate timescales in the training signal.
</p>
<a href="http://arxiv.org/abs/2012.06694" target="_blank">arXiv:2012.06694</a> [<a href="http://arxiv.org/pdf/2012.06694" target="_blank">pdf</a>]

<h2>Generating Adversarial Disturbances for Controller Verification. (arXiv:2012.06695v1 [cs.LG])</h2>
<h3>Udaya Ghai, David Snyder, Anirudha Majumdar, Elad Hazan</h3>
<p>We consider the problem of generating maximally adversarial disturbances for
a given controller assuming only blackbox access to it. We propose an online
learning approach to this problem that adaptively generates disturbances based
on control inputs chosen by the controller. The goal of the disturbance
generator is to minimize regret versus a benchmark disturbance-generating
policy class, i.e., to maximize the cost incurred by the controller as well as
possible compared to the best possible disturbance generator in hindsight
(chosen from a benchmark policy class). In the setting where the dynamics are
linear and the costs are quadratic, we formulate our problem as an online trust
region (OTR) problem with memory and present a new online learning algorithm
(MOTR) for this problem. We prove that this method competes with the best
disturbance generator in hindsight (chosen from a rich class of benchmark
policies that includes linear-dynamical disturbance generating policies). We
demonstrate our approach on two simulated examples: (i) synthetically generated
linear systems, and (ii) generating wind disturbances for the popular PX4
controller in the AirSim simulator. On these examples, we demonstrate that our
approach outperforms several baseline approaches, including $H_{\infty}$
disturbance generation and gradient-based methods.
</p>
<a href="http://arxiv.org/abs/2012.06695" target="_blank">arXiv:2012.06695</a> [<a href="http://arxiv.org/pdf/2012.06695" target="_blank">pdf</a>]

<h2>Communication-Efficient Federated Learning with Compensated Overlap-FedAvg. (arXiv:2012.06706v1 [cs.LG])</h2>
<h3>Yuhao Zhou, Ye Qing, Jiancheng Lv</h3>
<p>Petabytes of data are generated each day by emerging Internet of Things
(IoT), but only few of them can be finally collected and used for Machine
Learning (ML) purposes due to the apprehension of data &amp; privacy leakage, which
seriously retarding ML's growth. To alleviate this problem, Federated learning
is proposed to perform model training by multiple clients' combined data
without the dataset sharing within the cluster. Nevertheless, federated
learning introduces massive communication overhead as the synchronized data in
each epoch is of the same size as the model, and thereby leading to a low
communication efficiency. Consequently, variant methods mainly focusing on the
communication rounds reduction and data compression are proposed to reduce the
communication overhead of federated learning. In this paper, we propose
Overlap-FedAvg, a framework that parallels the model training phase with model
uploading &amp; downloading phase, so that the latter phase can be totally covered
by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg is further
developed with a hierarchical computing strategy, a data compensation mechanism
and a nesterov accelerated gradients~(NAG) algorithm. Besides, Overlap-FedAvg
is orthogonal to many other compression methods so that they can be applied
together to maximize the utilization of the cluster. Furthermore, the
theoretical analysis is provided to prove the convergence of the proposed
Overlap-FedAvg framework. Extensive experiments on both conventional and
recurrent tasks with multiple models and datasets also demonstrate that the
proposed Overlap-FedAvg framework substantially boosts the federated learning
process.
</p>
<a href="http://arxiv.org/abs/2012.06706" target="_blank">arXiv:2012.06706</a> [<a href="http://arxiv.org/pdf/2012.06706" target="_blank">pdf</a>]

<h2>Teacher-Student Asynchronous Learning with Multi-Source Consistency for Facial Landmark Detection. (arXiv:2012.06711v1 [cs.CV])</h2>
<h3>Rongye Meng, Sanping Zhou, Xingyu Wan, Mengliu Li, Jinjun Wang</h3>
<p>Due to the high annotation cost of large-scale facial landmark detection
tasks in videos, a semi-supervised paradigm that uses self-training for mining
high-quality pseudo-labels to participate in training has been proposed by
researchers. However, self-training based methods often train with a gradually
increasing number of samples, whose performances vary a lot depending on the
number of pseudo-labeled samples added.

In this paper, we propose a teacher-student asynchronous learning~(TSAL)
framework based on the multi-source supervision signal consistency criterion,
which implicitly mines pseudo-labels through consistency constraints.
Specifically, the TSAL framework contains two models with exactly the same
structure. The radical student uses multi-source supervision signals from the
same task to update parameters, while the calm teacher uses a single-source
supervision signal to update parameters. In order to reasonably absorb
student's suggestions, teacher's parameters are updated again through recursive
average filtering. The experimental results prove that asynchronous-learning
framework can effectively filter noise in multi-source supervision signals,
thereby mining the pseudo-labels which are more significant for network
parameter updating. And extensive experiments on 300W, AFLW, and 300VW
benchmarks show that the TSAL framework achieves state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2012.06711" target="_blank">arXiv:2012.06711</a> [<a href="http://arxiv.org/pdf/2012.06711" target="_blank">pdf</a>]

<h2>Learning Consistent Deep Generative Models from Sparse Data via Prediction Constraints. (arXiv:2012.06718v1 [cs.LG])</h2>
<h3>Gabriel Hope, Madina Abdrakhmanova, Xiaoyin Chen, Michael C. Hughes, Michael C. Hughes, Erik B. Sudderth</h3>
<p>We develop a new framework for learning variational autoencoders and other
deep generative models that balances generative and discriminative goals. Our
framework optimizes model parameters to maximize a variational lower bound on
the likelihood of observed data, subject to a task-specific prediction
constraint that prevents model misspecification from leading to inaccurate
predictions. We further enforce a consistency constraint, derived naturally
from the generative model, that requires predictions on reconstructed data to
match those on the original data. We show that these two contributions --
prediction constraints and consistency constraints -- lead to promising image
classification performance, especially in the semi-supervised scenario where
category labels are sparse but unlabeled data is plentiful. Our approach
enables advances in generative modeling to directly boost semi-supervised
classification performance, an ability we demonstrate by augmenting deep
generative models with latent variables capturing spatial transformations.
</p>
<a href="http://arxiv.org/abs/2012.06718" target="_blank">arXiv:2012.06718</a> [<a href="http://arxiv.org/pdf/2012.06718" target="_blank">pdf</a>]

<h2>Mask Guided Matting via Progressive Refinement Network. (arXiv:2012.06722v1 [cs.CV])</h2>
<h3>Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, Alan Yuille</h3>
<p>We propose Mask Guided (MG) Matting, a robust matting framework that takes a
general coarse mask as guidance. MG Matting leverages a network (PRN) design
which encourages the matting model to provide self-guidance to progressively
refine the uncertain regions through the decoding process. A series of guidance
mask perturbation operations are also introduced in the training to further
enhance its robustness to external guidance. We show that PRN can generalize to
unseen types of guidance masks such as trimap and low-quality alpha matte,
making it suitable for various application pipelines. In addition, we revisit
the foreground color prediction problem for matting and propose a surprisingly
simple improvement to address the dataset issue. Evaluation on real and
synthetic benchmarks shows that MG Matting achieves state-of-the-art
performance using various types of guidance inputs. Code and models will be
available at https://github.com/yucornetto/MGMatting
</p>
<a href="http://arxiv.org/abs/2012.06722" target="_blank">arXiv:2012.06722</a> [<a href="http://arxiv.org/pdf/2012.06722" target="_blank">pdf</a>]

<h2>On Duality Gap as a Measure for Monitoring GAN Training. (arXiv:2012.06723v1 [cs.LG])</h2>
<h3>Sahil Sidheekh, Aroof Aimen, Vineet Madan, Narayanan C. Krishnan</h3>
<p>Generative adversarial network (GAN) is among the most popular deep learning
models for learning complex data distributions. However, training a GAN is
known to be a challenging task. This is often attributed to the lack of
correlation between the training progress and the trajectory of the generator
and discriminator losses and the need for the GAN's subjective evaluation. A
recently proposed measure inspired by game theory - the duality gap, aims to
bridge this gap. However, as we demonstrate, the duality gap's capability
remains constrained due to limitations posed by its estimation process. This
paper presents a theoretical understanding of this limitation and proposes a
more dependable estimation process for the duality gap. At the crux of our
approach is the idea that local perturbations can help agents in a zero-sum
game escape non-Nash saddle points efficiently. Through exhaustive
experimentation across GAN models and datasets, we establish the efficacy of
our approach in capturing the GAN training progress with minimal increase to
the computational complexity. Further, we show that our estimate, with its
ability to identify model convergence/divergence, is a potential performance
measure that can be used to tune the hyperparameters of a GAN.
</p>
<a href="http://arxiv.org/abs/2012.06723" target="_blank">arXiv:2012.06723</a> [<a href="http://arxiv.org/pdf/2012.06723" target="_blank">pdf</a>]

<h2>Synthesis of a Six-Bar Gripper Mechanism for Aerial Grasping. (arXiv:2012.06724v1 [cs.RO])</h2>
<h3>Rajashekhar V S, Rokesh Laishram, Kaushik Das, Debasish Ghose</h3>
<p>In this paper, a 1-DoF gripper mechanism has been synthesized for the type of
mechanism, number of links and joints, and the dimensions of length, width and
thickness of links. The type synthesis is done by selecting the proper class of
mechanism from Reuleaux's six classes of mechanisms. The number synthesis is
done by using an algebraic method. The dimensions of the linkages are found
using the geometric programming method. The gripper is then modeled in a
computer aided design software and then fabricated using an additive
manufacturing technique. Finally the gripper mechanism with DC motor as an
actuator is mounted on an Unmanned Aerial Vehicle (UAV) to grip a spherical
object moving in space. This work is related to a task in challenge 1 of
Mohamed Bin Zayed International Robotics Challenge (MBZIRC)-2020.
</p>
<a href="http://arxiv.org/abs/2012.06724" target="_blank">arXiv:2012.06724</a> [<a href="http://arxiv.org/pdf/2012.06724" target="_blank">pdf</a>]

<h2>PiRank: Learning To Rank via Differentiable Sorting. (arXiv:2012.06731v1 [cs.LG])</h2>
<h3>Robin Swezey, Aditya Grover, Bruno Charron, Stefano Ermon</h3>
<p>A key challenge with machine learning approaches for ranking is the gap
between the performance metrics of interest and the surrogate loss functions
that can be optimized with gradient-based methods. This gap arises because
ranking metrics typically involve a sorting operation which is not
differentiable w.r.t. the model parameters. Prior works have proposed
surrogates that are loosely related to ranking metrics or simple smoothed
versions thereof. We propose PiRank, a new class of differentiable surrogates
for ranking, which employ a continuous, temperature-controlled relaxation to
the sorting operator. We show that PiRank exactly recovers the desired metrics
in the limit of zero temperature and scales favorably with the problem size,
both in theory and practice. Empirically, we demonstrate that PiRank
significantly improves over existing approaches on publicly available
internet-scale learning-to-rank benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.06731" target="_blank">arXiv:2012.06731</a> [<a href="http://arxiv.org/pdf/2012.06731" target="_blank">pdf</a>]

<h2>Human-in-the-Loop Imitation Learning using Remote Teleoperation. (arXiv:2012.06733v1 [cs.RO])</h2>
<h3>Ajay Mandlekar, Danfei Xu, Roberto Mart&#xed;n-Mart&#xed;n, Yuke Zhu, Li Fei-Fei, Silvio Savarese</h3>
<p>Imitation Learning is a promising paradigm for learning complex robot
manipulation skills by reproducing behavior from human demonstrations. However,
manipulation tasks often contain bottleneck regions that require a sequence of
precise actions to make meaningful progress, such as a robot inserting a pod
into a coffee machine to make coffee. Trained policies can fail in these
regions because small deviations in actions can lead the policy into states not
covered by the demonstrations. Intervention-based policy learning is an
alternative that can address this issue -- it allows human operators to monitor
trained policies and take over control when they encounter failures. In this
paper, we build a data collection system tailored to 6-DoF manipulation
settings, that enables remote human operators to monitor and intervene on
trained policies. We develop a simple and effective algorithm to train the
policy iteratively on new data collected by the system that encourages the
policy to learn how to traverse bottlenecks through the interventions. We
demonstrate that agents trained on data collected by our intervention-based
system and algorithm outperform agents trained on an equivalent number of
samples collected by non-interventional demonstrators, and further show that
our method outperforms multiple state-of-the-art baselines for learning from
the human interventions on a challenging robot threading task and a coffee
making task. Additional results and videos at
https://sites.google.com/stanford.edu/iwr .
</p>
<a href="http://arxiv.org/abs/2012.06733" target="_blank">arXiv:2012.06733</a> [<a href="http://arxiv.org/pdf/2012.06733" target="_blank">pdf</a>]

<h2>PoP-Net: Pose over Parts Network for Multi-Person 3D Pose Estimation from a Depth Image. (arXiv:2012.06734v1 [cs.CV])</h2>
<h3>Yuliang Guo, Zhong Li, Zekun Li, Xiangyu Du, Shuxue Quan, Yi Xu</h3>
<p>In this paper, a real-time method called PoP-Net is proposed to predict
multi-person 3D poses from a depth image. PoP-Net learns to predict bottom-up
part detection maps and top-down global poses in a single-shot framework. A
simple and effective fusion process is applied to fuse the global poses and
part detection. Specifically, a new part-level representation, called Truncated
Part Displacement Field (TPDF), is introduced. It drags low-precision global
poses towards more accurate part locations while maintaining the advantage of
global poses in handling severe occlusion and truncation cases. A mode
selection scheme is developed to automatically resolve the conflict between
global poses and local detection. Finally, due to the lack of high-quality
depth datasets for developing and evaluating multi-person 3D pose estimation
methods, a comprehensive depth dataset with 3D pose labels is released. The
dataset is designed to enable effective multi-person and background data
augmentation such that the developed models are more generalizable towards
uncontrolled real-world multi-person scenarios. We show that PoP-Net has
significant advantages in efficiency for multi-person processing and achieves
the state-of-the-art results both on the released challenging dataset and on
the widely used ITOP dataset.
</p>
<a href="http://arxiv.org/abs/2012.06734" target="_blank">arXiv:2012.06734</a> [<a href="http://arxiv.org/pdf/2012.06734" target="_blank">pdf</a>]

<h2>Multimodal In-bed Pose and Shape Estimation under the Blankets. (arXiv:2012.06735v1 [cs.CV])</h2>
<h3>Yu Yin, Joseph P. Robinson, Yun Fu</h3>
<p>Humans spend vast hours in bed -- about one-third of the lifetime on average.
Besides, a human at rest is vital in many healthcare applications. Typically,
humans are covered by a blanket when resting, for which we propose a multimodal
approach to uncover the subjects so their bodies at rest can be viewed without
the occlusion of the blankets above. We propose a pyramid scheme to effectively
fuse the different modalities in a way that best leverages the knowledge
captured by the multimodal sensors. Specifically, the two most informative
modalities (i.e., depth and infrared images) are first fused to generate good
initial pose and shape estimation. Then pressure map and RGB images are further
fused one by one to refine the result by providing occlusion-invariant
information for the covered part, and accurate shape information for the
uncovered part, respectively. However, even with multimodal data, the task of
detecting human bodies at rest is still very challenging due to the extreme
occlusion of bodies. To further reduce the negative effects of the occlusion
from blankets, we employ an attention-based reconstruction module to generate
uncovered modalities, which are further fused to update current estimation via
a cyclic fashion. Extensive experiments validate the superiority of the
proposed model over others.
</p>
<a href="http://arxiv.org/abs/2012.06735" target="_blank">arXiv:2012.06735</a> [<a href="http://arxiv.org/pdf/2012.06735" target="_blank">pdf</a>]

<h2>Computer Vision and Normalizing Flow Based Defect Detection. (arXiv:2012.06737v1 [cs.CV])</h2>
<h3>Zijian Kuang, Xinran Tie</h3>
<p>Surface defect detection is essential and necessary for controlling the
qualities of the products during manufacturing. The challenges in this complex
task include: 1) collecting defective samples and manually labeling for
training is time-consuming; 2) the defects' characteristics are difficult to
define as new types of defect can happen all the time; 3) and the real-world
product images contain lots of background noise. In this paper, we present a
two-stage defect detection network based on the object detection model YOLO,
and the normalizing flow-based defect detection model DifferNet. Our model has
high robustness and performance on defect detection using real-world video
clips taken from a production line monitoring system. The normalizing
flow-based anomaly detection model only requires a small number of good samples
for training and then perform defect detection on the product images detected
by YOLO. The model we invent employs two novel strategies: 1) a two-stage
network using YOLO and a normalizing flow-based model to perform product defect
detection, 2) multi-scale image transformations are implemented to solve the
issue product image cropped by YOLO includes many background noise. Besides,
extensive experiments are conducted on a new dataset collected from the
real-world factory production line. We demonstrate that our proposed model can
learn on a small number of defect-free samples of single or multiple product
types. The dataset will also be made public to encourage further studies and
research in surface defect detection.
</p>
<a href="http://arxiv.org/abs/2012.06737" target="_blank">arXiv:2012.06737</a> [<a href="http://arxiv.org/pdf/2012.06737" target="_blank">pdf</a>]

<h2>Learning Multi-Arm Manipulation Through Collaborative Teleoperation. (arXiv:2012.06738v1 [cs.RO])</h2>
<h3>Albert Tung, Josiah Wong, Ajay Mandlekar, Roberto Mart&#xed;n-Mart&#xed;n, Yuke Zhu, Li Fei-Fei, Silvio Savarese</h3>
<p>Imitation Learning (IL) is a powerful paradigm to teach robots to perform
manipulation tasks by allowing them to learn from human demonstrations
collected via teleoperation, but has mostly been limited to single-arm
manipulation. However, many real-world tasks require multiple arms, such as
lifting a heavy object or assembling a desk. Unfortunately, applying IL to
multi-arm manipulation tasks has been challenging -- asking a human to control
more than one robotic arm can impose significant cognitive burden and is often
only possible for a maximum of two robot arms. To address these challenges, we
present Multi-Arm RoboTurk (MART), a multi-user data collection platform that
allows multiple remote users to simultaneously teleoperate a set of robotic
arms and collect demonstrations for multi-arm tasks. Using MART, we collected
demonstrations for five novel two and three-arm tasks from several
geographically separated users. From our data we arrived at a critical insight:
most multi-arm tasks do not require global coordination throughout its full
duration, but only during specific moments. We show that learning from such
data consequently presents challenges for centralized agents that directly
attempt to model all robot actions simultaneously, and perform a comprehensive
study of different policy architectures with varying levels of centralization
on our tasks. Finally, we propose and evaluate a base-residual policy framework
that allows trained policies to better adapt to the mixed coordination setting
common in multi-arm manipulation, and show that a centralized policy augmented
with a decentralized residual model outperforms all other models on our set of
benchmark tasks. Additional results and videos at
https://roboturk.stanford.edu/multiarm .
</p>
<a href="http://arxiv.org/abs/2012.06738" target="_blank">arXiv:2012.06738</a> [<a href="http://arxiv.org/pdf/2012.06738" target="_blank">pdf</a>]

<h2>Sampling Training Data for Continual Learning Between Robots and the Cloud. (arXiv:2012.06739v1 [cs.RO])</h2>
<h3>Sandeep Chinchali, Evgenya Pergament, Manabu Nakanoya, Eyal Cidon, Edward Zhang, Dinesh Bharadia, Marco Pavone, Sachin Katti</h3>
<p>Today's robotic fleets are increasingly measuring high-volume video and LIDAR
sensory streams, which can be mined for valuable training data, such as rare
scenes of road construction sites, to steadily improve robotic perception
models. However, re-training perception models on growing volumes of rich
sensory data in central compute servers (or the "cloud") places an enormous
time and cost burden on network transfer, cloud storage, human annotation, and
cloud computing resources. Hence, we introduce HarvestNet, an intelligent
sampling algorithm that resides on-board a robot and reduces system bottlenecks
by only storing rare, useful events to steadily improve perception models
re-trained in the cloud. HarvestNet significantly improves the accuracy of
machine-learning models on our novel dataset of road construction sites, field
testing of self-driving cars, and streaming face recognition, while reducing
cloud storage, dataset annotation time, and cloud compute time by between
65.7-81.3%. Further, it is between 1.05-2.58x more accurate than baseline
algorithms and scalably runs on embedded deep learning hardware. We provide a
suite of compute-efficient perception models for the Google Edge Tensor
Processing Unit (TPU), an extended technical report, and a novel video dataset
to the research community at https://sites.google.com/view/harvestnet.
</p>
<a href="http://arxiv.org/abs/2012.06739" target="_blank">arXiv:2012.06739</a> [<a href="http://arxiv.org/pdf/2012.06739" target="_blank">pdf</a>]

<h2>Periocular in the Wild Embedding Learning with Cross-Modal Consistent Knowledge Distillation. (arXiv:2012.06746v1 [cs.CV])</h2>
<h3>Yoon Gyo Jung, Jaewoo Park, Cheng Yaw Low, Leslie Ching Ow Tiong, Andrew Beng Jin Teoh</h3>
<p>Periocular biometric, or peripheral area of ocular, is a collaborative
alternative to face, especially if a face is occluded or masked. In practice,
sole periocular biometric captures least salient facial features, thereby
suffering from intra-class compactness and inter-class dispersion issues
particularly in the wild environment. To address these problems, we transfer
useful information from face to support periocular modality by means of
knowledge distillation (KD) for embedding learning. However, applying typical
KD techniques to heterogeneous modalities directly is suboptimal. We put
forward in this paper a deep face-to-periocular distillation networks, coined
as cross-modal consistent knowledge distillation (CM-CKD) henceforward. The
three key ingredients of CM-CKD are (1) shared-weight networks, (2) consistent
batch normalization, and (3) a bidirectional consistency distillation for face
and periocular through an effectual CKD loss. To be more specific, we leverage
face modality for periocular embedding learning, but only periocular images are
targeted for identification or verification tasks. Extensive experiments on six
constrained and unconstrained periocular datasets disclose that the
CM-CKD-learned periocular embeddings extend identification and verification
performance by 50% in terms of relative performance gain computed based upon
face and periocular baselines. The experiments also reveal that the
CM-CKD-learned periocular features enjoy better subject-wise cluster
separation, thereby refining the overall accuracy performance.
</p>
<a href="http://arxiv.org/abs/2012.06746" target="_blank">arXiv:2012.06746</a> [<a href="http://arxiv.org/pdf/2012.06746" target="_blank">pdf</a>]

<h2>Efficient Incorporation of Multiple Latency Targets in the Once-For-All Network. (arXiv:2012.06748v1 [cs.LG])</h2>
<h3>Vidhur Kumar, Andrew Szidon</h3>
<p>Neural Architecture Search has proven an effective method of automating
architecture engineering. Recent work in the field has been to look for
architectures subject to multiple objectives such as accuracy and latency to
efficiently deploy them on different target hardware. Once-for-All (OFA) is one
such method that decouples training and search and is able to find
high-performance networks for different latency constraints. However, the
search phase is inefficient at incorporating multiple latency targets. In this
paper, we introduce two strategies (Top-down and Bottom-up) that use warm
starting and randomized network pruning for the efficient incorporation of
multiple latency targets in the OFA network. We evaluate these strategies
against the current OFA implementation and demonstrate that our strategies
offer significant running time performance gains while not sacrificing the
accuracy of the subnetworks that were found for each latency target. We further
demonstrate that these performance gains are generalized to every design space
used by the OFA network.
</p>
<a href="http://arxiv.org/abs/2012.06748" target="_blank">arXiv:2012.06748</a> [<a href="http://arxiv.org/pdf/2012.06748" target="_blank">pdf</a>]

<h2>A Meta-Learning Approach for Graph Representation Learning in Multi-Task Settings. (arXiv:2012.06755v1 [cs.LG])</h2>
<h3>Davide Buffelli, Fabio Vandin</h3>
<p>Graph Neural Networks (GNNs) are a framework for graph representation
learning, where a model learns to generate low dimensional node embeddings that
encapsulate structural and feature-related information. GNNs are usually
trained in an end-to-end fashion, leading to highly specialized node
embeddings. However, generating node embeddings that can be used to perform
multiple tasks (with performance comparable to single-task models) is an open
problem. We propose a novel meta-learning strategy capable of producing
multi-task node embeddings. Our method avoids the difficulties arising when
learning to perform multiple tasks concurrently by, instead, learning to
quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks
singularly. We show that the embeddings produced by our method can be used to
perform multiple tasks with comparable or higher performance than classically
trained models. Our method is model-agnostic and task-agnostic, thus applicable
to a wide variety of multi-task domains.
</p>
<a href="http://arxiv.org/abs/2012.06755" target="_blank">arXiv:2012.06755</a> [<a href="http://arxiv.org/pdf/2012.06755" target="_blank">pdf</a>]

<h2>Query-free Black-box Adversarial Attacks on Graphs. (arXiv:2012.06757v1 [cs.LG])</h2>
<h3>Jiarong Xu, Yizhou Sun, Xin Jiang, Yanhao Wang, Yang Yang, Chunping Wang, Jiangang Lu</h3>
<p>Many graph-based machine learning models are known to be vulnerable to
adversarial attacks, where even limited perturbations on input data can result
in dramatic performance deterioration. Most existing works focus on moderate
settings in which the attacker is either aware of the model structure and
parameters (white-box), or able to send queries to fetch model information. In
this paper, we propose a query-free black-box adversarial attack on graphs, in
which the attacker has no knowledge of the target model and no query access to
the model. With the mere observation of the graph topology, the proposed attack
strategy flips a limited number of links to mislead the graph models. We prove
that the impact of the flipped links on the target model can be quantified by
spectral changes, and thus be approximated using the eigenvalue perturbation
theory. Accordingly, we model the proposed attack strategy as an optimization
problem, and adopt a greedy algorithm to select the links to be flipped. Due to
its simplicity and scalability, the proposed model is not only generic in
various graph-based models, but can be easily extended when different knowledge
levels are accessible as well. Extensive experiments demonstrate the
effectiveness and efficiency of the proposed model on various downstream tasks,
as well as several different graph-based learning models.
</p>
<a href="http://arxiv.org/abs/2012.06757" target="_blank">arXiv:2012.06757</a> [<a href="http://arxiv.org/pdf/2012.06757" target="_blank">pdf</a>]

<h2>Anomaly detection through latent space restoration using vector-quantized variational autoencoders. (arXiv:2012.06765v1 [cs.CV])</h2>
<h3>Sergio Naval Marimont, Giacomo Tarroni</h3>
<p>We propose an out-of-distribution detection method that combines density and
restoration-based approaches using Vector-Quantized Variational Auto-Encoders
(VQ-VAEs). The VQ-VAE model learns to encode images in a categorical latent
space. The prior distribution of latent codes is then modelled using an
Auto-Regressive (AR) model. We found that the prior probability estimated by
the AR model can be useful for unsupervised anomaly detection and enables the
estimation of both sample and pixel-wise anomaly scores. The sample-wise score
is defined as the negative log-likelihood of the latent variables above a
threshold selecting highly unlikely codes. Additionally, out-of-distribution
images are restored into in-distribution images by replacing unlikely latent
codes with samples from the prior model and decoding to pixel space. The
average L1 distance between generated restorations and original image is used
as pixel-wise anomaly score. We tested our approach on the MOOD challenge
datasets, and report higher accuracies compared to a standard
reconstruction-based approach with VAEs.
</p>
<a href="http://arxiv.org/abs/2012.06765" target="_blank">arXiv:2012.06765</a> [<a href="http://arxiv.org/pdf/2012.06765" target="_blank">pdf</a>]

<h2>Fusion of Range and Stereo Data for High-Resolution Scene-Modeling. (arXiv:2012.06769v1 [cs.CV])</h2>
<h3>Georgios D. Evangelidis, Miles Hansard, Radu Horaud</h3>
<p>This paper addresses the problem of range-stereo fusion, for the construction
of high-resolution depth maps. In particular, we combine low-resolution depth
data with high-resolution stereo data, in a maximum a posteriori (MAP)
formulation. Unlike existing schemes that build on MRF optimizers, we infer the
disparity map from a series of local energy minimization problems that are
solved hierarchically, by growing sparse initial disparities obtained from the
depth data. The accuracy of the method is not compromised, owing to three
properties of the data-term in the energy function. Firstly, it incorporates a
new correlation function that is capable of providing refined correlations and
disparities, via subpixel correction. Secondly, the correlation scores rely on
an adaptive cost aggregation step, based on the depth data. Thirdly, the stereo
and depth likelihoods are adaptively fused, based on the scene texture and
camera geometry. These properties lead to a more selective growing process
which, unlike previous seed-growing methods, avoids the tendency to propagate
incorrect disparities. The proposed method gives rise to an intrinsically
efficient algorithm, which runs at 3FPS on 2.0MP images on a standard desktop
computer. The strong performance of the new method is established both by
quantitative comparisons with state-of-the-art methods, and by qualitative
comparisons using real depth-stereo data-sets.
</p>
<a href="http://arxiv.org/abs/2012.06769" target="_blank">arXiv:2012.06769</a> [<a href="http://arxiv.org/pdf/2012.06769" target="_blank">pdf</a>]

<h2>An Overview of Depth Cameras and Range Scanners Based on Time-of-Flight Technologies. (arXiv:2012.06772v1 [cs.CV])</h2>
<h3>Radu Horaud, Miles Hansard, Georgios Evangelidis, Clement Menier</h3>
<p>Time-of-flight (TOF) cameras are sensors that can measure the depths of
scene-points, by illuminating the scene with a controlled laser or LED source,
and then analyzing the reflected light. In this paper, we will first describe
the underlying measurement principles of time-of-flight cameras, including: (i)
pulsed-light cameras, which measure directly the time taken for a light pulse
to travel from the device to the object and back again, and (ii)
continuous-wave modulated-light cameras, which measure the phase difference
between the emitted and received signals, and hence obtain the travel time
indirectly. We review the main existing designs, including prototypes as well
as commercially available devices. We also review the relevant camera
calibration principles, and how they are applied to TOF devices. Finally, we
discuss the benefits and challenges of combined TOF and color camera systems.
</p>
<a href="http://arxiv.org/abs/2012.06772" target="_blank">arXiv:2012.06772</a> [<a href="http://arxiv.org/pdf/2012.06772" target="_blank">pdf</a>]

<h2>Uncalibrated Neural Inverse Rendering for Photometric Stereo of General Surfaces. (arXiv:2012.06777v1 [cs.CV])</h2>
<h3>Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, Luc Van Gool</h3>
<p>This paper presents an uncalibrated deep neural network framework for the
photometric stereo problem. For training models to solve the problem, existing
neural network-based methods either require exact light directions or
ground-truth surface normals of the object or both. However, in practice, it is
challenging to procure both of this information precisely, which restricts the
broader adoption of photometric stereo algorithms for vision application. To
bypass this difficulty, we propose an uncalibrated neural inverse rendering
approach to this problem. Our method first estimates the light directions from
the input images and then optimizes an image reconstruction loss to calculate
the surface normals, bidirectional reflectance distribution function value, and
depth. Additionally, our formulation explicitly models the concave and convex
parts of a complex surface to consider the effects of interreflections in the
image formation process. Extensive evaluation of the proposed method on the
challenging subjects generally shows comparable or better results than the
supervised and classical approaches.
</p>
<a href="http://arxiv.org/abs/2012.06777" target="_blank">arXiv:2012.06777</a> [<a href="http://arxiv.org/pdf/2012.06777" target="_blank">pdf</a>]

<h2>DETR for Pedestrian Detection. (arXiv:2012.06785v1 [cs.CV])</h2>
<h3>Matthieu Lin, Chuming Li, Xingyuan Bu, Ming Sun, Chen Lin, Junjie Yan, Wanli Ouyang, Zhidong Deng</h3>
<p>Pedestrian detection in crowd scenes poses a challenging problem due to the
heuristic defined mapping from anchors to pedestrians and the conflict between
NMS and highly overlapped pedestrians. The recently proposed end-to-end
detectors(ED), DETR and deformable DETR, replace hand designed components such
as NMS and anchors using the transformer architecture, which gets rid of
duplicate predictions by computing all pairwise interactions between queries.
Inspired by these works, we explore their performance on crowd pedestrian
detection. Surprisingly, compared to Faster-RCNN with FPN, the results are
opposite to those obtained on COCO. Furthermore, the bipartite match of ED
harms the training efficiency due to the large ground truth number in crowd
scenes. In this work, we identify the underlying motives driving ED's poor
performance and propose a new decoder to address them. Moreover, we design a
mechanism to leverage the less occluded visible parts of pedestrian
specifically for ED, and achieve further improvements. A faster bipartite match
algorithm is also introduced to make ED training on crowd dataset more
practical. The proposed detector PED(Pedestrian End-to-end Detector)
outperforms both previous EDs and the baseline Faster-RCNN on CityPersons and
CrowdHuman. It also achieves comparable performance with state-of-the-art
pedestrian detection methods. Code will be released soon.
</p>
<a href="http://arxiv.org/abs/2012.06785" target="_blank">arXiv:2012.06785</a> [<a href="http://arxiv.org/pdf/2012.06785" target="_blank">pdf</a>]

<h2>Knowledge Capture and Replay for Continual Learning. (arXiv:2012.06789v1 [cs.LG])</h2>
<h3>Saisubramaniam Gopalakrishnan, Pranshu Ranjan Singh, Haytham Fayek, Savitha Ramasamy, Arulmurugan Ambikapathi</h3>
<p>Deep neural networks have shown promise in several domains, and the learned
task-specific information is implicitly stored in the network parameters. It
will be vital to utilize representations from these networks for downstream
tasks such as continual learning. In this paper, we introduce the notion of
{\em flashcards} that are visual representations to {\em capture} the encoded
knowledge of a network, as a function of random image patterns. We demonstrate
the effectiveness of flashcards in capturing representations and show that they
are efficient replay methods for general and task agnostic continual learning
setting. Thus, while adapting to a new task, a limited number of constructed
flashcards, help to prevent catastrophic forgetting of the previously learned
tasks. Most interestingly, such flashcards neither require external memory
storage nor need to be accumulated over multiple tasks and only need to be
constructed just before learning the subsequent new task, irrespective of the
number of tasks trained before and are hence task agnostic. We first
demonstrate the efficacy of flashcards in capturing knowledge representation
from a trained network, and empirically validate the efficacy of flashcards on
a variety of continual learning tasks: continual unsupervised reconstruction,
continual denoising, and new-instance learning classification, using a number
of heterogeneous benchmark datasets. These studies also indicate that continual
learning algorithms with flashcards as the replay strategy perform better than
other state-of-the-art replay methods, and exhibits on par performance with the
best possible baseline using coreset sampling, with the least additional
computational complexity and storage.
</p>
<a href="http://arxiv.org/abs/2012.06789" target="_blank">arXiv:2012.06789</a> [<a href="http://arxiv.org/pdf/2012.06789" target="_blank">pdf</a>]

<h2>Fine-grained Classification via Categorical Memory Networks. (arXiv:2012.06793v1 [cs.CV])</h2>
<h3>Weijian Deng, Joshua Marsh, Stephen Gould, Liang Zheng</h3>
<p>Motivated by the desire to exploit patterns shared across classes, we present
a simple yet effective class-specific memory module for fine-grained feature
learning. The memory module stores the prototypical feature representation for
each category as a moving average. We hypothesize that the combination of
similarities with respect to each category is itself a useful discriminative
cue. To detect these similarities, we use attention as a querying mechanism.
The attention scores with respect to each class prototype are used as weights
to combine prototypes via weighted sum, producing a uniquely tailored response
feature representation for a given input. The original and response features
are combined to produce an augmented feature for classification. We integrate
our class-specific memory module into a standard convolutional neural network,
yielding a Categorical Memory Network. Our memory module significantly improves
accuracy over baseline CNNs, achieving competitive accuracy with
state-of-the-art methods on four benchmarks, including CUB-200-2011, Stanford
Cars, FGVC Aircraft, and NABirds.
</p>
<a href="http://arxiv.org/abs/2012.06793" target="_blank">arXiv:2012.06793</a> [<a href="http://arxiv.org/pdf/2012.06793" target="_blank">pdf</a>]

<h2>Delay Differential Neural Networks. (arXiv:2012.06800v1 [cs.LG])</h2>
<h3>Srinivas Anumasa, P.K. Srijith</h3>
<p>Neural ordinary differential equations (NODEs) treat computation of
intermediate feature vectors as trajectories of ordinary differential equation
parameterized by a neural network. In this paper, we propose a novel model,
delay differential neural networks (DDNN), inspired by delay differential
equations (DDEs). The proposed model considers the derivative of the hidden
feature vector as a function of the current feature vector and past feature
vectors (history). The function is modelled as a neural network and
consequently, it leads to continuous depth alternatives to many recent ResNet
variants. We propose two different DDNN architectures, depending on the way
current and past feature vectors are considered. For training DDNNs, we provide
a memory-efficient adjoint method for computing gradients and back-propagate
through the network. DDNN improves the data efficiency of NODE by further
reducing the number of parameters without affecting the generalization
performance. Experiments conducted on synthetic and real-world image
classification datasets such as Cifar10 and Cifar100 show the effectiveness of
the proposed models.
</p>
<a href="http://arxiv.org/abs/2012.06800" target="_blank">arXiv:2012.06800</a> [<a href="http://arxiv.org/pdf/2012.06800" target="_blank">pdf</a>]

<h2>Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation. (arXiv:2012.06815v1 [cs.CV])</h2>
<h3>Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, Xiaoyun Yang</h3>
<p>Visual object tracking aims to precisely estimate the bounding box for the
given target, which is a challenging problem due to factors such as deformation
and occlusion. Many recent trackers adopt the multiple-stage tracking strategy
to improve the quality of bounding box estimation. These methods first coarsely
locate the target and then refine the initial prediction in the following
stages. However, existing approaches still suffer from limited precision, and
the coupling of different stages severely restricts the method's
transferability. This work proposes a novel, flexible, and accurate refinement
module called Alpha-Refine, which can significantly improve the base trackers'
prediction quality. By exploring a series of design options, we conclude that
the key to successful refinement is extracting and maintaining detailed spatial
information as much as possible. Following this principle, Alpha-Refine adopts
a pixel-wise correlation, a corner prediction head, and an auxiliary mask head
as the core components. We apply Alpha-Refine to six famous base trackers to
verify our method's effectiveness: DiMPsuper, DiMP50, ATOM, SiamRPN++,
RT-MDNet, and ECO. Comprehensive experiments on TrackingNet, LaSOT, GOT-10K,
and VOT2020 benchmarks show that our approach significantly improves the base
tracker's performance with little extra latency. Code and pretrained model is
available at https://github.com/MasterBin-IIAU/AlphaRefine.
</p>
<a href="http://arxiv.org/abs/2012.06815" target="_blank">arXiv:2012.06815</a> [<a href="http://arxiv.org/pdf/2012.06815" target="_blank">pdf</a>]

<h2>Physics-Informed Machine Learning Simulator for Wildfire Propagation. (arXiv:2012.06825v1 [cs.LG])</h2>
<h3>Luca Bottero, Francesco Calisto, Giovanni Graziano, Valerio Pagliarino, Martina Scauda, Sara Tiengo, Simone Azeglio</h3>
<p>The aim of this work is to evaluate the feasibility of re-implementing some
key parts of the widely used Weather Research and Forecasting WRF-SFIRE
simulator by replacing its core differential equations numerical solvers with
state-of-the-art physics-informed machine learning techniques to solve ODEs and
PDEs, in order to transform it into a real-time simulator for wildfire spread
prediction. The main programming language used is Julia, a compiled language
which offers better perfomance than interpreted ones, providing Just in Time
(JIT) compilation with different optimization levels. Moreover, Julia is
particularly well suited for numerical computation and for the solution of
complex physical models, both considering the syntax and the presence of some
specific libraries such as DifferentialEquations.jl and ModellingToolkit.jl.
</p>
<a href="http://arxiv.org/abs/2012.06825" target="_blank">arXiv:2012.06825</a> [<a href="http://arxiv.org/pdf/2012.06825" target="_blank">pdf</a>]

<h2>Source Code Classification for Energy Efficiency in Parallel Ultra Low-Power Microcontrollers. (arXiv:2012.06836v1 [cs.LG])</h2>
<h3>Emanuele Parisi, Francesco Barchi, Andrea Bartolini, Giuseppe Tagliavini, Andrea Acquaviva</h3>
<p>The analysis of source code through machine learning techniques is an
increasingly explored research topic aiming at increasing smartness in the
software toolchain to exploit modern architectures in the best possible way. In
the case of low-power, parallel embedded architectures, this means finding the
configuration, for instance in terms of the number of cores, leading to minimum
energy consumption. Depending on the kernel to be executed, the energy optimal
scaling configuration is not trivial. While recent work has focused on
general-purpose systems to learn and predict the best execution target in terms
of the execution time of a snippet of code or kernel (e.g. offload OpenCL
kernel on multicore CPU or GPU), in this work we focus on static compile-time
features to assess if they can be successfully used to predict the minimum
energy configuration on PULP, an ultra-low-power architecture featuring an
on-chip cluster of RISC-V processors. Experiments show that using machine
learning models on the source code to select the best energy scaling
configuration automatically is viable and has the potential to be used in the
context of automatic system configuration for energy minimisation.
</p>
<a href="http://arxiv.org/abs/2012.06836" target="_blank">arXiv:2012.06836</a> [<a href="http://arxiv.org/pdf/2012.06836" target="_blank">pdf</a>]

<h2>High Order Local Directional Pattern Based Pyramidal Multi-structure for Robust Face Recognition. (arXiv:2012.06838v1 [cs.CV])</h2>
<h3>Almabrok Essa, Vijayan Asari</h3>
<p>Derived from a general definition of texture in a local neighborhood, local
directional pattern (LDP) encodes the directional information in the small
local 3x3 neighborhood of a pixel, which may fail to extract detailed
information especially during changes in the input image due to illumination
variations. Therefore, in this paper we introduce a novel feature extraction
technique that calculates the nth order direction variation patterns, named
high order local directional pattern (HOLDP). The proposed HOLDP can capture
more detailed discriminative information than the conventional LDP. Unlike the
LDP operator, our proposed technique extracts nth order local information by
encoding various distinctive spatial relationships from each neighborhood layer
of a pixel in the pyramidal multi-structure way. Then we concatenate the
feature vector of each neighborhood layer to form the final HOLDP feature
vector. The performance evaluation of the proposed HOLDP algorithm is conducted
on several publicly available face databases and observed the superiority of
HOLDP under extreme illumination conditions.
</p>
<a href="http://arxiv.org/abs/2012.06838" target="_blank">arXiv:2012.06838</a> [<a href="http://arxiv.org/pdf/2012.06838" target="_blank">pdf</a>]

<h2>Multi-Scale Cascading Network with Compact Feature Learning for RGB-Infrared Person Re-Identification. (arXiv:2012.06843v1 [cs.CV])</h2>
<h3>Can Zhang, Hong Liu, Wei Guo, Mang Ye</h3>
<p>RGB-Infrared person re-identification (RGB-IR Re-ID) aims to match persons
from heterogeneous images captured by visible and thermal cameras, which is of
great significance in the surveillance system under poor light conditions.
Facing great challenges in complex variances including conventional
single-modality and additional inter-modality discrepancies, most of the
existing RGB-IR Re-ID methods propose to impose constraints in image level,
feature level or a hybrid of both. Despite the better performance of hybrid
constraints, they are usually implemented with heavy network architecture. As a
matter of fact, previous efforts contribute more as pioneering works in new
cross-modal Re-ID area while leaving large space for improvement. This can be
mainly attributed to: (1) lack of abundant person image pairs from different
modalities for training, and (2) scarcity of salient modality-invariant
features especially on coarse representations for effective matching. To
address these issues, a novel Multi-Scale Part-Aware Cascading framework
(MSPAC) is formulated by aggregating multi-scale fine-grained features from
part to global in a cascading manner, which results in a unified representation
containing rich and enhanced semantic features. Furthermore, a marginal
exponential centre (MeCen) loss is introduced to jointly eliminate mixed
variances from intra- and inter-modal examples. Cross-modality correlations can
thus be efficiently explored on salient features for distinctive
modality-invariant feature learning. Extensive experiments are conducted to
demonstrate that the proposed method outperforms all the state-of-the-art by a
large margin.
</p>
<a href="http://arxiv.org/abs/2012.06843" target="_blank">arXiv:2012.06843</a> [<a href="http://arxiv.org/pdf/2012.06843" target="_blank">pdf</a>]

<h2>A Unified Model for the Two-stage Offline-then-Online Resource Allocation. (arXiv:2012.06845v1 [cs.AI])</h2>
<h3>Yifan Xu, Pan Xu, Jianping Pan, Jun Tao</h3>
<p>With the popularity of the Internet, traditional offline resource allocation
has evolved into a new form, called online resource allocation. It features the
online arrivals of agents in the system and the real-time decision-making
requirement upon the arrival of each online agent. Both offline and online
resource allocation have wide applications in various real-world matching
markets ranging from ridesharing to crowdsourcing. There are some emerging
applications such as rebalancing in bike sharing and trip-vehicle dispatching
in ridesharing, which involve a two-stage resource allocation process. The
process consists of an offline phase and another sequential online phase, and
both phases compete for the same set of resources. In this paper, we propose a
unified model which incorporates both offline and online resource allocation
into a single framework. Our model assumes non-uniform and known arrival
distributions for online agents in the second online phase, which can be
learned from historical data. We propose a parameterized linear programming
(LP)-based algorithm, which is shown to be at most a constant factor of $1/4$
from the optimal. Experimental results on the real dataset show that our
LP-based approaches outperform the LP-agnostic heuristics in terms of
robustness and effectiveness.
</p>
<a href="http://arxiv.org/abs/2012.06845" target="_blank">arXiv:2012.06845</a> [<a href="http://arxiv.org/pdf/2012.06845" target="_blank">pdf</a>]

<h2>A unified framework for closed-form nonparametric regression, classification, preference and mixed problems with Skew Gaussian Processes. (arXiv:2012.06846v1 [stat.ML])</h2>
<h3>Alessio Benavoli, Dario Azzimonti, Dario Piga</h3>
<p>Skew-Gaussian processes (SkewGPs) extend the multivariate Unified Skew-Normal
distributions over finite dimensional vectors to distribution over functions.
SkewGPs are more general and flexible than Gaussian processes, as SkewGPs may
also represent asymmetric distributions. In a recent contribution we showed
that SkewGP and probit likelihood are conjugate, which allows us to compute the
exact posterior for non-parametric binary classification and preference
learning. In this paper, we generalize previous results and we prove that
SkewGP is conjugate with both the normal and affine probit likelihood, and more
in general, with their product. This allows us to (i) handle classification,
preference, numeric and ordinal regression, and mixed problems in a unified
framework; (ii) derive closed-form expression for the corresponding posterior
distributions. We show empirically that the proposed framework based on SkewGP
provides better performance than Gaussian processes in active learning and
Bayesian (constrained) optimization.
</p>
<a href="http://arxiv.org/abs/2012.06846" target="_blank">arXiv:2012.06846</a> [<a href="http://arxiv.org/pdf/2012.06846" target="_blank">pdf</a>]

<h2>Trading the System Efficiency for the Income Equality of Drivers in Rideshare. (arXiv:2012.06850v1 [cs.AI])</h2>
<h3>Yifan Xu, Pan Xu</h3>
<p>Several scientific studies have reported the existence of the income gap
among rideshare drivers based on demographic factors such as gender, age, race,
etc. In this paper, we study the income inequality among rideshare drivers due
to discriminative cancellations from riders, and the tradeoff between the
income inequality (called fairness objective) with the system efficiency
(called profit objective). We proposed an online bipartite-matching model where
riders are assumed to arrive sequentially following a distribution known in
advance. The highlight of our model is the concept of acceptance rate between
any pair of driver-rider types, where types are defined based on demographic
factors. Specially, we assume each rider can accept or cancel the driver
assigned to her, each occurs with a certain probability which reflects the
acceptance degree from the rider type towards the driver type. We construct a
bi-objective linear program as a valid benchmark and propose two LP-based
parameterized online algorithms. Rigorous online competitive ratio analysis is
offered to demonstrate the flexibility and efficiency of our online algorithms
in balancing the two conflicting goals, promotions of fairness and profit.
Experimental results on a real-world dataset are provided as well, which
confirm our theoretical predictions.
</p>
<a href="http://arxiv.org/abs/2012.06850" target="_blank">arXiv:2012.06850</a> [<a href="http://arxiv.org/pdf/2012.06850" target="_blank">pdf</a>]

<h2>LiveChess2FEN: a Framework for Classifying Chess Pieces based on CNNs. (arXiv:2012.06858v1 [cs.CV])</h2>
<h3>David Mallas&#xe9;n Quintana, Alberto Antonio del Barrio Garc&#xed;a, Manuel Prieto Mat&#xed;as</h3>
<p>Automatic digitization of chess games using computer vision is a significant
technological challenge. This problem is of much interest for tournament
organizers and amateur or professional players to broadcast their
over-the-board (OTB) games online or analyze them using chess engines. Previous
work has shown promising results, but the recognition accuracy and the latency
of state-of-the-art techniques still need further enhancements to allow their
practical and affordable deployment. We have investigated how to implement them
on an Nvidia Jetson Nano single-board computer effectively. Our first
contribution has been accelerating the chessboard's detection algorithm.
Subsequently, we have analyzed different Convolutional Neural Networks for
chess piece classification and how to map them efficiently on our embedded
platform. Notably, we have implemented a functional framework that
automatically digitizes a chess position from an image in less than 1 second,
with 92% accuracy when classifying the pieces and 95% when detecting the board.
</p>
<a href="http://arxiv.org/abs/2012.06858" target="_blank">arXiv:2012.06858</a> [<a href="http://arxiv.org/pdf/2012.06858" target="_blank">pdf</a>]

<h2>Spectral Unmixing With Multinomial Mixture Kernel and Wasserstein Generative Adversarial Loss. (arXiv:2012.06859v1 [cs.CV])</h2>
<h3>Savas Ozkan, Gozde Bozdagi Akar</h3>
<p>This study proposes a novel framework for spectral unmixing by using 1D
convolution kernels and spectral uncertainty. High-level representations are
computed from data, and they are further modeled with the Multinomial Mixture
Model to estimate fractions under severe spectral uncertainty. Furthermore, a
new trainable uncertainty term based on a nonlinear neural network model is
introduced in the reconstruction step. All uncertainty models are optimized by
Wasserstein Generative Adversarial Network (WGAN) to improve stability and
capture uncertainty. Experiments are performed on both real and synthetic
datasets. The results validate that the proposed method obtains
state-of-the-art performance, especially for the real datasets compared to the
baselines. Project page at: https://github.com/savasozkan/dscn.
</p>
<a href="http://arxiv.org/abs/2012.06859" target="_blank">arXiv:2012.06859</a> [<a href="http://arxiv.org/pdf/2012.06859" target="_blank">pdf</a>]

<h2>AMINN: Autoencoder-based Multiple Instance Neural Network for Outcome Prediction of Multifocal Liver Metastases. (arXiv:2012.06875v1 [cs.CV])</h2>
<h3>Jianan Chen, Helen M. C. Cheung, Laurent Milot, Anne L. Martel</h3>
<p>Colorectal cancer is one of the most common and lethal cancers and colorectal
cancer liver metastases (CRLM) is the major cause of death in patients with
colorectal cancer. Multifocality occurs frequently in CRLM, but is relatively
unexplored in CRLM outcome prediction. Most existing clinical and imaging
biomarkers do not take the imaging features of all multifocal lesions into
account. In this paper, we present an end-to-end autoencoder-based multiple
instance neural network (AMINN) for the prediction of survival outcomes in
multifocal CRLM patients using radiomic features extracted from
contrast-enhanced MRIs. Specifically, we jointly train an autoencoder to
reconstruct input features and a multiple instance network to make predictions
by aggregating information from all tumour lesions of a patient. In addition,
we incorporate a two-step normalization technique to improve the training of
deep neural networks, built on the observation that the distributions of
radiomic features are almost always severely skewed. Experimental results
empirically validated our hypothesis that incorporating imaging features of all
lesions improves outcome prediction for multifocal cancer. The proposed ADMINN
framework achieved an area under the ROC curve (AUC) of 0.70, which is 19.5%
higher than baseline methods. We built a risk score based on the outputs of our
network and compared it to other clinical and imaging biomarkers. Our risk
score is the only one that achieved statistical significance in univariate and
multivariate cox proportional hazard modeling in our cohort of multifocal CRLM
patients. The effectiveness of incorporating all lesions and applying two-step
normalization is demonstrated by a series of ablation studies. Our code will be
released after the peer-review process.
</p>
<a href="http://arxiv.org/abs/2012.06875" target="_blank">arXiv:2012.06875</a> [<a href="http://arxiv.org/pdf/2012.06875" target="_blank">pdf</a>]

<h2>Normalized Label Distribution: Towards Learning Calibrated, Adaptable and Efficient Activation Maps. (arXiv:2012.06876v1 [cs.LG])</h2>
<h3>Utkarsh Uppal, Bharat Giddwani</h3>
<p>The vulnerability of models to data aberrations and adversarial attacks
influences their ability to demarcate distinct class boundaries efficiently.
The network's confidence and uncertainty play a pivotal role in weight
adjustments and the extent of acknowledging such attacks. In this paper, we
address the trade-off between the accuracy and calibration potential of a
classification network. We study the significance of ground-truth distribution
changes on the performance and generalizability of various state-of-the-art
networks and compare the proposed method's response to unanticipated attacks.
Furthermore, we demonstrate the role of label-smoothing regularization and
normalization in yielding better generalizability and calibrated probability
distribution by proposing normalized soft labels to enhance the calibration of
feature maps. Subsequently, we substantiate our inference by translating
conventional convolutions to padding based partial convolution to establish the
tangible impact of corrections in reinforcing the performance and convergence
rate. We graphically elucidate the implication of such variations with the
critical purpose of corroborating the reliability and reproducibility for
multiple datasets.
</p>
<a href="http://arxiv.org/abs/2012.06876" target="_blank">arXiv:2012.06876</a> [<a href="http://arxiv.org/pdf/2012.06876" target="_blank">pdf</a>]

<h2>Revisiting "Qualitatively Characterizing Neural Network Optimization Problems". (arXiv:2012.06898v1 [cs.LG])</h2>
<h3>Jonathan Frankle</h3>
<p>We revisit and extend the experiments of Goodfellow et al. (2014), who showed
that - for then state-of-the-art networks - "the objective function has a
simple, approximately convex shape" along the linear path between
initialization and the trained weights. We do not find this to be the case for
modern networks on CIFAR-10 and ImageNet. Instead, although loss is roughly
monotonically non-increasing along this path, it remains high until close to
the optimum. In addition, training quickly becomes linearly separated from the
optimum by loss barriers. We conclude that, although Goodfellow et al.'s
findings describe the "relatively easy to optimize" MNIST setting, behavior is
qualitatively different in modern settings.
</p>
<a href="http://arxiv.org/abs/2012.06898" target="_blank">arXiv:2012.06898</a> [<a href="http://arxiv.org/pdf/2012.06898" target="_blank">pdf</a>]

<h2>Semi-supervised reward learning for offline reinforcement learning. (arXiv:2012.06899v1 [cs.LG])</h2>
<h3>Ksenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed, Serkan Cabi, Nando de Freitas</h3>
<p>In offline reinforcement learning (RL) agents are trained using a logged
dataset. It appears to be the most natural route to attack real-life
applications because in domains such as healthcare and robotics interactions
with the environment are either expensive or unethical. Training agents usually
requires reward functions, but unfortunately, rewards are seldom available in
practice and their engineering is challenging and laborious. To overcome this,
we investigate reward learning under the constraint of minimizing human reward
annotations. We consider two types of supervision: timestep annotations and
demonstrations. We propose semi-supervised learning algorithms that learn from
limited annotations and incorporate unlabelled data. In our experiments with a
simulated robotic arm, we greatly improve upon behavioural cloning and closely
approach the performance achieved with ground truth rewards. We further
investigate the relationship between the quality of the reward model and the
final policies. We notice, for example, that the reward models do not need to
be perfect to result in useful policies.
</p>
<a href="http://arxiv.org/abs/2012.06899" target="_blank">arXiv:2012.06899</a> [<a href="http://arxiv.org/pdf/2012.06899" target="_blank">pdf</a>]

<h2>PAIRS AutoGeo: an Automated Machine Learning Framework for Massive Geospatial Data. (arXiv:2012.06907v1 [cs.CV])</h2>
<h3>Wang Zhou, Levente J. Klein, Siyuan Lu</h3>
<p>An automated machine learning framework for geospatial data named PAIRS
AutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform.
The framework simplifies the development of industrial machine learning
solutions leveraging geospatial data to the extent that the user inputs are
minimized to merely a text file containing labeled GPS coordinates. PAIRS
AutoGeo automatically gathers required data at the location coordinates,
assembles the training data, performs quality check, and trains multiple
machine learning models for subsequent deployment. The framework is validated
using a realistic industrial use case of tree species classification.
Open-source tree species data are used as the input to train a random forest
classifier and a modified ResNet model for 10-way tree species classification
based on aerial imagery, which leads to an accuracy of $59.8\%$ and $81.4\%$,
respectively. This use case exemplifies how PAIRS AutoGeo enables users to
leverage machine learning without extensive geospatial expertise.
</p>
<a href="http://arxiv.org/abs/2012.06907" target="_blank">arXiv:2012.06907</a> [<a href="http://arxiv.org/pdf/2012.06907" target="_blank">pdf</a>]

<h2>The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models. (arXiv:2012.06908v1 [cs.LG])</h2>
<h3>Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, Zhangyang Wang</h3>
<p>The computer vision world has been re-gaining enthusiasm in various
pre-trained models, including both classical ImageNet supervised pre-training
and recently emerged self-supervised pre-training such as simCLR and MoCo.
Pre-trained weights often boost a wide range of downstream tasks including
classification, detection, and segmentation. Latest studies suggest that the
pre-training benefits from gigantic model capacity. We are hereby curious and
ask: after pre-training, does a pre-trained model indeed have to stay large for
its universal downstream transferability?

In this paper, we examine the supervised and self-supervised pre-trained
models through the lens of lottery ticket hypothesis (LTH). LTH identifies
highly sparse matching subnetworks that can be trained in isolation from
(nearly) scratch, to reach the full models' performance. We extend the scope of
LTH to questioning whether matching subnetworks still exist in the pre-training
models, that enjoy the same downstream transfer performance. Our extensive
experiments convey an overall positive message: from all pre-trained weights
obtained by ImageNet classification, simCLR and MoCo, we are consistently able
to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer
universally to multiple downstream tasks, whose performance see no degradation
compared to using full pre-trained weights. Further analyses reveal that
subnetworks found from different pre-training tend to yield diverse mask
structures and perturbation sensitivities. We conclude that the core LTH
observations remain generally relevant in the pre-training paradigm of computer
vision, but more delicate discussions are needed in some cases. Codes and
pre-trained models will be made available at:
https://github.com/VITA-Group/CV_LTH_Pre-training.
</p>
<a href="http://arxiv.org/abs/2012.06908" target="_blank">arXiv:2012.06908</a> [<a href="http://arxiv.org/pdf/2012.06908" target="_blank">pdf</a>]

<h2>NP-ODE: Neural Process Aided Ordinary Differential Equations for Uncertainty Quantification of Finite Element Analysis. (arXiv:2012.06914v1 [cs.LG])</h2>
<h3>Yinan Wang, Kaiwen Wang, Wenjun Cai, Xiaowei Yue</h3>
<p>Finite element analysis (FEA) has been widely used to generate simulations of
complex and nonlinear systems. Despite its strength and accuracy, the
limitations of FEA can be summarized into two aspects: a) running high-fidelity
FEA often requires significant computational cost and consumes a large amount
of time; b) FEA is a deterministic method that is insufficient for uncertainty
quantification (UQ) when modeling complex systems with various types of
uncertainties. In this paper, a physics-informed data-driven surrogate model,
named Neural Process Aided Ordinary Differential Equation (NP-ODE), is proposed
to model the FEA simulations and capture both input and output uncertainties.
To validate the advantages of the proposed NP-ODE, we conduct experiments on
both the simulation data generated from a given ordinary differential equation
and the data collected from a real FEA platform for tribocorrosion. The
performances of the proposed NP-ODE and several benchmark methods are compared.
The results show that the proposed NP-ODE outperforms benchmark methods. The
NP-ODE method realizes the smallest predictive error as well as generates the
most reasonable confidence interval having the best coverage on testing data
points.
</p>
<a href="http://arxiv.org/abs/2012.06914" target="_blank">arXiv:2012.06914</a> [<a href="http://arxiv.org/pdf/2012.06914" target="_blank">pdf</a>]

<h2>Concept Drift Monitoring and Diagnostics of Supervised Learning Models via Score Vectors. (arXiv:2012.06916v1 [stat.ML])</h2>
<h3>Kungang Zhang, Anh T. Bui, Daniel W. Apley</h3>
<p>Supervised learning models are one of the most fundamental classes of models.
Viewing supervised learning from a probabilistic perspective, the set of
training data to which the model is fitted is usually assumed to follow a
stationary distribution. However, this stationarity assumption is often
violated in a phenomenon called concept drift, which refers to changes over
time in the predictive relationship between covariates $\mathbf{X}$ and a
response variable $Y$ and can render trained models suboptimal or obsolete. We
develop a comprehensive and computationally efficient framework for detecting,
monitoring, and diagnosing concept drift. Specifically, we monitor the Fisher
score vector, defined as the gradient of the log-likelihood for the fitted
model, using a form of multivariate exponentially weighted moving average,
which monitors for general changes in the mean of a random vector. In spite of
the substantial performance advantages that we demonstrate over popular
error-based methods, a score-based approach has not been previously considered
for concept drift monitoring. Advantages of the proposed score-based framework
include applicability to any parametric model, more powerful detection of
changes as shown in theory and experiments, and inherent diagnostic
capabilities for helping to identify the nature of the changes.
</p>
<a href="http://arxiv.org/abs/2012.06916" target="_blank">arXiv:2012.06916</a> [<a href="http://arxiv.org/pdf/2012.06916" target="_blank">pdf</a>]

<h2>Assessing The Importance Of Colours For CNNs In Object Recognition. (arXiv:2012.06917v1 [cs.CV])</h2>
<h3>Aditya Singh, Alessandro Bay, Andrea Mirabile</h3>
<p>Humans rely heavily on shapes as a primary cue for object recognition. As
secondary cues, colours and textures are also beneficial in this regard.
Convolutional neural networks (CNNs), an imitation of biological neural
networks, have been shown to exhibit conflicting properties. Some studies
indicate that CNNs are biased towards textures whereas, another set of studies
suggests shape bias for a classification task. However, they do not discuss the
role of colours, implying its possible humble role in the task of object
recognition. In this paper, we empirically investigate the importance of
colours in object recognition for CNNs. We are able to demonstrate that CNNs
often rely heavily on colour information while making a prediction. Our results
show that the degree of dependency on colours tend to vary from one dataset to
another. Moreover, networks tend to rely more on colours if trained from
scratch. Pre-training can allow the model to be less colour dependent. To
facilitate these findings, we follow the framework often deployed in
understanding role of colours in object recognition for humans. We evaluate a
model trained with congruent images (images in original colours eg. red
strawberries) on congruent, greyscale, and incongruent images (images in
unnatural colours eg. blue strawberries). We measure and analyse network's
predictive performance (top-1 accuracy) under these different stylisations. We
utilise standard datasets of supervised image classification and fine-grained
image classification in our experiments.
</p>
<a href="http://arxiv.org/abs/2012.06917" target="_blank">arXiv:2012.06917</a> [<a href="http://arxiv.org/pdf/2012.06917" target="_blank">pdf</a>]

<h2>Offline Policy Selection under Uncertainty. (arXiv:2012.06919v1 [cs.LG])</h2>
<h3>Mengjiao Yang, Bo Dai, Ofir Nachum, George Tucker, Dale Schuurmans</h3>
<p>The presence of uncertainty in policy evaluation significantly complicates
the process of policy ranking and selection in real-world settings. We formally
consider offline policy selection as learning preferences over a set of policy
prospects given a fixed experience dataset. While one can select or rank
policies based on point estimates of their policy values or high-confidence
intervals, access to the full distribution over one's belief of the policy
value enables more flexible selection algorithms under a wider range of
downstream evaluation metrics. We propose BayesDICE for estimating this belief
distribution in terms of posteriors of distribution correction ratios derived
from stochastic constraints (as opposed to explicit likelihood, which is not
available). Empirically, BayesDICE is highly competitive to existing
state-of-the-art approaches in confidence interval estimation. More
importantly, we show how the belief distribution estimated by BayesDICE may be
used to rank policies with respect to any arbitrary downstream policy selection
metric, and we empirically demonstrate that this selection procedure
significantly outperforms existing approaches, such as ranking policies
according to mean or high-confidence lower bound value estimates.
</p>
<a href="http://arxiv.org/abs/2012.06919" target="_blank">arXiv:2012.06919</a> [<a href="http://arxiv.org/pdf/2012.06919" target="_blank">pdf</a>]

<h2>Learning Symbolic Expressions via Gumbel-Max Equation Learner Network. (arXiv:2012.06921v1 [cs.LG])</h2>
<h3>Gang Chen</h3>
<p>Although modern machine learning, in particular deep learning, has achieved
outstanding success in scientific and engineering research, most of the neural
networks (NNs) learned via these state-of-the-art techniques are black-box
models. For a widespread success of machine learning in science and
engineering, it is important to develop new NN architectures to effectively
extract high-level mathematical knowledge from complex dataset. To meet this
research demand, this paper focuses on the symbolic regression problem and
develops a new NN architecture called the Gumbel-Max Equation Learner (GMEQL)
network. Different from previously proposed Equation Learner (EQL) networks,
GMEQL applies continuous relaxation to the network structure via the Gumbel-Max
trick and introduces two types of trainable parameters: structure parameters
and regression parameters. This paper also proposes a new two-stage training
process and new techniques to train structure parameters in both the online and
offline settings based on an elite repository. On 8 benchmark symbolic
regression problems, GMEQL is experimentally shown to outperform several
cutting-edge techniques for symbolic regression.
</p>
<a href="http://arxiv.org/abs/2012.06921" target="_blank">arXiv:2012.06921</a> [<a href="http://arxiv.org/pdf/2012.06921" target="_blank">pdf</a>]

<h2>Decimated Framelet System on Graphs and Fast G-Framelet Transforms. (arXiv:2012.06922v1 [cs.LG])</h2>
<h3>Xuebin Zheng, Bingxin Zhou, Yu Guang Wang, Xiaosheng Zhuang</h3>
<p>Graph representation learning has many real-world applications, from
super-resolution imaging, 3D computer vision to drug repurposing, protein
classification, social networks analysis. An adequate representation of graph
data is vital to the learning performance of a statistical or machine learning
model for graph-structured data. In this paper, we propose a novel multiscale
representation system for graph data, called decimated framelets, which form a
localized tight frame on the graph. The decimated framelet system allows
storage of the graph data representation on a coarse-grained chain and
processes the graph data at multi scales where at each scale, the data is
stored at a subgraph. Based on this, we then establish decimated G-framelet
transforms for the decomposition and reconstruction of the graph data at multi
resolutions via a constructive data-driven filter bank. The graph framelets are
built on a chain-based orthonormal basis that supports fast graph Fourier
transforms. From this, we give a fast algorithm for the decimated G-framelet
transforms, or FGT, that has linear computational complexity O(N) for a graph
of size N. The theory of decimated framelets and FGT is verified with numerical
examples for random graphs. The effectiveness is demonstrated by real-world
applications, including multiresolution analysis for traffic network, and graph
neural networks for graph classification tasks.
</p>
<a href="http://arxiv.org/abs/2012.06922" target="_blank">arXiv:2012.06922</a> [<a href="http://arxiv.org/pdf/2012.06922" target="_blank">pdf</a>]

<h2>Warm Starting CMA-ES for Hyperparameter Optimization. (arXiv:2012.06932v1 [cs.LG])</h2>
<h3>Masahiro Nomura, Shuhei Watanabe, Youhei Akimoto, Yoshihiko Ozaki, Masaki Onishi</h3>
<p>Hyperparameter optimization (HPO), formulated as black-box optimization
(BBO), is recognized as essential for automation and high performance of
machine learning approaches. The CMA-ES is a promising BBO approach with a high
degree of parallelism, and has been applied to HPO tasks, often under parallel
implementation, and shown superior performance to other approaches including
Bayesian optimization (BO). However, if the budget of hyperparameter
evaluations is severely limited, which is often the case for end users who do
not deserve parallel computing, the CMA-ES exhausts the budget without
improving the performance due to its long adaptation phase, resulting in being
outperformed by BO approaches. To address this issue, we propose to transfer
prior knowledge on similar HPO tasks through the initialization of the CMA-ES,
leading to significantly shortening the adaptation time. The knowledge transfer
is designed based on the novel definition of task similarity, with which the
correlation of the performance of the proposed approach is confirmed on
synthetic problems. The proposed warm starting CMA-ES, called WS-CMA-ES, is
applied to different HPO tasks where some prior knowledge is available, showing
its superior performance over the original CMA-ES as well as BO approaches with
or without using the prior knowledge.
</p>
<a href="http://arxiv.org/abs/2012.06932" target="_blank">arXiv:2012.06932</a> [<a href="http://arxiv.org/pdf/2012.06932" target="_blank">pdf</a>]

<h2>Human Pose Transfer by Adaptive Hierarchical Deformation. (arXiv:2012.06940v1 [cs.CV])</h2>
<h3>Jinsong Zhang, Xingzi Liu, Kun Li</h3>
<p>Human pose transfer, as a misaligned image generation task, is very
challenging. Existing methods cannot effectively utilize the input information,
which often fail to preserve the style and shape of hair and clothes. In this
paper, we propose an adaptive human pose transfer network with two hierarchical
deformation levels. The first level generates human semantic parsing aligned
with the target pose, and the second level generates the final textured person
image in the target pose with the semantic guidance. To avoid the drawback of
vanilla convolution that treats all the pixels as valid information, we use
gated convolution in both two levels to dynamically select the important
features and adaptively deform the image layer by layer. Our model has very few
parameters and is fast to converge. Experimental results demonstrate that our
model achieves better performance with more consistent hair, face and clothes
with fewer parameters than state-of-the-art methods. Furthermore, our method
can be applied to clothing texture transfer.
</p>
<a href="http://arxiv.org/abs/2012.06940" target="_blank">arXiv:2012.06940</a> [<a href="http://arxiv.org/pdf/2012.06940" target="_blank">pdf</a>]

<h2>MiniVLM: A Smaller and Faster Vision-Language Model. (arXiv:2012.06946v1 [cs.CV])</h2>
<h3>Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, Zicheng Liu</h3>
<p>Recent vision-language (VL) studies have shown remarkable progress by
learning generic representations from massive image-text pairs with transformer
models and then fine-tuning on downstream VL tasks. While existing research has
been focused on achieving high accuracy with large pre-trained models, building
a lightweight model is of great value in practice but is less explored. In this
paper, we propose a smaller and faster VL model, MiniVLM, which can be
finetuned with good performance on various downstream tasks like its larger
counterpart. MiniVLM consists of two modules, a vision feature extractor and a
transformer-based vision-language fusion module. We design a Two-stage
Efficient feature Extractor (TEE), inspired by the one-stage EfficientDet
network, to significantly reduce the time cost of visual feature extraction by
$95\%$, compared to a baseline model. We adopt the MiniLM structure to reduce
the computation cost of the transformer module after comparing different
compact BERT models. In addition, we improve the MiniVLM pre-training by adding
$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art
captioning model. We also pre-train with high-quality image tags obtained from
a strong tagging model to enhance cross-modality alignment. The large models
are used offline without adding any overhead in fine-tuning and inference. With
the above design choices, our MiniVLM reduces the model size by $73\%$ and the
inference time cost by $94\%$ while being able to retain $94-97\%$ of the
accuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the
state-of-the-art VL research for on-the-edge applications.
</p>
<a href="http://arxiv.org/abs/2012.06946" target="_blank">arXiv:2012.06946</a> [<a href="http://arxiv.org/pdf/2012.06946" target="_blank">pdf</a>]

<h2>Using Computer Vision to Automate Hand Detection and Tracking of Surgeon Movements in Videos of Open Surgery. (arXiv:2012.06948v1 [cs.CV])</h2>
<h3>Michael Zhang, Xiaotian Cheng, Daniel Copeland, Arjun Desai, Melody Y. Guan, Gabriel A. Brat, Serena Yeung</h3>
<p>Open, or non-laparoscopic surgery, represents the vast majority of all
operating room procedures, but few tools exist to objectively evaluate these
techniques at scale. Current efforts involve human expert-based visual
assessment. We leverage advances in computer vision to introduce an automated
approach to video analysis of surgical execution. A state-of-the-art
convolutional neural network architecture for object detection was used to
detect operating hands in open surgery videos. Automated assessment was
expanded by combining model predictions with a fast object tracker to enable
surgeon-specific hand tracking. To train our model, we used publicly available
videos of open surgery from YouTube and annotated these with spatial bounding
boxes of operating hands. Our model's spatial detections of operating hands
significantly outperforms the detections achieved using pre-existing
hand-detection datasets, and allow for insights into intra-operative movement
patterns and economy of motion.
</p>
<a href="http://arxiv.org/abs/2012.06948" target="_blank">arXiv:2012.06948</a> [<a href="http://arxiv.org/pdf/2012.06948" target="_blank">pdf</a>]

<h2>Attentional Biased Stochastic Gradient for Imbalanced Classification. (arXiv:2012.06951v1 [cs.LG])</h2>
<h3>Qi Qi, Yi Xu, Rong Jin, Wotao Yin, Tianbao Yang</h3>
<p>In this paper~\footnote{The original title is "Momentum SGD with Robust
Weighting For Imbalanced Classification"}, we present a simple yet effective
method (ABSGD) for addressing the data imbalance issue in deep learning. Our
method is a simple modification to momentum SGD where we leverage an
attentional mechanism to assign an individual importance weight to each
gradient in the mini-batch. Unlike existing individual weighting methods that
learn the individual weights by meta-learning on a separate balanced validation
data, our weighting scheme is self-adaptive and is grounded in distributionally
robust optimization. The weight of a sampled data is systematically
proportional to exponential of a scaled loss value of the data, where the
scaling factor is interpreted as the regularization parameter in the framework
of information-regularized distributionally robust optimization. We employ a
step damping strategy for the scaling factor to balance between the learning of
feature extraction layers and the learning of the classifier layer. Compared
with exiting meta-learning methods that require three backward propagations for
computing mini-batch stochastic gradients at three different points at each
iteration, our method is more efficient with only one backward propagation at
each iteration as in standard deep learning methods. Compared with existing
class-level weighting schemes, our method can be applied to online learning
without any knowledge of class prior, while enjoying further performance boost
in offline learning combined with existing class-level weighting schemes. Our
empirical studies on several benchmark datasets also demonstrate the
effectiveness of our proposed method
</p>
<a href="http://arxiv.org/abs/2012.06951" target="_blank">arXiv:2012.06951</a> [<a href="http://arxiv.org/pdf/2012.06951" target="_blank">pdf</a>]

<h2>MEME: Generating RNN Model Explanations via Model Extraction. (arXiv:2012.06954v1 [cs.LG])</h2>
<h3>Dmitry Kazhdan, Botty Dimanov, Mateja Jamnik, Pietro Li&#xf2;</h3>
<p>Recurrent Neural Networks (RNNs) have achieved remarkable performance on a
range of tasks. A key step to further empowering RNN-based approaches is
improving their explainability and interpretability. In this work we present
MEME: a model extraction approach capable of approximating RNNs with
interpretable models represented by human-understandable concepts and their
interactions. We demonstrate how MEME can be applied to two multivariate,
continuous data case studies: Room Occupation Prediction, and In-Hospital
Mortality Prediction. Using these case-studies, we show how our extracted
models can be used to interpret RNNs both locally and globally, by
approximating RNN decision-making via interpretable concept interactions.
</p>
<a href="http://arxiv.org/abs/2012.06954" target="_blank">arXiv:2012.06954</a> [<a href="http://arxiv.org/pdf/2012.06954" target="_blank">pdf</a>]

<h2>Learn-Prune-Share for Lifelong Learning. (arXiv:2012.06956v1 [cs.LG])</h2>
<h3>Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, Stratis Ioannidis</h3>
<p>In lifelong learning, we wish to maintain and update a model (e.g., a neural
network classifier) in the presence of new classification tasks that arrive
sequentially. In this paper, we propose a learn-prune-share (LPS) algorithm
which addresses the challenges of catastrophic forgetting, parsimony, and
knowledge reuse simultaneously. LPS splits the network into task-specific
partitions via an ADMM-based pruning strategy. This leads to no forgetting,
while maintaining parsimony. Moreover, LPS integrates a novel selective
knowledge sharing scheme into this ADMM optimization framework. This enables
adaptive knowledge sharing in an end-to-end fashion. Comprehensive experimental
results on two lifelong learning benchmark datasets and a challenging
real-world radio frequency fingerprinting dataset are provided to demonstrate
the effectiveness of our approach. Our experiments show that LPS consistently
outperforms multiple state-of-the-art competitors.
</p>
<a href="http://arxiv.org/abs/2012.06956" target="_blank">arXiv:2012.06956</a> [<a href="http://arxiv.org/pdf/2012.06956" target="_blank">pdf</a>]

<h2>Open-World Class Discovery with Kernel Networks. (arXiv:2012.06957v1 [cs.LG])</h2>
<h3>Zifeng Wang, Batool Salehi, Andrey Gritsenko, Kaushik Chowdhury, Stratis Ioannidis, Jennifer Dy</h3>
<p>We study an Open-World Class Discovery problem in which, given labeled
training samples from old classes, we need to discover new classes from
unlabeled test samples. There are two critical challenges to addressing this
paradigm: (a) transferring knowledge from old to new classes, and (b)
incorporating knowledge learned from new classes back to the original model. We
propose Class Discovery Kernel Network with Expansion (CD-KNet-Exp), a deep
learning framework, which utilizes the Hilbert Schmidt Independence Criterion
to bridge supervised and unsupervised information together in a systematic way,
such that the learned knowledge from old classes is distilled appropriately for
discovering new classes. Compared to competing methods, CD-KNet-Exp shows
superior performance on three publicly available benchmark datasets and a
challenging real-world radio frequency fingerprinting dataset.
</p>
<a href="http://arxiv.org/abs/2012.06957" target="_blank">arXiv:2012.06957</a> [<a href="http://arxiv.org/pdf/2012.06957" target="_blank">pdf</a>]

<h2>Online Stochastic Optimization with Wasserstein Based Non-stationarity. (arXiv:2012.06961v1 [cs.LG])</h2>
<h3>Jiashuo Jiang, Xiaocheng Li, Jiawei Zhang</h3>
<p>We consider a general online stochastic optimization problem with multiple
budget constraints over a horizon of finite time periods. At each time period,
a reward function and multiple cost functions, where each cost function is
involved in the consumption of one corresponding budget, are drawn from an
unknown distribution, which is assumed to be non-stationary across time. Then,
a decision maker needs to specify an action from a convex and compact action
set to collect the reward, and the consumption each budget is determined
jointly by the cost functions and the taken action. The objective of the
decision maker is to maximize the cumulative reward subject to the budget
constraints. Our model captures a wide range of applications including online
linear programming and network revenue management, among others. In this paper,
we design near-optimal policies for the decision maker under the following two
specific settings: a data-driven setting where the decision maker is given
prior estimates of the distributions beforehand and a no information setting
where the distributions are completely unknown to the decision maker. Under
each setting, we propose a new Wasserstein-distance based measure to measure
the non-stationarity of the distributions at different time periods and show
that this measure leads to a necessary and sufficient condition for the
attainability of a sublinear regret. For the first setting, we propose a new
algorithm which blends gradient descent steps with the prior estimates. We then
adapt our algorithm for the second setting and propose another gradient descent
based algorithm. We show that under both settings, our polices achieve a regret
upper bound of optimal order. Moreover, our policies could be naturally
incorporated with a re-solving procedure which further boosts the empirical
performance in numerical experiments.
</p>
<a href="http://arxiv.org/abs/2012.06961" target="_blank">arXiv:2012.06961</a> [<a href="http://arxiv.org/pdf/2012.06961" target="_blank">pdf</a>]

<h2>Fully-Automated Liver Tumor Localization and Characterization from Multi-Phase MR Volumes Using Key-Slice ROI Parsing: A Physician-Inspired Approach. (arXiv:2012.06964v1 [cs.CV])</h2>
<h3>Bolin Lai, Xiaoyu Bai, Yuhsuan Wu, Xiao-Yun Zhou, Jinzheng Cai, Yuankai Huo, Lingyun Huang, Peng Wang, Yong Xia, Le Lu, Adam Harrison, Heping Hu, Jing Xiao</h3>
<p>Using radiological scans to identify liver tumors is crucial for proper
patient treatment. This is highly challenging, as top radiologists only achieve
F1 scores of roughly 80% (hepatocellular carcinoma (HCC) vs. others) with only
moderate inter-rater agreement, even when using multi-phase magnetic resonance
(MR) imagery. Thus, there is great impetus for computer-aided diagnosis (CAD)
solutions. A critical challengeis to reliably parse a 3D MR volume to localize
diagnosable regions of interest (ROI). In this paper, we break down this
problem using a key-slice parser (KSP), which emulates physician workflows by
first identifying key slices and then localize their corresponding key ROIs.
Because performance demands are so extreme, (not to miss any key ROI),our KSP
integrates complementary modules--top-down classification-plus-detection (CPD)
and bottom-up localization-by-over-segmentation(LBOS). The CPD uses a
curve-parsing and detection confidence to re-weight classifier confidences. The
LBOS uses over-segmentation to flag CPD failure cases and provides its own
ROIs. For scalability, LBOS is only weakly trained on pseudo-masks using a new
distance-aware Tversky loss. We evaluate our approach on the largest
multi-phase MR liver lesion test dataset to date (430 biopsy-confirmed
patients). Experiments demonstrate that our KSP can localize diagnosable ROIs
with high reliability (85% patients have an average overlap of &gt;= 40% with the
ground truth). Moreover, we achieve an HCC vs. others F1 score of 0.804,
providing a fully-automated CAD solution comparable with top human physicians.
</p>
<a href="http://arxiv.org/abs/2012.06964" target="_blank">arXiv:2012.06964</a> [<a href="http://arxiv.org/pdf/2012.06964" target="_blank">pdf</a>]

<h2>Predicting Generalization in Deep Learning via Local Measures of Distortion. (arXiv:2012.06969v1 [stat.ML])</h2>
<h3>Abhejit Rajagopal, Vamshi C. Madala, Shivkumar Chandrasekaran, Peder E. Z. Larson</h3>
<p>We study generalization in deep learning by appealing to complexity measures
originally developed in approximation and information theory. While these
concepts are challenged by the high-dimensional and data-defined nature of deep
learning, we show that simple vector quantization approaches such as PCA, GMMs,
and SVMs capture their spirit when applied layer-wise to deep extracted
features giving rise to relatively inexpensive complexity measures that
correlate well with generalization performance. We discuss our results in 2020
NeurIPS PGDL challenge.
</p>
<a href="http://arxiv.org/abs/2012.06969" target="_blank">arXiv:2012.06969</a> [<a href="http://arxiv.org/pdf/2012.06969" target="_blank">pdf</a>]

<h2>Spontaneous Emotion Recognition from Facial Thermal Images. (arXiv:2012.06973v1 [cs.CV])</h2>
<h3>Chirag Kyal</h3>
<p>One of the key research areas in computer vision addressed by a vast number
of publications is the processing and understanding of images containing human
faces. The most often addressed tasks include face detection, facial landmark
localization, face recognition and facial expression analysis. Other, more
specialized tasks such as affective computing, the extraction of vital signs
from videos or analysis of social interaction usually require one or several of
the aforementioned tasks that have to be performed. In our work, we analyze
that a large number of tasks for facial image processing in thermal infrared
images that are currently solved using specialized rule-based methods or not
solved at all can be addressed with modern learning-based approaches. We have
used USTC-NVIE database for training of a number of machine learning algorithms
for facial landmark localization.
</p>
<a href="http://arxiv.org/abs/2012.06973" target="_blank">arXiv:2012.06973</a> [<a href="http://arxiv.org/pdf/2012.06973" target="_blank">pdf</a>]

<h2>MVFNet: Multi-View Fusion Network for Efficient Video Recognition. (arXiv:2012.06977v1 [cs.CV])</h2>
<h3>Wenhao Wu, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, Errui Ding</h3>
<p>Conventionally, spatiotemporal modeling network and its complexity are the
two most concentrated research topics in video action recognition. Existing
state-of-the-art methods have achieved excellent accuracy regardless of the
complexity meanwhile efficient spatiotemporal modeling solutions are slightly
inferior in performance. In this paper, we attempt to acquire both efficiency
and effectiveness simultaneously. First of all, besides traditionally treating
H x W x T video frames as space-time signal (viewing from the Height-Width
spatial plane), we propose to also model video from the other two Height-Time
and Width-Time planes, to capture the dynamics of video thoroughly. Secondly,
our model is designed based on 2D CNN backbones and model complexity is well
kept in mind by design. Specifically, we introduce a novel multi-view fusion
(MVF) module to exploit video dynamics using separable convolution for
efficiency. It is a plug-and-play module and can be inserted into off-the-shelf
2D CNNs to form a simple yet effective model called MVFNet. Moreover, MVFNet
can be thought of as a generalized video modeling framework and it can
specialize to be existing methods such as C2D, SlowOnly, and TSM under
different settings. Extensive experiments are conducted on popular benchmarks
(i.e., Something-Something V1 &amp; V2, Kinetics, UCF-101, and HMDB-51) to show its
superiority. The proposed MVFNet can achieve state-of-the-art performance with
2D CNN's complexity.
</p>
<a href="http://arxiv.org/abs/2012.06977" target="_blank">arXiv:2012.06977</a> [<a href="http://arxiv.org/pdf/2012.06977" target="_blank">pdf</a>]

<h2>Active Feature Selection for the Mutual Information Criterion. (arXiv:2012.06979v1 [cs.LG])</h2>
<h3>Shachar Schnapp, Sivan Sabato</h3>
<p>We study active feature selection, a novel feature selection setting in which
unlabeled data is available, but the budget for labels is limited, and the
examples to label can be actively selected by the algorithm. We focus on
feature selection using the classical mutual information criterion, which
selects the $k$ features with the largest mutual information with the label. In
the active feature selection setting, the goal is to use significantly fewer
labels than the data set size and still find $k$ features whose mutual
information with the label based on the \emph{entire} data set is large. We
explain and experimentally study the choices that we make in the algorithm, and
show that they lead to a successful algorithm, compared to other more naive
approaches. Our design draws on insights which relate the problem of active
feature selection to the study of pure-exploration multi-armed bandits
settings. While we focus here on mutual information, our general methodology
can be adapted to other feature-quality measures as well. The code is available
at the following url: https://github.com/ShacharSchnapp/ActiveFeatureSelection.
</p>
<a href="http://arxiv.org/abs/2012.06979" target="_blank">arXiv:2012.06979</a> [<a href="http://arxiv.org/pdf/2012.06979" target="_blank">pdf</a>]

<h2>GeoNet++: Iterative Geometric Neural Network with Edge-Aware Refinement for Joint Depth and Surface Normal Estimation. (arXiv:2012.06980v1 [cs.CV])</h2>
<h3>Xiaojuan Qi, Zhengzhe Liu, Renjie Liao, Philip H.S. Torr, Raquel Urtasun, Jiaya Jia</h3>
<p>In this paper, we propose a geometric neural network with edge-aware
refinement (GeoNet++) to jointly predict both depth and surface normal maps
from a single image. Building on top of two-stream CNNs, GeoNet++ captures the
geometric relationships between depth and surface normals with the proposed
depth-to-normal and normal-to-depth modules. In particular, the
"depth-to-normal" module exploits the least square solution of estimating
surface normals from depth to improve their quality, while the
"normal-to-depth" module refines the depth map based on the constraints on
surface normals through kernel regression. Boundary information is exploited
via an edge-aware refinement module. GeoNet++ effectively predicts depth and
surface normals with strong 3D consistency and sharp boundaries resulting in
better reconstructed 3D scenes. Note that GeoNet++ is generic and can be used
in other depth/normal prediction frameworks to improve the quality of 3D
reconstruction and pixel-wise accuracy of depth and surface normals.
Furthermore, we propose a new 3D geometric metric (3DGM) for evaluating depth
prediction in 3D. In contrast to current metrics that focus on evaluating
pixel-wise error/accuracy, 3DGM measures whether the predicted depth can
reconstruct high-quality 3D surface normals. This is a more natural metric for
many 3D application domains. Our experiments on NYUD-V2 and KITTI datasets
verify that GeoNet++ produces fine boundary details, and the predicted depth
can be used to reconstruct high-quality 3D surfaces. Code has been made
publicly available.
</p>
<a href="http://arxiv.org/abs/2012.06980" target="_blank">arXiv:2012.06980</a> [<a href="http://arxiv.org/pdf/2012.06980" target="_blank">pdf</a>]

<h2>Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v1 [cs.CV])</h2>
<h3>Xiangyun Zhao, Raviteja Vemulapalli, Philip Mansfield, Boqing Gong, Bradley Green, Lior Shapira, Ying Wu</h3>
<p>Collecting labeled data for the task of semantic segmentation is expensive
and time-consuming, as it requires dense pixel-level annotations. While recent
Convolutional Neural Network (CNN) based semantic segmentation approaches have
achieved impressive results by using large amounts of labeled training data,
their performance drops significantly as the amount of labeled data decreases.
This happens because deep CNNs trained with the de facto cross-entropy loss can
easily overfit to small amounts of labeled data. To address this issue, we
propose a simple and effective contrastive learning-based training strategy in
which we first pretrain the network using a pixel-wise class label-based
contrastive loss, and then fine-tune it using the cross-entropy loss. This
approach increases intra-class compactness and inter-class separability thereby
resulting in a better pixel classifier. We demonstrate the effectiveness of the
proposed training strategy in both fully-supervised and semi-supervised
settings using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our
results show that pretraining with label-based contrastive loss results in
large performance gains (more than 20% absolute improvement in some settings)
when the amount of labeled data is limited.
</p>
<a href="http://arxiv.org/abs/2012.06985" target="_blank">arXiv:2012.06985</a> [<a href="http://arxiv.org/pdf/2012.06985" target="_blank">pdf</a>]

<h2>Bi-Classifier Determinacy Maximization for Unsupervised Domain Adaptation. (arXiv:2012.06995v1 [cs.CV])</h2>
<h3>Shuang Li, Fangrui Lv, Binhui Xie, Chi Harold Liu, Jian Liang, Chen Qin</h3>
<p>Unsupervised domain adaptation challenges the problem of transferring
knowledge from a well-labelled source domain to an unlabelled target domain.
Recently,adversarial learning with bi-classifier has been proven effective in
pushing cross-domain distributions close. Prior approaches typically leverage
the disagreement between bi-classifier to learn transferable representations,
however, they often neglect the classifier determinacy in the target domain,
which could result in a lack of feature discriminability. In this paper, we
present a simple yet effective method, namely Bi-Classifier Determinacy
Maximization(BCDM), to tackle this problem. Motivated by the observation that
target samples cannot always be separated distinctly by the decision boundary,
here in the proposed BCDM, we design a novel classifier determinacy disparity
(CDD) metric, which formulates classifier discrepancy as the class relevance of
distinct target predictions and implicitly introduces constraint on the target
feature discriminability. To this end, the BCDM can generate discriminative
representations by encouraging target predictive outputs to be consistent and
determined, meanwhile, preserve the diversity of predictions in an adversarial
manner. Furthermore, the properties of CDD as well as the theoretical
guarantees of BCDM's generalization bound are both elaborated. Extensive
experiments show that BCDM compares favorably against the existing
state-of-the-art domain adaptation methods.
</p>
<a href="http://arxiv.org/abs/2012.06995" target="_blank">arXiv:2012.06995</a> [<a href="http://arxiv.org/pdf/2012.06995" target="_blank">pdf</a>]

<h2>KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning. (arXiv:2012.07000v1 [cs.AI])</h2>
<h3>Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao</h3>
<p>Reasoning is a critical ability towards complete visual understanding. To
develop machine with cognition-level visual understanding and reasoning
abilities, the visual commonsense reasoning (VCR) task has been introduced. In
VCR, given a challenging question about an image, a machine must answer
correctly and then provide a rationale justifying its answer. The methods
adopting the powerful BERT model as the backbone for learning joint
representation of image content and natural language have shown promising
improvements on VCR. However, none of the existing methods have utilized
commonsense knowledge in visual commonsense reasoning, which we believe will be
greatly helpful in this task. With the support of commonsense knowledge,
complex questions even if the required information is not depicted in the image
can be answered with cognitive reasoning. Therefore, we incorporate commonsense
knowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced
Visual-and-Linguistic BERT (KVL-BERT for short) model. Besides taking visual
and linguistic contents as input, external commonsense knowledge extracted from
ConceptNet is integrated into the multi-layer Transformer. In order to reserve
the structural information and semantic representation of the original
sentence, we propose using relative position embedding and mask-self-attention
to weaken the effect between the injected commonsense knowledge and other
unrelated components in the input sequence. Compared to other task-specific
models and general task-agnostic pre-training models, our KVL-BERT outperforms
them by a large margin.
</p>
<a href="http://arxiv.org/abs/2012.07000" target="_blank">arXiv:2012.07000</a> [<a href="http://arxiv.org/pdf/2012.07000" target="_blank">pdf</a>]

<h2>Effective multi-view registration of point sets based on student's t mixture model. (arXiv:2012.07002v1 [cs.CV])</h2>
<h3>Yanlin Ma, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, Yaochen Li</h3>
<p>Recently, Expectation-maximization (EM) algorithm has been introduced as an
effective means to solve multi-view registration problem. Most of the previous
methods assume that each data point is drawn from the Gaussian Mixture Model
(GMM), which is difficult to deal with the noise with heavy-tail or outliers.
Accordingly, this paper proposed an effective registration method based on
Student's t Mixture Model (StMM). More specially, we assume that each data
point is drawn from one unique StMM, where its nearest neighbors (NNs) in other
point sets are regarded as the t-distribution centroids with equal covariances,
membership probabilities, and fixed degrees of freedom. Based on this
assumption, the multi-view registration problem is formulated into the
maximization of the likelihood function including all rigid transformations.
Subsequently, the EM algorithm is utilized to optimize rigid transformations as
well as the only t-distribution covariance for multi-view registration. Since
only a few model parameters require to be optimized, the proposed method is
more likely to obtain the desired registration results. Besides, all
t-distribution centroids can be obtained by the NN search method, it is very
efficient to achieve multi-view registration. What's more, the t-distribution
takes the noise with heavy-tail into consideration, which makes the proposed
method be inherently robust to noises and outliers. Experimental results tested
on benchmark data sets illustrate its superior performance on robustness and
accuracy over state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.07002" target="_blank">arXiv:2012.07002</a> [<a href="http://arxiv.org/pdf/2012.07002" target="_blank">pdf</a>]

<h2>Split then Refine: Stacked Attention-guided ResUNets for Blind Single Image Visible Watermark Removal. (arXiv:2012.07007v1 [cs.CV])</h2>
<h3>Xiaodong Cun, Chi-Man Pun</h3>
<p>Digital watermark is a commonly used technique to protect the copyright of
medias. Simultaneously, to increase the robustness of watermark, attacking
technique, such as watermark removal, also gets the attention from the
community. Previous watermark removal methods require to gain the watermark
location from users or train a multi-task network to recover the background
indiscriminately. However, when jointly learning, the network performs better
on watermark detection than recovering the texture. Inspired by this
observation and to erase the visible watermarks blindly, we propose a novel
two-stage framework with a stacked attention-guided ResUNets to simulate the
process of detection, removal and refinement. In the first stage, we design a
multi-task network called SplitNet. It learns the basis features for three
sub-tasks altogether while the task-specific features separately use multiple
channel attentions. Then, with the predicted mask and coarser restored image,
we design RefineNet to smooth the watermarked region with a mask-guided spatial
attention. Besides network structure, the proposed algorithm also combines
multiple perceptual losses for better quality both visually and numerically. We
extensively evaluate our algorithm over four different datasets under various
settings and the experiments show that our approach outperforms other
state-of-the-art methods by a large margin. The code is available at
this http URL
</p>
<a href="http://arxiv.org/abs/2012.07007" target="_blank">arXiv:2012.07007</a> [<a href="http://arxiv.org/pdf/2012.07007" target="_blank">pdf</a>]

<h2>Efficient Online Trajectory Planning for Integrator Chain Dynamics using Polynomial Elimination. (arXiv:2012.07029v1 [cs.RO])</h2>
<h3>Florentin Rauscher, Oliver Sawodny</h3>
<p>Providing smooth reference trajectories can effectively increase performance
and accuracy of tracking control applications while overshoot and unwanted
vibrations are reduced. Trajectory planning computations can often be
simplified significantly by transforming the system dynamics into decoupled
integrator chains using methods such as feedback linearization, differential
flatness or the controller canonical form. We present an efficient method to
plan time optimal trajectories for integrator chains subject to derivative
bound constraints. Therefore, an algebraic precomputation algorithm formulates
the necessary conditions for time optimality in form of a set of polynomial
systems, followed by a symbolic polynomial elimination using Gr\"obner bases. A
fast online algorithm then plans the trajectories by calculating the roots of
the decomposed polynomial systems. These roots describe the switching time
instants of the input signal and the full trajectory simply follows by multiple
integration. This method presents a systematic way to compute time optimal
trajectories exactly via algebraic calculations without numerical approximation
iterations. It is applied to various trajectory types with different continuity
order, asymmetric derivative bounds and non-rest initial and final states.
</p>
<a href="http://arxiv.org/abs/2012.07029" target="_blank">arXiv:2012.07029</a> [<a href="http://arxiv.org/pdf/2012.07029" target="_blank">pdf</a>]

<h2>Efficient Human Pose Estimation by Learning Deeply Aggregated Representations. (arXiv:2012.07033v1 [cs.CV])</h2>
<h3>Zhengxiong Luo, Zhicheng Wang, Yuanhao Cai, Guanan Wang, Yan Huang, Liang Wang, Erjin Zhou, Jian Sun</h3>
<p>In this paper, we propose an efficient human pose estimation network (DANet)
by learning deeply aggregated representations. Most existing models explore
multi-scale information mainly from features with different spatial sizes.
Powerful multi-scale representations usually rely on the cascaded pyramid
framework. This framework largely boosts the performance but in the meanwhile
makes networks very deep and complex. Instead, we focus on exploiting
multi-scale information from layers with different receptive-field sizes and
then making full of use this information by improving the fusion method.
Specifically, we propose an orthogonal attention block (OAB) and a second-order
fusion unit (SFU). The OAB learns multi-scale information from different layers
and enhances them by encouraging them to be diverse. The SFU adaptively selects
and fuses diverse multi-scale information and suppress the redundant ones. This
could maximize the effective information in final fused representations. With
the help of OAB and SFU, our single pyramid network may be able to generate
deeply aggregated representations that contain even richer multi-scale
information and have a larger representing capacity than that of cascaded
networks. Thus, our networks could achieve comparable or even better accuracy
with much smaller model complexity. Specifically, our \mbox{DANet-72} achieves
$70.5$ in AP score on COCO test-dev set with only $1.0G$ FLOPs. Its speed on a
CPU platform achieves $58$ Persons-Per-Second~(PPS).
</p>
<a href="http://arxiv.org/abs/2012.07033" target="_blank">arXiv:2012.07033</a> [<a href="http://arxiv.org/pdf/2012.07033" target="_blank">pdf</a>]

<h2>Fault Injectors for TensorFlow: Evaluation of the Impact of Random Hardware Faults on Deep CNNs. (arXiv:2012.07037v1 [cs.LG])</h2>
<h3>Michael Beyer, Andrey Morozov, Emil Valiev, Christoph Schorn, Lydia Gauerhof, Kai Ding, Klaus Janschek</h3>
<p>Today, Deep Learning (DL) enhances almost every industrial sector, including
safety-critical areas. The next generation of safety standards will define
appropriate verification techniques for DL-based applications and propose
adequate fault tolerance mechanisms. DL-based applications, like any other
software, are susceptible to common random hardware faults such as bit flips,
which occur in RAM and CPU registers. Such faults can lead to silent data
corruption. Therefore, it is crucial to develop methods and tools that help to
evaluate how DL components operate under the presence of such faults. In this
paper, we introduce two new Fault Injection (FI) frameworks InjectTF and
InjectTF2 for TensorFlow 1 and TensorFlow 2, respectively. Both frameworks are
available on GitHub and allow the configurable injection of random faults into
Neural Networks (NN). In order to demonstrate the feasibility of the
frameworks, we also present the results of FI experiments conducted on four
VGG-based Convolutional NNs using two image sets. The results demonstrate how
random bit flips in the output of particular mathematical operations and layers
of NNs affect the classification accuracy. These results help to identify the
most critical operations and layers, compare the reliability characteristics of
functionally similar NNs, and introduce selective fault tolerance mechanisms.
</p>
<a href="http://arxiv.org/abs/2012.07037" target="_blank">arXiv:2012.07037</a> [<a href="http://arxiv.org/pdf/2012.07037" target="_blank">pdf</a>]

<h2>Uncertainty Estimation in Deep Neural Networks for Point Cloud Segmentation in Factory Planning. (arXiv:2012.07038v1 [cs.CV])</h2>
<h3>Christina Petschnigg, Juergen Pilz</h3>
<p>The digital factory provides undoubtedly a great potential for future
production systems in terms of efficiency and effectivity. A key aspect on the
way to realize the digital copy of a real factory is the understanding of
complex indoor environments on the basis of 3D data. In order to generate an
accurate factory model including the major components, i.e. building parts,
product assets and process details, the 3D data collected during digitalization
can be processed with advanced methods of deep learning. In this work, we
propose a fully Bayesian and an approximate Bayesian neural network for point
cloud segmentation. This allows us to analyze how different ways of estimating
uncertainty in these networks improve segmentation results on raw 3D point
clouds. We achieve superior model performance for both, the Bayesian and the
approximate Bayesian model compared to the frequentist one. This performance
difference becomes even more striking when incorporating the networks'
uncertainty in their predictions. For evaluation we use the scientific data set
S3DIS as well as a data set, which was collected by the authors at a German
automotive production plant. The methods proposed in this work lead to more
accurate segmentation results and the incorporation of uncertainty information
makes this approach especially applicable to safety critical applications.
</p>
<a href="http://arxiv.org/abs/2012.07038" target="_blank">arXiv:2012.07038</a> [<a href="http://arxiv.org/pdf/2012.07038" target="_blank">pdf</a>]

<h2>Semi-supervised Segmentation via Uncertainty Rectified Pyramid Consistency and Its Application to Gross Target Volume of Nasopharyngeal Carcinoma. (arXiv:2012.07042v1 [cs.CV])</h2>
<h3>Xiangde Luo, Wenjun Liao, Jieneng Chen, Tao Song, Yinan Chen, Guotai Wang, Shaoting Zhang</h3>
<p>Gross Target Volume (GTV) segmentation plays an irreplaceable role in
radiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that
convolutional neural networks (CNN) have achieved good performance for this
task, they rely on a large set of labeled images for training, which is
expensive and time-consuming to acquire. Recently, semi-supervised methods that
learn from a small set of labeled images with a large set of unlabeled images
have shown potential for dealing with this problem, but it is still challenging
to train a high-performance model with the limited number of labeled data. In
this paper, we propose a novel framework with Uncertainty Rectified Pyramid
Consistency (URPC) regularization for semi-supervised NPC GTV segmentation.
Concretely, we extend a backbone segmentation network to produce pyramid
predictions at different scales, the pyramid predictions network (PPNet) was
supervised by the ground truth of labeled images and a multi-scale consistency
loss for unlabeled images, motivated by the fact that prediction at different
scales for the same input should be similar and consistent. However, due to the
different resolution of these predictions, encouraging them to be consistent at
each pixel directly is not robust and may bring much noise and lead to a
performance drop. To deal with this dilemma, we further design a novel
uncertainty rectifying module to enable the framework to gradually learn from
meaningful and reliable consensual regions at different scales. Extensive
experiments on our collected NPC dataset with 258 volumes show that our method
can largely improve performance by incorporating the unlabeled data, and this
framework achieves a promising result compared with existing semi-supervised
methods, which achieves 81.22% of mean DSC and 1.88 voxels of mean ASD on the
test set, where the only 20% of the training set were annotated.
</p>
<a href="http://arxiv.org/abs/2012.07042" target="_blank">arXiv:2012.07042</a> [<a href="http://arxiv.org/pdf/2012.07042" target="_blank">pdf</a>]

<h2>One-Shot Object Localization in Medical Images based on Relative Position Regression. (arXiv:2012.07043v1 [cs.CV])</h2>
<h3>Wenhui Lei, Wei Xu, Ran Gu, Hao Fu, Shaoting Zhang, Guotai Wang</h3>
<p>Deep learning networks have shown promising performance for accurate object
localization in medial images, but require large amount of annotated data for
supervised training, which is expensive and expertise burdensome. To address
this problem, we present a one-shot framework for organ and landmark
localization in volumetric medical images, which does not need any annotation
during the training stage and could be employed to locate any landmarks or
organs in test images given a support (reference) image during the inference
stage. Our main idea comes from that tissues and organs from different human
bodies have a similar relative position and context. Therefore, we could
predict the relative positions of their non-local patches, thus locate the
target organ. Our framework is composed of three parts: (1) A projection
network trained to predict the 3D offset between any two patches from the same
volume, where human annotations are not required. In the inference stage, it
takes one given landmark in a reference image as a support patch and predicts
the offset from a random patch to the corresponding landmark in the test
(query) volume. (2) A coarse-to-fine framework contains two projection
networks, providing more accurate localization of the target. (3) Based on the
coarse-to-fine model, we transfer the organ boundingbox (B-box) detection to
locating six extreme points along x, y and z directions in the query volume.
Experiments on multi-organ localization from head-and-neck (HaN) CT volumes
showed that our method acquired competitive performance in real time, which is
more accurate and 10^5 times faster than template matching methods with the
same setting. Code is available: https://github.com/LWHYC/RPR-Loc.
</p>
<a href="http://arxiv.org/abs/2012.07043" target="_blank">arXiv:2012.07043</a> [<a href="http://arxiv.org/pdf/2012.07043" target="_blank">pdf</a>]

<h2>Monitoring multimode processes: a modified PCA algorithm with continual learning ability. (arXiv:2012.07044v1 [stat.ML])</h2>
<h3>ingxin Zhang, Donghua Zhou, Maoyin Chen</h3>
<p>For multimode processes, one has to establish local monitoring models
corresponding to local modes. However, the significant features of previous
modes may be catastrophically forgotten when a monitoring model for the current
mode is built. It would result in an abrupt performance decrease. Is it
possible to make local monitoring model remember the features of previous
modes? Choosing the principal component analysis (PCA) as a basic monitoring
model, we try to resolve this problem. A modified PCA algorithm is built with
continual learning ability for monitoring multimode processes, which adopts
elastic weight consolidation (EWC) to overcome catastrophic forgetting of PCA
for successive modes. It is called PCA-EWC, where the significant features of
previous modes are preserved when a PCA model is established for the current
mode. The computational complexity and key parameters are discussed to further
understand the relationship between PCA and the proposed algorithm. Numerical
case study and a practical industrial system in China are employed to
illustrate the effectiveness of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2012.07044" target="_blank">arXiv:2012.07044</a> [<a href="http://arxiv.org/pdf/2012.07044" target="_blank">pdf</a>]

<h2>Vision Based Adaptation to Kernelized Synergies for Human Inspired Robotic Manipulation. (arXiv:2012.07046v1 [cs.RO])</h2>
<h3>Sunny Katyara, Fanny Ficuciello, Fei Chen, Bruno Siciliano, Darwin G. Caldwell</h3>
<p>Humans in contrast to robots are excellent in performing fine manipulation
tasks owing to their remarkable dexterity and sensorimotor organization.
Enabling robots to acquire such capabilities, necessitates a framework that not
only replicates the human behaviour but also integrates the multi-sensory
information for autonomous object interaction. To address such limitations,
this research proposes to augment the previously developed kernelized synergies
framework with visual perception to automatically adapt to the unknown objects.
The kernelized synergies, inspired from humans, retain the same reduced
subspace for object grasping and manipulation. To detect object in the scene, a
simplified perception pipeline is used that leverages the RANSAC algorithm with
Euclidean clustering and SVM for object segmentation and recognition
respectively. Further, the comparative analysis of kernelized synergies with
other state of art approaches is made to confirm their flexibility and
effectiveness on the robotic manipulation tasks. The experiments conducted on
the robot hand confirm the robustness of modified kernelized synergies
framework against the uncertainties related to the perception of environment.
</p>
<a href="http://arxiv.org/abs/2012.07046" target="_blank">arXiv:2012.07046</a> [<a href="http://arxiv.org/pdf/2012.07046" target="_blank">pdf</a>]

<h2>Adaptive Algorithms for Multi-armed Bandit with Composite and Anonymous Feedback. (arXiv:2012.07048v1 [cs.LG])</h2>
<h3>Siwei Wang, Haoyun Wang, Longbo Huang</h3>
<p>We study the multi-armed bandit (MAB) problem with composite and anonymous
feedback. In this model, the reward of pulling an arm spreads over a period of
time (we call this period as reward interval) and the player receives partial
rewards of the action, convoluted with rewards from pulling other arms,
successively. Existing results on this model require prior knowledge about the
reward interval size as an input to their algorithms. In this paper, we propose
adaptive algorithms for both the stochastic and the adversarial cases, without
requiring any prior information about the reward interval. For the stochastic
case, we prove that our algorithm guarantees a regret that matches the lower
bounds (in order). For the adversarial case, we propose the first algorithm to
jointly handle non-oblivious adversary and unknown reward interval size. We
also conduct simulations based on real-world dataset. The results show that our
algorithms outperform existing benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.07048" target="_blank">arXiv:2012.07048</a> [<a href="http://arxiv.org/pdf/2012.07048" target="_blank">pdf</a>]

<h2>PoNA: Pose-guided Non-local Attention for Human Pose Transfer. (arXiv:2012.07049v1 [cs.CV])</h2>
<h3>Kun Li, Jinsong Zhang, Yebin Liu, Yu-Kun Lai, Qionghai Dai</h3>
<p>Human pose transfer, which aims at transferring the appearance of a given
person to a target pose, is very challenging and important in many
applications. Previous work ignores the guidance of pose features or only uses
local attention mechanism, leading to implausible and blurry results. We
propose a new human pose transfer method using a generative adversarial network
(GAN) with simplified cascaded blocks. In each block, we propose a pose-guided
non-local attention (PoNA) mechanism with a long-range dependency scheme to
select more important regions of image features to transfer. We also design
pre-posed image-guided pose feature update and post-posed pose-guided image
feature update to better utilize the pose and image features. Our network is
simple, stable, and easy to train. Quantitative and qualitative results on
Market-1501 and DeepFashion datasets show the efficacy and efficiency of our
model. Compared with state-of-the-art methods, our model generates sharper and
more realistic images with rich details, while having fewer parameters and
faster speed. Furthermore, our generated images can help to alleviate data
insufficiency for person re-identification.
</p>
<a href="http://arxiv.org/abs/2012.07049" target="_blank">arXiv:2012.07049</a> [<a href="http://arxiv.org/pdf/2012.07049" target="_blank">pdf</a>]

<h2>Budgeted and Non-budgeted Causal Bandits. (arXiv:2012.07058v1 [cs.LG])</h2>
<h3>Vineet Nair, Vishakha Patil, Gaurav Sinha</h3>
<p>Learning good interventions in a causal graph can be modelled as a stochastic
multi-armed bandit problem with side-information. First, we study this problem
when interventions are more expensive than observations and a budget is
specified. If there are no backdoor paths from an intervenable node to the
reward node then we propose an algorithm to minimize simple regret that
optimally trades-off observations and interventions based on the cost of
intervention. We also propose an algorithm that accounts for the cost of
interventions, utilizes causal side-information, and minimizes the expected
cumulative regret without exceeding the budget. Our cumulative-regret
minimization algorithm performs better than standard algorithms that do not
take side-information into account. Finally, we study the problem of learning
best interventions without budget constraint in general graphs and give an
algorithm that achieves constant expected cumulative regret in terms of the
instance parameters when the parent distribution of the reward variable for
each intervention is known. Our results are experimentally validated and
compared to the best-known bounds in the current literature.
</p>
<a href="http://arxiv.org/abs/2012.07058" target="_blank">arXiv:2012.07058</a> [<a href="http://arxiv.org/pdf/2012.07058" target="_blank">pdf</a>]

<h2>Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network. (arXiv:2012.07061v1 [cs.CV])</h2>
<h3>Jiayi Ji, Yunpeng Luo, Xiaoshuai Sun, Fuhai Chen, Gen Luo, Yongjian Wu, Yue Gao, Rongrong Ji</h3>
<p>Transformer-based architectures have shown great success in image captioning,
where object regions are encoded and then attended into the vectorial
representations to guide the caption decoding. However, such vectorial
representations only contain region-level information without considering the
global information reflecting the entire image, which fails to expand the
capability of complex multi-modal reasoning in image captioning. In this paper,
we introduce a Global Enhanced Transformer (termed GET) to enable the
extraction of a more comprehensive global representation, and then adaptively
guide the decoder to generate high-quality captions. In GET, a Global Enhanced
Encoder is designed for the embedding of the global feature, and a Global
Adaptive Decoder are designed for the guidance of the caption generation. The
former models intra- and inter-layer global representation by taking advantage
of the proposed Global Enhanced Attention and a layer-wise fusion module. The
latter contains a Global Adaptive Controller that can adaptively fuse the
global information into the decoder to guide the caption generation. Extensive
experiments on MS COCO dataset demonstrate the superiority of our GET over many
state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2012.07061" target="_blank">arXiv:2012.07061</a> [<a href="http://arxiv.org/pdf/2012.07061" target="_blank">pdf</a>]

<h2>Active Learning for Node Classification: The Additional Learning Ability from Unlabelled Nodes. (arXiv:2012.07065v1 [cs.LG])</h2>
<h3>Juncheng Liu, Yiwei Wang, Bryan Hooi, Renchi Yang, Xiaokui Xiao</h3>
<p>Node classification on graph data is an important task on many practical
domains. However, it requires labels for training, which can be difficult or
expensive to obtain in practice. Given a limited labelling budget, active
learning aims to improve performance by carefully choosing which nodes to
label. Our empirical study shows that existing active learning methods for node
classification are considerably outperformed by a simple method which randomly
selects nodes to label and trains a linear classifier with labelled nodes and
unsupervised learning features. This indicates that existing methods do not
fully utilize the information present in unlabelled nodes as they only use
unlabelled nodes for label acquisition. In this paper, we utilize the
information in unlabelled nodes by using unsupervised learning features. We
propose a novel latent space clustering-based active learning method for node
classification (LSCALE). Specifically, to select nodes for labelling, our
method uses the K-Medoids clustering algorithm on a feature space based on the
dynamic combination of both unsupervised features and supervised features. In
addition, we design an incremental clustering module to avoid redundancy
between nodes selected at different steps. We conduct extensive experiments on
three public citation datasets and two co-authorship datasets, where our
proposed method LSCALE consistently and significantly outperforms the
state-of-the-art approaches by a large margin.
</p>
<a href="http://arxiv.org/abs/2012.07065" target="_blank">arXiv:2012.07065</a> [<a href="http://arxiv.org/pdf/2012.07065" target="_blank">pdf</a>]

<h2>Robust Real-Time Pedestrian Detection on Embedded Devices. (arXiv:2012.07072v1 [cs.CV])</h2>
<h3>Mohamed Afifi, Yara Ali, Karim Amer, Mahmoud Shaker, Mohamed Elhelw</h3>
<p>Detection of pedestrians on embedded devices, such as those on-board of
robots and drones, has many applications including road intersection
monitoring, security, crowd monitoring and surveillance, to name a few.
However, the problem can be challenging due to continuously-changing camera
viewpoint and varying object appearances as well as the need for lightweight
algorithms suitable for embedded systems. This paper proposes a robust
framework for pedestrian detection in many footages. The framework performs
fine and coarse detections on different image regions and exploits temporal and
spatial characteristics to attain enhanced accuracy and real time performance
on embedded boards. The framework uses the Yolo-v3 object detection [1] as its
backbone detector and runs on the Nvidia Jetson TX2 embedded board, however
other detectors and/or boards can be used as well. The performance of the
framework is demonstrated on two established datasets and its achievement of
the second place in CVPR 2019 Embedded Real-Time Inference (ERTI) Challenge.
</p>
<a href="http://arxiv.org/abs/2012.07072" target="_blank">arXiv:2012.07072</a> [<a href="http://arxiv.org/pdf/2012.07072" target="_blank">pdf</a>]

<h2>EfficientPose: Efficient Human Pose Estimation with Neural Architecture Search. (arXiv:2012.07086v1 [cs.CV])</h2>
<h3>Wenqiang Zhang, Jiemin Fang, Xinggang Wang, Wenyu Liu</h3>
<p>Human pose estimation from image and video is a vital task in many multimedia
applications. Previous methods achieve great performance but rarely take
efficiency into consideration, which makes it difficult to implement the
networks on resource-constrained devices. Nowadays real-time multimedia
applications call for more efficient models for better interactions. Moreover,
most deep neural networks for pose estimation directly reuse the networks
designed for image classification as the backbone, which are not yet optimized
for the pose estimation task. In this paper, we propose an efficient framework
targeted at human pose estimation including two parts, the efficient backbone
and the efficient head. By implementing the differentiable neural architecture
search method, we customize the backbone network design for pose estimation and
reduce the computation cost with negligible accuracy degradation. For the
efficient head, we slim the transposed convolutions and propose a spatial
information correction module to promote the performance of the final
prediction. In experiments, we evaluate our networks on the MPII and COCO
datasets. Our smallest model has only 0.65 GFLOPs with 88.1% PCKh@0.5 on MPII
and our large model has only 2 GFLOPs while its accuracy is competitive with
the state-of-the-art large model, i.e., HRNet with 9.5 GFLOPs.
</p>
<a href="http://arxiv.org/abs/2012.07086" target="_blank">arXiv:2012.07086</a> [<a href="http://arxiv.org/pdf/2012.07086" target="_blank">pdf</a>]

<h2>Reinforcement Learning with Subspaces using Free Energy Paradigm. (arXiv:2012.07091v1 [cs.LG])</h2>
<h3>Milad Ghorbani, Reshad Hosseini, Seyed Pooya Shariatpanahi, Majid Nili Ahmadabadi</h3>
<p>In large-scale problems, standard reinforcement learning algorithms suffer
from slow learning speed. In this paper, we follow the framework of using
subspaces to tackle this problem. We propose a free-energy minimization
framework for selecting the subspaces and integrate the policy of the
state-space into the subspaces. Our proposed free-energy minimization framework
rests upon Thompson sampling policy and behavioral policy of subspaces and the
state-space. It is therefore applicable to a variety of tasks, discrete or
continuous state space, model-free and model-based tasks. Through a set of
experiments, we show that this general framework highly improves the learning
speed. We also provide a convergence proof.
</p>
<a href="http://arxiv.org/abs/2012.07091" target="_blank">arXiv:2012.07091</a> [<a href="http://arxiv.org/pdf/2012.07091" target="_blank">pdf</a>]

<h2>MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish. (arXiv:2012.07098v1 [cs.CV])</h2>
<h3>Begum Citamak, Ozan Caglayan, Menekse Kuyu, Erkut Erdem, Aykut Erdem, Pranava Madhyastha, Lucia Specia</h3>
<p>Automatic generation of video descriptions in natural language, also called
video captioning, aims to understand the visual content of the video and
produce a natural language sentence depicting the objects and actions in the
scene. This challenging integrated vision and language problem, however, has
been predominantly addressed for English. The lack of data and the linguistic
properties of other languages limit the success of existing approaches for such
languages. In this paper we target Turkish, a morphologically rich and
agglutinative language that has very different properties compared to English.
To do so, we create the first large scale video captioning dataset for this
language by carefully translating the English descriptions of the videos in the
MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In
addition to enabling research in video captioning in Turkish, the parallel
English-Turkish descriptions also enables the study of the role of video
context in (multimodal) machine translation. In our experiments, we build
models for both video captioning and multimodal machine translation and
investigate the effect of different word segmentation approaches and different
neural architectures to better address the properties of Turkish. We hope that
the MSVD-Turkish dataset and the results reported in this work will lead to
better video captioning and multimodal machine translation models for Turkish
and other morphology rich and agglutinative languages.
</p>
<a href="http://arxiv.org/abs/2012.07098" target="_blank">arXiv:2012.07098</a> [<a href="http://arxiv.org/pdf/2012.07098" target="_blank">pdf</a>]

<h2>Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation. (arXiv:2012.07101v1 [cs.CV])</h2>
<h3>Kun Zhang, Rui Wu, Ping Yao, Kai Deng, Ding Li, Renbiao Liu, Chuanguang Yang, Ge Chen, Min Du, Tianyao Zheng</h3>
<p>The target of 2D human pose estimation is to locate the keypoints of body
parts from input 2D images. State-of-the-art methods for pose estimation
usually construct pixel-wise heatmaps from keypoints as labels for learning
convolution neural networks, which are usually initialized randomly or using
classification models on ImageNet as their backbones. We note that 2D pose
estimation task is highly dependent on the contextual relationship between
image patches, thus we introduce a self-supervised method for pretraining 2D
pose estimation networks. Specifically, we propose Heatmap-Style Jigsaw Puzzles
(HSJP) problem as our pretext-task, whose target is to learn the location of
each patch from an image composed of shuffled patches. During our pretraining
process, we only use images of person instances in MS-COCO, rather than
introducing extra and much larger ImageNet dataset. A heatmap-style label for
patch location is designed and our learning process is in a non-contrastive
way. The weights learned by HSJP pretext task are utilised as backbones of 2D
human pose estimator, which are then finetuned on MS-COCO human keypoints
dataset. With two popular and strong 2D human pose estimators, HRNet and
SimpleBaseline, we evaluate mAP score on both MS-COCO validation and test-dev
datasets. Our experiments show that downstream pose estimators with our
self-supervised pretraining obtain much better performance than those trained
from scratch, and are comparable to those using ImageNet classification models
as their initial backbones.
</p>
<a href="http://arxiv.org/abs/2012.07101" target="_blank">arXiv:2012.07101</a> [<a href="http://arxiv.org/pdf/2012.07101" target="_blank">pdf</a>]

<h2>Leaking Sensitive Financial Accounting Data in Plain Sight using Deep Autoencoder Neural Networks. (arXiv:2012.07110v1 [cs.LG])</h2>
<h3>Marco Schreyer, Chistian Schulze, Damian Borth</h3>
<p>Nowadays, organizations collect vast quantities of sensitive information in
`Enterprise Resource Planning' (ERP) systems, such as accounting relevant
transactions, customer master data, or strategic sales price information. The
leakage of such information poses a severe threat for companies as the number
of incidents and the reputational damage to those experiencing them continue to
increase. At the same time, discoveries in deep learning research revealed that
machine learning models could be maliciously misused to create new attack
vectors. Understanding the nature of such attacks becomes increasingly
important for the (internal) audit and fraud examination practice. The creation
of such an awareness holds in particular for the fraudulent data leakage using
deep learning-based steganographic techniques that might remain undetected by
state-of-the-art `Computer Assisted Audit Techniques' (CAATs). In this work, we
introduce a real-world `threat model' designed to leak sensitive accounting
data. In addition, we show that a deep steganographic process, constituted by
three neural networks, can be trained to hide such data in unobtrusive
`day-to-day' images. Finally, we provide qualitative and quantitative
evaluations on two publicly available real-world payment datasets.
</p>
<a href="http://arxiv.org/abs/2012.07110" target="_blank">arXiv:2012.07110</a> [<a href="http://arxiv.org/pdf/2012.07110" target="_blank">pdf</a>]

<h2>Uniform Scattering of Robots on Alternate Nodes of a Grid. (arXiv:2012.07112v1 [cs.RO])</h2>
<h3>Moumita Mondal, Sruti Gan Chaudhuri, Punyasha Chatterjee</h3>
<p>In this paper, we propose a distributed algorithm to uniformly scatter the
robots along a grid, with robots on alternate nodes of this grid distribution.
These homogeneous, autonomous mobile robots place themselves equidistant apart
on the grid, which can be required for guarding or covering a geographical area
by the robots. The robots operate by executing cycles of the states
"look-compute-move". In the look phase, it looks to see the position of the
other robots; in the compute phase, it computes a destination to move to; and
then in the move phase, it moves to that computed destination. They do not
interact by message passing and can recollect neither the past actions nor the
looked data from the previous cycle, i.e., oblivious. The robots are
semi-synchronous, anonymous and have unlimited visibility. Eventually, the
robots uniformly distribute themselves on alternate nodes of a grid, leaving
the adjacent nodes of the grid vacant. The algorithm presented also assures no
collision or deadlock among the robots.
</p>
<a href="http://arxiv.org/abs/2012.07112" target="_blank">arXiv:2012.07112</a> [<a href="http://arxiv.org/pdf/2012.07112" target="_blank">pdf</a>]

<h2>Uniform Circle Formation By Oblivious Swarm Robots. (arXiv:2012.07113v1 [cs.RO])</h2>
<h3>Moumita Mondal, Sruti Gan Chaudhuri, Ayan Dutta, Krishnendu Mukhopadhyaya, Punyasha Chatterjee</h3>
<p>In this paper, we study the circle formation problem by multiple autonomous
and homogeneous disc-shaped robots (also known as fat robots). The goal of the
robots is to place themselves on the periphery of a circle. Circle formation
has many real-world applications, such as boundary surveillance. This paper
addresses one variant of such problem { uniform circle formation, where the
robots have to be equidistant apart. The robots operate by executing cycles of
the states wait-look-compute-move. They are oblivious, indistinguishable,
anonymous, and do not communicate via message passing. First, we solve the
uniform circle formation problem while assuming the robots to be transparent.
Next, we address an even weaker model, where the robots are non-transparent and
have limited visibility. We propose novel distributed algorithms to solve these
variants. Our presented algorithms in this paper are proved to be correct and
guarantee to prevent collision and deadlock among the swarm of robots.
</p>
<a href="http://arxiv.org/abs/2012.07113" target="_blank">arXiv:2012.07113</a> [<a href="http://arxiv.org/pdf/2012.07113" target="_blank">pdf</a>]

<h2>Demysifying Deep Neural Networks Through Interpretation: A Survey. (arXiv:2012.07119v1 [cs.LG])</h2>
<h3>Giang Dao, Minwoo Lee</h3>
<p>Modern deep learning algorithms tend to optimize an objective metric, such as
minimize a cross entropy loss on a training dataset, to be able to learn. The
problem is that the single metric is an incomplete description of the real
world tasks. The single metric cannot explain why the algorithm learn. When an
erroneous happens, the lack of interpretability causes a hardness of
understanding and fixing the error. Recently, there are works done to tackle
the problem of interpretability to provide insights into neural networks
behavior and thought process. The works are important to identify potential
bias and to ensure algorithm fairness as well as expected performance.
</p>
<a href="http://arxiv.org/abs/2012.07119" target="_blank">arXiv:2012.07119</a> [<a href="http://arxiv.org/pdf/2012.07119" target="_blank">pdf</a>]

<h2>Deliberative and Conceptual Inference in Service Robots. (arXiv:2012.07121v1 [cs.RO])</h2>
<h3>Luis A. Pineda, No&#xe9; Hern&#xe1;ndez, Arturo Rodr&#xed;guez, Ricardo Cruz, Gibr&#xe1;n Fuentes</h3>
<p>Service robots need to reason to support people in daily life situations.
Reasoning is an expensive resource that should be used on demand whenever the
expectations of the robot do not match the situation of the world and the
execution of the task is broken down; in such scenarios the robot must perform
the common sense daily life inference cycle consisting on diagnosing what
happened, deciding what to do about it, and inducing and executing a plan,
recurring in such behavior until the service task can be resumed. Here we
examine two strategies to implement this cycle: (1) a pipe-line strategy
involving abduction, decision-making and planning, which we call deliberative
inference and (2) the use of the knowledge and preferences stored in the
robot's knowledge-base, which we call conceptual inference. The former involves
an explicit definition of a problem space that is explored through heuristic
search, and the latter is based on conceptual knowledge including the human
user preferences, and its representation requires a non-monotonic
knowledge-based system. We compare the strengths and limitations of both
approaches. We also describe a service robot conceptual model and architecture
capable of supporting the daily life inference cycle during the execution of a
robotics service task. The model is centered in the declarative specification
and interpretation of robot's communication and task structure. We also show
the implementation of this framework in the fully autonomous robot Golem-III.
The framework is illustrated with two demonstration scenarios.
</p>
<a href="http://arxiv.org/abs/2012.07121" target="_blank">arXiv:2012.07121</a> [<a href="http://arxiv.org/pdf/2012.07121" target="_blank">pdf</a>]

<h2>DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation. (arXiv:2012.07122v1 [cs.CV])</h2>
<h3>Jie Yang, Yong Shi, Zhiquan Qi</h3>
<p>Automatic detecting anomalous regions in images of objects or textures
without priors of the anomalies is challenging, especially when the anomalies
appear in very small areas of the images, making difficult-to-detect visual
variations, such as defects on manufacturing products. This paper proposes an
effective unsupervised anomaly segmentation approach that can detect and
segment out the anomalies in small and confined regions of images. Concretely,
we develop a multi-scale regional feature generator that can generate multiple
spatial context-aware representations from pre-trained deep convolutional
networks for every subregion of an image. The regional representations not only
describe the local characteristics of corresponding regions but also encode
their multiple spatial context information, making them discriminative and very
beneficial for anomaly detection. Leveraging these descriptive regional
features, we then design a deep yet efficient convolutional autoencoder and
detect anomalous regions within images via fast feature reconstruction. Our
method is simple yet effective and efficient. It advances the state-of-the-art
performances on several benchmark datasets and shows great potential for real
applications.
</p>
<a href="http://arxiv.org/abs/2012.07122" target="_blank">arXiv:2012.07122</a> [<a href="http://arxiv.org/pdf/2012.07122" target="_blank">pdf</a>]

<h2>Iterative Knowledge Exchange Between Deep Learning and Space-Time Spectral Clustering for Unsupervised Segmentation in Videos. (arXiv:2012.07123v1 [cs.CV])</h2>
<h3>Emanuela Haller, Adina Magda Florea, Marius Leordeanu</h3>
<p>We propose a dual system for unsupervised object segmentation in video, which
brings together two modules with complementary properties: a space-time graph
that discovers objects in videos and a deep network that learns powerful object
features. The system uses an iterative knowledge exchange policy. A novel
spectral space-time clustering process on the graph produces unsupervised
segmentation masks passed to the network as pseudo-labels. The net learns to
segment in single frames what the graph discovers in video and passes back to
the graph strong image-level features that improve its node-level features in
the next iteration. Knowledge is exchanged for several cycles until
convergence. The graph has one node per each video pixel, but the object
discovery is fast. It uses a novel power iteration algorithm computing the main
space-time cluster as the principal eigenvector of a special Feature-Motion
matrix without actually computing the matrix. The thorough experimental
analysis validates our theoretical claims and proves the effectiveness of the
cyclical knowledge exchange. We also perform experiments on the supervised
scenario, incorporating features pretrained with human supervision. We achieve
state-of-the-art level on unsupervised and supervised scenarios on four
challenging datasets: DAVIS, SegTrack, YouTube-Objects, and DAVSOD.
</p>
<a href="http://arxiv.org/abs/2012.07123" target="_blank">arXiv:2012.07123</a> [<a href="http://arxiv.org/pdf/2012.07123" target="_blank">pdf</a>]

<h2>Location-aware Single Image Reflection Removal. (arXiv:2012.07131v1 [cs.CV])</h2>
<h3>Zheng Dong, Ke Xu, Yin Yang, Hujun Bao, Weiwei Xu, Rynson W.H. Lau</h3>
<p>This paper proposes a novel location-aware deep learning-based single image
reflection removal method. Our network has a reflection detection module to
regress a probabilistic reflection confidence map, taking multi-scale Laplacian
features as inputs. This probabilistic map tells whether a region is
reflection-dominated or transmission-dominated. The novelty is that we use the
reflection confidence map as the cues for the network to learn how to encode
the reflection information adaptively and control the feature flow when
predicting reflection and transmission layers. The integration of location
information into the network significantly improves the quality of reflection
removal results. Besides, a set of learnable Laplacian kernel parameters is
introduced to facilitate the extraction of discriminative Laplacian features
for reflection detection. We design our network as a recurrent network to
progressively refine each iteration's reflection removal results. Extensive
experiments verify the superior performance of the proposed method over
state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.07131" target="_blank">arXiv:2012.07131</a> [<a href="http://arxiv.org/pdf/2012.07131" target="_blank">pdf</a>]

<h2>Learning Contextual Causality from Time-consecutive Images. (arXiv:2012.07138v1 [cs.AI])</h2>
<h3>Hongming Zhang, Yintong Huo, Xinran Zhao, Yangqiu Song, Dan Roth</h3>
<p>Causality knowledge is crucial for many artificial intelligence systems.
Conventional textual-based causality knowledge acquisition methods typically
require laborious and expensive human annotations. As a result, their scale is
often limited. Moreover, as no context is provided during the annotation, the
resulting causality knowledge records (e.g., ConceptNet) typically do not take
the context into consideration. To explore a more scalable way of acquiring
causality knowledge, in this paper, we jump out of the textual domain and
investigate the possibility of learning contextual causality from the visual
signal. Compared with pure text-based approaches, learning causality from the
visual signal has the following advantages: (1) Causality knowledge belongs to
the commonsense knowledge, which is rarely expressed in the text but rich in
videos; (2) Most events in the video are naturally time-ordered, which provides
a rich resource for us to mine causality knowledge from; (3) All the objects in
the video can be used as context to study the contextual property of causal
relations. In detail, we first propose a high-quality dataset Vis-Causal and
then conduct experiments to demonstrate that with good language and visual
representation models as well as enough training signals, it is possible to
automatically discover meaningful causal knowledge from the videos. Further
analysis also shows that the contextual property of causal relations indeed
exists, taking which into consideration might be crucial if we want to use the
causality knowledge in real applications, and the visual signal could serve as
a good resource for learning such contextual causality.
</p>
<a href="http://arxiv.org/abs/2012.07138" target="_blank">arXiv:2012.07138</a> [<a href="http://arxiv.org/pdf/2012.07138" target="_blank">pdf</a>]

<h2>FSOCO: The Formula Student Objects in Context Dataset. (arXiv:2012.07139v1 [cs.CV])</h2>
<h3>David Dodel, Michael Sch&#xf6;tz, Niclas V&#xf6;disch</h3>
<p>This paper presents the FSOCO dataset, a collaborative dataset for
vision-based cone detection systems in Formula Student Driverless competitions.
It contains human annotated ground truth labels for both bounding boxes and
instance-wise segmentation masks. The data buy-in philosophy of FSOCO asks
student teams to contribute to the database first before being granted access
ensuring continuous growth. By providing clear labeling guidelines and tools
for a sophisticated raw image selection, new annotations are guaranteed to meet
the desired quality. The effectiveness of the approach is shown by comparing
prediction results of a network trained on FSOCO and its unregulated
predecessor. The FSOCO dataset can be found at fsoco-dataset.com.
</p>
<a href="http://arxiv.org/abs/2012.07139" target="_blank">arXiv:2012.07139</a> [<a href="http://arxiv.org/pdf/2012.07139" target="_blank">pdf</a>]

<h2>Comparing the costs of abstraction for DL frameworks. (arXiv:2012.07163v1 [cs.LG])</h2>
<h3>Maksim Levental, Elena Orlova</h3>
<p>High level abstractions for implementing, training, and testing Deep Learning
(DL) models abound. Such frameworks function primarily by abstracting away the
implementation details of arbitrary neural architectures, thereby enabling
researchers and engineers to focus on design. In principle, such frameworks
could be "zero-cost abstractions"; in practice, they incur translation and
indirection overheads. We study at which points exactly in the engineering
life-cycle of a DL model the highest costs are paid and whether they can be
mitigated. We train, test, and evaluate a representative DL model using
PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets, comparing
accuracy, execution time and memory efficiency.
</p>
<a href="http://arxiv.org/abs/2012.07163" target="_blank">arXiv:2012.07163</a> [<a href="http://arxiv.org/pdf/2012.07163" target="_blank">pdf</a>]

<h2>Decision-Time Postponing Motion Planning for Combinatorial Uncertain Maneuvering. (arXiv:2012.07170v1 [cs.RO])</h2>
<h3>&#xd6;mer &#x15e;ahin Ta&#x15f;, Felix Hauser, Christoph Stiller</h3>
<p>Motion planning involves decision making among combinatorial maneuver
variants in urban driving. A planner must consider uncertainties and associated
risks of the maneuver variants, and subsequently select a maneuver alternative.
In this paper we present a planning approach that considers the uncertainties
in the prediction and, in case of high uncertainty, postpones the combinatorial
decision making to a later time within the planning horizon. With our proposed
approach, safe but at the same time not overconservative motion is planned.
</p>
<a href="http://arxiv.org/abs/2012.07170" target="_blank">arXiv:2012.07170</a> [<a href="http://arxiv.org/pdf/2012.07170" target="_blank">pdf</a>]

<h2>A Memory-Augmented Neural Network Model of Abstract Rule Learning. (arXiv:2012.07172v1 [cs.AI])</h2>
<h3>Ishan Sinha (1), Taylor W. Webb (2), Jonathan D. Cohen (3) ((1) Department of Computer Science, Princeton University, (2) Department of Psychology, University of California, Los Angeles, (3) Department of Neuroscience, Princeton University)</h3>
<p>Human intelligence is characterized by a remarkable ability to infer abstract
rules from experience and apply these rules to novel domains. As such,
designing neural network algorithms with this capacity is an important step
toward the development of deep learning systems with more human-like
intelligence. However, doing so is a major outstanding challenge, one that some
argue will require neural networks to use explicit symbol-processing
mechanisms. In this work, we focus on neural networks' capacity for arbitrary
role-filler binding, the ability to associate abstract "roles" to
context-specific "fillers," which many have argued is an important mechanism
underlying the ability to learn and apply rules abstractly. Using a simplified
version of Raven's Progressive Matrices, a hallmark test of human intelligence,
we introduce a sequential formulation of a visual problem-solving task that
requires this form of binding. Further, we introduce the Emergent Symbol
Binding Network (ESBN), a recurrent neural network model that learns to use an
external memory as a binding mechanism. This mechanism enables symbol-like
variable representations to emerge through the ESBN's training process without
the need for explicit symbol-processing machinery. We empirically demonstrate
that the ESBN successfully learns the underlying abstract rule structure of our
task and perfectly generalizes this rule structure to novel fillers.
</p>
<a href="http://arxiv.org/abs/2012.07172" target="_blank">arXiv:2012.07172</a> [<a href="http://arxiv.org/pdf/2012.07172" target="_blank">pdf</a>]

<h2>MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v1 [cs.CV])</h2>
<h3>Lang Su, Chuqing Hu, Guofa Li, Dongpu Cao</h3>
<p>Multimodal learning mimics the reasoning process of the human multi-sensory
system, which is used to perceive the surrounding world. While making a
prediction, the human brain tends to relate crucial cues from multiple sources
of information. In this work, we propose a novel multimodal fusion module that
learns to emphasize more contributive features across all modalities.
Specifically, the proposed Multimodal Split Attention Fusion (MSAF) module
splits each modality into channel-wise equal feature blocks and creates a joint
representation that is used to generate soft attention for each channel across
the feature blocks. Further, the MSAF module is designed to be compatible with
features of various spatial dimensions and sequence lengths, suitable for both
CNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal
networks and utilize existing pretrained unimodal model weights. To demonstrate
the effectiveness of our fusion module, we design three multimodal networks
with MSAF for emotion recognition, sentiment analysis, and action recognition
tasks. Our approach achieves competitive results in each task and outperforms
other application-specific networks and multimodal fusion benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.07175" target="_blank">arXiv:2012.07175</a> [<a href="http://arxiv.org/pdf/2012.07175" target="_blank">pdf</a>]

<h2>Pseudo Shots: Few-Shot Learning with Auxiliary Data. (arXiv:2012.07176v1 [cs.LG])</h2>
<h3>Reza Esfandiarpoor, Mohsen Hajabdollahi, Stephen H. Bach</h3>
<p>In many practical few-shot learning problems, even though labeled examples
are scarce, there are abundant auxiliary data sets that potentially contain
useful information. We propose a framework to address the challenges of
efficiently selecting and effectively using auxiliary data in image
classification. Given an auxiliary dataset and a notion of semantic similarity
among classes, we automatically select pseudo shots, which are labeled examples
from other classes related to the target task. We show that naively assuming
that these additional examples come from the same distribution as the target
task examples does not significantly improve accuracy. Instead, we propose a
masking module that adjusts the features of auxiliary data to be more similar
to those of the target classes. We show that this masking module can improve
accuracy by up to 18 accuracy points, particularly when the auxiliary data is
semantically distant from the target task. We also show that incorporating
pseudo shots improves over the current state-of-the-art few-shot image
classification scores by an average of 4.81 percentage points of accuracy on
1-shot tasks and an average of 0.31 percentage points on 5-shot tasks.
</p>
<a href="http://arxiv.org/abs/2012.07176" target="_blank">arXiv:2012.07176</a> [<a href="http://arxiv.org/pdf/2012.07176" target="_blank">pdf</a>]

<h2>Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation. (arXiv:2012.07177v1 [cs.CV])</h2>
<h3>Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, Barret Zoph</h3>
<p>Building instance segmentation models that are data-efficient and can handle
rare object categories is an important challenge in computer vision. Leveraging
data augmentations is a promising direction towards addressing this challenge.
Here, we perform a systematic study of the Copy-Paste augmentation ([13, 12])
for instance segmentation where we randomly paste objects onto an image. Prior
studies on Copy-Paste relied on modeling the surrounding visual context for
pasting the objects. However, we find that the simple mechanism of pasting
objects randomly is good enough and can provide solid gains on top of strong
baselines. Furthermore, we show Copy-Paste is additive with semi-supervised
methods that leverage extra data through pseudo labeling (e.g. self-training).
On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an
improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art.
We further demonstrate that Copy-Paste can lead to significant improvements on
the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge
winning entry by +3.6 mask AP on rare categories.
</p>
<a href="http://arxiv.org/abs/2012.07177" target="_blank">arXiv:2012.07177</a> [<a href="http://arxiv.org/pdf/2012.07177" target="_blank">pdf</a>]

<h2>Explanation from Specification. (arXiv:2012.07179v1 [cs.LG])</h2>
<h3>Harish Naik, Gy&#xf6;rgy Tur&#xe1;n</h3>
<p>Explainable components in XAI algorithms often come from a familiar set of
models, such as linear models or decision trees. We formulate an approach where
the type of explanation produced is guided by a specification. Specifications
are elicited from the user, possibly using interaction with the user and
contributions from other areas. Areas where a specification could be obtained
include forensic, medical, and scientific applications. Providing a menu of
possible types of specifications in an area is an exploratory knowledge
representation and reasoning task for the algorithm designer, aiming at
understanding the possibilities and limitations of efficiently computable modes
of explanations. Two examples are discussed: explanations for Bayesian networks
using the theory of argumentation, and explanations for graph neural networks.
The latter case illustrates the possibility of having a representation
formalism available to the user for specifying the type of explanation
requested, for example, a chemical query language for classifying molecules.
The approach is motivated by a theory of explanation in the philosophy of
science, and it is related to current questions in the philosophy of science on
the role of machine learning.
</p>
<a href="http://arxiv.org/abs/2012.07179" target="_blank">arXiv:2012.07179</a> [<a href="http://arxiv.org/pdf/2012.07179" target="_blank">pdf</a>]

<h2>Meticulous Object Segmentation. (arXiv:2012.07181v1 [cs.CV])</h2>
<h3>Chenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zhe Lin, Alan Yuille</h3>
<p>Compared with common image segmentation tasks targeted at low-resolution
images, higher resolution detailed image segmentation receives much less
attention. In this paper, we propose and study a task named Meticulous Object
Segmentation (MOS), which is focused on segmenting well-defined foreground
objects with elaborate shapes in high resolution images (e.g. 2k - 4k). To this
end, we propose the MeticulousNet which leverages a dedicated decoder to
capture the object boundary details. Specifically, we design a Hierarchical
Point-wise Refining (HierPR) block to better delineate object boundaries, and
reformulate the decoding process as a recursive coarse to fine refinement of
the object mask. To evaluate segmentation quality near object boundaries, we
propose the Meticulosity Quality (MQ) score considering both the mask coverage
and boundary precision. In addition, we collect a MOS benchmark dataset
including 600 high quality images with complex objects. We provide
comprehensive empirical evidence showing that MeticulousNet can reveal
pixel-accurate segmentation boundaries and is superior to state-of-the-art
methods for high resolution object segmentation tasks.
</p>
<a href="http://arxiv.org/abs/2012.07181" target="_blank">arXiv:2012.07181</a> [<a href="http://arxiv.org/pdf/2012.07181" target="_blank">pdf</a>]

<h2>Privacy-preserving Decentralized Federated Learning. (arXiv:2012.07183v1 [cs.LG])</h2>
<h3>Beomyeol Jeon, S.M. Ferdous, Muntasir Raihan Rahman, Anwar Walid</h3>
<p>In this paper, we develop SecureD-FL, a privacy-preserving decentralized
federated learning algorithm, i.e., without the traditional centralized
aggregation server. For the decentralized aggregation, we employ the
Alternating Direction Method of Multiplier (ADMM) and examine its privacy
weakness. To address the privacy risk, we introduce a communication pattern
inspired by the combinatorial block design theory and establish its theoretical
privacy guarantee. We also propose an efficient algorithm to construct such a
communication pattern. We evaluate our method on image classification and
next-word prediction applications over federated benchmark datasets with nine
and fifteen distributed sites hosting training data. While preserving privacy,
SecureD-FL performs comparably to the standard centralized federated learning
method; the degradation in test accuracy is only up to 0.73%.
</p>
<a href="http://arxiv.org/abs/2012.07183" target="_blank">arXiv:2012.07183</a> [<a href="http://arxiv.org/pdf/2012.07183" target="_blank">pdf</a>]

<h2>Knowledge-Routed Visual Question Reasoning: Challenges for Deep Representation Embedding. (arXiv:2012.07192v1 [cs.CV])</h2>
<h3>Qingxing Cao, Bailin Li, Xiaodan Liang, Keze Wang, Liang Lin</h3>
<p>Though beneficial for encouraging the Visual Question Answering (VQA) models
to discover the underlying knowledge by exploiting the input-output correlation
beyond image and text contexts, the existing knowledge VQA datasets are mostly
annotated in a crowdsource way, e.g., collecting questions and external reasons
from different users via the internet. In addition to the challenge of
knowledge reasoning, how to deal with the annotator bias also remains unsolved,
which often leads to superficial over-fitted correlations between questions and
answers. To address this issue, we propose a novel dataset named
Knowledge-Routed Visual Question Reasoning for VQA model evaluation.
Considering that a desirable VQA model should correctly perceive the image
context, understand the question, and incorporate its learned knowledge, our
proposed dataset aims to cutoff the shortcut learning exploited by the current
deep embedding models and push the research boundary of the knowledge-based
visual question reasoning. Specifically, we generate the question-answer pair
based on both the Visual Genome scene graph and an external knowledge base with
controlled programs to disentangle the knowledge from other biases. The
programs can select one or two triplets from the scene graph or knowledge base
to push multi-step reasoning, avoid answer ambiguity, and balanced the answer
distribution. In contrast to the existing VQA datasets, we further imply the
following two major constraints on the programs to incorporate knowledge
reasoning: i) multiple knowledge triplets can be related to the question, but
only one knowledge relates to the image object. This can enforce the VQA model
to correctly perceive the image instead of guessing the knowledge based on the
given question solely; ii) all questions are based on different knowledge, but
the candidate answers are the same for both the training and test sets.
</p>
<a href="http://arxiv.org/abs/2012.07192" target="_blank">arXiv:2012.07192</a> [<a href="http://arxiv.org/pdf/2012.07192" target="_blank">pdf</a>]

<h2>Efficient Querying for Cooperative Probabilistic Commitments. (arXiv:2012.07195v1 [cs.AI])</h2>
<h3>Qi Zhang, Edmund H. Durfee, Satinder Singh</h3>
<p>Multiagent systems can use commitments as the core of a general coordination
infrastructure, supporting both cooperative and non-cooperative interactions.
Agents whose objectives are aligned, and where one agent can help another
achieve greater reward by sacrificing some of its own reward, should choose a
cooperative commitment to maximize their joint reward. We present a solution to
the problem of how cooperative agents can efficiently find an (approximately)
optimal commitment by querying about carefully-selected commitment choices. We
prove structural properties of the agents' values as functions of the
parameters of the commitment specification, and develop a greedy method for
composing a query with provable approximation bounds, which we empirically show
can find nearly optimal commitments in a fraction of the time methods that lack
our insights require.
</p>
<a href="http://arxiv.org/abs/2012.07195" target="_blank">arXiv:2012.07195</a> [<a href="http://arxiv.org/pdf/2012.07195" target="_blank">pdf</a>]

<h2>Bayes DistNet -- A Robust Neural Network for Algorithm Runtime Distribution Predictions. (arXiv:2012.07197v1 [cs.LG])</h2>
<h3>Jake Tuero, Michael Buro</h3>
<p>Randomized algorithms are used in many state-of-the-art solvers for
constraint satisfaction problems (CSP) and Boolean satisfiability (SAT)
problems. For many of these problems, there is no single solver which will
dominate others. Having access to the underlying runtime distributions (RTD) of
these solvers can allow for better use of algorithm selection, algorithm
portfolios, and restart strategies. Previous state-of-the-art methods directly
try to predict a fixed parametric distribution that the input instance follows.
In this paper, we extend RTD prediction models into the Bayesian setting for
the first time. This new model achieves robust predictive performance in the
low observation setting, as well as handling censored observations. This
technique also allows for richer representations which cannot be achieved by
the classical models which restrict their output representations. Our model
outperforms the previous state-of-the-art model in settings in which data is
scarce, and can make use of censored data such as lower bound time estimates,
where that type of data would otherwise be discarded. It can also quantify its
uncertainty in its predictions, allowing for algorithm portfolio models to make
better informed decisions about which algorithm to run on a particular
instance.
</p>
<a href="http://arxiv.org/abs/2012.07197" target="_blank">arXiv:2012.07197</a> [<a href="http://arxiv.org/pdf/2012.07197" target="_blank">pdf</a>]

<h2>On Convergence of Gradient Expected Sarsa($\lambda$). (arXiv:2012.07199v1 [cs.LG])</h2>
<h3>Long Yang, Gang Zheng, Yu Zhang, Qian Zheng, Pengfei Li, Gang Pan</h3>
<p>We study the convergence of $\mathtt{Expected~Sarsa}(\lambda)$ with linear
function approximation. We show that applying the off-line estimate (multi-step
bootstrapping) to $\mathtt{Expected~Sarsa}(\lambda)$ is unstable for off-policy
learning. Furthermore, based on convex-concave saddle-point framework, we
propose a convergent $\mathtt{Gradient~Expected~Sarsa}(\lambda)$
($\mathtt{GES}(\lambda)$) algorithm. The theoretical analysis shows that our
$\mathtt{GES}(\lambda)$ converges to the optimal solution at a linear
convergence rate, which is comparable to extensive existing state-of-the-art
gradient temporal difference learning algorithms. Furthermore, we develop a
Lyapunov function technique to investigate how the step-size influences
finite-time performance of $\mathtt{GES}(\lambda)$, such technique of Lyapunov
function can be potentially generalized to other GTD algorithms. Finally, we
conduct experiments to verify the effectiveness of our $\mathtt{GES}(\lambda)$.
</p>
<a href="http://arxiv.org/abs/2012.07199" target="_blank">arXiv:2012.07199</a> [<a href="http://arxiv.org/pdf/2012.07199" target="_blank">pdf</a>]

<h2>INSPIRE: Intensity and Spatial Information-Based Deformable Image Registration. (arXiv:2012.07208v1 [cs.CV])</h2>
<h3>Johan &#xd6;fverstedt, Joakim Lindblad, Nata&#x161;a Sladoje</h3>
<p>We present INSPIRE, a top-performing general-purpose method for deformable
image registration. INSPIRE extends our existing symmetric registration
framework based on distances combining intensity and spatial information to an
elastic B-splines based transformation model. We also present several
theoretical and algorithmic improvements which provide high computational
efficiency and thereby applicability of the framework in a wide range of real
scenarios. We show that the proposed method delivers both highly accurate as
well as stable and robust registration results. We evaluate the method on a
synthetic dataset created from retinal images, consisting of thin networks of
vessels, where INSPIRE exhibits excellent performance, substantially
outperforming the reference methods. We also evaluate the method on four
benchmark datasets of 3D images of brains, for a total of 2088 pairwise
registrations; a comparison with 15 other state-of-the-art methods reveals that
INSPIRE provides the best overall performance. Code is available at
github.com/MIDA-group/inspire.
</p>
<a href="http://arxiv.org/abs/2012.07208" target="_blank">arXiv:2012.07208</a> [<a href="http://arxiv.org/pdf/2012.07208" target="_blank">pdf</a>]

<h2>Breaking the Expressive Bottlenecks of Graph Neural Networks. (arXiv:2012.07219v1 [cs.LG])</h2>
<h3>Mingqi Yang, Yanming Shen, Heng Qi, Baocai Yin</h3>
<p>Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to
measure the expressiveness of graph neural networks (GNNs), showing that the
neighborhood aggregation GNNs were at most as powerful as 1-WL test in
distinguishing graph structures. There were also improvements proposed in
analogy to $k$-WL test ($k&gt;1$). However, the aggregators in these GNNs are far
from injective as required by the WL test, and suffer from weak distinguishing
strength, making it become expressive bottlenecks. In this paper, we improve
the expressiveness by exploring powerful aggregators. We reformulate
aggregation with the corresponding aggregation coefficient matrix, and then
systematically analyze the requirements of the aggregation coefficient matrix
for building more powerful aggregators and even injective aggregators. It can
also be viewed as the strategy for preserving the rank of hidden features, and
implies that basic aggregators correspond to a special case of low-rank
transformations. We also show the necessity of applying nonlinear units ahead
of aggregation, which is different from most aggregation-based GNNs. Based on
our theoretical analysis, we develop two GNN layers, ExpandingConv and
CombConv. Experimental results show that our models significantly boost
performance, especially for large and densely connected graphs.
</p>
<a href="http://arxiv.org/abs/2012.07219" target="_blank">arXiv:2012.07219</a> [<a href="http://arxiv.org/pdf/2012.07219" target="_blank">pdf</a>]

<h2>Trustworthy Preference Completion in Social Choice. (arXiv:2012.07228v1 [cs.AI])</h2>
<h3>Lei Li, Minghe Xue, Huanhuan Chen, Xindong Wu</h3>
<p>As from time to time it is impractical to ask agents to provide linear orders
over all alternatives, for these partial rankings it is necessary to conduct
preference completion. Specifically, the personalized preference of each agent
over all the alternatives can be estimated with partial rankings from
neighboring agents over subsets of alternatives. However, since the agents'
rankings are nondeterministic, where they may provide rankings with noise, it
is necessary and important to conduct the trustworthy preference completion.
Hence, in this paper firstly, a trust-based anchor-kNN algorithm is proposed to
find $k$-nearest trustworthy neighbors of the agent with trust-oriented
Kendall-Tau distances, which will handle the cases when an agent exhibits
irrational behaviors or provides only noisy rankings. Then, for alternative
pairs, a bijection can be built from the ranking space to the preference space,
and its certainty and conflict can be evaluated based on a well-built
statistical measurement Probability-Certainty Density Function. Therefore, a
certain common voting rule for the first $k$ trustworthy neighboring agents
based on certainty and conflict can be taken to conduct the trustworthy
preference completion. The properties of the proposed certainty and conflict
have been studied empirically, and the proposed approach has been
experimentally validated compared to state-of-arts approaches with several data
sets.
</p>
<a href="http://arxiv.org/abs/2012.07228" target="_blank">arXiv:2012.07228</a> [<a href="http://arxiv.org/pdf/2012.07228" target="_blank">pdf</a>]

<h2>Achieving Adversarial Robustness Requires An Active Teacher. (arXiv:2012.07233v1 [cs.LG])</h2>
<h3>Chao Ma, Lexing Ying</h3>
<p>A new understanding of adversarial examples and adversarial robustness is
proposed by decoupling the data generator and the label generator (which we
call the teacher). In our framework, adversarial robustness is a conditional
concept---the student model is not absolutely robust, but robust with respect
to the teacher. Based on the new understanding, we claim that adversarial
examples exist because the student cannot obtain sufficient information of the
teacher from the training data. Various ways of achieving robustness is
compared. Theoretical and numerical evidence shows that to efficiently attain
robustness, a teacher that actively provides its information to the student may
be necessary.
</p>
<a href="http://arxiv.org/abs/2012.07233" target="_blank">arXiv:2012.07233</a> [<a href="http://arxiv.org/pdf/2012.07233" target="_blank">pdf</a>]

<h2>Multi-Domain Multi-Task Rehearsal for Lifelong Learning. (arXiv:2012.07236v1 [cs.LG])</h2>
<h3>Fan Lyu, Shuai Wang, Wei Feng, Zihan Ye, Fuyuan Hu, Song Wang</h3>
<p>Rehearsal, seeking to remind the model by storing old knowledge in lifelong
learning, is one of the most effective ways to mitigate catastrophic
forgetting, i.e., biased forgetting of previous knowledge when moving to new
tasks. However, the old tasks of the most previous rehearsal-based methods
suffer from the unpredictable domain shift when training the new task. This is
because these methods always ignore two significant factors. First, the Data
Imbalance between the new task and old tasks that makes the domain of old tasks
prone to shift. Second, the Task Isolation among all tasks will make the domain
shift toward unpredictable directions; To address the unpredictable domain
shift, in this paper, we propose Multi-Domain Multi-Task (MDMT) rehearsal to
train the old tasks and new task parallelly and equally to break the isolation
among tasks. Specifically, a two-level angular margin loss is proposed to
encourage the intra-class/task compactness and inter-class/task discrepancy,
which keeps the model from domain chaos. In addition, to further address domain
shift of the old tasks, we propose an optional episodic distillation loss on
the memory to anchor the knowledge for each old task. Experiments on benchmark
datasets validate the proposed approach can effectively mitigate the
unpredictable domain shift.
</p>
<a href="http://arxiv.org/abs/2012.07236" target="_blank">arXiv:2012.07236</a> [<a href="http://arxiv.org/pdf/2012.07236" target="_blank">pdf</a>]

<h2>Deep Optimized Priors for 3D Shape Modeling and Reconstruction. (arXiv:2012.07241v1 [cs.CV])</h2>
<h3>Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia</h3>
<p>Many learning-based approaches have difficulty scaling to unseen data, as the
generality of its learned prior is limited to the scale and variations of the
training samples. This holds particularly true with 3D learning tasks, given
the sparsity of 3D datasets available. We introduce a new learning framework
for 3D modeling and reconstruction that greatly improves the generalization
ability of a deep generator. Our approach strives to connect the good ends of
both learning-based and optimization-based methods. In particular, unlike the
common practice that fixes the pre-trained priors at test time, we propose to
further optimize the learned prior and latent code according to the input
physical measurements after the training. We show that the proposed strategy
effectively breaks the barriers constrained by the pre-trained priors and could
lead to high-quality adaptation to unseen data. We realize our framework using
the implicit surface representation and validate the efficacy of our approach
in a variety of challenging tasks that take highly sparse or collapsed
observations as input. Experimental results show that our approach compares
favorably with the state-of-the-art methods in terms of both generality and
accuracy.
</p>
<a href="http://arxiv.org/abs/2012.07241" target="_blank">arXiv:2012.07241</a> [<a href="http://arxiv.org/pdf/2012.07241" target="_blank">pdf</a>]

<h2>Bayesian Neural Ordinary Differential Equations. (arXiv:2012.07244v1 [cs.LG])</h2>
<h3>Raj Dandekar, Vaibhav Dixit, Mohamed Tarek, Aslan Garcia-Valadez, Chris Rackauckas</h3>
<p>Recently, Neural Ordinary Differential Equations has emerged as a powerful
framework for modeling physical simulations without explicitly defining the
ODEs governing the system, but learning them via machine learning. However, the
question: Can Bayesian learning frameworks be integrated with Neural ODEs to
robustly quantify the uncertainty in the weights of a Neural ODE? remains
unanswered. In an effort to address this question, we demonstrate the
successful integration of Neural ODEs with two methods of Bayesian Inference:
(a) The No-U-Turn MCMC sampler (NUTS) and (b) Stochastic Langevin Gradient
Descent (SGLD). We test the performance of our Bayesian Neural ODE approach on
classical physical systems, as well as on standard machine learning datasets
like MNIST, using GPU acceleration. Finally, considering a simple example, we
demonstrate the probabilistic identification of model specification in
partially-described dynamical systems using universal ordinary differential
equations. Together, this gives a scientific machine learning tool for
probabilistic estimation of epistemic uncertainties.
</p>
<a href="http://arxiv.org/abs/2012.07244" target="_blank">arXiv:2012.07244</a> [<a href="http://arxiv.org/pdf/2012.07244" target="_blank">pdf</a>]

<h2>TDAF: Top-Down Attention Framework for Vision Tasks. (arXiv:2012.07248v1 [cs.CV])</h2>
<h3>Bo Pang, Yizhuo Li, Jiefeng Li, Muchen Li, Hanwen Cao, Cewu Lu</h3>
<p>Human attention mechanisms often work in a top-down manner, yet it is not
well explored in vision research. Here, we propose the Top-Down Attention
Framework (TDAF) to capture top-down attentions, which can be easily adopted in
most existing models. The designed Recursive Dual-Directional Nested Structure
in it forms two sets of orthogonal paths, recursive and structural ones, where
bottom-up spatial features and top-down attention features are extracted
respectively. Such spatial and attention features are nested deeply, therefore,
the proposed framework works in a mixed top-down and bottom-up manner.
Empirical evidence shows that our TDAF can capture effective stratified
attention information and boost performance. ResNet with TDAF achieves 2.0%
improvements on ImageNet. For object detection, the performance is improved by
2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And
for action recognition, the 3D-ResNet adopting TDAF achieves improvements of
1.7% accuracy.
</p>
<a href="http://arxiv.org/abs/2012.07248" target="_blank">arXiv:2012.07248</a> [<a href="http://arxiv.org/pdf/2012.07248" target="_blank">pdf</a>]

<h2>Variational State and Parameter Estimation. (arXiv:2012.07269v1 [stat.ML])</h2>
<h3>Jarrad Courts, Johannes Hendriks, Adrian Wills, Thomas Sch&#xf6;n, Brett Ninness</h3>
<p>This paper considers the problem of computing Bayesian estimates of both
states and model parameters for nonlinear state-space models. Generally, this
problem does not have a tractable solution and approximations must be utilised.
In this work, a variational approach is used to provide an assumed density
which approximates the desired, intractable, distribution. The approach is
deterministic and results in an optimisation problem of a standard form. Due to
the parametrisation of the assumed density selected first- and second-order
derivatives are readily available which allows for efficient solutions. The
proposed method is compared against state-of-the-art Hamiltonian Monte Carlo in
two numerical examples.
</p>
<a href="http://arxiv.org/abs/2012.07269" target="_blank">arXiv:2012.07269</a> [<a href="http://arxiv.org/pdf/2012.07269" target="_blank">pdf</a>]

<h2>Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs. (arXiv:2012.07277v1 [cs.RO])</h2>
<h3>Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Yuke Zhu</h3>
<p>We present a visually grounded hierarchical planning algorithm for
long-horizon manipulation tasks. Our algorithm offers a joint framework of
neuro-symbolic task planning and low-level motion generation conditioned on the
specified goal. At the core of our approach is a two-level scene graph
representation, namely geometric scene graph and symbolic scene graph. This
hierarchical representation serves as a structured, object-centric abstraction
of manipulation scenes. Our model uses graph neural networks to process these
scene graphs for predicting high-level task plans and low-level motions. We
demonstrate that our method scales to long-horizon tasks and generalizes well
to novel task goals. We validate our method in a kitchen storage task in both
physical simulation and the real world. Our experiments show that our method
achieved over 70% success rate and nearly 90% of subgoal completion rate on the
real robot while being four orders of magnitude faster in computation time
compared to standard search-based task-and-motion planner.
</p>
<a href="http://arxiv.org/abs/2012.07277" target="_blank">arXiv:2012.07277</a> [<a href="http://arxiv.org/pdf/2012.07277" target="_blank">pdf</a>]

<h2>Learning how to approve updates to machine learning algorithms in non-stationary settings. (arXiv:2012.07278v1 [stat.ML])</h2>
<h3>Jean Feng</h3>
<p>Machine learning algorithms in healthcare have the potential to continually
learn from real-world data generated during healthcare delivery and adapt to
dataset shifts. As such, the FDA is looking to design policies that can
autonomously approve modifications to machine learning algorithms while
maintaining or improving the safety and effectiveness of the deployed models.
However, selecting a fixed approval strategy, a priori, can be difficult
because its performance depends on the stationarity of the data and the quality
of the proposed modifications. To this end, we investigate a
learning-to-approve approach (L2A) that uses accumulating monitoring data to
learn how to approve modifications. L2A defines a family of strategies that
vary in their "optimism''---where more optimistic policies have faster approval
rates---and searches over this family using an exponentially weighted average
forecaster. To control the cumulative risk of the deployed model, we give L2A
the option to abstain from making a prediction and incur some fixed abstention
cost instead. We derive bounds on the average risk of the model deployed by
L2A, assuming the distributional shifts are smooth. In simulation studies and
empirical analyses, L2A tailors the level of optimism for each problem-setting:
It learns to abstain when performance drops are common and approve beneficial
modifications quickly when the distribution is stable.
</p>
<a href="http://arxiv.org/abs/2012.07278" target="_blank">arXiv:2012.07278</a> [<a href="http://arxiv.org/pdf/2012.07278" target="_blank">pdf</a>]

<h2>Towards Accurate Spatiotemporal COVID-19 Risk Scores using High Resolution Real-World Mobility Data. (arXiv:2012.07283v1 [cs.LG])</h2>
<h3>Sirisha Rambhatla, Sepanta Zeighami, Kameron Shahabi, Cyrus Shahabi, Yan Liu</h3>
<p>As countries look towards re-opening of economic activities amidst the
ongoing COVID-19 pandemic, ensuring public health has been challenging. While
contact tracing only aims to track past activities of infected users, one path
to safe reopening is to develop reliable spatiotemporal risk scores to indicate
the propensity of the disease. Existing works which aim to develop risk scores
either rely on compartmental model-based reproduction numbers (which assume
uniform population mixing) or develop coarse-grain spatial scores based on
reproduction number (R0) and macro-level density-based mobility statistics.
Instead, in this paper, we develop a Hawkes process-based technique to assign
relatively fine-grain spatial and temporal risk scores by leveraging
high-resolution mobility data based on cell-phone originated location signals.
While COVID-19 risk scores also depend on a number of factors specific to an
individual, including demography and existing medical conditions, the primary
mode of disease transmission is via physical proximity and contact. Therefore,
we focus on developing risk scores based on location density and mobility
behaviour. We demonstrate the efficacy of the developed risk scores via
simulation based on real-world mobility data. Our results show that fine-grain
spatiotemporal risk scores based on high-resolution mobility data can provide
useful insights and facilitate safe re-opening.
</p>
<a href="http://arxiv.org/abs/2012.07283" target="_blank">arXiv:2012.07283</a> [<a href="http://arxiv.org/pdf/2012.07283" target="_blank">pdf</a>]

<h2>Information-Theoretic Segmentation by Inpainting Error Maximization. (arXiv:2012.07287v1 [cs.CV])</h2>
<h3>Pedro Savarese, Sunnie S. Y. Kim, Michael Maire, Greg Shakhnarovich, David McAllester</h3>
<p>We study image segmentation from an information-theoretic perspective,
proposing a novel adversarial method that performs unsupervised segmentation by
partitioning images into maximally independent sets. More specifically, we
group image pixels into foreground and background, with the goal of minimizing
predictability of one set from the other. An easily computed loss drives a
greedy search process to maximize inpainting error over these partitions. Our
method does not involve training deep networks, is computationally cheap,
class-agnostic, and even applicable in isolation to a single unlabeled image.
Experiments demonstrate that it achieves a new state-of-the-art in unsupervised
segmentation quality, while being substantially faster and more general than
competing approaches.
</p>
<a href="http://arxiv.org/abs/2012.07287" target="_blank">arXiv:2012.07287</a> [<a href="http://arxiv.org/pdf/2012.07287" target="_blank">pdf</a>]

<h2>Semantic Layout Manipulation with High-Resolution Sparse Attention. (arXiv:2012.07288v1 [cs.CV])</h2>
<h3>Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Jianming Zhang, Ning Xu, Jiebo Luo</h3>
<p>We tackle the problem of semantic image layout manipulation, which aims to
manipulate an input image by editing its semantic label map. A core problem of
this task is how to transfer visual details from the input images to the new
semantic layout while making the resulting image visually realistic. Recent
work on learning cross-domain correspondence has shown promising results for
global layout transfer with dense attention-based warping. However, this method
tends to lose texture details due to the lack of smoothness and resolution in
the correspondence and warped images. To adapt this paradigm for the layout
manipulation task, we propose a high-resolution sparse attention module that
effectively transfers visual details to new layouts at a resolution up to
512x512. To further improve visual quality, we introduce a novel generator
architecture consisting of a semantic encoder and a two-stage decoder for
coarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets
demonstrate that our proposed approach achieves substantial improvements over
the existing inpainting and layout manipulation methods.
</p>
<a href="http://arxiv.org/abs/2012.07288" target="_blank">arXiv:2012.07288</a> [<a href="http://arxiv.org/pdf/2012.07288" target="_blank">pdf</a>]

<h2>Learning Category-level Shape Saliency via Deep Implicit Surface Networks. (arXiv:2012.07290v1 [cs.CV])</h2>
<h3>Chaozheng Wu, Lin Sun, Xun Xu, Kui Jia</h3>
<p>This paper is motivated from a fundamental curiosity on what defines a
category of object shapes. For example, we may have the common knowledge that a
plane has wings, and a chair has legs. Given the large shape variations among
different instances of a same category, we are formally interested in
developing a quantity defined for individual points on a continuous object
surface; the quantity specifies how individual surface points contribute to the
formation of the shape as the category. We term such a quantity as
category-level shape saliency or shape saliency for short. Technically, we
propose to learn saliency maps for shape instances of a same category from a
deep implicit surface network; sensible saliency scores for sampled points in
the implicit surface field are predicted by constraining the capacity of input
latent code. We also enhance the saliency prediction with an additional loss of
contrastive training. We expect such learned surface maps of shape saliency to
have the properties of smoothness, symmetry, and semantic representativeness.
We verify these properties by comparing our method with alternative ways of
saliency computation. Notably, we show that by leveraging the learned shape
saliency, we are able to reconstruct either category-salient or
instance-specific parts of object surfaces; semantic representativeness of the
learned saliency is also reflected in its efficacy to guide the selection of
surface points for better point cloud classification.
</p>
<a href="http://arxiv.org/abs/2012.07290" target="_blank">arXiv:2012.07290</a> [<a href="http://arxiv.org/pdf/2012.07290" target="_blank">pdf</a>]

<h2>Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer. (arXiv:2012.07297v1 [cs.CV])</h2>
<h3>Jian Liang, Dapeng Hu, Yunbo Wang, Ran He, Jiashi Feng</h3>
<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
related but different well-labeled source domain to a new unlabeled target
domain. Most existing UDA methods require access to the source data, and thus
are not applicable when the data are confidential and not shareable due to
privacy concerns. This paper aims to tackle a realistic setting with only a
classification model available trained over, instead of accessing to, the
source data. To effectively utilize the source model for adaptation, we propose
a novel approach called Source HypOthesis Transfer (SHOT), which learns the
feature extraction module for the target domain by fitting the target data
features to the frozen source classification module (representing
classification hypothesis). Specifically, SHOT exploits both information
maximization and self-supervised learning for the feature extraction module
learning to ensure the target features are implicitly aligned with the features
of unseen source data via the same hypothesis. Furthermore, we propose a new
labeling transfer strategy, which separates the target data into two splits
based on the confidence of predictions (labeling information), and then employ
semi-supervised learning to improve the accuracy of less-confident predictions
in the target domain. We denote labeling transfer as SHOT++ if the predictions
are obtained by SHOT. Extensive experiments on both digit classification and
object recognition tasks show that SHOT and SHOT++ achieve results surpassing
or comparable to the state-of-the-arts, demonstrating the effectiveness of our
approaches for various visual domain adaptation problems.
</p>
<a href="http://arxiv.org/abs/2012.07297" target="_blank">arXiv:2012.07297</a> [<a href="http://arxiv.org/pdf/2012.07297" target="_blank">pdf</a>]

<h2>Multi Modal Adaptive Normalization for Audio to Video Generation. (arXiv:2012.07304v1 [cs.CV])</h2>
<h3>Neeraj Kumar, Srishti Goel, Ankur Narang, Brejesh Lall</h3>
<p>Speech-driven facial video generation has been a complex problem due to its
multi-modal aspects namely audio and video domain. The audio comprises lots of
underlying features such as expression, pitch, loudness, prosody(speaking
style) and facial video has lots of variability in terms of head movement, eye
blinks, lip synchronization and movements of various facial action units along
with temporal smoothness. Synthesizing highly expressive facial videos from the
audio input and static image is still a challenging task for generative
adversarial networks. In this paper, we propose a multi-modal adaptive
normalization(MAN) based architecture to synthesize a talking person video of
arbitrary length using as input: an audio signal and a single image of a
person. The architecture uses the multi-modal adaptive normalization, keypoint
heatmap predictor, optical flow predictor and class activation map[58] based
layers to learn movements of expressive facial components and hence generates a
highly expressive talking-head video of the given person. The multi-modal
adaptive normalization uses the various features of audio and video such as Mel
spectrogram, pitch, energy from audio signals and predicted keypoint
heatmap/optical flow and a single image to learn the respective affine
parameters to generate highly expressive video. Experimental evaluation
demonstrates superior performance of the proposed method as compared to
Realistic Speech-Driven Facial Animation with GANs(RSDGAN) [53], Speech2Vid
[10], and other approaches, on multiple quantitative metrics including: SSIM
(structural similarity index), PSNR (peak signal to noise ratio), CPBD (image
sharpness), WER(word error rate), blinks/sec and LMD(landmark distance).
Further, qualitative evaluation and Online Turing tests demonstrate the
efficacy of our approach.
</p>
<a href="http://arxiv.org/abs/2012.07304" target="_blank">arXiv:2012.07304</a> [<a href="http://arxiv.org/pdf/2012.07304" target="_blank">pdf</a>]

<h2>Morphology on categorical distributions. (arXiv:2012.07315v1 [cs.CV])</h2>
<h3>Silas Nyboe &#xd8;rting, Hans Jacob Teglbj&#xe6;rg Stephensen, Jon Sporring</h3>
<p>The categorical distribution is a natural representation of uncertainty in
multi-class segmentations. In the two-class case the categorical distribution
reduces to the Bernoulli distribution, for which grayscale morphology provides
a range of useful operations. In the general case, applying morphological
operations on uncertain multi-class segmentations is not straightforward as an
image of categorical distributions is not a complete lattice. Although
morphology on color images has received wide attention, this is not so for
color-coded or categorical images and even less so for images of categorical
distributions. In this work, we establish a set of requirements for morphology
on categorical distributions by combining classic morphology with a
probabilistic view. We then define operators respecting these requirements,
introduce protected operations on categorical distributions and illustrate the
utility of these operators on two example tasks: modeling annotator bias in
brain tumor segmentations and segmenting vesicle instances from the predictions
of a multi-class U-Net.
</p>
<a href="http://arxiv.org/abs/2012.07315" target="_blank">arXiv:2012.07315</a> [<a href="http://arxiv.org/pdf/2012.07315" target="_blank">pdf</a>]

<h2>Optimizing Discrete Spaces via Expensive Evaluations: A Learning to Search Framework. (arXiv:2012.07320v1 [cs.LG])</h2>
<h3>Aryan Deshwal, Syrine Belakaria, Janardhan Rao Doppa, Alan Fern</h3>
<p>We consider the problem of optimizing expensive black-box functions over
discrete spaces (e.g., sets, sequences, graphs). The key challenge is to select
a sequence of combinatorial structures to evaluate, in order to identify
high-performing structures as quickly as possible. Our main contribution is to
introduce and evaluate a new learning-to-search framework for this problem
called L2S-DISCO. The key insight is to employ search procedures guided by
control knowledge at each step to select the next structure and to improve the
control knowledge as new function evaluations are observed. We provide a
concrete instantiation of L2S-DISCO for local search procedure and empirically
evaluate it on diverse real-world benchmarks. Results show the efficacy of
L2S-DISCO over state-of-the-art algorithms in solving complex optimization
problems.
</p>
<a href="http://arxiv.org/abs/2012.07320" target="_blank">arXiv:2012.07320</a> [<a href="http://arxiv.org/pdf/2012.07320" target="_blank">pdf</a>]

<h2>Active Hierarchical Imitation and Reinforcement Learning. (arXiv:2012.07330v1 [cs.RO])</h2>
<h3>Yaru Niu, Yijun Gu</h3>
<p>Humans can leverage hierarchical structures to split a task into sub-tasks
and solve problems efficiently. Both imitation and reinforcement learning or a
combination of them with hierarchical structures have been proven to be an
efficient way for robots to learn complex tasks with sparse rewards. However,
in the previous work of hierarchical imitation and reinforcement learning, the
tested environments are in relatively simple 2D games, and the action spaces
are discrete. Furthermore, many imitation learning works focusing on improving
the policies learned from the expert polices that are hard-coded or trained by
reinforcement learning algorithms, rather than human experts. In the scenarios
of human-robot interaction, humans can be required to provide demonstrations to
teach the robot, so it is crucial to improve the learning efficiency to reduce
expert efforts, and know human's perception about the learning/training
process. In this project, we explored different imitation learning algorithms
and designed active learning algorithms upon the hierarchical imitation and
reinforcement learning framework we have developed. We performed an experiment
where five participants were asked to guide a randomly initialized agent to a
random goal in a maze. Our experimental results showed that using DAgger and
reward-based active learning method can achieve better performance while saving
more human efforts physically and mentally during the training process.
</p>
<a href="http://arxiv.org/abs/2012.07330" target="_blank">arXiv:2012.07330</a> [<a href="http://arxiv.org/pdf/2012.07330" target="_blank">pdf</a>]

<h2>Kernel Mean Embedding of Distributions: A Review and Beyond. (arXiv:1605.09522v4 [stat.ML] UPDATED)</h2>
<h3>Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Sch&#xf6;lkopf</h3>
<p>A Hilbert space embedding of a distribution---in short, a kernel mean
embedding---has recently emerged as a powerful tool for machine learning and
inference. The basic idea behind this framework is to map distributions into a
reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel
methods can be extended to probability measures. It can be viewed as a
generalization of the original "feature map" common to support vector machines
(SVMs) and other kernel methods. While initially closely associated with the
latter, it has meanwhile found application in fields ranging from kernel
machines and probabilistic modeling to statistical inference, causal discovery,
and deep learning. The goal of this survey is to give a comprehensive review of
existing work and recent advances in this research area, and to discuss the
most challenging issues and open problems that could lead to new research
directions. The survey begins with a brief introduction to the RKHS and
positive definite kernels which forms the backbone of this survey, followed by
a thorough discussion of the Hilbert space embedding of marginal distributions,
theoretical guarantees, and a review of its applications. The embedding of
distributions enables us to apply RKHS methods to probability measures which
prompts a wide range of applications such as kernel two-sample testing,
independent testing, and learning on distributional data. Next, we discuss the
Hilbert space embedding for conditional distributions, give theoretical
insights, and review some applications. The conditional mean embedding enables
us to perform sum, product, and Bayes' rules---which are ubiquitous in
graphical model, probabilistic inference, and reinforcement learning---in a
non-parametric way. We then discuss relationships between this framework and
other related areas. Lastly, we give some suggestions on future research
directions.
</p>
<a href="http://arxiv.org/abs/1605.09522" target="_blank">arXiv:1605.09522</a> [<a href="http://arxiv.org/pdf/1605.09522" target="_blank">pdf</a>]

<h2>Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients. (arXiv:1705.07774v4 [cs.LG] UPDATED)</h2>
<h3>Lukas Balles, Philipp Hennig</h3>
<p>The ADAM optimizer is exceedingly popular in the deep learning community.
Often it works very well, sometimes it doesn't. Why? We interpret ADAM as a
combination of two aspects: for each weight, the update direction is determined
by the sign of stochastic gradients, whereas the update magnitude is determined
by an estimate of their relative variance. We disentangle these two aspects and
analyze them in isolation, gaining insight into the mechanisms underlying ADAM.
This analysis also extends recent results on adverse effects of ADAM on
generalization, isolating the sign aspect as the problematic one. Transferring
the variance adaptation to SGD gives rise to a novel method, completing the
practitioner's toolbox for problems where ADAM fails.
</p>
<a href="http://arxiv.org/abs/1705.07774" target="_blank">arXiv:1705.07774</a> [<a href="http://arxiv.org/pdf/1705.07774" target="_blank">pdf</a>]

<h2>On the achievability of blind source separation for high-dimensional nonlinear source mixtures. (arXiv:1808.00668v3 [stat.ML] UPDATED)</h2>
<h3>Takuya Isomura, Taro Toyoizumi</h3>
<p>For many years, a combination of principal component analysis (PCA) and
independent component analysis (ICA) has been used for blind source separation
(BSS). However, it remains unclear why these linear methods work well with
real-world data that involve nonlinear source mixtures. This work theoretically
validates that a cascade of linear PCA and ICA can solve a nonlinear BSS
problem accurately -- when the sensory inputs are generated from hidden sources
via nonlinear mappings with sufficient dimensionality. Our proposed theorem,
termed the asymptotic linearization theorem, theoretically guarantees that
applying linear PCA to the inputs can reliably extract a subspace spanned by
the linear projections from every hidden source as the major components -- and
thus projecting the inputs onto their major eigenspace can effectively recover
a linear transformation of the hidden sources. Then, subsequent application of
linear ICA can separate all the true independent hidden sources accurately.
Zero-element-wise-error nonlinear BSS is asymptotically attained when the
source dimensionality is large and the input dimensionality is sufficiently
larger than the source dimensionality. Our proposed theorem is validated
analytically and numerically. Moreover, the same computation can be performed
by using Hebbian-like plasticity rules, implying the biological plausibility of
this nonlinear BSS strategy. Our results highlight the utility of linear PCA
and ICA for accurately and reliably recovering nonlinearly mixed sources -- and
further suggest the importance of employing sensors with sufficient
dimensionality to identify true hidden sources of real-world data.
</p>
<a href="http://arxiv.org/abs/1808.00668" target="_blank">arXiv:1808.00668</a> [<a href="http://arxiv.org/pdf/1808.00668" target="_blank">pdf</a>]

<h2>Exploiting Local Indexing and Deep Feature Confidence Scores for Fast Image-to-Video Search. (arXiv:1808.01101v2 [cs.CV] UPDATED)</h2>
<h3>Savas Ozkan, Gozde Bozdagi Akar</h3>
<p>The cost-effective visual representation and fast query-by-example search are
two challenging goals that should be maintained for web-scale visual retrieval
tasks on moderate hardware. This paper introduces a fast and robust method that
ensures both of these goals by obtaining state-of-the-art performance for an
image-to-video search scenario. Hence, we present critical enhancements to
well-known indexing and visual representation techniques by promoting faster,
better and moderate retrieval performance. We also boost the superiority of our
method for some visual challenges by exploiting individual decisions of local
and global descriptors at query time. For instance, local content descriptors
represent copied/duplicated scenes with large geometric deformations such as
scale, orientation and affine transformation. In contrast, the use of global
content descriptors is more practical for near-duplicate and semantic searches.
Experiments are conducted on a large-scale Stanford I2V dataset. The
experimental results show that our method is useful in terms of complexity and
query processing time for large-scale visual retrieval scenarios, even if local
and global representations are used together. The proposed method is superior
and achieves state-of-the-art performance based on the mean average precision
(MAP) score of this dataset. Lastly, we report additional MAP scores after
updating the ground annotations unveiled by retrieval results of the proposed
method, and it shows that the actual performance.
</p>
<a href="http://arxiv.org/abs/1808.01101" target="_blank">arXiv:1808.01101</a> [<a href="http://arxiv.org/pdf/1808.01101" target="_blank">pdf</a>]

<h2>Learning Invariances for Policy Generalization. (arXiv:1809.02591v2 [cs.LG] UPDATED)</h2>
<h3>Remi Tachet, Philip Bachman, Harm van Seijen</h3>
<p>While recent progress has spawned very powerful machine learning systems,
those agents remain extremely specialized and fail to transfer the knowledge
they gain to similar yet unseen tasks. In this paper, we study a simple
reinforcement learning problem and focus on learning policies that encode the
proper invariances for generalization to different settings. We evaluate three
potential methods for policy generalization: data augmentation, meta-learning
and adversarial training. We find our data augmentation method to be effective,
and study the potential of meta-learning and adversarial learning as
alternative task-agnostic approaches.
</p>
<a href="http://arxiv.org/abs/1809.02591" target="_blank">arXiv:1809.02591</a> [<a href="http://arxiv.org/pdf/1809.02591" target="_blank">pdf</a>]

<h2>On the Learning Dynamics of Deep Neural Networks. (arXiv:1809.06848v3 [cs.LG] UPDATED)</h2>
<h3>Remi Tachet, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, Yoshua Bengio</h3>
<p>While a lot of progress has been made in recent years, the dynamics of
learning in deep nonlinear neural networks remain to this day largely
misunderstood. In this work, we study the case of binary classification and
prove various properties of learning in such networks under strong assumptions
such as linear separability of the data. Extending existing results from the
linear case, we confirm empirical observations by proving that the
classification error also follows a sigmoidal shape in nonlinear architectures.
We show that given proper initialization, learning expounds parallel
independent modes and that certain regions of parameter space might lead to
failed training. We also demonstrate that input norm and features' frequency in
the dataset lead to distinct convergence speeds which might shed some light on
the generalization capabilities of deep neural networks. We provide a
comparison between the dynamics of learning with cross-entropy and hinge
losses, which could prove useful to understand recent progress in the training
of generative adversarial networks. Finally, we identify a phenomenon that we
baptize gradient starvation where the most frequent features in a dataset
prevent the learning of other less frequent but equally informative features.
</p>
<a href="http://arxiv.org/abs/1809.06848" target="_blank">arXiv:1809.06848</a> [<a href="http://arxiv.org/pdf/1809.06848" target="_blank">pdf</a>]

<h2>Generating Realistic Training Images Based on Tonality-Alignment Generative Adversarial Networks for Hand Pose Estimation. (arXiv:1811.09916v4 [cs.CV] UPDATED)</h2>
<h3>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Hui Tang, Yufan Xue, Xiaohui Xie, Yen-Yu Lin, Wei Fan</h3>
<p>Hand pose estimation from a monocular RGB image is an important but
challenging task. The main factor affecting its performance is the lack of a
sufficiently large training dataset with accurate hand-keypoint annotations. In
this work, we circumvent this problem by proposing an effective method for
generating realistic hand poses and show that state-of-the-art algorithms for
hand pose estimation can be greatly improved by utilizing the generated hand
poses as training data. Specifically, we first adopt an augmented reality (AR)
simulator to synthesize hand poses with accurate hand-keypoint labels. Although
the synthetic hand poses come with precise joint labels, eliminating the need
of manual annotations, they look unnatural and are not the ideal training data.
To produce more realistic hand poses, we propose to blend a synthetic hand pose
with a real background, such as arms and sleeves. To this end, we develop
tonality-alignment generative adversarial networks (TAGANs), which align the
tonality and color distributions between synthetic hand poses and real
backgrounds, and can generate high quality hand poses. We evaluate TAGAN on
three benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With
the aid of the synthesized poses, our method performs favorably against the
state-of-the-arts in both 2D and 3D hand pose estimations.
</p>
<a href="http://arxiv.org/abs/1811.09916" target="_blank">arXiv:1811.09916</a> [<a href="http://arxiv.org/pdf/1811.09916" target="_blank">pdf</a>]

<h2>Finite-Sample Analysis For Decentralized Batch Multi-Agent Reinforcement Learning With Networked Agents. (arXiv:1812.02783v8 [cs.LG] UPDATED)</h2>
<h3>Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, Tamer Ba&#x15f;ar</h3>
<p>Despite the increasing interest in multi-agent reinforcement learning (MARL)
in multiple communities, understanding its theoretical foundation has long been
recognized as a challenging problem. In this work, we address this problem by
providing a finite-sample analysis for decentralized batch MARL with networked
agents. Specifically, we consider two decentralized MARL settings, where teams
of agents are connected by time-varying communication networks, and either
collaborate or compete in a zero-sum game setting, without any central
controller. These settings cover many conventional MARL settings in the
literature. For both settings, we develop batch MARL algorithms that can be
implemented in a decentralized fashion, and quantify the finite-sample errors
of the estimated action-value functions. Our error analysis captures how the
function class, the number of samples within each iteration, and the number of
iterations determine the statistical accuracy of the proposed algorithms. Our
results, compared to the finite-sample bounds for single-agent RL, involve
additional error terms caused by decentralized computation, which is inherent
in our decentralized MARL setting. This work appears to be the first
finite-sample analysis for batch MARL, a step towards rigorous theoretical
understanding of general MARL algorithms in the finite-sample regime.
</p>
<a href="http://arxiv.org/abs/1812.02783" target="_blank">arXiv:1812.02783</a> [<a href="http://arxiv.org/pdf/1812.02783" target="_blank">pdf</a>]

<h2>Compressed Domain Image Classification Using a Dynamic-Rate Neural Network. (arXiv:1901.09983v2 [cs.CV] UPDATED)</h2>
<h3>Yibo Xu, Weidi Liu, Kevin F. Kelly (Department of Electrical &amp; Computer Engineering, Rice University, Houston, USA)</h3>
<p>Compressed domain image classification performs classification directly on
compressive measurements acquired from the single-pixel camera, bypassing the
image reconstruction step. It is of great importance for extending high-speed
object detection and classification beyond the visible spectrum in a
cost-effective manner especially for resource-limited platforms. Previous
neural network methods require training a dedicated neural network for each
different measurement rate (MR), which is costly in computation and storage. In
this work, we develop an efficient training scheme that provides a neural
network with dynamic-rate property, where a single neural network is capable of
classifying over any MR within the range of interest with a given sensing
matrix. This training scheme uses only a few selected MRs for training and the
trained neural network is valid over the full range of MRs of interest. We
demonstrate the performance of the dynamic-rate neural network on datasets of
MNIST, CIFAR-10, Fashion-MNIST, COIL-100, and show that it generates
approximately equal performance at each MR as that of a single-rate neural
network valid only for one MR. Robustness to noise of the dynamic-rate model is
also demonstrated. The dynamic-rate training scheme can be regarded as a
general approach compatible with different types of sensing matrices, various
neural network architectures, and is a valuable step towards wider adoption of
compressive inference techniques and other compressive sensing related tasks
via neural networks.
</p>
<a href="http://arxiv.org/abs/1901.09983" target="_blank">arXiv:1901.09983</a> [<a href="http://arxiv.org/pdf/1901.09983" target="_blank">pdf</a>]

<h2>On Robustness of Principal Component Regression. (arXiv:1902.10920v9 [cs.LG] UPDATED)</h2>
<h3>Anish Agarwal, Devavrat Shah, Dennis Shen, Dogyoon Song</h3>
<p>Principal Component Regression (PCR) is a simple, but powerful and
ubiquitously utilized method. Its effectiveness is well established when the
covariates exhibit low-rank structure. However, its ability to handle settings
with noisy, missing, and mixed-valued covariates is not understood and remains
an important open challenge. As the main contribution of this work we establish
the robustness of PCR in this respect and provide meaningful finite-sample
analysis. In the process, we establish that PCR is equivalent to performing
Linear Regression after pre-processing the covariate matrix via Hard Singular
Value Thresholding (HSVT). That is, PCR is equivalent to the recently proposed
robust variant of the Synthetic Control method in the context of counterfactual
analysis using observational data. As an immediate consequence, we obtain
finite-sample analysis of the Robust Synthetic Control (RSC) estimator that was
previously absent. As an important contribution to the Synthetic Control
literature, we establish that an (approximate) linear synthetic control exists
in the setting of a generalized factor model; traditionally, the existence of a
synthetic control needs to be assumed to exist as an axiom. We further discuss
a surprising implication of the robustness property of PCR with respect to
noise, i.e., PCR can learn a good predictive model even if the covariates are
tactfully transformed to preserve differential privacy. Finally, this work
advances the state-of-the-art analysis for HSVT by establishing stronger
guarantees with respect to the $\ell_{2, \infty}$-norm rather than the
Frobenius norm as is commonly done in the matrix estimation literature, which
may be of interest in its own right.
</p>
<a href="http://arxiv.org/abs/1902.10920" target="_blank">arXiv:1902.10920</a> [<a href="http://arxiv.org/pdf/1902.10920" target="_blank">pdf</a>]

<h2>Reference-Based Sequence Classification. (arXiv:1905.07188v2 [cs.LG] UPDATED)</h2>
<h3>Zengyou He, Guangyao Xu, Chaohua Sheng, Bo Xu, Quan Zou</h3>
<p>Sequence classification is an important data mining task in many real world
applications. Over the past few decades, many sequence classification methods
have been proposed from different aspects. In particular, the pattern-based
method is one of the most important and widely studied sequence classification
methods in the literature. In this paper, we present a reference-based sequence
classification framework, which can unify existing pattern-based sequence
classification methods under the same umbrella. More importantly, this
framework can be used as a general platform for developing new sequence
classification algorithms. By utilizing this framework as a tool, we propose
new sequence classification algorithms that are quite different from existing
solutions. Experimental results show that new methods developed under the
proposed framework are capable of achieving comparable classification accuracy
to those state-of-the-art sequence classification algorithms.
</p>
<a href="http://arxiv.org/abs/1905.07188" target="_blank">arXiv:1905.07188</a> [<a href="http://arxiv.org/pdf/1905.07188" target="_blank">pdf</a>]

<h2>Universality Theorems for Generative Models. (arXiv:1905.11520v2 [cs.LG] UPDATED)</h2>
<h3>Valentin Khrulkov, Ivan Oseledets</h3>
<p>Despite the fact that generative models are extremely successful in practice,
the theory underlying this phenomenon is only starting to catch up with
practice. In this work we address the question of the universality of
generative models: is it true that neural networks can approximate any data
manifold arbitrarily well? We provide a positive answer to this question and
show that under mild assumptions on the activation function one can always find
a feedforward neural network that maps the latent space onto a set located
within the specified Hausdorff distance from the desired data manifold. We also
prove similar theorems for the case of multiclass generative models and cycle
generative models, trained to map samples from one manifold to another and vice
versa.
</p>
<a href="http://arxiv.org/abs/1905.11520" target="_blank">arXiv:1905.11520</a> [<a href="http://arxiv.org/pdf/1905.11520" target="_blank">pdf</a>]

<h2>Spectral Clustering with Graph Neural Networks for Graph Pooling. (arXiv:1907.00481v5 [cs.LG] UPDATED)</h2>
<h3>Filippo Maria Bianchi, Daniele Grattarola, Cesare Alippi</h3>
<p>Spectral clustering (SC) is a popular clustering technique to find strongly
connected communities on a graph. SC can be used in Graph Neural Networks
(GNNs) to implement pooling operations that aggregate nodes belonging to the
same cluster. However, the eigendecomposition of the Laplacian is expensive
and, since clustering results are graph-specific, pooling methods based on SC
must perform a new optimization for each new sample. In this paper, we propose
a graph clustering approach that addresses these limitations of SC. We
formulate a continuous relaxation of the normalized minCUT problem and train a
GNN to compute cluster assignments that minimize this objective. Our GNN-based
implementation is differentiable, does not require to compute the spectral
decomposition, and learns a clustering function that can be quickly evaluated
on out-of-sample graphs. From the proposed clustering method, we design a graph
pooling operator that overcomes some important limitations of state-of-the-art
graph pooling techniques and achieves the best performance in several
supervised and unsupervised tasks.
</p>
<a href="http://arxiv.org/abs/1907.00481" target="_blank">arXiv:1907.00481</a> [<a href="http://arxiv.org/pdf/1907.00481" target="_blank">pdf</a>]

<h2>IoU-balanced Loss Functions for Single-stage Object Detection. (arXiv:1908.05641v2 [cs.CV] UPDATED)</h2>
<h3>Shengkai Wu, Jinrong Yang, Xinggang Wang, Xiaoping Li</h3>
<p>Single-stage object detectors have been widely applied in computer vision
applications due to their high efficiency. However, we find that the loss
functions adopted by single-stage object detectors hurt the localization
accuracy seriously. Firstly, the standard cross-entropy loss for classification
is independent of the localization task and drives all the positive examples to
learn as high classification scores as possible regardless of localization
accuracy during training. As a result, there will be many detections that have
high classification scores but low IoU or detections that have low
classification scores but high IoU. Secondly, for the standard smooth L1 loss,
the gradient is dominated by the outliers that have poor localization accuracy
during training. The above two problems will decrease the localization accuracy
of single-stage detectors. In this work, IoU-balanced loss functions that
consist of IoU-balanced classification loss and IoU-balanced localization loss
are proposed to solve the above problems. The IoU-balanced classification loss
pays more attention to positive examples with high IoU and can enhance the
correlation between classification and localization tasks. The IoU-balanced
localization loss decreases the gradient of examples with low IoU and increases
the gradient of examples with high IoU, which can improve the localization
accuracy of models. Extensive experiments on challenging public datasets such
as MS COCO, PASCAL VOC and Cityscapes demonstrate that both IoU-balanced losses
can bring substantial improvement for the popular single-stage detectors,
especially for the localization accuracy. On COCO test-dev, the proposed
methods can substantially improve AP by $1.0\%\sim1.7\%$ and AP75 by
$1.0\%\sim2.4\%$. On PASCAL VOC, it can also substantially improve AP by
$1.3\%\sim1.5\%$ and AP80, AP90 by $1.6\%\sim3.9\%$.
</p>
<a href="http://arxiv.org/abs/1908.05641" target="_blank">arXiv:1908.05641</a> [<a href="http://arxiv.org/pdf/1908.05641" target="_blank">pdf</a>]

<h2>Exploiting Global Camera Network Constraints for Unsupervised Video Person Re-identification. (arXiv:1908.10486v3 [cs.CV] UPDATED)</h2>
<h3>Xueping Wang, Rameswar Panda, Min Liu, Yaonan Wang, Amit K Roy-Chowdhury</h3>
<p>Many unsupervised approaches have been proposed recently for the video-based
re-identification problem since annotations of samples across cameras are
time-consuming. However, higher-order relationships across the entire camera
network are ignored by these methods, leading to contradictory outputs when
matching results from different camera pairs are combined. In this paper, we
address the problem of unsupervised video-based re-identification by proposing
a consistent cross-view matching (CCM) framework, in which global camera
network constraints are exploited to guarantee the matched pairs are with
consistency. Specifically, we first propose to utilize the first neighbor of
each sample to discover relations among samples and find the groups in each
camera. Additionally, a cross-view matching strategy followed by global camera
network constraints is proposed to explore the matching relationships across
the entire camera network. Finally, we learn metric models for camera pairs
progressively by alternatively mining consistent cross-view matching pairs and
updating metric models using these obtained matches. Rigorous experiments on
two widely-used benchmarks for video re-identification demonstrate the
superiority of the proposed method over current state-of-the-art unsupervised
methods; for example, on the MARS dataset, our method achieves an improvement
of 4.2\% over unsupervised methods, and even 2.5\% over one-shot
supervision-based methods for rank-1 accuracy.
</p>
<a href="http://arxiv.org/abs/1908.10486" target="_blank">arXiv:1908.10486</a> [<a href="http://arxiv.org/pdf/1908.10486" target="_blank">pdf</a>]

<h2>AutoGMM: Automatic and Hierarchical Gaussian Mixture Modeling in Python. (arXiv:1909.02688v4 [cs.LG] UPDATED)</h2>
<h3>Thomas L. Athey, Benjamin D. Pedigo, Tingshan Liu, Joshua T. Vogelstein</h3>
<p>Gaussian mixture modeling is a fundamental tool in clustering, as well as
discriminant analysis and semiparametric density estimation. However,
estimating the optimal model for any given number of components is an NP-hard
problem, and estimating the number of components is in some respects an even
harder problem. In R, a popular package called mclust addresses both of these
problems. However, Python has lacked such a package. We therefore introduce
AutoGMM, a Python algorithm for automatic Gaussian mixture modeling, and its
hierarchical version, HGMM. AutoGMM builds upon scikit-learn's
AgglomerativeClustering and GaussianMixture classes, with certain modifications
to make the results more stable. Empirically, on several different
applications, AutoGMM performs approximately as well as mclust, and sometimes
better. This package is freely available, and further shrinks the gap between
functionality of R and Python for data science.
</p>
<a href="http://arxiv.org/abs/1909.02688" target="_blank">arXiv:1909.02688</a> [<a href="http://arxiv.org/pdf/1909.02688" target="_blank">pdf</a>]

<h2>Learning Enhanced Resolution-wise features for Human Pose Estimation. (arXiv:1909.05090v4 [cs.CV] UPDATED)</h2>
<h3>Kun Zhang, Peng He, Ping Yao, Ge Chen, Rui Wu, Min Du, Huimin Li, Li Fu, Tianyao Zheng</h3>
<p>Recently, multi-resolution networks (such as Hourglass, CPN, HRNet, etc.)
have achieved significant performance on pose estimation by combining feature
maps of various resolutions. In this paper, we propose a Resolution-wise
Attention Module (RAM) and Gradual Pyramid Refinement (GPR), to learn enhanced
resolution-wise feature maps for precise pose estimation. Specifically, RAM
learns a group of weights to represent the different importance of feature maps
across resolutions, and the GPR gradually merges every two feature maps from
low to high resolutions to regress final human keypoint heatmaps. With the
enhanced resolution-wise features learnt by CNN, we obtain more accurate human
keypoint locations. The efficacies of our proposed methods are demonstrated on
MS-COCO dataset, achieving state-of-the-art performance with average precision
of 77.7 on COCO val2017 set and 77.0 on test-dev2017 set without using extra
human keypoint training dataset.
</p>
<a href="http://arxiv.org/abs/1909.05090" target="_blank">arXiv:1909.05090</a> [<a href="http://arxiv.org/pdf/1909.05090" target="_blank">pdf</a>]

<h2>On the adequacy of untuned warmup for adaptive optimization. (arXiv:1910.04209v2 [cs.LG] UPDATED)</h2>
<h3>Jerry Ma, Denis Yarats</h3>
<p>Adaptive optimization algorithms such as Adam (Kingma &amp; Ba, 2014) are widely
used in deep learning. The stability of such algorithms is often improved with
a warmup schedule for the learning rate. Motivated by the difficulty of
choosing and tuning warmup schedules, Liu et al. (2020) propose automatic
variance rectification of Adam's adaptive learning rate, claiming that this
rectified approach ("RAdam") surpasses the vanilla Adam algorithm and reduces
the need for expensive tuning of Adam with warmup. In this work, we refute this
analysis and provide an alternative explanation for the necessity of warmup
based on the magnitude of the update term, which is of greater relevance to
training stability. We then provide some "rule-of-thumb" warmup schedules, and
we demonstrate that simple untuned warmup of Adam performs more-or-less
identically to RAdam in typical practical settings. We conclude by suggesting
that practitioners stick to linear warmup with Adam, with a sensible default
being linear warmup over $2 / (1 - \beta_2)$ training iterations.
</p>
<a href="http://arxiv.org/abs/1910.04209" target="_blank">arXiv:1910.04209</a> [<a href="http://arxiv.org/pdf/1910.04209" target="_blank">pdf</a>]

<h2>Variational Auto-encoder Based Bayesian Poisson Tensor Factorization for Sparse and Imbalanced Count Data. (arXiv:1910.05570v2 [cs.LG] UPDATED)</h2>
<h3>Yuan Jin, Ming Liu, Yunfeng Li, Ruohua Xu, Lan Du, Longxiang Gao, Yong Xiang</h3>
<p>Non-negative tensor factorization models enable predictive analysis on count
data. Among them, Bayesian Poisson-Gamma models can derive full posterior
distributions of latent factors and are less sensitive to sparse count data.
However, current inference methods for these Bayesian models adopt restricted
update rules for the posterior parameters. They also fail to share the update
information to better cope with the data sparsity. Moreover, these models are
not endowed with a component that handles the imbalance in count data values.
In this paper, we propose a novel variational auto-encoder framework called
VAE-BPTF which addresses the above issues. It uses multi-layer perceptron
networks to encode and share complex update information. The encoded
information is then reweighted per data instance to penalize common data values
before aggregated to compute the posterior parameters for the latent factors.
Under synthetic data evaluation, VAE-BPTF tended to recover the right number of
latent factors and posterior parameter values. It also outperformed current
models in both reconstruction errors and latent factor (semantic) coherence
across five real-world datasets. Furthermore, the latent factors inferred by
VAE-BPTF are perceived to be meaningful and coherent under a qualitative
analysis.
</p>
<a href="http://arxiv.org/abs/1910.05570" target="_blank">arXiv:1910.05570</a> [<a href="http://arxiv.org/pdf/1910.05570" target="_blank">pdf</a>]

<h2>ALET (Automated Labeling of Equipment and Tools): A Dataset, a Baseline and a Usecase for Tool Detection in the Wild. (arXiv:1910.11713v3 [cs.CV] UPDATED)</h2>
<h3>Fatih Can Kurnaz, Burak Hocao&#x11f;lu, Mert Kaan Y&#x131;lmaz, &#x130;dil S&#xfc;lo, Sinan Kalkan (KOVAN Research Lab, Dept. of Computer Engineering, Middle East Technical University, Ankara, Turkey)</h3>
<p>Robots collaborating with humans in realistic environments will need to be
able to detect the tools that can be used and manipulated. However, there is no
available dataset or study that addresses this challenge in real settings. In
this paper, we fill this gap by providing an extensive dataset (METU-ALET) for
detecting farming, gardening, office, stonemasonry, vehicle, woodworking and
workshop tools. The scenes correspond to sophisticated environments with or
without humans using the tools. The scenes we consider introduce several
challenges for object detection, including the small scale of the tools, their
articulated nature, occlusion, inter-class invariance, etc. Moreover, we train
and compare several state of the art deep object detectors (including Faster
R-CNN, Cascade R-CNN, RepPoint and RetinaNet) on our dataset. We observe that
the detectors have difficulty in detecting especially small-scale tools or
tools that are visually similar to parts of other tools. This in turn supports
the importance of our dataset and paper. With the dataset, the code and the
trained models, our work provides a basis for further research into tools and
their use in robotics applications.
</p>
<a href="http://arxiv.org/abs/1910.11713" target="_blank">arXiv:1910.11713</a> [<a href="http://arxiv.org/pdf/1910.11713" target="_blank">pdf</a>]

<h2>Decentralized SGD with Asynchronous, Local and Quantized Updates. (arXiv:1910.12308v3 [cs.LG] UPDATED)</h2>
<h3>Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Ilia Markov, Shigang Li, Dan Alistarh</h3>
<p>The ability to scale distributed optimization to large node counts has been
one of the main enablers of recent progress in machine learning. To this end,
several techniques have been explored, such as asynchronous, decentralized, or
quantized communication--which significantly reduce the cost of
synchronization, and the ability for nodes to perform several local model
updates before communicating--which reduces the frequency of synchronization.

In this paper, we show that these techniques, which have so far been
considered independently, can be jointly leveraged to minimize distribution
cost for training neural network models via stochastic gradient descent (SGD).
We consider a setting with minimal coordination: we have a large number of
nodes on a communication graph, each with a local subset of data, performing
independent SGD updates onto their local models. After some number of local
updates, each node chooses an interaction partner uniformly at random from its
neighbors, and averages a possibly quantized version of its local model with
the neighbor's model. Our first contribution is in proving that, even under
such a relaxed setting, SGD can still be guaranteed to converge under standard
assumptions. The proof is based on a new connection with parallel
load-balancing processes, and improves existing techniques by jointly handling
decentralization, asynchrony, quantization, and local updates, and by bounding
their impact. On the practical side, we implement variants of our algorithm and
deploy them onto distributed environments, and show that they can successfully
converge and scale for large-scale image classification and translation tasks,
matching or even slightly improving the accuracy of previous methods.
</p>
<a href="http://arxiv.org/abs/1910.12308" target="_blank">arXiv:1910.12308</a> [<a href="http://arxiv.org/pdf/1910.12308" target="_blank">pdf</a>]

<h2>Predicting gait events from tibial acceleration in rearfoot running: a structured machine learning approach. (arXiv:1910.13372v3 [cs.LG] UPDATED)</h2>
<h3>Pieter Robberechts, Rud Derie, Pieter Van den Berghe, Joeri Gerlo, Dirk De Clercq, Veerle Segers, Jesse Davis</h3>
<p>Gait event detection of the initial contact and toe off is essential for
running gait analysis, allowing the derivation of parameters such as stance
time. Heuristic-based methods exist to estimate these key gait events from
tibial accelerometry. However, these methods are tailored to very specific
acceleration profiles, which may offer complications when dealing with larger
data sets and inherent biological variability. Therefore, this paper
investigates whether a structured machine learning approach can achieve a more
accurate prediction of running gait event timings from tibial accelerometry.
Force-based event detection acted as the criterion measure in order to assess
the accuracy, repeatability and sensitivity of the predicted gait events. A
heuristic method and two structured machine learning methods were employed to
derive initial contact, toe off and stance time from tibial acceleration
signals. Both a structured perceptron model (median absolute error of stance
time estimation: 10.00 $\pm$ 8.73 ms) and a structured recurrent neural network
model (median absolute error of stance time estimation: 6.50 $\pm$ 5.74 ms)
significantly outperformed the existing heuristic approach (median absolute
error of stance time estimation: 11.25 $\pm$ 9.52 ms) on data from 93 rearfoot
runners. Thus, results indicate that a structured recurrent neural network
machine learning model offers the most accurate and consistent estimation of
the gait events and its derived stance time during level overground running.
The machine learning methods seem less affected by intra- and inter-subject
variation within the data, allowing for accurate and efficient automated data
output during rearfoot overground running. Furthermore offering possibilities
for real-time monitoring and biofeedback during prolonged measurements, even
outside the laboratory.
</p>
<a href="http://arxiv.org/abs/1910.13372" target="_blank">arXiv:1910.13372</a> [<a href="http://arxiv.org/pdf/1910.13372" target="_blank">pdf</a>]

<h2>Beyond Universal Person Re-ID Attack. (arXiv:1910.14184v3 [cs.CV] UPDATED)</h2>
<h3>Wenjie Ding, Xing Wei, Rongrong Ji, Xiaopeng Hong, Qi Tian, Yihong Gong</h3>
<p>Deep learning-based person re-identification (Re-ID) has made great progress
and achieved high performance recently. In this paper, we make the first
attempt to examine the vulnerability of current person Re-ID models against a
dangerous attack method, \ie, the universal adversarial perturbation (UAP)
attack, which has been shown to fool classification models with a little
overhead. We propose a \emph{more universal} adversarial perturbation (MUAP)
method for both image-agnostic and model-insensitive person Re-ID attack.
Firstly, we adopt a list-wise attack objective function to disrupt the
similarity ranking list directly. Secondly, we propose a model-insensitive
mechanism for cross-model attack. Extensive experiments show that the proposed
attack approach achieves high attack performance and outperforms other state of
the arts by large margin in cross-model scenario. The results also demonstrate
the vulnerability of current Re-ID models to MUAP and further suggest the need
of designing more robust Re-ID models.
</p>
<a href="http://arxiv.org/abs/1910.14184" target="_blank">arXiv:1910.14184</a> [<a href="http://arxiv.org/pdf/1910.14184" target="_blank">pdf</a>]

<h2>Ethical Dilemmas in Strategic Games. (arXiv:1911.00786v2 [cs.AI] UPDATED)</h2>
<h3>Pavel Naumov, Rui-Jie Yew</h3>
<p>An agent, or a coalition of agents, faces an ethical dilemma between several
statements if she is forced to make a conscious choice between which of these
statements will be true. This paper proposes to capture ethical dilemmas as a
modality in strategic game settings with and without limit on sacrifice and for
perfect and imperfect information games. The authors show that the dilemma
modality cannot be defined through the earlier proposed blameworthiness
modality. The main technical result is a sound and complete axiomatization of
the properties of this modality with sacrifice in games with perfect
information.
</p>
<a href="http://arxiv.org/abs/1911.00786" target="_blank">arXiv:1911.00786</a> [<a href="http://arxiv.org/pdf/1911.00786" target="_blank">pdf</a>]

<h2>A Cooperative Coordination Solver for Travelling Thief Problems. (arXiv:1911.03124v3 [cs.AI] UPDATED)</h2>
<h3>Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, Abdul Sattar</h3>
<p>The travelling thief problem (TTP) is a representative of multi-component
optimisation problems with interacting components. TTP combines the knapsack
problem (KP) and the travelling salesman problem (TSP). A thief performs a
cyclic tour through a set of cities, and pursuant to a collection plan,
collects a subset of items into a rented knapsack with finite capacity. The aim
is to maximise profit while minimising renting cost. Existing TTP solvers
typically solve the KP and TSP components in an interleaved manner: the
solution of one component is kept fixed while the solution of the other
component is modified. This suggests low coordination between solving the two
components, possibly leading to low quality TTP solutions. The 2-OPT heuristic
is often used for solving the TSP component, which reverses a segment in the
tour. Within TTP, 2-OPT does not take into account the collection plan, which
can result in a lower objective value. This in turn can result in the tour
modification to be rejected by a solver. We propose an expanded form of 2-OPT
to change the collection plan in coordination with tour modification. Items
regarded as less profitable and collected in cities located earlier in the
reversed segment are substituted by items that tend to be more profitable and
not collected in cities located later in the reversed segment. The collection
plan is further changed through a modified form of the hill-climbing bit-flip
search, where changes in the collection state are only permitted for boundary
items, which are defined as lowest profitable collected items or highest
profitable uncollected items. This restriction reduces the time spent on the KP
component, allowing more tours to be evaluated by the TSP component within a
time budget. The proposed approaches form the basis of a new cooperative
coordination solver, which is shown to outperform several state-of-the-art TTP
solvers.
</p>
<a href="http://arxiv.org/abs/1911.03124" target="_blank">arXiv:1911.03124</a> [<a href="http://arxiv.org/pdf/1911.03124" target="_blank">pdf</a>]

<h2>Attribute Restoration Framework for Anomaly Detection. (arXiv:1911.10676v3 [cs.CV] UPDATED)</h2>
<h3>Chaoqin Huang, Fei Ye, Jinkun Cao, Maosen Li, Ya Zhang, Cewu Lu</h3>
<p>With the recent advances in deep neural networks, anomaly detection in
multimedia has received much attention in the computer vision community. While
reconstruction-based methods have recently shown great promise for anomaly
detection, the information equivalence among input and supervision for
reconstruction tasks can not effectively force the network to learn semantic
feature embeddings. We here propose to break this equivalence by erasing
selected attributes from the original data and reformulate it as a restoration
task, where the normal and the anomalous data are expected to be
distinguishable based on restoration errors. Through forcing the network to
restore the original image, the semantic feature embeddings related to the
erased attributes are learned by the network. During testing phases, because
anomalous data are restored with the attribute learned from the normal data,
the restoration error is expected to be large. Extensive experiments have
demonstrated that the proposed method significantly outperforms several
state-of-the-arts on multiple benchmark datasets, especially on ImageNet,
increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate
our method on a real-world anomaly detection dataset MVTec AD and a video
anomaly detection dataset ShanghaiTech.
</p>
<a href="http://arxiv.org/abs/1911.10676" target="_blank">arXiv:1911.10676</a> [<a href="http://arxiv.org/pdf/1911.10676" target="_blank">pdf</a>]

<h2>Non-Autoregressive Coarse-to-Fine Video Captioning. (arXiv:1911.12018v5 [cs.CV] UPDATED)</h2>
<h3>Bang Yang, Yuexian Zou, Fenglin Liu, Can Zhang</h3>
<p>It is encouraged to see that progress has been made to bridge videos and
natural language. However, mainstream video captioning methods suffer from slow
inference speed due to the sequential manner of autoregressive decoding, and
prefer generating generic descriptions due to the insufficient training of
visual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this
paper, we propose a non-autoregressive decoding based model with a
coarse-to-fine captioning procedure to alleviate these defects. In
implementations, we employ a bi-directional self-attention based network as our
language model for achieving inference speedup, based on which we decompose the
captioning procedure into two stages, where the model has different focuses.
Specifically, given that visual words determine the semantic correctness of
captions, we design a mechanism of generating visual words to not only promote
the training of scene-related words but also capture relevant details from
videos to construct a coarse-grained sentence ``template''. Thereafter, we
devise dedicated decoding algorithms that fill in the ``template'' with
suitable words and modify inappropriate phrasing via iterative refinement to
obtain a fine-grained description. Extensive experiments on two mainstream
video captioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our
approach achieves state-of-the-art performance, generates diverse descriptions,
and obtains high inference efficiency.
</p>
<a href="http://arxiv.org/abs/1911.12018" target="_blank">arXiv:1911.12018</a> [<a href="http://arxiv.org/pdf/1911.12018" target="_blank">pdf</a>]

<h2>Solving Bayesian Inverse Problems via Variational Autoencoders. (arXiv:1912.04212v5 [stat.ML] UPDATED)</h2>
<h3>Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, Tan Bui-Thanh</h3>
<p>In recent years, the field of machine learning has made phenomenal progress
in the pursuit of simulating real-world data generation processes. One notable
example of such success is the variational autoencoder (VAE). In this work,
with a small shift in perspective, we leverage and adapt VAEs for a different
purpose: uncertainty quantification in scientific inverse problems. We
introduce UQ-VAE: a flexible, adaptive, hybrid data/model-informed framework
for training neural networks capable of rapid modelling of the posterior
distribution representing the unknown parameter of interest. Specifically, from
divergence-based variational inference, our framework is derived such that most
of the information usually present in scientific inverse problems is fully
utilized in the training procedure. Additionally, this framework includes an
adjustable hyperparameter that allows selection of the notion of distance
between the posterior model and the target distribution. This introduces more
flexibility in controlling how optimization directs the learning of the
posterior model. Further, this framework possesses an inherent adaptive
optimization property that emerges through the learning of the posterior
uncertainty.
</p>
<a href="http://arxiv.org/abs/1912.04212" target="_blank">arXiv:1912.04212</a> [<a href="http://arxiv.org/pdf/1912.04212" target="_blank">pdf</a>]

<h2>Soft Q Network. (arXiv:1912.10891v2 [cs.LG] UPDATED)</h2>
<h3>Jingbin Liu, Shuai Liu, Xinyang Gu</h3>
<p>Deep Q Network (DQN) is a very successful algorithm, yet the inherent problem
of reinforcement learning, i.e. the exploit-explore balance, remains. In this
work, we introduce entropy regularization into DQN and propose SQN. We find
that the backup equation of soft Q learning can enjoy the corrective feedback
if we view the soft backup as policy improvement in the form of Q, instead of
policy evaluation. We show that Soft Q Learning with Corrective Feedback
(SQL-CF) underlies the on-plicy nature of SQL and the equivalence of SQL and
Soft Policy Gradient (SPG). With these insights, we propose an on-policy
version of deep Q learning algorithm, i.e. Q On-Policy (QOP). We experiment
with QOP on a self-play environment called Google Research Football (GRF). The
QOP algorithm exhibits great stability and efficiency in training GRF agents.
</p>
<a href="http://arxiv.org/abs/1912.10891" target="_blank">arXiv:1912.10891</a> [<a href="http://arxiv.org/pdf/1912.10891" target="_blank">pdf</a>]

<h2>Geometry-Aware Generation of Adversarial Point Clouds. (arXiv:1912.11171v3 [cs.CV] UPDATED)</h2>
<h3>Yuxin Wen, Jiehong Lin, Ke Chen, C. L. Philip Chen, Kui Jia</h3>
<p>Machine learning models have been shown to be vulnerable to adversarial
examples. While most of the existing methods for adversarial attack and defense
work on the 2D image domain, a few recent attempts have been made to extend
them to 3D point cloud data. However, adversarial results obtained by these
methods typically contain point outliers, which are both noticeable and easy to
defend against using the simple techniques of outlier removal. Motivated by the
different mechanisms by which humans perceive 2D images and 3D shapes, in this
paper we propose the new design of \emph{geometry-aware objectives}, whose
solutions favor (the discrete versions of) the desired surface properties of
smoothness and fairness. To generate adversarial point clouds, we use a
targeted attack misclassification loss that supports continuous pursuit of
increasingly malicious signals. Regularizing the targeted attack loss with our
proposed geometry-aware objectives results in our proposed method,
Geometry-Aware Adversarial Attack ($GeoA^3$). The results of $GeoA^3$ tend to
be more harmful, arguably harder to defend against, and of the key adversarial
characterization of being imperceptible to humans. While the main focus of this
paper is to learn to generate adversarial point clouds, we also present a
simple but effective algorithm termed $Geo_{+}A^3$-IterNormPro, with Iterative
Normal Projection (IterNorPro) that solves a new objective function
$Geo_{+}A^3$, towards surface-level adversarial attacks via generation of
adversarial point clouds. We quantitatively evaluate our methods on both
synthetic and physical objects in terms of attack success rate and geometric
regularity. For a qualitative evaluation, we conduct subjective studies by
collecting human preferences from Amazon Mechanical Turk. Comparative results
in comprehensive experiments confirm the advantages of our proposed methods.
</p>
<a href="http://arxiv.org/abs/1912.11171" target="_blank">arXiv:1912.11171</a> [<a href="http://arxiv.org/pdf/1912.11171" target="_blank">pdf</a>]

<h2>privGAN: Protecting GANs from membership inference attacks at low cost. (arXiv:2001.00071v4 [cs.LG] UPDATED)</h2>
<h3>Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Juan Lavista Ferres</h3>
<p>Generative Adversarial Networks (GANs) have made releasing of synthetic
images a viable approach to share data without releasing the original dataset.
It has been shown that such synthetic data can be used for a variety of
downstream tasks such as training classifiers that would otherwise require the
original dataset to be shared. However, recent work has shown that the GAN
models and their synthetically generated data can be used to infer the training
set membership by an adversary who has access to the entire dataset and some
auxiliary information. Current approaches to mitigate this problem (such as
DPGAN) lead to dramatically poorer generated sample quality than the original
non--private GANs. Here we develop a new GAN architecture (privGAN), where the
generator is trained not only to cheat the discriminator but also to defend
membership inference attacks. The new mechanism provides protection against
this mode of attack while leading to negligible loss in downstream
performances. In addition, our algorithm has been shown to explicitly prevent
overfitting to the training set, which explains why our protection is so
effective. The main contributions of this paper are: i) we propose a novel GAN
architecture that can generate synthetic data in a privacy preserving manner
without additional hyperparameter tuning and architecture selection, ii) we
provide a theoretical understanding of the optimal solution of the privGAN loss
function, iii) we demonstrate the effectiveness of our model against several
white and black--box attacks on several benchmark datasets, iv) we demonstrate
on three common benchmark datasets that synthetic images generated by privGAN
lead to negligible loss in downstream performance when compared against
non--private GANs.
</p>
<a href="http://arxiv.org/abs/2001.00071" target="_blank">arXiv:2001.00071</a> [<a href="http://arxiv.org/pdf/2001.00071" target="_blank">pdf</a>]

<h2>Exploiting the Sensitivity of $L_2$ Adversarial Examples to Erase-and-Restore. (arXiv:2001.00116v2 [cs.CV] UPDATED)</h2>
<h3>Fei Zuo, Qiang Zeng</h3>
<p>By adding carefully crafted perturbations to input images, adversarial
examples (AEs) can be generated to mislead neural-network-based image
classifiers. $L_2$ adversarial perturbations by Carlini and Wagner (CW) are
among the most effective but difficult-to-detect attacks. While many
countermeasures against AEs have been proposed, detection of adaptive CW-$L_2$
AEs is still an open question. We find that, by randomly erasing some pixels in
an $L_2$ AE and then restoring it with an inpainting technique, the AE, before
and after the steps, tends to have different classification results, while a
benign sample does not show this symptom. We thus propose a novel AE detection
technique, Erase-and-Restore (E&amp;R), that exploits the intriguing sensitivity of
$L_2$ attacks. Experiments conducted on two popular image datasets, CIFAR-10
and ImageNet, show that the proposed technique is able to detect over 98% of
$L_2$ AEs and has a very low false positive rate on benign images. The
detection technique exhibits high transferability: a detection system trained
using CW-$L_2$ AEs can accurately detect AEs generated using another $L_2$
attack method. More importantly, our approach demonstrates strong resilience to
adaptive $L_2$ attacks, filling a critical gap in AE detection. Finally, we
interpret the detection technique through both visualization and
quantification.
</p>
<a href="http://arxiv.org/abs/2001.00116" target="_blank">arXiv:2001.00116</a> [<a href="http://arxiv.org/pdf/2001.00116" target="_blank">pdf</a>]

<h2>Separating Content from Style Using Adversarial Learning for Recognizing Text in the Wild. (arXiv:2001.04189v3 [cs.CV] UPDATED)</h2>
<h3>Canjie Luo, Qingxiang Lin, Yuliang Liu, Lianwen Jin, Chunhua Shen</h3>
<p>We propose to improve text recognition from a new perspective by separating
the text content from complex backgrounds. As vanilla GANs are not sufficiently
robust to generate sequence-like characters in natural images, we propose an
adversarial learning framework for the generation and recognition of multiple
characters in an image. The proposed framework consists of an attention-based
recognizer and a generative adversarial architecture. Furthermore, to tackle
the issue of lacking paired training samples, we design an interactive joint
training scheme, which shares attention masks from the recognizer to the
discriminator, and enables the discriminator to extract the features of each
character for further adversarial training. Benefiting from the character-level
adversarial training, our framework requires only unpaired simple data for
style supervision. Each target style sample containing only one randomly chosen
character can be simply synthesized online during the training. This is
significant as the training does not require costly paired samples or
character-level annotations. Thus, only the input images and corresponding text
labels are needed. In addition to the style normalization of the backgrounds,
we refine character patterns to ease the recognition task. A feedback mechanism
is proposed to bridge the gap between the discriminator and the recognizer.
Therefore, the discriminator can guide the generator according to the confusion
of the recognizer, so that the generated patterns are clearer for recognition.
Experiments on various benchmarks, including both regular and irregular text,
demonstrate that our method significantly reduces the difficulty of
recognition. Our framework can be integrated into recent recognition methods to
achieve new state-of-the-art recognition accuracy.
</p>
<a href="http://arxiv.org/abs/2001.04189" target="_blank">arXiv:2001.04189</a> [<a href="http://arxiv.org/pdf/2001.04189" target="_blank">pdf</a>]

<h2>Interpretation and Simplification of Deep Forest. (arXiv:2001.04721v4 [cs.AI] UPDATED)</h2>
<h3>Sangwon Kim, Mira Jeong, Byoung Chul Ko</h3>
<p>This paper proposes a new method for interpreting and simplifying a black box
model of a deep random forest (RF) using a proposed rule elimination. In deep
RF, a large number of decision trees are connected to multiple layers, thereby
making an analysis difficult. It has a high performance similar to that of a
deep neural network (DNN), but achieves a better generalizability. Therefore,
in this study, we consider quantifying the feature contributions and frequency
of the fully trained deep RF in the form of a decision rule set. The feature
contributions provide a basis for determining how features affect the decision
process in a rule set. Model simplification is achieved by eliminating
unnecessary rules by measuring the feature contributions. Consequently, the
simplified model has fewer parameters and rules than before. Experiment results
have shown that a feature contribution analysis allows a black box model to be
decomposed for quantitatively interpreting a rule set. The proposed method was
successfully applied to various deep RF models and benchmark datasets while
maintaining a robust performance despite the elimination of a large number of
rules.
</p>
<a href="http://arxiv.org/abs/2001.04721" target="_blank">arXiv:2001.04721</a> [<a href="http://arxiv.org/pdf/2001.04721" target="_blank">pdf</a>]

<h2>Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers. (arXiv:2002.06144v4 [cs.CV] UPDATED)</h2>
<h3>Rapha&#xeb;l Barman, Maud Ehrmann, Simon Clematide, Sofia Ares Oliveira, Fr&#xe9;d&#xe9;ric Kaplan</h3>
<p>The massive amounts of digitized historical documents acquired over the last
decades naturally lend themselves to automatic processing and exploration.
Research work seeking to automatically process facsimiles and extract
information thereby are multiplying with, as a first essential step, document
layout analysis. If the identification and categorization of segments of
interest in document images have seen significant progress over the last years
thanks to deep learning techniques, many challenges remain with, among others,
the use of finer-grained segmentation typologies and the consideration of
complex, heterogeneous documents such as historical newspapers. Besides, most
approaches consider visual features only, ignoring textual signal. In this
context, we introduce a multimodal approach for the semantic segmentation of
historical newspapers that combines visual and textual features. Based on a
series of experiments on diachronic Swiss and Luxembourgish newspapers, we
investigate, among others, the predictive power of visual and textual features
and their capacity to generalize across time and sources. Results show
consistent improvement of multimodal models in comparison to a strong visual
baseline, as well as better robustness to high material variance.
</p>
<a href="http://arxiv.org/abs/2002.06144" target="_blank">arXiv:2002.06144</a> [<a href="http://arxiv.org/pdf/2002.06144" target="_blank">pdf</a>]

<h2>Tensor denoising and completion based on ordinal observations. (arXiv:2002.06524v3 [stat.ML] UPDATED)</h2>
<h3>Chanwoo Lee, Miaoyan Wang</h3>
<p>Higher-order tensors arise frequently in applications such as neuroimaging,
recommendation system, social network analysis, and psychological studies. We
consider the problem of low-rank tensor estimation from possibly incomplete,
ordinal-valued observations. Two related problems are studied, one on tensor
denoising and the other on tensor completion. We propose a multi-linear
cumulative link model, develop a rank-constrained M-estimator, and obtain
theoretical accuracy guarantees. Our mean squared error bound enjoys a faster
convergence rate than previous results, and we show that the proposed estimator
is minimax optimal under the class of low-rank models. Furthermore, the
procedure developed serves as an efficient completion method which guarantees
consistent recovery of an order-$K$ $(d,\ldots,d)$-dimensional low-rank tensor
using only $\tilde{\mathcal{O}}(Kd)$ noisy, quantized observations. We
demonstrate the outperformance of our approach over previous methods on the
tasks of clustering and collaborative filtering.
</p>
<a href="http://arxiv.org/abs/2002.06524" target="_blank">arXiv:2002.06524</a> [<a href="http://arxiv.org/pdf/2002.06524" target="_blank">pdf</a>]

<h2>Guiding Graph Embeddings using Path-Ranking Methods for Error Detection innoisy Knowledge Graphs. (arXiv:2002.08762v2 [cs.LG] UPDATED)</h2>
<h3>K. Bougiatiotis, R. Fasoulis, F. Aisopos, A. Nentidis, G. Paliouras</h3>
<p>Nowadays Knowledge Graphs constitute a mainstream approach for the
representation of relational information on big heterogeneous data, however,
they may contain a big amount of imputed noise when constructed automatically.
To address this problem, different error detection methodologies have been
proposed, mainly focusing on path ranking and representation learning. This
work presents various mainstream approaches and proposes a hybrid and modular
methodology for the task. We compare different methods on two benchmarks and
one real-world biomedical publications dataset, showcasing the potential of our
approach and providing insights on graph embeddings when dealing with noisy
Knowledge Graphs.
</p>
<a href="http://arxiv.org/abs/2002.08762" target="_blank">arXiv:2002.08762</a> [<a href="http://arxiv.org/pdf/2002.08762" target="_blank">pdf</a>]

<h2>Adversarial Attacks and Defenses on Graphs: A Review, A Tool and Empirical Studies. (arXiv:2003.00653v3 [cs.LG] UPDATED)</h2>
<h3>Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, Jiliang Tang</h3>
<p>Deep neural networks (DNNs) have achieved significant performance in various
tasks. However, recent studies have shown that DNNs can be easily fooled by
small perturbation on the input, called adversarial attacks. As the extensions
of DNNs to graphs, Graph Neural Networks (GNNs) have been demonstrated to
inherit this vulnerability. Adversary can mislead GNNs to give wrong
predictions by modifying the graph structure such as manipulating a few edges.
This vulnerability has arisen tremendous concerns for adapting GNNs in
safety-critical applications and has attracted increasing research attention in
recent years. Thus, it is necessary and timely to provide a comprehensive
overview of existing graph adversarial attacks and the countermeasures. In this
survey, we categorize existing attacks and defenses, and review the
corresponding state-of-the-art methods. Furthermore, we have developed a
repository with representative algorithms
(https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph). The
repository enables us to conduct empirical studies to deepen our understandings
on attacks and defenses on graphs.
</p>
<a href="http://arxiv.org/abs/2003.00653" target="_blank">arXiv:2003.00653</a> [<a href="http://arxiv.org/pdf/2003.00653" target="_blank">pdf</a>]

<h2>Weakly-supervised Object Localization for Few-shot Learning and Fine-grained Few-shot Learning. (arXiv:2003.00874v3 [cs.CV] UPDATED)</h2>
<h3>Xiaojian He, Jinfu Lin, Junming Shen</h3>
<p>Few-shot learning (FSL) aims to learn novel visual categories from very few
samples, which is a challenging problem in real-world applications. Many
methods of few-shot classification work well on general images to learn global
representation. However, they can not deal with fine-grained categories well at
the same time due to a lack of subtle and local information. We argue that
localization is an efficient approach because it directly provides the
discriminative regions, which is critical for both general classification and
fine-grained classification in a low data regime. In this paper, we propose a
Self-Attention Based Complementary Module (SAC Module) to fulfill the
weakly-supervised object localization, and more importantly produce the
activated masks for selecting discriminative deep descriptors for few-shot
classification. Based on each selected deep descriptor, Semantic Alignment
Module (SAM) calculates the semantic alignment distance between the query and
support images to boost classification performance. Extensive experiments show
our method outperforms the state-of-the-art methods on benchmark datasets under
various settings, especially on the fine-grained few-shot tasks. Besides, our
method achieves superior performance over previous methods when training the
model on miniImageNet and evaluating it on the different datasets,
demonstrating its superior generalization capacity. Extra visualization shows
the proposed method can localize the key objects more interval.
</p>
<a href="http://arxiv.org/abs/2003.00874" target="_blank">arXiv:2003.00874</a> [<a href="http://arxiv.org/pdf/2003.00874" target="_blank">pdf</a>]

<h2>Anytime Inference with Distilled Hierarchical Neural Ensembles. (arXiv:2003.01474v3 [cs.CV] UPDATED)</h2>
<h3>Adria Ruiz, Jakob Verbeek</h3>
<p>Inference in deep neural networks can be computationally expensive, and
networks capable of anytime inference are important in mscenarios where the
amount of compute or quantity of input data varies over time. In such networks
the inference process can interrupted to provide a result faster, or continued
to obtain a more accurate result. We propose Hierarchical Neural Ensembles
(HNE), a novel framework to embed an ensemble of multiple networks in a
hierarchical tree structure, sharing intermediate layers. In HNE we control the
complexity of inference on-the-fly by evaluating more or less models in the
ensemble. Our second contribution is a novel hierarchical distillation method
to boost the prediction accuracy of small ensembles. This approach leverages
the nested structure of our ensembles, to optimally allocate accuracy and
diversity across the individual models. Our experiments show that, compared to
previous anytime inference models, HNE provides state-of-the-art
accuracy-computate trade-offs on the CIFAR-10/100 and ImageNet datasets.
</p>
<a href="http://arxiv.org/abs/2003.01474" target="_blank">arXiv:2003.01474</a> [<a href="http://arxiv.org/pdf/2003.01474" target="_blank">pdf</a>]

<h2>Variational Learning of Individual Survival Distributions. (arXiv:2003.04430v2 [stat.ML] UPDATED)</h2>
<h3>Zidi Xiu, Chenyang Tao, Benjamin A. Goldstein, Ricardo Henao</h3>
<p>The abundance of modern health data provides many opportunities for the use
of machine learning techniques to build better statistical models to improve
clinical decision making. Predicting time-to-event distributions, also known as
survival analysis, plays a key role in many clinical applications. We introduce
a variational time-to-event prediction model, named Variational Survival
Inference (VSI), which builds upon recent advances in distribution learning
techniques and deep neural networks. VSI addresses the challenges of
non-parametric distribution estimation by ($i$) relaxing the restrictive
modeling assumptions made in classical models, and ($ii$) efficiently handling
the censored observations, {\it i.e.}, events that occur outside the
observation window, all within the variational framework. To validate the
effectiveness of our approach, an extensive set of experiments on both
synthetic and real-world datasets is carried out, showing improved performance
relative to competing solutions.
</p>
<a href="http://arxiv.org/abs/2003.04430" target="_blank">arXiv:2003.04430</a> [<a href="http://arxiv.org/pdf/2003.04430" target="_blank">pdf</a>]

<h2>Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift. (arXiv:2003.04475v3 [cs.LG] UPDATED)</h2>
<h3>Remi Tachet, Han Zhao, Yu-Xiang Wang, Geoff Gordon</h3>
<p>Adversarial learning has demonstrated good performance in the unsupervised
domain adaptation setting, by learning domain-invariant representations.
However, recent work has shown limitations of this approach when label
distributions differ between the source and target domains. In this paper, we
propose a new assumption, generalized label shift ($GLS$), to improve
robustness against mismatched label distributions. $GLS$ states that,
conditioned on the label, there exists a representation of the input that is
invariant between the source and target domains. Under $GLS$, we provide
theoretical guarantees on the transfer performance of any classifier. We also
devise necessary and sufficient conditions for $GLS$ to hold, by using an
estimation of the relative class weights between domains and an appropriate
reweighting of samples. Our weight estimation method could be straightforwardly
and generically applied in existing domain adaptation (DA) algorithms that
learn domain-invariant representations, with small computational overhead. In
particular, we modify three DA algorithms, JAN, DANN and CDAN, and evaluate
their performance on standard and artificial DA tasks. Our algorithms
outperform the base versions, with vast improvements for large label
distribution mismatches. Our code is available at https://tinyurl.com/y585xt6j.
</p>
<a href="http://arxiv.org/abs/2003.04475" target="_blank">arXiv:2003.04475</a> [<a href="http://arxiv.org/pdf/2003.04475" target="_blank">pdf</a>]

<h2>MQA: Answering the Question via Robotic Manipulation. (arXiv:2003.04641v2 [cs.AI] UPDATED)</h2>
<h3>Yuhong Deng, Xiaofeng Guo, Naifu Zhang, Di Guo, Huaping Liu, Fuchun Sun</h3>
<p>In this paper, we propose a novel task -- Manipulation Question Answering
(MQA), where the robot is required to find the answer to the question by
actively exploring the environment via manipulation. A framework consisting of
a QA model and a manipulation model is proposed to solve this problem. For the
QA model, we adopt the method of Visual Question Answering (VQA). For the
manipulation model, a Deep Q Network (DQN) model is proposed to generate
manipulations. By manipulating objects, the robot can continuously explore the
bin until the answer to the question is found. Besides, a novel dataset for
simulation that contains a variety of object models, complicated scenarios and
corresponding question-answer pairs is established. Extensive experiments have
been conducted to validate the effectiveness of the proposed framework.
</p>
<a href="http://arxiv.org/abs/2003.04641" target="_blank">arXiv:2003.04641</a> [<a href="http://arxiv.org/pdf/2003.04641" target="_blank">pdf</a>]

<h2>Dynamic Region-Aware Convolution. (arXiv:2003.12243v2 [cs.CV] UPDATED)</h2>
<h3>Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, Jian Sun</h3>
<p>We propose a new convolution called Dynamic Region-Aware Convolution
(DRConv), which can automatically assign multiple filters to corresponding
spatial regions where features have similar representation. In this way, DRConv
outperforms standard convolution in modeling semantic variations. Standard
convolutional layer can increase the number of filers to extract more visual
elements but results in high computational cost. More gracefully, our DRConv
transfers the increasing channel-wise filters to spatial dimension with
learnable instructor, which not only improve representation ability of
convolution, but also maintains computational cost and the
translation-invariance as standard convolution dose. DRConv is an effective and
elegant method for handling complex and variable spatial information
distribution. It can substitute standard convolution in any existing networks
for its plug-and-play property, especially to power convolution layers in
efficient networks. We evaluate DRConv on a wide range of models (MobileNet
series, ShuffleNetV2, etc.) and tasks (Classification, Face Recognition,
Detection and Segmentation). On ImageNet classification, DRConv-based
ShuffleNetV2-0.5x achieves state-of-the-art performance of 67.1% at 46M
multiply-adds level with 6.3% relative improvement.
</p>
<a href="http://arxiv.org/abs/2003.12243" target="_blank">arXiv:2003.12243</a> [<a href="http://arxiv.org/pdf/2003.12243" target="_blank">pdf</a>]

<h2>CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks. (arXiv:2003.12798v2 [cs.CV] UPDATED)</h2>
<h3>Qihang Yu, Yingwei Li, Jieru Mei, Yuyin Zhou, Alan L. Yuille</h3>
<p>3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene
understanding, such as video analysis and volumetric image recognition.
However, 3D networks can easily lead to over-parameterization which incurs
expensive computation cost. In this paper, we propose Channel-wise Automatic
KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard
3D convolutions into a set of economic operations e.g., 1D, 2D convolutions.
Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which
enjoys the following benefits: 1) enabling operations deployed in every layer
to be heterogeneous, so that they can extract diverse and complementary
information to benefit the learning process; and 2) allowing for an efficient
and flexible replacement design, which can be generalized to both
spatial-temporal and volumetric data. Further, we propose a new search space
based on CAKES, so that the replacement configuration can be determined
automatically for simplifying 3D networks. CAKES shows superior performance to
other methods with similar model size, and it also achieves comparable
performance to state-of-the-art with much fewer parameters and computational
costs on tasks including 3D medical imaging segmentation and video action
recognition. Codes and models are available at
https://github.com/yucornetto/CAKES
</p>
<a href="http://arxiv.org/abs/2003.12798" target="_blank">arXiv:2003.12798</a> [<a href="http://arxiv.org/pdf/2003.12798" target="_blank">pdf</a>]

<h2>GANSpace: Discovering Interpretable GAN Controls. (arXiv:2004.02546v3 [cs.CV] UPDATED)</h2>
<h3>Erik H&#xe4;rk&#xf6;nen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris</h3>
<p>This paper describes a simple technique to analyze Generative Adversarial
Networks (GANs) and create interpretable controls for image synthesis, such as
change of viewpoint, aging, lighting, and time of day. We identify important
latent directions based on Principal Components Analysis (PCA) applied either
in latent space or feature space. Then, we show that a large number of
interpretable controls can be defined by layer-wise perturbation along the
principal directions. Moreover, we show that BigGAN can be controlled with
layer-wise inputs in a StyleGAN-like manner. We show results on different GANs
trained on various datasets, and demonstrate good qualitative matches to edit
directions found through earlier supervised approaches.
</p>
<a href="http://arxiv.org/abs/2004.02546" target="_blank">arXiv:2004.02546</a> [<a href="http://arxiv.org/pdf/2004.02546" target="_blank">pdf</a>]

<h2>Estimate of the Neural Network Dimension Using Algebraic Topology and Lie Theory. (arXiv:2004.02881v10 [stat.ML] UPDATED)</h2>
<h3>Luciano Melodia, Richard Lenz</h3>
<p>In this paper we present an approach to determine the smallest possible
number of neurons in a layer of a neural network in such a way that the
topology of the input space can be learned sufficiently well. We introduce a
general procedure based on persistent homology to investigate topological
invariants of the manifold on which we suspect the data set. We specify the
required dimensions precisely, assuming that there is a smooth manifold on or
near which the data are located. Furthermore, we require that this space is
connected and has a commutative group structure in the mathematical sense.
These assumptions allow us to derive a decomposition of the underlying space
whose topology is well known. We use the representatives of the $k$-dimensional
homology groups from the persistence landscape to determine an integer
dimension for this decomposition. This number is the dimension of the embedding
that is capable of capturing the topology of the data manifold. We derive the
theory and validate it experimentally on toy data sets.
</p>
<a href="http://arxiv.org/abs/2004.02881" target="_blank">arXiv:2004.02881</a> [<a href="http://arxiv.org/pdf/2004.02881" target="_blank">pdf</a>]

<h2>Self-Supervised Tuning for Few-Shot Segmentation. (arXiv:2004.05538v2 [cs.CV] UPDATED)</h2>
<h3>Kai Zhu, Wei Zhai, Zheng-Jun Zha, Yang Cao</h3>
<p>Few-shot segmentation aims at assigning a category label to each image pixel
with few annotated samples. It is a challenging task since the dense prediction
can only be achieved under the guidance of latent features defined by sparse
annotations. Existing meta-learning method tends to fail in generating
category-specifically discriminative descriptor when the visual features
extracted from support images are marginalized in embedding space. To address
this issue, this paper presents an adaptive tuning framework, in which the
distribution of latent features across different episodes is dynamically
adjusted based on a self-segmentation scheme, augmenting category-specific
descriptors for label prediction. Specifically, a novel self-supervised
inner-loop is firstly devised as the base learner to extract the underlying
semantic features from the support image. Then, gradient maps are calculated by
back-propagating self-supervised loss through the obtained features, and
leveraged as guidance for augmenting the corresponding elements in embedding
space. Finally, with the ability to continuously learn from different episodes,
an optimization-based meta-learner is adopted as outer loop of our proposed
framework to gradually refine the segmentation results. Extensive experiments
on benchmark PASCAL-$5^{i}$ and COCO-$20^{i}$ datasets demonstrate the
superiority of our proposed method over state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2004.05538" target="_blank">arXiv:2004.05538</a> [<a href="http://arxiv.org/pdf/2004.05538" target="_blank">pdf</a>]

<h2>Models Genesis. (arXiv:2004.07882v3 [cs.CV] UPDATED)</h2>
<h3>Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B. Gotway, Jianming Liang</h3>
<p>Transfer learning from natural images to medical images has been established
as one of the most practical paradigms in deep learning for medical image
analysis. To fit this paradigm, however, 3D imaging tasks in the most prominent
imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D,
losing rich 3D anatomical information, thereby inevitably compromising its
performance. To overcome this limitation, we have built a set of models, called
Generic Autodidactic Models, nicknamed Models Genesis, because they are created
ex nihilo (with no manual labeling), self-taught (learnt by self-supervision),
and generic (served as source models for generating application-specific target
models). Our extensive experiments demonstrate that our Models Genesis
significantly outperform learning from scratch and existing pre-trained 3D
models in all five target 3D applications covering both segmentation and
classification. More importantly, learning a model from scratch simply in 3D
may not necessarily yield performance better than transfer learning from
ImageNet in 2D, but our Models Genesis consistently top any 2D/2.5D approaches
including fine-tuning the models pre-trained from ImageNet as well as
fine-tuning the 2D versions of our Models Genesis, confirming the importance of
3D anatomical information and significance of Models Genesis for 3D medical
imaging. This performance is attributed to our unified self-supervised learning
framework, built on a simple yet powerful observation: the sophisticated and
recurrent anatomy in medical images can serve as strong yet free supervision
signals for deep models to learn common anatomical representation automatically
via self-supervision. As open science, all codes and pre-trained Models Genesis
are available at https://github.com/MrGiovanni/ModelsGenesis.
</p>
<a href="http://arxiv.org/abs/2004.07882" target="_blank">arXiv:2004.07882</a> [<a href="http://arxiv.org/pdf/2004.07882" target="_blank">pdf</a>]

<h2>Gradient-Induced Co-Saliency Detection. (arXiv:2004.13364v3 [cs.CV] UPDATED)</h2>
<h3>Zhao Zhang, Wenda Jin, Jun Xu, Ming-Ming Cheng</h3>
<p>Co-saliency detection (Co-SOD) aims to segment the common salient foreground
in a group of relevant images. In this paper, inspired by human behavior, we
propose a gradient-induced co-saliency detection (GICD) method. We first
abstract a consensus representation for the grouped images in the embedding
space; then, by comparing the single image with consensus representation, we
utilize the feedback gradient information to induce more attention to the
discriminative co-salient features. In addition, due to the lack of Co-SOD
training data, we design a jigsaw training strategy, with which Co-SOD networks
can be trained on general saliency datasets without extra pixel-level
annotations. To evaluate the performance of Co-SOD methods on discovering the
co-salient object among multiple foregrounds, we construct a challenging CoCA
dataset, where each image contains at least one extraneous foreground along
with the co-salient object. Experiments demonstrate that our GICD achieves
state-of-the-art performance. Our codes and dataset are available at
https://mmcheng.net/gicd/.
</p>
<a href="http://arxiv.org/abs/2004.13364" target="_blank">arXiv:2004.13364</a> [<a href="http://arxiv.org/pdf/2004.13364" target="_blank">pdf</a>]

<h2>Adversarial Training against Location-Optimized Adversarial Patches. (arXiv:2005.02313v2 [cs.CV] UPDATED)</h2>
<h3>Sukrut Rao, David Stutz, Bernt Schiele</h3>
<p>Deep neural networks have been shown to be susceptible to adversarial
examples -- small, imperceptible changes constructed to cause
mis-classification in otherwise highly accurate image classifiers. As a
practical alternative, recent work proposed so-called adversarial patches:
clearly visible, but adversarially crafted rectangular patches in images. These
patches can easily be printed and applied in the physical world. While defenses
against imperceptible adversarial examples have been studied extensively,
robustness against adversarial patches is poorly understood. In this work, we
first devise a practical approach to obtain adversarial patches while actively
optimizing their location within the image. Then, we apply adversarial training
on these location-optimized adversarial patches and demonstrate significantly
improved robustness on CIFAR10 and GTSRB. Additionally, in contrast to
adversarial training on imperceptible adversarial examples, our adversarial
patch training does not reduce accuracy.
</p>
<a href="http://arxiv.org/abs/2005.02313" target="_blank">arXiv:2005.02313</a> [<a href="http://arxiv.org/pdf/2005.02313" target="_blank">pdf</a>]

<h2>Project RISE: Recognizing Industrial Smoke Emissions. (arXiv:2005.06111v7 [cs.CV] UPDATED)</h2>
<h3>Yen-Chia Hsu, Ting-Hao &#x27;Kenneth&#x27; Huang, Ting-Yao Hu, Paul Dille, Sean Prendi, Ryan Hoffman, Anastasia Tsuhlares, Jessica Pachuta, Randy Sargent, Illah Nourbakhsh</h3>
<p>Industrial smoke emissions pose a significant concern to human health. Prior
works have shown that using Computer Vision (CV) techniques to identify smoke
as visual evidence can influence the attitude of regulators and empower
citizens to pursue environmental justice. However, existing datasets are not of
sufficient quality nor quantity to train the robust CV models needed to support
air quality advocacy. We introduce RISE, the first large-scale video dataset
for Recognizing Industrial Smoke Emissions. We adopted a citizen science
approach to collaborate with local community members to annotate whether a
video clip has smoke emissions. Our dataset contains 12,567 clips from 19
distinct views from cameras that monitored three industrial facilities. These
daytime clips span 30 days over two years, including all four seasons. We ran
experiments using deep neural networks to establish a strong performance
baseline and reveal smoke recognition challenges. Our survey study discussed
community feedback, and our data analysis displayed opportunities for
integrating citizen scientists and crowd workers into the application of
Artificial Intelligence for Social Impact.
</p>
<a href="http://arxiv.org/abs/2005.06111" target="_blank">arXiv:2005.06111</a> [<a href="http://arxiv.org/pdf/2005.06111" target="_blank">pdf</a>]

<h2>3D Face Anti-spoofing with Factorized Bilinear Coding. (arXiv:2005.06514v3 [cs.CV] UPDATED)</h2>
<h3>Shan Jia, Xin Li, Chuanbo Hu, Guodong Guo, Zhengquan Xu</h3>
<p>We have witnessed rapid advances in both face presentation attack models and
presentation attack detection (PAD) in recent years. When compared with widely
studied 2D face presentation attacks, 3D face spoofing attacks are more
challenging because face recognition systems are more easily confused by the 3D
characteristics of materials similar to real faces. In this work, we tackle the
problem of detecting these realistic 3D face presentation attacks, and propose
a novel anti-spoofing method from the perspective of fine-grained
classification. Our method, based on factorized bilinear coding of multiple
color channels (namely MC\_FBC), targets at learning subtle fine-grained
differences between real and fake images. By extracting discriminative and
fusing complementary information from RGB and YCbCr spaces, we have developed a
principled solution to 3D face spoofing detection. A large-scale wax figure
face database (WFFD) with both images and videos has also been collected as
super-realistic attacks to facilitate the study of 3D face presentation attack
detection. Extensive experimental results show that our proposed method
achieves the state-of-the-art performance on both our own WFFD and other face
spoofing databases under various intra-database and inter-database testing
scenarios.
</p>
<a href="http://arxiv.org/abs/2005.06514" target="_blank">arXiv:2005.06514</a> [<a href="http://arxiv.org/pdf/2005.06514" target="_blank">pdf</a>]

<h2>DeepSSM: Deep State-Space Model for 3D Human Motion Prediction. (arXiv:2005.12155v3 [cs.CV] UPDATED)</h2>
<h3>Xiaoli Liu, Jianqin Yin, Huaping Liu, Jun Liu</h3>
<p>Predicting future human motion plays a significant role in human-machine
interactions for a variety of real-life applications. In this paper, we build a
deep state-space model, DeepSSM, to predict future human motion. Specifically,
we formulate the human motion system as the state-space model of a dynamic
system and model the motion system by the state-space theory, offering a
unified formulation for diverse human motion systems. Moreover, a novel deep
network is designed to build this system, enabling us to utilize both the
advantages of deep network and state-space model. The deep network jointly
models the process of both the state-state transition and the state-observation
transition of the human motion system, and multiple future poses can be
generated via the state-observation transition of the model recursively. To
improve the modeling ability of the system, a unique loss function, ATPL
(Attention Temporal Prediction Loss), is introduced to optimize the model,
encouraging the system to achieve more accurate predictions by paying
increasing attention to the early time-steps. The experiments on two benchmark
datasets (i.e., Human3.6M and 3DPW) confirm that our method achieves
state-of-the-art performance with improved effectiveness. The code will be
available if the paper is accepted.
</p>
<a href="http://arxiv.org/abs/2005.12155" target="_blank">arXiv:2005.12155</a> [<a href="http://arxiv.org/pdf/2005.12155" target="_blank">pdf</a>]

<h2>SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition. (arXiv:2005.13117v2 [cs.CV] UPDATED)</h2>
<h3>Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Fei Wu, Futai Zou</h3>
<p>Arbitrary text appearance poses a great challenge in scene text recognition
tasks. Existing works mostly handle with the problem in consideration of the
shape distortion, including perspective distortions, line curvature or other
style variations. Therefore, methods based on spatial transformers are
extensively studied. However, chromatic difficulties in complex scenes have not
been paid much attention on. In this work, we introduce a new learnable
geometric-unrelated module, the Structure-Preserving Inner Offset Network
(SPIN), which allows the color manipulation of source data within the network.
This differentiable module can be inserted before any recognition architecture
to ease the downstream tasks, giving neural networks the ability to actively
transform input intensity rather than the existing spatial rectification. It
can also serve as a complementary module to known spatial transformations and
work in both independent and collaborative ways with them. Extensive
experiments show that the use of SPIN results in a significant improvement on
multiple text recognition benchmarks compared to the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2005.13117" target="_blank">arXiv:2005.13117</a> [<a href="http://arxiv.org/pdf/2005.13117" target="_blank">pdf</a>]

<h2>Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person Re-Identification. (arXiv:2006.00878v4 [cs.CV] UPDATED)</h2>
<h3>Hanrong Ye, Hong Liu, Fanyang Meng, Xia Li</h3>
<p>RGB-Infrared person re-identification (RGB-IR Re- ID) is a cross-modality
matching problem, where the modality discrepancy is a big challenge. Most
existing works use Euclidean metric based constraints to resolve the
discrepancy between features of images from different modalities. However,
these methods are incapable of learning angularly discriminative feature
embedding because Euclidean distance cannot measure the included angle between
embedding vectors effectively. As an angularly discriminative feature space is
important for classifying the human images based on their embedding vectors, in
this paper, we propose a novel ranking loss function, named Bi-directional
Exponential Angular Triplet Loss, to help learn an angularly separable common
feature space by explicitly constraining the included angles between embedding
vectors. Moreover, to help stabilize and learn the magnitudes of embedding
vectors, we adopt a common space batch normalization layer. The quantitative
and qualitative experiments on the SYSU-MM01 and RegDB dataset support our
analysis. On SYSU-MM01 dataset, the performance is improved from 7.40% / 11.46%
to 38.57% / 38.61% for rank-1 accuracy / mAP compared with the baseline. The
proposed method can be generalized to the task of single-modality Re-ID and
improves the rank-1 accuracy / mAP from 92.0% / 81.7% to 94.7% / 86.6% on the
Market-1501 dataset, from 82.6% / 70.6% to 87.6% / 77.1% on the DukeMTMC-reID
dataset.
</p>
<a href="http://arxiv.org/abs/2006.00878" target="_blank">arXiv:2006.00878</a> [<a href="http://arxiv.org/pdf/2006.00878" target="_blank">pdf</a>]

<h2>MultiXNet: Multiclass Multistage Multimodal Motion Prediction. (arXiv:2006.02000v3 [cs.CV] UPDATED)</h2>
<h3>Nemanja Djuric, Henggang Cui, Zhaoen Su, Shangxuan Wu, Huahua Wang, Fang-Chieh Chou, Luisa San Martin, Song Feng, Rui Hu, Yang Xu, Alyssa Dayan, Sidney Zhang, Brian C. Becker, Gregory P. Meyer, Carlos Vallespi-Gonzalez, Carl K. Wellington</h3>
<p>One of the critical pieces of the self-driving puzzle is understanding the
surroundings of the self-driving vehicle (SDV) and predicting how these
surroundings will change in the near future. To address this task we propose
MultiXNet, an end-to-end approach for detection and motion prediction based
directly on lidar sensor data. This approach builds on prior work by handling
multiple classes of traffic actors, adding a jointly trained second-stage
trajectory refinement step, and producing a multimodal probability distribution
over future actor motion that includes both multiple discrete traffic behaviors
and calibrated continuous uncertainties. The method was evaluated on a
large-scale, real-world data set collected by a fleet of SDVs in several
cities, with the results indicating that it outperforms existing
state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2006.02000" target="_blank">arXiv:2006.02000</a> [<a href="http://arxiv.org/pdf/2006.02000" target="_blank">pdf</a>]

<h2>UVeQFed: Universal Vector Quantization for Federated Learning. (arXiv:2006.03262v3 [cs.LG] UPDATED)</h2>
<h3>Nir Shlezinger, Mingzhe Chen, Yonina C. Eldar, H. Vincent Poor, Shuguang Cui</h3>
<p>Traditional deep learning models are trained at a centralized server using
labeled data samples collected from end devices or users. Such data samples
often include private information, which the users may not be willing to share.
Federated learning (FL) is an emerging approach to train such learning models
without requiring the users to share their possibly private labeled data. In
FL, each user trains its copy of the learning model locally. The server then
collects the individual updates and aggregates them into a global model. A
major challenge that arises in this method is the need of each user to
efficiently transmit its learned model over the throughput limited uplink
channel. In this work, we tackle this challenge using tools from quantization
theory. In particular, we identify the unique characteristics associated with
conveying trained models over rate-constrained channels, and propose a suitable
quantization scheme for such settings, referred to as universal vector
quantization for FL (UVeQFed). We show that combining universal vector
quantization methods with FL yields a decentralized training system in which
the compression of the trained models induces only a minimum distortion. We
then theoretically analyze the distortion, showing that it vanishes as the
number of users grows. We also characterize the convergence of models trained
with the traditional federated averaging method combined with UVeQFed to the
model which minimizes the loss function. Our numerical results demonstrate the
gains of UVeQFed over previously proposed methods in terms of both distortion
induced in quantization and accuracy of the resulting aggregated model.
</p>
<a href="http://arxiv.org/abs/2006.03262" target="_blank">arXiv:2006.03262</a> [<a href="http://arxiv.org/pdf/2006.03262" target="_blank">pdf</a>]

<h2>SHADOWCAST: Controllable Graph Generation with Explainability. (arXiv:2006.03774v2 [cs.LG] UPDATED)</h2>
<h3>Wesley Joon-Wie Tann, Ee-Chien Chang, Bryan Hooi</h3>
<p>We introduce the problem of explaining graph generation, formulated as
controlling the generative process to produce desired graphs with explainable
structures. By directing this generative process, we can explain the observed
outcomes. We propose SHADOWCAST, a controllable generative model capable of
mimicking networks and directing the generation, as an approach to this novel
problem. The proposed model is based on a conditional generative adversarial
network for graph data. We design it with the capability to control the
conditions using a simple and transparent Markov model. Comprehensive
experiments on three real-world network datasets demonstrate our model's
competitive performance in the graph generation task. Furthermore, we control
SHADOWCAST to generate graphs of different structures to show its effective
controllability and explainability. As the first work to pose the problem of
explaining generated graphs by controlling the generation, SHADOWCAST paves the
way for future research in this exciting area.
</p>
<a href="http://arxiv.org/abs/2006.03774" target="_blank">arXiv:2006.03774</a> [<a href="http://arxiv.org/pdf/2006.03774" target="_blank">pdf</a>]

<h2>Copy that! Editing Sequences by Copying Spans. (arXiv:2006.04771v2 [cs.LG] UPDATED)</h2>
<h3>Sheena Panthaplackel, Miltiadis Allamanis, Marc Brockschmidt</h3>
<p>Neural sequence-to-sequence models are finding increasing use in editing of
documents, for example in correcting a text document or repairing source code.
In this paper, we argue that common seq2seq models (with a facility to copy
single tokens) are not a natural fit for such tasks, as they have to explicitly
copy each unchanged token. We present an extension of seq2seq models capable of
copying entire spans of the input to the output in one step, greatly reducing
the number of decisions required during inference. This extension means that
there are now many ways of generating the same output, which we handle by
deriving a new objective for training and a variation of beam search for
inference that explicitly handles this problem. In our experiments on a range
of editing tasks of natural language and source code, we show that our new
model consistently outperforms simpler baselines.
</p>
<a href="http://arxiv.org/abs/2006.04771" target="_blank">arXiv:2006.04771</a> [<a href="http://arxiv.org/pdf/2006.04771" target="_blank">pdf</a>]

<h2>The Curious Case of Convex Neural Networks. (arXiv:2006.05103v2 [cs.LG] UPDATED)</h2>
<h3>Sarath Sivaprasad, Ankur Singh, Naresh Manwani, Vineet Gandhi</h3>
<p>In this paper, we investigate a constrained formulation of neural networks
where the output is a convex function of the input. We show that the convexity
constraints can be enforced on both fully connected and convolutional layers,
making them applicable to most architectures. The convexity constraints include
restricting the weights (for all but the first layer) to be non-negative and
using a non-decreasing convex activation function. Albeit simple, these
constraints have profound implications on the generalization abilities of the
network. We draw three valuable insights: (a) Input Output Convex Neural
Networks (IOC-NNs) self regularize and reduce the problem of overfitting; (b)
Although heavily constrained, they outperform the base multi layer perceptrons
and achieve similar performance as compared to base convolutional architectures
and (c) IOC-NNs show robustness to noise in train labels. We demonstrate the
efficacy of the proposed idea using thorough experiments and ablation studies
on standard image classification datasets with three different neural network
architectures.
</p>
<a href="http://arxiv.org/abs/2006.05103" target="_blank">arXiv:2006.05103</a> [<a href="http://arxiv.org/pdf/2006.05103" target="_blank">pdf</a>]

<h2>Dialog Policy Learning for Joint Clarification and Active Learning Queries. (arXiv:2006.05456v3 [cs.CV] UPDATED)</h2>
<h3>Aishwarya Padmakumar, Raymond J. Mooney</h3>
<p>Intelligent systems need to be able to recover from mistakes, resolve
uncertainty, and adapt to novel concepts not seen during training. Dialog
interaction can enable this by the use of clarifications for correction and
resolving uncertainty, and active learning queries to learn new concepts
encountered during operation. Prior work on dialog systems has either focused
on exclusively learning how to perform clarification/ information seeking, or
to perform active learning. In this work, we train a hierarchical dialog policy
to jointly perform both clarification and active learning in the context of an
interactive language-based image retrieval task motivated by an online shopping
application, and demonstrate that jointly learning dialog policies for
clarification and active learning is more effective than the use of static
dialog policies for one or both of these functions.
</p>
<a href="http://arxiv.org/abs/2006.05456" target="_blank">arXiv:2006.05456</a> [<a href="http://arxiv.org/pdf/2006.05456" target="_blank">pdf</a>]

<h2>Graph Neural Networks for Motion Planning. (arXiv:2006.06248v2 [cs.RO] UPDATED)</h2>
<h3>Arbaaz Khan, Alejandro Ribeiro, Vijay Kumar, Anthony G. Francis</h3>
<p>This paper investigates the feasibility of using Graph Neural Networks (GNNs)
for classical motion planning problems. We propose guiding both continuous and
discrete planning algorithms using GNNs' ability to robustly encode the
topology of the planning space using a property called permutation invariance.
We present two techniques, GNNs over dense fixed graphs for low-dimensional
problems and sampling-based GNNs for high-dimensional problems. We examine the
ability of a GNN to tackle planning problems such as identifying critical nodes
or learning the sampling distribution in Rapidly-exploring Random Trees (RRT).
Experiments with critical sampling, a pendulum and a six DoF robot arm show
GNNs improve on traditional analytic methods as well as learning approaches
using fully-connected or convolutional neural networks.
</p>
<a href="http://arxiv.org/abs/2006.06248" target="_blank">arXiv:2006.06248</a> [<a href="http://arxiv.org/pdf/2006.06248" target="_blank">pdf</a>]

<h2>Weakly-supervised Temporal Action Localization by Uncertainty Modeling. (arXiv:2006.07006v2 [cs.CV] UPDATED)</h2>
<h3>Pilhyeon Lee, Jinglu Wang, Yan Lu, Hyeran Byun</h3>
<p>Weakly-supervised temporal action localization aims to learn detecting
temporal intervals of action classes with only video-level labels. To this end,
it is crucial to separate frames of action classes from the background frames
(i.e., frames not belonging to any action classes). In this paper, we present a
new perspective on background frames where they are modeled as
out-of-distribution samples regarding their inconsistency. Then, background
frames can be detected by estimating the probability of each frame being
out-of-distribution, known as uncertainty, but it is infeasible to directly
learn uncertainty without frame-level labels. To realize the uncertainty
learning in the weakly-supervised setting, we leverage the multiple instance
learning formulation. Moreover, we further introduce a background entropy loss
to better discriminate background frames by encouraging their in-distribution
(action) probabilities to be uniformly distributed over all action classes.
Experimental results show that our uncertainty modeling is effective at
alleviating the interference of background frames and brings a large
performance gain without bells and whistles. We demonstrate that our model
significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14
and ActivityNet (1.2 &amp; 1.3). Our code is available at
https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.
</p>
<a href="http://arxiv.org/abs/2006.07006" target="_blank">arXiv:2006.07006</a> [<a href="http://arxiv.org/pdf/2006.07006" target="_blank">pdf</a>]

<h2>Flexible Dataset Distillation: Learn Labels Instead of Images. (arXiv:2006.08572v3 [cs.LG] UPDATED)</h2>
<h3>Ondrej Bohdal, Yongxin Yang, Timothy Hospedales</h3>
<p>We study the problem of dataset distillation - creating a small set of
synthetic examples capable of training a good model. In particular, we study
the problem of label distillation - creating synthetic labels for a small set
of real images, and show it to be more effective than the prior image-based
approach to dataset distillation. Methodologically, we introduce a more robust
and flexible meta-learning algorithm for distillation, as well as an effective
first-order strategy based on convex optimization layers. Distilling labels
with our new algorithm leads to improved results over prior image-based
distillation. More importantly, it leads to clear improvements in flexibility
of the distilled dataset in terms of compatibility with off-the-shelf
optimizers and diverse neural architectures. Interestingly, label distillation
can also be applied across datasets, for example enabling learning Japanese
character recognition by training only on synthetically labeled English
letters.
</p>
<a href="http://arxiv.org/abs/2006.08572" target="_blank">arXiv:2006.08572</a> [<a href="http://arxiv.org/pdf/2006.08572" target="_blank">pdf</a>]

<h2>Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming. (arXiv:2006.10518v2 [cs.LG] UPDATED)</h2>
<h3>Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry</h3>
<p>Lately, post-training quantization methods have gained considerable
attention, as they are simple to use, and require only a small unlabeled
calibration set. This small dataset cannot be used to fine-tune the model
without significant over-fitting. Instead, these methods only use the
calibration set to set the activations' dynamic ranges. However, such methods
always resulted in significant accuracy degradation, when used below 8-bits
(except on small datasets). Here we aim to break the 8-bit barrier. To this
end, we minimize the quantization errors of each layer separately by optimizing
its parameters over the calibration set. We empirically demonstrate that this
approach is: (1) much less susceptible to over-fitting than the standard
fine-tuning approaches, and can be used even on a very small calibration set;
and (2) more powerful than previous methods, which only set the activations'
dynamic ranges. Furthermore, we demonstrate how to optimally allocate the
bit-widths for each layer, while constraining accuracy degradation or model
compression by proposing a novel integer programming formulation. Finally, we
suggest model global statistics tuning, to correct biases introduced during
quantization. Together, these methods yield state-of-the-art results for both
vision and text models. For instance, on ResNet50, we obtain less than 1\%
accuracy degradation --- with 4-bit weights and activations in all layers, but
the smallest two. We open-sourced our code.
</p>
<a href="http://arxiv.org/abs/2006.10518" target="_blank">arXiv:2006.10518</a> [<a href="http://arxiv.org/pdf/2006.10518" target="_blank">pdf</a>]

<h2>Precise expressions for random projections: Low-rank approximation and randomized Newton. (arXiv:2006.10653v2 [cs.LG] UPDATED)</h2>
<h3>Micha&#x142; Derezi&#x144;ski, Feynman Liang, Zhenyu Liao, Michael W. Mahoney</h3>
<p>It is often desirable to reduce the dimensionality of a large dataset by
projecting it onto a low-dimensional subspace. Matrix sketching has emerged as
a powerful technique for performing such dimensionality reduction very
efficiently. Even though there is an extensive literature on the worst-case
performance of sketching, existing guarantees are typically very different from
what is observed in practice. We exploit recent developments in the spectral
analysis of random matrices to develop novel techniques that provide provably
accurate expressions for the expected value of random projection matrices
obtained via sketching. These expressions can be used to characterize the
performance of dimensionality reduction in a variety of common machine learning
tasks, ranging from low-rank approximation to iterative stochastic
optimization. Our results apply to several popular sketching methods, including
Gaussian and Rademacher sketches, and they enable precise analysis of these
methods in terms of spectral properties of the data. Empirical results show
that the expressions we derive reflect the practical performance of these
sketching methods, down to lower-order effects and even constant factors.
</p>
<a href="http://arxiv.org/abs/2006.10653" target="_blank">arXiv:2006.10653</a> [<a href="http://arxiv.org/pdf/2006.10653" target="_blank">pdf</a>]

<h2>Automatic Recall Machines: Internal Replay, Continual Learning and the Brain. (arXiv:2006.12323v3 [cs.LG] UPDATED)</h2>
<h3>Xu Ji, Joao Henriques, Tinne Tuytelaars, Andrea Vedaldi</h3>
<p>Replay in neural networks involves training on sequential data with memorized
samples, which counteracts forgetting of previous behavior caused by
non-stationarity. We present a method where these auxiliary samples are
generated on the fly, given only the model that is being trained for the
assessed objective, without extraneous buffers or generator networks. Instead
the implicit memory of learned samples within the assessed model itself is
exploited. Furthermore, whereas existing work focuses on reinforcing the full
seen data distribution, we show that optimizing for not forgetting calls for
the generation of samples that are specialized to each real training batch,
which is more efficient and scalable. We consider high-level parallels with the
brain, notably the use of a single model for inference and recall, the
dependency of recalled samples on the current environment batch, top-down
modulation of activations and learning, abstract recall, and the dependency
between the degree to which a task is learned and the degree to which it is
recalled. These characteristics emerge naturally from the method without being
controlled for.
</p>
<a href="http://arxiv.org/abs/2006.12323" target="_blank">arXiv:2006.12323</a> [<a href="http://arxiv.org/pdf/2006.12323" target="_blank">pdf</a>]

<h2>On Multivariate Singular Spectrum Analysis. (arXiv:2006.13448v2 [cs.LG] UPDATED)</h2>
<h3>Anish Agarwal, Abdullah Alomar, Devavrat Shah</h3>
<p>We analyze a variant of multivariate singular spectrum analysis (mSSA), a
widely used method used to impute and forecast a multivariate time series. Its
restriction to a single time series, known as singular spectrum analysis (SSA),
has been analyzed recently. Despite its popularity, theoretical understanding
of mSSA is absent. Towards this we introduce a spatio-temporal factor model to
analyze mSSA. We establish the in-sample prediction error for both imputation
and forecasting scales as $1/\sqrt{NT}$, for $N$ time series with $T$
observations per time series. In contrast, for SSA the error scales as
$1/\sqrt{T}$ and for popular matrix factorization based time series methods,
the error scales as ${1}/{\min(N, T)}$ -- we note these previous results are
established only for imputation. Further, we utilize an online learning
framework to analyze the one-step-ahead prediction error of mSSA and establish
it has a regret of ${1}/{(\sqrt{N}T^{0.04})}$ with respect to in-sample
forecasting error. Empirically, we find mSSA outperforms neural network based
methods, LSTM and DeepAR, two of the most widely used and empirically effective
methods, though they come with no theoretical guarantees. To establish our
results, we make three technical contributions. First, we show that the stacked
Page Matrix representation has an approximate low-rank structure for a large
class of time series models -- in doing so, we introduce a `calculus' for
approximate low-rank models. In particular, we establish that such models are
closed under linear combinations as well as multiplications. Second, to
establish our regret bounds, we extend the theory of online convex optimization
to when the constraints are time-varying, a variant not addressed by the
current literature. Third, we extend the prediction error analysis of Principle
Component Regression to when the covariate matrix is approximately low-rank.
</p>
<a href="http://arxiv.org/abs/2006.13448" target="_blank">arXiv:2006.13448</a> [<a href="http://arxiv.org/pdf/2006.13448" target="_blank">pdf</a>]

<h2>A Limitation of the PAC-Bayes Framework. (arXiv:2006.13508v2 [cs.LG] UPDATED)</h2>
<h3>Roi Livni, Shay Moran</h3>
<p>PAC-Bayes is a useful framework for deriving generalization bounds which was
introduced by McAllester ('98). This framework has the flexibility of deriving
distribution- and algorithm-dependent bounds, which are often tighter than
VC-related uniform convergence bounds. In this manuscript we present a
limitation for the PAC-Bayes framework. We demonstrate an easy learning task
that is not amenable to a PAC-Bayes analysis.

Specifically, we consider the task of linear classification in 1D; it is
well-known that this task is learnable using just $O(\log(1/\delta)/\epsilon)$
examples. On the other hand, we show that this fact can not be proved using a
PAC-Bayes analysis: for any algorithm that learns 1-dimensional linear
classifiers there exists a (realizable) distribution for which the PAC-Bayes
bound is arbitrarily large.
</p>
<a href="http://arxiv.org/abs/2006.13508" target="_blank">arXiv:2006.13508</a> [<a href="http://arxiv.org/pdf/2006.13508" target="_blank">pdf</a>]

<h2>The Gaussian equivalence of generative models for learning with shallow neural networks. (arXiv:2006.14709v2 [stat.ML] UPDATED)</h2>
<h3>Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M&#xe9;zard, Lenka Zdeborov&#xe1;</h3>
<p>Understanding the impact of data structure on the computational tractability
of learning is a key challenge for the theory of neural networks. Many
theoretical works do not explicitly model training data, or assume that inputs
are drawn component-wise independently from some simple probability
distribution. Here, we go beyond this simple paradigm by studying the
performance of neural networks trained on data drawn from pre-trained
generative models. This is possible due to a Gaussian equivalence stating that
the key metrics of interest, such as the training and test errors, can be fully
captured by an appropriately chosen Gaussian model. We provide three strands of
rigorous, analytical and numerical evidence corroborating this equivalence.
First, we establish rigorous conditions for the Gaussian equivalence to hold in
the case of single-layer generative models, as well as deterministic rates for
convergence in distribution. Second, we leverage this equivalence to derive a
closed set of equations describing the generalisation performance of two widely
studied machine learning problems: two-layer neural networks trained using
one-pass stochastic gradient descent, and full-batch pre-learned features or
kernel methods. Finally, we perform experiments demonstrating how our theory
applies to deep, pre-trained generative models. These results open a viable
path to the theoretical study of machine learning models with realistic data.
</p>
<a href="http://arxiv.org/abs/2006.14709" target="_blank">arXiv:2006.14709</a> [<a href="http://arxiv.org/pdf/2006.14709" target="_blank">pdf</a>]

<h2>Using Reinforcement Learning to Herd a Robotic Swarm to a Target Distribution. (arXiv:2006.15807v2 [cs.RO] UPDATED)</h2>
<h3>Zahi M. Kakish, Karthik Elamvazhuthi, Spring Berman</h3>
<p>In this paper, we present a reinforcement learning approach to designing a
control policy for a "leader" agent that herds a swarm of "follower" agents,
via repulsive interactions, as quickly as possible to a target probability
distribution over a strongly connected graph. The leader control policy is a
function of the swarm distribution, which evolves over time according to a
mean-field model in the form of an ordinary difference equation. The dependence
of the policy on agent populations at each graph vertex, rather than on
individual agent activity, simplifies the observations required by the leader
and enables the control strategy to scale with the number of agents. Two
Temporal-Difference learning algorithms, SARSA and Q-Learning, are used to
generate the leader control policy based on the follower agent distribution and
the leader's location on the graph. A simulation environment corresponding to a
grid graph with 4 vertices was used to train and validate the control policies
for follower agent populations ranging from 10 to 100. Finally, the control
policies trained on 100 simulated agents were used to successfully redistribute
a physical swarm of 10 small robots to a target distribution among 4 spatial
regions.
</p>
<a href="http://arxiv.org/abs/2006.15807" target="_blank">arXiv:2006.15807</a> [<a href="http://arxiv.org/pdf/2006.15807" target="_blank">pdf</a>]

<h2>DocVQA: A Dataset for VQA on Document Images. (arXiv:2007.00398v2 [cs.CV] UPDATED)</h2>
<h3>Minesh Mathew, Dimosthenis Karatzas, C.V. Jawahar</h3>
<p>We present a new dataset for Visual Question Answering (VQA) on document
images called DocVQA. The dataset consists of 50,000 questions defined on
12,000+ document images. Detailed analysis of the dataset in comparison with
similar datasets for VQA and reading comprehension is presented. We report
several baseline results by adopting existing VQA and reading comprehension
models. Although the existing models perform reasonably well on certain types
of questions, there is large performance gap compared to human performance
(94.36% accuracy). The models need to improve specifically on questions where
understanding structure of the document is crucial. The dataset, code and
leaderboard are available at this http URL
</p>
<a href="http://arxiv.org/abs/2007.00398" target="_blank">arXiv:2007.00398</a> [<a href="http://arxiv.org/pdf/2007.00398" target="_blank">pdf</a>]

<h2>Swapping Autoencoder for Deep Image Manipulation. (arXiv:2007.00653v2 [cs.CV] UPDATED)</h2>
<h3>Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, Richard Zhang</h3>
<p>Deep generative models have become increasingly effective at producing
realistic images from randomly sampled seeds, but using such models for
controllable manipulation of existing images remains challenging. We propose
the Swapping Autoencoder, a deep model designed specifically for image
manipulation, rather than random sampling. The key idea is to encode an image
with two independent components and enforce that any swapped combination maps
to a realistic image. In particular, we encourage the components to represent
structure and texture, by enforcing one component to encode co-occurrent patch
statistics across different parts of an image. As our method is trained with an
encoder, finding the latent codes for a new input image becomes trivial, rather
than cumbersome. As a result, it can be used to manipulate real input images in
various ways, including texture swapping, local and global editing, and latent
code vector arithmetic. Experiments on multiple datasets show that our model
produces better results and is substantially more efficient compared to recent
generative models.
</p>
<a href="http://arxiv.org/abs/2007.00653" target="_blank">arXiv:2007.00653</a> [<a href="http://arxiv.org/pdf/2007.00653" target="_blank">pdf</a>]

<h2>Progressive Domain Adaptation from a Source Pre-trained Model. (arXiv:2007.01524v3 [cs.CV] UPDATED)</h2>
<h3>Youngeun Kim, Donghyeon Cho, Priyadarshini Panda, Sungeun Hong</h3>
<p>Domain adaptation assumes that samples from source and target domains are
freely accessible during a training phase. However, such an assumption is
rarely plausible in the real-world and possibly causes data-privacy issues,
especially when the label of the source domain can be a sensitive attribute as
an identifier. To avoid accessing source data that may contain sensitive
information, we introduce progressive domain adaptation (PrDA). Our key idea is
to leverage a pre-trained model from the source domain and progressively update
the target model in a self-learning manner. We observe that target samples with
lower self-entropy measured by the pre-trained source model are more likely to
be classified correctly. From this, we select the reliable samples with the
self-entropy criterion and define these as class prototypes. We then assign
pseudo labels for every target sample based on the similarity score with class
prototypes. Furthermore, to reduce the uncertainty from the pseudo labeling
process, we propose set-to-set distance-based filtering which does not require
any tunable hyperparameters. Finally, we train the target model with the
filtered pseudo labels with regularization from the pre-trained source model.
Surprisingly, without direct usage of labeled source samples, our PrDA
outperforms conventional domain adaptation methods on benchmark datasets. Our
code is publicly available at
https://github.com/youngryan1993/PrDA-Progressive-Domain-Adaptation-from-a-Source-Pre-trained-Model.
</p>
<a href="http://arxiv.org/abs/2007.01524" target="_blank">arXiv:2007.01524</a> [<a href="http://arxiv.org/pdf/2007.01524" target="_blank">pdf</a>]

<h2>Automatic Target Recognition on Synthetic Aperture Radar Imagery: A Survey. (arXiv:2007.02106v2 [cs.CV] UPDATED)</h2>
<h3>O. Kechagias-Stamatis, N. Aouf</h3>
<p>Automatic Target Recognition (ATR) for military applications is one of the
core processes towards enhancing intelligencer and autonomously operating
military platforms. Spurred by this and given that Synthetic Aperture Radar
(SAR) presents several advantages over its counterpart data domains, this paper
surveys and assesses current SAR ATR architectures that employ the most popular
dataset for the SAR domain, namely the Moving and Stationary Target Acquisition
and Recognition (MSTAR) dataset. Based on the current methodology trends, we
propose a taxonomy for the SAR ATR architectures, along with a direct
comparison of the strengths and weaknesses of each method under both standard
and extended operational conditions. Additionally, despite MSTAR being the
standard SAR ATR benchmarking dataset we also highlight its weaknesses and
suggest future research directions.
</p>
<a href="http://arxiv.org/abs/2007.02106" target="_blank">arXiv:2007.02106</a> [<a href="http://arxiv.org/pdf/2007.02106" target="_blank">pdf</a>]

<h2>Domain Adaptation with Auxiliary Target Domain-Oriented Classifier. (arXiv:2007.04171v2 [cs.CV] UPDATED)</h2>
<h3>Jian Liang, Dapeng Hu, Jiashi Feng</h3>
<p>Domain adaptation (DA) aims to transfer knowledge from a label-rich but
heterogeneous domain to a label-scare domain, which alleviates the labeling
efforts and attracts considerable attention. Different from previous methods
focusing on learning domain-invariant feature representations, some recent
methods present generic semi-supervised learning (SSL) techniques and directly
apply them to DA tasks, even achieving competitive performance. One of the most
popular SSL techniques is pseudo-labeling that assigns pseudo labels for each
unlabeled data via the classifier trained by labeled data. However, it ignores
the distribution shift in DA problems and is inevitably biased to source data.
To address this issue, we propose a new pseudo-labeling framework called
Auxiliary Target Domain-Oriented Classifier (ATDOC). ATDOC alleviates the
classifier bias by introducing an auxiliary classifier for target data only, to
improve the quality of pseudo labels. Specifically, we employ the memory
mechanism and develop two types of non-parametric classifiers, i.e. the nearest
centroid classifier and neighborhood aggregation, without introducing any
additional network parameters. Despite its simplicity in a pseudo
classification objective, ATDOC with neighborhood aggregation significantly
outperforms domain alignment techniques and prior SSL techniques on a large
variety of DA benchmarks and even scare-labeled SSL tasks.
</p>
<a href="http://arxiv.org/abs/2007.04171" target="_blank">arXiv:2007.04171</a> [<a href="http://arxiv.org/pdf/2007.04171" target="_blank">pdf</a>]

<h2>Seeing eye-to-eye? A comparison of object recognition performance in humans and deep convolutional neural networks under image manipulation. (arXiv:2007.06294v2 [cs.CV] UPDATED)</h2>
<h3>Leonard E. van Dyck, Walter R. Gruber</h3>
<p>For a considerable time, deep convolutional neural networks (DCNNs) have
reached human benchmark performance in object recognition. On that account,
computational neuroscience and the field of machine learning have started to
attribute numerous similarities and differences to artificial and biological
vision. This study aims towards a behavioral comparison of visual core object
recognition performance between humans and feedforward neural networks in a
classification learning paradigm on an ImageNet data set. For this purpose,
human participants (n = 65) competed in an online experiment against different
feedforward DCNNs. The designed approach based on a typical learning process of
seven different monkey categories included a training and validation phase with
natural examples, as well as a testing phase with novel, unexperienced shape
and color manipulations. Analyses of accuracy revealed that humans not only
outperform DCNNs on all conditions, but also display significantly greater
robustness towards shape and most notably color alterations. Furthermore, a
precise examination of behavioral patterns highlights these findings by
revealing independent classification errors between the groups. The obtained
results show that humans contrast strongly with artificial feedforward
architectures when it comes to visual core object recognition of manipulated
images. In general, these findings are in line with a growing body of
literature, that hints towards recurrence as a crucial factor for adequate
generalization abilities.
</p>
<a href="http://arxiv.org/abs/2007.06294" target="_blank">arXiv:2007.06294</a> [<a href="http://arxiv.org/pdf/2007.06294" target="_blank">pdf</a>]

<h2>Self-Supervised Representation Learning for Detection of ACL Tear Injury in Knee MR Videos. (arXiv:2007.07761v3 [cs.CV] UPDATED)</h2>
<h3>Siladittya Manna, Saumik Bhattacharya, Umapada Pal</h3>
<p>The success of deep learning based models for computer vision applications
requires large scale human annotated data which are often expensive to
generate. Self-supervised learning, a subset of unsupervised learning, handles
this problem by learning meaningful features from unlabeled image or video
data. In this paper, we propose a self-supervised learning approach to learn
transferable features from MR video clips by enforcing the model to learn
anatomical features. The pretext task models are designed to predict the
correct ordering of the jumbled image patches that the MR video frames are
divided into. To the best of our knowledge, none of the supervised learning
models performing injury classification task from MR video provide any
explanation for the decisions made by the models and hence makes our work the
first of its kind on MR video data. Experiments on the pretext task show that
this proposed approach enables the model to learn spatial context invariant
features which help for reliable and explainable performance in downstream
tasks like classification of Anterior Cruciate Ligament tear injury from knee
MRI. The efficiency of the novel Convolutional Neural Network proposed in this
paper is reflected in the experimental results obtained in the downstream task.
</p>
<a href="http://arxiv.org/abs/2007.07761" target="_blank">arXiv:2007.07761</a> [<a href="http://arxiv.org/pdf/2007.07761" target="_blank">pdf</a>]

<h2>Collaborative Learning as an Agreement Problem. (arXiv:2008.00742v3 [cs.LG] UPDATED)</h2>
<h3>El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, L&#xea; Nguy&#xea;n Hoang, S&#xe9;bastien Rouault</h3>
<p>We address the problem of Byzantine collaborative learning: a set of $n$
nodes seek to collectively learn from each others' local data. The data
distribution may vary from one node to another. No node is trusted and $f &lt; n$
nodes can behave arbitrarily, i.e., they can be Byzantine. We prove that
collaborative learning is equivalent to a new and weak form of agreement, which
we call averaging agreement. In this problem, nodes start each with an initial
vector and seek to approximately agree on a common vector, which is close to
the average of honest nodes' initial vectors. More precisely, the "error" must
remain within a multiplicative constant (which we call averaging constant) of
the maximum $\ell_2$ distance between the honest nodes' initial vectors.
Essentially, the smaller the averaging constant, the better the learning.

We present two asynchronous solutions to averaging agreement, each we prove
optimal according to some dimension. The first, based on the minimum-diameter
averaging, requires $ n \geq 6f+1$, but achieves asymptotically the
best-possible averaging constant up to a multiplicative constant. The second,
based on reliable broadcast and coordinate-wise trimmed mean, achieves optimal
Byzantine resilience, i.e., $n \geq 3f+1$. Each of these algorithms induces an
optimal collaborative learning protocol.
</p>
<a href="http://arxiv.org/abs/2008.00742" target="_blank">arXiv:2008.00742</a> [<a href="http://arxiv.org/pdf/2008.00742" target="_blank">pdf</a>]

<h2>IntelligentPooling: Practical Thompson Sampling for mHealth. (arXiv:2008.01571v2 [cs.LG] UPDATED)</h2>
<h3>Sabina Tomkins, Peng Liao, Predrag Klasnja, Susan Murphy</h3>
<p>In mobile health (mHealth) smart devices deliver behavioral treatments
repeatedly over time to a user with the goal of helping the user adopt and
maintain healthy behaviors. Reinforcement learning appears ideal for learning
how to optimally make these sequential treatment decisions. However,
significant challenges must be overcome before reinforcement learning can be
effectively deployed in a mobile healthcare setting. In this work we are
concerned with the following challenges: 1) individuals who are in the same
context can exhibit differential response to treatments 2) only a limited
amount of data is available for learning on any one individual, and 3)
non-stationary responses to treatment. To address these challenges we
generalize Thompson-Sampling bandit algorithms to develop IntelligentPooling.
IntelligentPooling learns personalized treatment policies thus addressing
challenge one. To address the second challenge, IntelligentPooling updates each
user's degree of personalization while making use of available data on other
users to speed up learning. Lastly, IntelligentPooling allows responsivity to
vary as a function of a user's time since beginning treatment, thus addressing
challenge three. We show that IntelligentPooling achieves an average of 26%
lower regret than state-of-the-art. We demonstrate the promise of this approach
and its ability to learn from even a small group of users in a live clinical
trial.
</p>
<a href="http://arxiv.org/abs/2008.01571" target="_blank">arXiv:2008.01571</a> [<a href="http://arxiv.org/pdf/2008.01571" target="_blank">pdf</a>]

<h2>Reproducible Pruning System on Dynamic Natural Plants for Field Agricultural Robots. (arXiv:2008.11613v2 [cs.RO] UPDATED)</h2>
<h3>Sunny Katyara, Fanny Ficuciello, Darwin G. Caldwell, Fei Chen, Bruno Siciliano</h3>
<p>Pruning is the art of cutting unwanted and unhealthy plant branches and is
one of the difficult tasks in the field robotics. It becomes even more complex
when the plant branches are moving. Moreover, the reproducibility of robot
pruning skills is another challenge to deal with due to the heterogeneous
nature of vines in the vineyard. This research proposes a multi-modal framework
to deal with the dynamic vines with the aim of sim2real skill transfer. The 3D
models of vines are constructed in blender engine and rendered in simulated
environment as a need for training the robot. The Natural Admittance Controller
(NAC) is applied to deal with the dynamics of vines. It uses force feedback and
compensates the friction effects while maintaining the passivity of system. The
faster R-CNN is used to detect the spurs on the vines and then statistical
pattern recognition algorithm using K-means clustering is applied to find the
effective pruning points. The proposed framework is tested in simulated and
real environments.
</p>
<a href="http://arxiv.org/abs/2008.11613" target="_blank">arXiv:2008.11613</a> [<a href="http://arxiv.org/pdf/2008.11613" target="_blank">pdf</a>]

<h2>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding. (arXiv:2009.01449v2 [cs.CV] UPDATED)</h2>
<h3>Long Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, Shih-Fu Chang</h3>
<p>The prevailing framework for solving referring expression grounding is based
on a two-stage process: 1) detecting proposals with an object detector and 2)
grounding the referent to one of the proposals. Existing two-stage solutions
mostly focus on the grounding step, which aims to align the expressions with
the proposals. In this paper, we argue that these methods overlook an obvious
mismatch between the roles of proposals in the two stages: they generate
proposals solely based on the detection confidence (i.e., expression-agnostic),
hoping that the proposals contain all right instances in the expression (i.e.,
expression-aware). Due to this mismatch, current two-stage methods suffer from
a severe performance drop between detected and ground-truth proposals. To this
end, we propose Ref-NMS, which is the first method to yield expression-aware
proposals at the first stage. Ref-NMS regards all nouns in the expression as
critical objects, and introduces a lightweight module to predict a score for
aligning each box with a critical object. These scores can guide the NMS
operation to filter out the boxes irrelevant to the expression, increasing the
recall of critical objects, resulting in a significantly improved grounding
performance. Since Ref- NMS is agnostic to the grounding step, it can be easily
integrated into any state-of-the-art two-stage method. Extensive ablation
studies on several backbones, benchmarks, and tasks consistently demonstrate
the superiority of Ref-NMS. Codes are available at:
https://github.com/ChopinSharp/ref-nms.
</p>
<a href="http://arxiv.org/abs/2009.01449" target="_blank">arXiv:2009.01449</a> [<a href="http://arxiv.org/pdf/2009.01449" target="_blank">pdf</a>]

<h2>SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition. (arXiv:2009.06138v3 [cs.CV] UPDATED)</h2>
<h3>Liangzhi Li, Bowen Wang, Manisha Verma, Yuta Nakashima, Ryo Kawasaki, Hajime Nagahara</h3>
<p>Explainable artificial intelligence has been gaining attention in the past
few years. However, most existing methods are based on gradients or
intermediate features, which are not directly involved in the decision-making
process of the classifier. In this paper, we propose a slot attention-based
classifier called SCOUTER for transparent yet accurate classification. Two
major differences from other attention-based methods include: (a) SCOUTER's
explanation is involved in the final confidence for each category, offering
more intuitive interpretation, and (b) all the categories have their
corresponding positive or negative explanation, which tells "why the image is
of a certain category" or "why the image is not of a certain category." We
design a new loss tailored for SCOUTER that controls the model's behavior to
switch between positive and negative explanations, as well as the size of
explanatory regions. Experimental results show that SCOUTER can give better
visual explanations while keeping good accuracy on small and medium-sized
datasets.
</p>
<a href="http://arxiv.org/abs/2009.06138" target="_blank">arXiv:2009.06138</a> [<a href="http://arxiv.org/pdf/2009.06138" target="_blank">pdf</a>]

<h2>Variational Disentanglement for Rare Event Modeling. (arXiv:2009.08541v3 [stat.ML] UPDATED)</h2>
<h3>Zidi Xiu, Chenyang Tao, Michael Gao, Connor Davis, Benjamin A. Goldstein, Ricardo Henao</h3>
<p>Combining the increasing availability and abundance of healthcare data and
the current advances in machine learning methods have created renewed
opportunities to improve clinical decision support systems. However, in
healthcare risk prediction applications, the proportion of cases with the
condition (label) of interest is often very low relative to the available
sample size. Though very prevalent in healthcare, such imbalanced
classification settings are also common and challenging in many other
scenarios. So motivated, we propose a variational disentanglement approach to
semi-parametrically learn from rare events in heavily imbalanced classification
problems. Specifically, we leverage the imposed extreme-distribution behavior
on a latent space to extract information from low-prevalence events, and
develop a robust prediction arm that joins the merits of the generalized
additive model and isotonic neural nets. Results on synthetic studies and
diverse real-world datasets, including mortality prediction on a COVID-19
cohort, demonstrate that the proposed approach outperforms existing
alternatives.
</p>
<a href="http://arxiv.org/abs/2009.08541" target="_blank">arXiv:2009.08541</a> [<a href="http://arxiv.org/pdf/2009.08541" target="_blank">pdf</a>]

<h2>Deep N-ary Error Correcting Output Codes. (arXiv:2009.10465v3 [cs.CV] UPDATED)</h2>
<h3>Hao Zhang, Joey Tianyi Zhou, Tianying Wang, Ivor W. Tsang, Rick Siow Mong Goh</h3>
<p>Ensemble learning consistently improves the performance of multi-class
classification through aggregating a series of base classifiers. To this end,
data-independent ensemble methods like Error Correcting Output Codes (ECOC)
attract increasing attention due to its easiness of implementation and
parallelization. Specifically, traditional ECOCs and its general extension
N-ary ECOC decompose the original multi-class classification problem into a
series of independent simpler classification subproblems. Unfortunately,
integrating ECOCs, especially N-ary ECOC with deep neural networks, termed as
deep N-ary ECOC, is not straightforward and yet fully exploited in the
literature, due to the high expense of training base learners. To facilitate
the training of N-ary ECOC with deep learning base learners, we further propose
three different variants of parameter sharing architectures for deep N-ary
ECOC. To verify the generalization ability of deep N-ary ECOC, we conduct
experiments by varying the backbone with different deep neural network
architectures for both image and text classification tasks. Furthermore,
extensive ablation studies on deep N-ary ECOC show its superior performance
over other deep data-independent ensemble methods.
</p>
<a href="http://arxiv.org/abs/2009.10465" target="_blank">arXiv:2009.10465</a> [<a href="http://arxiv.org/pdf/2009.10465" target="_blank">pdf</a>]

<h2>DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation. (arXiv:2010.00400v3 [cs.CV] UPDATED)</h2>
<h3>Javier Hernandez-Ortega, Ruben Tolosana, Julian Fierrez, Aythami Morales</h3>
<p>This work introduces a novel DeepFake detection framework based on
physiological measurement. In particular, we consider information related to
the heart rate using remote photoplethysmography (rPPG). rPPG methods analyze
video sequences looking for subtle color changes in the human skin, revealing
the presence of human blood under the tissues. In this work we investigate to
what extent rPPG is useful for the detection of DeepFake videos.

The proposed fake detector named DeepFakesON-Phys uses a Convolutional
Attention Network (CAN), which extracts spatial and temporal information from
video frames, analyzing and combining both sources to better detect fake
videos. This detection approach has been experimentally evaluated using the
latest public databases in the field: Celeb-DF and DFDC. The results achieved,
above 98% AUC (Area Under the Curve) on both databases, outperform the state of
the art and prove the success of fake detectors based on physiological
measurement to detect the latest DeepFake videos.
</p>
<a href="http://arxiv.org/abs/2010.00400" target="_blank">arXiv:2010.00400</a> [<a href="http://arxiv.org/pdf/2010.00400" target="_blank">pdf</a>]

<h2>MIMOSA: Multi-constraint Molecule Sampling for Molecule Optimization. (arXiv:2010.02318v2 [cs.LG] UPDATED)</h2>
<h3>Tianfan Fu, Cao Xiao, Xinhao Li, Lucas M. Glass, Jimeng Sun</h3>
<p>Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate.
</p>
<a href="http://arxiv.org/abs/2010.02318" target="_blank">arXiv:2010.02318</a> [<a href="http://arxiv.org/pdf/2010.02318" target="_blank">pdf</a>]

<h2>Accelerating Simulation of Stiff Nonlinear Systems using Continuous-Time Echo State Networks. (arXiv:2010.04004v5 [cs.LG] UPDATED)</h2>
<h3>Ranjan Anantharaman, Yingbo Ma, Shashi Gowda, Chris Laughman, Viral Shah, Alan Edelman, Chris Rackauckas</h3>
<p>Modern design, control, and optimization often requires simulation of highly
nonlinear models, leading to prohibitive computational costs. These costs can
be amortized by evaluating a cheap surrogate of the full model. Here we present
a general data-driven method, the continuous-time echo state network (CTESN),
for generating surrogates of nonlinear ordinary differential equations with
dynamics at widely separated timescales. We empirically demonstrate
near-constant time performance using our CTESNs on a physically motivated
scalable model of a heating system whose full execution time increases
exponentially, while maintaining relative error of within 0.2 %. We also show
that our model captures fast transients as well as slow dynamics effectively,
while other techniques such as physics informed neural networks have
difficulties trying to train and predict the highly nonlinear behavior of these
models.
</p>
<a href="http://arxiv.org/abs/2010.04004" target="_blank">arXiv:2010.04004</a> [<a href="http://arxiv.org/pdf/2010.04004" target="_blank">pdf</a>]

<h2>Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms. (arXiv:2010.05273v2 [cs.LG] UPDATED)</h2>
<h3>Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, Afshin Rostamizadeh</h3>
<p>Federated learning is typically approached as an optimization problem, where
the goal is to minimize a global loss function by distributing computation
across client devices that possess local data and specify different parts of
the global objective. We present an alternative perspective and formulate
federated learning as a posterior inference problem, where the goal is to infer
a global posterior distribution by having client devices each infer the
posterior of their local data. While exact inference is often intractable, this
perspective provides a principled way to search for global optima in federated
settings. Further, starting with the analysis of federated quadratic
objectives, we develop a computation- and communication-efficient approximate
posterior inference algorithm -- federated posterior averaging (FedPA). Our
algorithm uses MCMC for approximate inference of local posteriors on the
clients and efficiently communicates their statistics to the server, where the
latter uses them to refine a global estimate of the posterior mode. Finally, we
show that FedPA generalizes federated averaging (FedAvg), can similarly benefit
from adaptive optimizers, and yields state-of-the-art results on four realistic
and challenging benchmarks, converging faster, to better optima.
</p>
<a href="http://arxiv.org/abs/2010.05273" target="_blank">arXiv:2010.05273</a> [<a href="http://arxiv.org/pdf/2010.05273" target="_blank">pdf</a>]

<h2>Oort: Informed Participant Selection for Scalable Federated Learning. (arXiv:2010.06081v2 [cs.LG] UPDATED)</h2>
<h3>Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, Mosharaf Chowdhury</h3>
<p>Federated Learning (FL) is an emerging direction in distributed machine
learning (ML) that enables in-situ model training and testing on edge data.
Despite having the same end goals as traditional ML, FL executions differ
significantly in scale, spanning thousands to millions of participating
devices. As a result, data characteristics and device capabilities vary widely
across clients. Yet, existing efforts randomly select FL participants, which
leads to poor model and system efficiency.

In this paper, we propose Kuiper to improve the performance of federated
training and testing with guided participant selection. With an aim to improve
time-to-accuracy performance in model training, Kuiper prioritizes the use of
those clients who have both data that offers the greatest utility in improving
model accuracy and the capability to run training quickly. To enable FL
developers to interpret their results in model testing, Kuiper enforces their
requirements on the distribution of participant data while improving the
duration of federated testing by cherry-picking clients. Our evaluation shows
that, compared to existing participant selection mechanisms, Kuiper improves
time-to-accuracy performance by 1.2x-14.1x and final model accuracy by
1.3%-9.8%, while efficiently enforcing developer-specified model testing
criteria at the scale of millions of clients.
</p>
<a href="http://arxiv.org/abs/2010.06081" target="_blank">arXiv:2010.06081</a> [<a href="http://arxiv.org/pdf/2010.06081" target="_blank">pdf</a>]

<h2>Rapid Robust Principal Component Analysis: CUR Accelerated Inexact Low Rank Estimation. (arXiv:2010.07422v2 [stat.ML] UPDATED)</h2>
<h3>HanQin Cai, Keaton Hamm, Longxiu Huang, Jiaqi Li, Tao Wang</h3>
<p>Robust principal component analysis (RPCA) is a widely used tool for
dimension reduction. In this work, we propose a novel non-convex algorithm,
coined Iterated Robust CUR (IRCUR), for solving RPCA problems, which
dramatically improves the computational efficiency in comparison with the
existing algorithms. IRCUR achieves this acceleration by employing CUR
decomposition when updating the low rank component, which allows us to obtain
an accurate low rank approximation via only three small submatrices.
Consequently, IRCUR is able to process only the small submatrices and avoid
expensive computing on the full matrix through the entire algorithm. Numerical
experiments establish the computational advantage of IRCUR over the
state-of-art algorithms on both synthetic and real-world datasets.
</p>
<a href="http://arxiv.org/abs/2010.07422" target="_blank">arXiv:2010.07422</a> [<a href="http://arxiv.org/pdf/2010.07422" target="_blank">pdf</a>]

<h2>Semantics of the Black-Box: Can knowledge graphs help make deep learning systems more interpretable and explainable?. (arXiv:2010.08660v4 [cs.AI] UPDATED)</h2>
<h3>Manas Gaur, Keyur Faldu, Amit Sheth</h3>
<p>The recent series of innovations in deep learning (DL) have shown enormous
potential to impact individuals and society, both positively and negatively.
The DL models utilizing massive computing power and enormous datasets have
significantly outperformed prior historical benchmarks on increasingly
difficult, well-defined research tasks across technology domains such as
computer vision, natural language processing, signal processing, and
human-computer interactions. However, the Black-Box nature of DL models and
their over-reliance on massive amounts of data condensed into labels and dense
representations poses challenges for interpretability and explainability of the
system. Furthermore, DLs have not yet been proven in their ability to
effectively utilize relevant domain knowledge and experience critical to human
understanding. This aspect is missing in early data-focused approaches and
necessitated knowledge-infused learning and other strategies to incorporate
computational knowledge. This article demonstrates how knowledge, provided as a
knowledge graph, is incorporated into DL methods using knowledge-infused
learning, which is one of the strategies. We then discuss how this makes a
fundamental difference in the interpretability and explainability of current
approaches, and illustrate it with examples from natural language processing
for healthcare and education applications.
</p>
<a href="http://arxiv.org/abs/2010.08660" target="_blank">arXiv:2010.08660</a> [<a href="http://arxiv.org/pdf/2010.08660" target="_blank">pdf</a>]

<h2>SWIPENET: Object detection in noisy underwater images. (arXiv:2010.10006v2 [cs.CV] UPDATED)</h2>
<h3>Long Chen, Feixiang Zhou, Shengke Wang, Junyu Dong, Ning Li, Haiping Ma, Xin Wang, Huiyu Zhou</h3>
<p>In recent years, deep learning based object detection methods have achieved
promising performance in controlled environments. However, these methods lack
sufficient capabilities to handle underwater object detection due to these
challenges: (1) images in the underwater datasets and real applications are
blurry whilst accompanying severe noise that confuses the detectors and (2)
objects in real applications are usually small. In this paper, we propose a
novel Sample-WeIghted hyPEr Network (SWIPENET), and a robust training paradigm
named Curriculum Multi-Class Adaboost (CMA), to address these two problems at
the same time. Firstly, the backbone of SWIPENET produces multiple high
resolution and semantic-rich Hyper Feature Maps, which significantly improve
small object detection. Secondly, a novel sample-weighted detection loss
function is designed for SWIPENET, which focuses on learning high weight
samples and ignore learning low weight samples. Moreover, inspired by the human
education process that drives the learning from easy to hard concepts, we here
propose the CMA training paradigm that first trains a clean detector which is
free from the influence of noisy data. Then, based on the clean detector,
multiple detectors focusing on learning diverse noisy data are trained and
incorporated into a unified deep ensemble of strong noise immunity. Experiments
on two underwater robot picking contest datasets (URPC2017 and URPC2018) show
that the proposed SWIPENET+CMA framework achieves better accuracy in object
detection against several state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2010.10006" target="_blank">arXiv:2010.10006</a> [<a href="http://arxiv.org/pdf/2010.10006" target="_blank">pdf</a>]

<h2>Two-Stage Generative Adversarial Networks for Document Image Binarization with Color Noise and Background Removal. (arXiv:2010.10103v2 [cs.CV] UPDATED)</h2>
<h3>Sungho Suh, Jihun Kim, Paul Lukowicz, Yong Oh Lee</h3>
<p>Document image enhancement and binarization methods are often used to improve
the accuracy and efficiency of document image analysis tasks such as text
recognition. Traditional non-machine-learning methods are constructed on
low-level features in an unsupervised manner but have difficulty with
binarization on documents with severely degraded backgrounds. Convolutional
neural network-based methods focus only on grayscale images and on local
textual features. In this paper, we propose a two-stage color document image
enhancement and binarization method using generative adversarial neural
networks. In the first stage, four color-independent adversarial networks are
trained to extract color foreground information from an input image for
document image enhancement. In the second stage, two independent adversarial
networks with global and local features are trained for image binarization of
documents of variable size. For the adversarial neural networks, we formulate
loss functions between a discriminator and generators having an encoder-decoder
structure. Experimental results show that the proposed method achieves better
performance than many classical and state-of-the-art algorithms over the
Document Image Binarization Contest (DIBCO) datasets, the LRDE Document
Binarization Dataset (LRDE DBD), and our shipping label image dataset.
</p>
<a href="http://arxiv.org/abs/2010.10103" target="_blank">arXiv:2010.10103</a> [<a href="http://arxiv.org/pdf/2010.10103" target="_blank">pdf</a>]

<h2>Stochastic analysis of heterogeneous porous material with modified neural architecture search (NAS) based physics-informed neural networks using transfer learning. (arXiv:2010.12344v2 [cs.LG] UPDATED)</h2>
<h3>Hongwei Guo, Xiaoying Zhuang, Timon Rabczuk</h3>
<p>In this work, a modified neural architecture search method (NAS) based
physics-informed deep learning model is presented for stochastic analysis in
heterogeneous porous material. Monte Carlo method based on a randomized
spectral representation is first employed to construct a stochastic model for
simulation of flow through porous media. To solve the governing equations for
stochastic groundwater flow problem, we build a modified NAS model based on
physics-informed neural networks (PINNs) with transfer learning in this paper
that will be able to fit different partial differential equations (PDEs) with
less calculation. The performance estimation strategies adopted is constructed
from an error estimation model using the method of manufactured solutions. A
sensitivity analysis is performed to obtain the prior knowledge of the PINNs
model and narrow down the range of parameters for search space and use
hyper-parameter optimization algorithms to further determine the values of the
parameters. Further the NAS based PINNs model also saves the weights and biases
of the most favorable architectures, then used in the fine-tuning process. It
is found that the log-conductivity field using Gaussian correlation function
will perform much better than exponential correlation case, which is more
fitted to the PINNs model and the modified neural architecture search based
PINNs model shows a great potential in approximating solutions to PDEs.
Moreover, a three dimensional stochastic flow model is built to provide a
benchmark to the simulation of groundwater flow in highly heterogeneous
aquifers. The NAS model based deep collocation method is verified to be
effective and accurate through numerical examples in different dimensions using
different manufactured solutions.
</p>
<a href="http://arxiv.org/abs/2010.12344" target="_blank">arXiv:2010.12344</a> [<a href="http://arxiv.org/pdf/2010.12344" target="_blank">pdf</a>]

<h2>Decentralized Attribution of Generative Models. (arXiv:2010.13974v2 [cs.CV] UPDATED)</h2>
<h3>Changhoon Kim, Yi Ren, Yezhou Yang</h3>
<p>Growing applications of generative models have led to new threats such as
malicious personation and digital copyright infringement. One solution to these
threats is model attribution, i.e., the identification of user-end models where
the contents under question are generated. Existing studies showed empirical
feasibility of attribution through a centralized classifier trained on all
existing user-end models. However, this approach is not scalable in a reality
where the number of models ever grows. Neither does it provide an
attributability guarantee. To this end, this paper studies decentralized
attribution, which relies on binary classifiers associated with each user-end
model. Each binary classifier is parameterized by a user-specific key and
distinguishes its associated model distribution from the authentic data
distribution. We develop sufficient conditions of the keys that guarantee an
attributability lower bound. Our method is validated on MNIST, CelebA, and FFHQ
datasets. We also examine the trade-off between generation quality and
robustness of attribution against adversarial post-processes. One-sentence
Summary: This paper investigates the feasibility of decentralized attribution
of generative models.
</p>
<a href="http://arxiv.org/abs/2010.13974" target="_blank">arXiv:2010.13974</a> [<a href="http://arxiv.org/pdf/2010.13974" target="_blank">pdf</a>]

<h2>Scientific intuition inspired by machine learning generated hypotheses. (arXiv:2010.14236v2 [cs.LG] UPDATED)</h2>
<h3>Pascal Friederich, Mario Krenn, Isaac Tamblyn, Alan Aspuru-Guzik</h3>
<p>Machine learning with application to questions in the physical sciences has
become a widely used tool, successfully applied to classification, regression
and optimization tasks in many areas. Research focus mostly lies in improving
the accuracy of the machine learning models in numerical predictions, while
scientific understanding is still almost exclusively generated by human
researchers analysing numerical results and drawing conclusions. In this work,
we shift the focus on the insights and the knowledge obtained by the machine
learning models themselves. In particular, we study how it can be extracted and
used to inspire human scientists to increase their intuitions and understanding
of natural systems. We apply gradient boosting in decision trees to extract
human interpretable insights from big data sets from chemistry and physics. In
chemistry, we not only rediscover widely know rules of thumb but also find new
interesting motifs that tell us how to control solubility and energy levels of
organic molecules. At the same time, in quantum physics, we gain new
understanding on experiments for quantum entanglement. The ability to go beyond
numerics and to enter the realm of scientific insight and hypothesis generation
opens the door to use machine learning to accelerate the discovery of
conceptual understanding in some of the most challenging domains of science.
</p>
<a href="http://arxiv.org/abs/2010.14236" target="_blank">arXiv:2010.14236</a> [<a href="http://arxiv.org/pdf/2010.14236" target="_blank">pdf</a>]

<h2>Towards Measuring Place Function Similarity at Fine Spatial Granularity with Trajectory Embedding. (arXiv:2011.00261v2 [cs.AI] UPDATED)</h2>
<h3>Cheng Fu, Robert Weibel</h3>
<p>Modeling place functions from a computational perspective is a prevalent
research topic. Trajectory embedding, as a neural-network-backed dimension
reduction technology, allows the possibility to put places with similar social
functions at close locations in the embedding space if the places share similar
chronological context as part of a trajectory. The embedding similarity was
previously proposed as a new metric for measuring the similarity of place
functions. This study explores if this approach is meaningful for geographical
units at a much smaller geographical granularity compared to previous studies.
In addition, this study investigates if the geographical distance can influence
the embedding similarity. The empirical evaluations based on a big vehicle
trajectory data set confirm that the embedding similarity can be a metric proxy
for place functions. However, the results also show that the embedding
similarity is still bounded by the distance at the local scale.
</p>
<a href="http://arxiv.org/abs/2011.00261" target="_blank">arXiv:2011.00261</a> [<a href="http://arxiv.org/pdf/2011.00261" target="_blank">pdf</a>]

<h2>Learning Causal Semantic Representation for Out-of-Distribution Prediction. (arXiv:2011.01681v2 [stat.ML] UPDATED)</h2>
<h3>Chang Liu, Xinwei Sun, Jindong Wang, Tao Li, Tao Qin, Wei Chen, Tie-Yan Liu</h3>
<p>Conventional supervised learning methods, especially deep ones, are found to
be sensitive to out-of-distribution (OOD) examples, largely because the learned
representation mixes the semantic factor with the variation factor due to their
domain-specific correlation, while only the semantic factor causes the output.
To address the problem, we propose a Causal Semantic Generative model (CSG)
based on a causal thought where the two factors are modeled separately, and
develop methods to learn it on a single training domain and predict in a test
domain without (OOD generalization) or with unsupervised data (domain
adaptation). We prove that under proper conditions CSG identifies the semantic
factor by fitting training data, and this semantic identification guarantees
the boundedness of OOD generalization error and the success of adaptation. The
methods and theory are based on the invariance principle of causal generative
mechanisms, which is more fundamental and general than inference invariance.
The methods come from a novel design for both efficient learning and easy
prediction, following the first principle of variational Bayes and the
graphical structure of CSG. Empirical study demonstrates the improved test
accuracy for OOD generalization and domain adaptation.
</p>
<a href="http://arxiv.org/abs/2011.01681" target="_blank">arXiv:2011.01681</a> [<a href="http://arxiv.org/pdf/2011.01681" target="_blank">pdf</a>]

<h2>Physics-Informed Neural Network Super Resolution for Advection-Diffusion Models. (arXiv:2011.02519v2 [cs.CV] UPDATED)</h2>
<h3>Chulin Wang, Eloisa Bentivegna, Wang Zhou, Levente Klein, Bruce Elmegreen</h3>
<p>Physics-informed neural networks (NN) are an emerging technique to improve
spatial resolution and enforce physical consistency of data from physics models
or satellite observations. A super-resolution (SR) technique is explored to
reconstruct high-resolution images ($4\times$) from lower resolution images in
an advection-diffusion model of atmospheric pollution plumes. SR performance is
generally increased when the advection-diffusion equation constrains the NN in
addition to conventional pixel-based constraints. The ability of SR techniques
to also reconstruct missing data is investigated by randomly removing image
pixels from the simulations and allowing the system to learn the content of
missing data. Improvements in S/N of $11\%$ are demonstrated when physics
equations are included in SR with $40\%$ pixel loss. Physics-informed NNs
accurately reconstruct corrupted images and generate better results compared to
the standard SR approaches.
</p>
<a href="http://arxiv.org/abs/2011.02519" target="_blank">arXiv:2011.02519</a> [<a href="http://arxiv.org/pdf/2011.02519" target="_blank">pdf</a>]

<h2>DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation of Complex 3D Surfaces. (arXiv:2011.02570v2 [cs.CV] UPDATED)</h2>
<h3>Rahul Venkatesh, Sarthak Sharma, Aurobrata Ghosh, Laszlo Jeni, Maneesh Singh</h3>
<p>High fidelity representation of shapes with arbitrary topology is an
important problem for a variety of vision and graphics applications. Owing to
their limited resolution, classical discrete shape representations using point
clouds, voxels and meshes produce low quality results when used in these
applications. Several implicit 3D shape representation approaches using deep
neural networks have been proposed leading to significant improvements in both
quality of representations as well as the impact on downstream applications.
However, these methods can only be used to represent topologically closed
shapes which greatly limits the class of shapes that they can represent. As a
consequence, they also often require clean, watertight meshes for training. In
this work, we propose DUDE - a Deep Unsigned Distance Embedding method which
alleviates both of these shortcomings. DUDE is a disentangled shape
representation that utilizes an unsigned distance field (uDF) to represent
proximity to a surface, and a normal vector field (nVF) to represent surface
orientation. We show that a combination of these two (uDF+nVF) can be used to
learn high fidelity representations for arbitrary open/closed shapes. As
opposed to prior work such as DeepSDF, our shape representations can be
directly learnt from noisy triangle soups, and do not need watertight meshes.
Additionally, we propose novel algorithms for extracting and rendering
iso-surfaces from the learnt representations. We validate DUDE on benchmark 3D
datasets and demonstrate that it produces significant improvements over the
state of the art.
</p>
<a href="http://arxiv.org/abs/2011.02570" target="_blank">arXiv:2011.02570</a> [<a href="http://arxiv.org/pdf/2011.02570" target="_blank">pdf</a>]

<h2>Online Sparse Reinforcement Learning. (arXiv:2011.04018v3 [cs.LG] UPDATED)</h2>
<h3>Botao Hao, Tor Lattimore, Csaba Szepesv&#xe1;ri, Mengdi Wang</h3>
<p>We investigate the hardness of online reinforcement learning in fixed
horizon, sparse linear Markov decision process (MDP), with a special focus on
the high-dimensional regime where the ambient dimension is larger than the
number of episodes. Our contribution is two-fold. First, we provide a lower
bound showing that linear regret is generally unavoidable in this case, even if
there exists a policy that collects well-conditioned data. The lower bound
construction uses an MDP with a fixed number of states while the number of
actions scales with the ambient dimension. Note that when the horizon is fixed
to one, the case of linear stochastic bandits, the linear regret can be
avoided. Second, we show that if the learner has oracle access to a policy that
collects well-conditioned data then a variant of Lasso fitted Q-iteration
enjoys a nearly dimension-free regret of $\tilde{O}( s^{2/3} N^{2/3})$ where
$N$ is the number of episodes and $s$ is the sparsity level. This shows that in
the large-action setting, the difficulty of learning can be attributed to the
difficulty of finding a good exploratory policy.
</p>
<a href="http://arxiv.org/abs/2011.04018" target="_blank">arXiv:2011.04018</a> [<a href="http://arxiv.org/pdf/2011.04018" target="_blank">pdf</a>]

<h2>Automatic Open-World Reliability Assessment. (arXiv:2011.05506v2 [cs.CV] UPDATED)</h2>
<h3>Mohsen Jafarzadeh, Touqeer Ahmad, Akshay Raj Dhamija, Chunchun Li, Steve Cruz, Terrance E. Boult</h3>
<p>Image classification in the open-world must handle out-of-distribution (OOD)
images. Systems should ideally reject OOD images, or they will map atop of
known classes and reduce reliability. Using open-set classifiers that can
reject OOD inputs can help. However, optimal accuracy of open-set classifiers
depend on the frequency of OOD data. Thus, for either standard or open-set
classifiers, it is important to be able to determine when the world changes and
increasing OOD inputs will result in reduced system reliability. However,
during operations, we cannot directly assess accuracy as there are no labels.
Thus, the reliability assessment of these classifiers must be done by human
operators, made more complex because networks are not 100% accurate, so some
failures are to be expected. To automate this process, herein, we formalize the
open-world recognition reliability problem and propose multiple automatic
reliability assessment policies to address this new problem using only the
distribution of reported scores/probability data. The distributional algorithms
can be applied to both classic classifiers with SoftMax as well as the
open-world Extreme Value Machine (EVM) to provide automated reliability
assessment. We show that all of the new algorithms significantly outperform
detection using the mean of SoftMax.
</p>
<a href="http://arxiv.org/abs/2011.05506" target="_blank">arXiv:2011.05506</a> [<a href="http://arxiv.org/pdf/2011.05506" target="_blank">pdf</a>]

<h2>Deep learning and hand-crafted features for virus image classification. (arXiv:2011.06123v2 [cs.CV] UPDATED)</h2>
<h3>Loris Nanni, Eugenio De Luca, Marco Ludovico Facin, Gianluca Maguolo</h3>
<p>In this work, we present an ensemble of descriptors for the classification of
transmission electron microscopy images of viruses. We propose to combine
handcrafted and deep learning approaches for virus image classification. The
set of handcrafted is mainly based on Local Binary Pattern variants, for each
descriptor a different Support Vector Machine is trained, then the set of
classifiers is combined by sum rule. The deep learning approach is a
densenet201 pretrained on ImageNet and then tuned in the virus dataset, the net
is used as features extractor for feeding another Support Vector Machine, in
particular the last average pooling layer is used as feature extractor.
Finally, classifiers trained on handcrafted features and classifier trained on
deep learning features are combined by sum rule. The proposed fusion strongly
boosts the performance obtained by each stand-alone approach, obtaining state
of the art performance.
</p>
<a href="http://arxiv.org/abs/2011.06123" target="_blank">arXiv:2011.06123</a> [<a href="http://arxiv.org/pdf/2011.06123" target="_blank">pdf</a>]

<h2>DeepMind Lab2D. (arXiv:2011.07027v2 [cs.AI] UPDATED)</h2>
<h3>Charles Beattie, Thomas K&#xf6;ppe, Edgar A. Du&#xe9;&#xf1;ez-Guzm&#xe1;n, Joel Z. Leibo</h3>
<p>We present DeepMind Lab2D, a scalable environment simulator for artificial
intelligence research that facilitates researcher-led experimentation with
environment design. DeepMind Lab2D was built with the specific needs of
multi-agent deep reinforcement learning researchers in mind, but it may also be
useful beyond that particular subfield.
</p>
<a href="http://arxiv.org/abs/2011.07027" target="_blank">arXiv:2011.07027</a> [<a href="http://arxiv.org/pdf/2011.07027" target="_blank">pdf</a>]

<h2>On Focal Loss for Class-Posterior Probability Estimation: A Theoretical Perspective. (arXiv:2011.09172v2 [stat.ML] UPDATED)</h2>
<h3>Nontawat Charoenphakdee, Jayakorn Vongkulbhisal, Nuttapong Chairatanakul, Masashi Sugiyama</h3>
<p>The focal loss has demonstrated its effectiveness in many real-world
applications such as object detection and image classification, but its
theoretical understanding has been limited so far. In this paper, we first
prove that the focal loss is classification-calibrated, i.e., its minimizer
surely yields the Bayes-optimal classifier and thus the use of the focal loss
in classification can be theoretically justified. However, we also prove a
negative fact that the focal loss is not strictly proper, i.e., the confidence
score of the classifier obtained by focal loss minimization does not match the
true class-posterior probability and thus it is not reliable as a
class-posterior probability estimator. To mitigate this problem, we next prove
that a particular closed-form transformation of the confidence score allows us
to recover the true class-posterior probability. Through experiments on
benchmark datasets, we demonstrate that our proposed transformation
significantly improves the accuracy of class-posterior probability estimation.
</p>
<a href="http://arxiv.org/abs/2011.09172" target="_blank">arXiv:2011.09172</a> [<a href="http://arxiv.org/pdf/2011.09172" target="_blank">pdf</a>]

<h2>Seeing Through your Skin: Recognizing Objects with a Novel Visuotactile Sensor. (arXiv:2011.09552v2 [cs.RO] UPDATED)</h2>
<h3>Francois Robert Hogan, Michael Jenkin, Sahand Rezaei-Shoshtari, Yogesh Girdhar, David Meger, Gregory Dudek</h3>
<p>We introduce a new class of vision-based sensor and associated algorithmic
processes that combine visual imaging with high-resolution tactile sending, all
in a uniform hardware and computational architecture. We demonstrate the
sensor's efficacy for both multi-modal object recognition and metrology. Object
recognition is typically formulated as an unimodal task, but by combining two
sensor modalities we show that we can achieve several significant performance
improvements. This sensor, named the See-Through-your-Skin sensor (STS), is
designed to provide rich multi-modal sensing of contact surfaces. Inspired by
recent developments in optical tactile sensing technology, we address a key
missing feature of these sensors: the ability to capture a visual perspective
of the region beyond the contact surface. Whereas optical tactile sensors are
typically opaque, we present a sensor with a semitransparent skin that has the
dual capabilities of acting as a tactile sensor and/or as a visual camera
depending on its internal lighting conditions. This paper details the design of
the sensor, showcases its dual sensing capabilities, and presents a deep
learning architecture that fuses vision and touch. We validate the ability of
the sensor to classify household objects, recognize fine textures, and infer
their physical properties both through numerical simulations and experiments
with a smart countertop prototype.
</p>
<a href="http://arxiv.org/abs/2011.09552" target="_blank">arXiv:2011.09552</a> [<a href="http://arxiv.org/pdf/2011.09552" target="_blank">pdf</a>]

<h2>Hyper-parameter estimation method with particle swarm optimization. (arXiv:2011.11944v2 [cs.LG] UPDATED)</h2>
<h3>Yaru Li, Yulai Zhang</h3>
<p>Particle swarm optimization (PSO) method cannot be directly used in the
problem of hyper-parameter estimation since the mathematical formulation of the
mapping from hyper-parameters to loss function or generalization accuracy is
unclear. Bayesian optimization (BO) framework is capable of converting the
optimization of the hyper-parameters into the optimization of an acquisition
function. The acquisition function is non-convex and multi-peak. So the problem
can be better solved by the PSO. The proposed method in this paper uses the
particle swarm method to optimize the acquisition function in the BO framework
to get better hyper-parameters. The performances of proposed method in both of
the classification and regression models are evaluated and demonstrated. The
results on several benchmark problems are improved.
</p>
<a href="http://arxiv.org/abs/2011.11944" target="_blank">arXiv:2011.11944</a> [<a href="http://arxiv.org/pdf/2011.11944" target="_blank">pdf</a>]

<h2>AI Discovering a Coordinate System of Chemical Elements: Dual Representation by Variational Autoencoders. (arXiv:2011.12090v3 [cs.LG] UPDATED)</h2>
<h3>Alex Glushkovsky</h3>
<p>The periodic table is a fundamental representation of chemical elements that
plays essential theoretical and practical roles. The research article discusses
the experiences of unsupervised training of neural networks to represent
elements on the 2D latent space based on their electron configurations while
forcing disentanglement. To emphasize chemical properties of the elements, the
original data of electron configurations has been realigned towards the
outermost valence orbitals. Recognizing seven shells and four subshells, the
input data has been arranged as (7x4) images. Latent space representation has
been performed using a convolutional beta variational autoencoder (beta-VAE).
Despite discrete and sparse input data, the beta-VAE disentangles elements of
different periods, blocks, groups, and types, while retaining the order along
atomic numbers. In addition, it isolates outliers on the latent space that
turned out to be known cases of Madelung's rule violations for lanthanide and
actinide elements. Considering the generative capabilities of beta-VAE and
discrete input data, the supervised machine learning has been set to find out
if there are insightful patterns distinguishing electron configurations between
real elements and decoded artificial ones. Also, the article addresses the
capability of dual representation by autoencoders. Conventionally, autoencoders
represent observations of input data on the latent space. However, by
transposing and duplicating original input data, it is possible to represent
variables on the latent space as well. The latest can lead to the discovery of
meaningful patterns among input variables. Applying that unsupervised learning
for transposed data of electron configurations, the order of input variables
that has been arranged by the encoder on the latent space has turned out to
exactly match the sequence of Madelung's rule.
</p>
<a href="http://arxiv.org/abs/2011.12090" target="_blank">arXiv:2011.12090</a> [<a href="http://arxiv.org/pdf/2011.12090" target="_blank">pdf</a>]

<h2>C-Learning: Horizon-Aware Cumulative Accessibility Estimation. (arXiv:2011.12363v2 [cs.LG] UPDATED)</h2>
<h3>Panteha Naderian, Gabriel Loaiza-Ganem, Harry J. Braviner, Anthony L. Caterini, Jesse C. Cresswell, Tong Li, Animesh Garg</h3>
<p>Multi-goal reaching is an important problem in reinforcement learning needed
to achieve algorithmic generalization. Despite recent advances in this field,
current algorithms suffer from three major challenges: high sample complexity,
learning only a single way of reaching the goals, and difficulties in solving
complex motion planning tasks. In order to address these limitations, we
introduce the concept of cumulative accessibility functions, which measure the
reachability of a goal from a given state within a specified horizon. We show
that these functions obey a recurrence relation, which enables learning from
offline interactions. We also prove that optimal cumulative accessibility
functions are monotonic in the planning horizon. Additionally, our method can
trade off speed and reliability in goal-reaching by suggesting multiple paths
to a single goal depending on the provided horizon. We evaluate our approach on
a set of multi-goal discrete and continuous control tasks. We show that our
method outperforms state-of-the-art goal-reaching algorithms in success rate,
sample complexity, and path optimality. Additional visualizations can be found
at https://sites.google.com/view/learning-cae/.
</p>
<a href="http://arxiv.org/abs/2011.12363" target="_blank">arXiv:2011.12363</a> [<a href="http://arxiv.org/pdf/2011.12363" target="_blank">pdf</a>]

<h2>Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations. (arXiv:2011.12854v3 [cs.LG] UPDATED)</h2>
<h3>Wolfgang Stammer, Patrick Schramowski, Kristian Kersting</h3>
<p>Most explanation methods in deep learning map importance estimates for a
model's prediction back to the original input space. These "visual"
explanations are often insufficient, as the model's actual concept remains
elusive. Moreover, without insights into the model's semantic concept, it is
difficult -- if not impossible -- to intervene on the model's behavior via its
explanations, called Explanatory Interactive Learning. Consequently, we propose
to intervene on a Neuro-Symbolic scene representation, which allows one to
revise the model on the semantic level, e.g. "never focus on the color to make
your decision". We compiled a novel confounded visual scene data set, the
CLEVR-Hans data set, capturing complex compositions of different objects. The
results of our experiments on CLEVR-Hans demonstrate that our semantic
explanations, i.e. compositional explanations at a per-object level, can
identify confounders that are not identifiable using "visual" explanations
only. More importantly, feedback on this semantic level makes it possible to
revise the model from focusing on these confounding factors.
</p>
<a href="http://arxiv.org/abs/2011.12854" target="_blank">arXiv:2011.12854</a> [<a href="http://arxiv.org/pdf/2011.12854" target="_blank">pdf</a>]

<h2>Open-World Learning Without Labels. (arXiv:2011.12906v2 [cs.CV] UPDATED)</h2>
<h3>Mohsen Jafarzadeh, Akshay Raj Dhamija, Steve Cruz, Chunchun Li, Touqeer Ahmad, Terrance E. Boult</h3>
<p>Open-world learning is a problem where an autonomous agent detects things
that it does not know and learns them over time from a non-stationary and
never-ending stream of data; in an open-world environment, the training data
and objective criteria are never available at once. The agent should grasp new
knowledge from learning without forgetting acquired prior knowledge.
Researchers proposed a few open-world learning agents for image classification
tasks that operate in complex scenarios. However, all prior work on open-world
learning has all labeled data to learn the new classes from the stream of
images. In scenarios where autonomous agents should respond in near real-time
or work in areas with limited communication infrastructure, human labeling of
data is not possible. Therefore, supervised open-world learning agents are not
scalable solutions for such applications. Herein, we propose a new framework
that enables agents to learn new classes from a stream of unlabeled data in an
unsupervised manner. Also, we study the robustness and learning speed of such
agents with supervised and unsupervised feature representation. We also
introduce a new metric for open-world learning without labels. We anticipate
our theories and method to be a starting point for developing autonomous true
open-world never-ending learning agents.
</p>
<a href="http://arxiv.org/abs/2011.12906" target="_blank">arXiv:2011.12906</a> [<a href="http://arxiv.org/pdf/2011.12906" target="_blank">pdf</a>]

<h2>Automatic Detection of Cardiac Chambers Using an Attention-based YOLOv4 Framework from Four-chamber View of Fetal Echocardiography. (arXiv:2011.13096v2 [cs.CV] UPDATED)</h2>
<h3>Sibo Qiao, Shanchen Pang, Gang Luo, Silin Pan, Xun Wang, Min Wang, Xue Zhai, Taotao Chen</h3>
<p>Echocardiography is a powerful prenatal examination tool for early diagnosis
of fetal congenital heart diseases (CHDs). The four-chamber (FC) view is a
crucial and easily accessible ultrasound (US) image among echocardiography
images. Automatic analysis of FC views contributes significantly to the early
diagnosis of CHDs. The first step to automatically analyze fetal FC views is
locating the fetal four crucial chambers of heart in a US image. However, it is
a greatly challenging task due to several key factors, such as numerous
speckles in US images, the fetal cardiac chambers with small size and unfixed
positions, and category indistinction caused by the similarity of cardiac
chambers. These factors hinder the process of capturing robust and
discriminative features, hence destroying fetal cardiac anatomical chambers
precise localization. Therefore, we first propose a multistage residual hybrid
attention module (MRHAM) to improve the feature learning. Then, we present an
improved YOLOv4 detection model, namely MRHAM-YOLOv4-Slim. Specially, the
residual identity mapping is replaced with the MRHAM in the backbone of
MRHAM-YOLOv4-Slim, accurately locating the four important chambers in fetal FC
views. Extensive experiments demonstrate that our proposed method outperforms
current state-of-the-art, including the precision of 0.919, the recall of
0.971, the F1 score of 0.944, the mAP of 0.953, and the frames per second (FPS)
of 43.
</p>
<a href="http://arxiv.org/abs/2011.13096" target="_blank">arXiv:2011.13096</a> [<a href="http://arxiv.org/pdf/2011.13096" target="_blank">pdf</a>]

<h2>Video Self-Stitching Graph Network for Temporal Action Localization. (arXiv:2011.14598v2 [cs.CV] UPDATED)</h2>
<h3>Chen Zhao, Ali Thabet, Bernard Ghanem</h3>
<p>Temporal action localization (TAL) in videos is a challenging task,
especially due to the large scale variation of actions. In the data, short
actions usually occupy the major proportion, but have the lowest performance
with all current methods. In this paper, we confront the challenge of short
actions and propose a multi-level cross-scale solution dubbed as video
self-stitching graph network (VSGN). We have two key components in VSGN: video
self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we
focus on a short period of a video and magnify it along the temporal dimension
to obtain a larger scale. By our self-stitching approach, we are able to
utilize the original clip and its magnified counterpart in one input sequence
to take advantage of the complementary properties of both scales. The xGPN
component further exploits the cross-scale correlations by a pyramid of
cross-scale graph networks, each containing a hybrid temporal-graph module to
aggregate features from across scales as well as within the same scale. Our
VSGN not only enhances the feature representations, but also generates more
positive anchors for short actions and more short training samples. Experiments
demonstrate that VSGN obviously improves the localization performance of short
actions as well as achieving the state-of-the-art overall performance on
ActivityNet-v1.3, reaching an average mAP of 35.07 %.
</p>
<a href="http://arxiv.org/abs/2011.14598" target="_blank">arXiv:2011.14598</a> [<a href="http://arxiv.org/pdf/2011.14598" target="_blank">pdf</a>]

<h2>DenserNet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation. (arXiv:2012.02366v2 [cs.CV] UPDATED)</h2>
<h3>Dongfang Liu, Yiming Cui, Liqi Yan, Christos Mousas, Baijian Yang, Yingjie Chen</h3>
<p>In this work, we introduce a Denser Feature Network (DenserNet) for visual
localization. Our work provides three principal contributions. First, we
develop a convolutional neural network (CNN) architecture which aggregates
feature maps at different semantic levels for image representations. Using
denser feature maps, our method can produce more keypoint features and increase
image retrieval accuracy. Second, our model is trained end-to-end without
pixel-level annotation other than positive and negative GPS-tagged image pairs.
We use a weakly supervised triplet ranking loss to learn discriminative
features and encourage keypoint feature repeatability for image representation.
Finally, our method is computationally efficient as our architecture has shared
features and parameters during computation. Our method can perform accurate
large-scale localization under challenging conditions while remaining the
computational constraint. Extensive experiment results indicate that our method
sets a new state-of-the-art on four challenging large-scale localization
benchmarks and three image retrieval benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.02366" target="_blank">arXiv:2012.02366</a> [<a href="http://arxiv.org/pdf/2012.02366" target="_blank">pdf</a>]

<h2>Fixed Priority Global Scheduling from a Deep Learning Perspective. (arXiv:2012.03002v2 [cs.AI] UPDATED)</h2>
<h3>Hyunsung Lee, Michael Wang, Honguk Woo</h3>
<p>Deep Learning has been recently recognized as one of the feasible solutions
to effectively address combinatorial optimization problems, which are often
considered important yet challenging in various research domains. In this work,
we first present how to adopt Deep Learning for real-time task scheduling
through our preliminary work upon fixed priority global scheduling (FPGS)
problems. We then briefly discuss possible generalizations of Deep Learning
adoption for several realistic and complicated FPGS scenarios, e.g., scheduling
tasks with dependency, mixed-criticality task scheduling. We believe that there
are many opportunities for leveraging advanced Deep Learning technologies to
improve the quality of scheduling in various system configurations and problem
scenarios.
</p>
<a href="http://arxiv.org/abs/2012.03002" target="_blank">arXiv:2012.03002</a> [<a href="http://arxiv.org/pdf/2012.03002" target="_blank">pdf</a>]

<h2>Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations. (arXiv:2012.03434v2 [cs.CV] UPDATED)</h2>
<h3>Woo-Jeoung Nam, Jaesik Choi, Seong-Whan Lee</h3>
<p>The clear transparency of Deep Neural Networks (DNNs) is hampered by complex
internal structures and nonlinear transformations along deep hierarchies. In
this paper, we propose a new attribution method, Relative Sectional Propagation
(RSP), for fully decomposing the output predictions with the characteristics of
class-discriminative attributions and clear objectness. We carefully revisit
some shortcomings of backpropagation-based attribution methods, which are
trade-off relations in decomposing DNNs. We define hostile factor as an element
that interferes with finding the attributions of the target and propagate it in
a distinguishable way to overcome the non-suppressed nature of activated
neurons. As a result, it is possible to assign the bi-polar relevance scores of
the target (positive) and hostile (negative) attributions while maintaining
each attribution aligned with the importance. We also present the purging
techniques to prevent the decrement of the gap between the relevance scores of
the target and hostile attributions during backward propagation by eliminating
the conflicting units to channel attribution map. Therefore, our method makes
it possible to decompose the predictions of DNNs with clearer
class-discriminativeness and detailed elucidations of activation neurons
compared to the conventional attribution methods. In a verified experimental
environment, we report the results of the assessments: (i) Pointing Game, (ii)
mIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and
ImageNet datasets. The results demonstrate that our method outperforms existing
backward decomposition methods, including distinctive and intuitive
visualizations.
</p>
<a href="http://arxiv.org/abs/2012.03434" target="_blank">arXiv:2012.03434</a> [<a href="http://arxiv.org/pdf/2012.03434" target="_blank">pdf</a>]

<h2>Improving Constraint Satisfaction Algorithm Efficiency for the AllDifferent Constraint. (arXiv:2012.03624v2 [cs.AI] UPDATED)</h2>
<h3>Geoff Harris</h3>
<p>Combinatorial problems stated as Constraint Satisfaction Problems (CSP) are
examined. It is shown by example that any algorithm designed for the original
CSP, and involving the AllDifferent constraint, has at least the same level of
efficacy when simultaneously applied to both the original and its complementary
problem. The 1-to-1 mapping employed to transform a CSP to its complementary
problem, which is also a CSP, is introduced. This "Dual CSP" method and its
application are outlined. The analysis of several random problem instances
demonstrate the benefits of this method for variable domain reduction compared
to the standard approach to CSP. Extensions to additional constraints other
than AllDifferent, as well as the use of hybrid algorithms, are proposed as
candidates for this Dual CSP method.
</p>
<a href="http://arxiv.org/abs/2012.03624" target="_blank">arXiv:2012.03624</a> [<a href="http://arxiv.org/pdf/2012.03624" target="_blank">pdf</a>]

<h2>Improved Convergence Rates for Non-Convex Federated Learning with Compression. (arXiv:2012.04061v2 [stat.ML] UPDATED)</h2>
<h3>Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S. Dhillon</h3>
<p>Federated learning is a new distributed learning paradigm that enables
efficient training of emerging large-scale machine learning models. In this
paper, we consider federated learning on non-convex objectives with compressed
communication from the clients to the central server. We propose a novel
first-order algorithm (\texttt{FedSTEPH2}) that employs compressed
communication and achieves the optimal iteration complexity of
$\mathcal{O}(1/\epsilon^{1.5})$ to reach an $\epsilon$-stationary point (i.e.
$\mathbb{E}[\|\nabla f(\bm{x})\|^2] \leq \epsilon$) on smooth non-convex
objectives. The proposed scheme is the first algorithm that attains the
aforementioned optimal complexity with compressed communication and without
using full client gradients at each communication round. The key idea of
\texttt{FedSTEPH2} that enables attaining this optimal complexity is applying
judicious momentum terms both in the local client updates and the global server
update. As a prequel to \texttt{FedSTEPH2}, we propose \texttt{FedSTEPH} which
involves a momentum term only in the local client updates. We establish that
\texttt{FedSTEPH} enjoys improved convergence rates under various non-convex
settings (such as the Polyak-\L{}ojasiewicz condition) and with fewer
assumptions than prior work.
</p>
<a href="http://arxiv.org/abs/2012.04061" target="_blank">arXiv:2012.04061</a> [<a href="http://arxiv.org/pdf/2012.04061" target="_blank">pdf</a>]

<h2>GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms. (arXiv:2012.04201v2 [cs.LG] UPDATED)</h2>
<h3>Jiwei Liu, Bojan Tunguz, Gilberto Titericz</h3>
<p>Black-box optimization is essential for tuning complex machine learning
algorithms which are easier to experiment with than to understand. In this
paper, we show that a simple ensemble of black-box optimization algorithms can
outperform any single one of them. However, searching for such an optimal
ensemble requires a large number of experiments. We propose a
Multi-GPU-optimized framework to accelerate a brute force search for the
optimal ensemble of black-box optimization algorithms by running many
experiments in parallel. The lightweight optimizations are performed by CPU
while expensive model training and evaluations are assigned to GPUs. We
evaluate 15 optimizers by training 2.7 million models and running 541,440
optimizations. On a DGX-1, the search time is reduced from more than 10 days on
two 20-core CPUs to less than 24 hours on 8-GPUs. With the optimal ensemble
found by GPU-accelerated exhaustive search, we won the 2nd place of NeurIPS
2020 black-box optimization challenge.
</p>
<a href="http://arxiv.org/abs/2012.04201" target="_blank">arXiv:2012.04201</a> [<a href="http://arxiv.org/pdf/2012.04201" target="_blank">pdf</a>]

<h2>Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework. (arXiv:2012.04240v2 [cs.LG] UPDATED)</h2>
<h3>Sung-En Chang, Yanyu Li, Mengshu Sun, Runbin Shi, Hayden K.-H. So, Xuehai Qian, Yanzhi Wang, Xue Lin</h3>
<p>Deep Neural Networks (DNNs) have achieved extraordinary performance in
various application domains. To support diverse DNN models, efficient
implementations of DNN inference on edge-computing platforms, e.g., ASICs,
FPGAs, and embedded systems, are extensively investigated. Due to the huge
model size and computation amount, model compression is a critical step to
deploy DNN models on edge devices. This paper focuses on weight quantization, a
hardware-friendly model compression approach that is complementary to weight
pruning. Unlike existing methods that use the same quantization scheme for all
weights, we propose the first solution that applies different quantization
schemes for different rows of the weight matrix. It is motivated by (1) the
distribution of the weights in the different rows are not the same; and (2) the
potential of achieving better utilization of heterogeneous FPGA hardware
resources. To achieve that, we first propose a hardware-friendly quantization
scheme named sum-of-power-of-2 (SP2) suitable for Gaussian-like weight
distribution, in which the multiplication arithmetic can be replaced with logic
shifter and adder, thereby enabling highly efficient implementations with the
FPGA LUT resources. In contrast, the existing fixed-point quantization is
suitable for Uniform-like weight distribution and can be implemented
efficiently by DSP. Then to fully explore the resources, we propose an
FPGA-centric mixed scheme quantization (MSQ) with an ensemble of the proposed
SP2 and the fixed-point schemes. Combining the two schemes can maintain, or
even increase accuracy due to better matching with weight distributions.
</p>
<a href="http://arxiv.org/abs/2012.04240" target="_blank">arXiv:2012.04240</a> [<a href="http://arxiv.org/pdf/2012.04240" target="_blank">pdf</a>]

<h2>A Quality Diversity Approach to Automatically Generating Human-Robot Interaction Scenarios in Shared Autonomy. (arXiv:2012.04283v2 [cs.RO] UPDATED)</h2>
<h3>Matthew Fontaine, Stefanos Nikolaidis</h3>
<p>The growth of scale and complexity of interactions between humans and robots
highlights the need for new computational methods to automatically evaluate the
performance of novel algorithms and applications. Strong evaluation methods
should explore the diverse scenarios of interaction between humans and robots.
We propose quality diversity (QD) algorithms as a method for simultaneously
exploring both environments and human actions to discover diverse failure
scenarios. We focus on the shared autonomy domain, where the robot attempts to
infer the goal of a human operator. We evaluate our approach by automatically
generating scenarios for two published algorithms in this domain: shared
autonomy via hindsight optimization and linear policy blending. Some of the
generated scenarios confirm previous theoretical findings, while others are
surprising and bring about a new understanding of state-of-the-art
implementations. Our experiments show that QD outperforms Monte-Carlo
simulation and optimization based methods in effectively searching the scenario
space, highlighting its promise for automatic evaluation of algorithms in
shared autonomy.
</p>
<a href="http://arxiv.org/abs/2012.04283" target="_blank">arXiv:2012.04283</a> [<a href="http://arxiv.org/pdf/2012.04283" target="_blank">pdf</a>]

<h2>Combining Reinforcement Learning with Lin-Kernighan-Helsgaun Algorithm for the Traveling Salesman Problem. (arXiv:2012.04461v3 [cs.AI] UPDATED)</h2>
<h3>Jiongzhi Zheng, Kun He, Jianrong Zhou, Yan Jin, Chu-min Li</h3>
<p>We address the Traveling Salesman Problem (TSP), a famous NP-hard
combinatorial optimization problem. And we propose a variable strategy
reinforced approach, denoted as VSR-LKH, which combines three reinforcement
learning methods (Q-learning, Sarsa and Monte Carlo) with the well-known TSP
algorithm, called Lin-Kernighan-Helsgaun (LKH). VSR-LKH replaces the inflexible
traversal operation in LKH, and lets the program learn to make choice at each
search step by reinforcement learning. Experimental results on 111 TSP
benchmarks from the TSPLIB with up to 85,900 cities demonstrate the excellent
performance of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.04461" target="_blank">arXiv:2012.04461</a> [<a href="http://arxiv.org/pdf/2012.04461" target="_blank">pdf</a>]

<h2>CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing. (arXiv:2012.04551v2 [cs.CV] UPDATED)</h2>
<h3>Ajinkya Kadu, Tristan van Leeuwen, K. Joost Batenburg</h3>
<p>We introduce single-shot X-ray tomography that aims to estimate the target
image from a single cone-beam projection measurement. This linear inverse
problem is extremely under-determined since the measurements are far fewer than
the number of unknowns. Moreover, it is more challenging than conventional
tomography where a sufficiently large number of projection angles forms the
measurements, allowing for a simple inversion process. However, single-shot
tomography becomes less severe if the target image is only composed of known
shapes. Hence, the shape prior transforms a linear ill-posed image estimation
problem to a non-linear problem of estimating the roto-translations of the
shapes. In this paper, we circumvent the non-linearity by using a dictionary of
possible roto-translations of the shapes. We propose a convex program CoShaRP
to recover the dictionary-coefficients successfully. CoShaRP relies on
simplex-type constraint and can be solved quickly using a primal-dual
algorithm. The numerical experiments show that CoShaRP recovers shapes stably
from moderately noisy measurements.
</p>
<a href="http://arxiv.org/abs/2012.04551" target="_blank">arXiv:2012.04551</a> [<a href="http://arxiv.org/pdf/2012.04551" target="_blank">pdf</a>]

<h2>Concept Drift and Covariate Shift Detection Ensemble with Lagged Labels. (arXiv:2012.04759v2 [cs.AI] UPDATED)</h2>
<h3>Yiming Xu, Diego Klabjan</h3>
<p>In model serving, having one fixed model during the entire often life-long
inference process is usually detrimental to model performance, as data
distribution evolves over time, resulting in lack of reliability of the model
trained on historical data. It is important to detect changes and retrain the
model in time. The existing methods generally have three weaknesses: 1) using
only classification error rate as signal, 2) assuming ground truth labels are
immediately available after features from samples are received and 3) unable to
decide what data to use to retrain the model when change occurs. We address the
first problem by utilizing six different signals to capture a wide range of
characteristics of data, and we address the second problem by allowing lag of
labels, where labels of corresponding features are received after a lag in
time. For the third problem, our proposed method automatically decides what
data to use to retrain based on the signals. Extensive experiments on
structured and unstructured data for different type of data changes establish
that our method consistently outperforms the state-of-the-art methods by a
large margin.
</p>
<a href="http://arxiv.org/abs/2012.04759" target="_blank">arXiv:2012.04759</a> [<a href="http://arxiv.org/pdf/2012.04759" target="_blank">pdf</a>]

<h2>Multi-Classifier Interactive Learning for Ambiguous Speech Emotion Recognition. (arXiv:2012.05429v2 [cs.AI] UPDATED)</h2>
<h3>Ying Zhou, Xuefeng Liang, Yu Gu, Yifei Yin, Longshan Yao</h3>
<p>In recent years, speech emotion recognition technology is of great
significance in industrial applications such as call centers, social robots and
health care. The combination of speech recognition and speech emotion
recognition can improve the feedback efficiency and the quality of service.
Thus, the speech emotion recognition has been attracted much attention in both
industry and academic. Since emotions existing in an entire utterance may have
varied probabilities, speech emotion is likely to be ambiguous, which poses
great challenges to recognition tasks. However, previous studies commonly
assigned a single-label or multi-label to each utterance in certain. Therefore,
their algorithms result in low accuracies because of the inappropriate
representation. Inspired by the optimally interacting theory, we address the
ambiguous speech emotions by proposing a novel multi-classifier interactive
learning (MCIL) method. In MCIL, multiple different classifiers first mimic
several individuals, who have inconsistent cognitions of ambiguous emotions,
and construct new ambiguous labels (the emotion probability distribution).
Then, they are retrained with the new labels to interact with their cognitions.
This procedure enables each classifier to learn better representations of
ambiguous data from others, and further improves the recognition ability. The
experiments on three benchmark corpora (MAS, IEMOCAP, and FAU-AIBO) demonstrate
that MCIL does not only improve each classifier's performance, but also raises
their recognition consistency from moderate to substantial.
</p>
<a href="http://arxiv.org/abs/2012.05429" target="_blank">arXiv:2012.05429</a> [<a href="http://arxiv.org/pdf/2012.05429" target="_blank">pdf</a>]

<h2>SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains. (arXiv:2012.05535v2 [cs.CV] UPDATED)</h2>
<h3>Yuanqi Chen, Ge Li, Cece Jin, Shan Liu, Thomas Li</h3>
<p>This paper observes that there is an issue of high frequencies missing in the
discriminator of standard GAN, and we reveal it stems from downsampling layers
employed in the network architecture. This issue makes the generator lack the
incentive from the discriminator to learn high-frequency content of data,
resulting in a significant spectrum discrepancy between generated images and
real images. Since the Fourier transform is a bijective mapping, we argue that
reducing this spectrum discrepancy would boost the performance of GANs. To this
end, we introduce SSD-GAN, an enhancement of GANs to alleviate the spectral
information loss in the discriminator. Specifically, we propose to embed a
frequency-aware classifier into the discriminator to measure the realness of
the input in both the spatial and spectral domains. With the enhanced
discriminator, the generator of SSD-GAN is encouraged to learn high-frequency
content of real data and generate exact details. The proposed method is general
and can be easily integrated into most existing GANs framework without
excessive cost. The effectiveness of SSD-GAN is validated on various network
architectures, objective functions, and datasets. Code will be available at
https://github.com/cyq373/SSD-GAN.
</p>
<a href="http://arxiv.org/abs/2012.05535" target="_blank">arXiv:2012.05535</a> [<a href="http://arxiv.org/pdf/2012.05535" target="_blank">pdf</a>]

<h2>Asymptotic study of stochastic adaptive algorithm in non-convex landscape. (arXiv:2012.05640v2 [stat.ML] UPDATED)</h2>
<h3>S&#xe9;bastien Gadat, Ioana Gavra</h3>
<p>This paper studies some asymptotic properties of adaptive algorithms widely
used in optimization and machine learning, and among them Adagrad and Rmsprop,
which are involved in most of the blackbox deep learning algorithms. Our setup
is the non-convex landscape optimization point of view, we consider a one time
scale parametrization and we consider the situation where these algorithms may
be used or not with mini-batches. We adopt the point of view of stochastic
algorithms and establish the almost sure convergence of these methods when
using a decreasing step-size point of view towards the set of critical points
of the target function. With a mild extra assumption on the noise, we also
obtain the convergence towards the set of minimizer of the function. Along our
study, we also obtain a "convergence rate" of the methods, in the vein of the
works of \cite{GhadimiLan}.
</p>
<a href="http://arxiv.org/abs/2012.05640" target="_blank">arXiv:2012.05640</a> [<a href="http://arxiv.org/pdf/2012.05640" target="_blank">pdf</a>]

<h2>Clustering multivariate functional data using unsupervised binary trees. (arXiv:2012.05973v2 [stat.ML] UPDATED)</h2>
<h3>Steven Golovkine, Nicolas Klutchnikoff, Valentin Patilea</h3>
<p>We propose a model-based clustering algorithm for a general class of
functional data for which the components could be curves or images. The random
functional data realizations could be measured with error at discrete, and
possibly random, points in the definition domain. The idea is to build a set of
binary trees by recursive splitting of the observations. The number of groups
are determined in a data-driven way. The new algorithm provides easily
interpretable results and fast predictions for online data sets. Results on
simulated datasets reveal good performance in various complex settings. The
methodology is applied to the analysis of vehicle trajectories on a German
roundabout.
</p>
<a href="http://arxiv.org/abs/2012.05973" target="_blank">arXiv:2012.05973</a> [<a href="http://arxiv.org/pdf/2012.05973" target="_blank">pdf</a>]

<h2>One Point is All You Need: Directional Attention Point for Feature Learning. (arXiv:2012.06257v2 [cs.CV] UPDATED)</h2>
<h3>Liqiang Lin, Pengdi Huang, Chi-Wing Fu, Kai Xu, Hao Zhang, Hui Huang</h3>
<p>We present a novel attention-based mechanism for learning enhanced point
features for tasks such as point cloud classification and segmentation. Our key
message is that if the right attention point is selected, then "one point is
all you need" -- not a sequence as in a recurrent model and not a pre-selected
set as in all prior works. Also, where the attention point is should be
learned, from data and specific to the task at hand. Our mechanism is
characterized by a new and simple convolution, which combines the feature at an
input point with the feature at its associated attention point. We call such a
point a directional attention point (DAP), since it is found by adding to the
original point an offset vector that is learned by maximizing the task
performance in training. We show that our attention mechanism can be easily
incorporated into state-of-the-art point cloud classification and segmentation
networks. Extensive experiments on common benchmarks such as ModelNet40,
ShapeNetPart, and S3DIS demonstrate that our DAP-enabled networks consistently
outperform the respective original networks, as well as all other competitive
alternatives, including those employing pre-selected sets of attention points.
</p>
<a href="http://arxiv.org/abs/2012.06257" target="_blank">arXiv:2012.06257</a> [<a href="http://arxiv.org/pdf/2012.06257" target="_blank">pdf</a>]

<h2>EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream. (arXiv:2012.06475v2 [cs.CV] UPDATED)</h2>
<h3>Viktor Rudnev, Vladislav Golyanik, Jiayi Wang, Hans-Peter Seidel, Franziska Mueller, Mohamed Elgharib, Christian Theobalt</h3>
<p>3D hand pose estimation from monocular videos is a long-standing and
challenging problem, which is now seeing a strong upturn. In this work, we
address it for the first time using a single event camera, i.e., an
asynchronous vision sensor reacting on brightness changes. Our EventHands
approach has characteristics previously not demonstrated with a single RGB or
depth camera such as high temporal resolution at low data throughputs and
real-time performance at 1000 Hz. Due to the different data modality of event
cameras compared to classical cameras, existing methods cannot be directly
applied to and re-trained for event streams. We thus design a new neural
approach which accepts a new event stream representation suitable for learning,
which is trained on newly-generated synthetic event streams and can generalise
to real data. Experiments show that EventHands outperforms recent monocular
methods using a colour (or depth) camera in terms of accuracy and its ability
to capture hand motions of unprecedented speed. Our method, the event stream
simulator and the dataset will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.06475" target="_blank">arXiv:2012.06475</a> [<a href="http://arxiv.org/pdf/2012.06475" target="_blank">pdf</a>]

<h2>Glucose values prediction five years ahead with a new framework of missing responses in reproducing kernel Hilbert spaces, and the use of continuous glucose monitoring technology. (arXiv:2012.06564v2 [stat.ML] UPDATED)</h2>
<h3>Marcos Matabuena, Paulo F&#xe9;lix, Carlos Meijide-Garcia, Francisco Gude</h3>
<p>AEGIS study possesses unique information on longitudinal changes in
circulating glucose through continuous glucose monitoring technology (CGM).
However, as usual in longitudinal medical studies, there is a significant
amount of missing data in the outcome variables. For example, 40 percent of
glycosylated hemoglobin (A1C) biomarker data are missing five years ahead. With
the purpose to reduce the impact of this issue, this article proposes a new
data analysis framework based on learning in reproducing kernel Hilbert spaces
(RKHS) with missing responses that allows to capture non-linear relations
between variable studies in different supervised modeling tasks. First, we
extend the Hilbert-Schmidt dependence measure to test statistical independence
in this context introducing a new bootstrap procedure, for which we prove
consistency. Next, we adapt or use existing models of variable selection,
regression, and conformal inference to obtain new clinical findings about
glucose changes five years ahead with the AEGIS data. The most relevant
findings are summarized below: i) We identify new factors associated with
long-term glucose evolution; ii) We show the clinical sensibility of CGM data
to detect changes in glucose metabolism; iii) We can improve clinical
interventions based on our algorithms' expected glucose changes according to
patients' baseline characteristics.
</p>
<a href="http://arxiv.org/abs/2012.06564" target="_blank">arXiv:2012.06564</a> [<a href="http://arxiv.org/pdf/2012.06564" target="_blank">pdf</a>]

<h2>m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks. (arXiv:2008.10134v2 [cs.CV] CROSS LISTED)</h2>
<h3>Salman Maqbool, Aqsa Riaz, Hasan Sajid, Osman Hasan</h3>
<p>Autonomous surgical procedures, in particular minimal invasive surgeries, are
the next frontier for Artificial Intelligence research. However, the existing
challenges include precise identification of the human anatomy and the surgical
settings, and modeling the environment for training of an autonomous agent. To
address the identification of human anatomy and the surgical settings, we
propose a deep learning based semantic segmentation algorithm to identify and
label the tissues and organs in the endoscopic video feed of the human torso
region. We present an annotated dataset, m2caiSeg, created from endoscopic
video feeds of real-world surgical procedures. Overall, the data consists of
307 images, each of which is annotated for the organs and different surgical
instruments present in the scene. We propose and train a deep convolutional
neural network for the semantic segmentation task. To cater for the low
quantity of annotated data, we use unsupervised pre-training and data
augmentation. The trained model is evaluated on an independent test set of the
proposed dataset. We obtained a F1 score of 0.33 while using all the labeled
categories for the semantic segmentation task. Secondly, we labeled all
instruments into an 'Instruments' superclass to evaluate the model's
performance on discerning the various organs and obtained a F1 score of 0.57.
We propose a new dataset and a deep learning method for pixel level
identification of various organs and instruments in a endoscopic surgical
scene. Surgical scene understanding is one of the first steps towards
automating surgical procedures.
</p>
<a href="http://arxiv.org/abs/2008.10134" target="_blank">arXiv:2008.10134</a> [<a href="http://arxiv.org/pdf/2008.10134" target="_blank">pdf</a>]

<h2>Spatio-attentive Graphs for Human-Object Interaction Detection. (arXiv:2012.06060v1 [cs.CV] CROSS LISTED)</h2>
<h3>Frederic Z. Zhang, Dylan Campbell, Stephen Gould</h3>
<p>We address the problem of detecting human--object interactions in images
using graphical neural networks. Our network constructs a bipartite graph of
nodes representing detected humans and objects, wherein messages passed between
the nodes encode relative spatial and appearance information. Unlike existing
approaches that separate appearance and spatial features, our method fuses
these two cues within a single graphical model allowing information conditioned
on both modalities to influence the prediction of interactions with neighboring
nodes. Through extensive experimentation we demonstrate the advantages of
fusing relative spatial information with appearance features in the computation
of adjacency structure, message passing and the ultimate refined graph
features. On the popular HICO-DET benchmark dataset, our model outperforms
state-of-the-art with an mAP of 27.18, a 10% relative improvement.
</p>
<a href="http://arxiv.org/abs/2012.06060" target="_blank">arXiv:2012.06060</a> [<a href="http://arxiv.org/pdf/2012.06060" target="_blank">pdf</a>]

<h2>Better scalability under potentially heavy-tailed feedback. (arXiv:2012.07346v1 [stat.ML])</h2>
<h3>Matthew J. Holland</h3>
<p>We study scalable alternatives to robust gradient descent (RGD) techniques
that can be used when the losses and/or gradients can be heavy-tailed, though
this will be unknown to the learner. The core technique is simple: instead of
trying to robustly aggregate gradients at each step, which is costly and leads
to sub-optimal dimension dependence in risk bounds, we instead focus
computational effort on robustly choosing (or newly constructing) a strong
candidate based on a collection of cheap stochastic sub-processes which can be
run in parallel. The exact selection process depends on the convexity of the
underlying objective, but in all cases, our selection technique amounts to a
robust form of boosting the confidence of weak learners. In addition to formal
guarantees, we also provide empirical analysis of robustness to perturbations
to experimental conditions, under both sub-Gaussian and heavy-tailed data,
along with applications to a variety of benchmark datasets. The overall
take-away is an extensible procedure that is simple to implement, trivial to
parallelize, which keeps the formal merits of RGD methods but scales much
better to large learning problems.
</p>
<a href="http://arxiv.org/abs/2012.07346" target="_blank">arXiv:2012.07346</a> [<a href="http://arxiv.org/pdf/2012.07346" target="_blank">pdf</a>]

<h2>Bandit Learning in Decentralized Matching Markets. (arXiv:2012.07348v1 [cs.LG])</h2>
<h3>Lydia T. Liu, Feng Ruan, Horia Mania, Michael I. Jordan</h3>
<p>We study two-sided matching markets in which one side of the market (the
players) does not have a priori knowledge about its preferences for the other
side (the arms) and is required to learn its preferences from experience. Also,
we assume the players have no direct means of communication. This model extends
the standard stochastic multi-armed bandit framework to a decentralized
multiple player setting with competition. We introduce a new algorithm for this
setting that, over a time horizon $T$, attains $\mathcal{O}(\log(T))$ stable
regret when preferences of the arms over players are shared, and
$\mathcal{O}(\log(T)^2)$ regret when there are no assumptions on the
preferences on either side.
</p>
<a href="http://arxiv.org/abs/2012.07348" target="_blank">arXiv:2012.07348</a> [<a href="http://arxiv.org/pdf/2012.07348" target="_blank">pdf</a>]

<h2>Phase Retrieval with Holography and Untrained Priors: Tackling the Challenges of Low-Photon Nanoscale Imaging. (arXiv:2012.07386v1 [cs.LG])</h2>
<h3>Hannah Lawrence, David Bramherzig, Henry Li, Michael Eickenberg, Marylou Gabri&#xe9;</h3>
<p>Phase retrieval is the inverse problem of recovering a signal from
magnitude-only Fourier measurements, and underlies numerous imaging modalities,
such as Coherent Diffraction Imaging (CDI). A variant of this setup, known as
holography, includes a reference object that is placed adjacent to the specimen
of interest before measurements are collected. The resulting inverse problem,
known as holographic phase retrieval, is well-known to have improved problem
conditioning relative to the original. This innovation, i.e. Holographic CDI,
becomes crucial at the nanoscale, where imaging specimens such as viruses,
proteins, and crystals require low-photon measurements. This data is highly
corrupted by Poisson shot noise, and often lacks low-frequency content as well.
In this work, we introduce a dataset-free deep learning framework for
holographic phase retrieval adapted to these challenges. The key ingredients of
our approach are the explicit and flexible incorporation of the physical
forward model into an automatic differentiation procedure, the Poisson
log-likelihood objective function, and an optional untrained deep image prior.
We perform extensive evaluation under realistic conditions. Compared to
competing classical methods, our method recovers signal from higher noise
levels and is more resilient to suboptimal reference design, as well as to
large missing regions of low frequencies in the observations. To the best of
our knowledge, this is the first work to consider a dataset-free machine
learning approach for holographic phase retrieval.
</p>
<a href="http://arxiv.org/abs/2012.07386" target="_blank">arXiv:2012.07386</a> [<a href="http://arxiv.org/pdf/2012.07386" target="_blank">pdf</a>]

<h2>Molecular graph generation with Graph Neural Networks. (arXiv:2012.07397v1 [stat.ML])</h2>
<h3>Pietro Bongini, Monica Bianchini, Franco Scarselli</h3>
<p>The generation of graph-structured data is an emerging problem in the field
of deep learning. Various solutions have been proposed in the last few years,
yet the exploration of this branch is still in an early phase. In sequential
approaches, the construction of a graph is the result of a sequence of
decisions, in which, at each step, a node or a group of nodes is added to the
graph, along with its connections. A very relevant application of graph
generation methods is the discovery of new drug molecules, which are naturally
represented as graphs. In this paper, we introduce a sequential molecular graph
generator based on a set of graph neural network modules, which we call
MG^2N^2. Its modular architecture simplifies the training procedure, also
allowing an independent retraining of a single module. The use of graph neural
networks maximizes the information in input at each generative step, which
consists of the subgraph produced during the previous steps. Experiments of
unconditional generation on the QM9 dataset show that our model is capable of
generalizing molecular patterns seen during the training phase, without
overfitting. The results indicate that our method outperforms very competitive
baselines, and can be placed among the state of the art approaches for
unconditional generation on QM9.
</p>
<a href="http://arxiv.org/abs/2012.07397" target="_blank">arXiv:2012.07397</a> [<a href="http://arxiv.org/pdf/2012.07397" target="_blank">pdf</a>]

<h2>A Single Iterative Step for Anytime Causal Discovery. (arXiv:2012.07513v1 [cs.AI])</h2>
<h3>Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Gal Novik</h3>
<p>We present a sound and complete algorithm for recovering causal graphs from
observed, non-interventional data, in the possible presence of latent
confounders and selection bias. We rely on the causal Markov and faithfulness
assumptions and recover the equivalence class of the underlying causal graph by
performing a series of conditional independence (CI) tests between observed
variables. We propose a single step that is applied iteratively, such that the
independence and causal relations entailed from the resulting graph, after any
iteration, is correct and becomes more informative with successive iteration.
Essentially, we tie the size of the CI condition set to its distance from the
tested nodes on the resulting graph. Each iteration refines the skeleton and
orientation by performing CI tests having condition sets that are larger than
in the preceding iteration. In an iteration, condition sets of CI tests are
constructed from nodes that are within a specified search distance, and the
sizes of these condition sets is equal to this search distance. The algorithm
then iteratively increases the search distance along with the condition set
sizes. Thus, each iteration refines a graph, that was recovered by previous
iterations having smaller condition sets---having a higher statistical power.
We demonstrate that our algorithm requires significantly fewer CI tests and
smaller condition sets compared to the FCI algorithm. This is evident for both
recovering the true underlying graph using a perfect CI oracle, and accurately
estimating the graph using limited observed data.
</p>
<a href="http://arxiv.org/abs/2012.07513" target="_blank">arXiv:2012.07513</a> [<a href="http://arxiv.org/pdf/2012.07513" target="_blank">pdf</a>]

<h2>An Expectation-Based Network Scan Statistic for a COVID-19 Early Warning System. (arXiv:2012.07574v1 [cs.LG])</h2>
<h3>Chance Haycock, Edward Thorpe-Woods, James Walsh, Patrick O&#x27;Hara, Oscar Giles, Neil Dhir, Theodoros Damoulas</h3>
<p>One of the Greater London Authority's (GLA) response to the COVID-19 pandemic
brings together multiple large-scale and heterogeneous datasets capturing
mobility, transportation and traffic activity over the city of London to better
understand 'busyness' and enable targeted interventions and effective
policy-making. As part of Project Odysseus we describe an early-warning system
and introduce an expectation-based scan statistic for networks to help the GLA
and Transport for London, understand the extent to which populations are
following government COVID-19 guidelines. We explicitly treat the case of
geographically fixed time-series data located on a (road) network and primarily
focus on monitoring the dynamics across large regions of the capital.
Additionally, we also focus on the detection and reporting of significant
spatio-temporal regions. Our approach is extending the Network Based Scan
Statistic (NBSS) by making it expectation-based (EBP) and by using stochastic
processes for time-series forecasting, which enables us to quantify metric
uncertainty in both the EBP and NBSS frameworks. We introduce a variant of the
metric used in the EBP model which focuses on identifying space-time regions in
which activity is quieter than expected.
</p>
<a href="http://arxiv.org/abs/2012.07574" target="_blank">arXiv:2012.07574</a> [<a href="http://arxiv.org/pdf/2012.07574" target="_blank">pdf</a>]

<h2>Biomechanical modelling of brain atrophy through deep learning. (arXiv:2012.07596v1 [cs.LG])</h2>
<h3>Mariana da Silva, Kara Garcia, Carole H. Sudre, Cher Bass, M. Jorge Cardoso, Emma Robinson</h3>
<p>We present a proof-of-concept, deep learning (DL) based, differentiable
biomechanical model of realistic brain deformations. Using prescribed maps of
local atrophy and growth as input, the network learns to deform images
according to a Neo-Hookean model of tissue deformation. The tool is validated
using longitudinal brain atrophy data from the Alzheimer's Disease Neuroimaging
Initiative (ADNI) dataset, and we demonstrate that the trained model is capable
of rapidly simulating new brain deformations with minimal residuals. This
method has the potential to be used in data augmentation or for the exploration
of different causal hypotheses reflecting brain growth and atrophy.
</p>
<a href="http://arxiv.org/abs/2012.07596" target="_blank">arXiv:2012.07596</a> [<a href="http://arxiv.org/pdf/2012.07596" target="_blank">pdf</a>]

<h2>Intrinsic persistent homology via density-based metric learning. (arXiv:2012.07621v1 [stat.ML])</h2>
<h3>Eugenio Borghini, Ximena Fern&#xe1;ndez, Pablo Groisman, Gabriel Mindlin</h3>
<p>We address the problem of estimating intrinsic distances in a manifold from a
finite sample. We prove that the metric space defined by the sample endowed
with a computable metric known as sample Fermat distance converges a.s. in the
sense of Gromov-Hausdorff. The limiting object is the manifold itself endowed
with the population Fermat distance, an intrinsic metric that accounts for both
the geometry of the manifold and the density that produces the sample. This
result is applied to obtain sample persistence diagrams that converge towards
an intrinsic persistence diagram. We show that this method outperforms more
standard approaches based on Euclidean norm with theoretical results and
computational experiments.
</p>
<a href="http://arxiv.org/abs/2012.07621" target="_blank">arXiv:2012.07621</a> [<a href="http://arxiv.org/pdf/2012.07621" target="_blank">pdf</a>]

<h2>Sparse Multi-Family Deep Scattering Network. (arXiv:2012.07662v1 [stat.ML])</h2>
<h3>Romain Cosentino, Randall Balestriero</h3>
<p>In this work, we propose the Sparse Multi-Family Deep Scattering Network
(SMF-DSN), a novel architecture exploiting the interpretability of the Deep
Scattering Network (DSN) and improving its expressive power. The DSN extracts
salient and interpretable features in signals by cascading wavelet transforms,
complex modulus and extract the representation of the data via a
translation-invariant operator. First, leveraging the development of highly
specialized wavelet filters over the last decades, we propose a multi-family
approach to DSN. In particular, we propose to cross multiple wavelet transforms
at each layer of the network, thus increasing the feature diversity and
removing the need for an expert to select the appropriate filter. Secondly, we
develop an optimal thresholding strategy adequate for the DSN that regularizes
the network and controls possible instabilities induced by the signals, such as
non-stationary noise. Our systematic and principled solution sparsifies the
network's latent representation by acting as a local mask distinguishing
between activity and noise. The SMF-DSN enhances the DSN by (i) increasing the
diversity of the scattering coefficients and (ii) improves its robustness with
respect to non-stationary noise.
</p>
<a href="http://arxiv.org/abs/2012.07662" target="_blank">arXiv:2012.07662</a> [<a href="http://arxiv.org/pdf/2012.07662" target="_blank">pdf</a>]

<h2>Small Covers for Near-Zero Sets of Polynomials and Learning Latent Variable Models. (arXiv:2012.07774v1 [cs.LG])</h2>
<h3>Ilias Diakonikolas, Daniel M. Kane</h3>
<p>Let $V$ be any vector space of multivariate degree-$d$ homogeneous
polynomials with co-dimension at most $k$, and $S$ be the set of points where
all polynomials in $V$ {\em nearly} vanish. We establish a qualitatively
optimal upper bound on the size of $\epsilon$-covers for $S$, in the
$\ell_2$-norm. Roughly speaking, we show that there exists an $\epsilon$-cover
for $S$ of cardinality $M = (k/\epsilon)^{O_d(k^{1/d})}$. Our result is
constructive yielding an algorithm to compute such an $\epsilon$-cover that
runs in time $\mathrm{poly}(M)$.

Building on our structural result, we obtain significantly improved learning
algorithms for several fundamental high-dimensional probabilistic models with
hidden variables. These include density and parameter estimation for
$k$-mixtures of spherical Gaussians (with known common covariance), PAC
learning one-hidden-layer ReLU networks with $k$ hidden units (under the
Gaussian distribution), density and parameter estimation for $k$-mixtures of
linear regressions (with Gaussian covariates), and parameter estimation for
$k$-mixtures of hyperplanes. Our algorithms run in time {\em quasi-polynomial}
in the parameter $k$. Previous algorithms for these problems had running times
exponential in $k^{\Omega(1)}$.

At a high-level our algorithms for all these learning problems work as
follows: By computing the low-degree moments of the hidden parameters, we are
able to find a vector space of polynomials that nearly vanish on the unknown
parameters. Our structural result allows us to compute a quasi-polynomial sized
cover for the set of hidden parameters, which we exploit in our learning
algorithms.
</p>
<a href="http://arxiv.org/abs/2012.07774" target="_blank">arXiv:2012.07774</a> [<a href="http://arxiv.org/pdf/2012.07774" target="_blank">pdf</a>]

<h2>At the Intersection of Deep Sequential Model Framework and State-space Model Framework: Study on Option Pricing. (arXiv:2012.07784v1 [stat.ML])</h2>
<h3>Ziyang Ding, Sayan Mukherjee</h3>
<p>Inference and forecast problems of the nonlinear dynamical system have arisen
in a variety of contexts. Reservoir computing and deep sequential models, on
the one hand, have demonstrated efficient, robust, and superior performance in
modeling simple and chaotic dynamical systems. However, their innate
deterministic feature has partially detracted their robustness to noisy system,
and their inability to offer uncertainty measurement has also been an
insufficiency of the framework. On the other hand, the traditional state-space
model framework is robust to noise. It also carries measured uncertainty,
forming a just-right complement to the reservoir computing and deep sequential
model framework. We propose the unscented reservoir smoother, a model that
unifies both deep sequential and state-space models to achieve both frameworks'
superiorities. Evaluated in the option pricing setting on top of noisy
datasets, URS strikes highly competitive forecasting accuracy, especially those
of longer-term, and uncertainty measurement. Further extensions and
implications on URS are also discussed to generalize a full integration of both
frameworks.
</p>
<a href="http://arxiv.org/abs/2012.07784" target="_blank">arXiv:2012.07784</a> [<a href="http://arxiv.org/pdf/2012.07784" target="_blank">pdf</a>]

<h2>Noisy Linear Convergence of Stochastic Gradient Descent for CV@R Statistical Learning under Polyak-{\L}ojasiewicz Conditions. (arXiv:2012.07785v1 [cs.LG])</h2>
<h3>Dionysios S. Kalogerias</h3>
<p>Conditional Value-at-Risk ($\mathrm{CV@R}$) is one of the most popular
measures of risk, which has been recently considered as a performance criterion
in supervised statistical learning, as it is related to desirable operational
features in modern applications, such as safety, fairness, distributional
robustness, and prediction error stability. However, due to its variational
definition, $\mathrm{CV@R}$ is commonly believed to result in difficult
optimization problems, even for smooth and strongly convex loss functions. We
disprove this statement by establishing noisy (i.e., fixed-accuracy) linear
convergence of stochastic gradient descent for sequential $\mathrm{CV@R}$
learning, for a large class of not necessarily strongly-convex (or even convex)
loss functions satisfying a set-restricted Polyak-Lojasiewicz inequality. This
class contains all smooth and strongly convex losses, confirming that classical
problems, such as linear least squares regression, can be solved efficiently
under the $\mathrm{CV@R}$ criterion, just as their risk-neutral versions. Our
results are illustrated numerically on such a risk-aware ridge regression task,
also verifying their validity in practice.
</p>
<a href="http://arxiv.org/abs/2012.07785" target="_blank">arXiv:2012.07785</a> [<a href="http://arxiv.org/pdf/2012.07785" target="_blank">pdf</a>]

