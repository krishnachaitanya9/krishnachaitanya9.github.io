---
title: Latest Deep Learning Papers
date: 2020-10-13 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>Quantifying Membership Privacy via Information Leakage. (arXiv:2010.05965v1 [cs.IT])</h2>
<h3>Sara Saeidian (1), Giulia Cervia (2), Tobias J. Oechtering (1), Mikael Skoglund (1) ((1) KTH Royal Institute of Technology, (2) IMT Lille Douai)</h3>
<p>Machine learning models are known to memorize the unique properties of
individual data points in a training set. This memorization capability can be
exploited by several types of attacks to infer information about the training
data, most notably, membership inference attacks. In this paper, we propose an
approach based on information leakage for guaranteeing membership privacy.
Specifically, we propose to use a conditional form of the notion of maximal
leakage to quantify the information leaking about individual data entries in a
dataset, i.e., the entrywise information leakage. We apply our privacy analysis
to the Private Aggregation of Teacher Ensembles (PATE) framework for
privacy-preserving classification of sensitive data and prove that the
entrywise information leakage of its aggregation mechanism is Schur-concave
when the injected noise has a log-concave probability density. The
Schur-concavity of this leakage implies that increased consensus among teachers
in labeling a query reduces its associated privacy cost. Finally, we derive
upper bounds on the entrywise information leakage when the aggregation
mechanism uses Laplace distributed noise.
</p>
<a href="http://arxiv.org/abs/2010.05965" target="_blank">arXiv:2010.05965</a> [<a href="http://arxiv.org/pdf/2010.05965" target="_blank">pdf</a>]

<h2>Computing Hecke Operators for Arithmetic Subgroups of General Linear Groups. (arXiv:2010.06036v1 [math.NT])</h2>
<h3>Mark McConnell, Robert MacPherson</h3>
<p>We present an algorithm to compute the Hecke operators on the equivariant
cohomology of an arithmetic subgroup $\Gamma$ of the general linear group
$\mathrm{GL}_n$. This includes $\mathrm{GL}_n$ over a number field or a
finite-dimensional division algebra. As coefficients, we may use any
finite-dimensional local coefficient system. Unlike earlier methods, the
algorithm works for the cohomology $H^i$ in all degrees $i$. It starts from the
well-rounded retract $\tilde{W}$, a $\Gamma$-invariant cell complex which
computes the cohomology. It extends $\tilde{W}$ to a new well-tempered complex
$\tilde{W}^+$ of one higher real dimension, using a real parameter called the
temperament. The algorithm has been coded up for $\mathrm{SL}_n(\mathbb{Z})$
for $n=2,3,4$, and it performs successfully in tests for various $\Gamma$.
</p>
<a href="http://arxiv.org/abs/2010.06036" target="_blank">arXiv:2010.06036</a> [<a href="http://arxiv.org/pdf/2010.06036" target="_blank">pdf</a>]

<h2>An Information-Theoretic Perspective on Overfitting and Underfitting. (arXiv:2010.06076v1 [cs.LG])</h2>
<h3>Daniel Bashir, George D. Montanez, Sonia Sehra, Pedro Sandoval Segura, Julius Lauw</h3>
<p>We present an information-theoretic framework for understanding overfitting
and underfitting in machine learning and prove the formal undecidability of
determining whether an arbitrary classification algorithm will overfit a
dataset. Measuring algorithm capacity via the information transferred from
datasets to models, we consider mismatches between algorithm capacities and
datasets to provide a signature for when a model can overfit or underfit a
dataset. We present results upper-bounding algorithm capacity, establish its
relationship to quantities in the algorithmic search framework for machine
learning, and relate our work to recent information-theoretic approaches to
generalization.
</p>
<a href="http://arxiv.org/abs/2010.06076" target="_blank">arXiv:2010.06076</a> [<a href="http://arxiv.org/pdf/2010.06076" target="_blank">pdf</a>]

<h2>Gradient Descent Ascent for Min-Max Problems on Riemannian Manifold. (arXiv:2010.06097v1 [cs.LG])</h2>
<h3>Feihu Huang, Shangqian Gao, Heng Huang</h3>
<p>In the paper, we study a class of useful non-convex minimax optimization
problems on the Riemanian manifold and propose a class of Riemanian gradient
descent ascent algorithms to solve these minimax problems. Specifically, we
propose a new Riemannian gradient descent ascent (RGDA) algorithm for the
deterministic minimax optimization. Moreover, we prove that the RGDA has a
sample complexity of $O(\kappa^2\epsilon^{-2})$ for finding an
$\epsilon$-stationary point of the nonconvex strongly-concave minimax problems,
where $\kappa$ denotes the condition number. At the same time, we introduce a
Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the
stochastic minimax optimization. In the theoretical analysis, we prove that the
RSGDA can achieve a sample complexity of $O(\kappa^4\epsilon^{-4})$. To further
reduce the sample complexity, we propose a novel momentum variance-reduced
Riemannian stochastic gradient descent ascent (MVR-RSGDA) algorithm based on a
new momentum variance-reduced technique of STORM. We prove that the MVR-RSGDA
algorithm achieves a lower sample complexity of
$\tilde{O}(\kappa^{4}\epsilon^{-3})$ without large batches, which reaches near
the best known sample complexity for its Euclidean counterparts. This is the
first study of the minimax optimization over the Riemannian manifold. Extensive
experimental results on the robust deep neural networks training over Stiefel
manifold demonstrate the efficiency of our proposed algorithms.
</p>
<a href="http://arxiv.org/abs/2010.06097" target="_blank">arXiv:2010.06097</a> [<a href="http://arxiv.org/pdf/2010.06097" target="_blank">pdf</a>]

<h2>Inverse Multiobjective Optimization Through Online Learning. (arXiv:2010.06140v1 [cs.LG])</h2>
<h3>Chaosheng Dong, Bo Zeng</h3>
<p>We study the problem of learning the objective functions or constraints of a
multiobjective decision making model, based on a set of sequentially arrived
decisions. In particular, these decisions might not be exact and possibly carry
measurement noise or are generated with the bounded rationality of decision
makers. In this paper, we propose a general online learning framework to deal
with this learning problem using inverse multiobjective optimization. More
precisely, we develop two online learning algorithms with implicit update rules
which can handle noisy data. Numerical results show that both algorithms can
learn the parameters with great accuracy and are robust to noise.
</p>
<a href="http://arxiv.org/abs/2010.06140" target="_blank">arXiv:2010.06140</a> [<a href="http://arxiv.org/pdf/2010.06140" target="_blank">pdf</a>]

<h2>Analysis of the rate of convergence of fully connected deep neural network regression estimates with smooth activation function. (arXiv:2010.06168v1 [math.ST])</h2>
<h3>Sophie Langer</h3>
<p>This article contributes to the current statistical theory of deep neural
networks (DNNs). It was shown that DNNs are able to circumvent the so--called
curse of dimensionality in case that suitable restrictions on the structure of
the regression function hold. In most of those results the tuning parameter is
the sparsity of the network, which describes the number of non-zero weights in
the network. This constraint seemed to be the key factor for the good rate of
convergence results. Recently, the assumption was disproved. In particular, it
was shown that simple fully connected DNNs can achieve the same rate of
convergence. Those fully connected DNNs are based on the unbounded ReLU
activation function. In this article we extend the results to smooth activation
functions, i.e., to the sigmoid activation function. It is shown that
estimators based on fully connected DNNs with sigmoid activation function also
achieve the minimax rates of convergence (up to $\ln n$-factors). In our result
the number of hidden layers is fixed, the number of neurons per layer tends to
infinity for sample size tending to infinity and a bound for the weights in the
network is given.
</p>
<a href="http://arxiv.org/abs/2010.06168" target="_blank">arXiv:2010.06168</a> [<a href="http://arxiv.org/pdf/2010.06168" target="_blank">pdf</a>]

<h2>Learning to Cache: Distributed Coded Caching in a Cellular Network With Correlated Demands. (arXiv:2010.06195v1 [cs.IT])</h2>
<h3>S. Krishnendu, B. N. Bharath, Navneet Garg, Vimal Bhatia, Tharmalingam Ratnarajah</h3>
<p>Design of distributed caching mechanisms is considered as an active area of
research due to its promising solution in reducing data load in the backhaul
link of a cellular network. In this paper, the problem of distributed content
caching in a small-cell Base Stations (sBSs) wireless network that maximizes
the cache hit performance is considered. Most of the existing works focus on
static demands, however, here, data at each sBS is considered to be correlated
across time and sBSs. The caching strategy is assumed to be a weighted
combination of past caching strategies. A high probability generalization
guarantees on the performance of the proposed caching strategy is derived. The
theoretical guarantee provides following insights on obtaining the caching
strategy: (i) run regret minimization at each sBS to obtain a sequence of
caching strategies across time, and (ii) maximize an estimate of the bound to
obtain a set of weights for the caching strategy which depends on the
discrepancy. Also, theoretical guarantee on the performance of the LRFU caching
strategy is derived. Further, federated learning based heuristic caching
algorithm is also proposed. Finally, it is shown through simulations using
Movie Lens dataset that the proposed algorithm significantly outperforms LRFU
algorithm.
</p>
<a href="http://arxiv.org/abs/2010.06195" target="_blank">arXiv:2010.06195</a> [<a href="http://arxiv.org/pdf/2010.06195" target="_blank">pdf</a>]

<h2>Average Cost Optimal Control of Stochastic Systems Using Reinforcement Learning. (arXiv:2010.06236v1 [eess.SY])</h2>
<h3>Jing Lai, Junlin Xiong</h3>
<p>This paper addresses the average cost minimization problem for discrete-time
systems with multiplicative and additive noises via reinforcement learning. By
using Q-function, we propose an online learning scheme to estimate the kernel
matrix of Q-function and to update the control gain using the data along the
system trajectories. The obtained control gain and kernel matrix are proved to
converge to the optimal ones. To implement the proposed learning scheme, an
online model-free reinforcement learning algorithm is given, where recursive
least squares method is used to estimate the kernel matrix of Q-function. A
numerical example is presented to illustrate the proposed approach.
</p>
<a href="http://arxiv.org/abs/2010.06236" target="_blank">arXiv:2010.06236</a> [<a href="http://arxiv.org/pdf/2010.06236" target="_blank">pdf</a>]

<h2>Regret minimization in stochastic non-convex learning via a proximal-gradient approach. (arXiv:2010.06250v1 [cs.LG])</h2>
<h3>Nadav Hallak, Panayotis Mertikopoulos, Volkan Cevher</h3>
<p>Motivated by applications in machine learning and operations research, we
study regret minimization with stochastic first-order oracle feedback in online
constrained, and possibly non-smooth, non-convex problems. In this setting, the
minimization of external regret is beyond reach for first-order methods, so we
focus on a local regret measure defined via a proximal-gradient mapping. To
achieve no (local) regret in this setting, we develop a prox-grad method based
on stochastic first-order feedback, and a simpler method for when access to a
perfect first-order oracle is possible. Both methods are min-max order-optimal,
and we also establish a bound on the number of prox-grad queries these methods
require. As an important application of our results, we also obtain a link
between online and offline non-convex stochastic optimization manifested as a
new prox-grad scheme with complexity guarantees matching those obtained via
variance reduction techniques.
</p>
<a href="http://arxiv.org/abs/2010.06250" target="_blank">arXiv:2010.06250</a> [<a href="http://arxiv.org/pdf/2010.06250" target="_blank">pdf</a>]

<h2>Learning Active Constraints to Efficiently Solve Bilevel Problems. (arXiv:2010.06344v1 [math.OC])</h2>
<h3>El&#xe9;a Prat, Spyros Chatzivasileiadis</h3>
<p>Bilevel programming can be used to formulate many engineering and economics
problems. However, solving such problems is hard, which impedes their
implementation in real-life. In this paper, we propose to address this
tractability challenge using machine learning classification techniques to
learn the active constraints of the lower-level problem, in order to reduce it
to those constraints only. Unlike in the commonly used reformulation of bilevel
programs with the Karush-Kuhn-Tucker conditions as a mixed-integer linear
problem, our approach avoids introducing binaries and big-M constants. The
application of machine learning reduces the online solving time, and is
particularly necessary when the same problem has to be solved multiple times.
In particular, it is very adapted to power systems problems, and especially to
market applications in which the same problem is solved many times for
different loads. Three methods are developed and applied to the problem of a
strategic generator, with a DCOPF in the lower-level. We show that for networks
of varying sizes, the computational burden is significantly reduced with a good
probability of retrieving the optimal solution. We manage to find solutions for
problems that were previously intractable.
</p>
<a href="http://arxiv.org/abs/2010.06344" target="_blank">arXiv:2010.06344</a> [<a href="http://arxiv.org/pdf/2010.06344" target="_blank">pdf</a>]

<h2>Generalized Rescaled P\'olya urn and its statistical applications. (arXiv:2010.06373v1 [math.ST])</h2>
<h3>Giacomo Aletti, Irene Crimaldi</h3>
<p>We introduce the Generalized Rescaled P\'olya (GRP) urn. In particular, the
GRP urn provides three different generative models for a chi-squared test of
goodness of fit for the long-term probabilities of correlated data, generated
by means of a reinforcement mechanism. Beside this statistical application, we
point out that the GRP urn is a simple variant of the standard
Eggenberger-P\'olya urn, that, with suitable choices of the parameters, shows
"local" reinforcement, almost sure convergence of the empirical mean to a
deterministic limit and different asymptotic behaviours of the predictive mean.
</p>
<a href="http://arxiv.org/abs/2010.06373" target="_blank">arXiv:2010.06373</a> [<a href="http://arxiv.org/pdf/2010.06373" target="_blank">pdf</a>]

<h2>Modeling Atmospheric Data and Identifying Dynamics: Temporal Data-Driven Modeling of Air Pollutants. (arXiv:2010.06538v1 [stat.AP])</h2>
<h3>Javier Rubio-Herrero, Carlos Ortiz Marrero, Wai-Tong Louis Fan</h3>
<p>Atmospheric modelling has recently experienced a surge with the advent of
deep learning. Most of these models, however, predict concentrations of
pollutants following a data-driven approach in which the physical laws that
govern their behaviors and relationships remain hidden. With the aid of
real-world air quality data collected hourly in different stations throughout
Madrid, we present a case study using a series of data-driven techniques with
the following goals: (1) Find systems of ordinary differential equations that
model the concentration of pollutants and their changes over time; (2) assess
the performance and limitations of our models using stability analysis; (3)
reconstruct the time series of chemical pollutants not measured in certain
stations using delay coordinate embedding results.
</p>
<a href="http://arxiv.org/abs/2010.06538" target="_blank">arXiv:2010.06538</a> [<a href="http://arxiv.org/pdf/2010.06538" target="_blank">pdf</a>]

<h2>Deep Limits of Residual Neural Networks. (arXiv:1810.11741v3 [math.CA] UPDATED)</h2>
<h3>Matthew Thorpe, Yves van Gennip</h3>
<p>Neural networks have been very successful in many applications; we often,
however, lack a theoretical understanding of what the neural networks are
actually learning. This problem emerges when trying to generalise to new data
sets. The contribution of this paper is to show that, for the residual neural
network model, the deep layer limit coincides with a parameter estimation
problem for a nonlinear ordinary differential equation. In particular, whilst
it is known that the residual neural network model is a discretisation of an
ordinary differential equation, we show convergence in a variational sense.
This implies that optimal parameters converge in the deep layer limit. This is
a stronger statement than saying for a fixed parameter the residual neural
network model converges (the latter does not in general imply the former). Our
variational analysis provides a discrete-to-continuum $\Gamma$-convergence
result for the objective function of the residual neural network training step
to a variational problem constrained by a system of ordinary differential
equations; this rigorously connects the discrete setting to a continuum
problem.
</p>
<a href="http://arxiv.org/abs/1810.11741" target="_blank">arXiv:1810.11741</a> [<a href="http://arxiv.org/pdf/1810.11741" target="_blank">pdf</a>]

<h2>Election Coding for Distributed Learning: Protecting SignSGD against Byzantine Attacks. (arXiv:1910.06093v2 [cs.IT] UPDATED)</h2>
<h3>Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, Jaekyun Moon</h3>
<p>Recent advances in large-scale distributed learning algorithms have enabled
communication-efficient training via SIGNSGD. Unfortunately, a major issue
continues to plague distributed learning: namely, Byzantine failures may incur
serious degradation in learning accuracy. This paper proposes ELECTION CODING,
a coding-theoretic framework to guarantee Byzantine-robustness for SIGNSGD WITH
MAJORITY VOTE, which uses minimum worker-master communication in both
directions. The suggested framework explores new information-theoretic limits
of finding the majority opinion when some workers could be malicious, and paves
the road to implement robust and efficient distributed learning algorithms.
Under this framework, we construct two types of explicit codes, random
Bernoulli codes and deterministic algebraic codes, that can tolerate Byzantine
attacks with a controlled amount of computational redundancy. For the Bernoulli
codes, we provide upper bounds on the error probability in estimating the
majority opinion, which give useful insights into code design for tolerating
Byzantine attacks. As for deterministic codes, we construct an explicit code
which perfectly tolerates Byzantines, and provide tight upper/lower bounds on
the minimum required computational redundancy. Finally, the Byzantine-tolerance
of the suggested coding schemes is confirmed by deep learning experiments on
Amazon EC2 using Python with MPI4py package.
</p>
<a href="http://arxiv.org/abs/1910.06093" target="_blank">arXiv:1910.06093</a> [<a href="http://arxiv.org/pdf/1910.06093" target="_blank">pdf</a>]

<h2>On Projection Robust Optimal Transport: Sample Complexity and Model Misspecification. (arXiv:2006.12301v3 [math.ST] UPDATED)</h2>
<h3>Tianyi Lin, Zeyu Zheng, Elynn Y. Chen, Marco Cuturi, Michael I. Jordan</h3>
<p>Optimal transport (OT) distances are increasingly used as loss functions for
statistical inference, notably in the learning of generative models or
supervised learning. Yet, the behavior of minimum Wasserstein estimators is
poorly understood, notably in high-dimensional regimes or under model
misspecification. In this work we adopt the viewpoint of projection robust (PR)
OT, which seeks to maximize the OT cost between two measures by choosing a
$k$-dimensional subspace onto which they can be projected. Our first
contribution is to establish several fundamental statistical properties of PR
Wasserstein distances, complementing and improving previous literature that has
been restricted to one-dimensional and well-specified cases. Next, we propose
the integral PR Wasserstein (IPRW) distance as an alternative to the PRW
distance, by averaging rather than optimizing on subspaces. Our complexity
bounds can help explain why both PRW and IPRW distances outperform Wasserstein
distances empirically in high-dimensional inference tasks. Finally, we consider
parametric inference using the PRW distance. We provide an asymptotic guarantee
of two types of minimum PRW estimators and formulate a central limit theorem
for max-sliced Wasserstein estimator under model misspecification. To enable
our analysis on PRW with projection dimension larger than one, we devise a
novel combination of variational analysis and statistical theory.
</p>
<a href="http://arxiv.org/abs/2006.12301" target="_blank">arXiv:2006.12301</a> [<a href="http://arxiv.org/pdf/2006.12301" target="_blank">pdf</a>]

<h2>Accelerated Zeroth-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v2 [math.OC] UPDATED)</h2>
<h3>Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang</h3>
<p>In the paper, we propose a new accelerated zeroth-order momentum (Acc-ZOM)
method to solve the non-convex stochastic mini-optimization problems. We prove
that the Acc-ZOM method achieves a lower query complexity of
$O(d^{3/4}\epsilon^{-3})$ for finding an $\epsilon$-stationary point, which
improves the best known result by a factor of $O(d^{1/4})$ where $d$ denotes
the parameter dimension. The Acc-ZOM does not require any batches compared to
the large batches required in the existing zeroth-order stochastic algorithms.
Further, we extend the Acc-ZOM method to solve the non-convex stochastic
minimax-optimization problems and propose an accelerated zeroth-order momentum
descent ascent (Acc-ZOMDA) method. We prove that the Acc-ZOMDA method reaches
the best know query complexity of
$\tilde{O}(\kappa_y^3(d_1+d_2)^{3/2}\epsilon^{-3})$ for finding an
$\epsilon$-stationary point, where $d_1$ and $d_2$ denote dimensions of the
mini and max optimization parameters respectively and $\kappa_y$ is condition
number. In particular, our theoretical result does not rely on large batches
required in the existing methods. Moreover, we propose a momentum-based
accelerated framework for the minimax-optimization problems. At the same time,
we present an accelerated momentum descent ascent (Acc-MDA) method for solving
the white-box minimax problems, and prove that it achieves near the best known
gradient complexity of $\tilde{O}(\kappa_y^{4}\epsilon^{-3})$ without large
batches. Extensive experimental results on the black-box adversarial attack to
deep neural networks (DNNs) and poisoning attack demonstrate the efficiency of
our algorithms.
</p>
<a href="http://arxiv.org/abs/2008.08170" target="_blank">arXiv:2008.08170</a> [<a href="http://arxiv.org/pdf/2008.08170" target="_blank">pdf</a>]

<h2>End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication. (arXiv:2009.05261v2 [cs.IT] UPDATED)</h2>
<h3>Fay&#xe7;al Ait Aoudia, Jakob Hoydis</h3>
<p>Previous studies have demonstrated that end-to-end learning enables
significant shaping gains over additive white Gaussian noise (AWGN) channels.
However, its benefits have not yet been quantified over realistic wireless
channel models. This work aims to fill this gap by exploring the gains of
end-to-end learning over a frequency- and time-selective fading channel using
orthogonal frequency division multiplexing (OFDM). With imperfect channel
knowledge at the receiver, the shaping gains observed on AWGN channels vanish.
Nonetheless, we identify two other sources of performance improvements. The
first comes from a neural network (NN)-based receiver operating over a large
number of subcarriers and OFDM symbols which allows to significantly reduce the
number of orthogonal pilots without loss of bit error rate (BER). The second
comes from entirely eliminating orthognal pilots by jointly learning a neural
receiver together with either superimposed pilots (SIPs), linearly combined
with conventional quadrature amplitude modulation (QAM), or an optimized
constellation geometry. The learned geometry works for a wide range of
signal-to-noise ratios (SNRs), Doppler and delay spreads, has zero mean and
does hence not contain any form of superimposed pilots. Both schemes achieve
the same BER as the pilot-based baseline with around 7% higher throughput.
Thus, we believe that a jointly learned transmitter and receiver are a very
interesting component for beyond-5G communication systems which could remove
the need and associated control overhead for demodulation reference signals
(DMRSs).
</p>
<a href="http://arxiv.org/abs/2009.05261" target="_blank">arXiv:2009.05261</a> [<a href="http://arxiv.org/pdf/2009.05261" target="_blank">pdf</a>]

<h2>Metrics and Uniqueness Criteria on the Signatures of Closed Curves. (arXiv:2009.13004v2 [math.DG] UPDATED)</h2>
<h3>Alex Kokot, Ian Klein</h3>
<p>This paper explores the paradigm of the differential signature introduced in
1996 by Calabi et al. This methodology has vast implications in fields such as
computer vision, where these techniques can potentially be used to verify a
person's handwriting is consistent with prior documents, or in medical imaging,
to name a few examples. Motivated by examples provided by Hickman in 2011 and
Musso and Nicolodi in 2009 regarding key failures in this invariant, we provide
new criteria for the correspondence between a curve and its signature to be
unique in a general setting. To show this result, we introduce new methods
regarding the signature, particularly through the lens of differential
equations, and the extension of the signature to include information on higher
order derivatives of the curvature function corresponding to the curve and
desired group action. We additionally show results regarding the robustness of
the signature, showing that under a suitable metric on the space of subsets of
$\mathbb{R}^n$, if two signatures are sufficiently close then so too will the
corresponding equivalence classes of curves they correspond to, given certain
conditions on these signatures.
</p>
<a href="http://arxiv.org/abs/2009.13004" target="_blank">arXiv:2009.13004</a> [<a href="http://arxiv.org/pdf/2009.13004" target="_blank">pdf</a>]

<h2>$L_1$-norm regularized $L_1$-norm best-fit line problem. (arXiv:2010.04684v2 [math.OC] UPDATED)</h2>
<h3>Xiao Ling, J. Paul Brooks</h3>
<p>We develop a sparse and outlier-insensitive method for one-dimensional line
fitting that can be used as the basis for outlier-insensitive machine learning
methods such as principal component analysis. The method is insensitive to
outlier observations by formulating procedures as optimization problems seeking
the $L_1$-norm best-fit line. It is also able to produce a small number of
non-zero principal components with additional penalty term to take sparseness
into account. Our algorithm has a worst-case time complexity of $O{(m^2n \log
n)}$. Computational results demonstrate that this method can provide
outlier-insensitive and sparse solutions. The space required rarely approaches
the worst-case bound.
</p>
<a href="http://arxiv.org/abs/2010.04684" target="_blank">arXiv:2010.04684</a> [<a href="http://arxiv.org/pdf/2010.04684" target="_blank">pdf</a>]

<h2>Kernel Methods for Policy Evaluation: Treatment Effects, Mediation Analysis, and Off-Policy Planning. (arXiv:2010.04855v2 [econ.EM] UPDATED)</h2>
<h3>Rahul Singh, Liyuan Xu, Arthur Gretton</h3>
<p>We propose a novel framework for non-parametric policy evaluation in static
and dynamic settings. Under the assumption of selection on observables, we
consider treatment effects of the population, of sub-populations, and of
alternative populations that may have alternative covariate distributions. We
further consider the decomposition of a total effect into a direct effect and
an indirect effect (as mediated by a particular mechanism). Under the
assumption of sequential selection on observables, we consider the effects of
sequences of treatments. Across settings, we allow for treatments that may be
discrete, continuous, or even text. Across settings, we allow for estimation of
not only counterfactual mean outcomes but also counterfactual distributions of
outcomes. We unify analyses across settings by showing that all of these causal
learning problems reduce to the re-weighting of a prediction, i.e. causal
adjustment. We implement the re-weighting as an inner product in a function
space called a reproducing kernel Hilbert space (RKHS), with a closed form
solution that can be computed in one line of code. We prove uniform consistency
and provide finite sample rates of convergence. We evaluate our estimators in
simulations devised by other authors. We use our new estimators to evaluate
continuous and heterogeneous treatment effects of the US Jobs Corps training
program for disadvantaged youth.
</p>
<a href="http://arxiv.org/abs/2010.04855" target="_blank">arXiv:2010.04855</a> [<a href="http://arxiv.org/pdf/2010.04855" target="_blank">pdf</a>]

<h2>On The Convergence of First Order Methods for Quasar-Convex Optimization. (arXiv:2010.04937v2 [math.OC] UPDATED)</h2>
<h3>Jikai Jin</h3>
<p>In recent years, the success of deep learning has inspired many researchers
to study the optimization of general smooth non-convex functions. However,
recent works have established pessimistic worst-case complexities for this
class functions, which is in stark contrast with their superior performance in
real-world applications (e.g. training deep neural networks). On the other
hand, it is found that many popular non-convex optimization problems enjoy
certain structured properties which bear some similarities to convexity. In
this paper, we study the class of \textit{quasar-convex functions} to close the
gap between theory and practice. We study the convergence of first order
methods in a variety of different settings and under different optimality
criterions. We prove complexity upper bounds that are similar to standard
results established for convex functions and much better that state-of-the-art
convergence rates of non-convex functions. Overall, this paper suggests that
\textit{quasar-convexity} allows efficient optimization procedures, and we are
looking forward to seeing more problems that demonstrate similar properties in
practice.
</p>
<a href="http://arxiv.org/abs/2010.04937" target="_blank">arXiv:2010.04937</a> [<a href="http://arxiv.org/pdf/2010.04937" target="_blank">pdf</a>]

<h2>Active learning with RESSPECT: Resource allocation for extragalactic astronomical transients. (arXiv:2010.05941v1 [astro-ph.IM])</h2>
<h3>Noble Kennamer, Emille E. O. Ishida, Santiago Gonzalez-Gaitan, Rafael S. de Souza, Alexander Ihler, Kara Ponder, Ricardo Vilalta, Anais Moller, David O. Jones, Mi Dai, Alberto Krone-Martins, Bruno Quint, Sreevarsha Sreejith, Alex I. Malz, Lluis Galbany (The LSST Dark Energy Science Collaboration and the COIN collaboration)</h3>
<p>The recent increase in volume and complexity of available astronomical data
has led to a wide use of supervised machine learning techniques. Active
learning strategies have been proposed as an alternative to optimize the
distribution of scarce labeling resources. However, due to the specific
conditions in which labels can be acquired, fundamental assumptions, such as
sample representativeness and labeling cost stability cannot be fulfilled. The
Recommendation System for Spectroscopic follow-up (RESSPECT) project aims to
enable the construction of optimized training samples for the Rubin Observatory
Legacy Survey of Space and Time (LSST), taking into account a realistic
description of the astronomical data environment. In this work, we test the
robustness of active learning techniques in a realistic simulated astronomical
data scenario. Our experiment takes into account the evolution of training and
pool samples, different costs per object, and two different sources of budget.
Results show that traditional active learning strategies significantly
outperform random sampling. Nevertheless, more complex batch strategies are not
able to significantly overcome simple uncertainty sampling techniques. Our
findings illustrate three important points: 1) active learning strategies are a
powerful tool to optimize the label-acquisition task in astronomy, 2) for
upcoming large surveys like LSST, such techniques allow us to tailor the
construction of the training sample for the first day of the survey, and 3) the
peculiar data environment related to the detection of astronomical transients
is a fertile ground that calls for the development of tailored machine learning
algorithms.
</p>
<a href="http://arxiv.org/abs/2010.05941" target="_blank">arXiv:2010.05941</a> [<a href="http://arxiv.org/pdf/2010.05941" target="_blank">pdf</a>]

<h2>COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs. (arXiv:2010.05953v1 [cs.CL])</h2>
<h3>Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, Yejin Choi</h3>
<p>Recent years have brought about a renewed interest in commonsense
representation and reasoning in the field of natural language understanding.
The development of new commonsense knowledge graphs (CSKG) has been central to
these advances as their diverse facts can be used and referenced by machine
learning models for tackling new and challenging tasks. At the same time, there
remain questions about the quality and coverage of these resources due to the
massive scale required to comprehensively encompass general commonsense
knowledge.

In this work, we posit that manually constructed CSKGs will never achieve the
coverage necessary to be applicable in all situations encountered by NLP
agents. Therefore, we propose a new evaluation framework for testing the
utility of KGs based on how effectively implicit knowledge representations can
be learned from them.

With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose
commonsense knowledge containing knowledge that is not readily available in
pretrained language models. We evaluate its properties in comparison with other
leading CSKGs, performing the first large-scale pairwise study of commonsense
knowledge resources. Next, we show that ATOMIC 2020 is better suited for
training knowledge models that can generate accurate, representative knowledge
for new, unseen entities and events. Finally, through human evaluation, we show
that the few-shot performance of GPT-3 (175B parameters), while impressive,
remains ~12 absolute points lower than a BART-based knowledge model trained on
ATOMIC 2020 despite using over 430x fewer parameters.
</p>
<a href="http://arxiv.org/abs/2010.05953" target="_blank">arXiv:2010.05953</a> [<a href="http://arxiv.org/pdf/2010.05953" target="_blank">pdf</a>]

<h2>FedAT: A Communication-Efficient Federated Learning Method with Asynchronous Tiers under Non-IID Data. (arXiv:2010.05958v1 [cs.DC])</h2>
<h3>Zheng Chai, Yujing Chen, Liang Zhao, Yue Cheng, Huzefa Rangwala</h3>
<p>Federated learning (FL) involves training a model over massive distributed
devices, while keeping the training data localized. This form of collaborative
learning exposes new tradeoffs among model convergence speed, model accuracy,
balance across clients, and communication cost, with new challenges including:
(1) straggler problem, where the clients lag due to data or (computing and
network) resource heterogeneity, and (2) communication bottleneck, where a
large number of clients communicate their local updates to a central server and
bottleneck the server. Many existing FL methods focus on optimizing along only
one dimension of the tradeoff space. Existing solutions use asynchronous model
updating or tiering-based synchronous mechanisms to tackle the straggler
problem. However, the asynchronous methods can easily create a network
communication bottleneck, while tiering may introduce biases as tiering favors
faster tiers with shorter response latencies. To address these issues, we
present FedAT, a novel Federated learning method with Asynchronous Tiers under
Non-i.i.d. data. FedAT synergistically combines synchronous intra-tier training
and asynchronous cross-tier training. By bridging the synchronous and
asynchronous training through tiering, FedAT minimizes the straggler effect
with improved convergence speed and test accuracy. FedAT uses a
straggler-aware, weighted aggregation heuristic to steer and balance the
training for further accuracy improvement. FedAT compresses the uplink and
downlink communications using an efficient, polyline-encoding-based compression
algorithm, therefore minimizing the communication cost. Results show that FedAT
improves the prediction performance by up to 21.09%, and reduces the
communication cost by up to 8.5x, compared to state-of-the-art FL methods.
</p>
<a href="http://arxiv.org/abs/2010.05958" target="_blank">arXiv:2010.05958</a> [<a href="http://arxiv.org/pdf/2010.05958" target="_blank">pdf</a>]

<h2>The Zero Resource Speech Challenge 2020: Discovering discrete subword and word units. (arXiv:2010.05967v1 [cs.CL])</h2>
<h3>Ewan Dunbar, Julien Karadayi, Mathieu Bernard, Xuan-Nga Cao, Robin Algayres, Lucas Ondel, Laurent Besacier, Sakriani Sakti, Emmanuel Dupoux</h3>
<p>We present the Zero Resource Speech Challenge 2020, which aims at learning
speech representations from raw audio signals without any labels. It combines
the data sets and metrics from two previous benchmarks (2017 and 2019) and
features two tasks which tap into two levels of speech representation. The
first task is to discover low bit-rate subword representations that optimize
the quality of speech synthesis; the second one is to discover word-like units
from unsegmented raw speech. We present the results of the twenty submitted
models and discuss the implications of the main findings for unsupervised
speech learning.
</p>
<a href="http://arxiv.org/abs/2010.05967" target="_blank">arXiv:2010.05967</a> [<a href="http://arxiv.org/pdf/2010.05967" target="_blank">pdf</a>]

<h2>Monitoring War Destruction from Space: A Machine Learning Approach. (arXiv:2010.05970v1 [econ.GN])</h2>
<h3>Hannes Mueller, Andre Groger, Jonathan Hersh, Andrea Matranga, Joan Serrat</h3>
<p>Existing data on building destruction in conflict zones rely on eyewitness
reports or manual detection, which makes it generally scarce, incomplete and
potentially biased. This lack of reliable data imposes severe limitations for
media reporting, humanitarian relief efforts, human rights monitoring,
reconstruction initiatives, and academic studies of violent conflict. This
article introduces an automated method of measuring destruction in
high-resolution satellite images using deep learning techniques combined with
data augmentation to expand training samples. We apply this method to the
Syrian civil war and reconstruct the evolution of damage in major cities across
the country. The approach allows generating destruction data with unprecedented
scope, resolution, and frequency - only limited by the available satellite
imagery - which can alleviate data limitations decisively.
</p>
<a href="http://arxiv.org/abs/2010.05970" target="_blank">arXiv:2010.05970</a> [<a href="http://arxiv.org/pdf/2010.05970" target="_blank">pdf</a>]

<h2>Signal classification using weighted orthogonal regression method. (arXiv:2010.05979v1 [cs.LG])</h2>
<h3>Sahar Tavakoli</h3>
<p>In this paper, a new classifier based on the intrinsic properties of the data
is proposed. Classification is an essential task in data mining-based
applications. The classification problem will be challenging when the size of
the training set is not sufficient to compare to the dimension of the problem.
This paper proposes a new classification method that exploits the intrinsic
structure of each class through the corresponding Eigen components. Each
component contributes to the learned span of each class by specific weight. The
weight is determined by the associated eigenvalue. This approach results in
reliable learning robust in the case of facing a classification problem with
limited training data. The proposed method involves the obtained Eigenvectors
by SVD of data from each class to select the bases for each subspace. Moreover,
it considers an efficient weighting for the decision-making criterion to
discriminate two classes. In addition to high performance on artificial data,
this method has increased the best result of international competition.
</p>
<a href="http://arxiv.org/abs/2010.05979" target="_blank">arXiv:2010.05979</a> [<a href="http://arxiv.org/pdf/2010.05979" target="_blank">pdf</a>]

<h2>Shape-Texture Debiased Neural Network Training. (arXiv:2010.05981v1 [cs.CV])</h2>
<h3>Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie</h3>
<p>Shape and texture are two prominent and complementary cues for recognizing
objects. Nonetheless, Convolutional Neural Networks are often biased towards
either texture or shape, depending on the training dataset. Our ablation shows
that such bias degenerates model performance. Motivated by this observation, we
develop a simple algorithm for shape-texture debiased learning. To prevent
models from exclusively attending on a single cue in representation learning,
we augment training data with images with conflicting shape and texture
information (e.g., an image of chimpanzee shape but with lemon texture) and,
most importantly, provide the corresponding supervisions from shape and texture
simultaneously.

Experiments show that our method successfully improves model performance on
several image recognition benchmarks and adversarial robustness. For example,
by training on ImageNet, it helps ResNet-152 achieve substantial improvements
on ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%) and
Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker
on ImageNet (+14.4%). Our method also claims to be compatible to other advanced
data augmentation strategies, e.g., Mixup and CutMix. The code is available
here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.
</p>
<a href="http://arxiv.org/abs/2010.05981" target="_blank">arXiv:2010.05981</a> [<a href="http://arxiv.org/pdf/2010.05981" target="_blank">pdf</a>]

<h2>How does Weight Correlation Affect the Generalisation Ability of Deep Neural Networks. (arXiv:2010.05983v1 [cs.LG])</h2>
<h3>Gaojie Jin, Xinping Yi, Liang Zhang, Lijun Zhang, Sven Schewe, Xiaowei Huang</h3>
<p>This paper studies the novel concept of weight correlation in deep neural
networks and discusses its impact on the networks' generalisation ability. For
fully-connected layers, the weight correlation is defined as the average cosine
similarity between weight vectors of neurons, and for convolutional layers, the
weight correlation is defined as the cosine similarity between filter matrices.
Theoretically, we show that, weight correlation can, and should, be
incorporated into the PAC Bayesian framework for the generalisation of neural
networks, and the resulting generalisation bound is monotonic with respect to
the weight correlation. We formulate a new complexity measure, which lifts the
PAC Bayes measure with weight correlation, and experimentally confirm that it
is able to rank the generalisation errors of a set of networks more precisely
than existing measures. More importantly, we develop a new regulariser for
training, and provide extensive experiments that show that the generalisation
error can be greatly reduced with our novel approach.
</p>
<a href="http://arxiv.org/abs/2010.05983" target="_blank">arXiv:2010.05983</a> [<a href="http://arxiv.org/pdf/2010.05983" target="_blank">pdf</a>]

<h2>Chatbot Interaction with Artificial Intelligence: Human Data Augmentation with T5 and Language Transformer Ensemble for Text Classification. (arXiv:2010.05990v1 [cs.CL])</h2>
<h3>Jordan J. Bird, Anik&#xf3; Ek&#xe1;rt, Diego R. Faria</h3>
<p>In this work, we present the Chatbot Interaction with Artificial Intelligence
(CI-AI) framework as an approach to the training of deep learning chatbots for
task classification. The intelligent system augments human-sourced data via
artificial paraphrasing in order to generate a large set of training data for
further classical, attention, and language transformation-based learning
approaches for Natural Language Processing. Human beings are asked to
paraphrase commands and questions for task identification for further execution
of a machine. The commands and questions are split into training and validation
sets. A total of 483 responses were recorded. Secondly, the training set is
paraphrased by the T5 model in order to augment it with further data. Seven
state-of-the-art transformer-based text classification algorithms (BERT,
DistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are
benchmarked for both sets after fine-tuning on the training data for two
epochs. We find that all models are improved when training data is augmented by
the T5 model, with an average increase of classification accuracy by 4.01%. The
best result was the RoBERTa model trained on T5 augmented data which achieved
98.96% classification accuracy. Finally, we found that an ensemble of the five
best-performing transformer models via Logistic Regression of output label
predictions led to an accuracy of 99.59% on the dataset of human responses. A
highly-performing model allows the intelligent system to interpret human
commands at the social-interaction level through a chatbot-like interface (e.g.
"Robot, can we have a conversation?") and allows for better accessibility to AI
by non-technical users.
</p>
<a href="http://arxiv.org/abs/2010.05990" target="_blank">arXiv:2010.05990</a> [<a href="http://arxiv.org/pdf/2010.05990" target="_blank">pdf</a>]

<h2>Improving Text Generation with Student-Forcing Optimal Transport. (arXiv:2010.05994v1 [cs.CL])</h2>
<h3>Guoyin Wang, Chunyuan Li, Jianqiao Li, Hao Fu, Yuh-Chen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Dinghan Shen, Qian Yang, Lawrence Carin</h3>
<p>Neural language models are often trained with maximum likelihood estimation
(MLE), where the next word is generated conditioned on the ground-truth word
tokens. During testing, however, the model is instead conditioned on previously
generated tokens, resulting in what is termed exposure bias. To reduce this gap
between training and testing, we propose using optimal transport (OT) to match
the sequences generated in these two modes. An extension is further proposed to
improve the OT learning, based on the structural and contextual information of
the text sequences. The effectiveness of the proposed method is validated on
machine translation, text summarization, and text generation tasks.
</p>
<a href="http://arxiv.org/abs/2010.05994" target="_blank">arXiv:2010.05994</a> [<a href="http://arxiv.org/pdf/2010.05994" target="_blank">pdf</a>]

<h2>Thinking Fast and Slow in AI. (arXiv:2010.06002v1 [cs.AI])</h2>
<h3>Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner, Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, Biplav Srivastava</h3>
<p>This paper proposes a research direction to advance AI which draws
inspiration from cognitive theories of human decision making. The premise is
that if we gain insights about the causes of some human capabilities that are
still lacking in AI (for instance, adaptability, generalizability, common
sense, and causal reasoning), we may obtain similar capabilities in an AI
system by embedding these causal components. We hope that the high-level
description of our vision included in this paper, as well as the several
research questions that we propose to consider, can stimulate the AI research
community to define, try and evaluate new methodologies, frameworks, and
evaluation metrics, in the spirit of achieving a better understanding of both
human and machine intelligence.
</p>
<a href="http://arxiv.org/abs/2010.06002" target="_blank">arXiv:2010.06002</a> [<a href="http://arxiv.org/pdf/2010.06002" target="_blank">pdf</a>]

<h2>The Cone of Silence: Speech Separation by Localization. (arXiv:2010.06007v1 [cs.SD])</h2>
<h3>Teerapat Jenrungrot, Vivek Jayaram, Steve Seitz, Ira Kemelmacher-Shlizerman</h3>
<p>Given a multi-microphone recording of an unknown number of speakers talking
concurrently, we simultaneously localize the sources and separate the
individual speakers. At the core of our method is a deep network, in the
waveform domain, which isolates sources within an angular region $\theta \pm
w/2$, given an angle of interest $\theta$ and angular window size $w$. By
exponentially decreasing $w$, we can perform a binary search to localize and
separate all sources in logarithmic time. Our algorithm allows for an arbitrary
number of potentially moving speakers at test time, including more speakers
than seen during training. Experiments demonstrate state-of-the-art performance
for both source separation and source localization, particularly in high levels
of background noise.
</p>
<a href="http://arxiv.org/abs/2010.06007" target="_blank">arXiv:2010.06007</a> [<a href="http://arxiv.org/pdf/2010.06007" target="_blank">pdf</a>]

<h2>Machine Learning for Material Characterization with an Application for Predicting Mechanical Properties. (arXiv:2010.06010v1 [cs.LG])</h2>
<h3>Anke Stoll, Peter Benner</h3>
<p>Currently, the growth of material data from experiments and simulations is
expanding beyond processable amounts. This makes the development of new
data-driven methods for the discovery of patterns among multiple lengthscales
and time-scales and structure-property relationships essential. These
data-driven approaches show enormous promise within materials science. The
following review covers machine learning applications for metallic material
characterization. Many parameters associated with the processing and the
structure of materials affect the properties and the performance of
manufactured components. Thus, this study is an attempt to investigate the
usefulness of machine learning methods for material property prediction.
Material characteristics such as strength, toughness, hardness, brittleness or
ductility are relevant to categorize a material or component according to their
quality. In industry, material tests like tensile tests, compression tests or
creep tests are often time consuming and expensive to perform. Therefore, the
application of machine learning approaches is considered helpful for an easier
generation of material property information. This study also gives an
application of machine learning methods on small punch test data for the
determination of the property ultimate tensile strength for various materials.
A strong correlation between small punch test data and tensile test data was
found which ultimately allows to replace more costly tests by simple and fast
tests in combination with machine learning.
</p>
<a href="http://arxiv.org/abs/2010.06010" target="_blank">arXiv:2010.06010</a> [<a href="http://arxiv.org/pdf/2010.06010" target="_blank">pdf</a>]

<h2>Probabilistic Social Learning Improves the Public's Detection of Misinformation. (arXiv:2010.06019v1 [cs.SI])</h2>
<h3>Douglas Guilbeault, Samuel Woolley, Joshua Becker</h3>
<p>The digital spread of misinformation is one of the leading threats to
democracy, public health, and the global economy. Popular strategies for
mitigating misinformation include crowdsourcing, machine learning, and media
literacy programs that require social media users to classify news in binary
terms as either true or false. However, research on peer influence suggests
that framing decisions in binary terms can amplify judgment errors and limit
social learning, whereas framing decisions in probabilistic terms can reliably
improve judgments. In this preregistered experiment, we compare online peer
networks that collaboratively evaluate the veracity of news by communicating
either binary or probabilistic judgments. Exchanging probabilistic estimates of
news veracity substantially improved individuals and group judgments, with the
effect of eliminating polarization in news evaluation. By contrast, exchanging
binary classifications reduced social learning and entrenched polarization. The
benefits of probabilistic social learning are robust to participants'
education, gender, race, income, religion, and partisanship.
</p>
<a href="http://arxiv.org/abs/2010.06019" target="_blank">arXiv:2010.06019</a> [<a href="http://arxiv.org/pdf/2010.06019" target="_blank">pdf</a>]

<h2>Assessing Lesion Segmentation Bias of Neural Networks on Motion Corrupted Brain MRI. (arXiv:2010.06027v1 [eess.IV])</h2>
<h3>Tejas Sudharshan Mathai, Yi Wang, Nathan Cross</h3>
<p>Patient motion during the magnetic resonance imaging (MRI) acquisition
process results in motion artifacts, which limits the ability of radiologists
to provide a quantitative assessment of a condition visualized. Often times,
radiologists either "see through" the artifacts with reduced diagnostic
confidence, or the MR scans are rejected and patients are asked to be recalled
and re-scanned. Presently, there are many published approaches that focus on
MRI artifact detection and correction. However, the key question of the bias
exhibited by these algorithms on motion corrupted MRI images is still
unanswered. In this paper, we seek to quantify the bias in terms of the impact
that different levels of motion artifacts have on the performance of neural
networks engaged in a lesion segmentation task. Additionally, we explore the
effect of a different learning strategy, curriculum learning, on the
segmentation performance. Our results suggest that a network trained using
curriculum learning is effective at compensating for different levels of motion
artifacts, and improved the segmentation performance by ~9%-15% (p &lt; 0.05) when
compared against a conventional shuffled learning strategy on the same motion
data. Within each motion category, it either improved or maintained the dice
score. To the best of our knowledge, we are the first to quantitatively assess
the segmentation bias on various levels of motion artifacts present in a brain
MRI image.
</p>
<a href="http://arxiv.org/abs/2010.06027" target="_blank">arXiv:2010.06027</a> [<a href="http://arxiv.org/pdf/2010.06027" target="_blank">pdf</a>]

<h2>A translational pathway of deep learning methods in GastroIntestinal Endoscopy. (arXiv:2010.06034v1 [cs.CV])</h2>
<h3>Sharib Ali, Mariia Dmitrieva, Noha Ghatwary, Sophia Bano, Gorkem Polat, Alptekin Temizel, Adrian Krenzer, Amar Hekalo, Yun Bo Guo, Bogdan Matuszewski, Mourad Gridach, Irina Voiculescu, Vishnusai Yoganand, Arnav Chavan, Aryan Raj, Nhan T. Nguyen, Dat Q. Tran, Le Duy Huynh, Nicolas Boutry, Shahadate Rezvy, Haijian Chen, Yoon Ho Choi, Anand Subramanian, Velmurugan Balasubramanian, Xiaohong W. Gao, Hongyu Hu, Yusheng Liao, Danail Stoyanov, Christian Daul, Stefano Realdon, Renato Cannizzaro, Dominique Lamarque, Terry Tran-Nguyen, Adam Bailey, Barbara Braden, James East, Jens Rittscher</h3>
<p>The Endoscopy Computer Vision Challenge (EndoCV) is a crowd-sourcing
initiative to address eminent problems in developing reliable computer aided
detection and diagnosis endoscopy systems and suggest a pathway for clinical
translation of technologies. Whilst endoscopy is a widely used diagnostic and
treatment tool for hollow-organs, there are several core challenges often faced
by endoscopists, mainly: 1) presence of multi-class artefacts that hinder their
visual interpretation, and 2) difficulty in identifying subtle precancerous
precursors and cancer abnormalities. Artefacts often affect the robustness of
deep learning methods applied to the gastrointestinal tract organs as they can
be confused with tissue of interest. EndoCV2020 challenges are designed to
address research questions in these remits. In this paper, we present a summary
of methods developed by the top 17 teams and provide an objective comparison of
state-of-the-art methods and methods designed by the participants for two
sub-challenges: i) artefact detection and segmentation (EAD2020), and ii)
disease detection and segmentation (EDD2020). Multi-center, multi-organ,
multi-class, and multi-modal clinical endoscopy datasets were compiled for both
EAD2020 and EDD2020 sub-challenges. An out-of-sample generalisation ability of
detection algorithms was also evaluated. Whilst most teams focused on accuracy
improvements, only a few methods hold credibility for clinical usability. The
best performing teams provided solutions to tackle class imbalance, and
variabilities in size, origin, modality and occurrences by exploring data
augmentation, data fusion, and optimal class thresholding techniques.
</p>
<a href="http://arxiv.org/abs/2010.06034" target="_blank">arXiv:2010.06034</a> [<a href="http://arxiv.org/pdf/2010.06034" target="_blank">pdf</a>]

<h2>Artificial Intelligence, speech and language processing approaches to monitoring Alzheimer's Disease: a systematic review. (arXiv:2010.06047v1 [cs.AI])</h2>
<h3>Sofia de la Fuente Garcia, Craig Ritchie, Saturnino Luz</h3>
<p>Language is a valuable source of clinical information in Alzheimer's Disease,
as it declines concurrently with neurodegeneration. Consequently, speech and
language data have been extensively studied in connection with its diagnosis.
This paper summarises current findings on the use of artificial intelligence,
speech and language processing to predict cognitive decline in the context of
Alzheimer's Disease, detailing current research procedures, highlighting their
limitations and suggesting strategies to address them. We conducted a
systematic review of original research between 2000 and 2019, registered in
PROSPERO (reference CRD42018116606). An interdisciplinary search covered six
databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine
(PubMed and Embase) and Web of Science. Bibliographies of relevant papers were
screened until December 2019. From 3,654 search results 51 articles were
selected against the eligibility criteria. Four tables summarise their
findings: study details (aim, population, interventions, comparisons, methods
and outcomes), data details (size, type, modalities, annotation, balance,
availability and language of study), methodology (pre-processing, feature
generation, machine learning, evaluation and results) and clinical
applicability (research implications, clinical potential, risk of bias and
strengths/limitations). While promising results are reported across nearly all
51 studies, very few have been implemented in clinical research or practice. We
concluded that the main limitations of the field are poor standardisation,
limited comparability of results, and a degree of disconnect between study aims
and clinical applications. Attempts to close these gaps should support
translation of future research into clinical practice.
</p>
<a href="http://arxiv.org/abs/2010.06047" target="_blank">arXiv:2010.06047</a> [<a href="http://arxiv.org/pdf/2010.06047" target="_blank">pdf</a>]

<h2>A Physics-Guided Neural Netwrok Framework for Elastic Plates: Comparison of Governing Equations-Based and Energy-Based Approaches. (arXiv:2010.06050v1 [cs.CE])</h2>
<h3>Wei Li, Juner Zhu, Martin Z. Bazant</h3>
<p>One of the obstacles hindering the scaling-up of the initial successes of
machine learning in practical engineering applications is the dependence of the
accuracy on the size of the database that "drives" the algorithms.
Incorporating the already-known physical laws into the training process can
significantly reduce the size of the required database. In this study, we
establish a neural network-based computational framework to characterize the
finite deformation of elastic plates, which in classic theories is described by
the F\"oppl--von K\'arm\'an (FvK) equations with a set of boundary conditions
(BCs). A neural network is constructed by taking the spatial coordinates as the
input and the displacement field as the output to approximate the exact
solution of the FvK equations. The physical information (PDEs, BCs, and
potential energies) is then incorporated into the loss function, and a pseudo
dataset is sampled without knowing the exact solution to finally train the
neural network. The prediction accuracy of the modeling framework is carefully
examined by applying it to four different loading cases: in-plane tension with
non-uniformly distributed stretching forces, in-plane central-hole tension,
out-of-plane deflection, and buckling under compression. Two ways of
formulating the loss function are compared, one based on the PDEs and BCs, and
the other based on the total potential energy of the plate. Through the
comparison with the finite element simulation results, it is found that our
computational framework is capable of characterizing the elastic deformation of
plates with a satisfactory accuracy. Compared with incorporating the PDEs and
BCs in the loss, using the total potential energy is a better way in terms of
training accuracy and efficiency.
</p>
<a href="http://arxiv.org/abs/2010.06050" target="_blank">arXiv:2010.06050</a> [<a href="http://arxiv.org/pdf/2010.06050" target="_blank">pdf</a>]

<h2>TextHide: Tackling Data Privacy in Language Understanding Tasks. (arXiv:2010.06053v1 [cs.CL])</h2>
<h3>Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora</h3>
<p>An unsolved challenge in distributed or federated learning is to effectively
mitigate privacy risks without slowing down training or reducing accuracy. In
this paper, we propose TextHide aiming at addressing this challenge for natural
language understanding tasks. It requires all participants to add a simple
encryption step to prevent an eavesdropping attacker from recovering private
text data. Such an encryption step is efficient and only affects the task
performance slightly. In addition, TextHide fits well with the popular
framework of fine-tuning pre-trained language models (e.g., BERT) for any
sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and
our experiments show that TextHide can effectively defend attacks on shared
gradients or representations and the averaged accuracy reduction is only
$1.9\%$. We also present an analysis of the security of TextHide using a
conjecture about the computational intractability of a mathematical problem.

Our code is available at https://github.com/Hazelsuko07/TextHide
</p>
<a href="http://arxiv.org/abs/2010.06053" target="_blank">arXiv:2010.06053</a> [<a href="http://arxiv.org/pdf/2010.06053" target="_blank">pdf</a>]

<h2>BioMegatron: Larger Biomedical Domain Language Model. (arXiv:2010.06060v1 [cs.CL])</h2>
<h3>Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani</h3>
<p>There has been an influx of biomedical domain-specific language models,
showing language models pre-trained on biomedical text perform better on
biomedical domain benchmarks than those trained on general domain text corpora
such as Wikipedia and Books. Yet, most works do not study the factors affecting
each domain language application deeply. Additionally, the study of model size
on domain-specific models has been mostly missing. We empirically study and
evaluate several factors that can affect performance on domain language
applications, such as the sub-word vocabulary set, model size, pre-training
corpus, and domain transfer. We show consistent improvements on benchmarks with
our larger BioMegatron model trained on a larger domain corpus, contributing to
our understanding of domain language model applications. We demonstrate
noticeable improvements over the previous state-of-the-art (SOTA) on standard
biomedical NLP benchmarks of named entity recognition, relation extraction, and
question answering. Model checkpoints and code are available at
[ngc.nvidia.com] and [github.com/NVIDIA/NeMo].
</p>
<a href="http://arxiv.org/abs/2010.06060" target="_blank">arXiv:2010.06060</a> [<a href="http://arxiv.org/pdf/2010.06060" target="_blank">pdf</a>]

<h2>A catalog of broad morphology of Pan-STARRS galaxies based on deep learning. (arXiv:2010.06073v1 [astro-ph.GA])</h2>
<h3>Hunter Goddard, Lior Shamir</h3>
<p>Autonomous digital sky surveys such as Pan-STARRS have the ability to image a
very large number of galactic and extra-galactic objects, and the large and
complex nature of the image data reinforces the use of automation. Here we
describe the design and implementation of a data analysis process for automatic
broad morphology annotation of galaxies, and applied it to the data of
Pan-STARRS DR1. The process is based on filters followed by a two-step
convolutional neural network (CNN) classification. Training samples are
generated by using an augmented and balanced set of manually classified
galaxies. Results are evaluated for accuracy by comparison to the annotation of
Pan-STARRS included in a previous broad morphology catalog of SDSS galaxies.
Our analysis shows that a CNN combined with several filters is an effective
approach for annotating the galaxies and removing unclean images. The catalog
contains morphology labels for 1,662,190 galaxies with ~95% accuracy. The
accuracy can be further improved by selecting labels above certain confidence
thresholds. The catalog is publicly available.
</p>
<a href="http://arxiv.org/abs/2010.06073" target="_blank">arXiv:2010.06073</a> [<a href="http://arxiv.org/pdf/2010.06073" target="_blank">pdf</a>]

<h2>Oort: Informed Participant Selection for Scalable Federated Learning. (arXiv:2010.06081v1 [cs.LG])</h2>
<h3>Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, Mosharaf Chowdhury</h3>
<p>Federated Learning (FL) is an emerging direction in distributed machine
learning (ML) that enables in-situ model training and testing on edge data.
Despite having the same end goals as traditional ML, FL executions differ
significantly in scale, spanning thousands to millions of participating
devices. As a result, data characteristics and device capabilities vary widely
across clients. Yet, existing efforts randomly select FL participants, which
leads to poor model and system efficiency.

In this paper, we propose Kuiper to improve the performance of federated
training and testing with guided participant selection. With an aim to improve
time-to-accuracy performance in model training, Kuiper prioritizes the use of
those clients who have both data that offers the greatest utility in improving
model accuracy and the capability to run training quickly. To enable FL
developers to interpret their results in model testing, Kuiper enforces their
requirements on the distribution of participant data while improving the
duration of federated testing by cherry-picking clients. Our evaluation shows
that, compared to existing participant selection mechanisms, Kuiper improves
time-to-accuracy performance by 1.2x-14.1x and final model accuracy by
1.3%-9.8%, while efficiently enforcing developer requirements on data
distributions at the scale of millions of clients.
</p>
<a href="http://arxiv.org/abs/2010.06081" target="_blank">arXiv:2010.06081</a> [<a href="http://arxiv.org/pdf/2010.06081" target="_blank">pdf</a>]

<h2>Contrast and Classify: Alternate Training for Robust VQA. (arXiv:2010.06087v1 [cs.CV])</h2>
<h3>Yash Kant, Abhinav Moudgil, Dhruv Batra, Devi Parikh, Harsh Agrawal</h3>
<p>Recent Visual Question Answering (VQA) models have shown impressive
performance on the VQA benchmark but remain sensitive to small linguistic
variations in input questions. Existing approaches address this by augmenting
the dataset with question paraphrases from visual question generation models or
adversarial perturbations. These approaches use the combined data to learn an
answer classifier by minimizing the standard cross-entropy loss. To more
effectively leverage the augmented data, we build on the recent success in
contrastive learning. We propose a novel training paradigm (ConCAT) that
alternately optimizes cross-entropy and contrastive losses. The contrastive
loss encourages representations to be robust to linguistic variations in
questions while the cross-entropy loss preserves the discriminative power of
the representations for answer classification. We find that alternately
optimizing both losses is key to effective training. VQA models trained with
ConCAT achieve higher consensus scores on the VQA-Rephrasings dataset as well
as higher VQA accuracy on the VQA 2.0 dataset compared to existing approaches
across a variety of data augmentation strategies.
</p>
<a href="http://arxiv.org/abs/2010.06087" target="_blank">arXiv:2010.06087</a> [<a href="http://arxiv.org/pdf/2010.06087" target="_blank">pdf</a>]

<h2>Attn-HybridNet: Improving Discriminability of Hybrid Features with Attention Fusion. (arXiv:2010.06096v1 [cs.CV])</h2>
<h3>Sunny Verma, Chen Wang, Liming Zhu, Wei Liu</h3>
<p>The principal component analysis network (PCANet) is an unsupervised
parsimonious deep network, utilizing principal components as filters in its
convolution layers. Albeit powerful, the PCANet consists of basic operations
such as principal components and spatial pooling, which suffers from two
fundamental problems. First, the principal components obtain information by
transforming it to column vectors (which we call the amalgamated view), which
incurs the loss of the spatial information in the data. Second, the generalized
spatial pooling utilized in the PCANet induces feature redundancy and also
fails to accommodate spatial statistics of natural images. In this research, we
first propose a tensor-factorization based deep network called the Tensor
Factorization Network (TFNet). The TFNet extracts features from the spatial
structure of the data (which we call the minutiae view). We then show that the
information obtained by the PCANet and the TFNet are distinctive and
non-trivial but individually insufficient. This phenomenon necessitates the
development of proposed HybridNet, which integrates the information discovery
with the two views of the data. To enhance the discriminability of hybrid
features, we propose Attn-HybridNet, which alleviates the feature redundancy by
performing attention-based feature fusion. The significance of our proposed
Attn-HybridNet is demonstrated on multiple real-world datasets where the
features obtained with Attn-HybridNet achieves better classification
performance over other popular baseline methods, demonstrating the
effectiveness of the proposed technique.
</p>
<a href="http://arxiv.org/abs/2010.06096" target="_blank">arXiv:2010.06096</a> [<a href="http://arxiv.org/pdf/2010.06096" target="_blank">pdf</a>]

<h2>Infant Pose Learning with Small Data. (arXiv:2010.06100v1 [cs.CV])</h2>
<h3>Xiaofei Huang, Nihang Fu, Sarah Ostadabbas</h3>
<p>With the increasing maturity of the human pose estimation domain, its
applications have become more and more broaden. Yet, the state-of-the-art pose
estimation models performance degrades significantly in the applications that
include novel subjects or poses, such as infants with their unique movements.
Infant motion analysis is a topic with critical importance in child health and
developmental studies. However, models trained on large-scale adult pose
datasets are barely successful in estimating infant poses due to significant
differences in their body ratio and the versatility of poses they can take
compared to adults. Moreover, the privacy and security considerations hinder
the availability of enough infant images required for training a robust pose
estimation model from scratch. Here, we propose a fine-tuned domain-adapted
infant pose (FiDIP) estimation model, that transfers the knowledge of adult
poses into estimating infant pose with the supervision of a domain adaptation
technique on a mixed real and synthetic infant pose dataset. In developing
FiDIP, we also built a synthetic and real infant pose (SyRIP) dataset with
diverse and fully-annotated real infant images and generated synthetic infant
images. We demonstrated that our FiDIP model outperforms other state-of-the-art
human pose estimation model for the infant pose estimation, with the mean
average precision (AP) as high as 92.2.
</p>
<a href="http://arxiv.org/abs/2010.06100" target="_blank">arXiv:2010.06100</a> [<a href="http://arxiv.org/pdf/2010.06100" target="_blank">pdf</a>]

<h2>Machine learning for the diagnosis of Parkinson's disease: A systematic review. (arXiv:2010.06101v1 [cs.LG])</h2>
<h3>Jie Mei, Christian Desrosiers, Johannes Frasnelli</h3>
<p>Diagnosis of Parkinson's disease (PD) is commonly based on medical
observations and assessment of clinical signs, including the characterization
of a variety of motor symptoms. However, traditional diagnostic approaches may
suffer from subjectivity as they rely on the evaluation of movements that are
sometimes subtle to human eyes and therefore difficult to classify, leading to
possible misclassification. In the meantime, early non-motor symptoms of PD may
be mild and can be caused by many other conditions. Therefore, these symptoms
are often overlooked, making diagnosis of PD at an early stage challenging. To
address these difficulties and to refine the diagnosis and assessment
procedures of PD, machine learning methods have been implemented for the
classification of PD and healthy controls or patients with similar clinical
presentations (e.g., movement disorders or other Parkinsonian syndromes). To
provide a comprehensive overview of data modalities and machine learning
methods that have been used in the diagnosis and differential diagnosis of PD,
in this study, we conducted a systematic literature review of studies published
until February 14, 2020, using the PubMed and IEEE Xplore databases. A total of
209 studies were included, extracted for relevant information and presented in
this systematic review, with an investigation of their aims, sources of data,
types of data, machine learning methods and associated outcomes. These studies
demonstrate a high potential for adaptation of machine learning methods and
novel biomarkers in clinical decision making, leading to increasingly
systematic, informed diagnosis of PD.
</p>
<a href="http://arxiv.org/abs/2010.06101" target="_blank">arXiv:2010.06101</a> [<a href="http://arxiv.org/pdf/2010.06101" target="_blank">pdf</a>]

<h2>Universal Model for 3D Medical Image Analysis. (arXiv:2010.06107v1 [cs.CV])</h2>
<h3>Xiaoman Zhang, Ya Zhang, Xiaoyun Zhang, Yanfeng Wang</h3>
<p>Deep Learning-based methods recently have achieved remarkable progress in
medical image analysis, but heavily rely on massive amounts of labeled training
data. Transfer learning from pre-trained models has been proposed as a standard
pipeline on medical image analysis to address this bottleneck. Despite their
success, the existing pre-trained models are mostly not tuned for multi-modal
multi-task generalization in medical domains. Specifically, their training data
are either from non-medical domain or in single modality, failing to attend to
the problem of performance degradation with cross-modal transfer. Furthermore,
there is no effort to explicitly extract multi-level features required by a
variety of downstream tasks. To overcome these limitations, we propose
Universal Model, a transferable and generalizable pre-trained model for 3D
medical image analysis. A unified self-supervised learning scheme is leveraged
to learn representations from multiple unlabeled source datasets with different
modalities and distinctive scan regions. A modality invariant adversarial
learning module is further introduced to improve the cross-modal
generalization. To fit a wide range of tasks, a simple yet effective scale
classifier is incorporated to capture multi-level visual representations. To
validate the effectiveness of the Universal Model, we perform extensive
experimental analysis on five target tasks, covering multiple imaging
modalities, distinctive scan regions, and different analysis tasks. Compared
with both public 3D pre-trained models and newly investigated 3D
self-supervised learning methods, Universal Model demonstrates superior
generalizability, manifested by its higher performance, stronger robustness and
faster convergence. The pre-trained Universal Model is available at:
\href{https://github.com/xm-cmic/Universal-Model}{https://github.com/xm-cmic/Universal-Model}.
</p>
<a href="http://arxiv.org/abs/2010.06107" target="_blank">arXiv:2010.06107</a> [<a href="http://arxiv.org/pdf/2010.06107" target="_blank">pdf</a>]

<h2>FaiR-N: Fair and Robust Neural Networks for Structured Data. (arXiv:2010.06113v1 [cs.LG])</h2>
<h3>Shubham Sharma, Alan H. Gee, David Paydarfar, Joydeep Ghosh</h3>
<p>Fairness in machine learning is crucial when individuals are subject to
automated decisions made by models in high-stake domains. Organizations that
employ these models may also need to satisfy regulations that promote
responsible and ethical A.I. While fairness metrics relying on comparing model
error rates across subpopulations have been widely investigated for the
detection and mitigation of bias, fairness in terms of the equalized ability to
achieve recourse for different protected attribute groups has been relatively
unexplored. We present a novel formulation for training neural networks that
considers the distance of data points to the decision boundary such that the
new objective: (1) reduces the average distance to the decision boundary
between two groups for individuals subject to a negative outcome in each group,
i.e. the network is more fair with respect to the ability to obtain recourse,
and (2) increases the average distance of data points to the boundary to
promote adversarial robustness. We demonstrate that training with this loss
yields more fair and robust neural networks with similar accuracies to models
trained without it. Moreover, we qualitatively motivate and empirically show
that reducing recourse disparity across groups also improves fairness measures
that rely on error rates. To the best of our knowledge, this is the first time
that recourse capabilities across groups are considered to train fairer neural
networks, and a relation between error rates based fairness and recourse based
fairness is investigated.
</p>
<a href="http://arxiv.org/abs/2010.06113" target="_blank">arXiv:2010.06113</a> [<a href="http://arxiv.org/pdf/2010.06113" target="_blank">pdf</a>]

<h2>Mass Estimation in Manipulation Tasks of Domestic Service Robots using Fault Reconstruction Techniques. (arXiv:2010.06116v1 [cs.RO])</h2>
<h3>Marco Negrete, Jes&#xfa;s Savage, Jos&#xe9; Avenda&#xf1;o</h3>
<p>Manipulation is a key capability in domestic service robots, as can be seen
in the rulebooks of last Robocup@Home editions. Currently, object recognition
is performed based mostly on visual information. Some robots use also 3D
information such as point clouds or laser scans but, to the knowledge of
authors, robots don't use physical properties to improve object recognition.
Estimation of an object's weight during a manipulation task is something new in
the @Home league and such ability can improve performance of domestic service
robots. In this work we propose to estimate the weight of the grasped object
using Sliding Mode Observers. If we consider the manipulator without load as
the nominal system and object's weight as a fault signal, we can estimate such
weight by an appropriate filtering of the output error injection term of the
sliding mode observer. To implement our proposal we used MATLAB and Simulink
Robotics System Toolbox, ROS Toolbox and Simscape. To improve computation time
we exported all algorithms to standalone ROS nodes from Simulink models. Tests
were performed using two platforms: Justina's left manipulator (a robot
developed at Biorobotics Laboratory, UNAM) and Neuronics Katana manipulators.
We present results in simulation and discuss the performance of the proposed
system and the possible sources of error. Finally we present our conclusions
and state the future work.
</p>
<a href="http://arxiv.org/abs/2010.06116" target="_blank">arXiv:2010.06116</a> [<a href="http://arxiv.org/pdf/2010.06116" target="_blank">pdf</a>]

<h2>Map-Based Temporally Consistent Geolocalization through Learning Motion Trajectories. (arXiv:2010.06117v1 [cs.CV])</h2>
<h3>Bing Zha, Alper Yilmaz</h3>
<p>In this paper, we propose a novel trajectory learning method that exploits
motion trajectories on topological map using recurrent neural network for
temporally consistent geolocalization of object. Inspired by human's ability to
both be aware of distance and direction of self-motion in navigation, our
trajectory learning method learns a pattern representation of trajectories
encoded as a sequence of distances and turning angles to assist
self-localization. We pose the learning process as a conditional sequence
prediction problem in which each output locates the object on a traversable
path in a map. Considering the prediction sequence ought to be topologically
connected in the graph-structured map, we adopt two different hypotheses
generation and elimination strategies to eliminate disconnected sequence
prediction. We demonstrate our approach on the KITTI stereo visual odometry
dataset which is a city-scale environment and can generate trajectory with
metric information. The key benefits of our approach to geolocalization are
that 1) we take advantage of powerful sequence modeling ability of recurrent
neural network and its robustness to noisy input, 2) only require a map in the
form of a graph and simply use an affordable sensor that generates motion
trajectory and 3) do not need initial position. The experiments show that the
motion trajectories can be learned by training an recurrent neural network, and
temporally consistent geolocation can be predicted with both of the proposed
strategies.
</p>
<a href="http://arxiv.org/abs/2010.06117" target="_blank">arXiv:2010.06117</a> [<a href="http://arxiv.org/pdf/2010.06117" target="_blank">pdf</a>]

<h2>To be Robust or to be Fair: Towards Fairness in Adversarial Training. (arXiv:2010.06121v1 [cs.LG])</h2>
<h3>Han Xu, Xiaorui Liu, Yaxin Li, Jiliang Tang</h3>
<p>Adversarial training algorithms have been proven to be reliable to improve
machine learning models' robustness against adversarial examples. However, we
find that adversarial training algorithms tend to introduce severe disparity of
accuracy and robustness between different groups of data. For instance, PGD
adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67%
PGD $l_\infty-8$ adversarial accuracy on the class "automobile" but only 59%
and 17% on class "cat". This phenomenon happens in balanced datasets and does
not exist in naturally trained models when only using clean samples. In this
work, we theoretically show that this phenomenon can generally happen under
adversarial training algorithms which minimize DNN models' robust errors.
Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework
to mitigate this unfairness problem when doing adversarial defenses and
experimental results validate the effectiveness of FRL.
</p>
<a href="http://arxiv.org/abs/2010.06121" target="_blank">arXiv:2010.06121</a> [<a href="http://arxiv.org/pdf/2010.06121" target="_blank">pdf</a>]

<h2>Asking Crowdworkers to Write Entailment Examples: The Best of Bad Options. (arXiv:2010.06122v1 [cs.CL])</h2>
<h3>Clara Vania, Ruijie Chen, Samuel R. Bowman</h3>
<p>Large-scale natural language inference (NLI) datasets such as SNLI or MNLI
have been created by asking crowdworkers to read a premise and write three new
hypotheses, one for each possible semantic relationships (entailment,
contradiction, and neutral). While this protocol has been used to create useful
benchmark data, it remains unclear whether the writing-based annotation
protocol is optimal for any purpose, since it has not been evaluated directly.
Furthermore, there is ample evidence that crowdworker writing can introduce
artifacts in the data. We investigate two alternative protocols which
automatically create candidate (premise, hypothesis) pairs for annotators to
label. Using these protocols and a writing-based baseline, we collect several
new English NLI datasets of over 3k examples each, each using a fixed amount of
annotator time, but a varying number of examples to fit that time budget. Our
experiments on NLI and transfer learning show negative results: None of the
alternative protocols outperforms the baseline in evaluations of generalization
within NLI or on transfer to outside target tasks. We conclude that crowdworker
writing still the best known option for entailment data, highlighting the need
for further data collection work to focus on improving writing-based annotation
processes.
</p>
<a href="http://arxiv.org/abs/2010.06122" target="_blank">arXiv:2010.06122</a> [<a href="http://arxiv.org/pdf/2010.06122" target="_blank">pdf</a>]

<h2>Model Selection for Cross-Lingual Transfer using a Learned Scoring Function. (arXiv:2010.06127v1 [cs.CL])</h2>
<h3>Yang Chen, Alan Ritter</h3>
<p>Transformers that are pre-trained on multilingual text corpora, such as,
mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning
results. In the zero-shot cross-lingual transfer setting, only English training
data is assumed, and the fine-tuned model is evaluated on another target
language. No target-language validation data is assumed in this setting,
however substantial variance has been observed in target language performance
between different fine-tuning runs. Prior work has relied on English
validation/development data to select among models that are fine-tuned with
different learning rates, number of steps and other hyperparameters, often
resulting in suboptimal choices. To address this challenge, we propose a
meta-learning approach to model selection that uses the fine-tuned model's own
internal representations to predict its cross-lingual capabilities. In
extensive experiments we find that our approach consistently selects better
models than English validation data across five languages and five well-studied
NLP tasks, achieving results that are comparable to small amounts of target
language development data.
</p>
<a href="http://arxiv.org/abs/2010.06127" target="_blank">arXiv:2010.06127</a> [<a href="http://arxiv.org/pdf/2010.06127" target="_blank">pdf</a>]

<h2>Towards Understanding Pixel Vulnerability under Adversarial Attacks for Images. (arXiv:2010.06131v1 [cs.CV])</h2>
<h3>He Zhao, Trung Le, Paul Montague, Olivier De Vel, Tamas Abraham, Dinh Phung</h3>
<p>Deep neural network image classifiers are reported to be susceptible to
adversarial evasion attacks, which use carefully crafted images created to
mislead a classifier. Recently, various kinds of adversarial attack methods
have been proposed, most of which focus on adding small perturbations to all of
the pixels of a real image. We find that a considerable amount of the
perturbations on an image generated by some widely-used attacks may contribute
little in attacking a classifier. However, they usually result in a more easily
detectable adversarial image by both humans and adversarial attack detection
algorithms. Therefore, it is important to impose the perturbations on the most
vulnerable pixels of an image that can change the predictions of classifiers
more readily. With the pixel vulnerability, given an existing attack, we can
make its adversarial images more realistic and less detectable with fewer
perturbations but keep its attack performance the same. Moreover, the
discovered vulnerability assists to get a better understanding of the weakness
of deep classifiers. Derived from the information-theoretic perspective, we
propose a probabilistic approach for automatically finding the pixel
vulnerability of an image, which is compatible with and improves over many
existing adversarial attacks.
</p>
<a href="http://arxiv.org/abs/2010.06131" target="_blank">arXiv:2010.06131</a> [<a href="http://arxiv.org/pdf/2010.06131" target="_blank">pdf</a>]

<h2>Session-layer Attack Traffic Classification by Program Synthesis. (arXiv:2010.06135v1 [cs.PL])</h2>
<h3>Lei Shi, Yahui Li, Rajeev Alur, Boon Thau Loo</h3>
<p>Writing classification rules to identify malicious network traffic is a
time-consuming and error-prone task. Learning-based classification systems
automatically extract such rules from positive and negative traffic examples.
However, due to limitations in the representation of network traffic and the
learning strategy, these systems lack both expressiveness to cover a range of
attacks and interpretability in fully describing the attack traffic's structure
at the session layer. This paper presents Sharingan system, which uses program
synthesis techniques to generate network classification programs at the session
layer. Sharingan accepts raw network traces as inputs, and reports potential
patterns of the attack traffic in NetQRE, a domain specific language designed
for specifying session-layer quantitative properties. Using Sharingan, network
operators can better analyze the attack pattern due to the following advantages
of Sharingan's learning process: (1) it requires minimal feature engineering,
(2) it is amenable to efficient implementation of the learnt classifier, and
(3) the synthesized program is easy to decipher and edit. We develop a range of
novel optimizations that reduce the synthesis time for large and complex tasks
to a matter of minutes. Our experiments show that Sharingan is able to
correctly identify attacks from a diverse set of network attack traces and
generates explainable outputs, while achieving accuracy comparable to
state-of-the-art learning-based intrusion detection systems.
</p>
<a href="http://arxiv.org/abs/2010.06135" target="_blank">arXiv:2010.06135</a> [<a href="http://arxiv.org/pdf/2010.06135" target="_blank">pdf</a>]

<h2>Corruption Is Not All Bad: Incorporating Discourse Structure into Pre-training via Corruption for Essay Scoring. (arXiv:2010.06137v1 [cs.CL])</h2>
<h3>Farjana Sultana Mim, Naoya Inoue, Paul Reisert, Hiroki Ouchi, Kentaro Inui</h3>
<p>Existing approaches for automated essay scoring and document representation
learning typically rely on discourse parsers to incorporate discourse structure
into text representation. However, the performance of parsers is not always
adequate, especially when they are used on noisy texts, such as student essays.
In this paper, we propose an unsupervised pre-training approach to capture
discourse structure of essays in terms of coherence and cohesion that does not
require any discourse parser or annotation. We introduce several types of
token, sentence and paragraph-level corruption techniques for our proposed
pre-training approach and augment masked language modeling pre-training with
our pre-training method to leverage both contextualized and discourse
information. Our proposed unsupervised approach achieves new state-of-the-art
result on essay Organization scoring task.
</p>
<a href="http://arxiv.org/abs/2010.06137" target="_blank">arXiv:2010.06137</a> [<a href="http://arxiv.org/pdf/2010.06137" target="_blank">pdf</a>]

<h2>Hindsight Experience Replay with Kronecker Product Approximate Curvature. (arXiv:2010.06142v1 [cs.LG])</h2>
<h3>Dhuruva Priyan G M, Abhik Singla, Shalabh Bhatnagar</h3>
<p>Hindsight Experience Replay (HER) is one of the efficient algorithm to solve
Reinforcement Learning tasks related to sparse rewarded environments.But due to
its reduced sample efficiency and slower convergence HER fails to perform
effectively. Natural gradients solves these challenges by converging the model
parameters better. It avoids taking bad actions that collapse the training
performance. However updating parameters in neural networks requires expensive
computation and thus increase in training time. Our proposed method solves the
above mentioned challenges with better sample efficiency and faster convergence
with increased success rate. A common failure mode for DDPG is that the learned
Q-function begins to dramatically overestimate Q-values, which then leads to
the policy breaking, because it exploits the errors in the Q-function. We solve
this issue by including Twin Delayed Deep Deterministic Policy Gradients(TD3)
in HER. TD3 learns two Q-functions instead of one and it adds noise tothe
target action, to make it harder for the policy to exploit Q-function errors.
The experiments are done with the help of OpenAis Mujoco environments. Results
on these environments show that our algorithm (TDHER+KFAC) performs better
inmost of the scenarios
</p>
<a href="http://arxiv.org/abs/2010.06142" target="_blank">arXiv:2010.06142</a> [<a href="http://arxiv.org/pdf/2010.06142" target="_blank">pdf</a>]

<h2>Multi-layer Residual Sparsifying Transform (MARS) Model for Low-dose CT Image Reconstruction. (arXiv:2010.06144v1 [eess.IV])</h2>
<h3>Xikai Yang, Yong Long, Saiprasad Ravishankar</h3>
<p>Signal models based on sparse representations have received considerable
attention in recent years. On the other hand, deep models consisting of a
cascade of functional layers, commonly known as deep neural networks, have been
highly successful for the task of object classification and have been recently
introduced to image reconstruction. In this work, we develop a new image
reconstruction approach based on a novel multi-layer model learned in an
unsupervised manner by combining both sparse representations and deep models.
The proposed framework extends the classical sparsifying transform model for
images to a Multi-lAyer Residual Sparsifying transform (MARS) model, wherein
the transform domain data are jointly sparsified over layers. We investigate
the application of MARS models learned from limited regular-dose images for
low-dose CT reconstruction using Penalized Weighted Least Squares (PWLS)
optimization. We propose new formulations for multi-layer transform learning
and image reconstruction. We derive an efficient block coordinate descent
algorithm to learn the transforms across layers, in an unsupervised manner from
limited regular-dose images. The learned model is then incorporated into the
low-dose image reconstruction phase. Low-dose CT experimental results with both
the XCAT phantom and Mayo Clinic data show that the MARS model outperforms
conventional methods such as FBP and PWLS methods based on the edge-preserving
(EP) regularizer and the single-layer learned transform (ST) model, especially
in terms of reducing noise and maintaining some subtle details.
</p>
<a href="http://arxiv.org/abs/2010.06144" target="_blank">arXiv:2010.06144</a> [<a href="http://arxiv.org/pdf/2010.06144" target="_blank">pdf</a>]

<h2>Customer Support Ticket Escalation Prediction using Feature Engineering. (arXiv:2010.06145v1 [cs.SE])</h2>
<h3>Lloyd Montgomery, Daniela Damian, Tyson Bulmer, Shaikh Quader</h3>
<p>Understanding and keeping the customer happy is a central tenet of
requirements engineering. Strategies to gather, analyze, and negotiate
requirements are complemented by efforts to manage customer input after
products have been deployed. For the latter, support tickets are key in
allowing customers to submit their issues, bug reports, and feature requests.
If insufficient attention is given to support issues, however, their escalation
to management becomes time-consuming and expensive, especially for large
organizations managing hundreds of customers and thousands of support tickets.
Our work provides a step towards simplifying the job of support analysts and
managers, particularly in predicting the risk of escalating support tickets. In
a field study at our large industrial partner, IBM, we used a design science
research methodology to characterize the support process and data available to
IBM analysts in managing escalations. We then implemented these features into a
machine learning model to predict support ticket escalations. We trained and
evaluated our machine learning model on over 2.5 million support tickets and
10,000 escalations, obtaining a recall of 87.36% and an 88.23% reduction in the
workload for support analysts looking to identify support tickets at risk of
escalation. Finally, in addition to these research evaluation activities, we
compared the performance of our support ticket model with that of a model
developed with no feature engineering; the support ticket model features
outperformed the non-engineered model. The artifacts created in this research
are designed to serve as a starting place for organizations interested in
predicting support ticket escalations, and for future researchers to build on
to advance research in escalation prediction.
</p>
<a href="http://arxiv.org/abs/2010.06145" target="_blank">arXiv:2010.06145</a> [<a href="http://arxiv.org/pdf/2010.06145" target="_blank">pdf</a>]

<h2>Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance. (arXiv:2010.06150v1 [cs.CL])</h2>
<h3>Xi Chen, Nan Ding, Tomer Levinboim, Radu Soricut</h3>
<p>Recent advances in automatic evaluation metrics for text have shown that deep
contextualized word representations, such as those generated by BERT encoders,
are helpful for designing metrics that correlate well with human judgements. At
the same time, it has been argued that contextualized word representations
exhibit sub-optimal statistical properties for encoding the true similarity
between words or sentences. In this paper, we present two techniques for
improving encoding representations for similarity metrics: a batch-mean
centering strategy that improves statistical properties; and a computationally
efficient tempered Word Mover Distance, for better fusion of the information in
the contextualized word representations. We conduct numerical experiments that
demonstrate the robustness of our techniques, reporting results over various
BERT-backbone learned metrics and achieving state of the art correlation with
human ratings on several benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.06150" target="_blank">arXiv:2010.06150</a> [<a href="http://arxiv.org/pdf/2010.06150" target="_blank">pdf</a>]

<h2>Real-Time Detection of Simulator Sickness in Virtual Reality Games Based on Players' Psychophysiological Data during Gameplay. (arXiv:2010.06152v1 [cs.HC])</h2>
<h3>Jialin Wang, Hai-Ning Liang, Diego Monteiro, Wenge Xu, Hao Chen, Qiwen Chen</h3>
<p>Virtual Reality (VR) technology has been proliferating in the last decade,
especially in the last few years. However, Simulator Sickness (SS) still
represents a significant problem for its wider adoption. Currently, the most
common way to detect SS is using the Simulator Sickness Questionnaire (SSQ).
SSQ is a subjective measurement and is inadequate for real-time applications
such as VR games. This research aims to investigate how to use machine learning
techniques to detect SS based on in-game characters' and users' physiological
data during gameplay in VR games. To achieve this, we designed an experiment to
collect such data with three types of games. We trained a Long Short-Term
Memory neural network with the dataset eye-tracking and character movement data
to detect SS in real-time. Our results indicate that, in VR games, our model is
an accurate and efficient way to detect SS in real-time.
</p>
<a href="http://arxiv.org/abs/2010.06152" target="_blank">arXiv:2010.06152</a> [<a href="http://arxiv.org/pdf/2010.06152" target="_blank">pdf</a>]

<h2>On the Power of Abstention and Data-Driven Decision Making for Adversarial Robustness. (arXiv:2010.06154v1 [cs.LG])</h2>
<h3>Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, Hongyang Zhang</h3>
<p>We prove that classifiers with the ability to abstain are provably more
powerful than those that cannot against an adversary that can perturb
datapoints by arbitrary amounts in random directions. Specifically, we show
that no matter how well-behaved the natural data is, any classifier that cannot
abstain will be defeated by such an adversary. However, by allowing abstention,
we give a parameterized algorithm with provably good performance against such
an adversary when classes are reasonably well-separated and the data dimension
is high. We further use a data-driven method to set our algorithm parameters to
optimize over the accuracy vs. abstention trade-off with strong theoretical
guarantees. Our theory has direct applications to the technique of contrastive
learning, where we empirically demonstrate the ability of our algorithms to
obtain high robust accuracy with only small amounts of abstention in both
supervised and self-supervised settings.
</p>
<a href="http://arxiv.org/abs/2010.06154" target="_blank">arXiv:2010.06154</a> [<a href="http://arxiv.org/pdf/2010.06154" target="_blank">pdf</a>]

<h2>Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study. (arXiv:2010.06163v1 [cs.CV])</h2>
<h3>Yichi Zhang, Qingcheng Liao, Jicong Zhang</h3>
<p>With the unprecedented developments in deep learning, many methods are
proposed and have achieved great success for medical image segmentation.
However, unlike segmentation of natural images, most medical images such as MRI
and CT are volumetric data. In order to make full use of volumetric
information, 3D CNNs are widely used. However, 3D CNNs suffer from higher
inference time and computation cost, which hinders their further clinical
applications. Additionally, with the increased number of parameters, the risk
of overfitting is higher, especially for medical images where data and
annotations are expensive to acquire. To issue this problem, many 2.5D
segmentation methods have been proposed to make use of volumetric spatial
information with less computation cost. Despite these works lead to
improvements on a variety of segmentation tasks, to the best of our knowledge,
there has not previously been a large-scale empirical comparison of these
methods. In this paper, we aim to present a review of the latest developments
of 2.5D methods for volumetric medical image segmentation. Additionally, to
compare the performance and effectiveness of these methods, we provide an
empirical study of these methods on three representative segmentation tasks
involving different modalities and targets. Our experimental results highlight
that 3D CNNs may not always be the best choice. Besides, although all these
2.5D methods can bring performance gains to 2D baseline, not all the methods
hold the benefits on different datasets. We hope the results and conclusions of
our study will prove useful for the community on exploring and developing
efficient volumetric medical image segmentation methods.
</p>
<a href="http://arxiv.org/abs/2010.06163" target="_blank">arXiv:2010.06163</a> [<a href="http://arxiv.org/pdf/2010.06163" target="_blank">pdf</a>]

<h2>Causal Structure Learning: a Bayesian approach based on random graphs. (arXiv:2010.06164v1 [cs.AI])</h2>
<h3>Mauricio Gonzalez-Soto, Ivan R. Feliciano-Avelino, L. Enrique Sucar, Hugo J. Escalante Balderas</h3>
<p>A Random Graph is a random object which take its values in the space of
graphs. We take advantage of the expressibility of graphs in order to model the
uncertainty about the existence of causal relationships within a given set of
variables. We adopt a Bayesian point of view in order to capture a causal
structure via interaction and learning with a causal environment. We test our
method over two different scenarios, and the experiments mainly confirm that
our technique can learn a causal structure. Furthermore, the experiments and
results presented for the first test scenario demonstrate the usefulness of our
method to learn a causal structure as well as the optimal action. On the other
hand the second experiment, shows that our proposal manages to learn the
underlying causal structure of several tasks with different sizes and different
causal structures.
</p>
<a href="http://arxiv.org/abs/2010.06164" target="_blank">arXiv:2010.06164</a> [<a href="http://arxiv.org/pdf/2010.06164" target="_blank">pdf</a>]

<h2>Neural Gaussian Mirror for Controlled Feature Selection in Neural Networks. (arXiv:2010.06175v1 [stat.ML])</h2>
<h3>Xin Xing, Yu Gui, Chenguang Dai, Jun S. Liu</h3>
<p>Deep neural networks (DNNs) have become increasingly popular and achieved
outstanding performance in predictive tasks. However, the DNN framework itself
cannot inform the user which features are more or less relevant for making the
prediction, which limits its applicability in many scientific fields. We
introduce neural Gaussian mirrors (NGMs), in which mirrored features are
created, via a structured perturbation based on a kernel-based conditional
dependence measure, to help evaluate feature importance. We design two
modifications of the DNN architecture for incorporating mirrored features and
providing mirror statistics to measure feature importance. As shown in
simulated and real data examples, the proposed method controls the feature
selection error rate at a predefined level and maintains a high selection power
even with the presence of highly correlated features.
</p>
<a href="http://arxiv.org/abs/2010.06175" target="_blank">arXiv:2010.06175</a> [<a href="http://arxiv.org/pdf/2010.06175" target="_blank">pdf</a>]

<h2>COVID-19 Imaging Data Privacy by Federated Learning Design: A Theoretical Framework. (arXiv:2010.06177v1 [cs.CR])</h2>
<h3>Anwaar Ulhaq, Oliver Burmeister</h3>
<p>To address COVID-19 healthcare challenges, we need frequent sharing of health
data, knowledge and resources at a global scale. However, in this digital age,
data privacy is a big concern that requires the secure embedding of privacy
assurance into the design of all technological solutions that use health data.
In this paper, we introduce differential privacy by design (dPbD) framework and
discuss its embedding into the federated machine learning system. To limit the
scope of our paper, we focus on the problem scenario of COVID-19 imaging data
privacy for disease diagnosis by computer vision and deep learning approaches.
We discuss the evaluation of the proposed design of federated machine learning
systems and discuss how differential privacy by design (dPbD) framework can
enhance data privacy in federated learning systems with scalability and
robustness. We argue that scalable differentially private federated learning
design is a promising solution for building a secure, private and collaborative
machine learning model such as required to combat COVID19 challenge.
</p>
<a href="http://arxiv.org/abs/2010.06177" target="_blank">arXiv:2010.06177</a> [<a href="http://arxiv.org/pdf/2010.06177" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning and Transportation Research: A Comprehensive Review. (arXiv:2010.06187v1 [cs.LG])</h2>
<h3>Nahid Parvez Farazi, Tanvir Ahamed, Limon Barua, Bo Zou</h3>
<p>Deep reinforcement learning (DRL) is an emerging methodology that is
transforming the way many complicated transportation decision-making problems
are tackled. Researchers have been increasingly turning to this powerful
learning-based methodology to solve challenging problems across transportation
fields. While many promising applications have been reported in the literature,
there remains a lack of comprehensive synthesis of the many DRL algorithms and
their uses and adaptations. The objective of this paper is to fill this gap by
conducting a comprehensive, synthesized review of DRL applications in
transportation. We start by offering an overview of the DRL mathematical
background, popular and promising DRL algorithms, and some highly effective DRL
extensions. Building on this overview, a systematic investigation of about 150
DRL studies that have appeared in the transportation literature, divided into
seven different categories, is performed. Building on this review, we continue
to examine the applicability, strengths, shortcomings, and common and
application-specific issues of DRL techniques with regard to their applications
in transportation. In the end, we recommend directions for future research and
present available resources for actually implementing DRL.
</p>
<a href="http://arxiv.org/abs/2010.06187" target="_blank">arXiv:2010.06187</a> [<a href="http://arxiv.org/pdf/2010.06187" target="_blank">pdf</a>]

<h2>When Wireless Communications Meet Computer Vision in Beyond 5G. (arXiv:2010.06188v1 [cs.CV])</h2>
<h3>Takayuki Nishio, Yusuke Koda, Jihong Park, Mehdi Bennis, Klaus Doppler</h3>
<p>This article articulates the emerging paradigm, sitting at the confluence of
computer vision and wireless communication, to enable beyond-5G/6G
mission-critical applications (autonomous/remote-controlled vehicles,
visuo-haptic VR, and other cyber-physical applications). First, drawing on
recent advances in machine learning and the availability of non-RF data,
vision-aided wireless networks are shown to significantly enhance the
reliability of wireless communication without sacrificing spectral efficiency.
In particular, we demonstrate how computer vision enables {look-ahead}
prediction in a millimeter-wave channel blockage scenario, before the blockage
actually happens. From a computer vision perspective, we highlight how radio
frequency (RF) based sensing and imaging are instrumental in robustifying
computer vision applications against occlusion and failure. This is
corroborated via an RF-based image reconstruction use case, showcasing a
receiver-side image failure correction resulting in reduced retransmission and
latency. Taken together, this article sheds light on the much-needed
convergence of RF and non-RF modalities to enable ultra-reliable communication
and truly intelligent 6G networks.
</p>
<a href="http://arxiv.org/abs/2010.06188" target="_blank">arXiv:2010.06188</a> [<a href="http://arxiv.org/pdf/2010.06188" target="_blank">pdf</a>]

<h2>Revisiting BFloat16 Training. (arXiv:2010.06192v1 [cs.LG])</h2>
<h3>Pedram Zamirai, Jian Zhang, Christopher R. Aberger, Christopher De Sa</h3>
<p>State-of-the-art generic low-precision training algorithms use a mix of
16-bit and 32-bit precision, creating the folklore that 16-bit precision alone
is not enough to maximize model accuracy. As a result, deep learning
accelerators are forced to support both 16-bit and 32-bit compute units which
is more costly than only using 16-bit units for hardware design. We ask can we
do pure 16-bit training which requires only 16-bit compute units, while still
matching the model accuracy attained by 32-bit training. Towards this end, we
study pure 16-bit training algorithms on the widely adopted BFloat16 compute
unit. While these units conventionally use nearest rounding to cast output to
16-bit precision, we show that nearest rounding for model weight updates can
often cancel small updates, which degrades the convergence and model accuracy.
Motivated by this, we identify two simple existing techniques, stochastic
rounding and Kahan summation, to remedy the model accuracy degradation in pure
16-bit training. We empirically show that these two techniques can enable up to
7% absolute validation accuracy gain in pure 16-bit training. This leads to
0.1% lower to 0.2% higher matching validation accuracy compared to 32-bit
precision training across seven deep learning applications.
</p>
<a href="http://arxiv.org/abs/2010.06192" target="_blank">arXiv:2010.06192</a> [<a href="http://arxiv.org/pdf/2010.06192" target="_blank">pdf</a>]

<h2>Context-Aware Drive-thru Recommendation Service at Fast Food Restaurants. (arXiv:2010.06197v1 [cs.IR])</h2>
<h3>Luyang Wang, Kai Huang, Jiao Wang, Shengsheng Huang, Jason Dai, Yue Zhuang</h3>
<p>Drive-thru is a popular sales channel in the fast food industry where
consumers can make food purchases without leaving their cars. Drive-thru
recommendation systems allow restaurants to display food recommendations on the
digital menu board as guests are making their orders. Popular recommendation
models in eCommerce scenarios rely on user attributes (such as user profiles or
purchase history) to generate recommendations, while such information is hard
to obtain in the drive-thru use case. Thus, in this paper, we propose a new
recommendation model Transformer Cross Transformer (TxT), which exploits the
guest order behavior and contextual features (such as location, time, and
weather) using Transformer encoders for drive-thru recommendations. Empirical
results show that our TxT model achieves superior results in Burger King's
drive-thru production environment compared with existing recommendation
solutions. In addition, we implement a unified system to run end-to-end big
data analytics and deep learning workloads on the same cluster. We find that in
practice, maintaining a single big data cluster for the entire pipeline is more
efficient and cost-saving. Our recommendation system is not only beneficial for
drive-thru scenarios, and it can also be generalized to other customer
interaction channels.
</p>
<a href="http://arxiv.org/abs/2010.06197" target="_blank">arXiv:2010.06197</a> [<a href="http://arxiv.org/pdf/2010.06197" target="_blank">pdf</a>]

<h2>Visual Security Evaluation of Learnable Image Encryption Methods against Ciphertext-only Attacks. (arXiv:2010.06198v1 [cs.CR])</h2>
<h3>Warit Sirichotedumrong, Hitoshi Kiya</h3>
<p>Various visual information protection methods have been proposed for
privacy-preserving deep neural networks (DNNs). In contrast, attack methods on
such protection methods have been studied simultaneously. In this paper, we
evaluate state-of-the-art visual protection methods for privacy-preserving DNNs
in terms of visual security against ciphertext-only attacks (COAs). We focus on
brute-force attack, feature reconstruction attack (FR-Attack), inverse
transformation attack (ITN-Attack), and GAN-based attack (GAN-Attack), which
have been proposed to reconstruct visual information on plain images from the
visually-protected images. The detail of various attack is first summarized,
and then visual security of the protection methods is evaluated. Experimental
results demonstrate that most of protection methods, including pixel-wise
encryption, have not enough robustness against GAN-Attack, while a few
protection methods are robust enough against GAN-Attack.
</p>
<a href="http://arxiv.org/abs/2010.06198" target="_blank">arXiv:2010.06198</a> [<a href="http://arxiv.org/pdf/2010.06198" target="_blank">pdf</a>]

<h2>End-to-end Triplet Loss based Emotion Embedding System for Speech Emotion Recognition. (arXiv:2010.06200v1 [cs.SD])</h2>
<h3>Puneet Kumar, Sidharth Jain, Balasubramanian Raman, Partha Pratim Roy, Masakazu Iwamura</h3>
<p>In this paper, an end-to-end neural embedding system based on triplet loss
and residual learning has been proposed for speech emotion recognition. The
proposed system learns the embeddings from the emotional information of the
speech utterances. The learned embeddings are used to recognize the emotions
portrayed by given speech samples of various lengths. The proposed system
implements Residual Neural Network architecture. It is trained using softmax
pre-training and triplet loss function. The weights between the fully connected
and embedding layers of the trained network are used to calculate the embedding
values. The embedding representations of various emotions are mapped onto a
hyperplane, and the angles among them are computed using the cosine similarity.
These angles are utilized to classify a new speech sample into its appropriate
emotion class. The proposed system has demonstrated 91.67% and 64.44% accuracy
while recognizing emotions for RAVDESS and IEMOCAP dataset, respectively.
</p>
<a href="http://arxiv.org/abs/2010.06200" target="_blank">arXiv:2010.06200</a> [<a href="http://arxiv.org/pdf/2010.06200" target="_blank">pdf</a>]

<h2>Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v1 [quant-ph])</h2>
<h3>He-Liang Huang, Yuxuan Du, Ming Gong, Youwei Zhao, Yulin Wu, Chaoyue Wang, Shaowei Li, Futian Liang, Jin Lin, Yu Xu, Rui Yang, Tongliang Liu, Min-Hsiu Hsieh, Hui Deng, Hao Rong, Cheng-Zhi Peng, Chao-Yang Lu, Yu-Ao Chen, Dacheng Tao, Xiaobo Zhu, Jian-Wei Pan</h3>
<p>Quantum machine learning is expected to be one of the first practical
applications of near-term quantum devices. Pioneer theoretical works suggest
that quantum generative adversarial networks (GANs) may exhibit a potential
exponential advantage over classical GANs, thus attracting widespread
attention. However, it remains elusive whether quantum GANs implemented on
near-term quantum devices can actually solve real-world learning tasks. Here,
we devise a flexible quantum GAN scheme to narrow this knowledge gap, which
could accomplish image generation with arbitrarily high-dimensional features,
and could also take advantage of quantum superposition to train multiple
examples in parallel. For the first time, we experimentally achieve the
learning and generation of real-world hand-written digit images on a
superconducting quantum processor. Moreover, we utilize a gray-scale bar
dataset to exhibit the competitive performance between quantum GANs and the
classical GANs based on multilayer perceptron and convolutional neural network
architectures, respectively, benchmarked by the Fr\'echet Distance score. Our
work provides guidance for developing advanced quantum generative models on
near-term quantum devices and opens up an avenue for exploring quantum
advantages in various GAN-related learning tasks.
</p>
<a href="http://arxiv.org/abs/2010.06201" target="_blank">arXiv:2010.06201</a> [<a href="http://arxiv.org/pdf/2010.06201" target="_blank">pdf</a>]

<h2>Mitigating Gender Bias in Machine Translation with Target Gender Annotations. (arXiv:2010.06203v1 [cs.CL])</h2>
<h3>Toms Bergmanis, Art&#x16b;rs Stafanovi&#x10d;s, M&#x101;rcis Pinnis</h3>
<p>When translating "The secretary asked for details." to a language with
grammatical gender, it might be necessary to determine the gender of the
subject "secretary". If the sentence does not contain the necessary
information, it is not always possible to disambiguate. In such cases, machine
translation systems select the most common translation option, which often
corresponds to the stereotypical translations, thus potentially exacerbating
prejudice and marginalisation of certain groups and people. We argue that the
information necessary for an adequate translation can not always be deduced
from the sentence being translated or even might depend on external knowledge.
Therefore, in this work, we propose to decouple the task of acquiring the
necessary information from the task of learning to translate correctly when
such information is available. To that end, we present a method for training
machine translation systems to use word-level annotations containing
information about subject's gender. To prepare training data, we annotate
regular source language words with grammatical gender information of the
corresponding target language words. Using such data to train machine
translation systems reduces their reliance on gender stereotypes when
information about the subject's gender is available. Our experiments on five
language pairs show that this allows improving accuracy on the WinoMT test set
by up to 25.8 percentage points.
</p>
<a href="http://arxiv.org/abs/2010.06203" target="_blank">arXiv:2010.06203</a> [<a href="http://arxiv.org/pdf/2010.06203" target="_blank">pdf</a>]

<h2>DoFE: Domain-oriented Feature Embedding for Generalizable Fundus Image Segmentation on Unseen Datasets. (arXiv:2010.06208v1 [cs.CV])</h2>
<h3>Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, Pheng-Ann Heng</h3>
<p>Deep convolutional neural networks have significantly boosted the performance
of fundus image segmentation when test datasets have the same distribution as
the training datasets. However, in clinical practice, medical images often
exhibit variations in appearance for various reasons, e.g., different scanner
vendors and image quality. These distribution discrepancies could lead the deep
networks to over-fit on the training datasets and lack generalization ability
on the unseen test datasets. To alleviate this issue, we present a novel
Domain-oriented Feature Embedding (DoFE) framework to improve the
generalization ability of CNNs on unseen target domains by exploring the
knowledge from multiple source domains. Our DoFE framework dynamically enriches
the image features with additional domain prior knowledge learned from
multi-source domains to make the semantic features more discriminative.
Specifically, we introduce a Domain Knowledge Pool to learn and memorize the
prior information extracted from multi-source domains. Then the original image
features are augmented with domain-oriented aggregated features, which are
induced from the knowledge pool based on the similarity between the input image
and multi-source domain images. We further design a novel domain code
prediction branch to infer this similarity and employ an attention-guided
mechanism to dynamically combine the aggregated features with the semantic
features. We comprehensively evaluate our DoFE framework on two fundus image
segmentation tasks, including the optic cup and disc segmentation and vessel
segmentation. Our DoFE framework generates satisfying segmentation results on
unseen datasets and surpasses other domain generalization and network
regularization methods.
</p>
<a href="http://arxiv.org/abs/2010.06208" target="_blank">arXiv:2010.06208</a> [<a href="http://arxiv.org/pdf/2010.06208" target="_blank">pdf</a>]

<h2>Deep Reservoir Computing with Learned Hidden Reservoir Weights using Direct Feedback Alignment. (arXiv:2010.06209v1 [cs.NE])</h2>
<h3>Matthew Evanusa, Yiannis Aloimonos, Cornelia Ferm&#xfc;ller</h3>
<p>Deep Reservoir Computing has emerged as a new paradigm for deep learning,
which is based around the reservoir computing principle of maintaining random
pools of neurons. The reservoir paradigm reflects and respects the high degree
of recurrence in biological brains, and the role that neuronal dynamics play in
learning. However, one issue hampering deep reservoir development is that one
cannot backpropagate through the reservoir layers. Recent deep reservoir
architectures do not learn hidden or hierarchical representations in the same
manner as deep artifical neural neteworks (ANNs), but rather concatenate all
hidden reservoirs together to perform traditional regression. Here we present a
novel Deep Reservoir Computer for time series prediction and classification
that learns through the non-differentiable hidden reservoir layers using a
biologically-inspired backpropagation alternative called Direct Feedback
Alignment, which resembles global dopamine signal broadcasting in the brain.
The hope is that this will enable future deep reservoir architectures to learn
hidden temporal representations.
</p>
<a href="http://arxiv.org/abs/2010.06209" target="_blank">arXiv:2010.06209</a> [<a href="http://arxiv.org/pdf/2010.06209" target="_blank">pdf</a>]

<h2>S3ML: A Secure Serving System for Machine Learning Inference. (arXiv:2010.06212v1 [cs.LG])</h2>
<h3>Junming Ma, Chaofan Yu, Aihui Zhou, Bingzhe Wu, Xibin Wu, Xingyu Chen, Xiangqun Chen, Lei Wang, Donggang Cao</h3>
<p>We present S3ML, a secure serving system for machine learning inference in
this paper. S3ML runs machine learning models in Intel SGX enclaves to protect
users' privacy. S3ML designs a secure key management service to construct
flexible privacy-preserving server clusters and proposes novel SGX-aware load
balancing and scaling methods to satisfy users' Service-Level Objectives. We
have implemented S3ML based on Kubernetes as a low-overhead, high-available,
and scalable system. We demonstrate the system performance and effectiveness of
S3ML through extensive experiments on a series of widely-used models.
</p>
<a href="http://arxiv.org/abs/2010.06212" target="_blank">arXiv:2010.06212</a> [<a href="http://arxiv.org/pdf/2010.06212" target="_blank">pdf</a>]

<h2>Few-shot Action Recognition with Implicit Temporal Alignment and Pair Similarity Optimization. (arXiv:2010.06215v1 [cs.CV])</h2>
<h3>Congqi Cao, Yajuan Li, Qinyi Lv, Peng Wang, Yanning Zhang</h3>
<p>Few-shot learning aims to recognize instances from novel classes with few
labeled samples, which has great value in research and application. Although
there has been a lot of work in this area recently, most of the existing work
is based on image classification tasks. Video-based few-shot action recognition
has not been explored well and remains challenging: 1) the differences of
implementation details among different papers make a fair comparison difficult;
2) the wide variations and misalignment of temporal sequences make the
video-level similarity comparison difficult; 3) the scarcity of labeled data
makes the optimization difficult. To solve these problems, this paper presents
1) a specific setting to evaluate the performance of few-shot action
recognition algorithms; 2) an implicit sequence-alignment algorithm for better
video-level similarity comparison; 3) an advanced loss for few-shot learning to
optimize pair similarity with limited data. Specifically, we propose a novel
few-shot action recognition framework that uses long short-term memory
following 3D convolutional layers for sequence modeling and alignment. Circle
loss is introduced to maximize the within-class similarity and minimize the
between-class similarity flexibly towards a more definite convergence target.
Instead of using random or ambiguous experimental settings, we set a concrete
criterion analogous to the standard image-based few-shot learning setting for
few-shot action recognition evaluation. Extensive experiments on two datasets
demonstrate the effectiveness of our proposed method.
</p>
<a href="http://arxiv.org/abs/2010.06215" target="_blank">arXiv:2010.06215</a> [<a href="http://arxiv.org/pdf/2010.06215" target="_blank">pdf</a>]

<h2>TM-NET: Deep Generative Networks for Textured Meshes. (arXiv:2010.06217v1 [cs.GR])</h2>
<h3>Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, Hao Zhang</h3>
<p>We introduce TM-NET, a novel deep generative model capable of generating
meshes with detailed textures, as well as synthesizing plausible textures for a
given shape. To cope with complex geometry and structure, inspired by the
recently proposed SDM-NET, our method produces texture maps for individual
parts, each as a deformed box, which further leads to a natural UV map with
minimum distortions. To provide a generic framework for different application
scenarios, we encode geometry and texture separately and learn the texture
probability distribution conditioned on the geometry. We address challenges for
textured mesh generation by sampling textures on the conditional probability
distribution. Textures also often contain high-frequency details (e.g. wooden
texture), and we encode them effectively with a variational autoencoder (VAE)
using dictionary-based vector quantization. We also exploit the transparency in
the texture as an effective approach to modeling highly complicated topology
and geometry. This work is the first to synthesize high-quality textured meshes
for shapes with complex structures. Extensive experiments show that our method
produces high-quality textures, and avoids the inconsistency issue common for
novel view synthesis methods where textured shapes from different views are
generated separately.
</p>
<a href="http://arxiv.org/abs/2010.06217" target="_blank">arXiv:2010.06217</a> [<a href="http://arxiv.org/pdf/2010.06217" target="_blank">pdf</a>]

<h2>Self-Supervised Multi-View Synchronization Learning for 3D Pose Estimation. (arXiv:2010.06218v1 [cs.CV])</h2>
<h3>Simon Jenni, Paolo Favaro</h3>
<p>Current state-of-the-art methods cast monocular 3D human pose estimation as a
learning problem by training neural networks on large data sets of images and
corresponding skeleton poses. In contrast, we propose an approach that can
exploit small annotated data sets by fine-tuning networks pre-trained via
self-supervised learning on (large) unlabeled data sets. To drive such networks
towards supporting 3D pose estimation during the pre-training step, we
introduce a novel self-supervised feature learning task designed to focus on
the 3D structure in an image. We exploit images extracted from videos captured
with a multi-view camera system. The task is to classify whether two images
depict two views of the same scene up to a rigid transformation. In a
multi-view data set, where objects deform in a non-rigid manner, a rigid
transformation occurs only between two views taken at the exact same time,
i.e., when they are synchronized. We demonstrate the effectiveness of the
synchronization task on the Human3.6M data set and achieve state-of-the-art
results in 3D human pose estimation.
</p>
<a href="http://arxiv.org/abs/2010.06218" target="_blank">arXiv:2010.06218</a> [<a href="http://arxiv.org/pdf/2010.06218" target="_blank">pdf</a>]

<h2>Investigating the Scalability and Biological Plausibility of the Activation Relaxation Algorithm. (arXiv:2010.06219v1 [cs.AI])</h2>
<h3>Beren Millidge, Alexander Tschantz, Anil Seth, Christopher L Buckley</h3>
<p>The recently proposed Activation Relaxation (AR) algorithm provides a simple
and robust approach for approximating the backpropagation of error algorithm
using only local learning rules. Unlike competing schemes, it converges to the
exact backpropagation gradients, and utilises only a single type of
computational unit and a single backwards relaxation phase. We have previously
shown that the algorithm can be further simplified and made more biologically
plausible by (i) introducing a learnable set of backwards weights, which
overcomes the weight-transport problem, and (ii) avoiding the computation of
nonlinear derivatives at each neuron. However, tthe efficacy of these
simplifications has, so far, only been tested on simple multi-layer-perceptron
(MLP) networks. Here, we show that these simplifications still maintain
performance using more complex CNN architectures and challenging datasets,
which have proven difficult for other biologically-plausible schemes to scale
to. We also investigate whether another biologically implausible assumption of
the original AR algorithm -- the frozen feedforward pass -- can be relaxed
without damaging performance.
</p>
<a href="http://arxiv.org/abs/2010.06219" target="_blank">arXiv:2010.06219</a> [<a href="http://arxiv.org/pdf/2010.06219" target="_blank">pdf</a>]

<h2>Direct Federated Neural Architecture Search. (arXiv:2010.06223v1 [cs.LG])</h2>
<h3>Anubhav Garg, Amit Kumar Saha, Debo Dutta</h3>
<p>Neural Architecture Search (NAS) is a collection of methods to craft the way
neural networks are built. We apply this idea to Federated Learning (FL),
wherein predefined neural network models with known hyperparameter values are
trained on the client/device data. This approach is not optimal as the model
developers can't observe the local data, and hence, are unable to build highly
accurate and efficient models. NAS is promising for FL which can search for
global and personalized models automatically for the non-IID data. Most NAS
methods are computationally expensive and require fine-tuning after the search,
making it a two-stage complex process with possible human intervention. Thus
there is a need for end-to-end NAS which can run on the heterogeneous data and
resource distribution typically seen in the FL scenario. In this paper, we
present an effective approach for direct federated NAS which is hardware
agnostic, computationally lightweight, and a one-stage method to search for
ready-to-deploy neural network models. Our results show an order of magnitude
reduction in resource consumption while edging out prior art in accuracy. This
opens up a window of opportunity to create optimized and computationally
efficient federated learning systems.
</p>
<a href="http://arxiv.org/abs/2010.06223" target="_blank">arXiv:2010.06223</a> [<a href="http://arxiv.org/pdf/2010.06223" target="_blank">pdf</a>]

<h2>Variable impedance control and learning -- A review. (arXiv:2010.06246v1 [cs.RO])</h2>
<h3>Fares J. Abu-Dakka, Matteo Saveriano</h3>
<p>Robots that physically interact with their surroundings, in order to
accomplish some tasks or assist humans in their activities, require to exploit
contact forces in a safe and proficient manner. Impedance control is considered
as a prominent approach in robotics to avoid large impact forces while
operating in unstructured environments. In such environments, the conditions
under which the interaction occurs may significantly vary during the task
execution. This demands robots to be endowed with on-line adaptation
capabilities to cope with sudden and unexpected changes in the environment. In
this context, variable impedance control arises as a powerful tool to modulate
the robot's behavior in response to variations in its surroundings. In this
survey, we present the state-of-the-art of approaches devoted to variable
impedance control from control and learning perspectives (separately and
jointly). Moreover, we propose a new taxonomy for mechanical impedance based on
variability, learning, and control. The objective of this survey is to put
together the concepts and efforts that have been done so far in this field, and
to describe advantages and disadvantages of each approach. The survey concludes
with open issues in the field and an envisioned framework that may potentially
solve them.
</p>
<a href="http://arxiv.org/abs/2010.06246" target="_blank">arXiv:2010.06246</a> [<a href="http://arxiv.org/pdf/2010.06246" target="_blank">pdf</a>]

<h2>Annotationsaurus: A Searchable Directory of Annotation Tools. (arXiv:2010.06251v1 [cs.CL])</h2>
<h3>Mariana neves, Jurica Seva</h3>
<p>Manual annotation of textual documents is a necessary task when constructing
benchmark corpora for training and evaluating machine learning algorithms. We
created a comprehensive directory of annotation tools that currently includes
93 tools. We analyzed the tools over a set of 31 features and implemented
simple scripts and a Web application that filters the tools based on chosen
criteria. We present two use cases using the directory and propose ideas for
its maintenance. The directory, source codes for scripts, and link to the Web
application are available at: https://github.com/mariananeves/annotation-tools
</p>
<a href="http://arxiv.org/abs/2010.06251" target="_blank">arXiv:2010.06251</a> [<a href="http://arxiv.org/pdf/2010.06251" target="_blank">pdf</a>]

<h2>Behavior Trees in Action: A Study of Robotics Applications. (arXiv:2010.06256v1 [cs.RO])</h2>
<h3>Razan Ghzouli, Thorsten Berger, Einar Broch Johnsen, Swaib Dragule, Andrzej W&#x105;sowski</h3>
<p>Autonomous robots combine a variety of skills to form increasingly complex
behaviors called missions. While the skills are often programmed at a
relatively low level of abstraction, their coordination is architecturally
separated and often expressed in higher-level languages or frameworks.
Recently, the language of Behavior Trees gained attention among roboticists for
this reason. Originally designed for computer games to model autonomous actors,
Behavior Trees offer an extensible tree-based representation of missions.
However, even though, several implementations of the language are in use,
little is known about its usage and scope in the real world. How do behavior
trees relate to traditional languages for describing behavior? How are behavior
tree concepts used in applications? What are the benefits of using them?

We present a study of the key language concepts in Behavior Trees and their
use in real-world robotic applications. We identify behavior tree languages and
compare their semantics to the most well-known behavior modeling languages:
state and activity diagrams. We mine open source repositories for robotics
applications that use the language and analyze this usage. We find that
Behavior Trees are a pragmatic language, not fully specified, allowing projects
to extend it even for just one model. Behavior trees clearly resemble the
models-at-runtime paradigm. We contribute a dataset of real-world behavior
models, hoping to inspire the community to use and further develop this
language, associated tools, and analysis techniques.
</p>
<a href="http://arxiv.org/abs/2010.06256" target="_blank">arXiv:2010.06256</a> [<a href="http://arxiv.org/pdf/2010.06256" target="_blank">pdf</a>]

<h2>Stochastic embeddings of dynamical phenomena through variational autoencoders. (arXiv:2010.06265v1 [physics.comp-ph])</h2>
<h3>Constantino A. Garcia, Paulo Felix, Jesus M. Presedo, Abraham Otero</h3>
<p>System identification in scenarios where the observed number of variables is
less than the degrees of freedom in the dynamics is an important challenge. In
this work we tackle this problem by using a recognition network to increase the
observed space dimensionality during the reconstruction of the phase space. The
phase space is forced to have approximately Markovian dynamics described by a
Stochastic Differential Equation (SDE), which is also to be discovered. To
enable robust learning from stochastic data we use the Bayesian paradigm and
place priors on the drift and diffusion terms. To handle the complexity of
learning the posteriors, a set of mean field variational approximations to the
true posteriors are introduced, enabling efficient statistical inference.
Finally, a decoder network is used to obtain plausible reconstructions of the
experimental data. The main advantage of this approach is that the resulting
model is interpretable within the paradigm of statistical physics. Our
validation shows that this approach not only recovers a state space that
resembles the original one, but it is also able to synthetize new time series
capturing the main properties of the experimental data.
</p>
<a href="http://arxiv.org/abs/2010.06265" target="_blank">arXiv:2010.06265</a> [<a href="http://arxiv.org/pdf/2010.06265" target="_blank">pdf</a>]

<h2>Model-Based Reinforcement Learning for Type 1Diabetes Blood Glucose Control. (arXiv:2010.06266v1 [cs.LG])</h2>
<h3>Taku Yamagata (1), Aisling O&#x27;Kane (1), Amid Ayobi (1), Dmitri Katz (2), Katarzyna Stawarz (3), Paul Marshall (1), Peter Flach (1), Ra&#xfa;l Santos-Rodr&#xed;guez (1) ((1) University of Bristol, (2) The Open University, (3) Cardiff University)</h3>
<p>In this paper we investigate the use of model-based reinforcement learning to
assist people with Type 1 Diabetes with insulin dose decisions. The proposed
architecture consists of multiple Echo State Networks to predict blood glucose
levels combined with Model Predictive Controller for planning. Echo State
Network is a version of recurrent neural networks which allows us to learn long
term dependencies in the input of time series data in an online manner.
Additionally, we address the quantification of uncertainty for a more robust
control. Here, we used ensembles of Echo State Networks to capture model
(epistemic) uncertainty. We evaluated the approach with the FDA-approved
UVa/Padova Type 1 Diabetes simulator and compared the results against baseline
algorithms such as Basal-Bolus controller and Deep Q-learning. The results
suggest that the model-based reinforcement learning algorithm can perform
equally or better than the baseline algorithms for the majority of virtual Type
1 Diabetes person profiles tested.
</p>
<a href="http://arxiv.org/abs/2010.06266" target="_blank">arXiv:2010.06266</a> [<a href="http://arxiv.org/pdf/2010.06266" target="_blank">pdf</a>]

<h2>BRUMS at SemEval-2020 Task 12 : Transformer based Multilingual Offensive Language Identification in Social Media. (arXiv:2010.06278v1 [cs.CL])</h2>
<h3>Tharindu Ranasinghe, Hansi Hettiarachchi</h3>
<p>In this paper, we describe the team \textit{BRUMS} entry to OffensEval 2:
Multilingual Offensive Language Identification in Social Media in SemEval-2020.
The OffensEval organizers provided participants with annotated datasets
containing posts from social media in Arabic, Danish, English, Greek and
Turkish. We present a multilingual deep learning model to identify offensive
language in social media. Overall, the approach achieves acceptable evaluation
scores, while maintaining flexibility between languages.
</p>
<a href="http://arxiv.org/abs/2010.06278" target="_blank">arXiv:2010.06278</a> [<a href="http://arxiv.org/pdf/2010.06278" target="_blank">pdf</a>]

<h2>An Immersive Virtual Environment for Collaborative Geovisualization. (arXiv:2010.06279v1 [cs.HC])</h2>
<h3>Milan Dolezal, Jiri Chmelik, Fotis Liarokapis</h3>
<p>This paper presents an immersive virtual reality environment that can be used
to develop collaborative educational applications. Multiple users can
collaborate within the virtual shared space and communicate with each other
through voice. To asses the feasibility of the collaborative environment a
novel case-study concerned the education of a geography was developed and
evaluated. The geovisualization experiment scenario explores the possibility of
learning geography in a collaborative virtual environment. A user-study with 30
participants was performed. Participants evaluated and commented on the
usability and interaction methods used within the virtual environment.
</p>
<a href="http://arxiv.org/abs/2010.06279" target="_blank">arXiv:2010.06279</a> [<a href="http://arxiv.org/pdf/2010.06279" target="_blank">pdf</a>]

<h2>Land Cover Semantic Segmentation Using ResUNet. (arXiv:2010.06285v1 [cs.CV])</h2>
<h3>Vasilis Pollatos, Loukas Kouvaras, Eleni Charou</h3>
<p>In this paper we present our work on developing an automated system for land
cover classification. This system takes a multiband satellite image of an area
as input and outputs the land cover map of the area at the same resolution as
the input. For this purpose convolutional machine learning models were trained
in the task of predicting the land cover semantic segmentation of satellite
images. This is a case of supervised learning. The land cover label data were
taken from the CORINE Land Cover inventory and the satellite images were taken
from the Copernicus hub. As for the model, U-Net architecture variations were
applied. Our area of interest are the Ionian islands (Greece). We created a
dataset from scratch covering this particular area. In addition, transfer
learning from the BigEarthNet dataset [1] was performed. In [1] simple
classification of satellite images into the classes of CLC is performed but not
segmentation as we do. However, their models have been trained into a dataset
much bigger than ours, so we applied transfer learning using their pretrained
models as the first part of out network, utilizing the ability these networks
have developed to extract useful features from the satellite images (we
transferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer
learning other techniques were applied in order to overcome the limitations set
by the small size of our area of interest. We used data augmentation (cutting
images into overlapping patches, applying random transformations such as
rotations and flips) and cross validation. The results are tested on the 3 CLC
class hierarchy levels and a comparative study is made on the results of
different approaches.
</p>
<a href="http://arxiv.org/abs/2010.06285" target="_blank">arXiv:2010.06285</a> [<a href="http://arxiv.org/pdf/2010.06285" target="_blank">pdf</a>]

<h2>Lightweight IoT Malware Detection Solution Using CNN Classification. (arXiv:2010.06286v1 [cs.CR])</h2>
<h3>Ahmad M.N. Zaza, Suleiman K. Kharroub, Mohsen Guizani, Khalid Abualsaud</h3>
<p>Internet of Things (IoT) is becoming more frequently used in more
applications as the number of connected devices is in a rapid increase. More
connected devices result in bigger challenges in terms of scalability,
maintainability and most importantly security especially when it comes to 5G
networks. The security aspect of IoT devices is an infant field, which is why
it is our focus in this paper. Multiple IoT device manufacturers do not
consider securing the devices they produce for different reasons like cost
reduction or to avoid using energy-harvesting components. Such potentially
malicious devices might be exploited by the adversary to do multiple harmful
attacks. Therefore, we developed a system that can recognize malicious behavior
of a specific IoT node on the network. Through convolutional neural network and
monitoring, we were able to provide malware detection for IoT using a central
node that can be installed within the network. The achievement shows how such
models can be generalized and applied easily to any network while clearing out
any stigma regarding deep learning techniques.
</p>
<a href="http://arxiv.org/abs/2010.06286" target="_blank">arXiv:2010.06286</a> [<a href="http://arxiv.org/pdf/2010.06286" target="_blank">pdf</a>]

<h2>Impact of Thermal Throttling on Long-Term Visual Inference in a CPU-based Edge Device. (arXiv:2010.06291v1 [cs.CV])</h2>
<h3>Th&#xe9;o Benoit-Cattin, Delia Velasco-Montero, Jorge Fern&#xe1;ndez-Berni</h3>
<p>Many application scenarios of edge visual inference, e.g., robotics or
environmental monitoring, eventually require long periods of continuous
operation. In such periods, the processor temperature plays a critical role to
keep a prescribed frame rate. Particularly, the heavy computational load of
convolutional neural networks (CNNs) may lead to thermal throttling and hence
performance degradation in few seconds. In this paper, we report and analyze
the long-term performance of 80 different cases resulting from running 5 CNN
models on 4 software frameworks and 2 operating systems without and with active
cooling. This comprehensive study was conducted on a low-cost edge platform,
namely Raspberry Pi 4B (RPi4B), under stable indoor conditions. The results
show that hysteresis-based active cooling prevented thermal throttling in all
cases, thereby improving the throughput up to approximately 90% versus no
cooling. Interestingly, the range of fan usage during active cooling varied
from 33% to 65%. Given the impact of the fan on the power consumption of the
system as a whole, these results stress the importance of a suitable selection
of CNN model and software components. To assess the performance in outdoor
applications, we integrated an external temperature sensor with the RPi4B and
conducted a set of experiments with no active cooling in a wide interval of
ambient temperature, ranging from 22 {\deg}C to 36 {\deg}C. Variations up to
27.7% were measured with respect to the maximum throughput achieved in that
interval. This demonstrates that ambient temperature is a critical parameter in
case active cooling cannot be applied.
</p>
<a href="http://arxiv.org/abs/2010.06291" target="_blank">arXiv:2010.06291</a> [<a href="http://arxiv.org/pdf/2010.06291" target="_blank">pdf</a>]

<h2>Deep Multi-Agent Reinforcement Learning for Cost Efficient Distributed Load Frequency Control. (arXiv:2010.06293v1 [eess.SY])</h2>
<h3>Sergio Rozada, Dimitra Apostolopoulou, Eduardo Alonso</h3>
<p>The rise of microgrid-based architectures is heavily modifying the energy
control landscape in distribution systems making distributed control mechanisms
necessary to ensure reliable power system operations. In this paper, we propose
the use of Reinforcement Learning techniques to implement load frequency
control without requiring a central authority. To this end, we approximate the
optimal solution of the primary, secondary, and tertiary control with the use
of the Multi- Agent Deep Deterministic Policy Gradient (MADDPG) algorithm.
Generation units are characterised as agents that learn how to maximise their
long-term performance by acting and interacting with the environment to balance
generation and load in a cost efficient way. Network effects are also modelled
in our framework for the restoration of frequency to the nominal value. We
validate our Reinforcement Learning methodology through numerical results and
show that it can be used to implement the load frequency control in a
distributed and cost efficient way.
</p>
<a href="http://arxiv.org/abs/2010.06293" target="_blank">arXiv:2010.06293</a> [<a href="http://arxiv.org/pdf/2010.06293" target="_blank">pdf</a>]

<h2>Humane Visual AI: Telling the Stories Behind a Medical Condition. (arXiv:2010.06296v1 [cs.HC])</h2>
<h3>Wonyoung So, Edyta P. Bogucka, Sanja &#x160;&#x107;epanovi&#x107;, Sagar Joglekar, Ke Zhou, Daniele Quercia</h3>
<p>A biological understanding is key for managing medical conditions, yet
psychological and social aspects matter too. The main problem is that these two
aspects are hard to quantify and inherently difficult to communicate. To
quantify psychological aspects, this work mined around half a million Reddit
posts in the sub-communities specialised in 14 medical conditions, and it did
so with a new deep-learning framework. In so doing, it was able to associate
mentions of medical conditions with those of emotions. To then quantify social
aspects, this work designed a probabilistic approach that mines open
prescription data from the National Health Service in England to compute the
prevalence of drug prescriptions, and to relate such a prevalence to census
data. To finally visually communicate each medical condition's biological,
psychological, and social aspects through storytelling, we designed a
narrative-style layered Martini Glass visualization. In a user study involving
52 participants, after interacting with our visualization, a considerable
number of them changed their mind on previously held opinions: 10% gave more
importance to the psychological aspects of medical conditions, and 27% were
more favourable to the use of social media data in healthcare, suggesting the
importance of persuasive elements in interactive visualizations.
</p>
<a href="http://arxiv.org/abs/2010.06296" target="_blank">arXiv:2010.06296</a> [<a href="http://arxiv.org/pdf/2010.06296" target="_blank">pdf</a>]

<h2>Tire Force Estimation in Intelligent Tires Using Machine Learning. (arXiv:2010.06299v1 [eess.SY])</h2>
<h3>Nan Xu, Hassan Askari, Yanjun Huang, Jianfeng Zhou, Amir Khajepour</h3>
<p>The concept of intelligent tires has drawn attention of researchers in the
areas of autonomous driving, advanced vehicle control, and artificial
intelligence. The focus of this paper is on intelligent tires and the
application of machine learning techniques to tire force estimation. We present
an intelligent tire system with a tri-axial acceleration sensor, which is
installed onto the inner liner of the tire, and Neural Network techniques for
real-time processing of the sensor data. The accelerometer is capable of
measuring the acceleration in x,y, and z directions. When the accelerometer
enters the tire contact patch, it starts generating signals until it fully
leaves it. Simultaneously, by using MTS Flat-Trac test platform, tire actual
forces are measured. Signals generated by the accelerometer and MTS Flat-Trac
testing system are used for training three different machine learning
techniques with the purpose of online prediction of tire forces. It is shown
that the developed intelligent tire in conjunction with machine learning is
effective in accurate prediction of tire forces under different driving
conditions. The results presented in this work will open a new avenue of
research in the area of intelligent tires, vehicle systems,~and tire force
estimation.
</p>
<a href="http://arxiv.org/abs/2010.06299" target="_blank">arXiv:2010.06299</a> [<a href="http://arxiv.org/pdf/2010.06299" target="_blank">pdf</a>]

<h2>MixCo: Mix-up Contrastive Learning for Visual Representation. (arXiv:2010.06300v1 [cs.CV])</h2>
<h3>Sungnyun Kim, Gihun Lee, Sangmin Bae, Se-Young Yun</h3>
<p>Contrastive learning has shown remarkable results in recent self-supervised
approaches for visual representation. By learning to contrast positive pairs'
representation from the corresponding negatives pairs, one can train good
visual representations without human annotations. This paper proposes Mix-up
Contrast (MixCo), which extends the contrastive learning concept to
semi-positives encoded from the mix-up of positive and negative images. MixCo
aims to learn the relative similarity of representations, reflecting how much
the mixed images have the original positives. We validate the efficacy of MixCo
when applied to the recent self-supervised learning algorithms under the
standard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In
the experiments, MixCo consistently improves test accuracy. Remarkably, the
improvement is more significant when the learning capacity (e.g., model size)
is limited, suggesting that MixCo might be more useful in real-world scenarios.
</p>
<a href="http://arxiv.org/abs/2010.06300" target="_blank">arXiv:2010.06300</a> [<a href="http://arxiv.org/pdf/2010.06300" target="_blank">pdf</a>]

<h2>Data Engineering for HPC with Python. (arXiv:2010.06312v1 [cs.DC])</h2>
<h3>Vibhatha Abeykoon, Niranda Perera, Chathura Widanage, Supun Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi Wickramasinghe, Ahmet Uyar, Geoffrey Fox</h3>
<p>Data engineering is becoming an increasingly important part of scientific
discoveries with the adoption of deep learning and machine learning. Data
engineering deals with a variety of data formats, storage, data extraction,
transformation, and data movements. One goal of data engineering is to
transform data from original data to vector/matrix/tensor formats accepted by
deep learning and machine learning applications. There are many structures such
as tables, graphs, and trees to represent data in these data engineering
phases. Among them, tables are a versatile and commonly used format to load and
process data. In this paper, we present a distributed Python API based on table
abstraction for representing and processing data. Unlike existing
state-of-the-art data engineering tools written purely in Python, our solution
adopts high performance compute kernels in C++, with an in-memory table
representation with Cython-based Python bindings. In the core system, we use
MPI for distributed memory computations with a data-parallel approach for
processing large datasets in HPC clusters.
</p>
<a href="http://arxiv.org/abs/2010.06312" target="_blank">arXiv:2010.06312</a> [<a href="http://arxiv.org/pdf/2010.06312" target="_blank">pdf</a>]

<h2>Controllable Pareto Multi-Task Learning. (arXiv:2010.06313v1 [cs.LG])</h2>
<h3>Xi Lin, Zhiyuan Yang, Qingfu Zhang, Sam Kwong</h3>
<p>A multi-task learning (MTL) system aims at solving multiple related tasks at
the same time. With a fixed model capacity, the tasks would be conflicted with
each other, and the system usually has to make a trade-off among learning all
of them together. Multiple models with different preferences over tasks have to
be trained and stored for many real-world applications where the trade-off has
to be made online. This work proposes a novel controllable Pareto multi-task
learning framework, to enable the system to make real-time trade-off switch
among different tasks with a single model. To be specific, we formulate the MTL
as a preference-conditioned multiobjective optimization problem, for which
there is a parametric mapping from the preferences to the optimal Pareto
solutions. A single hypernetwork-based multi-task neural network is built to
learn all tasks with different trade-off preferences among them, where the
hypernetwork generates the model parameters conditioned on the preference. At
the inference time, MTL practitioners can easily control the model performance
based on different trade-off preferences in real-time. Experiments on different
applications demonstrate that the proposed model is efficient for solving
various multi-task learning problems.
</p>
<a href="http://arxiv.org/abs/2010.06313" target="_blank">arXiv:2010.06313</a> [<a href="http://arxiv.org/pdf/2010.06313" target="_blank">pdf</a>]

<h2>Audio-Visual Self-Supervised Terrain Type Discovery for Mobile Platforms. (arXiv:2010.06318v1 [cs.CV])</h2>
<h3>Akiyoshi Kurobe, Yoshikatsu Nakajima, Hideo Saito, Kris Kitani</h3>
<p>The ability to both recognize and discover terrain characteristics is an
important function required for many autonomous ground robots such as social
robots, assistive robots, autonomous vehicles, and ground exploration robots.
Recognizing and discovering terrain characteristics is challenging because
similar terrains may have very different appearances (e.g., carpet comes in
many colors), while terrains with very similar appearance may have very
different physical properties (e.g. mulch versus dirt). In order to address the
inherent ambiguity in vision-based terrain recognition and discovery, we
propose a multi-modal self-supervised learning technique that switches between
audio features extracted from a mic attached to the underside of a mobile
platform and image features extracted by a camera on the platform to cluster
terrain types. The terrain cluster labels are then used to train an image-based
convolutional neural network to predict changes in terrain types. Through
experiments, we demonstrate that the proposed self-supervised terrain type
discovery method achieves over 80% accuracy, which greatly outperforms several
baselines and suggests strong potential for assistive applications.
</p>
<a href="http://arxiv.org/abs/2010.06318" target="_blank">arXiv:2010.06318</a> [<a href="http://arxiv.org/pdf/2010.06318" target="_blank">pdf</a>]

<h2>Balancing Constraints and Rewards with Meta-Gradient D4PG. (arXiv:2010.06324v1 [cs.LG])</h2>
<h3>Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, Timothy Mann</h3>
<p>Deploying Reinforcement Learning (RL) agents to solve real-world applications
often requires satisfying complex system constraints. Often the constraint
thresholds are incorrectly set due to the complex nature of a system or the
inability to verify the thresholds offline (e.g, no simulator or reasonable
offline evaluation procedure exists). This results in solutions where a task
cannot be solved without violating the constraints. However, in many real-world
cases, constraint violations are undesirable yet they are not catastrophic,
motivating the need for soft-constrained RL approaches. We present two
soft-constrained RL approaches that utilize meta-gradients to find a good
trade-off between expected return and minimizing constraint violations. We
demonstrate the effectiveness of these approaches by showing that they
consistently outperform the baselines across four different Mujoco domains.
</p>
<a href="http://arxiv.org/abs/2010.06324" target="_blank">arXiv:2010.06324</a> [<a href="http://arxiv.org/pdf/2010.06324" target="_blank">pdf</a>]

<h2>Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration. (arXiv:2010.06349v1 [cs.CV])</h2>
<h3>Zongxin Yang, Yunchao Wei, Yi Yang</h3>
<p>This paper investigates the principles of embedding learning to tackle the
challenging semi-supervised video object segmentation. Unlike previous
practices that focus on exploring the embedding learning of foreground object
(s), we consider background should be equally treated. Thus, we propose a
Collaborative video object segmentation by Foreground-Background Integration
(CFBI) approach. CFBI separates the feature embedding into the foreground
object region and its corresponding background region, implicitly promoting
them to be more contrastive and improving the segmentation results accordingly.
Moreover, CFBI performs both pixel-level matching processes and instance-level
attention mechanisms between the reference and the predicted sequence, making
CFBI robust to various object scales. Based on CFBI, we introduce a multi-scale
matching structure and propose an Atrous Matching strategy, resulting in a more
robust and efficient framework, CFBI+. We conduct extensive experiments on two
popular benchmarks, i.e., DAVIS and YouTube-VOS. Without applying any simulated
data for pre-training, our CFBI+ achieves the performance (J&amp;F) of 82.9% and
82.8%, outperforming all the other state-of-the-art methods. Code:
https://github.com/z-x-yang/CFBI.
</p>
<a href="http://arxiv.org/abs/2010.06349" target="_blank">arXiv:2010.06349</a> [<a href="http://arxiv.org/pdf/2010.06349" target="_blank">pdf</a>]

<h2>CAPT: Contrastive Pre-Training for LearningDenoised Sequence Representations. (arXiv:2010.06351v1 [cs.CL])</h2>
<h3>Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun</h3>
<p>Pre-trained self-supervised models such as BERT have achieved striking
success in learning sequence representations, especially for natural language
processing. These models typically corrupt the given sequences with certain
types of noise, such as masking, shuffling, or substitution, and then try to
recover the original input. However, such pre-training approaches are prone to
learning representations that are covariant with the noise, leading to the
discrepancy between the pre-training and fine-tuning stage. To remedy this, we
present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence
representations. The proposed CAPT encourages the consistency between
representations of the original sequence and its corrupted version via
unsupervised instance-wise training signals. In this way, it not only
alleviates the pretrain-finetune discrepancy induced by the noise of
pre-training, but also aids the pre-trained model in better capturing global
semantics of the input via more effective sentence-level supervision. Different
from most prior work that focuses on a particular modality, comprehensive
empirical evidence on 11 natural language understanding and cross-modal tasks
illustrates that CAPT is applicable for both language and vision-language
tasks, and obtains surprisingly consistent improvement, including 0.6% absolute
gain on GLUE benchmarks and 0.8% absolute increment on NLVR.
</p>
<a href="http://arxiv.org/abs/2010.06351" target="_blank">arXiv:2010.06351</a> [<a href="http://arxiv.org/pdf/2010.06351" target="_blank">pdf</a>]

<h2>The Tatoeba Translation Challenge -- Realistic Data Sets for Low Resource and Multilingual MT. (arXiv:2010.06354v1 [cs.CL])</h2>
<h3>J&#xf6;rg Tiedemann</h3>
<p>This paper describes the development of a new benchmark for machine
translation that provides training and test data for thousands of language
pairs covering over 500 languages and tools for creating state-of-the-art
translation models from that collection. The main goal is to trigger the
development of open translation tools and models with a much broader coverage
of the World's languages. Using the package it is possible to work on realistic
low-resource scenarios avoiding artificially reduced setups that are common
when demonstrating zero-shot or few-shot learning. For the first time, this
package provides a comprehensive collection of diverse data sets in hundreds of
languages with systematic language and script annotation and data splits to
extend the narrow coverage of existing benchmarks. Together with the data
release, we also provide a growing number of pre-trained baseline models for
individual language pairs and selected language groups.
</p>
<a href="http://arxiv.org/abs/2010.06354" target="_blank">arXiv:2010.06354</a> [<a href="http://arxiv.org/pdf/2010.06354" target="_blank">pdf</a>]

<h2>A Generalized Zero-Shot Framework for Emotion Recognition from Body Gestures. (arXiv:2010.06362v1 [cs.CV])</h2>
<h3>Jinting Wu, Yujia Zhang, Xiaoguang Zhao</h3>
<p>Although automatic emotion recognition from facial expressions and speech has
made remarkable progress, emotion recognition from body gestures has not been
thoroughly explored. People often use a variety of body language to express
emotions, and it is difficult to enumerate all emotional body gestures and
collect enough samples for each category. Therefore, recognizing new emotional
body gestures is critical for better understanding human emotions. However, the
existing methods fail to accurately determine which emotional state a new body
gesture belongs to. In order to solve this problem, we introduce a Generalized
Zero-Shot Learning (GZSL) framework, which consists of three branches to infer
the emotional state of the new body gestures with only their semantic
descriptions. The first branch is a Prototype-Based Detector (PBD) which is
used to determine whether an sample belongs to a seen body gesture category and
obtain the prediction results of the samples from the seen categories. The
second branch is a Stacked AutoEncoder (StAE) with manifold regularization,
which utilizes semantic representations to predict samples from unseen
categories. Note that both of the above branches are for body gesture
recognition. We further add an emotion classifier with a softmax layer as the
third branch in order to better learn the feature representations for this
emotion classification task. The input features for these three branches are
learned by a shared feature extraction network, i.e., a Bidirectional Long
Short-Term Memory Networks (BLSTM) with a self-attention module. We treat these
three branches as subtasks and use multi-task learning strategies for joint
training. The performance of our framework on an emotion recognition dataset is
significantly superior to the traditional method of emotion classification and
state-of-the-art zero-shot learning methods.
</p>
<a href="http://arxiv.org/abs/2010.06362" target="_blank">arXiv:2010.06362</a> [<a href="http://arxiv.org/pdf/2010.06362" target="_blank">pdf</a>]

<h2>Coarse and fine-grained automatic cropping deep convolutional neural network. (arXiv:2010.06379v1 [cs.CV])</h2>
<h3>Jingfei Chang</h3>
<p>The existing convolutional neural network pruning algorithms can be divided
into two categories: coarse-grained clipping and fine-grained clipping. This
paper proposes a coarse and fine-grained automatic pruning algorithm, which can
achieve more efficient and accurate compression acceleration for convolutional
neural networks. First, cluster the intermediate feature maps of the
convolutional neural network to obtain the network structure after
coarse-grained clipping, and then use the particle swarm optimization algorithm
to iteratively search and optimize the structure. Finally, the optimal network
tailoring substructure is obtained.
</p>
<a href="http://arxiv.org/abs/2010.06379" target="_blank">arXiv:2010.06379</a> [<a href="http://arxiv.org/pdf/2010.06379" target="_blank">pdf</a>]

<h2>Escalation Prediction using Feature Engineering: Addressing Support Ticket Escalations within IBM's Ecosystem. (arXiv:2010.06390v1 [cs.SE])</h2>
<h3>Lloyd Montgomery</h3>
<p>Large software organizations handle many customer support issues every day in
the form of bug reports, feature requests, and general misunderstandings as
submitted by customers. Strategies to gather, analyze, and negotiate
requirements are complemented by efforts to manage customer input after
products have been deployed. For the latter, support tickets are key in
allowing customers to submit their issues, bug reports, and feature requests.
Whenever insufficient attention is given to support issues, there is a chance
customers will escalate their issues, and escalation to management is
time-consuming and expensive, especially for large organizations managing
hundreds of customers and thousands of support tickets. This thesis provides a
step towards simplifying the job for support analysts and managers,
particularly in predicting the risk of escalating support tickets. In a field
study at our large industrial partner, IBM, a design science methodology was
employed to characterize the support process and data available to IBM analysts
in managing escalations. Through iterative cycles of design and evaluation,
support analysts' expert knowledge about their customers was translated into
features of a support ticket model to be implemented into a Machine Learning
model to predict support ticket escalations. The Machine Learning model was
trained and evaluated on over 2.5 million support tickets and 10,000
escalations, obtaining a recall of 79.9% and an 80.8% reduction in the workload
for support analysts looking to identify support tickets at risk of escalation.
The features developed in the Support Ticket Model are designed to serve as a
starting place for organizations interested in implementing the model to
predict support ticket escalations, and for future researchers to build on to
advance research in Escalation Prediction.
</p>
<a href="http://arxiv.org/abs/2010.06390" target="_blank">arXiv:2010.06390</a> [<a href="http://arxiv.org/pdf/2010.06390" target="_blank">pdf</a>]

<h2>ProportionNet: Balancing Fairness and Revenue for Auction Design with Deep Learning. (arXiv:2010.06398v1 [cs.GT])</h2>
<h3>Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J. Curry, Samuel Dooley, Ping-yeh Chiang, Tom Goldstein, John P. Dickerson</h3>
<p>The design of revenue-maximizing auctions with strong incentive guarantees is
a core concern of economic theory. Computational auctions enable online
advertising, sourcing, spectrum allocation, and myriad financial markets.
Analytic progress in this space is notoriously difficult; since Myerson's 1981
work characterizing single-item optimal auctions, there has been limited
progress outside of restricted settings. A recent paper by D\"utting et al.
circumvents analytic difficulties by applying deep learning techniques to,
instead, approximate optimal auctions. In parallel, new research from Ilvento
et al. and other groups has developed notions of fairness in the context of
auction design. Inspired by these advances, in this paper, we extend techniques
for approximating auctions using deep learning to address concerns of fairness
while maintaining high revenue and strong incentive guarantees.
</p>
<a href="http://arxiv.org/abs/2010.06398" target="_blank">arXiv:2010.06398</a> [<a href="http://arxiv.org/pdf/2010.06398" target="_blank">pdf</a>]

<h2>Which Model to Transfer? Finding the Needle in the Growing Haystack. (arXiv:2010.06402v1 [cs.LG])</h2>
<h3>Cedric Renggli, Andr&#xe9; Susano Pinto, Luka Rimanic, Joan Puigcerver, Carlos Riquelme, Ce Zhang, Mario Lucic</h3>
<p>Transfer learning has been recently popularized as a data-efficient
alternative to training models from scratch, in particular in vision and NLP
where it provides a remarkably solid baseline. The emergence of rich model
repositories, such as TensorFlow Hub, enables the practitioners and researchers
to unleash the potential of these models across a wide range of downstream
tasks. As these repositories keep growing exponentially, efficiently selecting
a good model for the task at hand becomes paramount. We provide a formalization
of this problem through a familiar notion of regret and introduce the
predominant strategies, namely task-agnostic (e.g. picking the highest scoring
ImageNet model) and task-aware search strategies (such as linear or kNN
evaluation). We conduct a large-scale empirical study and show that both
task-agnostic and task-aware methods can yield high regret. We then propose a
simple and computationally efficient hybrid search strategy which outperforms
the existing approaches. We highlight the practical benefits of the proposed
solution on a set of 19 diverse vision tasks.
</p>
<a href="http://arxiv.org/abs/2010.06402" target="_blank">arXiv:2010.06402</a> [<a href="http://arxiv.org/pdf/2010.06402" target="_blank">pdf</a>]

<h2>RANDGAN: Randomized Generative Adversarial Network for Detection of COVID-19 in Chest X-ray. (arXiv:2010.06418v1 [eess.IV])</h2>
<h3>Saman Motamed, Patrik Rogalla, Farzad Khalvati</h3>
<p>COVID-19 spread across the globe at an immense rate has left healthcare
systems incapacitated to diagnose and test patients at the needed rate. Studies
have shown promising results for detection of COVID-19 from viral bacterial
pneumonia in chest X-rays. Automation of COVID-19 testing using medical images
can speed up the testing process of patients where health care systems lack
sufficient numbers of the reverse-transcription polymerase chain reaction
(RT-PCR) tests. Supervised deep learning models such as convolutional neural
networks (CNN) need enough labeled data for all classes to correctly learn the
task of detection. Gathering labeled data is a cumbersome task and requires
time and resources which could further strain health care systems and
radiologists at the early stages of a pandemic such as COVID-19. In this study,
we propose a randomized generative adversarial network (RANDGAN) that detects
images of an unknown class (COVID-19) from known and labelled classes (Normal
and Viral Pneumonia) without the need for labels and training data from the
unknown class of images (COVID-19). We used the largest publicly available
COVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia,
and COVID-19 images from multiple public databases. In this work, we use
transfer learning to segment the lungs in the COVIDx dataset. Next, we show why
segmentation of the region of interest (lungs) is vital to correctly learn the
task of classification, specifically in datasets that contain images from
different resources as it is the case for the COVIDx dataset. Finally, we show
improved results in detection of COVID-19 cases using our generative model
(RANDGAN) compared to conventional generative adversarial networks (GANs) for
anomaly detection in medical images, improving the area under the ROC curve
from 0.71 to 0.77.
</p>
<a href="http://arxiv.org/abs/2010.06418" target="_blank">arXiv:2010.06418</a> [<a href="http://arxiv.org/pdf/2010.06418" target="_blank">pdf</a>]

<h2>Multilingual Argument Mining: Datasets and Analysis. (arXiv:2010.06432v1 [cs.CL])</h2>
<h3>Orith Toledo-Ronen, Matan Orbach, Yonatan Bilu, Artem Spector, Noam Slonim</h3>
<p>The growing interest in argument mining and computational argumentation
brings with it a plethora of Natural Language Understanding (NLU) tasks and
corresponding datasets. However, as with many other NLU tasks, the dominant
language is English, with resources in other languages being few and far
between. In this work, we explore the potential of transfer learning using the
multilingual BERT model to address argument mining tasks in non-English
languages, based on English datasets and the use of machine translation. We
show that such methods are well suited for classifying the stance of arguments
and detecting evidence, but less so for assessing the quality of arguments,
presumably because quality is harder to preserve under translation. In
addition, focusing on the translate-train approach, we show how the choice of
languages for translation, and the relations among them, affect the accuracy of
the resultant model. Finally, to facilitate evaluation of transfer learning on
argument mining tasks, we provide a human-generated dataset with more than 10k
arguments in multiple languages, as well as machine translation of the English
datasets.
</p>
<a href="http://arxiv.org/abs/2010.06432" target="_blank">arXiv:2010.06432</a> [<a href="http://arxiv.org/pdf/2010.06432" target="_blank">pdf</a>]

<h2>RMDL: Recalibrated multi-instance deep learning for whole slide gastric image classification. (arXiv:2010.06440v1 [cs.CV])</h2>
<h3>Shujun Wang, Yaxi Zhu, Lequan Yu, Hao Chen, Huangjing Lin, Xiangbo Wan, Xinjuan Fan, Pheng-Ann Hen</h3>
<p>The whole slide histopathology images (WSIs) play a critical role in gastric
cancer diagnosis. However, due to the large scale of WSIs and various sizes of
the abnormal area, how to select informative regions and analyze them are quite
challenging during the automatic diagnosis process. The multi-instance learning
based on the most discriminative instances can be of great benefit for whole
slide gastric image diagnosis. In this paper, we design a recalibrated
multi-instance deep learning method (RMDL) to address this challenging problem.
We first select the discriminative instances, and then utilize these instances
to diagnose diseases based on the proposed RMDL approach. The designed RMDL
network is capable of capturing instance-wise dependencies and recalibrating
instance features according to the importance coefficient learned from the
fused features. Furthermore, we build a large whole-slide gastric
histopathology image dataset with detailed pixel-level annotations.
Experimental results on the constructed gastric dataset demonstrate the
significant improvement on the accuracy of our proposed framework compared with
other state-of-the-art multi-instance learning methods. Moreover, our method is
general and can be extended to other diagnosis tasks of different cancer types
based on WSIs.
</p>
<a href="http://arxiv.org/abs/2010.06440" target="_blank">arXiv:2010.06440</a> [<a href="http://arxiv.org/pdf/2010.06440" target="_blank">pdf</a>]

<h2>Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource Language. (arXiv:2010.06447v1 [cs.CL])</h2>
<h3>Dan John Velasco</h3>
<p>Low-resource languages such as Filipino suffer from data scarcity which makes
it challenging to develop NLP applications for Filipino language. The use of
Transfer Learning (TL) techniques alleviates this problem in low-resource
setting. In recent years, transformer-based models are proven to be effective
in low-resource tasks but faces challenges in accessibility due to its high
compute and memory requirements. There's a need for a cheaper but effective
alternative. This paper has three contributions. First, release a pre-trained
AWD LSTM language model for Filipino language. Second, benchmark AWD LSTM in
the Hate Speech classification task and show that it performs on par with
transformer-based models. Third, analyze the degradation rate of AWD-LSTM to
smaller data using degradation test and compare it with transformer-based
models.

-----

Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na
datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer
Learning (TL) techniques ay malaking tulong para sa mga pagkakataong gipit tayo
sa datos. Sa mga nagdaang taon, nanaig ang mga transformer-based TL techniques
pagdating sa low-resource tasks ngunit ito ay magastos sa resources. Kaya
nangangailangan ng mas mura pero epektibong alternatibo. Ang papel na ito ay
may tatlong kontribusyon. Una, maglabas ng pre-trained AWD LSTM language model
sa wikang Filipino upang maging tuntungan sa pagbuo ng mga NLP applications sa
wikang Filipino. Pangalawa, mag benchmark ng AWD LSTM sa Hate Speech
classification task at ipakita na kayang nitong makipagsabayan sa mga
transformer-based models. Pangatlo, suriin ang degradation rate ng AWD-LSTM sa
mas maliit na data gamit ang degradation test at ikumpara ito sa mga
transformer-based models.
</p>
<a href="http://arxiv.org/abs/2010.06447" target="_blank">arXiv:2010.06447</a> [<a href="http://arxiv.org/pdf/2010.06447" target="_blank">pdf</a>]

<h2>A review of 3D human pose estimation algorithms for markerless motion capture. (arXiv:2010.06449v1 [cs.CV])</h2>
<h3>Yann Desmarais, Denis Mottet, Pierre Slangen, Philippe Montesinos</h3>
<p>Human pose estimation (HPE) in 3D is an active research field that have many
applications in entertainment, health and sport science, robotics. In the last
five years markerless motion captures techniques have seen their average error
decrease from more than 10cm to less than 2cm today. This evolution is mainly
driven by the improvements in 2D pose estimation task that benefited from the
use of convolutional networks. However with the multiplication of different
approaches it can be difficult to identify what is more adapted to the
specifics of any applications. We suggest to classify existing methods with a
taxonomy based on the performance criteria of accuracy, speed and robustness.
We review more than twenty methods from the last three years. Additionally we
analyze the metrics, benchmarks and structure of the different pose estimation
systems and propose several direction for future research. We hope to offer a
good introduction to 3D markerless pose estimation as well as discussing the
leading contemporary algorithms.
</p>
<a href="http://arxiv.org/abs/2010.06449" target="_blank">arXiv:2010.06449</a> [<a href="http://arxiv.org/pdf/2010.06449" target="_blank">pdf</a>]

<h2>The DongNiao International Birds 10000 Dataset. (arXiv:2010.06454v1 [cs.CV])</h2>
<h3>Jian Mei, Hao Dong</h3>
<p>DongNiao International Birds 10000 (DIB-10K) is a challenging image dataset
which has more than 10 thousand different types of birds. It was created to
enable the study of machine learning and also ornithology research. DIB-10K
does not own the copyright of these images. It only provides thumbnails of
images, in a way similar to ImageNet.
</p>
<a href="http://arxiv.org/abs/2010.06454" target="_blank">arXiv:2010.06454</a> [<a href="http://arxiv.org/pdf/2010.06454" target="_blank">pdf</a>]

<h2>CrypTFlow2: Practical 2-Party Secure Inference. (arXiv:2010.06457v1 [cs.CR])</h2>
<h3>Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem Rastogi, Rahul Sharma</h3>
<p>We present CrypTFlow2, a cryptographic framework for secure inference over
realistic Deep Neural Networks (DNNs) using secure 2-party computation.
CrypTFlow2 protocols are both correct -- i.e., their outputs are bitwise
equivalent to the cleartext execution -- and efficient -- they outperform the
state-of-the-art protocols in both latency and scale. At the core of
CrypTFlow2, we have new 2PC protocols for secure comparison and division,
designed carefully to balance round and communication complexity for secure
inference tasks. Using CrypTFlow2, we present the first secure inference over
ImageNet-scale DNNs like ResNet50 and DenseNet121. These DNNs are at least an
order of magnitude larger than those considered in the prior work of 2-party
DNN inference. Even on the benchmarks considered by prior work, CrypTFlow2
requires an order of magnitude less communication and 20x-30x less time than
the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2010.06457" target="_blank">arXiv:2010.06457</a> [<a href="http://arxiv.org/pdf/2010.06457" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Real-Time Optimization of Pumps in Water Distribution Systems. (arXiv:2010.06460v1 [cs.AI])</h2>
<h3>Gergely Hajgat&#xf3;, Gy&#xf6;rgy Pa&#xe1;l, B&#xe1;lint Gyires-T&#xf3;th</h3>
<p>Real-time control of pumps can be an infeasible task in water distribution
systems (WDSs) because the calculation to find the optimal pump speeds is
resource-intensive. The computational need cannot be lowered even with the
capabilities of smart water networks when conventional optimization techniques
are used. Deep reinforcement learning (DRL) is presented here as a controller
of pumps in two WDSs. An agent based on a dueling deep q-network is trained to
maintain the pump speeds based on instantaneous nodal pressure data. General
optimization techniques (e.g., Nelder-Mead method, differential evolution)
serve as baselines. The total efficiency achieved by the DRL agent compared to
the best performing baseline is above 0.98, whereas the speedup is around 2x
compared to that. The main contribution of the presented approach is that the
agent can run the pumps in real-time because it depends only on measurement
data. If the WDS is replaced with a hydraulic simulation, the agent still
outperforms conventional techniques in search speed.
</p>
<a href="http://arxiv.org/abs/2010.06460" target="_blank">arXiv:2010.06460</a> [<a href="http://arxiv.org/pdf/2010.06460" target="_blank">pdf</a>]

<h2>Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge. (arXiv:2010.06469v1 [cs.CV])</h2>
<h3>Clemens-Alexander Brust, Bj&#xf6;rn Barz, Joachim Denzler</h3>
<p>Noisy data, crawled from the web or supplied by volunteers such as Mechanical
Turkers or citizen scientists, is considered an alternative to professionally
labeled data. There has been research focused on mitigating the effects of
label noise. It is typically modeled as inaccuracy, where the correct label is
replaced by an incorrect label from the same set. We consider an additional
dimension of label noise: imprecision. For example, a non-breeding snow bunting
is labeled as a bird. This label is correct, but not as precise as the task
requires.

Standard softmax classifiers cannot learn from such a weak label because they
consider all classes mutually exclusive, which non-breeding snow bunting and
bird are not. We propose CHILLAX (Class Hierarchies for Imprecise Label
Learning and Annotation eXtrapolation), a method based on hierarchical
classification, to fully utilize labels of any precision.

Experiments on noisy variants of NABirds and ILSVRC2012 show that our method
outperforms strong baselines by as much as 16.4 percentage points, and the
current state of the art by up to 3.9 percentage points.
</p>
<a href="http://arxiv.org/abs/2010.06469" target="_blank">arXiv:2010.06469</a> [<a href="http://arxiv.org/pdf/2010.06469" target="_blank">pdf</a>]

<h2>3DMolNet: A Generative Network for Molecular Structures. (arXiv:2010.06477v1 [q-bio.BM])</h2>
<h3>Vitali Nesterov, Mario Wieser, Volker Roth</h3>
<p>With the recent advances in machine learning for quantum chemistry, it is now
possible to predict the chemical properties of compounds and to generate novel
molecules. Existing generative models mostly use a string- or graph-based
representation, but the precise three-dimensional coordinates of the atoms are
usually not encoded. First attempts in this direction have been proposed, where
autoregressive or GAN-based models generate atom coordinates. Those either lack
a latent space in the autoregressive setting, such that a smooth exploration of
the compound space is not possible, or cannot generalize to varying chemical
compositions. We propose a new approach to efficiently generate molecular
structures that are not restricted to a fixed size or composition. Our model is
based on the variational autoencoder which learns a translation-, rotation-,
and permutation-invariant low-dimensional representation of molecules. Our
experiments yield a mean reconstruction error below 0.05 Angstrom,
outperforming the current state-of-the-art methods by a factor of four, and
which is even lower than the spatial quantization error of most chemical
descriptors. The compositional and structural validity of newly generated
molecules has been confirmed by quantum chemical methods in a set of
experiments.
</p>
<a href="http://arxiv.org/abs/2010.06477" target="_blank">arXiv:2010.06477</a> [<a href="http://arxiv.org/pdf/2010.06477" target="_blank">pdf</a>]

<h2>Credit card fraud detection using machine learning: A survey. (arXiv:2010.06479v1 [cs.LG])</h2>
<h3>Yvan Lucas, Johannes Jurgovsky</h3>
<p>Credit card fraud has emerged as major problem in the electronic payment
sector. In this survey, we study data-driven credit card fraud detection
particularities and several machine learning methods to address each of its
intricate challenges with the goal to identify fraudulent transactions that
have been issued illegitimately on behalf of the rightful card owner. In
particular, we first characterize a typical credit card detection task: the
dataset and its attributes, the metric choice along with some methods to handle
such unbalanced datasets. These questions are the entry point of every credit
card fraud detection problem. Then we focus on dataset shift (sometimes called
concept drift), which refers to the fact that the underlying distribution
generating the dataset evolves over times: For example, card holders may change
their buying habits over seasons and fraudsters may adapt their strategies.
This phenomenon may hinder the usage of machine learning methods for real world
datasets such as credit card transactions datasets. Afterwards we highlights
different approaches used in order to capture the sequential properties of
credit card transactions. These approaches range from feature engineering
techniques (transactions aggregations for example) to proper sequence modeling
methods such as recurrent neural networks (LSTM) or graphical models (hidden
markov models).
</p>
<a href="http://arxiv.org/abs/2010.06479" target="_blank">arXiv:2010.06479</a> [<a href="http://arxiv.org/pdf/2010.06479" target="_blank">pdf</a>]

<h2>Simultaneously forecasting global geomagnetic activity using Recurrent Networks. (arXiv:2010.06487v1 [cs.LG])</h2>
<h3>Charles Topliff, Morris Cohen, William Bristow</h3>
<p>Many systems used by society are extremely vulnerable to space weather events
such as solar flares and geomagnetic storms which could potentially cause
catastrophic damage. In recent years, many works have emerged to provide early
warning to such systems by forecasting these events through some proxy, but
these approaches have largely focused on a specific phenomenon. We present a
sequence-to-sequence learning approach to the problem of forecasting global
space weather conditions at an hourly resolution. This approach improves upon
other work in this field by simultaneously forecasting several key proxies for
geomagnetic activity up to 6 hours in advance. We demonstrate an improvement
over the best currently known predictor of geomagnetic storms, and an
improvement over a persistence baseline several hours in advance.
</p>
<a href="http://arxiv.org/abs/2010.06487" target="_blank">arXiv:2010.06487</a> [<a href="http://arxiv.org/pdf/2010.06487" target="_blank">pdf</a>]

<h2>Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning. (arXiv:2010.06491v1 [cs.RO])</h2>
<h3>Brian Ichter, Pierre Sermanet, Corey Lynch</h3>
<p>Long-horizon planning in realistic environments requires the ability to
reason over sequential tasks in high-dimensional state spaces with complex
dynamics. Classical motion planning algorithms, such as rapidly-exploring
random trees, are capable of efficiently exploring large state spaces and
computing long-horizon, sequential plans. However, these algorithms are
generally challenged with complex, stochastic, and high-dimensional state
spaces as well as in the presence of narrow passages, which naturally emerge in
tasks that interact with the environment. Machine learning offers a promising
solution for its ability to learn general policies that can handle complex
interactions and high-dimensional observations. However, these policies are
generally limited in horizon length. Our approach, Broadly-Exploring,
Local-policy Trees (BELT), merges these two approaches to leverage the
strengths of both through a task-conditioned, model-based tree search. BELT
uses an RRT-inspired tree search to efficiently explore the state space.
Locally, the exploration is guided by a task-conditioned, learned policy
capable of performing general short-horizon tasks. This task space can be quite
general and abstract; its only requirements are to be sampleable and to
well-cover the space of useful tasks. This search is aided by a
task-conditioned model that temporally extends dynamics propagation to allow
long-horizon search and sequential reasoning over tasks. BELT is demonstrated
experimentally to be able to plan long-horizon, sequential trajectories with a
goal conditioned policy and generate plans that are robust.
</p>
<a href="http://arxiv.org/abs/2010.06491" target="_blank">arXiv:2010.06491</a> [<a href="http://arxiv.org/pdf/2010.06491" target="_blank">pdf</a>]

<h2>Satellite Image Classification with Deep Learning. (arXiv:2010.06497v1 [cs.CV])</h2>
<h3>Mark Pritt, Gary Chern</h3>
<p>Satellite imagery is important for many applications including disaster
response, law enforcement, and environmental monitoring. These applications
require the manual identification of objects and facilities in the imagery.
Because the geographic expanses to be covered are great and the analysts
available to conduct the searches are few, automation is required. Yet
traditional object detection and classification algorithms are too inaccurate
and unreliable to solve the problem. Deep learning is a family of machine
learning algorithms that have shown promise for the automation of such tasks.
It has achieved success in image understanding by means of convolutional neural
networks. In this paper we apply them to the problem of object and facility
recognition in high-resolution, multi-spectral satellite imagery. We describe a
deep learning system for classifying objects and facilities from the IARPA
Functional Map of the World (fMoW) dataset into 63 different classes. The
system consists of an ensemble of convolutional neural networks and additional
neural networks that integrate satellite metadata with image features. It is
implemented in Python using the Keras and TensorFlow deep learning libraries
and runs on a Linux server with an NVIDIA Titan X graphics card. At the time of
writing the system is in 2nd place in the fMoW TopCoder competition. Its total
accuracy is 83%, the F1 score is 0.797, and it classifies 15 of the classes
with accuracies of 95% or better.
</p>
<a href="http://arxiv.org/abs/2010.06497" target="_blank">arXiv:2010.06497</a> [<a href="http://arxiv.org/pdf/2010.06497" target="_blank">pdf</a>]

<h2>Cross-Domain Few-Shot Learning by Representation Fusion. (arXiv:2010.06498v1 [cs.LG])</h2>
<h3>Thomas Adler, Johannes Brandstetter, Michael Widrich, Andreas Mayr, David Kreil, Michael Kopp, G&#xfc;nter Klambauer, Sepp Hochreiter</h3>
<p>In order to quickly adapt to new data, few-shot learning aims at learning
from few examples, often by using already acquired knowledge. The new data
often differs from the previously seen data due to a domain shift, that is, a
change of the input-target distribution. While several methods perform well on
small domain shifts like new target classes with similar inputs, larger domain
shifts are still challenging. Large domain shifts may result in high-level
concepts that are not shared between the original and the new domain. However,
low-level concepts like edges in images might still be shared and useful. For
cross-domain few-shot learning, we suggest representation fusion to unify
different abstraction levels of a deep neural network into one representation.
We propose Cross-domain Hebbian Ensemble Few-shot learning (CHEF), which
achieves representation fusion by an ensemble of Hebbian learners acting on
different layers of a deep neural network that was trained on the original
domain. On the few-shot datasets miniImagenet and tieredImagenet, where the
domain shift is small, CHEF is competitive with state-of-the-art methods. On
cross-domain few-shot benchmark challenges with larger domain shifts, CHEF
establishes novel state-of-the-art results in all categories. We further apply
CHEF on a real-world cross-domain application in drug discovery. We consider a
domain shift from bioactive molecules to environmental chemicals and drugs with
twelve associated toxicity prediction tasks. On these tasks, that are highly
relevant for computational drug discovery, CHEF significantly outperforms all
its competitors. Github: https://github.com/ml-jku/chef
</p>
<a href="http://arxiv.org/abs/2010.06498" target="_blank">arXiv:2010.06498</a> [<a href="http://arxiv.org/pdf/2010.06498" target="_blank">pdf</a>]

<h2>LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis. (arXiv:2010.06499v1 [cs.CV])</h2>
<h3>Quan Huu Cap, Hiroki Tani, Hiroyuki Uga, Satoshi Kagiwada, Hitoshi Iyatomi</h3>
<p>The collection of high-resolution training data is crucial in building robust
plant disease diagnosis systems, since such data have a significant impact on
diagnostic performance. However, they are very difficult to obtain and are not
always available in practice. Deep learning-based techniques, and particularly
generative adversarial networks (GANs), can be applied to generate high-quality
super-resolution images, but these methods often produce unexpected artifacts
that can lower the diagnostic performance. In this paper, we propose a novel
artifact-suppression super-resolution method that is specifically designed for
diagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution
(LASSR). Thanks to its own artifact removal module that detects and suppresses
artifacts to a considerable extent, LASSR can generate much more pleasing,
high-quality images compared to the state-of-the-art ESRGAN model. Experiments
based on a five-class cucumber disease (including healthy) discrimination model
show that training with data generated by LASSR significantly boosts the
performance on an unseen test dataset by nearly 22% compared with the baseline,
and that our approach is more than 2% better than a model trained with images
generated by ESRGAN.
</p>
<a href="http://arxiv.org/abs/2010.06499" target="_blank">arXiv:2010.06499</a> [<a href="http://arxiv.org/pdf/2010.06499" target="_blank">pdf</a>]

<h2>Transfer Learning and SpecAugment applied to SSVEP Based BCI Classification. (arXiv:2010.06503v1 [eess.SP])</h2>
<h3>Pedro R. A. S. Bassi, Willian Rampazzo, Romis Attux</h3>
<p>In this work, we used a deep convolutional neural network (DCNN) to classify
electroencephalography (EEG) signals in a steady-state visually evoked
potentials (SSVEP) based brain-computer interface (BCI). The raw EEG signals
were converted to spectrograms and served as input to train a DCNN using the
transfer learning technique. We applied a second technique, data augmentation,
mostly SpecAugment, generally employed to speech recognition. The results, when
excluding the evaluated user's data from the fine-tuning process, reached 99.3%
mean test accuracy and 0.992 mean F1 score on 35 subjects from an open dataset.
</p>
<a href="http://arxiv.org/abs/2010.06503" target="_blank">arXiv:2010.06503</a> [<a href="http://arxiv.org/pdf/2010.06503" target="_blank">pdf</a>]

<h2>Piece-wise Matching Layer in Representation Learning for ECG Classification. (arXiv:2010.06510v1 [eess.SP])</h2>
<h3>Behzad Ghazanfari, Fatemeh Afghah, Sixian Zhang</h3>
<p>This paper proposes piece-wise matching layer as a novel layer in
representation learning methods for electrocardiogram (ECG) classification.
Despite the remarkable performance of representation learning methods in the
analysis of time series, there are still several challenges associated with
these methods ranging from the complex structures of methods, the lack of
generality of solutions, the need for expert knowledge, and large-scale
training datasets. We introduce the piece-wise matching layer that works based
on two levels to address some of the aforementioned challenges. At the first
level, a set of morphological, statistical, and frequency features and
comparative forms of them are computed based on each periodic part and its
neighbors. At the second level, these features are modified by predefined
transformation functions based on a receptive field scenario. Several scenarios
of offline processing, incremental processing, fixed sliding receptive field,
and event-based triggering receptive field can be implemented based on the
choice of length and mechanism of indicating the receptive field. We propose
dynamic time wrapping as a mechanism that indicates a receptive field based on
event triggering tactics. To evaluate the performance of this method in time
series analysis, we applied the proposed layer in two publicly available
datasets of PhysioNet competitions in 2015 and 2017 where the input data is ECG
signal. We compared the performance of our method against a variety of known
tuned methods from expert knowledge, machine learning, deep learning methods,
and the combination of them. The proposed approach improves the state of the
art in two known completions 2015 and 2017 around 4% and 7% correspondingly
while it does not rely on in advance knowledge of the classes or the possible
places of arrhythmia.
</p>
<a href="http://arxiv.org/abs/2010.06510" target="_blank">arXiv:2010.06510</a> [<a href="http://arxiv.org/pdf/2010.06510" target="_blank">pdf</a>]

<h2>Transforming Neural Network Visual Representations to Predict Human Judgments of Similarity. (arXiv:2010.06512v1 [cs.NE])</h2>
<h3>Maria Attarian, Brett D. Roads, Michael C. Mozer</h3>
<p>Deep-learning vision models have shown intriguing similarities and
differences with respect to human vision. We investigate how to bring machine
visual representations into better alignment with human representations. Human
representations are often inferred from behavioral evidence such as the
selection of an image most similar to a query image. We find that with
appropriate linear transformations of deep embeddings, we can improve
prediction of human binary choice on a data set of bird images from 72% at
baseline to 89%. We hypothesized that deep embeddings have redundant, high
(4096) dimensional representations; however, reducing the rank of these
representations results in a loss of explanatory power. We hypothesized that
the dilation transformation of representations explored in past research is too
restrictive, and indeed we found that model explanatory power can be
significantly improved with a more expressive linear transform. Most surprising
and exciting, we found that, consistent with classic psychological literature,
human similarity judgments are asymmetric: the similarity of X to Y is not
necessarily equal to the similarity of Y to X, and allowing models to express
this asymmetry improves explanatory power.
</p>
<a href="http://arxiv.org/abs/2010.06512" target="_blank">arXiv:2010.06512</a> [<a href="http://arxiv.org/pdf/2010.06512" target="_blank">pdf</a>]

<h2>Deep Learning for Recognizing Mobile Targets in Satellite Imagery. (arXiv:2010.06520v1 [cs.CV])</h2>
<h3>Mark Pritt</h3>
<p>There is an increasing demand for software that automatically detects and
classifies mobile targets such as airplanes, cars, and ships in satellite
imagery. Applications of such automated target recognition (ATR) software
include economic forecasting, traffic planning, maritime law enforcement, and
disaster response. This paper describes the extension of a convolutional neural
network (CNN) for classification to a sliding window algorithm for detection.
It is evaluated on mobile targets of the xView dataset, on which it achieves
detection and classification accuracies higher than 95%.
</p>
<a href="http://arxiv.org/abs/2010.06520" target="_blank">arXiv:2010.06520</a> [<a href="http://arxiv.org/pdf/2010.06520" target="_blank">pdf</a>]

<h2>Autotuning Search Space for Loop Transformations. (arXiv:2010.06521v1 [cs.DC])</h2>
<h3>Michael Kruse, Hal Finkel, Xingfu Wu</h3>
<p>One of the challenges for optimizing compilers is to predict whether applying
an optimization will improve its execution speed. Programmers may override the
compiler's profitability heuristic using optimization directives such as
pragmas in the source code. Machine learning in the form of autotuning can
assist users in finding the best optimizations for each platform.

In this paper we propose a loop transformation search space that takes the
form of a tree, in contrast to previous approaches that usually use vector
spaces to represent loop optimization configurations. We implemented a simple
autotuner exploring the search space and applied it to a selected set of
PolyBench kernels. While the autotuner is capable of representing every
possible sequence of loop transformations and their relations, the results
motivate the use of better search strategies such as Monte Carlo tree search to
find sophisticated loop transformations such as multilevel tiling.
</p>
<a href="http://arxiv.org/abs/2010.06521" target="_blank">arXiv:2010.06521</a> [<a href="http://arxiv.org/pdf/2010.06521" target="_blank">pdf</a>]

<h2>A Deep Learning Forecaster with Exogenous Variables for Day-Ahead Locational Marginal Price. (arXiv:2010.06525v1 [cs.LG])</h2>
<h3>Dipanwita Saha, Felipe Lopez</h3>
<p>Several approaches have been proposed to forecast day-ahead locational
marginal price (daLMP) in deregulated energy markets. The rise of deep learning
has motivated its use in energy price forecasts but most deep learning
approaches fail to accommodate for exogenous variables, which have significant
influence in the peaks and valleys of the daLMP. Accurate forecasts of the
daLMP valleys are of crucial importance for power generators since one of the
most important decisions they face is whether to sell power at a loss to
prevent incurring in shutdown and start-up costs, or to bid at production cost
and face the risk of shutting down. In this article we propose a deep learning
model that incorporates both the history of daLMP and the effect of exogenous
variables (e.g., forecasted load, weather data). A numerical study at the PJM
independent system operator (ISO) illustrates how the proposed model
outperforms traditional time series techniques while supporting risk-based
analysis of shutdown decisions.
</p>
<a href="http://arxiv.org/abs/2010.06525" target="_blank">arXiv:2010.06525</a> [<a href="http://arxiv.org/pdf/2010.06525" target="_blank">pdf</a>]

<h2>Provable Benefits of Representation Learning in Linear Bandits. (arXiv:2010.06531v1 [cs.LG])</h2>
<h3>Jiaqi Yang, Wei Hu, Jason D. Lee, Simon S. Du</h3>
<p>We study how representation learning can improve the efficiency of bandit
problems. We study the setting where we play $T$ linear bandits with dimension
$d$ concurrently, and these $T$ bandit tasks share a common $k (\ll d)$
dimensional linear representation. For the finite-action setting, we present a
new algorithm which achieves $\widetilde{O}(T\sqrt{kN} + \sqrt{dkNT})$ regret,
where $N$ is the number of rounds we play for each bandit. When $T$ is
sufficiently large, our algorithm significantly outperforms the naive algorithm
(playing $T$ bandits independently) that achieves $\widetilde{O}(T\sqrt{d N})$
regret. We also provide an $\Omega(T\sqrt{kN} + \sqrt{dkNT})$ regret lower
bound, showing that our algorithm is minimax-optimal up to poly-logarithmic
factors. Furthermore, we extend our algorithm to the infinite-action setting
and obtain a corresponding regret bound which demonstrates the benefit of
representation learning in certain regimes. We also present experiments on
synthetic and real-world data to illustrate our theoretical findings and
demonstrate the effectiveness of our proposed algorithms.
</p>
<a href="http://arxiv.org/abs/2010.06531" target="_blank">arXiv:2010.06531</a> [<a href="http://arxiv.org/pdf/2010.06531" target="_blank">pdf</a>]

<h2>A first look into the carbon footprint of federated learning. (arXiv:2010.06537v1 [cs.LG])</h2>
<h3>Xinchi Qiu, Titouan Parcolle, Daniel J. Beutel, Taner Topa, Akhil Mathur, Nicholas D. Lane</h3>
<p>Despite impressive results, deep learning-based technologies also raise
severe privacy and environmental concerns induced by the training procedure
often conducted in data centers. In response, alternatives to centralized
training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL
in particular is starting to be deployed at a global scale by companies that
must adhere to new legal demands and policies originating from governments and
the civil society for privacy protection. However, the potential environmental
impact related to FL remains unclear and unexplored. This paper offers the
first-ever systematic study of the carbon footprint of FL. First, we propose a
rigorous model to quantify the carbon footprint, hence facilitating the
investigation of the relationship between FL design and carbon emissions. Then,
we compare the carbon footprint of FL to traditional centralized learning. We
also formalize an early-stage FL optimization problem enabling the community to
consider the importance of optimizing the rate of CO$_2$ emissions jointly to
the accuracy of neural networks. Finally, we highlight and connect the reported
results to the future challenges and trends in FL to reduce its environmental
impact, including algorithms efficiency, hardware capabilities, and stronger
industry transparency.
</p>
<a href="http://arxiv.org/abs/2010.06537" target="_blank">arXiv:2010.06537</a> [<a href="http://arxiv.org/pdf/2010.06537" target="_blank">pdf</a>]

<h2>Real-Time Deep Learning Approach to Visual Servo Control and Grasp Detection for Autonomous Robotic Manipulation. (arXiv:2010.06544v1 [cs.RO])</h2>
<h3>E. G. Ribeiro, R. Q. Mendes, V. Grassi Jr</h3>
<p>In order to explore robotic grasping in unstructured and dynamic
environments, this work addresses the visual perception phase involved in the
task. This phase involves the processing of visual data to obtain the location
of the object to be grasped, its pose and the points at which the robot`s
grippers must make contact to ensure a stable grasp. For this, the Cornell
Grasping dataset is used to train a convolutional neural network that, having
an image of the robot`s workspace, with a certain object, is able to predict a
grasp rectangle that symbolizes the position, orientation and opening of the
robot`s grippers before its closing. In addition to this network, which runs in
real-time, another one is designed to deal with situations in which the object
moves in the environment. Therefore, the second network is trained to perform a
visual servo control, ensuring that the object remains in the robot`s field of
view. This network predicts the proportional values of the linear and angular
velocities that the camera must have so that the object is always in the image
processed by the grasp network. The dataset used for training was automatically
generated by a Kinova Gen3 manipulator. The robot is also used to evaluate the
applicability in real-time and obtain practical results from the designed
algorithms. Moreover, the offline results obtained through validation sets are
also analyzed and discussed regarding their efficiency and processing speed.
The developed controller was able to achieve a millimeter accuracy in the final
position considering a target object seen for the first time. To the best of
our knowledge, we have not found in the literature other works that achieve
such precision with a controller learned from scratch. Thus, this work presents
a new system for autonomous robotic manipulation with high processing speed and
the ability to generalize to several different objects.
</p>
<a href="http://arxiv.org/abs/2010.06544" target="_blank">arXiv:2010.06544</a> [<a href="http://arxiv.org/pdf/2010.06544" target="_blank">pdf</a>]

<h2>Controlling the Interaction Between Generation and Inference in Semi-Supervised Variational Autoencoders Using Importance Weighting. (arXiv:2010.06549v1 [cs.LG])</h2>
<h3>Ghazi Felhi, Joseph Leroux, Djam&#xe9; Seddah</h3>
<p>Even though Variational Autoencoders (VAEs) are widely used for
semi-supervised learning, the reason why they work remains unclear. In fact,
the addition of the unsupervised objective is most often vaguely described as a
regularization. The strength of this regularization is controlled by
down-weighting the objective on the unlabeled part of the training set. Through
an analysis of the objective of semi-supervised VAEs, we observe that they use
the posterior of the learned generative model to guide the inference model in
learning the partially observed latent variable. We show that given this
observation, it is possible to gain finer control on the effect of the
unsupervised objective on the training procedure. Using importance weighting,
we derive two novel objectives that prioritize either one of the partially
observed latent variable, or the unobserved latent variable. Experiments on the
IMDB english sentiment analysis dataset and on the AG News topic classification
dataset show the improvements brought by our prioritization mechanism and and
exhibit a behavior that is inline with our description of the inner working of
Semi-Supervised VAEs.
</p>
<a href="http://arxiv.org/abs/2010.06549" target="_blank">arXiv:2010.06549</a> [<a href="http://arxiv.org/pdf/2010.06549" target="_blank">pdf</a>]

<h2>Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!. (arXiv:2010.06572v1 [cs.CL])</h2>
<h3>Jack Hessel, Lillian Lee</h3>
<p>Modeling expressive cross-modal interactions seems crucial in multimodal
tasks, such as visual question answering. However, sometimes high-performing
black-box algorithms turn out to be mostly exploiting unimodal signals in the
data. We propose a new diagnostic tool, empirical multimodally-additive
function projection (EMAP), for isolating whether or not cross-modal
interactions improve performance for a given model on a given task. This
function projection modifies model predictions so that cross-modal interactions
are eliminated, isolating the additive, unimodal structure. For seven
image+text classification tasks (on each of which we set new state-of-the-art
benchmarks), we find that, in many cases, removing cross-modal interactions
results in little to no performance degradation. Surprisingly, this holds even
when expressive models, with capacity to consider interactions, otherwise
outperform less expressive models; thus, performance improvements, even when
present, often cannot be attributed to consideration of cross-modal feature
interactions. We hence recommend that researchers in multimodal machine
learning report the performance not only of unimodal baselines, but also the
EMAP of their best-performing model.
</p>
<a href="http://arxiv.org/abs/2010.06572" target="_blank">arXiv:2010.06572</a> [<a href="http://arxiv.org/pdf/2010.06572" target="_blank">pdf</a>]

<h2>Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach. (arXiv:2010.06579v1 [cs.LG])</h2>
<h3>Benjamin Eyre, Aparna Balagopalan, Jekaterina Novikova</h3>
<p>Despite the widely reported success of embedding-based machine learning
methods on natural language processing tasks, the use of more easily
interpreted engineered features remains common in fields such as cognitive
impairment (CI) detection. Manually engineering features from noisy text is
time and resource consuming, and can potentially result in features that do not
enhance model performance. To combat this, we describe a new approach to
feature engineering that leverages sequential machine learning models and
domain knowledge to predict which features help enhance performance. We provide
a concrete example of this method on a standard data set of CI speech and
demonstrate that CI classification accuracy improves by 2.3% over a strong
baseline when using features produced by this method. This demonstration
provides an ex-ample of how this method can be used to assist classification in
fields where interpretability is important, such as health care.
</p>
<a href="http://arxiv.org/abs/2010.06579" target="_blank">arXiv:2010.06579</a> [<a href="http://arxiv.org/pdf/2010.06579" target="_blank">pdf</a>]

<h2>Scenic: A Language for Scenario Specification and Data Generation. (arXiv:2010.06580v1 [cs.PL])</h2>
<h3>Daniel J. Fremont, Edward Kim, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia</h3>
<p>We propose a new probabilistic programming language for the design and
analysis of cyber-physical systems, especially those based on machine learning.
Specifically, we consider the problems of training a system to be robust to
rare events, testing its performance under different conditions, and debugging
failures. We show how a probabilistic programming language can help address
these problems by specifying distributions encoding interesting types of
inputs, then sampling these to generate specialized training and test data.
More generally, such languages can be used to write environment models, an
essential prerequisite to any formal analysis. In this paper, we focus on
systems like autonomous cars and robots, whose environment at any point in time
is a 'scene', a configuration of physical objects and agents. We design a
domain-specific language, Scenic, for describing scenarios that are
distributions over scenes and the behaviors of their agents over time. As a
probabilistic programming language, Scenic allows assigning distributions to
features of the scene, as well as declaratively imposing hard and soft
constraints over the scene. We develop specialized techniques for sampling from
the resulting distribution, taking advantage of the structure provided by
Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a
convolutional neural network designed to detect cars in road images, improving
its performance beyond that achieved by state-of-the-art synthetic data
generation methods.
</p>
<a href="http://arxiv.org/abs/2010.06580" target="_blank">arXiv:2010.06580</a> [<a href="http://arxiv.org/pdf/2010.06580" target="_blank">pdf</a>]

<h2>Boundary loss for highly unbalanced segmentation. (arXiv:1812.07032v3 [eess.IV] UPDATED)</h2>
<h3>Hoel Kervadec, Jihene Bouchtiba, Christian Desrosiers, Eric Granger, Jose Dolz, Ismail Ben Ayed</h3>
<p>Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy,
are based on integrals over the segmentation regions. Unfortunately, for highly
unbalanced segmentations, such regional summations have values that differ by
several orders of magnitude across classes, which affects training performance
and stability. We propose a boundary loss, which takes the form of a distance
metric on the space of contours, not regions. This can mitigate the
difficulties of highly unbalanced problems because it uses integrals over the
interface between regions instead of unbalanced integrals over the regions.
Furthermore, a boundary loss complements regional information. Inspired by
graph-based optimization techniques for computing active-contour flows, we
express a non-symmetric $L_2$ distance on the space of contours as a regional
integral, which avoids completely local differential computations involving
contour points. This yields a boundary loss expressed with the regional softmax
probability outputs of the network, which can be easily combined with standard
regional losses and implemented with any existing deep network architecture for
N-D segmentation. We report comprehensive evaluations and comparisons on
different unbalanced problems, showing that our boundary loss can yield
significant increases in performances while improving training stability. Our
code is publicly available: https://github.com/LIVIAETS/surface-loss .
</p>
<a href="http://arxiv.org/abs/1812.07032" target="_blank">arXiv:1812.07032</a> [<a href="http://arxiv.org/pdf/1812.07032" target="_blank">pdf</a>]

<h2>Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces. (arXiv:1902.06626v4 [cs.CR] UPDATED)</h2>
<h3>Mohammad Saidur Rahman, Mohsen Imani, Nate Mathews, Matthew Wright</h3>
<p>Website Fingerprinting (WF) is a type of traffic analysis attack that enables
a local passive eavesdropper to infer the victim's activity, even when the
traffic is protected by a VPN or an anonymity system like Tor. Leveraging a
deep-learning classifier, a WF attacker can gain over 98\% accuracy on Tor
traffic. In this paper, we explore a novel defense, Mockingbird, based on the
idea of adversarial examples that have been shown to undermine machine-learning
classifiers in other domains. Since the attacker gets to design and train his
attack classifier based on the defense, we first demonstrate that at a
straightforward technique for generating adversarial-example based traces fails
to protect against an attacker using adversarial training for robust
classification. We then propose Mockingbird, a technique for generating traces
that resists adversarial training by moving randomly in the space of viable
traces and not following more predictable gradients. The technique drops the
accuracy of the state-of-the-art attack hardened with adversarial training from
98\% to 38-58\% while incurring only 58\% bandwidth overhead. The attack
accuracy is generally lower than state-of-the-art defenses, and much lower when
considering Top-2 accuracy, while incurring lower bandwidth overheads.
</p>
<a href="http://arxiv.org/abs/1902.06626" target="_blank">arXiv:1902.06626</a> [<a href="http://arxiv.org/pdf/1902.06626" target="_blank">pdf</a>]

<h2>Joint Perception and Control as Inference with an Object-based Implementation. (arXiv:1903.01385v3 [cs.LG] UPDATED)</h2>
<h3>Minne Li, Zheng Tian, Pranav Nashikkar, Ian Davies, Ying Wen, Jun Wang</h3>
<p>Existing model-based reinforcement learning methods often study perception
modeling and decision making separately. We introduce joint Perception and
Control as Inference (PCI), a general framework to combine perception and
control for partially observable environments through Bayesian inference. Based
on the fact that object-level inductive biases are critical in human perceptual
learning and reasoning, we propose Object-based Perception Control (OPC), an
instantiation of PCI which manages to facilitate control using automatic
discovered object-based representations. We develop an unsupervised end-to-end
solution and analyze the convergence of the perception model update.
Experiments in a high-dimensional pixel environment demonstrate the learning
effectiveness of our object-based perception control approach. Specifically, we
show that OPC achieves good perceptual grouping quality and outperforms several
strong baselines in accumulated rewards.
</p>
<a href="http://arxiv.org/abs/1903.01385" target="_blank">arXiv:1903.01385</a> [<a href="http://arxiv.org/pdf/1903.01385" target="_blank">pdf</a>]

<h2>TTR-Based Reward for Reinforcement Learning with Implicit Model Priors. (arXiv:1903.09762v3 [cs.RO] UPDATED)</h2>
<h3>Xubo Lyu, Mo Chen</h3>
<p>Model-free reinforcement learning (RL) is a powerful approach for learning
control policies directly from high-dimensional state and observation. However,
it tends to be data-inefficient, which is especially costly in robotic learning
tasks. On the other hand, optimal control does not require data if the system
model is known, but cannot scale to models with high-dimensional states and
observations. To exploit benefits of both model-free RL and optimal control, we
propose time-to-reach-based (TTR-based) reward shaping, an optimal
control-inspired technique to alleviate data inefficiency while retaining
advantages of model-free RL. This is achieved by summarizing key system model
information using a TTR function to greatly speed up the RL process, as shown
in our simulation results. The TTR function is defined as the minimum time
required to move from any state to the goal under assumed system dynamics
constraints. Since the TTR function is computationally intractable for systems
with high-dimensional states, we compute it for approximate, lower-dimensional
system models that still captures key dynamic behaviors. Our approach can be
flexibly and easily incorporated into any model-free RL algorithm without
altering the original algorithm structure, and is compatible with any other
techniques that may facilitate the RL process. We evaluate our approach on two
representative robotic learning tasks and three well-known model-free RL
algorithms, and show significant improvements in data efficiency and
performance.
</p>
<a href="http://arxiv.org/abs/1903.09762" target="_blank">arXiv:1903.09762</a> [<a href="http://arxiv.org/pdf/1903.09762" target="_blank">pdf</a>]

<h2>DotSCN: Group Re-identification via Domain-Transferred Single and Couple Representation Learning. (arXiv:1905.04854v2 [cs.CV] UPDATED)</h2>
<h3>Ziling Huang, Zheng Wang, Chung-Chi Tsai, Shin&#x27;ichi Satoh, Chia-Wen Lin</h3>
<p>Group re-identification (G-ReID) is an important yet less-studied task. Its
challenges not only lie in appearance changes of individuals which have been
well-investigated in general person re-identification (ReID), but also derive
from group layout and membership changes. So the key task of G-ReID is to learn
representations robust to such changes. To address this issue, we propose a
Transferred Single and Couple Representation Learning Network (TSCN). Its
merits are two aspects: 1) Due to the lack of labelled training samples,
existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To
gain the superiority of deep learning models, we treat a group as multiple
persons and transfer the domain of a labeled ReID dataset to a G-ReID target
dataset style to learn single representations. 2) Taking into account the
neighborhood relationship in a group, we further propose learning a novel
couple representation between two group members, that achieves more
discriminative power in G-ReID tasks. In addition, an unsupervised weight
learning method is exploited to adaptively fuse the results of different views
together according to result patterns. Extensive experimental results
demonstrate the effectiveness of our approach that significantly outperforms
state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by
39.0\% CMC-1 on the DukeMCMT dataset.
</p>
<a href="http://arxiv.org/abs/1905.04854" target="_blank">arXiv:1905.04854</a> [<a href="http://arxiv.org/pdf/1905.04854" target="_blank">pdf</a>]

<h2>A collaborative filtering model with heterogeneous neural networks for recommender systems. (arXiv:1905.11133v3 [cs.IR] UPDATED)</h2>
<h3>Ge Fan, Wei Zeng, Shan Sun, Biao Geng, Weiyi Wang, Weibo Liu</h3>
<p>In recent years, deep neural network is introduced in recommender systems to
solve the collaborative filtering problem, which has achieved immense success
on computer vision, speech recognition and natural language processing. On one
hand, deep neural network can be used to model the auxiliary information in
recommender systems. On the other hand, it is also capable of modeling
nonlinear relationships between users and items. One advantage of deep neural
network is that the performance of the algorithm can be easily enhanced by
augmenting the depth of the neural network. However, two potential problems may
emerge when the deep neural work is exploited to model relationships between
users and items. The fundamental problem is that the complexity of the
algorithm grows significantly with the increment in the depth of the neural
network. The second one is that a deeper neural network may undermine the
accuracy of the algorithm. In order to alleviate these problems, we propose a
hybrid neural network that combines heterogeneous neural networks with
different structures. The experimental results on real datasets reveal that our
method is superior to the state-of-the-art methods in terms of the item
ranking.
</p>
<a href="http://arxiv.org/abs/1905.11133" target="_blank">arXiv:1905.11133</a> [<a href="http://arxiv.org/pdf/1905.11133" target="_blank">pdf</a>]

<h2>Taming Momentum in a Distributed Asynchronous Environment. (arXiv:1907.11612v2 [cs.LG] UPDATED)</h2>
<h3>Ido Hakimi, Saar Barkai, Moshe Gabel, Assaf Schuster</h3>
<p>Although distributed computing can significantly reduce the training time of
deep neural networks, scaling the training process while maintaining high
efficiency and final accuracy is challenging. Distributed asynchronous training
enjoys near-linear speedup, but asynchrony causes gradient staleness - the main
difficulty in scaling stochastic gradient descent to large clusters. Momentum,
which is often used to accelerate convergence and escape local minima,
exacerbates the gradient staleness, thereby hindering convergence. We propose
DANA: a novel technique for asynchronous distributed SGD with momentum that
mitigates gradient staleness by computing the gradient on an estimated future
position of the model's parameters. Thereby, we show for the first time that
momentum can be fully incorporated in asynchronous training with almost no
ramifications to final accuracy. Our evaluation on the CIFAR and ImageNet
datasets shows that DANA outperforms existing methods, in both final accuracy
and convergence speed while scaling up to a total batch size of 16K on 64
asynchronous workers.
</p>
<a href="http://arxiv.org/abs/1907.11612" target="_blank">arXiv:1907.11612</a> [<a href="http://arxiv.org/pdf/1907.11612" target="_blank">pdf</a>]

<h2>Neural Puppet: Generative Layered Cartoon Characters. (arXiv:1910.02060v3 [cs.CV] UPDATED)</h2>
<h3>Omid Poursaeed, Vladimir G. Kim, Eli Shechtman, Jun Saito, Serge Belongie</h3>
<p>We propose a learning based method for generating new animations of a cartoon
character given a few example images. Our method is designed to learn from a
traditionally animated sequence, where each frame is drawn by an artist, and
thus the input images lack any common structure, correspondences, or labels. We
express pose changes as a deformation of a layered 2.5D template mesh, and
devise a novel architecture that learns to predict mesh deformations matching
the template to a target image. This enables us to extract a common
low-dimensional structure from a diverse set of character poses. We combine
recent advances in differentiable rendering as well as mesh-aware models to
successfully align common template even if only a few character images are
available during training. In addition to coarse poses, character appearance
also varies due to shading, out-of-plane motions, and artistic effects. We
capture these subtle changes by applying an image translation network to refine
the mesh rendering, providing an end-to-end model to generate new animations of
a character with high visual quality. We demonstrate that our generative model
can be used to synthesize in-between frames and to create data-driven
deformation. Our template fitting procedure outperforms state-of-the-art
generic techniques for detecting image correspondences.
</p>
<a href="http://arxiv.org/abs/1910.02060" target="_blank">arXiv:1910.02060</a> [<a href="http://arxiv.org/pdf/1910.02060" target="_blank">pdf</a>]

<h2>Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels. (arXiv:1910.05199v4 [cs.LG] UPDATED)</h2>
<h3>Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O&#x27;Boyle, Amos Storkey</h3>
<p>Recently, different machine learning methods have been introduced to tackle
the challenging few-shot learning scenario that is, learning from a small
labeled dataset related to a specific task. Common approaches have taken the
form of meta-learning: learning to learn on the new problem given the old.
Following the recognition that meta-learning is implementing learning in a
multi-level model, we present a Bayesian treatment for the meta-learning inner
loop through the use of deep kernels. As a result we can learn a kernel that
transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach
has many advantages: is straightforward to implement as a single optimizer,
provides uncertainty quantification, and does not require estimation of
task-specific parameters. We empirically demonstrate that DKT outperforms
several state-of-the-art algorithms in few-shot classification, and is the
state of the art for cross-domain adaptation and regression. We conclude that
complex meta-learning routines can be replaced by a simpler Bayesian model
without loss of accuracy.
</p>
<a href="http://arxiv.org/abs/1910.05199" target="_blank">arXiv:1910.05199</a> [<a href="http://arxiv.org/pdf/1910.05199" target="_blank">pdf</a>]

<h2>Dynamic Graph Convolutional Networks Using the Tensor M-Product. (arXiv:1910.07643v2 [cs.LG] UPDATED)</h2>
<h3>Osman Asif Malik, Shashanka Ubaru, Lior Horesh, Misha E. Kilmer, Haim Avron</h3>
<p>Many irregular domains such as social networks, financial transactions,
neuron connections, and natural language constructs are represented using graph
structures. In recent years, a variety of graph neural networks (GNNs) have
been successfully applied for representation learning and prediction on such
graphs. In many of the real-world applications, the underlying graph changes
over time, however, most of the existing GNNs are inadequate for handling such
dynamic graphs. In this paper we propose a novel technique for learning
embeddings of dynamic graphs using a tensor algebra framework. Our method
extends the popular graph convolutional network (GCN) for learning
representations of dynamic graphs using the recently proposed tensor M-product
technique. Theoretical results presented establish a connection between the
proposed tensor approach and spectral convolution of tensors. The proposed
method TM-GCN is consistent with the Message Passing Neural Network (MPNN)
framework, accounting for both spatial and temporal message passing. Numerical
experiments on real-world datasets demonstrate the performance of the proposed
method for edge classification and link prediction tasks on dynamic graphs. We
also consider an application related to the COVID-19 pandemic, and show how our
method can be used for early detection of infected individuals from contact
tracing data.
</p>
<a href="http://arxiv.org/abs/1910.07643" target="_blank">arXiv:1910.07643</a> [<a href="http://arxiv.org/pdf/1910.07643" target="_blank">pdf</a>]

<h2>Learning GENERAL Principles from Hundreds of Software Projects. (arXiv:1911.04250v2 [cs.SE] UPDATED)</h2>
<h3>Suvodeep Majumder, Rahul Krishna, Tim Menzies</h3>
<p>Managers and practitioners become dubious about software analytics when its
conclusions keep changing as we look at new projects. GENERAL is a new approach
for quickly finding conclusions that generalize across hundreds of projects.
This algorithm (a) removes spurious attributes via feature selection; (b) fixes
training data imbalance via synthetic instances; (c) recursively clusters the
project data; (d) finds the best model within any cluster, then promotes it up
the cluster tree; (e) returns the model promoted to the top. GENERAL is much
faster than prior methods (4.8 hours versus 204 hours our case studies) and
theoretically scales better (O(N^2/m) versus O(N^2), which is a large reduction
since often we find m&gt;20 clusters).

When tested on 756 Github projects, a single defect prediction model
generalized over all those projects while also being useful and insightful and
generalizable; i.e. that model worked just as well as 756 separate models
learned from each project; and that model succinctly show what key factors most
contributed to defects. Hence, when exploring hundreds of projects, we endorse
GENERAL reasoning.
</p>
<a href="http://arxiv.org/abs/1911.04250" target="_blank">arXiv:1911.04250</a> [<a href="http://arxiv.org/pdf/1911.04250" target="_blank">pdf</a>]

<h2>Continual adaptation for efficient machine communication. (arXiv:1911.09896v2 [cs.CL] UPDATED)</h2>
<h3>Robert D. Hawkins, Minae Kwon, Dorsa Sadigh, Noah D. Goodman</h3>
<p>To communicate with new partners in new contexts, humans rapidly form new
linguistic conventions. Recent neural language models are able to comprehend
and produce the existing conventions present in their training data, but are
not able to flexibly and interactively adapt those conventions on the fly as
humans do. We introduce an interactive repeated reference task as a benchmark
for models of adaptation in communication and propose a regularized continual
learning framework that allows an artificial agent initialized with a generic
language model to more accurately and efficiently communicate with a partner
over time. We evaluate this framework through simulations on COCO and in
real-time reference game experiments with human partners.
</p>
<a href="http://arxiv.org/abs/1911.09896" target="_blank">arXiv:1911.09896</a> [<a href="http://arxiv.org/pdf/1911.09896" target="_blank">pdf</a>]

<h2>Learning Adversarial MDPs with Bandit Feedback and Unknown Transition. (arXiv:1912.01192v4 [cs.LG] UPDATED)</h2>
<h3>Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, Tiancheng Yu</h3>
<p>We consider the problem of learning in episodic finite-horizon Markov
decision processes with an unknown transition function, bandit feedback, and
adversarial losses. We propose an efficient algorithm that achieves
$\mathcal{\tilde{O}}(L|X|\sqrt{|A|T})$ regret with high probability, where $L$
is the horizon, $|X|$ is the number of states, $|A|$ is the number of actions,
and $T$ is the number of episodes. To the best of our knowledge, our algorithm
is the first to ensure $\mathcal{\tilde{O}}(\sqrt{T})$ regret in this
challenging setting; in fact it achieves the same regret bound as (Rosenberg &amp;
Mansour, 2019a) that considers an easier setting with full-information
feedback. Our key technical contributions are two-fold: a tighter confidence
set for the transition function, and an optimistic loss estimator that is
inversely weighted by an $\textit{upper occupancy bound}$.
</p>
<a href="http://arxiv.org/abs/1912.01192" target="_blank">arXiv:1912.01192</a> [<a href="http://arxiv.org/pdf/1912.01192" target="_blank">pdf</a>]

<h2>MALA: Cross-Domain Dialogue Generation with Action Learning. (arXiv:1912.08442v2 [cs.CL] UPDATED)</h2>
<h3>Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang</h3>
<p>Response generation for task-oriented dialogues involves two basic
components: dialogue planning and surface realization. These two components,
however, have a discrepancy in their objectives, i.e., task completion and
language quality. To deal with such discrepancy, conditioned response
generation has been introduced where the generation process is factorized into
action decision and language generation via explicit action representations. To
obtain action representations, recent studies learn latent actions in an
unsupervised manner based on the utterance lexical similarity. Such an action
learning approach is prone to diversities of language surfaces, which may
impinge task completion and language quality. To address this issue, we propose
multi-stage adaptive latent action learning (MALA) that learns semantic latent
actions by distinguishing the effects of utterances on dialogue progress. We
model the utterance effect using the transition of dialogue states caused by
the utterance and develop a semantic similarity measurement that estimates
whether utterances have similar effects. For learning semantic actions on
domains without dialogue states, MsALA extends the semantic similarity
measurement across domains progressively, i.e., from aligning shared actions to
learning domain-specific actions. Experiments using multi-domain datasets, SMD
and MultiWOZ, show that our proposed model achieves consistent improvements
over the baselines models in terms of both task completion and language
quality.
</p>
<a href="http://arxiv.org/abs/1912.08442" target="_blank">arXiv:1912.08442</a> [<a href="http://arxiv.org/pdf/1912.08442" target="_blank">pdf</a>]

<h2>Alpha Discovery Neural Network based on Prior Knowledge. (arXiv:1912.11761v7 [q-fin.ST] UPDATED)</h2>
<h3>Jie Fang, Shutao Xia, Jianwu Lin, Zhikang Xia, Xiang Liu, Yong Jiang</h3>
<p>Genetic programming (GP) is the state-of-the-art in financial automated
feature construction task. It employs reverse polish expression to represent
features and then conducts the evolution process. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Alpha Discovery Neural Network (ADNN), a tailored neural network
structure which can automatically construct diversified financial technical
indicators based on prior knowledge. We mainly made three contributions. First,
we use domain knowledge in quantitative trading to design the sampling rules
and object function. Second, pre-training and model pruning has been used to
replace genetic programming, because it can conduct more efficient evolution
process. Third, the feature extractors in ADNN can be replaced by different
feature extractors and produce different functions. The experiment results show
that ADNN can construct more informative and diversified features than GP,
which can effectively enriches the current factor pool. The fully-connected
network and recurrent network are better at extracting information from the
financial time series than the convolution neural network. In real practice,
features constructed by ADNN can always improve multi-factor strategies'
revenue, sharpe ratio, and max draw-down, compared with the investment
strategies without these factors.
</p>
<a href="http://arxiv.org/abs/1912.11761" target="_blank">arXiv:1912.11761</a> [<a href="http://arxiv.org/pdf/1912.11761" target="_blank">pdf</a>]

<h2>Localized Debiased Machine Learning: Efficient Inference on Quantile Treatment Effects and Beyond. (arXiv:1912.12945v4 [stat.ML] UPDATED)</h2>
<h3>Nathan Kallus, Xiaojie Mao, Masatoshi Uehara</h3>
<p>We consider the efficient estimation of a low-dimensional parameter in an
estimating equation involving high-dimensional nuisances that depend on the
parameter of interest. An important example is the (local) quantile treatment
effect ((L)QTE) in causal inference, for which the efficient estimating
equation involves as a nuisance the covariate-conditional cumulative
distribution function evaluated at the quantile to be estimated. Debiased
machine learning (DML) is a data-splitting approach to address the need to
estimate nuisances using flexible machine learning methods that may not satisfy
strong metric entropy conditions, but applying it to problems with
parameter-dependent nuisances is impractical. For (L)QTE estimation, DML
requires we learn the whole conditional cumulative distribution function,
conditioned on potentially high-dimensional covariates, which is far more
challenging than the standard supervised regression task in machine learning.
We instead propose localized debiased machine learning (LDML), a new
data-splitting approach that avoids this burdensome step and needs only
estimate the nuisances at a single initial rough guess for the parameter. For
(L)QTE estimation, this involves just learning two binary regression (i.e.,
classification) models, for which many standard, time-tested machine learning
methods exist, and the initial rough guess may be given by inverse propensity
weighting. We prove that under lax rate conditions on nuisances, our estimator
has the same favorable asymptotic behavior as the infeasible oracle estimator
that solves the estimating equation with the unknown true nuisance functions.
Thus, our proposed approach uniquely enables practically-feasible and
theoretically-grounded efficient estimation of important quantities in causal
inference such as (L)QTEs and in other coarsened data settings.
</p>
<a href="http://arxiv.org/abs/1912.12945" target="_blank">arXiv:1912.12945</a> [<a href="http://arxiv.org/pdf/1912.12945" target="_blank">pdf</a>]

<h2>Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale Chest Computed Tomography Volumes. (arXiv:2002.04752v3 [eess.IV] UPDATED)</h2>
<h3>Rachel Lea Draelos, David Dov, Maciej A. Mazurowski, Joseph Y. Lo, Ricardo Henao, Geoffrey D. Rubin, Lawrence Carin</h3>
<p>Machine learning models for radiology benefit from large-scale data sets with
high quality labels for abnormalities. We curated and analyzed a chest computed
tomography (CT) data set of 36,316 volumes from 19,993 unique patients. This is
the largest multiply-annotated volumetric medical imaging data set reported. To
annotate this data set, we developed a rule-based method for automatically
extracting abnormality labels from free-text radiology reports with an average
F-score of 0.976 (min 0.941, max 1.0). We also developed a model for
multi-organ, multi-disease classification of chest CT volumes that uses a deep
convolutional neural network (CNN). This model reached a classification
performance of AUROC greater than 0.90 for 18 abnormalities, with an average
AUROC of 0.773 for all 83 abnormalities, demonstrating the feasibility of
learning from unfiltered whole volume CT data. We show that training on more
labels improves performance significantly: for a subset of 9 labels - nodule,
opacity, atelectasis, pleural effusion, consolidation, mass, pericardial
effusion, cardiomegaly, and pneumothorax - the model's average AUROC increased
by 10% when the number of training labels was increased from 9 to all 83. All
code for volume preprocessing, automated label extraction, and the volume
abnormality prediction model will be made publicly available. The 36,316 CT
volumes and labels will also be made publicly available pending institutional
approval.
</p>
<a href="http://arxiv.org/abs/2002.04752" target="_blank">arXiv:2002.04752</a> [<a href="http://arxiv.org/pdf/2002.04752" target="_blank">pdf</a>]

<h2>Transfer among Agents: An Efficient Multiagent Transfer Learning Framework. (arXiv:2002.08030v2 [cs.MA] UPDATED)</h2>
<h3>Tianpei Yang, Weixun Wang, Hongyao Tang, Jianye Hao, Zhaopeng Meng, Hangyu Mao, Dong Li, Wulong Liu, Yujing Hu, Yingfeng Chen, Changjie Fan</h3>
<p>Transfer Learning has shown great potential to enhance the single-agent
Reinforcement Learning (RL) efficiency, by sharing learned policies of previous
tasks. Similarly, in multiagent settings, the learning performance can also be
promoted if agents can share knowledge between each other. However, it remains
an open question of how an agent should learn from other agents' knowledge. In
this paper, we propose a novel multiagent option-based policy transfer (MAOPT)
framework to improve multiagent learning efficiency. Our framework learns what
advice to give to each agent and when to terminate it by modeling multiagent
policy transfer as the option learning problem. MAOPT provides different kinds
of variants which can be classified into two types in terms of the experience
used during training. One type is the MAOPT with the Global Option Advisor
which has the access to the global information of the environment. However, in
many realistic scenarios, we can only obtain each agent's local information due
to the partial observation. The other type contains MAOPT with the Local Option
Advisor and MAOPT with the Successor Representation Option (SRO) which are
suitable for this setting and collect each agent's local experience for the
update. In many cases, each agent's experience is inconsistent with each other
which causes the option-value estimation to oscillate and to become inaccurate.
SRO is used to handle the experience inconsistency by decoupling the dynamics
of the environment from the rewards to learn the option-value function under
each agent's preference. MAOPT can be easily combined with existing deep RL
approaches. Experimental results show it significantly boosts the performance
of existing deep RL methods in both discrete and continuous state spaces.
</p>
<a href="http://arxiv.org/abs/2002.08030" target="_blank">arXiv:2002.08030</a> [<a href="http://arxiv.org/pdf/2002.08030" target="_blank">pdf</a>]

<h2>Provable Meta-Learning of Linear Representations. (arXiv:2002.11684v3 [cs.LG] UPDATED)</h2>
<h3>Nilesh Tripuraneni, Chi Jin, Michael I. Jordan</h3>
<p>Meta-learning, or learning-to-learn, seeks to design algorithms that can
utilize previous experience to rapidly learn new skills or adapt to new
environments. Representation learning---a key tool for performing
meta-learning---learns a data representation that can transfer knowledge across
multiple tasks, which is essential in regimes where data is scarce. Despite a
recent surge of interest in the practice of meta-learning, the theoretical
underpinnings of meta-learning algorithms are lacking, especially in the
context of learning transferable representations. In this paper, we focus on
the problem of multi-task linear regression---in which multiple linear
regression models share a common, low-dimensional linear representation. Here,
we provide provably fast, sample-efficient algorithms to address the dual
challenges of (1) learning a common set of features from multiple, related
tasks, and (2) transferring this knowledge to new, unseen tasks. Both are
central to the general problem of meta-learning. Finally, we complement these
results by providing information-theoretic lower bounds on the sample
complexity of learning these linear features.
</p>
<a href="http://arxiv.org/abs/2002.11684" target="_blank">arXiv:2002.11684</a> [<a href="http://arxiv.org/pdf/2002.11684" target="_blank">pdf</a>]

<h2>Optimization of Genomic Classifiers for Clinical Deployment: Evaluation of Bayesian Optimization to Select Predictive Models of Acute Infection and In-Hospital Mortality. (arXiv:2003.12310v3 [cs.LG] UPDATED)</h2>
<h3>Michael B. Mayhew, Elizabeth Tran, Kirindi Choi, Uros Midic, Roland Luethy, Nandita Damaraju, Ljubomir Buturovic</h3>
<p>Acute infection, if not rapidly and accurately detected, can lead to sepsis,
organ failure and even death. Current detection of acute infection as well as
assessment of a patient's severity of illness are imperfect. Characterization
of a patient's immune response by quantifying expression levels of specific
genes from blood represents a potentially more timely and precise means of
accomplishing both tasks. Machine learning methods provide a platform to
leverage this 'host response' for development of deployment-ready
classification models. Prioritization of promising classifiers is dependent, in
part, on hyperparameter optimization for which a number of approaches including
grid search, random sampling and Bayesian optimization have been shown to be
effective. We compare HO approaches for the development of diagnostic
classifiers of acute infection and in-hospital mortality from gene expression
of 29 diagnostic markers. We take a deployment-centered approach to our
comprehensive analysis, accounting for heterogeneity in our multi-study patient
cohort with our choices of dataset partitioning and hyperparameter optimization
objective as well as assessing selected classifiers in external (as well as
internal) validation. We find that classifiers selected by Bayesian
optimization for in-hospital mortality can outperform those selected by grid
search or random sampling. However, in contrast to previous research: 1)
Bayesian optimization is not more efficient in selecting classifiers in all
instances compared to grid search or random sampling-based methods and 2) we
note marginal gains in classifier performance in only specific circumstances
when using a common variant of Bayesian optimization (i.e. automatic relevance
determination). Our analysis highlights the need for further practical,
deployment-centered benchmarking of HO approaches in the healthcare context.
</p>
<a href="http://arxiv.org/abs/2003.12310" target="_blank">arXiv:2003.12310</a> [<a href="http://arxiv.org/pdf/2003.12310" target="_blank">pdf</a>]

<h2>3D Deep Learning on Medical Images: A Review. (arXiv:2004.00218v4 [q-bio.QM] UPDATED)</h2>
<h3>Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan, Bal&#xe1;zs Guly&#xe1;s</h3>
<p>The rapid advancements in machine learning, graphics processing technologies
and the availability of medical imaging data have led to a rapid increase in
the use of deep learning models in the medical domain. This was exacerbated by
the rapid advancements in convolutional neural network (CNN) based
architectures, which were adopted by the medical imaging community to assist
clinicians in disease diagnosis. Since the grand success of AlexNet in 2012,
CNNs have been increasingly used in medical image analysis to improve the
efficiency of human clinicians. In recent years, three-dimensional (3D) CNNs
have been employed for the analysis of medical images. In this paper, we trace
the history of how the 3D CNN was developed from its machine learning roots, we
provide a brief mathematical description of 3D CNN and provide the
preprocessing steps required for medical images before feeding them to 3D CNNs.
We review the significant research in the field of 3D medical imaging analysis
using 3D CNNs (and its variants) in different medical areas such as
classification, segmentation, detection and localization. We conclude by
discussing the challenges associated with the use of 3D CNNs in the medical
imaging domain (and the use of deep learning models in general) and possible
future trends in the field.
</p>
<a href="http://arxiv.org/abs/2004.00218" target="_blank">arXiv:2004.00218</a> [<a href="http://arxiv.org/pdf/2004.00218" target="_blank">pdf</a>]

<h2>The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection. (arXiv:2004.02421v4 [cs.CL] UPDATED)</h2>
<h3>Zibo Lin, Deng Cai, Yan Wang, Xiaojiang Liu, Hai-Tao Zheng, Shuming Shi</h3>
<p>Response selection plays a vital role in building retrieval-based
conversation systems. Despite that response selection is naturally a
learning-to-rank problem, most prior works take a point-wise view and train
binary classifiers for this task: each response candidate is labeled either
relevant (one) or irrelevant (zero). On the one hand, this formalization can be
sub-optimal due to its ignorance of the diversity of response quality. On the
other hand, annotating grayscale data for learning-to-rank can be prohibitively
expensive and challenging. In this work, we show that grayscale data can be
automatically constructed without human effort. Our method employs
off-the-shelf response retrieval models and response generation models as
automatic grayscale data generators. With the constructed grayscale data, we
propose multi-level ranking objectives for training, which can (1) teach a
matching model to capture more fine-grained context-response relevance
difference and (2) reduce the train-test discrepancy in terms of distractor
strength. Our method is simple, effective, and universal. Experiments on three
benchmark datasets and four state-of-the-art matching models show that the
proposed approach brings significant and consistent performance improvements.
</p>
<a href="http://arxiv.org/abs/2004.02421" target="_blank">arXiv:2004.02421</a> [<a href="http://arxiv.org/pdf/2004.02421" target="_blank">pdf</a>]

<h2>What do Models Learn from Question Answering Datasets?. (arXiv:2004.03490v2 [cs.CL] UPDATED)</h2>
<h3>Priyanka Sen, Amir Saffari</h3>
<p>While models have reached superhuman performance on popular question
answering (QA) datasets such as SQuAD, they have yet to outperform humans on
the task of question answering itself. In this paper, we investigate if models
are learning reading comprehension from QA datasets by evaluating BERT-based
models across five datasets. We evaluate models on their generalizability to
out-of-domain examples, responses to missing or incorrect data, and ability to
handle question variations. We find that no single dataset is robust to all of
our experiments and identify shortcomings in both datasets and evaluation
methods. Following our analysis, we make recommendations for building future QA
datasets that better evaluate the task of question answering through reading
comprehension. We also release code to convert QA datasets to a shared format
for easier experimentation at
https://github.com/amazon-research/qa-dataset-converter.
</p>
<a href="http://arxiv.org/abs/2004.03490" target="_blank">arXiv:2004.03490</a> [<a href="http://arxiv.org/pdf/2004.03490" target="_blank">pdf</a>]

<h2>Adversarial Weight Perturbation Helps Robust Generalization. (arXiv:2004.05884v2 [cs.LG] UPDATED)</h2>
<h3>Dongxian Wu, Shu-tao Xia, Yisen Wang</h3>
<p>The study on improving the robustness of deep neural networks against
adversarial examples grows rapidly in recent years. Among them, adversarial
training is the most promising one, which flattens the input loss landscape
(loss change with respect to input) via training on adversarially perturbed
examples. However, how the widely used weight loss landscape (loss change with
respect to weight) performs in adversarial training is rarely explored. In this
paper, we investigate the weight loss landscape from a new perspective, and
identify a clear correlation between the flatness of weight loss landscape and
robust generalization gap. Several well-recognized adversarial training
improvements, such as early stopping, designing new objective functions, or
leveraging unlabeled data, all implicitly flatten the weight loss landscape.
Based on these observations, we propose a simple yet effective Adversarial
Weight Perturbation (AWP) to explicitly regularize the flatness of weight loss
landscape, forming a double-perturbation mechanism in the adversarial training
framework that adversarially perturbs both inputs and weights. Extensive
experiments demonstrate that AWP indeed brings flatter weight loss landscape
and can be easily incorporated into various existing adversarial training
methods to further boost their adversarial robustness.
</p>
<a href="http://arxiv.org/abs/2004.05884" target="_blank">arXiv:2004.05884</a> [<a href="http://arxiv.org/pdf/2004.05884" target="_blank">pdf</a>]

<h2>A Natural Language Processing Pipeline of Chinese Free-text Radiology Reports for Liver Cancer Diagnosis. (arXiv:2004.13848v2 [cs.CL] UPDATED)</h2>
<h3>Honglei Liu, Yan Xu, Zhiqiang Zhang, Ni Wang, Yanqun Huang, Yanjun Hu, Zhenghan Yang, Rui Jiang, Hui Chen</h3>
<p>Despite the rapid development of natural language processing (NLP)
implementation in electronic medical records (EMRs), Chinese EMRs processing
remains challenging due to the limited corpus and specific grammatical
characteristics, especially for radiology reports. In this study, we designed
an NLP pipeline for the direct extraction of clinically relevant features from
Chinese radiology reports, which is the first key step in computer-aided
radiologic diagnosis. The pipeline was comprised of named entity recognition,
synonyms normalization, and relationship extraction to finally derive the
radiological features composed of one or more terms. In named entity
recognition, we incorporated lexicon into deep learning model bidirectional
long short-term memory-conditional random field (BiLSTM-CRF), and the model
finally achieved an F1 score of 93.00%. With the extracted radiological
features, least absolute shrinkage and selection operator and machine learning
methods (support vector machine, random forest, decision tree, and logistic
regression) were used to build the classifiers for liver cancer prediction. For
liver cancer diagnosis, random forest had the highest predictive performance in
liver cancer diagnosis (F1 score 86.97%, precision 87.71%, and recall 86.25%).
This work was a comprehensive NLP study focusing on Chinese radiology reports
and the application of NLP in cancer risk prediction. The proposed NLP pipeline
for the radiological feature extraction could be easily implemented in other
kinds of Chinese clinical texts and other disease predictive tasks.
</p>
<a href="http://arxiv.org/abs/2004.13848" target="_blank">arXiv:2004.13848</a> [<a href="http://arxiv.org/pdf/2004.13848" target="_blank">pdf</a>]

<h2>Interpretable Entity Representations through Large-Scale Typing. (arXiv:2005.00147v2 [cs.CL] UPDATED)</h2>
<h3>Yasumasa Onoe, Greg Durrett</h3>
<p>In standard methodology for natural language processing, entities in text are
typically embedded in dense vector spaces with pre-trained models. The
embeddings produced this way are effective when fed into downstream models, but
they require end-task fine-tuning and are fundamentally difficult to interpret.
In this paper, we present an approach to creating entity representations that
are human readable and achieve high performance on entity-related tasks out of
the box. Our representations are vectors whose values correspond to posterior
probabilities over fine-grained entity types, indicating the confidence of a
typing model's decision that the entity belongs to the corresponding type. We
obtain these representations using a fine-grained entity typing model, trained
either on supervised ultra-fine entity typing data (Choi et al. 2018) or
distantly-supervised examples from Wikipedia. On entity probing tasks involving
recognizing entity identity, our embeddings used in parameter-free downstream
models achieve competitive performance with ELMo- and BERT-based embeddings in
trained models. We also show that it is possible to reduce the size of our type
set in a learning-based way for particular domains. Finally, we show that these
embeddings can be post-hoc modified through a small number of rules to
incorporate domain knowledge and improve performance.
</p>
<a href="http://arxiv.org/abs/2005.00147" target="_blank">arXiv:2005.00147</a> [<a href="http://arxiv.org/pdf/2005.00147" target="_blank">pdf</a>]

<h2>Probabilistic Hyperproperties of Markov Decision Processes. (arXiv:2005.03362v3 [cs.LO] UPDATED)</h2>
<h3>Rayna Dimitrova, Bernd Finkbeiner, Hazem Torfah</h3>
<p>Hyperproperties are properties that describe the correctness of a system as a
relation between multiple executions. Hyperproperties generalize trace
properties and include information-flow security requirements, like
noninterference, as well as requirements like symmetry, partial observation,
robustness, and fault tolerance. We initiate the study of the specification and
verification of hyperproperties of Markov decision processes (MDPs). We
introduce the temporal logic PHL (Probabilistic Hyper Logic), which extends
classic probabilistic logics with quantification over schedulers and traces.
PHL can express a wide range of hyperproperties for probabilistic systems,
including both classical applications, such as probabilistic noninterference,
and novel applications in areas such as robotics and planning. While the model
checking problem for PHL is in general undecidable, we provide methods both for
proving and for refuting formulas from a fragment of the logic. The fragment
includes many probabilistic hyperproperties of interest.
</p>
<a href="http://arxiv.org/abs/2005.03362" target="_blank">arXiv:2005.03362</a> [<a href="http://arxiv.org/pdf/2005.03362" target="_blank">pdf</a>]

<h2>Multiple Access in Aerial Networks: From Orthogonal and Non-Orthogonal to Rate-Splitting. (arXiv:2005.13122v3 [eess.SP] UPDATED)</h2>
<h3>Wael Jaafar, Shimaa Naser, Sami Muhaidat, Paschalis C. Sofotasios, Halim Yanikomeroglu</h3>
<p>Recently, interest on the utilization of unmanned aerial vehicles (UAVs) has
aroused. Specifically, UAVs can be used in cellular networks as aerial users
for delivery, surveillance, rescue search, or as an aerial base station (aBS)
for communication with ground users in remote uncovered areas or in dense
environments requiring prompt high capacity. Aiming to satisfy the high
requirements of wireless aerial networks, several multiple access techniques
have been investigated. In particular, space-division multiple access(SDMA) and
power-domain non-orthogonal multiple access (NOMA) present promising
multiplexing gains for aerial downlink and uplink. Nevertheless, these gains
are limited as they depend on the conditions of the environment. Hence, a
generalized scheme has been recently proposed, called rate-splitting multiple
access (RSMA), which is capable of achieving better spectral efficiency gains
compared to SDMA and NOMA. In this paper, we present a comprehensive survey of
key multiple access technologies adopted for aerial networks, where aBSs are
deployed to serve ground users. Since there have been only sporadic results
reported on the use of RSMA in aerial systems, we aim to extend the discussion
on this topic by modelling and analyzing the weighted sum-rate performance of a
two-user downlink network served by an RSMA-based aBS. Finally, related open
issues and future research directions are exposed.
</p>
<a href="http://arxiv.org/abs/2005.13122" target="_blank">arXiv:2005.13122</a> [<a href="http://arxiv.org/pdf/2005.13122" target="_blank">pdf</a>]

<h2>Blockchain is Watching You: Profiling and Deanonymizing Ethereum Users. (arXiv:2005.14051v2 [cs.CR] UPDATED)</h2>
<h3>Ferenc B&#xe9;res, Istv&#xe1;n Andr&#xe1;s Seres, Andr&#xe1;s A. Bencz&#xfa;r, Mikerah Quintyne-Collins</h3>
<p>Ethereum is the largest public blockchain by usage. It applies an
account-based model, which is inferior to Bitcoin's unspent transaction output
model from a privacy perspective. Due to its privacy shortcomings, recently
several privacy-enhancing overlays have been deployed on Ethereum, such as
non-custodial, trustless coin mixers and confidential transactions. In our
privacy analysis of Ethereum's account-based model, we describe several
patterns that characterize only a limited set of users and successfully apply
these quasi-identifiers in address deanonymization tasks. Using Ethereum Name
Service identifiers as ground truth information, we quantitatively compare
algorithms in recent branch of machine learning, the so-called graph
representation learning, as well as time-of-day activity and transaction fee
based user profiling techniques. As an application, we rigorously assess the
privacy guarantees of the Tornado Cash coin mixer by discovering strong
heuristics to link the mixing parties. To the best of our knowledge, we are the
first to propose and implement Ethereum user profiling techniques based on
quasi-identifiers. Finally, we describe a malicious value-fingerprinting
attack, a variant of the Danaan-gift attack, applicable for the confidential
transaction overlays on Ethereum. By incorporating user activity statistics
from our data set, we estimate the success probability of such an attack.
</p>
<a href="http://arxiv.org/abs/2005.14051" target="_blank">arXiv:2005.14051</a> [<a href="http://arxiv.org/pdf/2005.14051" target="_blank">pdf</a>]

<h2>Theory and Algorithms for Shapelet-based Multiple-Instance Learning. (arXiv:2006.01130v3 [cs.LG] UPDATED)</h2>
<h3>Daiki Suehiro, Kohei Hatano, Eiji Takimoto, Shuji Yamamoto, Kenichi Bannai, Akiko Takeda</h3>
<p>We propose a new formulation of Multiple-Instance Learning (MIL), in which a
unit of data consists of a set of instances called a bag. The goal is to find a
good classifier of bags based on the similarity with a "shapelet" (or pattern),
where the similarity of a bag with a shapelet is the maximum similarity of
instances in the bag. In previous work, some of the training instances are
chosen as shapelets with no theoretical justification. In our formulation, we
use all possible, and thus infinitely many shapelets, resulting in a richer
class of classifiers. We show that the formulation is tractable, that is, it
can be reduced through Linear Programming Boosting (LPBoost) to Difference of
Convex (DC) programs of finite (actually polynomial) size. Our theoretical
result also gives justification to the heuristics of some of the previous work.
The time complexity of the proposed algorithm highly depends on the size of the
set of all instances in the training sample. To apply to the data containing a
large number of instances, we also propose a heuristic option of the algorithm
without the loss of the theoretical guarantee. Our empirical study demonstrates
that our algorithm uniformly works for Shapelet Learning tasks on time-series
classification and various MIL tasks with comparable accuracy to the existing
methods. Moreover, we show that the proposed heuristics allow us to achieve the
result with reasonable computational time.
</p>
<a href="http://arxiv.org/abs/2006.01130" target="_blank">arXiv:2006.01130</a> [<a href="http://arxiv.org/pdf/2006.01130" target="_blank">pdf</a>]

<h2>Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID. (arXiv:2006.02713v2 [cs.CV] UPDATED)</h2>
<h3>Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, Hongsheng Li</h3>
<p>Domain adaptive object re-ID aims to transfer the learned knowledge from the
labeled source domain to the unlabeled target domain to tackle the open-class
re-identification problems. Although state-of-the-art pseudo-label-based
methods have achieved great success, they did not make full use of all valuable
information because of the domain gap and unsatisfying clustering performance.
To solve these problems, we propose a novel self-paced contrastive learning
framework with hybrid memory. The hybrid memory dynamically generates
source-domain class-level, target-domain cluster-level and un-clustered
instance-level supervisory signals for learning feature representations.
Different from the conventional contrastive learning strategy, the proposed
framework jointly distinguishes source-domain classes, and target-domain
clusters and un-clustered instances. Most importantly, the proposed self-paced
method gradually creates more reliable clusters to refine the hybrid memory and
learning targets, and is shown to be the key to our outstanding performance.
Our method outperforms state-of-the-arts on multiple domain adaptation tasks of
object re-ID and even boosts the performance on the source domain without any
extra annotations. Our generalized version on unsupervised object re-ID
surpasses state-of-the-art algorithms by considerable 16.7% and 7.9% on
Market-1501 and MSMT17 benchmarks.
</p>
<a href="http://arxiv.org/abs/2006.02713" target="_blank">arXiv:2006.02713</a> [<a href="http://arxiv.org/pdf/2006.02713" target="_blank">pdf</a>]

<h2>Triple descent and the two kinds of overfitting: Where & why do they appear?. (arXiv:2006.03509v2 [cs.LG] UPDATED)</h2>
<h3>St&#xe9;phane d&#x27;Ascoli, Levent Sagun, Giulio Biroli</h3>
<p>A recent line of research has highlighted the existence of a "double descent"
phenomenon in deep learning, whereby increasing the number of training examples
$N$ causes the generalization error of neural networks to peak when $N$ is of
the same order as the number of parameters $P$. In earlier works, a similar
phenomenon was shown to exist in simpler models such as linear regression,
where the peak instead occurs when $N$ is equal to the input dimension $D$.
Since both peaks coincide with the interpolation threshold, they are often
conflated in the litterature. In this paper, we show that despite their
apparent similarity, these two scenarios are inherently different. In fact,
both peaks can co-exist when neural networks are applied to noisy regression
tasks. The relative size of the peaks is then governed by the degree of
nonlinearity of the activation function. Building on recent developments in the
analysis of random feature models, we provide a theoretical ground for this
sample-wise triple descent. As shown previously, the nonlinear peak at
$N\!=\!P$ is a true divergence caused by the extreme sensitivity of the output
function to both the noise corrupting the labels and the initialization of the
random features (or the weights in neural networks). This peak survives in the
absence of noise, but can be suppressed by regularization. In contrast, the
linear peak at $N\!=\!D$ is solely due to overfitting the noise in the labels,
and forms earlier during training. We show that this peak is implicitly
regularized by the nonlinearity, which is why it only becomes salient at high
noise and is weakly affected by explicit regularization. Throughout the paper,
we compare analytical results obtained in the random feature model with the
outcomes of numerical experiments involving deep neural networks.
</p>
<a href="http://arxiv.org/abs/2006.03509" target="_blank">arXiv:2006.03509</a> [<a href="http://arxiv.org/pdf/2006.03509" target="_blank">pdf</a>]

<h2>Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample Complexity. (arXiv:2006.03864v2 [cs.LG] UPDATED)</h2>
<h3>Zihan Zhang, Yuan Zhou, Xiangyang Ji</h3>
<p>In this paper we consider the problem of learning an $\epsilon$-optimal
policy for a discounted Markov Decision Process (MDP). Given an MDP with $S$
states, $A$ actions, the discount factor $\gamma \in (0,1)$, and an
approximation threshold $\epsilon &gt; 0$, we provide a model-free algorithm to
learn an $\epsilon$-optimal policy with sample complexity
$\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{5.5}})$ (where the notation
$\tilde{O}(\cdot)$ hides poly-logarithmic factors of $S,A,1/(1-\gamma)$, and
$1/\epsilon$) and success probability $(1-p)$. For small enough $\epsilon$, we
show an improved algorithm with sample complexity
$\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{3}})$. While the first bound
improves upon all known model-free algorithms and model-based ones with tight
dependence on $S$, our second algorithm beats all known sample complexity
bounds and matches the information theoretic lower bound up to logarithmic
factors.
</p>
<a href="http://arxiv.org/abs/2006.03864" target="_blank">arXiv:2006.03864</a> [<a href="http://arxiv.org/pdf/2006.03864" target="_blank">pdf</a>]

<h2>Learning the Truth From Only One Side of the Story. (arXiv:2006.04858v2 [cs.LG] UPDATED)</h2>
<h3>Heinrich Jiang, Qijia Jiang, Aldo Pacchiano</h3>
<p>Learning under one-sided feedback (i.e., where we only observe the labels for
examples we predicted positively on) is a fundamental problem in machine
learning -- applications include lending and recommendation systems. Despite
this, there has been surprisingly little progress made in ways to mitigate the
effects of the sampling bias that arises. We focus on generalized linear models
and show that without adjusting for this sampling bias, the model may converge
suboptimally or even fail to converge to the optimal solution. We propose an
adaptive approach that comes with theoretical guarantees and show that it
outperforms several existing methods empirically. Our method leverages variance
estimation techniques to efficiently learn under uncertainty, offering a more
principled alternative compared to existing approaches.
</p>
<a href="http://arxiv.org/abs/2006.04858" target="_blank">arXiv:2006.04858</a> [<a href="http://arxiv.org/pdf/2006.04858" target="_blank">pdf</a>]

<h2>Few-shot Neural Architecture Search. (arXiv:2006.06863v5 [cs.LG] UPDATED)</h2>
<h3>Yiyang Zhao, Linnan Wang, Yuandong Tian, Rodrigo Fonseca, Tian Guo</h3>
<p>Efficient evaluation of a network architecture drawn from a large search
space remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS
evaluates each architecture by training from scratch, which gives the true
performance but is extremely time-consuming. Recently, one-shot NAS
substantially reduces the computation cost by training only one supernetwork,
a.k.a. supernet, to approximate the performance of every architecture in the
search space via weight-sharing. However, the performance estimation can be
very inaccurate due to the co-adaption among operations. In this paper, we
propose few-shot NAS that uses multiple supernetworks, called sub-supernet,
each covering different regions of the search space to alleviate the undesired
co-adaption. Since each subsupernet only covers a small search space, compared
to one-shot NAS, few-shot NAS improves the accuracy of architecture evaluation
with a small increase of evaluation cost. With only up to 7 sub-supernets,
few-shot NAS establishes new SoTAs: on ImageNet, it finds models that reach
80.5 top-1 at 600 MB FLOPS and 77.3 top-1 at 230 MFLOPS; on CIFAR10, it reaches
98.72 top-1 without using extra data or transfer learning. In Auto-GAN,
few-shot NAS outperforms the previously published results by up to 20\%.
Extensive experiments show that few-shot NAS significantly improves various
one-shot methods, including 4 gradient-based and 6 search-based methods on 3
different tasks in NASBench-201 and NASBench one-shot-one.
</p>
<a href="http://arxiv.org/abs/2006.06863" target="_blank">arXiv:2006.06863</a> [<a href="http://arxiv.org/pdf/2006.06863" target="_blank">pdf</a>]

<h2>GradAug: A New Regularization Method for Deep Neural Networks. (arXiv:2006.07989v2 [cs.CV] UPDATED)</h2>
<h3>Taojiannan Yang, Sijie Zhu, Chen Chen</h3>
<p>We propose a new regularization method to alleviate over-fitting in deep
neural networks. The key idea is utilizing randomly transformed training
samples to regularize a set of sub-networks, which are originated by sampling
the width of the original network, in the training process. As such, the
proposed method introduces self-guided disturbances to the raw gradients of the
network and therefore is termed as Gradient Augmentation (GradAug). We
demonstrate that GradAug can help the network learn well-generalized and more
diverse representations. Moreover, it is easy to implement and can be applied
to various structures and applications. GradAug improves ResNet-50 to 78.79% on
ImageNet classification, which is a new state-of-the-art accuracy. By combining
with CutMix, it further boosts the performance to 79.67%, which outperforms an
ensemble of advanced training tricks. The generalization ability is evaluated
on COCO object detection and instance segmentation where GradAug significantly
surpasses other state-of-the-art methods. GradAug is also robust to image
distortions and FGSM adversarial attacks and is highly effective in low data
regimes. Code is available at https://github.com/taoyang1122/GradAug
</p>
<a href="http://arxiv.org/abs/2006.07989" target="_blank">arXiv:2006.07989</a> [<a href="http://arxiv.org/pdf/2006.07989" target="_blank">pdf</a>]

<h2>SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning. (arXiv:2006.08432v2 [cs.CV] UPDATED)</h2>
<h3>Gencer Sumbul, Sonali Nayak, Beg&#xfc;m Demir</h3>
<p>Deep neural networks (DNNs) have been recently found popular for image
captioning problems in remote sensing (RS). Existing DNN based approaches rely
on the availability of a training set made up of a high number of RS images
with their captions. However, captions of training images may contain redundant
information (they can be repetitive or semantically similar to each other),
resulting in information deficiency while learning a mapping from the image
domain to the language domain. To overcome this limitation, in this paper, we
present a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC)
approach. The proposed approach consists of three main steps. The first step
obtains the standard image captions by jointly exploiting convolutional neural
networks (CNNs) with long short-term memory (LSTM) networks. The second step,
unlike the existing RS image captioning methods, summarizes the ground-truth
captions of each training image into a single caption by exploiting sequence to
sequence neural networks and eliminates the redundancy present in the training
set. The third step automatically defines the adaptive weights associated to
each RS image to combine the standard captions with the summarized captions
based on the semantic content of the image. This is achieved by a novel
adaptive weighting strategy defined in the context of LSTM networks.
Experimental results obtained on the RSCID, UCM-Captions and Sydney-Captions
datasets show the effectiveness of the proposed approach compared to the
state-of-the-art RS image captioning approaches. The code of the proposed
approach is publicly available at
https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.
</p>
<a href="http://arxiv.org/abs/2006.08432" target="_blank">arXiv:2006.08432</a> [<a href="http://arxiv.org/pdf/2006.08432" target="_blank">pdf</a>]

<h2>Dynamic Tensor Rematerialization. (arXiv:2006.09616v3 [cs.LG] UPDATED)</h2>
<h3>Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, Zachary Tatlock</h3>
<p>Checkpointing enables training deep learning models under restricted memory
budgets by freeing intermediate activations from memory and recomputing them on
demand. Previous checkpointing techniques statically plan these recomputations
offline and assume static computation graphs. We demonstrate that a simple
online algorithm can achieve comparable performance by introducing Dynamic
Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing
that is extensible and general, is parameterized by eviction policy, and
supports dynamic models. We prove that DTR can train an $N$-layer linear
feedforward network on an $\Omega(\sqrt{N})$ memory budget with only
$\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of
optimal static checkpointing in simulated experiments. We incorporate a DTR
prototype into PyTorch just by interposing on tensor allocations and operator
calls and collecting lightweight metadata on tensors.
</p>
<a href="http://arxiv.org/abs/2006.09616" target="_blank">arXiv:2006.09616</a> [<a href="http://arxiv.org/pdf/2006.09616" target="_blank">pdf</a>]

<h2>Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design. (arXiv:2006.10504v2 [cs.AI] UPDATED)</h2>
<h3>Xiufeng Yang, Tanuj Kr Aasawat, Kazuki Yoshizoe</h3>
<p>It is common practice to use large computational resources to train neural
networks, as is known from many examples, such as reinforcement learning
applications. However, while massively parallel computing is often used for
training models, it is rarely used for searching solutions for combinatorial
optimization problems. In this paper, we propose a novel massively parallel
Monte-Carlo Tree Search (MP-MCTS) algorithm that works efficiently for 1,000
worker scale, and apply it to molecular design. This is the first work that
applies distributed MCTS to a real-world and non-game problem. Existing work on
large-scale parallel MCTS show efficient scalability in terms of the number of
rollouts up to 100 workers, but suffer from the degradation in the quality of
the solutions. MP-MCTS maintains the search quality at larger scale, and by
running MP-MCTS on 256 CPU cores for only 10 minutes, we obtained candidate
molecules having similar score to non-parallel MCTS running for 42 hours.
Moreover, our results based on parallel MCTS (combined with a simple RNN model)
significantly outperforms existing state-of-the-art work. Our method is generic
and is expected to speed up other applications of MCTS.
</p>
<a href="http://arxiv.org/abs/2006.10504" target="_blank">arXiv:2006.10504</a> [<a href="http://arxiv.org/pdf/2006.10504" target="_blank">pdf</a>]

<h2>Continual Learning in Recurrent Neural Networks. (arXiv:2006.12109v2 [cs.LG] UPDATED)</h2>
<h3>Benjamin Ehret, Christian Henning, Maria R. Cervera, Alexander Meulemans, Johannes von Oswald, Benjamin F. Grewe</h3>
<p>While a diverse collection of continual learning (CL) methods has been
proposed to prevent catastrophic forgetting, a thorough investigation of their
effectiveness for processing sequential data with recurrent neural networks
(RNNs) is lacking. Here, we provide the first comprehensive evaluation of
established CL methods on a variety of sequential data benchmarks.
Specifically, we shed light on the particularities that arise when applying
weight-importance methods, such as elastic weight consolidation, to RNNs. In
contrast to feedforward networks, RNNs iteratively reuse a shared set of
weights and require working memory to process input samples. We show that the
performance of weight-importance methods is not directly affected by the length
of the processed sequences, but rather by high working memory requirements,
which lead to an increased need for stability at the cost of decreased
plasticity for learning subsequent tasks. We additionally provide theoretical
arguments supporting this interpretation by studying linear RNNs. Our study
shows that established CL methods can be successfully ported to the recurrent
case, and that a recent regularization approach based on hypernetworks
outperforms weight-importance methods, thus emerging as a promising candidate
for CL in RNNs. Overall, we provide insights on the differences between CL in
feedforward networks and RNNs, while guiding towards effective solutions to
tackle CL on sequential data.
</p>
<a href="http://arxiv.org/abs/2006.12109" target="_blank">arXiv:2006.12109</a> [<a href="http://arxiv.org/pdf/2006.12109" target="_blank">pdf</a>]

<h2>Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning. (arXiv:2006.12621v3 [cs.LG] UPDATED)</h2>
<h3>Vedant Nanda, Samuel Dooley, Sahil Singla, Soheil Feizi, John P. Dickerson</h3>
<p>Deep neural networks (DNNs) are increasingly used in real-world applications
(e.g. facial recognition). This has resulted in concerns about the fairness of
decisions made by these models. Various notions and measures of fairness have
been proposed to ensure that a decision-making system does not
disproportionately harm (or benefit) particular subgroups of the population. In
this paper, we argue that traditional notions of fairness that are only based
on models' outputs are not sufficient when decision-making systems are
vulnerable to adversarial attacks. We argue that in some cases, it may be
easier for an attacker to target a particular subgroup, resulting in a form of
\textit{robustness bias}. We propose a new notion of fairness that requires all
subgroups to be equally robust to perturbations, thus ensuring a low
\textit{robustness bias}. We show that measuring robustness bias is a
challenging task for DNNs and propose two methods to measure this form of bias.
We then conduct an empirical study on state-of-the-art neural networks on
commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and
UTKFace and show that in almost all cases there are subgroups (often based on
sensitive attributes like race, gender, etc when applicable) which are less
robust to perturbations and are thus at a disadvantage. We argue that this kind
of bias arises due to both the data distribution and the highly complex nature
of the learned decision boundary in the case of deep neural networks, thus
making mitigation of such bias a non-trivial task. Our results show that
robustness bias is an important criterion to consider while auditing real-world
systems that rely on DNNs for decision making.
</p>
<a href="http://arxiv.org/abs/2006.12621" target="_blank">arXiv:2006.12621</a> [<a href="http://arxiv.org/pdf/2006.12621" target="_blank">pdf</a>]

<h2>Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. (arXiv:2006.12655v2 [cs.LG] UPDATED)</h2>
<h3>Cassidy Laidlaw, Sahil Singla, Soheil Feizi</h3>
<p>A key challenge in adversarial robustness is the lack of a precise
mathematical characterization of human perception, used in the very definition
of adversarial attacks that are imperceptible to human eyes. Most current
attacks and defenses try to avoid this issue by considering restrictive
adversarial threat models such as those bounded by $L_2$ or $L_\infty$
distance, spatial perturbations, etc. However, models that are robust against
any of these restrictive threat models are still fragile against other threat
models. To resolve this issue, we propose adversarial training against the set
of all imperceptible adversarial examples, approximated using deep neural
networks. We call this threat model the neural perceptual threat model (NPTM);
it includes adversarial examples with a bounded neural perceptual distance (a
neural network-based approximation of the true perceptual distance) to natural
images. Through an extensive perceptual study, we show that the neural
perceptual distance correlates well with human judgements of perceptibility of
adversarial examples, validating our threat model.

Under the NPTM, we develop novel perceptual adversarial attacks and defenses.
Because the NPTM is very broad, we find that Perceptual Adversarial Training
(PAT) against a perceptual attack gives robustness against many other types of
adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five
diverse adversarial attacks. We find that PAT achieves state-of-the-art
robustness against the union of these five attacks, more than doubling the
accuracy over the next best model, without training against any of them. That
is, PAT generalizes well to unforeseen perturbation types. This is vital in
sensitive applications where a particular threat model cannot be assumed, and
to the best of our knowledge, PAT is the first adversarial defense with this
property.
</p>
<a href="http://arxiv.org/abs/2006.12655" target="_blank">arXiv:2006.12655</a> [<a href="http://arxiv.org/pdf/2006.12655" target="_blank">pdf</a>]

<h2>DanHAR: Dual Attention Network For Multimodal Human Activity Recognition Using Wearable Sensors. (arXiv:2006.14435v2 [cs.CV] UPDATED)</h2>
<h3>Wenbin Gao, Lei Zhang, Qi Teng, Hao Wu, Fuhong Min, Jun He (Member, IEEE)</h3>
<p>Human activity recognition (HAR) in ubiquitous computing has been beginning
to incorporate attention into the context of deep neural networks (DNNs), in
which the rich sensing data from multimodal sensors such as accelerometer and
gyroscope is used to infer human activities. Recently, two attention methods
are proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term
Memory (LSTM) network, which can capture the dependencies of sensing signals in
both spatial and temporal domains simultaneously. However, recurrent networks
often have a weak feature representing power compared with convolutional neural
networks (CNNs). On the other hand, two attention, i.e., hard attention and
soft attention, are applied in temporal domains via combining with CNN, which
pay more attention to the target activity from a long sequence. However, they
can only tell where to focus and miss channel information, which plays an
important role in deciding what to focus. As a result, they fail to address the
spatial-temporal dependencies of multimodal sensing signals, compared with
attention-based GRU or LSTM. In the paper, we propose a novel dual attention
method called DanHAR, which introduces the framework of blending channel
attention and temporal attention on a CNN, demonstrating superiority in
improving the comprehensibility for multimodal HAR. Extensive experiments on
four public HAR datasets and weakly labeled dataset show that DanHAR achieves
state-of-the-art performance with negligible overhead of parameters.
Furthermore, visualizing analysis is provided to show that our attention can
amplifies more important sensor modalities and timesteps during classification,
which agrees well with human common intuition.
</p>
<a href="http://arxiv.org/abs/2006.14435" target="_blank">arXiv:2006.14435</a> [<a href="http://arxiv.org/pdf/2006.14435" target="_blank">pdf</a>]

<h2>Uniform Priors for Data-Efficient Transfer. (arXiv:2006.16524v2 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Karsten Roth, Anirudh Goyal, Marzyeh Ghassemi, Hugo Larochelle, Animesh Garg</h3>
<p>Deep Neural Networks have shown great promise on a variety of downstream
applications; but their ability to adapt and generalize to new data and tasks
remains a challenge. However, the ability to perform few or zero-shot
adaptation to novel tasks is important for the scalability and deployment of
machine learning models. It is therefore crucial to understand what makes for
good, transfer-able features in deep networks that best allow for such
adaptation. In this paper, we shed light on this by showing that features that
are most transferable have high uniformity in the embedding space and propose a
uniformity regularization scheme that encourages better transfer and feature
reuse. We evaluate the regularization on its ability to facilitate adaptation
to unseen tasks and data, for which we conduct a thorough experimental study
covering four relevant, and distinct domains: few-shot Meta-Learning, Deep
Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution
classification. Across all experiments, we show that uniformity regularization
consistently offers benefits over baseline methods and is able to achieve
state-of-the-art performance in Deep Metric Learning and Meta-Learning.
</p>
<a href="http://arxiv.org/abs/2006.16524" target="_blank">arXiv:2006.16524</a> [<a href="http://arxiv.org/pdf/2006.16524" target="_blank">pdf</a>]

<h2>NeuMiss networks: differential programming for supervised learning with missing values. (arXiv:2007.01627v3 [cs.LG] UPDATED)</h2>
<h3>Marine Le Morvan (PARIETAL, IJCLab), Julie Josse (CMAP, XPOP), Thomas Moreau (PARIETAL), Erwan Scornet (CMAP), Ga&#xeb;l Varoquaux (PARIETAL, MILA)</h3>
<p>The presence of missing values makes supervised learning much more
challenging. Indeed, previous work has shown that even when the response is a
linear function of the complete data, the optimal predictor is a complex
function of the observed entries and the missingness indicator. As a result,
the computational or sample complexities of consistent approaches depend on the
number of missing patterns, which can be exponential in the number of
dimensions. In this work, we derive the analytical form of the optimal
predictor under a linearity assumption and various missing data mechanisms
including Missing at Random (MAR) and self-masking (Missing Not At Random).
Based on a Neumann-series approximation of the optimal predictor, we propose a
new principled architecture, named NeuMiss networks. Their originality and
strength come from the use of a new type of non-linearity: the multiplication
by the missingness indicator. We provide an upper bound on the Bayes risk of
NeuMiss networks, and show that they have good predictive accuracy with both a
number of parameters and a computational complexity independent of the number
of missing data patterns. As a result they scale well to problems with many
features, and remain statistically efficient for medium-sized samples.
Moreover, we show that, contrary to procedures using EM or imputation, they are
robust to the missing data mechanism, including difficult MNAR settings such as
self-masking.
</p>
<a href="http://arxiv.org/abs/2007.01627" target="_blank">arXiv:2007.01627</a> [<a href="http://arxiv.org/pdf/2007.01627" target="_blank">pdf</a>]

<h2>Wandering Within a World: Online Contextualized Few-Shot Learning. (arXiv:2007.04546v2 [cs.LG] UPDATED)</h2>
<h3>Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard S. Zemel</h3>
<p>We aim to bridge the gap between typical human and machine-learning
environments by extending the standard framework of few-shot learning to an
online, continual setting. In this setting, episodes do not have separate
training and testing phases, and instead models are evaluated online while
learning novel classes. As in the real world, where the presence of
spatiotemporal context helps us retrieve learned skills in the past, our online
few-shot learning setting also features an underlying context that changes
throughout time. Object classes are correlated within a context and inferring
the correct context can lead to better performance. Building upon this setting,
we propose a new few-shot learning dataset based on large scale indoor imagery
that mimics the visual experience of an agent wandering within a world.
Furthermore, we convert popular few-shot learning approaches into online
versions and we also propose a new contextual prototypical memory model that
can make use of spatiotemporal contextual information from the recent past.
</p>
<a href="http://arxiv.org/abs/2007.04546" target="_blank">arXiv:2007.04546</a> [<a href="http://arxiv.org/pdf/2007.04546" target="_blank">pdf</a>]

<h2>Batch-Incremental Triplet Sampling for Training Triplet Networks Using Bayesian Updating Theorem. (arXiv:2007.05610v2 [stat.ML] UPDATED)</h2>
<h3>Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H.R. Tizhoosh</h3>
<p>Variants of Triplet networks are robust entities for learning a
discriminative embedding subspace. There exist different triplet mining
approaches for selecting the most suitable training triplets. Some of these
mining methods rely on the extreme distances between instances, and some others
make use of sampling. However, sampling from stochastic distributions of data
rather than sampling merely from the existing embedding instances can provide
more discriminative information. In this work, we sample triplets from
distributions of data rather than from existing instances. We consider a
multivariate normal distribution for the embedding of each class. Using
Bayesian updating and conjugate priors, we update the distributions of classes
dynamically by receiving the new mini-batches of training data. The proposed
triplet mining with Bayesian updating can be used with any triplet-based loss
function, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss.
Accordingly, Our triplet mining approaches are called Bayesian Updating Triplet
(BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is
being used. Experimental results on two public datasets, namely MNIST and
histopathology colorectal cancer (CRC), substantiate the effectiveness of the
proposed triplet mining method.
</p>
<a href="http://arxiv.org/abs/2007.05610" target="_blank">arXiv:2007.05610</a> [<a href="http://arxiv.org/pdf/2007.05610" target="_blank">pdf</a>]

<h2>Modulation of viability signals for self-regulatory control. (arXiv:2007.09297v2 [q-bio.NC] UPDATED)</h2>
<h3>Alvaro Ovalle, Simon M. Lucas</h3>
<p>We revisit the role of instrumental value as a driver of adaptive behavior.
In active inference, instrumental or extrinsic value is quantified by the
information-theoretic surprisal of a set of observations measuring the extent
to which those observations conform to prior beliefs or preferences. That is,
an agent is expected to seek the type of evidence that is consistent with its
own model of the world. For reinforcement learning tasks, the distribution of
preferences replaces the notion of reward. We explore a scenario in which the
agent learns this distribution in a self-supervised manner. In particular, we
highlight the distinction between observations induced by the environment and
those pertaining more directly to the continuity of an agent in time. We
evaluate our methodology in a dynamic environment with discrete time and
actions. First with a surprisal minimizing model-free agent (in the RL sense)
and then expanding to the model-based case to minimize the expected free
energy.
</p>
<a href="http://arxiv.org/abs/2007.09297" target="_blank">arXiv:2007.09297</a> [<a href="http://arxiv.org/pdf/2007.09297" target="_blank">pdf</a>]

<h2>Byzantine-Fault-Tolerant Consensus via Reinforcement Learning for Permissioned Blockchain Implemented in a V2X Network. (arXiv:2007.13957v3 [cs.NI] UPDATED)</h2>
<h3>Seungmo Kim, Ahmed S. Ibrahim</h3>
<p>Blockchain has been at the center of various trust-promoting applications for
vehicle-to-everything (V2X) networks. Recently, permissioned blockchains gain
practical popularity thanks to their improved scalability and diverse needs for
different organizations. One representative example of permissioned blockchain
is Hyperledger Fabric. Due to its unique execute-order procedure, there is a
critical need for a client to select an optimal number of peers. The
interesting problem that this paper targets to address is the tradeoff in the
number of peers: a too large number will lead to a lower scalability and a too
small number will leave a narrow margin in the number of peers sufficing the
Byzantine fault tolerance (BFT). This channel selection issue gets especially
challenging to deal with in V2X networks due to the mobility: a transaction
must be executed and the associated block must be committed before the vehicle
leaves a network. To this end, this paper proposes an optimal channel selection
mechanism based on reinforcement learning (RL) to keep a Hyperledger
Fabric-empowered V2X network impervious to dynamicity due to mobility. We model
the RL as a contextual multi-armed bandit (MAB) problem. The results prove the
outperformance of the proposed scheme.
</p>
<a href="http://arxiv.org/abs/2007.13957" target="_blank">arXiv:2007.13957</a> [<a href="http://arxiv.org/pdf/2007.13957" target="_blank">pdf</a>]

<h2>Detecting Anomalous Inputs to DNN Classifiers By Joint Statistical Testing at the Layers. (arXiv:2007.15147v2 [cs.LG] UPDATED)</h2>
<h3>Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, Suman Banerjee</h3>
<p>Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD)
inputs, is critical for classifiers deployed in real-world applications,
especially deep neural network (DNN) classifiers that are known to be brittle
on such inputs. We propose an unsupervised statistical testing framework for
detecting such anomalous inputs to a trained DNN classifier based on its
internal layer representations. By calculating test statistics at the input and
intermediate-layer representations of the DNN, conditioned individually on the
predicted class and on the true class of labeled training data, the method
characterizes their class-conditional distributions on natural inputs. Given a
test input, its extent of non-conformity with respect to the training
distribution is captured using p-values of the class-conditional test
statistics across the layers, which are then combined using a scoring function
designed to score high on anomalous inputs. We focus on adversarial inputs,
which are an important class of anomalous inputs, and also demonstrate the
effectiveness of our method on general OOD inputs. The proposed framework also
provides an alternative class prediction that can be used to correct the DNNs
prediction on (detected) adversarial inputs. Experiments on well-known image
classification datasets with strong adversarial attacks, including a custom
attack method that uses the internal layer representations of the DNN,
demonstrate that our method outperforms or performs comparably with five
recently-proposed, competing detection methods.
</p>
<a href="http://arxiv.org/abs/2007.15147" target="_blank">arXiv:2007.15147</a> [<a href="http://arxiv.org/pdf/2007.15147" target="_blank">pdf</a>]

<h2>A Survey on Text Classification: From Shallow to Deep Learning. (arXiv:2008.00364v4 [cs.CL] UPDATED)</h2>
<h3>Qian Li, Hao Peng, Jianxin Li, Congyin Xia, Renyu Yang, Lichao Sun, Philip S. Yu, Lifang He</h3>
<p>Text classification is the most fundamental and essential task in natural
language processing. The last decade has seen a surge of research in this area
due to the unprecedented success of deep learning. Numerous methods, datasets,
and evaluation metrics have been proposed in the literature, raising the need
for a comprehensive and updated survey. This paper fills the gap by reviewing
the state of the art approaches from 1961 to 2020, focusing on models from
shallow to deep learning. We create a taxonomy for text classification
according to the text involved and the models used for feature extraction and
classification. We then discuss each of these categories in detail, dealing
with both the technical developments and benchmark datasets that support tests
of predictions. A comprehensive comparison between different techniques, as
well as identifying the pros and cons of various evaluation metrics are also
provided in this survey. Finally, we conclude by summarizing key implications,
future research directions, and the challenges facing the research area.
</p>
<a href="http://arxiv.org/abs/2008.00364" target="_blank">arXiv:2008.00364</a> [<a href="http://arxiv.org/pdf/2008.00364" target="_blank">pdf</a>]

<h2>Swipe dynamics as a means of authentication: results from a Bayesian unsupervised approach. (arXiv:2008.01013v2 [cs.CR] UPDATED)</h2>
<h3>Parker Lamb, Alexander Millar, Ramon Fuentes</h3>
<p>The field of behavioural biometrics stands as an appealing alternative to
more traditional biometric systems due to the ease of use from a user
perspective and potential robustness to presentation attacks. This paper
focuses its attention to a specific type of behavioural biometric utilising
swipe dynamics, also referred to as touch gestures. In touch gesture
authentication, a user swipes across the touchscreen of a mobile device to
perform an authentication attempt. A key characteristic of touch gesture
authentication and new behavioural biometrics in general is the lack of
available data to train and validate models. From a machine learning
perspective, this presents the classic curse of dimensionality problem and the
methodology presented here focuses on Bayesian unsupervised models as they are
well suited to such conditions. This paper presents results from a set of
experiments consisting of 38 sessions with labelled victim as well as blind and
over-the-shoulder presentation attacks. Three models are compared using this
dataset; two single-mode models: a shrunk covariance estimate and a Bayesian
Gaussian distribution, as well as a Bayesian non-parametric infinite mixture of
Gaussians, modelled as a Dirichlet Process. Equal error rates (EER) for the
three models are compared and attention is paid to how these vary across the
two single-mode models at differing numbers of enrolment samples.
</p>
<a href="http://arxiv.org/abs/2008.01013" target="_blank">arXiv:2008.01013</a> [<a href="http://arxiv.org/pdf/2008.01013" target="_blank">pdf</a>]

<h2>TinySpeech: Attention Condensers for Deep Speech Recognition Neural Networks on Edge Devices. (arXiv:2008.04245v6 [eess.AS] UPDATED)</h2>
<h3>Alexander Wong, Mahmoud Famouri, Maya Pavlova, Siddharth Surana</h3>
<p>Advances in deep learning have led to state-of-the-art performance across a
multitude of speech recognition tasks. Nevertheless, the widespread deployment
of deep neural networks for on-device speech recognition remains a challenge,
particularly in edge scenarios where the memory and computing resources are
highly constrained (e.g., low-power embedded devices) or where the memory and
computing budget dedicated to speech recognition is low (e.g., mobile devices
performing numerous tasks besides speech recognition). In this study, we
introduce the concept of attention condensers for building low-footprint,
highly-efficient deep neural networks for on-device speech recognition on the
edge. An attention condenser is a self-attention mechanism that learns and
produces a condensed embedding characterizing joint local and cross-channel
activation relationships, and performs selective attention accordingly. To
illustrate its efficacy, we introduce TinySpeech, low-precision deep neural
networks comprising largely of attention condensers tailored for on-device
speech recognition using a machine-driven design exploration strategy, with one
tailored specifically with microcontroller operation constraints. Experimental
results on the Google Speech Commands benchmark dataset for limited-vocabulary
speech recognition showed that TinySpeech networks achieved significantly lower
architectural complexity (as much as $507\times$ fewer parameters), lower
computational complexity (as much as $48\times$ fewer multiply-add operations),
and lower storage requirements (as much as $2028\times$ lower weight memory
requirements) when compared to previous work. These results not only
demonstrate the efficacy of attention condensers for building highly efficient
networks for on-device speech recognition, but also illuminate its potential
for accelerating deep learning on the edge and empowering TinyML applications.
</p>
<a href="http://arxiv.org/abs/2008.04245" target="_blank">arXiv:2008.04245</a> [<a href="http://arxiv.org/pdf/2008.04245" target="_blank">pdf</a>]

<h2>Neural Network-based Automatic Factor Construction. (arXiv:2008.06225v3 [q-fin.ST] UPDATED)</h2>
<h3>Jie Fang, Jianwu Lin, Shutao Xia, Yong Jiang, Zhikang Xia, Xiang Liu</h3>
<p>Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
</p>
<a href="http://arxiv.org/abs/2008.06225" target="_blank">arXiv:2008.06225</a> [<a href="http://arxiv.org/pdf/2008.06225" target="_blank">pdf</a>]

<h2>Machine learning for COVID-19 detection and prognostication using chest radiographs and CT scans: a systematic methodological review. (arXiv:2008.06388v3 [cs.LG] UPDATED)</h2>
<h3>Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Aviles-Rivero, Christian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, James H.F. Rudd, Evis Sala, Carola-Bibiane Sch&#xf6;nlieb (on behalf of the AIX-COVNET collaboration)</h3>
<p>Background: Machine learning methods offer great potential for fast and
accurate detection and prognostication of COVID-19 from standard-of-care chest
radiographs (CXR) and computed tomography (CT) images. In this systematic
review we critically evaluate the machine learning methodologies employed in
the rapidly growing literature.

Methods: In this systematic review we reviewed EMBASE via OVID, MEDLINE via
PubMed, bioRxiv, medRxiv and arXiv for published papers and preprints uploaded
from Jan 1, 2020 to June 24, 2020. Studies which consider machine learning
models for the diagnosis or prognosis of COVID-19 from CXR or CT images were
included. A methodology quality review of each paper was performed against
established benchmarks to ensure the review focusses only on high-quality
reproducible papers. This study is registered with PROSPERO [CRD42020188887].

Interpretation: Our review finds that none of the developed models discussed
are of potential clinical use due to methodological flaws and underlying
biases. This is a major weakness, given the urgency with which validated
COVID-19 models are needed. Typically, we find that the documentation of a
model's development is not sufficient to make the results reproducible and
therefore of 168 candidate papers only 29 are deemed to be reproducible and
subsequently considered in this review. We therefore encourage authors to use
established machine learning checklists to ensure sufficient documentation is
made available, and to follow the PROBAST (prediction model risk of bias
assessment tool) framework to determine the underlying biases in their model
development process and to mitigate these where possible. This is key to safe
clinical implementation which is urgently needed.
</p>
<a href="http://arxiv.org/abs/2008.06388" target="_blank">arXiv:2008.06388</a> [<a href="http://arxiv.org/pdf/2008.06388" target="_blank">pdf</a>]

<h2>Inductive logic programming at 30: a new introduction. (arXiv:2008.07912v3 [cs.AI] UPDATED)</h2>
<h3>Andrew Cropper, Sebastijan Duman&#x10d;i&#x107;</h3>
<p>Inductive logic programming (ILP) is a form of machine learning. The goal of
ILP is to induce a hypothesis (a set of logical rules) that generalises given
training examples. In contrast to most forms of machine learning, ILP can learn
human-readable hypotheses from small amounts of data. As ILP approaches 30, we
provide a new introduction to the field. We introduce the necessary logical
notation and the main ILP learning settings. We describe the main building
blocks of an ILP system. We compare several ILP systems on several dimensions.
We describe in detail four systems (Aleph, TILDE, ASPAL, and Metagol). We
document some of the main application areas of ILP. Finally, we summarise the
current limitations and outline promising directions for future research.
</p>
<a href="http://arxiv.org/abs/2008.07912" target="_blank">arXiv:2008.07912</a> [<a href="http://arxiv.org/pdf/2008.07912" target="_blank">pdf</a>]

<h2>Deep Ice Layer Tracking and Thickness Estimation using Fully Convolutional Networks. (arXiv:2009.00191v2 [cs.CV] UPDATED)</h2>
<h3>Maryam Rahnemoonfar, Debvrat Varshney, Masoud Yari, John Paden</h3>
<p>Global warming is rapidly reducing glaciers and ice sheets across the world.
Real time assessment of this reduction is required so as to monitor its global
climatic impact. In this paper, we introduce a novel way of estimating the
thickness of each internal ice layer using Snow Radar images and Fully
Convolutional Networks. The estimated thickness can be analysed to understand
snow accumulation each year. To understand the depth and structure of each
internal ice layer, we carry out a set of image processing techniques and
perform semantic segmentation on the radar images. After detecting each ice
layer uniquely, we calculate its thickness and compare it with the available
ground truth. Through this procedure we were able to estimate the ice layer
thicknesses within a Mean Absolute Error of approximately 3.6 pixels. Such a
Deep Learning based method can be used with ever-increasing datasets to make
accurate assessments for cryospheric studies.
</p>
<a href="http://arxiv.org/abs/2009.00191" target="_blank">arXiv:2009.00191</a> [<a href="http://arxiv.org/pdf/2009.00191" target="_blank">pdf</a>]

<h2>BOP Challenge 2020 on 6D Object Localization. (arXiv:2009.07378v2 [cs.CV] UPDATED)</h2>
<h3>Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann, Frank Michel, Carsten Rother, Jiri Matas</h3>
<p>This paper presents the evaluation methodology, datasets, and results of the
BOP Challenge 2020, the third in a series of public competitions organized with
the goal to capture the status quo in the field of 6D object pose estimation
from an RGB-D image. In 2020, to reduce the domain gap between synthetic
training and real test RGB images, the participants were provided 350K
photorealistic training images generated by BlenderProc4BOP, a new open-source
and light-weight physically-based renderer (PBR) and procedural data generator.
Methods based on deep neural networks have finally caught up with methods based
on point pair features, which were dominating previous editions of the
challenge. Although the top-performing methods rely on RGB-D image channels,
strong results were achieved when only RGB channels were used at both training
and test time - out of the 26 evaluated methods, the third method was trained
on RGB channels of PBR and real images, while the fifth on RGB channels of PBR
images only. Strong data augmentation was identified as a key component of the
top-performing CosyPose method, and the photorealism of PBR images was
demonstrated effective despite the augmentation. The online evaluation system
stays open and is available on the project website: bop.felk.cvut.cz.
</p>
<a href="http://arxiv.org/abs/2009.07378" target="_blank">arXiv:2009.07378</a> [<a href="http://arxiv.org/pdf/2009.07378" target="_blank">pdf</a>]

<h2>Group-wise Contrastive Learning for Neural Dialogue Generation. (arXiv:2009.07543v2 [cs.CL] UPDATED)</h2>
<h3>Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, Xiaofang Zhao</h3>
<p>Neural dialogue response generation has gained much popularity in recent
years. Maximum Likelihood Estimation (MLE) objective is widely adopted in
existing dialogue model learning. However, models trained with MLE objective
function are plagued by the low-diversity issue when it comes to the
open-domain conversational setting. Inspired by the observation that humans not
only learn from the positive signals but also benefit from correcting behaviors
of undesirable actions, in this work, we introduce contrastive learning into
dialogue generation, where the model explicitly perceives the difference
between the well-chosen positive and negative utterances. Specifically, we
employ a pretrained baseline model as a reference. During contrastive learning,
the target dialogue model is trained to give higher conditional probabilities
for the positive samples, and lower conditional probabilities for those
negative samples, compared to the reference model. To manage the multi-mapping
relations prevailed in human conversation, we augment contrastive dialogue
learning with group-wise dual sampling. Extensive experimental results show
that the proposed group-wise contrastive learning framework is suited for
training a wide range of neural dialogue generation models with very favorable
performance over the baseline training approaches.
</p>
<a href="http://arxiv.org/abs/2009.07543" target="_blank">arXiv:2009.07543</a> [<a href="http://arxiv.org/pdf/2009.07543" target="_blank">pdf</a>]

<h2>A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning. (arXiv:2009.08115v3 [cs.CL] UPDATED)</h2>
<h3>Yichi Zhang, Zhijian Ou, Huixin Wang, Junlan Feng</h3>
<p>Structured belief states are crucial for user goal tracking and database
query in task-oriented dialog systems. However, training belief trackers often
requires expensive turn-level annotations of every user utterance. In this
paper we aim at alleviating the reliance on belief state labels in building
end-to-end dialog systems, by leveraging unlabeled dialog data towards
semi-supervised learning. We propose a probabilistic dialog model, called the
LAtent BElief State (LABES) model, where belief states are represented as
discrete latent variables and jointly modeled with system responses given user
inputs. Such latent variable modeling enables us to develop semi-supervised
learning under the principled variational learning framework. Furthermore, we
introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of
LABES. In supervised experiments, LABES-S2S obtains strong results on three
benchmark datasets of different scales. In utilizing unlabeled dialog data,
semi-supervised LABES-S2S significantly outperforms both supervised-only and
semi-supervised baselines. Remarkably, we can reduce the annotation demands to
50% without performance loss on MultiWOZ.
</p>
<a href="http://arxiv.org/abs/2009.08115" target="_blank">arXiv:2009.08115</a> [<a href="http://arxiv.org/pdf/2009.08115" target="_blank">pdf</a>]

<h2>Pose Imitation Constraints for Collaborative Robots. (arXiv:2009.10947v2 [cs.RO] UPDATED)</h2>
<h3>Glebys Gonzalez, Juan Wachs</h3>
<p>Achieving human-like motion in robots has been a fundamental goal in many
areas of robotics research. Inverse kinematic (IK) solvers have been explored
as a solution to provide kinematic structures with anthropomorphic movements.
In particular, numeric solvers based on geometry, such as FABRIK, have shown
potential for producing human-like motion at a low computational cost.
Nevertheless, these methods have shown limitations when solving for robot
kinematic constraints. This work proposes a framework inspired by FABRIK for
human pose imitation in real-time. The goal is to mitigate the problems of the
original algorithm while retaining the resulting humanlike fluidity and low
cost. We first propose a human constraint model for pose imitation. Then, we
present a pose imitation algorithm (PIC), and it's soft version (PICs) that can
successfully imitate human poses using the proposed constraint system. PIC was
tested on two collaborative robots (Baxter and YuMi). Fifty human
demonstrations were collected for a bi-manual assembly and an incision task.
Then, two performance metrics were obtained for both robots: pose accuracy with
respect to the human and the percentage of environment occlusion/obstruction.
The performance of PIC and PICs was compared against the numerical solver
baseline (FABRIK). The proposed algorithms achieve a higher pose accuracy than
FABRIK for both tasks (25%-FABRIK, 53%-PICs, 58%-PICs). In addition, PIC and
it's soft version achieve a lower percentage of occlusion during incision
(10%-FABRIK, 4%-PICs, 9%-PICs). These results indicate that the PIC method can
reproduce human poses and achieve key desired effects of human imitation.
</p>
<a href="http://arxiv.org/abs/2009.10947" target="_blank">arXiv:2009.10947</a> [<a href="http://arxiv.org/pdf/2009.10947" target="_blank">pdf</a>]

<h2>Learning in a Small/Big World. (arXiv:2009.11917v4 [econ.TH] UPDATED)</h2>
<h3>Benson Tsz Kin Leung</h3>
<p>Savage (1972) lays down the foundation of Bayesian decision theory, but
asserts that it is not applicable in big worlds where the environment is
complex. Using the theory of finite automaton to model belief formation, this
paper studies the characteristics of optimal learning behavior in small and big
worlds, where the complexity of the environment is low and high, respectively,
relative to the cognitive ability of the decision maker. Confirming Savage's
claim, optimal learning behavior is closed to Bayesian in small worlds but
significantly different in big worlds. In addition, I show that in big worlds,
the optimal learning behavior could exhibit a wide range of well-documented
non-Bayesian learning behavior, including the use of heuristic, correlation
neglect, persistent over-confidence, inattentive learning, and other behaviors
of model simplification or misspecification. These results establish a clear
and testable relationship between the prominence of non-Bayesian learning
behavior, complexity and cognitive ability.
</p>
<a href="http://arxiv.org/abs/2009.11917" target="_blank">arXiv:2009.11917</a> [<a href="http://arxiv.org/pdf/2009.11917" target="_blank">pdf</a>]

<h2>Towards Debiasing NLU Models from Unknown Biases. (arXiv:2009.12303v4 [cs.CL] UPDATED)</h2>
<h3>Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych</h3>
<p>NLU models often exploit biases to achieve high dataset-specific performance
without properly learning the intended task. Recently proposed debiasing
methods are shown to be effective in mitigating this tendency. However, these
methods rely on a major assumption that the types of bias should be known
a-priori, which limits their application to many NLU tasks and datasets. In
this work, we present the first step to bridge this gap by introducing a
self-debiasing framework that prevents models from mainly utilizing biases
without knowing them in advance. The proposed framework is general and
complementary to the existing debiasing methods. We show that it allows these
existing methods to retain the improvement on the challenge datasets (i.e.,
sets of examples designed to expose models' reliance on biases) without
specifically targeting certain biases. Furthermore, the evaluation suggests
that applying the framework results in improved overall robustness.
</p>
<a href="http://arxiv.org/abs/2009.12303" target="_blank">arXiv:2009.12303</a> [<a href="http://arxiv.org/pdf/2009.12303" target="_blank">pdf</a>]

<h2>Benchmarking deep inverse models over time, and the neural-adjoint method. (arXiv:2009.12919v2 [cs.LG] UPDATED)</h2>
<h3>Simiao Ren, Willie Padilla, Jordan Malof</h3>
<p>We consider the task of solving generic inverse problems, where one wishes to
determine the hidden parameters of a natural system that will give rise to a
particular set of measurements. Recently many new approaches based upon deep
learning have arisen generating impressive results. We conceptualize these
models as different schemes for efficiently, but randomly, exploring the space
of possible inverse solutions. As a result, the accuracy of each approach
should be evaluated as a function of time rather than a single estimated
solution, as is often done now. Using this metric, we compare several
state-of-the-art inverse modeling approaches on four benchmark tasks: two
existing tasks, one simple task for visualization and one new task from
metamaterial design. Finally, inspired by our conception of the inverse
problem, we explore a solution that uses a deep learning model to approximate
the forward model, and then uses backpropagation to search for good inverse
solutions. This approach, termed the neural-adjoint, achieves the best
performance in many scenarios.
</p>
<a href="http://arxiv.org/abs/2009.12919" target="_blank">arXiv:2009.12919</a> [<a href="http://arxiv.org/pdf/2009.12919" target="_blank">pdf</a>]

<h2>Avoiding Help Avoidance: Using Interface Design Changes to Promote Unsolicited Hint Usage in an Intelligent Tutor. (arXiv:2009.13371v2 [cs.AI] UPDATED)</h2>
<h3>Mehak Maniktala, Christa Cody, Tiffany Barnes, Min Chi</h3>
<p>Within intelligent tutoring systems, considerable research has investigated
hints, including how to generate data-driven hints, what hint content to
present, and when to provide hints for optimal learning outcomes. However, less
attention has been paid to how hints are presented. In this paper, we propose a
new hint delivery mechanism called "Assertions" for providing unsolicited hints
in a data-driven intelligent tutor. Assertions are partially-worked example
steps designed to appear within a student workspace, and in the same format as
student-derived steps, to show students a possible subgoal leading to the
solution. We hypothesized that Assertions can help address the well-known hint
avoidance problem. In systems that only provide hints upon request, hint
avoidance results in students not receiving hints when they are needed. Our
unsolicited Assertions do not seek to improve student help-seeking, but rather
seek to ensure students receive the help they need. We contrast Assertions with
Messages, text-based, unsolicited hints that appear after student inactivity.
Our results show that Assertions significantly increase unsolicited hint usage
compared to Messages. Further, they show a significant aptitude-treatment
interaction between Assertions and prior proficiency, with Assertions leading
students with low prior proficiency to generate shorter (more efficient)
posttest solutions faster. We also present a clustering analysis that shows
patterns of productive persistence among students with low prior knowledge when
the tutor provides unsolicited help in the form of Assertions. Overall, this
work provides encouraging evidence that hint presentation can significantly
impact how students use them and using Assertions can be an effective way to
address help avoidance.
</p>
<a href="http://arxiv.org/abs/2009.13371" target="_blank">arXiv:2009.13371</a> [<a href="http://arxiv.org/pdf/2009.13371" target="_blank">pdf</a>]

<h2>The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity. (arXiv:2009.13676v2 [cs.CY] UPDATED)</h2>
<h3>Mohamed Abdalla, Moustafa Abdalla</h3>
<p>As governmental bodies rely on academics' expert advice to shape policy
regarding Artificial Intelligence, it is important that these academics not
have conflicts of interests that may cloud or bias their judgement. Our work
explores how Big Tech is actively distorting the academic landscape to suit its
needs. By comparing the well-studied actions of another industry, that of Big
Tobacco, to the current actions of Big Tech we see similar strategies employed
by both industries to sway and influence academic and public discourse. We
examine the funding of academic research as a tool used by Big Tech to put
forward a socially responsible public image, influence events hosted by and
decisions made by funded universities, influence the research questions and
plans of individual scientists, and discover receptive academics who can be
leveraged. We demonstrate, in a rigorous manner, how Big Tech can affect
academia from the institutional level down to individual researchers. Thus, we
believe that it is vital, particularly for universities and other institutions
of higher learning, to discuss the appropriateness and the tradeoffs of
accepting funding from Big Tech, and what limitations or conditions should be
put in place.
</p>
<a href="http://arxiv.org/abs/2009.13676" target="_blank">arXiv:2009.13676</a> [<a href="http://arxiv.org/pdf/2009.13676" target="_blank">pdf</a>]

<h2>Deep Evolution for Facial Emotion Recognition. (arXiv:2009.14194v2 [cs.NE] UPDATED)</h2>
<h3>Emmanuel Dufourq, Bruce A. Bassett</h3>
<p>Deep facial expression recognition faces two challenges that both stem from
the large number of trainable parameters: long training times and a lack of
interpretability. We propose a novel method based on evolutionary algorithms,
that deals with both challenges by massively reducing the number of trainable
parameters, whilst simultaneously retaining classification performance, and in
some cases achieving superior performance. We are robustly able to reduce the
number of parameters on average by 95% (e.g. from 2M to 100k parameters) with
no loss in classification accuracy. The algorithm learns to choose small
patches from the image, relative to the nose, which carry the most important
information about emotion, and which coincide with typical human choices of
important features. Our work implements a novel form attention and shows that
evolutionary algorithms are a valuable addition to machine learning in the deep
learning era, both for reducing the number of parameters for facial expression
recognition and for providing interpretable features that can help reduce bias.
</p>
<a href="http://arxiv.org/abs/2009.14194" target="_blank">arXiv:2009.14194</a> [<a href="http://arxiv.org/pdf/2009.14194" target="_blank">pdf</a>]

<h2>S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency. (arXiv:2009.14711v2 [cs.RO] UPDATED)</h2>
<h3>Mel Vecerik, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile Pevceviciute, Thomas Roth&#xf6;rl, Christopher Schuster, Raia Hadsell, Lourdes Agapito, Jonathan Scholz</h3>
<p>A robot's ability to act is fundamentally constrained by what it can
perceive. Many existing approaches to visual representation learning utilize
general-purpose training criteria, e.g. image reconstruction, smoothness in
latent space, or usefulness for control, or else make use of large datasets
annotated with specific features (bounding boxes, segmentations, etc.).
However, both approaches often struggle to capture the fine-detail required for
precision tasks on specific objects, e.g. grasping and mating a plug and
socket. We argue that these difficulties arise from a lack of geometric
structure in these models. In this work we advocate semantic 3D keypoints as a
visual representation, and present a semi-supervised training objective that
can allow instance or category-level keypoints to be trained to 1-5
millimeter-accuracy with minimal supervision. Furthermore, unlike local
texture-based approaches, our model integrates contextual information from a
large area and is therefore robust to occlusion, noise, and lack of discernible
texture. We demonstrate that this ability to locate semantic keypoints enables
high level scripting of human understandable behaviours. Finally we show that
these keypoints provide a good way to define reward functions for reinforcement
learning and are a good representation for training agents.
</p>
<a href="http://arxiv.org/abs/2009.14711" target="_blank">arXiv:2009.14711</a> [<a href="http://arxiv.org/pdf/2009.14711" target="_blank">pdf</a>]

<h2>High Speed Event Camera TRacking. (arXiv:2010.02771v2 [cs.CV] UPDATED)</h2>
<h3>William Chamorro, Juan Andrade-Cetto, Joan Sol&#xe0;</h3>
<p>Event cameras are bioinspired sensors with reaction times in the order of
microseconds. This property makes them appealing for use in highly-dynamic
computer vision applications. In this work,we explore the limits of this
sensing technology and present an ultra-fast tracking algorithm able to
estimate six-degree-of-freedom motion with dynamics over 25.8 g, at a
throughput of 10 kHz,processing over a million events per second. Our method is
capable of tracking either camera motion or the motion of an object in front of
it, using an error-state Kalman filter formulated in a Lie-theoretic sense. The
method includes a robust mechanism for the matching of events with projected
line segments with very fast outlier rejection. Meticulous treatment of sparse
matrices is applied to achieve real-time performance. Different motion models
of varying complexity are considered for the sake of comparison and performance
analysis
</p>
<a href="http://arxiv.org/abs/2010.02771" target="_blank">arXiv:2010.02771</a> [<a href="http://arxiv.org/pdf/2010.02771" target="_blank">pdf</a>]

<h2>Reinforcement Learning in Deep Structured Teams: Initial Results with Finite and Infinite Valued Features. (arXiv:2010.02868v3 [cs.MA] UPDATED)</h2>
<h3>Jalal Arabneydi, Masoud Roudneshin, Amir G. Aghdam</h3>
<p>In this paper, we consider Markov chain and linear quadratic models for deep
structured teams with discounted and time-average cost functions under two
non-classical information structures, namely, deep state sharing and no
sharing. In deep structured teams, agents are coupled in dynamics and cost
functions through deep state, where deep state refers to a set of orthogonal
linear regressions of the states. In this article, we consider a homogeneous
linear regression for Markov chain models (i.e., empirical distribution of
states) and a few orthonormal linear regressions for linear quadratic models
(i.e., weighted average of states). Some planning algorithms are developed for
the case when the model is known, and some reinforcement learning algorithms
are proposed for the case when the model is not known completely. The
convergence of two model-free (reinforcement learning) algorithms, one for
Markov chain models and one for linear quadratic models, is established. The
results are then applied to a smart grid.
</p>
<a href="http://arxiv.org/abs/2010.02868" target="_blank">arXiv:2010.02868</a> [<a href="http://arxiv.org/pdf/2010.02868" target="_blank">pdf</a>]

<h2>Variational Transfer Learning for Fine-grained Few-shot Visual Recognition. (arXiv:2010.03255v2 [cs.CV] UPDATED)</h2>
<h3>Jingyi Xu, Mingzhen Huang, ShahRukh Athar, Dimitris Samaras</h3>
<p>Fine-grained few-shot recognition often suffers from the problem of training
data scarcity for novel categories.The network tends to overfit and does not
generalize well to unseen classes due to insufficient training data. Many
methods have been proposed to synthesize additional data to support the
training. In this paper, we focus one enlarging the intra-class variance of the
unseen class to improve few-shot classification performance. We assume that the
distribution of intra-class variance generalizes across the base class and the
novel class. Thus, the intra-class variance of the base set can be transferred
to the novel set for feature augmentation. Specifically, we first model the
distribution of intra-class variance on the base set via variational inference.
Then the learned distribution is transferred to the novel set to generate
additional features, which are used together with the original ones to train a
classifier. Experimental results show a significant boost over the
state-of-the-art methods on the challenging fine-grained few-shot image
classification benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.03255" target="_blank">arXiv:2010.03255</a> [<a href="http://arxiv.org/pdf/2010.03255" target="_blank">pdf</a>]

<h2>BoMuDA: Boundless Multi-Source Domain Adaptive Segmentation in Unconstrained Environments. (arXiv:2010.03523v2 [cs.CV] UPDATED)</h2>
<h3>Divya Kothandaraman, Rohan Chandra, Dinesh Manocha</h3>
<p>We present an unsupervised multi-source domain adaptive semantic segmentation
approach in unstructured and unconstrained traffic environments. We propose a
novel training strategy that alternates between single-source domain adaptation
(DA) and multi-source distillation, and also between setting up an improvised
cost function and optimizing it. In each iteration, the single-source DA first
learns a neural network on a selected source, which is followed by a
multi-source fine-tuning step using the remaining sources. We call this
training routine the Alternating-Incremental ("Alt-Inc") algorithm.
Furthermore, our approach is also boundless i.e. it can explicitly classify
categories that do not belong to the training dataset (as opposed to labeling
such objects as "unknown"). We have conducted extensive experiments and
ablation studies using the Indian Driving Dataset, CityScapes, Berkeley
DeepDrive, GTA V, and the Synscapes datasets, and we show that our unsupervised
approach outperforms other unsupervised and semi-supervised SOTA benchmarks by
5.17% - 42.9% with a reduced model size by up to 5.2x.
</p>
<a href="http://arxiv.org/abs/2010.03523" target="_blank">arXiv:2010.03523</a> [<a href="http://arxiv.org/pdf/2010.03523" target="_blank">pdf</a>]

<h2>Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task. (arXiv:2010.04297v2 [cs.CL] UPDATED)</h2>
<h3>Thibault Sellam, Amy Pu, Hyung Won Chung, Sebastian Gehrmann, Qijun Tan, Markus Freitag, Dipanjan Das, Ankur P. Parikh</h3>
<p>The quality of machine translation systems has dramatically improved over the
last decade, and as a result, evaluation has become an increasingly challenging
problem. This paper describes our contribution to the WMT 2020 Metrics Shared
Task, the main benchmark for automatic evaluation of translation. We make
several submissions based on BLEURT, a previously published metric based on
transfer learning. We extend the metric beyond English and evaluate it on 14
language pairs for which fine-tuning data is available, as well as 4
"zero-shot" language pairs, for which we have no labelled examples.
Additionally, we focus on English to German and demonstrate how to combine
BLEURT's predictions with those of YiSi and use alternative reference
translations to enhance the performance. Empirical results show that the models
achieve competitive results on the WMT Metrics 2019 Shared Task, indicating
their promise for the 2020 edition.
</p>
<a href="http://arxiv.org/abs/2010.04297" target="_blank">arXiv:2010.04297</a> [<a href="http://arxiv.org/pdf/2010.04297" target="_blank">pdf</a>]

<h2>Weaponizing Unicodes with Deep Learning -- Identifying Homoglyphs with Weakly Labeled Data. (arXiv:2010.04382v3 [cs.CR] UPDATED)</h2>
<h3>Perry Deng, Cooper Linsky, Matthew Wright</h3>
<p>Visually similar characters, or homoglyphs, can be used to perform social
engineering attacks or to evade spam and plagiarism detectors. It is thus
important to understand the capabilities of an attacker to identify homoglyphs
-- particularly ones that have not been previously spotted -- and leverage them
in attacks. We investigate a deep-learning model using embedding learning,
transfer learning, and augmentation to determine the visual similarity of
characters and thereby identify potential homoglyphs. Our approach uniquely
takes advantage of weak labels that arise from the fact that most characters
are not homoglyphs. Our model drastically outperforms the Normalized
Compression Distance approach on pairwise homoglyph identification, for which
we achieve an average precision of 0.97. We also present the first attempt at
clustering homoglyphs into sets of equivalence classes, which is more efficient
than pairwise information for security practitioners to quickly lookup
homoglyphs or to normalize confusable string encodings. To measure clustering
performance, we propose a metric (mBIOU) building on the classic
Intersection-Over-Union (IOU) metric. Our clustering method achieves 0.592
mBIOU, compared to 0.430 for the naive baseline. We also use our model to
predict over 8,000 previously unknown homoglyphs, and find good early
indications that many of these may be true positives. Source code and list of
predicted homoglyphs are uploaded to Github:
https://github.com/PerryXDeng/weaponizing_unicode
</p>
<a href="http://arxiv.org/abs/2010.04382" target="_blank">arXiv:2010.04382</a> [<a href="http://arxiv.org/pdf/2010.04382" target="_blank">pdf</a>]

<h2>Joint State-Action Embedding for Efficient Reinforcement Learning. (arXiv:2010.04444v2 [cs.LG] UPDATED)</h2>
<h3>Paul J. Pritz, Liang Ma, Kin K. Leung</h3>
<p>While reinforcement learning has achieved considerable successes in recent
years, state-of-the-art models are often still limited by the size of state and
action spaces. Model-free reinforcement learning approaches use some form of
state representations and the latest work has explored embedding techniques for
actions, both with the aim of achieving better generalization and
applicability. However, these approaches consider only states or actions,
ignoring the interaction between them when generating embedded representations.
In this work, we propose a new approach for jointly embedding states and
actions that combines aspects of model-free and model-based reinforcement
learning, which can be applied in both discrete and continuous domains.
Specifically, we use a model of the environment to obtain embeddings for states
and actions and present a generic architecture that uses these to learn a
policy. In this way, the embedded representations obtained via our approach
enable better generalization over both states and actions by capturing
similarities in the embedding spaces. Evaluations of our approach on several
gaming and recommender system environments show it significantly outperforms
state-of-the-art models in discrete domains with large state/action space, thus
confirming the efficacy of joint embedding and its overall superior
performance.
</p>
<a href="http://arxiv.org/abs/2010.04444" target="_blank">arXiv:2010.04444</a> [<a href="http://arxiv.org/pdf/2010.04444" target="_blank">pdf</a>]

<h2>Self-Paced Learning for Neural Machine Translation. (arXiv:2010.04505v2 [cs.CL] UPDATED)</h2>
<h3>Yu Wan, Baosong Yang, Derek F. Wong, Yikai Zhou, Lidia S. Chao, Haibo Zhang, Boxing Chen</h3>
<p>Recent studies have proven that the training of neural machine translation
(NMT) can be facilitated by mimicking the learning process of humans.
Nevertheless, achievements of such kind of curriculum learning rely on the
quality of artificial schedule drawn up with the handcrafted features, e.g.
sentence length or word rarity. We ameliorate this procedure with a more
flexible manner by proposing self-paced learning, where NMT model is allowed to
1) automatically quantify the learning confidence over training examples; and
2) flexibly govern its learning via regulating the loss in each iteration step.
Experimental results over multiple translation tasks demonstrate that the
proposed model yields better performance than strong baselines and those models
trained with human-designed curricula on both translation quality and
convergence speed.
</p>
<a href="http://arxiv.org/abs/2010.04505" target="_blank">arXiv:2010.04505</a> [<a href="http://arxiv.org/pdf/2010.04505" target="_blank">pdf</a>]

<h2>RNN Training along Locally Optimal Trajectories via Frank-Wolfe Algorithm. (arXiv:2010.05397v2 [cs.LG] UPDATED)</h2>
<h3>Yun Yue, Ming Li, Venkatesh Saligrama, Ziming Zhang</h3>
<p>We propose a novel and efficient training method for RNNs by iteratively
seeking a local minima on the loss surface within a small region, and leverage
this directional vector for the update, in an outer-loop. We propose to utilize
the Frank-Wolfe (FW) algorithm in this context. Although, FW implicitly
involves normalized gradients, which can lead to a slow convergence rate, we
develop a novel RNN training method that, surprisingly, even with the
additional cost, the overall training cost is empirically observed to be lower
than back-propagation. Our method leads to a new Frank-Wolfe method, that is in
essence an SGD algorithm with a restart scheme. We prove that under certain
conditions our algorithm has a sublinear convergence rate of $O(1/\epsilon)$
for $\epsilon$ error. We then conduct empirical experiments on several
benchmark datasets including those that exhibit long-term dependencies, and
show significant performance improvement. We also experiment with deep RNN
architectures and show efficient training performance. Finally, we demonstrate
that our training method is robust to noisy data.
</p>
<a href="http://arxiv.org/abs/2010.05397" target="_blank">arXiv:2010.05397</a> [<a href="http://arxiv.org/pdf/2010.05397" target="_blank">pdf</a>]

<h2>TUTOR: Training Neural Networks Using Decision Rules as Model Priors. (arXiv:2010.05429v2 [cs.NE] UPDATED)</h2>
<h3>Shayan Hassantabar, Prerit Terway, Niraj K. Jha</h3>
<p>The human brain has the ability to carry out new tasks with limited
experience. It utilizes prior learning experiences to adapt the solution
strategy to new domains. On the other hand, deep neural networks (DNNs)
generally need large amounts of data and computational resources for training.
However, this requirement is not met in many settings. To address these
challenges, we propose the TUTOR DNN synthesis framework. TUTOR targets
non-image datasets. It synthesizes accurate DNN models with limited available
data, and reduced memory and computational requirements. It consists of three
sequential steps: (1) drawing synthetic data from the same probability
distribution as the training data and labeling the synthetic data based on a
set of rules extracted from the real dataset, (2) use of two training schemes
that combine synthetic data and training data to learn DNN weights, and (3)
employing a grow-and-prune synthesis paradigm to learn both the weights and the
architecture of the DNN to reduce model size while ensuring its accuracy. We
show that in comparison with fully-connected DNNs, on an average TUTOR reduces
the need for data by 6.0x (geometric mean), improves accuracy by 3.6%, and
reduces the number of parameters (floating-point operations) by 4.7x (4.3x)
(geometric mean). Thus, TUTOR is a less data-hungry, accurate, and efficient
DNN synthesis framework.
</p>
<a href="http://arxiv.org/abs/2010.05429" target="_blank">arXiv:2010.05429</a> [<a href="http://arxiv.org/pdf/2010.05429" target="_blank">pdf</a>]

<h2>A Lightweight Speaker Recognition System Using Timbre Properties. (arXiv:2010.05502v2 [cs.SD] UPDATED)</h2>
<h3>Abu Quwsar Ohi, M. F. Mridha, Md. Abdul Hamid, Muhammad Mostafa Monowar, Dongsu Lee, Jinsul Kim</h3>
<p>Speaker recognition is an active research area that contains notable usage in
biometric security and authentication system. Currently, there exist many
well-performing models in the speaker recognition domain. However, most of the
advanced models implement deep learning that requires GPU support for real-time
speech recognition, and it is not suitable for low-end devices. In this paper,
we propose a lightweight text-independent speaker recognition model based on
random forest classifier. It also introduces new features that are used for
both speaker verification and identification tasks. The proposed model uses
human speech based timbral properties as features that are classified using
random forest. Timbre refers to the very basic properties of sound that allow
listeners to discriminate among them. The prototype uses seven most actively
searched timbre properties, boominess, brightness, depth, hardness, roughness,
sharpness, and warmth as features of our speaker recognition model. The
experiment is carried out on speaker verification and speaker identification
tasks and shows the achievements and drawbacks of the proposed model. In the
speaker identification phase, it achieves a maximum accuracy of 78%. On the
contrary, in the speaker verification phase, the model maintains an accuracy of
80% having an equal error rate (ERR) of 0.24.
</p>
<a href="http://arxiv.org/abs/2010.05502" target="_blank">arXiv:2010.05502</a> [<a href="http://arxiv.org/pdf/2010.05502" target="_blank">pdf</a>]

<h2>FILM: A Fast, Interpretable, and Low-rank Metric Learning Approach for Sentence Matching. (arXiv:2010.05523v2 [cs.CL] UPDATED)</h2>
<h3>Xiangru Tang, Alan Aw</h3>
<p>Detection of semantic similarity plays a vital role in sentence matching. It
requires to learn discriminative representations of natural language. Recently,
owing to more and more sophisticated model architecture, impressive progress
has been made, along with a time-consuming training process and
not-interpretable inference. To alleviate this problem, we explore a metric
learning approach, named FILM (Fast, Interpretable, and Low-rank Metric
learning) to efficiently find a high discriminative projection of the
high-dimensional data. We construct this metric learning problem as a manifold
optimization problem and solve it with the Cayley transformation method with
the Barzilai-Borwein step size. In experiments, we apply FILM with triplet loss
minimization objective to the Quora Challenge and Semantic Textual Similarity
(STS) Task. The results demonstrate that the FILM method achieves superior
performance as well as the fastest computation speed, which is consistent with
our theoretical analysis of time complexity.
</p>
<a href="http://arxiv.org/abs/2010.05523" target="_blank">arXiv:2010.05523</a> [<a href="http://arxiv.org/pdf/2010.05523" target="_blank">pdf</a>]

<h2>Explaining Clinical Decision Support Systems in Medical Imaging using Cycle-Consistent Activation Maximization. (arXiv:2010.05759v2 [eess.IV] UPDATED)</h2>
<h3>Alexander Katzmann, Oliver Taubmann, Stephen Ahmad, Alexander M&#xfc;hlberg, Michael S&#xfc;hling, Horst-Michael Gro&#xdf;</h3>
<p>Clinical decision support using deep neural networks has become a topic of
steadily growing interest. While recent work has repeatedly demonstrated that
deep learning offers major advantages for medical image classification over
traditional methods, clinicians are often hesitant to adopt the technology
because its underlying decision-making process is considered to be
intransparent and difficult to comprehend. In recent years, this has been
addressed by a variety of approaches that have successfully contributed to
providing deeper insight. Most notably, additive feature attribution methods
are able to propagate decisions back into the input space by creating a
saliency map which allows the practitioner to "see what the network sees."
However, the quality of the generated maps can become poor and the images noisy
if only limited data is available - a typical scenario in clinical contexts. We
propose a novel decision explanation scheme based on CycleGAN activation
maximization which generates high-quality visualizations of classifier
decisions even in smaller data sets. We conducted a user study in which these
visualizations significantly outperformed existing methods on the LIDC dataset
for lung lesion malignancy classification. With our approach we make a
significant contribution to a better understanding of clinical decision support
systems based on deep neural networks and thus aim to foster overall clinical
acceptance.
</p>
<a href="http://arxiv.org/abs/2010.05759" target="_blank">arXiv:2010.05759</a> [<a href="http://arxiv.org/pdf/2010.05759" target="_blank">pdf</a>]

<h2>Learning Pugachev's Cobra Maneuver for Tail-sitter UAVs Using Acceleration Model. (arXiv:1906.02596v2 [eess.SY] CROSS LISTED)</h2>
<h3>Wei Xu, Fu Zhang</h3>
<p>The Pugachev's cobra maneuver is a dramatic and demanding maneuver requiring
the aircraft to fly at extremely high Angle of Attacks (AOA) where stalling
occurs. This paper considers this maneuver on tail-sitter UAVs. We present a
simple yet very effective feedback-iterative learning position control
structure to regulate the altitude error and lateral displacement during the
maneuver. Both the feedback controller and the iterative learning controller
are based on the aircraft acceleration model, which is directly measurable by
the onboard accelerometer. Moreover, the acceleration model leads to an
extremely simple dynamic model that does not require any model identification
in designing the position controller, greatly simplifying the implementation of
the iterative learning control. Real-world outdoor flight experiments on the
"Hong Hu" UAV, an aerobatic yet efficient quadrotor tail-sitter UAV of
small-size, are provided to show the effectiveness of the proposed controller.
</p>
<a href="http://arxiv.org/abs/1906.02596" target="_blank">arXiv:1906.02596</a> [<a href="http://arxiv.org/pdf/1906.02596" target="_blank">pdf</a>]

<h2>Unfolding recurrence by Green's functions for optimized reservoir computing. (arXiv:2010.06247v1 [cond-mat.dis-nn])</h2>
<h3>Sandra Nestler, Christian Keup, David Dahmen, Matthieu Gilson, Holger Rauhut, Moritz Helias</h3>
<p>Cortical networks are strongly recurrent, and neurons have intrinsic temporal
dynamics. This sets them apart from deep feed-forward networks. Despite the
tremendous progress in the application of feed-forward networks and their
theoretical understanding, it remains unclear how the interplay of recurrence
and non-linearities in recurrent cortical networks contributes to their
function. The purpose of this work is to present a solvable recurrent network
model that links to feed forward networks. By perturbative methods we transform
the time-continuous, recurrent dynamics into an effective feed-forward
structure of linear and non-linear temporal kernels. The resulting analytical
expressions allow us to build optimal time-series classifiers from random
reservoir networks. Firstly, this allows us to optimize not only the readout
vectors, but also the input projection, demonstrating a strong potential
performance gain. Secondly, the analysis exposes how the second order stimulus
statistics is a crucial element that interacts with the non-linearity of the
dynamics and boosts performance.
</p>
<a href="http://arxiv.org/abs/2010.06247" target="_blank">arXiv:2010.06247</a> [<a href="http://arxiv.org/pdf/2010.06247" target="_blank">pdf</a>]

<h2>Simulation-based inference methods for particle physics. (arXiv:2010.06439v1 [hep-ph])</h2>
<h3>Johann Brehmer, Kyle Cranmer</h3>
<p>Our predictions for particle physics processes are realized in a chain of
complex simulators. They allow us to generate high-fidelty simulated data, but
they are not well-suited for inference on the theory parameters with observed
data. We explain why the likelihood function of high-dimensional LHC data
cannot be explicitly evaluated, why this matters for data analysis, and reframe
what the field has traditionally done to circumvent this problem. We then
review new simulation-based inference methods that let us directly analyze
high-dimensional data by combining machine learning techniques and information
from the simulator. Initial studies indicate that these techniques have the
potential to substantially improve the precision of LHC measurements. Finally,
we discuss probabilistic programming, an emerging paradigm that lets us extend
inference to the latent process of the simulator.
</p>
<a href="http://arxiv.org/abs/2010.06439" target="_blank">arXiv:2010.06439</a> [<a href="http://arxiv.org/pdf/2010.06439" target="_blank">pdf</a>]

<h2>Interpretable pathological test for Cardio-vascular disease: Approximate Bayesian computation with distance learning. (arXiv:2010.06465v1 [stat.ME])</h2>
<h3>Ritabrata Dutta, Karim Zouaoui-Boudjeltia, Christos Kotsalos, Alexandre Rousseau, Daniel Ribeiro de Sousa, Jean-Marc Desmet, Alain Van Meerhaeghe, Antonietta Mira, Bastien Chopard</h3>
<p>Cardio/cerebrovascular diseases (CVD) have become one of the major health
issue in our societies. But recent studies show that the present clinical tests
to detect CVD are ineffectual as they do not consider different stages of
platelet activation or the molecular dynamics involved in platelet interactions
and are incapable to consider inter-individual variability. Here we propose a
stochastic platelet deposition model and an inferential scheme for uncertainty
quantification of these parameters using Approximate Bayesian Computation and
distance learning. Finally we show that our methodology can learn biologically
meaningful parameters, which are the specific dysfunctioning parameters in each
type of patients, from data collected from healthy volunteers and patients.
This work opens up an unprecedented opportunity of personalized pathological
test for CVD detection and medical treatment. Also our proposed methodology can
be used to other fields of science where we would need machine learning tools
to be interpretable.
</p>
<a href="http://arxiv.org/abs/2010.06465" target="_blank">arXiv:2010.06465</a> [<a href="http://arxiv.org/pdf/2010.06465" target="_blank">pdf</a>]

<h2>Batch-sequential design and heteroskedastic surrogate modeling for delta smelt conservation. (arXiv:2010.06515v1 [stat.AP])</h2>
<h3>Boya Zhang, Robert B. Gramacy, Leah Johnson, Kenneth A. Rose, Eric Smith</h3>
<p>Delta smelt is an endangered fish species in the San Francisco estuary that
have shown an overall population decline over the past 30 years. Researchers
have developed a stochastic, agent-based simulator to virtualize the system,
with the goal of understanding the relative contribution of natural and
anthropogenic factors suggested as playing a role in their decline. However,
the input configuration space is high-dimensional, running the simulator is
time-consuming, and its noisy outputs change nonlinearly in both mean and
variance. Getting enough runs to effectively learn input--output dynamics
requires both a nimble modeling strategy and parallel supercomputer evaluation.
Recent advances in heteroskedastic Gaussian process (HetGP) surrogate modeling
helps, but little is known about how to appropriately plan experiments for
highly distributed simulator evaluation. We propose a batch sequential design
scheme, generalizing one-at-a-time variance-based active learning for HetGP
surrogates, as a means of keeping multi-core cluster nodes fully engaged with
expensive runs. Our acquisition strategy is carefully engineered to favor
selection of replicates which boost statistical and computational efficiencies
when training surrogates to isolate signal in high noise regions. Design and
modeling performance is illustrated on a range of toy examples before embarking
on a large-scale smelt simulation campaign and downstream high-fidelity input
sensitivity analysis.
</p>
<a href="http://arxiv.org/abs/2010.06515" target="_blank">arXiv:2010.06515</a> [<a href="http://arxiv.org/pdf/2010.06515" target="_blank">pdf</a>]

<h2>Geometry of learning neural quantum states. (arXiv:1910.11163v2 [quant-ph] UPDATED)</h2>
<h3>Chae-Yeun Park, Michael J. Kastoryano</h3>
<p>Combining insights from machine learning and quantum Monte Carlo, the
stochastic reconfiguration method with neural network Ansatz states is a
promising new direction for high-precision ground state estimation of quantum
many-body problems. Even though this method works well in practice, little is
known about the learning dynamics. In this paper, we bring to light several
hidden details of the algorithm by analyzing the learning landscape. In
particular, the spectrum of the quantum Fisher matrix of complex restricted
Boltzmann machine states exhibits a universal initial dynamics, but the
converged spectrum can dramatically change across a phase transition. In
contrast to the spectral properties of the quantum Fisher matrix, the actual
weights of the network at convergence do not reveal much information about the
system or the dynamics. Furthermore, we identify a new measure of correlation
in the state by analyzing entanglement in eigenvectors. We show that,
generically, the learning landscape modes with least entanglement have largest
eigenvalue, suggesting that correlations are encoded in large flat valleys of
the learning landscape, favoring stable representations of the ground state.
</p>
<a href="http://arxiv.org/abs/1910.11163" target="_blank">arXiv:1910.11163</a> [<a href="http://arxiv.org/pdf/1910.11163" target="_blank">pdf</a>]

<h2>The Risks of Invariant Risk Minimization. (arXiv:2010.05761v1 [cs.LG] CROSS LISTED)</h2>
<h3>Elan Rosenfeld, Pradeep Ravikumar, Andrej Risteski</h3>
<p>Invariant Causal Prediction (Peters et al., 2016) is a technique for
out-of-distribution generalization which assumes that some aspects of the data
distribution vary across the training set but that the underlying causal
mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant
Risk Minimization (IRM), an objective based on this idea for learning deep,
invariant features of data which are a complex function of latent variables;
many alternatives have subsequently been suggested. However, formal guarantees
for all of these works are severely lacking. In this paper, we present the
first analysis of classification under the IRM objective$-$as well as these
recently proposed alternatives$-$under a fairly natural and general model. In
the linear case, we show simple conditions under which the optimal solution
succeeds or, more often, fails to recover the optimal invariant predictor. We
furthermore present the very first results in the non-linear regime: we
demonstrate that IRM can fail catastrophically unless the test data are
sufficiently similar to the training distribution$-$this is precisely the issue
that it was intended to solve. Thus, in this setting we find that IRM and its
alternatives fundamentally do not improve over standard Empirical Risk
Minimization.
</p>
<a href="http://arxiv.org/abs/2010.05761" target="_blank">arXiv:2010.05761</a> [<a href="http://arxiv.org/pdf/2010.05761" target="_blank">pdf</a>]

