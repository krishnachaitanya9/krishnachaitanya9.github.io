---
title: Latest Deep Learning Papers
date: 2021-03-05 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (107 Articles)</h1>
<h2>Deep Clustering by Semantic Contrastive Learning. (arXiv:2103.02662v1 [cs.CV])</h2>
<h3>Jiabo Huang, Shaogang Gong</h3>
<p>Whilst contrastive learning has achieved remarkable success in
self-supervised representation learning, its potential for deep clustering
remains unknown. This is due to its fundamental limitation that the instance
discrimination strategy it takes is not class sensitive and hence unable to
reason about the underlying decision boundaries between semantic concepts or
classes. In this work, we solve this problem by introducing a novel variant
called Semantic Contrastive Learning (SCL). It explores the characteristics of
both conventional contrastive learning and deep clustering by imposing
distance-based cluster structures on unlabelled training data and also
introducing a discriminative contrastive loss formulation. For explicitly
modelling class boundaries on-the-fly, we further formulate a clustering
consistency condition on the two different predictions given by visual
similarities and semantic decision boundaries. By advancing implicit
representation learning towards explicit understandings of visual semantics,
SCL can amplify jointly the strengths of contrastive learning and deep
clustering in a unified approach. Extensive experiments show that the proposed
model outperforms the state-of-the-art deep clustering methods on six
challenging object recognition benchmarks, especially on finer-grained and
larger datasets.
</p>
<a href="http://arxiv.org/abs/2103.02662" target="_blank">arXiv:2103.02662</a> [<a href="http://arxiv.org/pdf/2103.02662" target="_blank">pdf</a>]

<h2>A comprehensive survey on point cloud registration. (arXiv:2103.02690v1 [cs.CV])</h2>
<h3>Xiaoshui Huang, Guofeng Mei, Jian Zhang, Rana Abbas</h3>
<p>Registration is a problem of transformation estimation between two point
clouds, which has experienced a long history of development from an
optimization aspect. The recent success of deep learning has vastly improved
registration robustness and efficiency. This survey tries to conduct a
comprehensive review and build the connection between optimization-based
methods and deep learning methods, to provide further research insight.
Moreover, with the recent development of 3D sensors and 3D reconstruction
techniques, a new research direction also emerges to align cross-source point
clouds. This survey reviews the development of cross-source point cloud
registration and builds a new benchmark to evaluate the state-of-the-art
registration algorithms. Besides, this survey summarizes the benchmark data
sets and discusses point cloud registration applications across various
domains. Finally, this survey proposes potential research directions in this
rapidly growing field.
</p>
<a href="http://arxiv.org/abs/2103.02690" target="_blank">arXiv:2103.02690</a> [<a href="http://arxiv.org/pdf/2103.02690" target="_blank">pdf</a>]

<h2>Policy Decomposition: Approximate Optimal Control with Suboptimality Estimates. (arXiv:2103.02716v1 [cs.RO])</h2>
<h3>Ashwin Khadke, Hartmut Geyer</h3>
<p>Numerically computing global policies to optimal control problems for complex
dynamical systems is mostly intractable. In consequence, a number of
approximation methods have been developed. However, none of the current methods
can quantify by how much the resulting control underperforms the elusive
globally optimal solution. Here we propose policy decomposition, an
approximation method with explicit suboptimality estimates. Our method
decomposes the optimal control problem into lower-dimensional subproblems,
whose optimal solutions are recombined to build a control policy for the entire
system. Many such combinations exist, and we introduce the value error and its
LQR and DDP estimates to predict the suboptimality of possible combinations and
prioritize the ones that minimize it. Using a cart-pole, a 3-link balancing
biped and N-link planar manipulators as example systems, we find that the
estimates correctly identify the best combinations, yielding control policies
in a fraction of the time it takes to compute the optimal control without a
notable sacrifice in closed-loop performance. While more research will be
needed to find ways of dealing with the combinatorics of policy decomposition,
the results suggest this method could be an effective alternative for
approximating optimal control in intractable systems.
</p>
<a href="http://arxiv.org/abs/2103.02716" target="_blank">arXiv:2103.02716</a> [<a href="http://arxiv.org/pdf/2103.02716" target="_blank">pdf</a>]

<h2>Preference-based Learning of Reward Function Features. (arXiv:2103.02727v1 [cs.RO])</h2>
<h3>Sydney M. Katz, Amir Maleki, Erdem B&#x131;y&#x131;k, Mykel J. Kochenderfer</h3>
<p>Preference-based learning of reward functions, where the reward function is
learned using comparison data, has been well studied for complex robotic tasks
such as autonomous driving. Existing algorithms have focused on learning reward
functions that are linear in a set of trajectory features. The features are
typically hand-coded, and preference-based learning is used to determine a
particular user's relative weighting for each feature. Designing a
representative set of features to encode reward is challenging and can result
in inaccurate models that fail to model the users' preferences or perform the
task properly. In this paper, we present a method to learn both the relative
weighting among features as well as additional features that help encode a
user's reward function. The additional features are modeled as a neural network
that is trained on the data from pairwise comparison queries. We apply our
methods to a driving scenario used in previous work and compare the predictive
power of our method to that of only hand-coded features. We perform additional
analysis to interpret the learned features and examine the optimal
trajectories. Our results show that adding an additional learned feature to the
reward model enhances both its predictive power and expressiveness, producing
unique results for each user.
</p>
<a href="http://arxiv.org/abs/2103.02727" target="_blank">arXiv:2103.02727</a> [<a href="http://arxiv.org/pdf/2103.02727" target="_blank">pdf</a>]

<h2>Resilient Active Information Acquisition with Teams of Robots. (arXiv:2103.02733v1 [cs.RO])</h2>
<h3>Brent Schlotfeldt, Vasileios Tzoumas, George J. Pappas</h3>
<p>Emerging applications of collaborative autonomy, such as Multi-Target
Tracking, Unknown Map Exploration, and Persistent Surveillance, require robots
plan paths to navigate an environment while maximizing the information
collected via on-board sensors. In this paper, we consider such information
acquisition tasks but in adversarial environments, where attacks may
temporarily disable the robots' sensors. We propose the first receding horizon
algorithm, aiming for robust and adaptive multi-robot planning against any
number of attacks, which we call Resilient Active Information acquisitioN
(RAIN). RAIN calls, in an online fashion, a Robust Trajectory Planning (RTP)
subroutine which plans attack-robust control inputs over a look-ahead planning
horizon. We quantify RTP's performance by bounding its suboptimality. We base
our theoretical analysis on notions of curvature introduced in combinatorial
optimization. We evaluate RAIN in three information acquisition scenarios:
Multi-Target Tracking, Occupancy Grid Mapping, and Persistent Surveillance. The
scenarios are simulated in C++ and a Unity-based simulator. In all simulations,
RAIN runs in real-time, and exhibits superior performance against a
state-of-the-art baseline information acquisition algorithm, even in the
presence of a high number of attacks. We also demonstrate RAIN's robustness and
effectiveness against varying models of attacks (worst-case and random), as
well as, varying replanning rates.
</p>
<a href="http://arxiv.org/abs/2103.02733" target="_blank">arXiv:2103.02733</a> [<a href="http://arxiv.org/pdf/2103.02733" target="_blank">pdf</a>]

<h2>Efficient data-driven encoding of scene motion using Eccentricity. (arXiv:2103.02743v1 [cs.CV])</h2>
<h3>Bruno Costa, Enrique Corona, Mostafa Parchami, Gint Puskorius, Dimitar Filev</h3>
<p>This paper presents a novel approach of representing dynamic visual scenes
with static maps generated from video/image streams. Such representation allows
easy visual assessment of motion in dynamic environments. These maps are 2D
matrices calculated recursively, in a pixel-wise manner, that is based on the
recently introduced concept of Eccentricity data analysis. Eccentricity works
as a metric of a discrepancy between a particular pixel of an image and its
normality model, calculated in terms of mean and variance of past readings of
the same spatial region of the image. While Eccentricity maps carry temporal
information about the scene, actual images do not need to be stored nor
processed in batches. Rather, all the calculations are done recursively, based
on a small amount of statistical information stored in memory, thus resulting
in a very computationally efficient (processor- and memory-wise) method. The
list of potential applications includes video-based activity recognition,
intent recognition, object tracking, video description, and so on.
</p>
<a href="http://arxiv.org/abs/2103.02743" target="_blank">arXiv:2103.02743</a> [<a href="http://arxiv.org/pdf/2103.02743" target="_blank">pdf</a>]

<h2>Multiple-Channel Real Time Filtering for a Myoelectric Prosthetic Hand-Arm Robot System. (arXiv:2103.02750v1 [cs.RO])</h2>
<h3>Weibang Bai, Yinlai Jiang, Hiroshi Yokoi</h3>
<p>On the base of the developed master-slave prosthetic hand-arm robot system,
which is controlled mainly based on signals obtained from bending sensors fixed
on the data glove, the first idea deduced was to develop and add a
multi-dimensional filter into the original control system to make the control
signals cleaner and more stable at real time. By going further, a second new
idea was also proposed to predict new control information based on the
combination of a new algorithm and prediction control theory. In order to
fulfill the first idea properly, the possible methods to process data in real
time, the different ways to produce Gaussian distributed random data, the way
to combine the new algorithm with the previous complex program project, and the
way to simplify and reduce the running time of the algorithm to maintain the
high efficiency, the real time processing with multiple channels of the sensory
system and the real-time performance of the control system were researched.
Eventually, the experiment on the same provided robot system gives the results
of the first idea and shows the improved performance of the filter comparing
with the original control method.
</p>
<a href="http://arxiv.org/abs/2103.02750" target="_blank">arXiv:2103.02750</a> [<a href="http://arxiv.org/pdf/2103.02750" target="_blank">pdf</a>]

<h2>A toolbox for neuromorphic sensing in robotics. (arXiv:2103.02751v1 [cs.RO])</h2>
<h3>Julien Dupeyroux</h3>
<p>The third generation of artificial intelligence (AI) introduced by
neuromorphic computing is revolutionizing the way robots and autonomous systems
can sense the world, process the information, and interact with their
environment. The promises of high flexibility, energy efficiency, and
robustness of neuromorphic systems is widely supported by software tools for
simulating spiking neural networks, and hardware integration (neuromorphic
processors). Yet, while efforts have been made on neuromorphic vision
(event-based cameras), it is worth noting that most of the sensors available
for robotics remain inherently incompatible with neuromorphic computing, where
information is encoded into spikes. To facilitate the use of traditional
sensors, we need to convert the output signals into streams of spikes, i.e., a
series of events (+1, -1) along with their corresponding timestamps. In this
paper, we propose a review of the coding algorithms from a robotics perspective
and further supported by a benchmark to assess their performance. We also
introduce a ROS (Robot Operating System) toolbox to encode and decode input
signals coming from any type of sensor available on a robot. This initiative is
meant to stimulate and facilitate robotic integration of neuromorphic AI, with
the opportunity to adapt traditional off-the-shelf sensors to spiking neural
nets within one of the most powerful robotic tools, ROS.
</p>
<a href="http://arxiv.org/abs/2103.02751" target="_blank">arXiv:2103.02751</a> [<a href="http://arxiv.org/pdf/2103.02751" target="_blank">pdf</a>]

<h2>Learning Asynchronous and Sparse Human-Object Interaction in Videos. (arXiv:2103.02758v1 [cs.CV])</h2>
<h3>Romero Morais, Vuong Le, Svetha Venkatesh, Truyen Tran</h3>
<p>Human activities can be learned from video. With effective modeling it is
possible to discover not only the action labels but also the temporal
structures of the activities such as the progression of the sub-activities.
Automatically recognizing such structure from raw video signal is a new
capability that promises authentic modeling and successful recognition of
human-object interactions. Toward this goal, we introduce Asynchronous-Sparse
Interaction Graph Networks (ASSIGN), a recurrent graph network that is able to
automatically detect the structure of interaction events associated with
entities in a video scene. ASSIGN pioneers learning of autonomous behavior of
video entities including their dynamic structure and their interaction with the
coexisting neighbors. Entities' lives in our model are asynchronous to those of
others therefore more flexible in adaptation to complex scenarios. Their
interactions are sparse in time hence more faithful to the true underlying
nature and more robust in inference and learning. ASSIGN is tested on
human-object interaction recognition and shows superior performance in
segmenting and labeling of human sub-activities and object affordances from raw
videos. The native ability for discovering temporal structures of the model
also eliminates the dependence on external segmentation that was previously
mandatory for this task.
</p>
<a href="http://arxiv.org/abs/2103.02758" target="_blank">arXiv:2103.02758</a> [<a href="http://arxiv.org/pdf/2103.02758" target="_blank">pdf</a>]

<h2>Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions. (arXiv:2103.02760v1 [cs.RO])</h2>
<h3>Ivan Fursa, Elias Fandi, Valentina Musat, Jacob Culley, Enric Gil, Louise Bilous, Isaac Vander Sluis, Alexander Rast, Andrew Bradley</h3>
<p>Autonomous vehicles rely heavily upon their perception subsystems to see the
environment in which they operate. Unfortunately, the effect of varying weather
conditions presents a significant challenge to object detection algorithms, and
thus it is imperative to test the vehicle extensively in all conditions which
it may experience. However, unpredictable weather can make real-world testing
in adverse conditions an expensive and time consuming task requiring access to
specialist facilities, and weatherproofing of sensitive electronics. Simulation
provides an alternative to real world testing, with some studies developing
increasingly visually realistic representations of the real world on powerful
compute hardware. Given that subsequent subsystems in the autonomous vehicle
pipeline are unaware of the visual realism of the simulation, when developing
modules downstream of perception the appearance is of little consequence -
rather it is how the perception system performs in the prevailing weather
condition that is important. This study explores the potential of using a
simple, lightweight image augmentation system in an autonomous racing vehicle -
focusing not on visual accuracy, but rather the effect upon perception system
performance. With minimal adjustment, the prototype system developed in this
study can replicate the effects of both water droplets on the camera lens, and
fading light conditions. The system introduces a latency of less than 8 ms
using compute hardware that is well suited to being carried in the vehicle -
rendering it ideally suited to real-time implementation that can be run during
experiments in simulation, and augmented reality testing in the real world.
</p>
<a href="http://arxiv.org/abs/2103.02760" target="_blank">arXiv:2103.02760</a> [<a href="http://arxiv.org/pdf/2103.02760" target="_blank">pdf</a>]

<h2>PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds. (arXiv:2103.02766v1 [cs.CV])</h2>
<h3>Yujia Liu, Stefano D&#x27;Aronco, Konrad Schindler, Jan Dirk Wegner</h3>
<p>We introduce PC2WF, the first end-to-end trainable deep network architecture
to convert a 3D point cloud into a wireframe model. The network takes as input
an unordered set of 3D points sampled from the surface of some object, and
outputs a wireframe of that object, i.e., a sparse set of corner points linked
by line segments. Recovering the wireframe is a challenging task, where the
numbers of both vertices and edges are different for every instance, and
a-priori unknown. Our architecture gradually builds up the model: It starts by
encoding the points into feature vectors. Based on those features, it
identifies a pool of candidate vertices, then prunes those candidates to a
final set of corner vertices and refines their locations. Next, the corners are
linked with an exhaustive set of candidate edges, which is again pruned to
obtain the final wireframe. All steps are trainable, and errors can be
backpropagated through the entire sequence. We validate the proposed model on a
publicly available synthetic dataset, for which the ground truth wireframes are
accessible, as well as on a new real-world dataset. Our model produces
wireframe abstractions of good quality and outperforms several baselines.
</p>
<a href="http://arxiv.org/abs/2103.02766" target="_blank">arXiv:2103.02766</a> [<a href="http://arxiv.org/pdf/2103.02766" target="_blank">pdf</a>]

<h2>SVMax: A Feature Embedding Regularizer. (arXiv:2103.02770v1 [cs.CV])</h2>
<h3>Ahmed Taha, Alex Hanson, Abhinav Shrivastava, Larry Davis</h3>
<p>A neural network regularizer (e.g., weight decay) boosts performance by
explicitly penalizing the complexity of a network. In this paper, we penalize
inferior network activations -- feature embeddings -- which in turn regularize
the network's weights implicitly. We propose singular value maximization
(SVMax) to learn a more uniform feature embedding. The SVMax regularizer
supports both supervised and unsupervised learning. Our formulation mitigates
model collapse and enables larger learning rates. We evaluate the SVMax
regularizer using both retrieval and generative adversarial networks. We
leverage a synthetic mixture of Gaussians dataset to evaluate SVMax in an
unsupervised setting. For retrieval networks, SVMax achieves significant
improvement margins across various ranking losses. Code available at
https://bit.ly/3jNkgDt
</p>
<a href="http://arxiv.org/abs/2103.02770" target="_blank">arXiv:2103.02770</a> [<a href="http://arxiv.org/pdf/2103.02770" target="_blank">pdf</a>]

<h2>DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images. (arXiv:2103.02772v1 [cs.CV])</h2>
<h3>Meng Ye, Mikael Kanski, Dong Yang, Qi Chang, Zhennan Yan, Qiaoying Huang, Leon Axel, Dimitris Metaxas</h3>
<p>Cardiac tagging magnetic resonance imaging (t-MRI) is the gold standard for
regional myocardium deformation and cardiac strain estimation. However, this
technique has not been widely used in clinical diagnosis, as a result of the
difficulty of motion tracking encountered with t-MRI images. In this paper, we
propose a novel deep learning-based fully unsupervised method for in vivo
motion tracking on t-MRI images. We first estimate the motion field (INF)
between any two consecutive t-MRI frames by a bi-directional generative
diffeomorphic registration neural network. Using this result, we then estimate
the Lagrangian motion field between the reference frame and any other frame
through a differentiable composition layer. By utilizing temporal information
to perform reasonable estimations on spatio-temporal motion fields, this novel
method provides a useful solution for motion tracking and image registration in
dynamic medical imaging. Our method has been validated on a representative
clinical t-MRI dataset; the experimental results show that our method is
superior to conventional motion tracking methods in terms of landmark tracking
accuracy and inference efficiency.
</p>
<a href="http://arxiv.org/abs/2103.02772" target="_blank">arXiv:2103.02772</a> [<a href="http://arxiv.org/pdf/2103.02772" target="_blank">pdf</a>]

<h2>Structure-Preserving Progressive Low-rank Image Completion for Defending Adversarial Attacks. (arXiv:2103.02781v1 [cs.CV])</h2>
<h3>Zhiqun Zhao, Hengyou Wang, Hao Sun, Zhihai He</h3>
<p>Deep neural networks recognize objects by analyzing local image details and
summarizing their information along the inference layers to derive the final
decision. Because of this, they are prone to adversarial attacks. Small
sophisticated noise in the input images can accumulate along the network
inference path and produce wrong decisions at the network output. On the other
hand, human eyes recognize objects based on their global structure and semantic
cues, instead of local image textures. Because of this, human eyes can still
clearly recognize objects from images which have been heavily damaged by
adversarial attacks. This leads to a very interesting approach for defending
deep neural networks against adversarial attacks. In this work, we propose to
develop a structure-preserving progressive low-rank image completion (SPLIC)
method to remove unneeded texture details from the input images and shift the
bias of deep neural networks towards global object structures and semantic
cues. We formulate the problem into a low-rank matrix completion problem with
progressively smoothed rank functions to avoid local minimums during the
optimization process. Our experimental results demonstrate that the proposed
method is able to successfully remove the insignificant local image details
while preserving important global object structures. On black-box, gray-box,
and white-box attacks, our method outperforms existing defense methods (by up
to 12.6%) and significantly improves the adversarial robustness of the network.
</p>
<a href="http://arxiv.org/abs/2103.02781" target="_blank">arXiv:2103.02781</a> [<a href="http://arxiv.org/pdf/2103.02781" target="_blank">pdf</a>]

<h2>Feature Boosting, Suppression, and Diversification for Fine-Grained Visual Classification. (arXiv:2103.02782v1 [cs.CV])</h2>
<h3>Jianwei Song, Ruoyu Yang</h3>
<p>Learning feature representation from discriminative local regions plays a key
role in fine-grained visual classification. Employing attention mechanisms to
extract part features has become a trend. However, there are two major
limitations in these methods: First, they often focus on the most salient part
while neglecting other inconspicuous but distinguishable parts. Second, they
treat different part features in isolation while neglecting their
relationships. To handle these limitations, we propose to locate multiple
different distinguishable parts and explore their relationships in an explicit
way. In this pursuit, we introduce two lightweight modules that can be easily
plugged into existing convolutional neural networks. On one hand, we introduce
a feature boosting and suppression module that boosts the most salient part of
feature maps to obtain a part-specific representation and suppresses it to
force the following network to mine other potential parts. On the other hand,
we introduce a feature diversification module that learns semantically
complementary information from the correlated part-specific representations.
Our method does not need bounding boxes/part annotations and can be trained
end-to-end. Extensive experimental results show that our method achieves
state-of-the-art performances on several benchmark fine-grained datasets.
</p>
<a href="http://arxiv.org/abs/2103.02782" target="_blank">arXiv:2103.02782</a> [<a href="http://arxiv.org/pdf/2103.02782" target="_blank">pdf</a>]

<h2>Learning Granularity-Aware Convolutional Neural Network for Fine-Grained Visual Classification. (arXiv:2103.02788v1 [cs.CV])</h2>
<h3>Jianwei Song, Ruoyu Yang</h3>
<p>Locating discriminative parts plays a key role in fine-grained visual
classification due to the high similarities between different objects. Recent
works based on convolutional neural networks utilize the feature maps taken
from the last convolutional layer to mine discriminative regions. However, the
last convolutional layer tends to focus on the whole object due to the large
receptive field, which leads to a reduced ability to spot the differences. To
address this issue, we propose a novel Granularity-Aware Convolutional Neural
Network (GA-CNN) that progressively explores discriminative features.
Specifically, GA-CNN utilizes the differences of the receptive fields at
different layers to learn multi-granularity features, and it exploits larger
granularity information based on the smaller granularity information found at
the previous stages. To further boost the performance, we introduce an
object-attentive module that can effectively localize the object given a raw
image. GA-CNN does not need bounding boxes/part annotations and can be trained
end-to-end. Extensive experimental results show that our approach achieves
state-of-the-art performances on three benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2103.02788" target="_blank">arXiv:2103.02788</a> [<a href="http://arxiv.org/pdf/2103.02788" target="_blank">pdf</a>]

<h2>Learning the Next Best View for 3D Point Clouds via Topological Features. (arXiv:2103.02789v1 [cs.RO])</h2>
<h3>Christopher Collander, William J. Beksi, Manfred Huber</h3>
<p>In this paper, we introduce a reinforcement learning approach utilizing a
novel topology-based information gain metric for directing the next best view
of a noisy 3D sensor. The metric combines the disjoint sections of an observed
surface to focus on high-detail features such as holes and concave sections.
Experimental results show that our approach can aid in establishing the
placement of a robotic sensor to optimize the information provided by its
streaming point cloud data. Furthermore, a labeled dataset of 3D objects, a CAD
design for a custom robotic manipulator, and software for the transformation,
union, and registration of point clouds has been publicly released to the
research community.
</p>
<a href="http://arxiv.org/abs/2103.02789" target="_blank">arXiv:2103.02789</a> [<a href="http://arxiv.org/pdf/2103.02789" target="_blank">pdf</a>]

<h2>When Face Recognition Meets Occlusion: A New Benchmark. (arXiv:2103.02805v1 [cs.CV])</h2>
<h3>Baojin Huang, Zhongyuan Wang, Guangcheng Wang, Kui Jiang, Kangli Zeng, Zhen Han, Xin Tian, Yuhong Yang</h3>
<p>The existing face recognition datasets usually lack occlusion samples, which
hinders the development of face recognition. Especially during the COVID-19
coronavirus epidemic, wearing a mask has become an effective means of
preventing the virus spread. Traditional CNN-based face recognition models
trained on existing datasets are almost ineffective for heavy occlusion. To
this end, we pioneer a simulated occlusion face recognition dataset. In
particular, we first collect a variety of glasses and masks as occlusion, and
randomly combine the occlusion attributes (occlusion objects, textures,and
colors) to achieve a large number of more realistic occlusion types. We then
cover them in the proper position of the face image with the normal occlusion
habit. Furthermore, we reasonably combine original normal face images and
occluded face images to form our final dataset, termed as Webface-OCC. It
covers 804,704 face images of 10,575 subjects, with diverse occlusion types to
ensure its diversity and stability. Extensive experiments on public datasets
show that the ArcFace retrained by our dataset significantly outperforms the
state-of-the-arts. Webface-OCC is available at
https://github.com/Baojin-Huang/Webface-OCC.
</p>
<a href="http://arxiv.org/abs/2103.02805" target="_blank">arXiv:2103.02805</a> [<a href="http://arxiv.org/pdf/2103.02805" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation for Image Classification via Structure-Conditioned Adversarial Learning. (arXiv:2103.02808v1 [cs.CV])</h2>
<h3>Hui Wang, Jian Tian, Songyuan Li, Hanbin Zhao, Qi Tian, Fei Wu, Xi Li</h3>
<p>Unsupervised domain adaptation (UDA) typically carries out knowledge transfer
from a label-rich source domain to an unlabeled target domain by adversarial
learning. In principle, existing UDA approaches mainly focus on the global
distribution alignment between domains while ignoring the intrinsic local
distribution properties. Motivated by this observation, we propose an
end-to-end structure-conditioned adversarial learning scheme (SCAL) that is
able to preserve the intra-class compactness during domain distribution
alignment. By using local structures as structure-aware conditions, the
proposed scheme is implemented in a structure-conditioned adversarial learning
pipeline. The above learning procedure is iteratively performed by alternating
between local structures establishment and structure-conditioned adversarial
learning. Experimental results demonstrate the effectiveness of the proposed
scheme in UDA scenarios.
</p>
<a href="http://arxiv.org/abs/2103.02808" target="_blank">arXiv:2103.02808</a> [<a href="http://arxiv.org/pdf/2103.02808" target="_blank">pdf</a>]

<h2>MT* : Multi-Robot Path Planning for Temporal Logic Specifications. (arXiv:2103.02821v1 [cs.RO])</h2>
<h3>Dhaval Gujarathi, Indranil Saha</h3>
<p>We address the path planning problem for a team of robots satisfying a
complex high-level mission specification given in the form of an Linear
Temporal Logic (LTL) formula. The state-of-the-art approach to this problem
employs the automata-theoretic model checking technique to solve this problem.
This approach involves computation of a product graph of the Buchi automaton
generated from the LTL specification and a joint transition system which
captures the collective motion of the robots and then computation of the
shortest path using Dijkstra's shortest path algorithm. We propose MT*, an
algorithm that reduces the computation burden for generating such plans for
multi-robot systems significantly. Our approach generates a reduced version of
the product graph without computing the complete joint transition system, which
is computationally expensive. It then divides the complete mission
specification among the participating robots and generates the trajectories for
the individual robots independently. Our approach demonstrates substantial
speedup in terms of computation time over the state-of-the-art approach, and
unlike the state of the art approach, scales well with both the number of
robots and the size of the workspace
</p>
<a href="http://arxiv.org/abs/2103.02821" target="_blank">arXiv:2103.02821</a> [<a href="http://arxiv.org/pdf/2103.02821" target="_blank">pdf</a>]

<h2>STEP: Stochastic Traversability Evaluation and Planning for Safe Off-road Navigation. (arXiv:2103.02828v1 [cs.RO])</h2>
<h3>David D. Fan, Kyohei Otsu, Yuki Kubo, Anushri Dixit, Joel Burdick, Ali-Akbar Agha-Mohammadi</h3>
<p>Although ground robotic autonomy has gained widespread usage in structured
and controlled environments, autonomy in unknown and off-road terrain remains a
difficult problem. Extreme, off-road, and unstructured environments such as
undeveloped wilderness, caves, and rubble pose unique and challenging problems
for autonomous navigation. To tackle these problems we propose an approach for
assessing traversability and planning a safe, feasible, and fast trajectory in
real-time. Our approach, which we name STEP (Stochastic Traversability
Evaluation and Planning), relies on: 1) rapid uncertainty-aware mapping and
traversability evaluation, 2) tail risk assessment using the Conditional
Value-at-Risk (CVaR), and 3) efficient risk and constraint-aware kinodynamic
motion planning using sequential quadratic programming-based (SQP) model
predictive control (MPC). We analyze our method in simulation and validate its
efficacy on wheeled and legged robotic platforms exploring extreme terrains
including an underground lava tube.
</p>
<a href="http://arxiv.org/abs/2103.02828" target="_blank">arXiv:2103.02828</a> [<a href="http://arxiv.org/pdf/2103.02828" target="_blank">pdf</a>]

<h2>Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning. (arXiv:2103.02833v1 [cs.RO])</h2>
<h3>Alexander You, Cindy Grimm, Abhisesh Silwal, Joseph R. Davidson</h3>
<p>Dormant pruning for fresh market fruit trees is a relatively unexplored
application of agricultural robotics for which few end-to-end systems exist.
One of the biggest challenges in creating an autonomous pruning system is the
need to reconstruct a model of a tree which is accurate and informative enough
to be useful for deciding where to cut. One useful structure for modeling a
tree is a skeleton: a 1D, lightweight representation of the geometry and the
topology of a tree. This skeletonization problem is an important one within the
field of computer graphics, and a number of algorithms have been specifically
developed for the task of modeling trees. These skeletonization algorithms have
largely addressed the problem as a geometric one. In agricultural contexts,
however, the parts of the tree have distinct labels, such as the trunk,
supporting branches, etc. This labeled structure is important for understanding
where to prune. We introduce an algorithm which produces such a labeled
skeleton, using the topological and geometric priors associated with these
labels to improve our skeletons. We test our skeletonization algorithm on point
clouds from 29 upright fruiting offshoot (UFO) trees and demonstrate a median
accuracy of 70% with respect to a human-evaluated gold standard. We also make
point cloud scans of 82 UFO trees open-source to other researchers. Our work
represents a significant first step towards a robust tree modeling framework
which can be used in an autonomous pruning system.
</p>
<a href="http://arxiv.org/abs/2103.02833" target="_blank">arXiv:2103.02833</a> [<a href="http://arxiv.org/pdf/2103.02833" target="_blank">pdf</a>]

<h2>A Novel Application of Image-to-Image Translation: Chromosome Straightening Framework by Learning from a Single Image. (arXiv:2103.02835v1 [cs.CV])</h2>
<h3>Sifan Song, Daiyun Huang, Yalun Hu, Chunxiao Yang, Jia Meng, Fei Ma, Jiaming Zhang, Jionglong Su</h3>
<p>In medical imaging, chromosome straightening plays a significant role in the
pathological study of chromosomes and in the development of cytogenetic maps.
Whereas different approaches exist for the straightening task, they are mostly
geometric algorithms whose outputs are characterized by jagged edges or
fragments with discontinued banding patterns. To address the flaws in the
geometric algorithms, we propose a novel framework based on image-to-image
translation to learn a pertinent mapping dependence for synthesizing
straightened chromosomes with uninterrupted banding patterns and preserved
details. In addition, to avoid the pitfall of deficient input chromosomes, we
construct an augmented dataset using only one single curved chromosome image
for training models. Based on this framework, we apply two popular
image-to-image translation architectures, U-shape networks and conditional
generative adversarial networks, to assess its efficacy. Experiments on a
dataset comprising of 642 real-world chromosomes demonstrate the superiority of
our framework as compared to the geometric method in straightening performance
by rendering realistic and continued chromosome details. Furthermore, our
straightened results improve the chromosome classification, achieving
0.98%-1.39% in mean accuracy.
</p>
<a href="http://arxiv.org/abs/2103.02835" target="_blank">arXiv:2103.02835</a> [<a href="http://arxiv.org/pdf/2103.02835" target="_blank">pdf</a>]

<h2>Estimation and Planning of Exploration Over Grid Map Using A Spatiotemporal Model with Incomplete State Observations. (arXiv:2103.02840v1 [cs.RO])</h2>
<h3>Hyung-Jin Yoon, Hunmin Kim, Kripash Shrestha, Naira Hovakimyan, Petros Voulgaris</h3>
<p>Path planning over spatiotemporal models can be applied to a variety of
applications such as UAVs searching for spreading wildfire in mountains or
network of balloons in time-varying atmosphere deployed for inexpensive
internet service. A notable aspect in such applications is the dynamically
changing environment. However, path planning algorithms often assume static
environments and only consider the vehicle's dynamics exploring the
environment. We present a spatiotemporal model that uses a cross-correlation
operator to consider spatiotemporal dependence. Also, we present an adaptive
state estimation for path planning. Since the state estimation depends on the
vehicle's path, the path planning needs to consider the trade-off between
exploration and exploitation. We use a high-level decision-maker to choose an
explorative path or an exploitative path. The overall proposed framework
consists of an adaptive state estimator, a short-term path planner, and a
high-level decision-maker. We tested the framework with a spatiotemporal model
simulation where the state of each grid transits from normal, latent, and fire
state. For the mission objective of visiting the grids with fire, the proposed
framework outperformed the random walk (baseline) and the single-minded
exploitation (or exploration) path.
</p>
<a href="http://arxiv.org/abs/2103.02840" target="_blank">arXiv:2103.02840</a> [<a href="http://arxiv.org/pdf/2103.02840" target="_blank">pdf</a>]

<h2>Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration. (arXiv:2103.02845v1 [cs.CV])</h2>
<h3>Xingyu Chen, Yufeng Liu, Chongyang Ma, Jianlong Chang, Huayan Wang, Tian Chen, Xiaoyan Guo, Pengfei Wan, Wen Zheng</h3>
<p>Recent years have witnessed significant progress in 3D hand mesh recovery.
Nevertheless, because of the intrinsic 2D-to-3D ambiguity, recovering
camera-space 3D information from a single RGB image remains challenging. To
tackle this problem, we divide camera-space mesh recovery into two sub-tasks,
i.e., root-relative mesh recovery and root recovery. First, joint landmarks and
silhouette are extracted from a single input image to provide 2D cues for the
3D tasks. In the root-relative mesh recovery task, we exploit semantic
relations among joints to generate a 3D mesh from the extracted 2D cues. Such
generated 3D mesh coordinates are expressed relative to a root position, i.e.,
wrist of the hand. In the root recovery task, the root position is registered
to the camera space by aligning the generated 3D mesh back to 2D cues, thereby
completing camera-space 3D mesh recovery. Our pipeline is novel in that (1) it
explicitly makes use of known semantic relations among joints and (2) it
exploits 1D projections of the silhouette and mesh to achieve robust
registration. Extensive experiments on popular datasets such as FreiHAND, RHD,
and Human3.6M demonstrate that our approach achieves state-of-the-art
performance on both root-relative mesh recovery and root recovery. Our code is
publicly available at https://github.com/SeanChenxy/HandMesh.
</p>
<a href="http://arxiv.org/abs/2103.02845" target="_blank">arXiv:2103.02845</a> [<a href="http://arxiv.org/pdf/2103.02845" target="_blank">pdf</a>]

<h2>DT*: Temporal Logic Path Planning in a Dynamic Environment. (arXiv:2103.02849v1 [cs.RO])</h2>
<h3>Priya Purohit, Indranil Saha</h3>
<p>Path planning for a robot is one of the major problems in the area of
robotics. When a robot is given a task in the form of a Linear Temporal Logic
(LTL) specification such that the task needs to be carried out repetitively, we
want the robot to follow the shortest cyclic path so that the number of times
the robot completes the mission within a given duration gets maximized. In this
paper, we address the LTL path planning problem in a dynamic environment where
the newly arrived dynamic obstacles may invalidate some of the available paths
at any arbitrary point in time. We present DT*, an SMT-based receding horizon
planning strategy that solves an optimization problem repetitively based on the
current status of the workspace to lead the robot to follow the best available
path in the current situation. We implement our algorithm using the Z3 SMT
solver and evaluate it extensively on an LTL specification capturing a
pick-and-drop application in a warehouse environment. We compare our SMT-based
algorithm with two carefully crafted greedy algorithms. Our experimental
results show that the proposed algorithm can deal with the dynamism in the
workspace in LTL path planning effectively.
</p>
<a href="http://arxiv.org/abs/2103.02849" target="_blank">arXiv:2103.02849</a> [<a href="http://arxiv.org/pdf/2103.02849" target="_blank">pdf</a>]

<h2>Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception. (arXiv:2103.02850v1 [cs.CV])</h2>
<h3>Qi Yang, Siheng Chen, Yiling Xu, Jun Sun, M. Salman Asif, Zhan Ma</h3>
<p>Distortion quantification of point clouds plays a stealth, yet vital role in
a wide range of human and machine perception tasks. For human perception tasks,
a distortion quantification can substitute subjective experiments to guide 3D
visualization; while for machine perception tasks, a distortion quantification
can work as a loss function to guide the training of deep neural networks for
unsupervised learning tasks. To handle a variety of demands in many
applications, a distortion quantification needs to be distortion discriminable,
differentiable, and have a low computational complexity. Currently, however,
there is a lack of a general distortion quantification that can satisfy all
three conditions. To fill this gap, this work proposes multiscale potential
energy discrepancy (MPED), a distortion quantification to measure point cloud
geometry and color difference. By evaluating at various neighborhood sizes, the
proposed MPED achieves global-local tradeoffs, capturing distortion in a
multiscale fashion. Extensive experimental studies validate MPED's superiority
for both human and machine perception tasks.
</p>
<a href="http://arxiv.org/abs/2103.02850" target="_blank">arXiv:2103.02850</a> [<a href="http://arxiv.org/pdf/2103.02850" target="_blank">pdf</a>]

<h2>Data Augmentation for Object Detection via Differentiable Neural Rendering. (arXiv:2103.02852v1 [cs.CV])</h2>
<h3>Guanghan Ning, Guang Chen, Chaowei Tan, Si Luo, Liefeng Bo, Heng Huang</h3>
<p>It is challenging to train a robust object detector when annotated data is
scarce. Existing approaches to tackle this problem include semi-supervised
learning that interpolates labeled data from unlabeled data, self-supervised
learning that exploit signals within unlabeled data via pretext tasks. Without
changing the supervised learning paradigm, we introduce an offline data
augmentation method for object detection, which semantically interpolates the
training data with novel views. Specifically, our proposed system generates
controllable views of training images based on differentiable neural rendering,
together with corresponding bounding box annotations which involve no human
intervention. Firstly, we extract and project pixel-aligned image features into
point clouds while estimating depth maps. We then re-project them with a target
camera pose and render a novel-view 2d image. Objects in the form of keypoints
are marked in point clouds to recover annotations in new views. It is fully
compatible with online data augmentation methods, such as affine transform,
image mixup, etc. Extensive experiments show that our method, as a cost-free
tool to enrich images and labels, can significantly boost the performance of
object detection systems with scarce training data. Code is available at
\url{https://github.com/Guanghan/DANR}.
</p>
<a href="http://arxiv.org/abs/2103.02852" target="_blank">arXiv:2103.02852</a> [<a href="http://arxiv.org/pdf/2103.02852" target="_blank">pdf</a>]

<h2>Morphset:Augmenting categorical emotion datasets with dimensional affect labels using face morphing. (arXiv:2103.02854v1 [cs.CV])</h2>
<h3>Vassilios Vonikakis, Dexter Neo, Stefan Winkler</h3>
<p>Emotion recognition and understanding is a vital componentin human-machine
interaction. Dimensional models of affectsuch as those using valence and
arousal have advantages overtraditional categorical ones due to the complexity
of emo-tional states in humans. However, dimensional emotion an-notations are
difficult and expensive to collect, therefore theyare still limited in the
affective computing community. To ad-dress these issues, we propose a method to
generate syntheticimages from existing categorical emotion datasets using
facemorphing, with full control over the resulting sample distri-bution as well
as dimensional labels in the circumplex space,while achieving augmentation
factors of at least 20x or more.
</p>
<a href="http://arxiv.org/abs/2103.02854" target="_blank">arXiv:2103.02854</a> [<a href="http://arxiv.org/pdf/2103.02854" target="_blank">pdf</a>]

<h2>Mask DnGAN: Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask. (arXiv:2103.02861v1 [cs.CV])</h2>
<h3>Avinash Paliwal, Libing Zeng, Nima Khademi Kalantari</h3>
<p>In this paper, we propose a learning-based approach for denoising raw videos
captured under low lighting conditions. We propose to do this by first
explicitly aligning the neighboring frames to the current frame using a
convolutional neural network (CNN). We then fuse the registered frames using
another CNN to obtain the final denoised frame. To avoid directly aligning the
temporally distant frames, we perform the two processes of alignment and fusion
in multiple stages. Specifically, at each stage, we perform the denoising
process on three consecutive input frames to generate the intermediate denoised
frames which are then passed as the input to the next stage. By performing the
process in multiple stages, we can effectively utilize the information of
neighboring frames without directly aligning the temporally distant frames. We
train our multi-stage system using an adversarial loss with a conditional
discriminator. Specifically, we condition the discriminator on a soft gradient
mask to prevent introducing high-frequency artifacts in smooth regions. We show
that our system is able to produce temporally coherent videos with realistic
details. Furthermore, we demonstrate through extensive experiments that our
approach outperforms state-of-the-art image and video denoising methods both
numerically and visually.
</p>
<a href="http://arxiv.org/abs/2103.02861" target="_blank">arXiv:2103.02861</a> [<a href="http://arxiv.org/pdf/2103.02861" target="_blank">pdf</a>]

<h2>Effective and Fast: A Novel Sequential Single Path Search for Mixed-Precision Quantization. (arXiv:2103.02904v1 [cs.CV])</h2>
<h3>Qigong Sun, Licheng Jiao, Yan Ren, Xiufang Li, Fanhua Shang, Fang Liu</h3>
<p>Since model quantization helps to reduce the model size and computation
latency, it has been successfully applied in many applications of mobile
phones, embedded devices and smart chips. The mixed-precision quantization
model can match different quantization bit-precisions according to the
sensitivity of different layers to achieve great performance. However, it is a
difficult problem to quickly determine the quantization bit-precision of each
layer in deep neural networks according to some constraints (e.g., hardware
resources, energy consumption, model size and computation latency). To address
this issue, we propose a novel sequential single path search (SSPS) method for
mixed-precision quantization,in which the given constraints are introduced into
its loss function to guide searching process. A single path search cell is used
to combine a fully differentiable supernet, which can be optimized by
gradient-based algorithms. Moreover, we sequentially determine the candidate
precisions according to the selection certainties to exponentially reduce the
search space and speed up the convergence of searching process. Experiments
show that our method can efficiently search the mixed-precision models for
different architectures (e.g., ResNet-20, 18, 34, 50 and MobileNet-V2) and
datasets (e.g., CIFAR-10, ImageNet and COCO) under given constraints, and our
experimental results verify that SSPS significantly outperforms their uniform
counterparts.
</p>
<a href="http://arxiv.org/abs/2103.02904" target="_blank">arXiv:2103.02904</a> [<a href="http://arxiv.org/pdf/2103.02904" target="_blank">pdf</a>]

<h2>Humanoid Control Under Interchangeable Fixed and Sliding Unilateral Contacts. (arXiv:2103.02906v1 [cs.RO])</h2>
<h3>Saeid Samadi (LIRMM, IDH), Julien Roux (LIRMM), Arnaud Tanguy, St&#xe9;phane Caron, Abderrahmane Kheddar (LIRMM)</h3>
<p>In this letter, we propose a whole-body control strategy for humanoid robots
in multi-contact settings that enables switching between fixed and sliding
contacts under active balance. We compute, in real-time, a safe center-of-mass
position and wrench distribution of the contact points based on the Chebyshev
center. Our solution is formulated as a quadratic programming problem without a
priori computation of balance regions. We assess our approach with experiments
highlighting switches between fixed and sliding contact modes in multi-contact
configurations. A humanoid robot demonstrates such contact interchanges from
fully-fixed to multi-sliding and also shuffling of the foot. The scenarios
illustrate the performance of our control scheme in achieving the desired
forces, CoM position attractor, and planned trajectories while actively
maintaining balance.
</p>
<a href="http://arxiv.org/abs/2103.02906" target="_blank">arXiv:2103.02906</a> [<a href="http://arxiv.org/pdf/2103.02906" target="_blank">pdf</a>]

<h2>Coordinate Attention for Efficient Mobile Network Design. (arXiv:2103.02907v1 [cs.CV])</h2>
<h3>Qibin Hou, Daquan Zhou, Jiashi Feng</h3>
<p>Recent studies on mobile network design have demonstrated the remarkable
effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention)
for lifting model performance, but they generally neglect the positional
information, which is important for generating spatially selective attention
maps. In this paper, we propose a novel attention mechanism for mobile networks
by embedding positional information into channel attention, which we call
"coordinate attention". Unlike channel attention that transforms a feature
tensor to a single feature vector via 2D global pooling, the coordinate
attention factorizes channel attention into two 1D feature encoding processes
that aggregate features along the two spatial directions, respectively. In this
way, long-range dependencies can be captured along one spatial direction and
meanwhile precise positional information can be preserved along the other
spatial direction. The resulting feature maps are then encoded separately into
a pair of direction-aware and position-sensitive attention maps that can be
complementarily applied to the input feature map to augment the representations
of the objects of interest. Our coordinate attention is simple and can be
flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt,
and EfficientNet with nearly no computational overhead. Extensive experiments
demonstrate that our coordinate attention is not only beneficial to ImageNet
classification but more interestingly, behaves better in down-stream tasks,
such as object detection and semantic segmentation. Code is available at
https://github.com/Andrew-Qibin/CoordAttention.
</p>
<a href="http://arxiv.org/abs/2103.02907" target="_blank">arXiv:2103.02907</a> [<a href="http://arxiv.org/pdf/2103.02907" target="_blank">pdf</a>]

<h2>Semi-supervised Left Atrium Segmentation with Mutual Consistency Training. (arXiv:2103.02911v1 [cs.CV])</h2>
<h3>Yicheng Wu, Minfeng Xu, Zongyuan Ge, Jianfei Cai, Lei Zhang</h3>
<p>Semi-supervised learning has attracted great attention in the field of
machine learning, especially for medical image segmentation tasks, since it
alleviates the heavy burden of collecting abundant densely annotated data for
training. However, most of existing methods underestimate the importance of
challenging regions (e.g. small branches or blurred edges) during training. We
believe that these unlabeled regions may contain more crucial information to
minimize the uncertainty prediction for the model and should be emphasized in
the training process. Therefore, in this paper, we propose a novel Mutual
Consistency Network (MC-Net) for semi-supervised left atrium segmentation from
3D MR images. Particularly, our MC-Net consists of one encoder and two slightly
different decoders, and the prediction discrepancies of two decoders are
transformed as an unsupervised loss by our designed cycled pseudo label scheme
to encourage mutual consistency. Such mutual consistency encourages the two
decoders to have consistent and low-entropy predictions and enables the model
to gradually capture generalized features from these unlabeled challenging
regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it
obtains impressive performance gains by exploiting the unlabeled data
effectively. Our MC-Net outperforms six recent semi-supervised methods for left
atrium segmentation, and sets the new state-of-the-art performance on the LA
database.
</p>
<a href="http://arxiv.org/abs/2103.02911" target="_blank">arXiv:2103.02911</a> [<a href="http://arxiv.org/pdf/2103.02911" target="_blank">pdf</a>]

<h2>QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval. (arXiv:2103.02927v1 [cs.CV])</h2>
<h3>Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui Wang, Hang Su, Hui Xue</h3>
<p>We study the query-based attack against image retrieval to evaluate its
robustness against adversarial examples under the black-box setting, where the
adversary only has query access to the top-k ranked unlabeled images from the
database. Compared with query attacks in image classification, which produce
adversaries according to the returned labels or confidence score, the challenge
becomes even more prominent due to the difficulty in quantifying the attack
effectiveness on the partial retrieved list. In this paper, we make the first
attempt in Query-based Attack against Image Retrieval (QAIR), to completely
subvert the top-k retrieval results. Specifically, a new relevance-based loss
is designed to quantify the attack effects by measuring the set similarity on
the top-k retrieval results before and after attacks and guide the gradient
optimization. To further boost the attack efficiency, a recursive model
stealing method is proposed to acquire transferable priors on the target model
and generate the prior-guided gradients. Comprehensive experiments show that
the proposed attack achieves a high attack success rate with few queries
against the image retrieval systems under the black-box setting. The attack
evaluations on the real-world visual search engine show that it successfully
deceives a commercial system such as Bing Visual Search with 98% attack success
rate by only 33 queries on average.
</p>
<a href="http://arxiv.org/abs/2103.02927" target="_blank">arXiv:2103.02927</a> [<a href="http://arxiv.org/pdf/2103.02927" target="_blank">pdf</a>]

<h2>Graph-based Task-specific Prediction Models for Interactions between Deformable and Rigid Objects. (arXiv:2103.02932v1 [cs.RO])</h2>
<h3>Zehang Weng, Fabian Paus, Anastasiia Varava, Hang Yin, Tamim Asfour, Danica Kragic</h3>
<p>Capturing scene dynamics and predicting the future scene state is challenging
but essential for robotic manipulation tasks, especially when the scene
contains both rigid and deformable objects. In this work, we contribute a
simulation environment and generate a novel dataset for task-specific
manipulation, involving interactions between rigid objects and a deformable
bag. The dataset incorporates a rich variety of scenarios including different
object sizes, object numbers and manipulation actions. We approach dynamics
learning by proposing an object-centric graph representation and two modules
which are Active Prediction Module (APM) and Position Prediction Module (PPM)
based on graph neural networks with an encode-process-decode architecture. At
the inference stage, we build a two-stage model based on the learned modules
for single time step prediction. We combine modules with different prediction
horizons into a mixed-horizon model which addresses long-term prediction. In an
ablation study, we show the benefits of the two-stage model for single time
step prediction and the effectiveness of the mixed-horizon model for long-term
prediction tasks. Supplementary material is available at
https://github.com/wengzehang/deformable_rigid_interaction_prediction
</p>
<a href="http://arxiv.org/abs/2103.02932" target="_blank">arXiv:2103.02932</a> [<a href="http://arxiv.org/pdf/2103.02932" target="_blank">pdf</a>]

<h2>Visual Question Answering: which investigated applications?. (arXiv:2103.02937v1 [cs.CV])</h2>
<h3>Silvio Barra, Carmen Bisogni, Maria De Marsico, Stefano Ricciardi</h3>
<p>Visual Question Answering (VQA) is an extremely stimulating and challenging
research area where Computer Vision (CV) and Natural Language Processig (NLP)
have recently met. In image captioning and video summarization, the semantic
information is completely contained in still images or video dynamics, and it
has only to be mined and expressed in a human-consistent way. Differently from
this, in VQA semantic information in the same media must be compared with the
semantics implied by a question expressed in natural language, doubling the
artificial intelligence-related effort. Some recent surveys about VQA
approaches have focused on methods underlying either the image-related
processing or the verbal-related one, or on the way to consistently fuse the
conveyed information. Possible applications are only suggested, and, in fact,
most cited works rely on general-purpose datasets that are used to assess the
building blocks of a VQA system. This paper rather considers the proposals that
focus on real-world applications, possibly using as benchmarks suitable data
bound to the application domain. The paper also reports about some recent
challenges in VQA research.
</p>
<a href="http://arxiv.org/abs/2103.02937" target="_blank">arXiv:2103.02937</a> [<a href="http://arxiv.org/pdf/2103.02937" target="_blank">pdf</a>]

<h2>Towards Ultrafast MRI via Extreme k-Space Undersampling and Superresolution. (arXiv:2103.02940v1 [cs.CV])</h2>
<h3>Aleksandr Belov, Joel Stadelmann, Sergey Kastryulin, Dmitry V. Dylov</h3>
<p>We went below the MRI acceleration factors (a.k.a., k-space undersampling)
reported by all published papers that reference the original fastMRI challenge,
and then considered powerful deep learning based image enhancement methods to
compensate for the underresolved images. We thoroughly study the influence of
the sampling patterns, the undersampling and the downscaling factors, as well
as the recovery models on the final image quality for both the brain and the
knee fastMRI benchmarks. The quality of the reconstructed images surpasses that
of the other methods, yielding an MSE of 0.00114, a PSNR of 29.6 dB, and an
SSIM of 0.956 at x16 acceleration factor. More extreme undersampling factors of
x32 and x64 are also investigated, holding promise for certain clinical
applications such as computer-assisted surgery or radiation planning. We survey
5 expert radiologists to assess 100 pairs of images and show that the recovered
undersampled images statistically preserve their diagnostic value.
</p>
<a href="http://arxiv.org/abs/2103.02940" target="_blank">arXiv:2103.02940</a> [<a href="http://arxiv.org/pdf/2103.02940" target="_blank">pdf</a>]

<h2>Motion-blurred Video Interpolation and Extrapolation. (arXiv:2103.02984v1 [cs.CV])</h2>
<h3>Dawit Mureja Argaw, Junsik Kim, Francois Rameau, In So Kweon</h3>
<p>Abrupt motion of camera or objects in a scene result in a blurry video, and
therefore recovering high quality video requires two types of enhancements:
visual enhancement and temporal upsampling. A broad range of research attempted
to recover clean frames from blurred image sequences or temporally upsample
frames by interpolation, yet there are very limited studies handling both
problems jointly. In this work, we present a novel framework for deblurring,
interpolating and extrapolating sharp frames from a motion-blurred video in an
end-to-end manner. We design our framework by first learning the pixel-level
motion that caused the blur from the given inputs via optical flow estimation
and then predict multiple clean frames by warping the decoded features with the
estimated flows. To ensure temporal coherence across predicted frames and
address potential temporal ambiguity, we propose a simple, yet effective
flow-based rule. The effectiveness and favorability of our approach are
highlighted through extensive qualitative and quantitative evaluations on
motion-blurred datasets from high speed videos.
</p>
<a href="http://arxiv.org/abs/2103.02984" target="_blank">arXiv:2103.02984</a> [<a href="http://arxiv.org/pdf/2103.02984" target="_blank">pdf</a>]

<h2>Optical Flow Estimation from a Single Motion-blurred Image. (arXiv:2103.02996v1 [cs.CV])</h2>
<h3>Dawit Mureja Argaw, Junsik Kim, Francois Rameau, Jae Won Cho, In So Kweon</h3>
<p>In most of computer vision applications, motion blur is regarded as an
undesirable artifact. However, it has been shown that motion blur in an image
may have practical interests in fundamental computer vision problems. In this
work, we propose a novel framework to estimate optical flow from a single
motion-blurred image in an end-to-end manner. We design our network with
transformer networks to learn globally and locally varying motions from encoded
features of a motion-blurred input, and decode left and right frame features
without explicit frame supervision. A flow estimator network is then used to
estimate optical flow from the decoded features in a coarse-to-fine manner. We
qualitatively and quantitatively evaluate our model through a large set of
experiments on synthetic and real motion-blur datasets. We also provide
in-depth analysis of our model in connection with related approaches to
highlight the effectiveness and favorability of our approach. Furthermore, we
showcase the applicability of the flow estimated by our method on deblurring
and moving object segmentation tasks.
</p>
<a href="http://arxiv.org/abs/2103.02996" target="_blank">arXiv:2103.02996</a> [<a href="http://arxiv.org/pdf/2103.02996" target="_blank">pdf</a>]

<h2>MOGAN: Morphologic-structure-aware Generative Learning from a Single Image. (arXiv:2103.02997v1 [cs.CV])</h2>
<h3>Jinshu Chen, Qihui Xu, Qi Kang, MengChu Zhou</h3>
<p>In most interactive image generation tasks, given regions of interest (ROI)
by users, the generated results are expected to have adequate diversities in
appearance while maintaining correct and reasonable structures in original
images. Such tasks become more challenging if only limited data is available.
Recently proposed generative models complete training based on only one image.
They pay much attention to the monolithic feature of the sample while ignoring
the actual semantic information of different objects inside the sample. As a
result, for ROI-based generation tasks, they may produce inappropriate samples
with excessive randomicity and without maintaining the related objects' correct
structures. To address this issue, this work introduces a
MOrphologic-structure-aware Generative Adversarial Network named MOGAN that
produces random samples with diverse appearances and reliable structures based
on only one image. For training for ROI, we propose to utilize the data coming
from the original image being augmented and bring in a novel module to
transform such augmented data into knowledge containing both structures and
appearances, thus enhancing the model's comprehension of the sample. To learn
the rest areas other than ROI, we employ binary masks to ensure the generation
isolated from ROI. Finally, we set parallel and hierarchical branches of the
mentioned learning process. Compared with other single image GAN schemes, our
approach focuses on internal features including the maintenance of rational
structures and variation on appearance. Experiments confirm a better capacity
of our model on ROI-based image generation tasks than its competitive peers.
</p>
<a href="http://arxiv.org/abs/2103.02997" target="_blank">arXiv:2103.02997</a> [<a href="http://arxiv.org/pdf/2103.02997" target="_blank">pdf</a>]

<h2>A framework for power line inspection tasks with multi-robot systems from signal temporal logic specifications. (arXiv:2103.02999v1 [cs.RO])</h2>
<h3>Giuseppe Silano, Davide Liuzza, Luigi Iannelli, Martin Saska</h3>
<p>Inspection of power line infrastructures must be periodically conducted by
electric companies in order to ensure reliable electric power distribution.
Research efforts are focused on automating the power line inspection process by
looking for strategies that satisfy different requirements expressed in terms
of potential damage and faults detection. This problem comes up with the need
of safe planning and control techniques for autonomous robots to perform visual
inspection tasks. Such an application becomes even more interesting and of
critical importance when considering a multi-robot extension. In this paper, we
propose to compute feasible and constrained trajectories for a fleet of
quad-rotors leveraging on Signal Temporal Logic (STL) specifications. The
planner allows to formulate rather complex missions avoiding obstacles and
forbidden areas along the path. Simulations results achieved in MATLAB show the
effectiveness of the proposed approach leading the way to experimental tests on
the hardware.
</p>
<a href="http://arxiv.org/abs/2103.02999" target="_blank">arXiv:2103.02999</a> [<a href="http://arxiv.org/pdf/2103.02999" target="_blank">pdf</a>]

<h2>SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier Domain. (arXiv:2103.03000v1 [cs.CV])</h2>
<h3>Paula Harder, Franz-Josef Pfreundt, Margret Keuper, Janis Keuper</h3>
<p>Despite the success of convolutional neural networks (CNNs) in many computer
vision and image analysis tasks, they remain vulnerable against so-called
adversarial attacks: Small, crafted perturbations in the input images can lead
to false predictions. A possible defense is to detect adversarial examples. In
this work, we show how analysis in the Fourier domain of input images and
feature maps can be used to distinguish benign test samples from adversarial
images. We propose two novel detection methods: Our first method employs the
magnitude spectrum of the input images to detect an adversarial attack. This
simple and robust classifier can successfully detect adversarial perturbations
of three commonly used attack methods. The second method builds upon the first
and additionally extracts the phase of Fourier coefficients of feature-maps at
different layers of the network. With this extension, we are able to improve
adversarial detection rates compared to state-of-the-art detectors on five
different attack methods.
</p>
<a href="http://arxiv.org/abs/2103.03000" target="_blank">arXiv:2103.03000</a> [<a href="http://arxiv.org/pdf/2103.03000" target="_blank">pdf</a>]

<h2>Reinforcement Learning Trajectory Generation and Control for Aggressive Perching on Vertical Walls with Quadrotors. (arXiv:2103.03011v1 [cs.RO])</h2>
<h3>Chen-Huan Pi, Kai-Chun Hu, Yu-Ting Huang, Stone Cheng</h3>
<p>Micro aerial vehicles are widely being researched and employed due to their
relative low operation costs and high flexibility in various applications. We
study the under-actuated quadrotor perching problem, designing a trajectory
planner and controller which generates feasible trajectories and drives
quadrotors to desired state in state space. This paper proposes a trajectory
generating and tracking method for quadrotor perching that takes the advantages
of reinforcement learning controller and traditional controller. The trained
low-level reinforcement learning controller would manipulate quadrotor toward
the perching point in simulation environment. Once the simulated quadrotor has
successfully perched, the relative trajectory information in simulation will be
sent to tracking controller on real quadrotor and start the actual perching
task. Generating feasible trajectories via the trained reinforcement learning
controller requires less time, and the traditional trajectory tracking
controller could easily be modified to control the quadrotor and mathematically
analysis its stability and robustness. We show that this approach permits the
control structure of trajectories and controllers enabling such aggressive
maneuvers perching on vertical surfaces with high precision.
</p>
<a href="http://arxiv.org/abs/2103.03011" target="_blank">arXiv:2103.03011</a> [<a href="http://arxiv.org/pdf/2103.03011" target="_blank">pdf</a>]

<h2>CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation. (arXiv:2103.03024v1 [cs.CV])</h2>
<h3>Yutong Xie, Jianpeng Zhang, Chunhua Shen, Yong Xia</h3>
<p>Convolutional neural networks (CNNs) have been the de facto standard for
nowadays 3D medical image segmentation. The convolutional operations used in
these networks, however, inevitably have limitations in modeling the long-range
dependency due to their inductive bias of locality and weight sharing. Although
Transformer was born to address this issue, it suffers from extreme
computational and spatial complexities in processing high-resolution 3D feature
maps. In this paper, we propose a novel framework that efficiently bridges a
{\bf Co}nvolutional neural network and a {\bf Tr}ansformer {\bf (CoTr)} for
accurate 3D medical image segmentation. Under this framework, the CNN is
constructed to extract feature representations and an efficient deformable
Transformer (DeTrans) is built to model the long-range dependency on the
extracted feature maps. Different from the vanilla Transformer which treats all
image positions equally, our DeTrans pays attention only to a small set of key
positions by introducing the deformable self-attention mechanism. Thus, the
computational and spatial complexities of DeTrans have been greatly reduced,
making it possible to process the multi-scale and high-resolution feature maps,
which are usually of paramount importance for image segmentation. We conduct an
extensive evaluation on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV)
dataset that covers 11 major human organs. The results indicate that our CoTr
leads to a substantial performance improvement over other CNN-based,
transformer-based, and hybrid methods on the 3D multi-organ segmentation task.
Code is available at \def\UrlFont{\rm\small\ttfamily}
\url{https://github.com/YtongXie/CoTr}
</p>
<a href="http://arxiv.org/abs/2103.03024" target="_blank">arXiv:2103.03024</a> [<a href="http://arxiv.org/pdf/2103.03024" target="_blank">pdf</a>]

<h2>Modeling Multi-Label Action Dependencies for Temporal Action Localization. (arXiv:2103.03027v1 [cs.CV])</h2>
<h3>Praveen Tirupattur, Kevin Duarte, Yogesh Rawat, Mubarak Shah</h3>
<p>Real-world videos contain many complex actions with inherent relationships
between action classes. In this work, we propose an attention-based
architecture that models these action relationships for the task of temporal
action localization in untrimmed videos. As opposed to previous works that
leverage video-level co-occurrence of actions, we distinguish the relationships
between actions that occur at the same time-step and actions that occur at
different time-steps (i.e. those which precede or follow each other). We define
these distinct relationships as action dependencies. We propose to improve
action localization performance by modeling these action dependencies in a
novel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer
consists of two branches: a Co-occurrence Dependency Branch and a Temporal
Dependency Branch to model co-occurrence action dependencies and temporal
action dependencies, respectively. We observe that existing metrics used for
multi-label classification do not explicitly measure how well action
dependencies are modeled, therefore, we propose novel metrics that consider
both co-occurrence and temporal dependencies between action classes. Through
empirical evaluation and extensive analysis, we show improved performance over
state-of-the-art methods on multi-label action localization
benchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.
</p>
<a href="http://arxiv.org/abs/2103.03027" target="_blank">arXiv:2103.03027</a> [<a href="http://arxiv.org/pdf/2103.03027" target="_blank">pdf</a>]

<h2>Mobile Touchless Fingerprint Recognition: Implementation, Performance and Usability Aspects. (arXiv:2103.03038v1 [cs.CV])</h2>
<h3>Jannis Priesnitz, Rolf Huesmann, Christian Rathgeb, Nicolas Buchmann, Christoph Busch</h3>
<p>This work presents an automated touchless fingerprint recognition system for
smartphones. We provide a comprehensive description of the entire recognition
pipeline and discuss important requirements for a fully automated capturing
system. Also, our implementation is made publicly available for research
purposes. During a database acquisition, a total number of 1,360 touchless and
touch-based samples of 29 subjects are captured in two different environmental
situations. Experiments on the acquired database show a comparable performance
of our touchless scheme and the touch-based baseline scheme under constrained
environmental influences. A comparative usability study on both capturing
device types indicates that the majority of subjects prefer the touchless
capturing method. Based on our experimental results we analyze the impact of
the current COVID-19 pandemic on fingerprint recognition systems. Finally,
implementation aspects of touchless fingerprint recognition are summarized.
</p>
<a href="http://arxiv.org/abs/2103.03038" target="_blank">arXiv:2103.03038</a> [<a href="http://arxiv.org/pdf/2103.03038" target="_blank">pdf</a>]

<h2>Real-Time Navigation System for a Low-Cost Mobile Robot with an RGB-D Camera. (arXiv:2103.03054v1 [cs.RO])</h2>
<h3>Taekyung Kim, Seunghyun Lim, Gwanjun Shin, Geonhee Sim, Dongwon Yun</h3>
<p>Currently, mobile robots are developing rapidly and are finding numerous
applications in industry. However, there remain a number of problems related to
their practical use, such as the need for expensive hardware and their high
power consumption levels. In this study, we propose a navigation system that is
operable on a low-end computer with an RGB-D camera and a mobile robot platform
for the operation of an integrated autonomous driving system. The proposed
system does not require LiDARs or a GPU. Our raw depth image ground
segmentation approach extracts a traversability map for the safe driving of
low-body mobile robots. It is designed to guarantee real-time performance on a
low-cost commercial single board computer with integrated SLAM, global path
planning, and motion planning. Running sensor data processing and other
autonomous driving functions simultaneously, our navigation method performs
rapidly at a refresh rate of 18Hz for control command, whereas other systems
have slower refresh rates. Our method outperforms current state-of-the-art
navigation approaches as shown in 3D simulation tests. In addition, we
demonstrate the applicability of our mobile robot system through successful
autonomous driving in a residential lobby.
</p>
<a href="http://arxiv.org/abs/2103.03054" target="_blank">arXiv:2103.03054</a> [<a href="http://arxiv.org/pdf/2103.03054" target="_blank">pdf</a>]

<h2>Sub-pixel face landmarks using heatmaps and a bag of tricks. (arXiv:2103.03059v1 [cs.CV])</h2>
<h3>Samuel W. F. Earp, Aubin Samacoits, Sanjana Jain, Pavit Noinongyao, Siwa Boonpunmongkol</h3>
<p>Accurate face landmark localization is an essential part of face recognition,
reconstruction and morphing. To accurately localize face landmarks, we present
our heatmap regression approach. Each model consists of a MobileNetV2 backbone
followed by several upscaling layers, with different tricks to optimize both
performance and inference cost. We use five na\"ive face landmarks from a
publicly available face detector to position and align the face instead of
using the bounding box like traditional methods. Moreover, we show by adding
random rotation, displacement and scaling -- after alignment -- that the model
is more sensitive to the face position than orientation. We also show that it
is possible to reduce the upscaling complexity by using a mixture of
deconvolution and pixel-shuffle layers without impeding localization
performance. We present our state-of-the-art face landmark localization model
(ranking second on The 2nd Grand Challenge of 106-Point Facial Landmark
Localization validation set). Finally, we test the effect on face recognition
using these landmarks, using a publicly available model and benchmarks.
</p>
<a href="http://arxiv.org/abs/2103.03059" target="_blank">arXiv:2103.03059</a> [<a href="http://arxiv.org/pdf/2103.03059" target="_blank">pdf</a>]

<h2>BM3D vs 2-Layer ONN. (arXiv:2103.03060v1 [cs.CV])</h2>
<h3>Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Moncef Gabbouj</h3>
<p>Despite their recent success on image denoising, the need for deep and
complex architectures still hinders the practical usage of CNNs. Older but
computationally more efficient methods such as BM3D remain a popular choice,
especially in resource-constrained scenarios. In this study, we aim to find out
whether compact neural networks can learn to produce competitive results as
compared to BM3D for AWGN image denoising. To this end, we configure networks
with only two hidden layers and employ different neuron models and layer widths
for comparing the performance with BM3D across different AWGN noise levels. Our
results conclusively show that the recently proposed self-organized variant of
operational neural networks based on a generative neuron model (Self-ONNs) is
not only a better choice as compared to CNNs, but also provide competitive
results as compared to BM3D and even significantly surpass it for high noise
levels.
</p>
<a href="http://arxiv.org/abs/2103.03060" target="_blank">arXiv:2103.03060</a> [<a href="http://arxiv.org/pdf/2103.03060" target="_blank">pdf</a>]

<h2>TPCN: Temporal Point Cloud Networks for Motion Forecasting. (arXiv:2103.03067v1 [cs.CV])</h2>
<h3>Maosheng Ye, Tongyi Cao, Qifeng Chen</h3>
<p>We propose the Temporal Point Cloud Networks (TPCN), a novel and flexible
framework with joint spatial and temporal learning for trajectory prediction.
Unlike existing approaches that rasterize agents and map information as 2D
images or operate in a graph representation, our approach extends ideas from
point cloud learning with dynamic temporal learning to capture both spatial and
temporal information by splitting trajectory prediction into both spatial and
temporal dimensions. In the spatial dimension, agents can be viewed as an
unordered point set, and thus it is straightforward to apply point cloud
learning techniques to model agents' locations. While the spatial dimension
does not take kinematic and motion information into account, we further propose
dynamic temporal learning to model agents' motion over time. Experiments on the
Argoverse motion forecasting benchmark show that our approach achieves the
state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2103.03067" target="_blank">arXiv:2103.03067</a> [<a href="http://arxiv.org/pdf/2103.03067" target="_blank">pdf</a>]

<h2>Convolutional versus Self-Organized Operational Neural Networks for Real-World Blind Image Denoising. (arXiv:2103.03070v1 [cs.CV])</h2>
<h3>Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Esin Guldogan, Moncef Gabbouj</h3>
<p>Real-world blind denoising poses a unique image restoration challenge due to
the non-deterministic nature of the underlying noise distribution. Prevalent
discriminative networks trained on synthetic noise models have been shown to
generalize poorly to real-world noisy images. While curating real-world noisy
images and improving ground truth estimation procedures remain key points of
interest, a potential research direction is to explore extensions to the widely
used convolutional neuron model to enable better generalization with fewer data
and lower network complexity, as opposed to simply using deeper Convolutional
Neural Networks (CNNs). Operational Neural Networks (ONNs) and their recent
variant, Self-organized ONNs (Self-ONNs), propose to embed enhanced
non-linearity into the neuron model and have been shown to outperform CNNs
across a variety of regression tasks. However, all such comparisons have been
made for compact networks and the efficacy of deploying operational layers as a
drop-in replacement for convolutional layers in contemporary deep architectures
remains to be seen. In this work, we tackle the real-world blind image
denoising problem by employing, for the first time, a deep Self-ONN. Extensive
quantitative and qualitative evaluations spanning multiple metrics and four
high-resolution real-world noisy image datasets against the state-of-the-art
deep CNN network, DnCNN, reveal that deep Self-ONNs consistently achieve
superior results with performance gains of up to 1.76dB in PSNR. Furthermore,
Self-ONNs with half and even quarter the number of layers that require only a
fraction of computational resources as that of DnCNN can still achieve similar
or better results compared to the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.03070" target="_blank">arXiv:2103.03070</a> [<a href="http://arxiv.org/pdf/2103.03070" target="_blank">pdf</a>]

<h2>Toward Automated Generation of Affective Gestures from Text:A Theory-Driven Approach. (arXiv:2103.03079v1 [cs.RO])</h2>
<h3>Micol Spitale, Maja J Matari&#x107;</h3>
<p>Communication in both human-human and human-robot interac-tion (HRI) contexts
consists of verbal (speech-based) and non-verbal(facial expressions, eye gaze,
gesture, body pose, etc.) components.The verbal component contains semantic and
affective information;accordingly, HRI work on the gesture component so far has
focusedon rule-based (mapping words to gestures) and data-driven
(deep-learning) approaches to generating speech-paired gestures basedon either
semantics or the affective state. Consequently, most ges-ture systems are
confined to producing either semantically-linkedor affect-based gesticures.
This paper introduces an approach forenabling human-robot communication based
on a theory-drivenapproach to generate speech-paired robot gestures using both
se-mantic and affective information. Our model takes as input textand sentiment
analysis, and generates robot gestures in terms oftheir shape, intensity, and
speed.
</p>
<a href="http://arxiv.org/abs/2103.03079" target="_blank">arXiv:2103.03079</a> [<a href="http://arxiv.org/pdf/2103.03079" target="_blank">pdf</a>]

<h2>An Optimization Approach for a Robust and Flexible Control in Collaborative Applications. (arXiv:2103.03082v1 [cs.RO])</h2>
<h3>Federico Benzi, Cristian Secchi</h3>
<p>In Human-Robot Collaboration, the robot operates in a highly dynamic
environment. Thus, it is pivotal to guarantee the robust stability of the
system during the interaction but also a high flexibility of the robot behavior
in order to ensure safety and reactivity to the variable conditions of the
collaborative scenario. In this paper we propose a control architecture capable
of maximizing the flexibility of the robot while guaranteeing a stable behavior
when physically interacting with the environment. This is achieved by combining
an energy tank based variable admittance architecture with control barrier
functions. The proposed architecture is experimentally validated on a
collaborative robot.
</p>
<a href="http://arxiv.org/abs/2103.03082" target="_blank">arXiv:2103.03082</a> [<a href="http://arxiv.org/pdf/2103.03082" target="_blank">pdf</a>]

<h2>Towards Designing Computer Vision-based Explainable-AI Solution: A Use Case of Livestock Mart Industry. (arXiv:2103.03096v1 [cs.CV])</h2>
<h3>Devam Dave, Het Naik, Smiti Singhal, Rudresh Dwivedi, Pankesh Patel</h3>
<p>The objective of an online Mart is to match buyers and sellers, to weigh
animals and to oversee their sale. A reliable pricing method can be developed
by ML models that can read through historical sales data. However, when AI
models suggest or recommend a price, that in itself does not reveal too much
(i.e., it acts like a black box) about the qualities and the abilities of an
animal. An interested buyer would like to know more about the salient features
of an animal before making the right choice based on his requirements. A model
capable of explaining the different factors that impact the price point is
essential for the needs of the market. It can also inspire confidence in buyers
and sellers about the price point offered. To achieve these objectives, we have
been working with the team at MartEye, a startup based in Portershed in Galway
City, Ireland. Through this paper, we report our work-in-progress research
towards building a smart video analytic platform, leveraging Explainable AI
techniques.
</p>
<a href="http://arxiv.org/abs/2103.03096" target="_blank">arXiv:2103.03096</a> [<a href="http://arxiv.org/pdf/2103.03096" target="_blank">pdf</a>]

<h2>ILoSA: Interactive Learning of Stiffness and Attractors. (arXiv:2103.03099v1 [cs.RO])</h2>
<h3>Giovanni Franzese, Anna M&#xe9;sz&#xe1;ros, Luka Peternel, Jens Kober</h3>
<p>Teaching robots how to apply forces according to our preferences is still an
open challenge that has to be tackled from multiple engineering perspectives.
This paper studies how to learn variable impedance policies where both the
Cartesian stiffness and the attractor can be learned from human demonstrations
and corrections with a user-friendly interface. The presented framework, named
ILoSA, uses Gaussian Processes for policy learning, identifying regions of
uncertainty and allowing interactive corrections, stiffness modulation and
active disturbance rejection. The experimental evaluation of the framework is
carried out on a Franka-Emika Panda in three separate cases with unique force
interaction properties: 1) pulling a plug wherein a sudden force discontinuity
occurs upon successful removal of the plug, 2) pushing a box where a sustained
force is required to keep the robot in motion, and 3) wiping a whiteboard in
which the force is applied perpendicular to the direction of movement.
</p>
<a href="http://arxiv.org/abs/2103.03099" target="_blank">arXiv:2103.03099</a> [<a href="http://arxiv.org/pdf/2103.03099" target="_blank">pdf</a>]

<h2>Self-supervised Geometric Perception. (arXiv:2103.03114v1 [cs.CV])</h2>
<h3>Heng Yang, Wei Dong, Luca Carlone, Vladlen Koltun</h3>
<p>We present self-supervised geometric perception (SGP), the first general
framework to learn a feature descriptor for correspondence matching without any
ground-truth geometric model labels (e.g., camera poses, rigid
transformations). Our first contribution is to formulate geometric perception
as an optimization problem that jointly optimizes the feature descriptor and
the geometric models given a large corpus of visual measurements (e.g., images,
point clouds). Under this optimization formulation, we show that two important
streams of research in vision, namely robust model fitting and deep feature
learning, correspond to optimizing one block of the unknown variables while
fixing the other block. This analysis naturally leads to our second
contribution -- the SGP algorithm that performs alternating minimization to
solve the joint optimization. SGP iteratively executes two meta-algorithms: a
teacher that performs robust model fitting given learned features to generate
geometric pseudo-labels, and a student that performs deep feature learning
under noisy supervision of the pseudo-labels. As a third contribution, we apply
SGP to two perception problems on large-scale real datasets, namely relative
camera pose estimation on MegaDepth and point cloud registration on 3DMatch. We
demonstrate that SGP achieves state-of-the-art performance that is on-par or
superior to the supervised oracles trained using ground-truth labels.
</p>
<a href="http://arxiv.org/abs/2103.03114" target="_blank">arXiv:2103.03114</a> [<a href="http://arxiv.org/pdf/2103.03114" target="_blank">pdf</a>]

<h2>Learning Whole-Slide Segmentation from Inexact and Incomplete Labels using Tissue Graphs. (arXiv:2103.03129v1 [cs.CV])</h2>
<h3>Valentin Anklin, Pushpak Pati, Guillaume Jaume, Behzad Bozorgtabar, Antonio Foncubierta-Rodr&#xed;guez, Jean-Philippe Thiran, Mathilde Sibony, Maria Gabrani, Orcun Goksel</h3>
<p>Segmenting histology images into diagnostically relevant regions is
imperative to support timely and reliable decisions by pathologists. To this
end, computer-aided techniques have been proposed to delineate relevant regions
in scanned histology slides. However, the techniques necessitate task-specific
large datasets of annotated pixels, which is tedious, time-consuming,
expensive, and infeasible to acquire for many histology tasks. Thus,
weakly-supervised semantic segmentation techniques are proposed to utilize weak
supervision that is cheaper and quicker to acquire. In this paper, we propose
SegGini, a weakly supervised segmentation method using graphs, that can utilize
weak multiplex annotations, i.e. inexact and incomplete annotations, to segment
arbitrary and large images, scaling from tissue microarray (TMA) to whole slide
image (WSI). Formally, SegGini constructs a tissue-graph representation for an
input histology image, where the graph nodes depict tissue regions. Then, it
performs weakly-supervised segmentation via node classification by using
inexact image-level labels, incomplete scribbles, or both. We evaluated SegGini
on two public prostate cancer datasets containing TMAs and WSIs. Our method
achieved state-of-the-art segmentation performance on both datasets for various
annotation settings while being comparable to a pathologist baseline.
</p>
<a href="http://arxiv.org/abs/2103.03129" target="_blank">arXiv:2103.03129</a> [<a href="http://arxiv.org/pdf/2103.03129" target="_blank">pdf</a>]

<h2>Visual diagnosis of the Varroa destructor parasitic mite in honeybees using object detector techniques. (arXiv:2103.03133v1 [cs.CV])</h2>
<h3>Simon Bilik, Lukas Kratochvila, Adam Ligocki, Ondrej Bostik, Tomas Zemcik, Matous Hybl, Karel Horak, Ludek Zalud</h3>
<p>The Varroa destructor mite is one of the most dangerous Honey Bee (Apis
mellifera) parasites worldwide and the bee colonies have to be regularly
monitored in order to control its spread. Here we present an object detector
based method for health state monitoring of bee colonies. This method has the
potential for online measurement and processing. In our experiment, we compare
the YOLO and SSD object detectors along with the Deep SVDD anomaly detector.
Based on the custom dataset with 600 ground-truth images of healthy and
infected bees in various scenes, the detectors reached a high F1 score up to
0.874 in the infected bee detection and up to 0.727 in the detection of the
Varroa Destructor mite itself. The results demonstrate the potential of this
approach, which will be later used in the real-time computer vision based honey
bee inspection system. To the best of our knowledge, this study is the first
one using object detectors for this purpose. We expect that performance of
those object detectors will enable us to inspect the health status of the honey
bee colonies.
</p>
<a href="http://arxiv.org/abs/2103.03133" target="_blank">arXiv:2103.03133</a> [<a href="http://arxiv.org/pdf/2103.03133" target="_blank">pdf</a>]

<h2>SSTN: Self-Supervised Domain Adaptation Thermal Object Detection for Autonomous Driving. (arXiv:2103.03150v1 [cs.CV])</h2>
<h3>Farzeen Munir, Shoaib Azam, Moongu Jeon</h3>
<p>The sensibility and sensitivity of the environment play a decisive role in
the safe and secure operation of autonomous vehicles. This perception of the
surrounding is way similar to human visual representation. The human's brain
perceives the environment by utilizing different sensory channels and develop a
view-invariant representation model. Keeping in this context, different
exteroceptive sensors are deployed on the autonomous vehicle for perceiving the
environment. The most common exteroceptive sensors are camera, Lidar and radar
for autonomous vehicle's perception. Despite being these sensors have
illustrated their benefit in the visible spectrum domain yet in the adverse
weather conditions, for instance, at night, they have limited operation
capability, which may lead to fatal accidents. In this work, we explore thermal
object detection to model a view-invariant model representation by employing
the self-supervised contrastive learning approach. For this purpose, we have
proposed a deep neural network Self Supervised Thermal Network (SSTN) for
learning the feature embedding to maximize the information between visible and
infrared spectrum domain by contrastive learning, and later employing these
learned feature representation for the thermal object detection using
multi-scale encoder-decoder transformer network. The proposed method is
extensively evaluated on the two publicly available datasets: the FLIR-ADAS
dataset and the KAIST Multi-Spectral dataset. The experimental results
illustrate the efficacy of the proposed method.
</p>
<a href="http://arxiv.org/abs/2103.03150" target="_blank">arXiv:2103.03150</a> [<a href="http://arxiv.org/pdf/2103.03150" target="_blank">pdf</a>]

<h2>A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v1 [cs.CV])</h2>
<h3>Jacob C. Reinhold, Aaron Carass, Jerry L. Prince</h3>
<p>Precision medicine involves answering counterfactual questions such as "Would
this patient respond better to treatment A or treatment B?" These types of
questions are causal in nature and require the tools of causal inference to be
answered, e.g., with a structural causal model (SCM). In this work, we develop
an SCM that models the interaction between demographic information, disease
covariates, and magnetic resonance (MR) images of the brain for people with
multiple sclerosis (MS). Inference in the SCM generates counterfactual images
that show what an MR image of the brain would look like when demographic or
disease covariates are changed. These images can be used for modeling disease
progression or used for downstream image processing tasks where controlling for
confounders is necessary.
</p>
<a href="http://arxiv.org/abs/2103.03158" target="_blank">arXiv:2103.03158</a> [<a href="http://arxiv.org/pdf/2103.03158" target="_blank">pdf</a>]

<h2>Contrastive Learning Meets Transfer Learning: A Case Study In Medical Image Analysis. (arXiv:2103.03166v1 [cs.CV])</h2>
<h3>Yuzhe Lu, Aadarsh Jha, Yuankai Huo</h3>
<p>Annotated medical images are typically rarer than labeled natural images
since they are limited by domain knowledge and privacy constraints. Recent
advances in transfer and contrastive learning have provided effective solutions
to tackle such issues from different perspectives. The state-of-the-art
transfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,
Simple Siamese Contrastive Learning (SimSiam)) approaches have been
investigated independently, without considering the complementary nature of
such techniques. It would be appealing to accelerate contrastive learning with
transfer learning, given that slow convergence speed is a critical limitation
of modern contrastive learning approaches. In this paper, we investigate the
feasibility of aligning BiT with SimSiam. From empirical analyses, different
normalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the
key hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we
evaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10
and HAM10000 datasets. The results suggest that the BiT models accelerate the
convergence speed of SimSiam. When used together, the model gives superior
performance over both of its counterparts. We hope this study will motivate
researchers to revisit the task of aggregating big pre-trained models with
contrastive learning models for image analysis.
</p>
<a href="http://arxiv.org/abs/2103.03166" target="_blank">arXiv:2103.03166</a> [<a href="http://arxiv.org/pdf/2103.03166" target="_blank">pdf</a>]

<h2>Enhanced 3D Human Pose Estimation from Videos by using Attention-Based Neural Network with Dilated Convolutions. (arXiv:2103.03170v1 [cs.CV])</h2>
<h3>Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung, Vijayan K. Asari</h3>
<p>The attention mechanism provides a sequential prediction framework for
learning spatial models with enhanced implicit temporal consistency. In this
work, we show a systematic design (from 2D to 3D) for how conventional networks
and other forms of constraints can be incorporated into the attention framework
for learning long-range dependencies for the task of pose estimation. The
contribution of this paper is to provide a systematic approach for designing
and training of attention-based models for the end-to-end pose estimation, with
the flexibility and scalability of arbitrary video sequences as input. We
achieve this by adapting temporal receptive field via a multi-scale structure
of dilated convolutions. Besides, the proposed architecture can be easily
adapted to a causal model enabling real-time performance. Any off-the-shelf 2D
pose estimation systems, e.g. Mocap libraries, can be easily integrated in an
ad-hoc fashion. Our method achieves the state-of-the-art performance and
outperforms existing methods by reducing the mean per joint position error to
33.4 mm on Human3.6M dataset.
</p>
<a href="http://arxiv.org/abs/2103.03170" target="_blank">arXiv:2103.03170</a> [<a href="http://arxiv.org/pdf/2103.03170" target="_blank">pdf</a>]

<h2>Prostate Tissue Grading with Deep Quantum Measurement Ordinal Regression. (arXiv:2103.03188v1 [cs.CV])</h2>
<h3>Santiago Toledo-Cort&#xe9;s, Diego H. Useche, Fabio A. Gonz&#xe1;lez</h3>
<p>Prostate cancer (PCa) is one of the most common and aggressive cancers
worldwide. The Gleason score (GS) system is the standard way of classifying
prostate cancer and the most reliable method to determine the severity and
treatment to follow. The pathologist looks at the arrangement of cancer cells
in the prostate and assigns a score on a scale that ranges from 6 to 10.
Automatic analysis of prostate whole-slide images (WSIs) is usually addressed
as a binary classification problem, which misses the finer distinction between
stages given by the GS. This paper presents a probabilistic deep learning
ordinal classification method that can estimate the GS from a prostate WSI.
Approaching the problem as an ordinal regression task using a differentiable
probabilistic model not only improves the interpretability of the results, but
also improves the accuracy of the model when compared to conventional deep
classification and regression architectures.
</p>
<a href="http://arxiv.org/abs/2103.03188" target="_blank">arXiv:2103.03188</a> [<a href="http://arxiv.org/pdf/2103.03188" target="_blank">pdf</a>]

<h2>Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v1 [cs.CV])</h2>
<h3>Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira</h3>
<p>Biological systems understand the world by simultaneously processing
high-dimensional inputs from modalities as diverse as vision, audition, touch,
proprioception, etc. The perception models used in deep learning on the other
hand are designed for individual modalities, often relying on domain-specific
assumptions such as the local grid structures exploited by virtually all
existing vision models. These priors introduce helpful inductive biases, but
also lock models to individual modalities. In this paper we introduce the
Perceiver - a model that builds upon Transformers and hence makes few
architectural assumptions about the relationship between its inputs, but that
also scales to hundreds of thousands of inputs, like ConvNets. The model
leverages an asymmetric attention mechanism to iteratively distill inputs into
a tight latent bottleneck, allowing it to scale to handle very large inputs. We
show that this architecture performs competitively or beyond strong,
specialized models on classification tasks across various modalities: images,
point clouds, audio, video and video+audio. The Perceiver obtains performance
comparable to ResNet-50 on ImageNet without convolutions and by directly
attending to 50,000 pixels. It also surpasses state-of-the-art results for all
modalities in AudioSet.
</p>
<a href="http://arxiv.org/abs/2103.03206" target="_blank">arXiv:2103.03206</a> [<a href="http://arxiv.org/pdf/2103.03206" target="_blank">pdf</a>]

<h2>Barlow Twins: Self-Supervised Learning via Redundancy Reduction. (arXiv:2103.03230v1 [cs.CV])</h2>
<h3>Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, St&#xe9;phane Deny</h3>
<p>Self-supervised learning (SSL) is rapidly closing the gap with supervised
methods on large computer vision benchmarks. A successful approach to SSL is to
learn representations which are invariant to distortions of the input sample.
However, a recurring issue with this approach is the existence of trivial
constant representations. Most current methods avoid such collapsed solutions
by careful implementation details. We propose an objective function that
naturally avoids such collapse by measuring the cross-correlation matrix
between the outputs of two identical networks fed with distorted versions of a
sample, and making it as close to the identity matrix as possible. This causes
the representation vectors of distorted versions of a sample to be similar,
while minimizing the redundancy between the components of these vectors. The
method is called Barlow Twins, owing to neuroscientist H. Barlow's
redundancy-reduction principle applied to a pair of identical networks. Barlow
Twins does not require large batches nor asymmetry between the network twins
such as a predictor network, gradient stopping, or a moving average on the
weight updates. It allows the use of very high-dimensional output vectors.
Barlow Twins outperforms previous methods on ImageNet for semi-supervised
classification in the low-data regime, and is on par with current state of the
art for ImageNet classification with a linear classifier head, and for transfer
tasks of classification and object detection.
</p>
<a href="http://arxiv.org/abs/2103.03230" target="_blank">arXiv:2103.03230</a> [<a href="http://arxiv.org/pdf/2103.03230" target="_blank">pdf</a>]

<h2>DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks. (arXiv:2103.03231v1 [cs.CV])</h2>
<h3>Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger</h3>
<p>The recent research explosion around Neural Radiance Fields (NeRFs) shows
that there is immense potential for implicitly storing scene and lighting
information in neural networks, e.g., for novel view generation. However, one
major limitation preventing the widespread use of NeRFs is the prohibitive
computational cost of excessive network evaluations along each view ray,
requiring dozens of petaFLOPS when aiming for real-time rendering on current
devices. We show that the number of samples required for each view ray can be
significantly reduced when local samples are placed around surfaces in the
scene. To this end, we propose a depth oracle network, which predicts ray
sample locations for each view ray with a single network evaluation. We show
that using a classification network around logarithmically discretized and
spherically warped depth values is essential to encode surface locations rather
than directly estimating depth. The combination of these techniques leads to
DONeRF, a dual network design with a depth oracle network as a first step and a
locally sampled shading network for ray accumulation. With our design, we
reduce the inference costs by up to 48x compared to NeRF. Using an
off-the-shelf inference API in combination with simple compute kernels, we are
the first to render raymarching-based neural representations at interactive
frame rates (15 frames per second at 800x800) on a single GPU. At the same
time, since we focus on the important parts of the scene around surfaces, we
achieve equal or better quality compared to NeRF.
</p>
<a href="http://arxiv.org/abs/2103.03231" target="_blank">arXiv:2103.03231</a> [<a href="http://arxiv.org/pdf/2103.03231" target="_blank">pdf</a>]

<h2>Anycost GANs for Interactive Image Synthesis and Editing. (arXiv:2103.03243v1 [cs.CV])</h2>
<h3>Ji Lin, Richard Zhang, Frieder Ganz, Song Han, Jun-Yan Zhu</h3>
<p>Generative adversarial networks (GANs) have enabled photorealistic image
synthesis and editing. However, due to the high computational cost of
large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the
results of a single edit on edge devices, prohibiting interactive user
experience. In this paper, we take inspirations from modern rendering software
and propose Anycost GAN for interactive natural image editing. We train the
Anycost GAN to support elastic resolutions and channels for faster image
generation at versatile speeds. Running subsets of the full generator produce
outputs that are perceptually similar to the full generator, making them a good
proxy for preview. By using sampling-based multi-resolution training,
adaptive-channel training, and a generator-conditioned discriminator, the
anycost generator can be evaluated at various configurations while achieving
better image quality compared to separately trained models. Furthermore, we
develop new encoder training and latent code optimization techniques to
encourage consistency between the different sub-generators during image
projection. Anycost GAN can be executed at various cost budgets (up to 10x
computation reduction) and adapt to a wide range of hardware and latency
requirements. When deployed on desktop CPUs and edge devices, our model can
provide perceptually similar previews at 6-12x speedup, enabling interactive
image editing. The code and demo are publicly available:
https://github.com/mit-han-lab/anycost-gan.
</p>
<a href="http://arxiv.org/abs/2103.03243" target="_blank">arXiv:2103.03243</a> [<a href="http://arxiv.org/pdf/2103.03243" target="_blank">pdf</a>]

<h2>Semantic Driven Multi-Camera Pedestrian Detection. (arXiv:1812.10779v2 [cs.CV] UPDATED)</h2>
<h3>Alejandro L&#xf3;pez-Cifuentes, Marcos Escudero-Vi&#xf1;olo, Jes&#xfa;s Besc&#xf3;s, Pablo Carballeira</h3>
<p>In the current worldwide situation, pedestrian detection has reemerged as a
pivotal tool for intelligent video-based systems aiming to solve tasks such as
pedestrian tracking, social distancing monitoring or pedestrian mass counting.
Pedestrian detection methods, even the top performing ones, are highly
sensitive to occlusions among pedestrians, which dramatically degrades their
performance in crowded scenarios. The generalization of multi-camera set-ups
permits to better confront occlusions by combining information from different
viewpoints. In this paper, we present a multi-camera approach to globally
combine pedestrian detections leveraging automatically extracted scene context.
Contrarily to the majority of the methods of the state-of-the-art, the proposed
approach is scene-agnostic, not requiring a tailored adaptation to the target
scenario\textemdash e.g., via fine-tunning. This noteworthy attribute does not
require \textit{ad hoc} training with labelled data, expediting the deployment
of the proposed method in real-world situations. Context information, obtained
via semantic segmentation, is used 1) to automatically generate a common Area
of Interest for the scene and all the cameras, avoiding the usual need of
manually defining it; and 2) to obtain detections for each camera by solving a
global optimization problem that maximizes coherence of detections both in each
2D image and in the 3D scene. This process yields tightly-fitted bounding boxes
that circumvent occlusions or miss-detections. Experimental results on five
publicly available datasets show that the proposed approach outperforms
state-of-the-art multi-camera pedestrian detectors, even some specifically
trained on the target scenario, signifying the versatility and robustness of
the proposed method without requiring ad-hoc annotations nor human-guided
configuration.
</p>
<a href="http://arxiv.org/abs/1812.10779" target="_blank">arXiv:1812.10779</a> [<a href="http://arxiv.org/pdf/1812.10779" target="_blank">pdf</a>]

<h2>Revisiting Knowledge Distillation via Label Smoothing Regularization. (arXiv:1909.11723v3 [cs.CV] UPDATED)</h2>
<h3>Li Yuan, Francis E.H.Tay, Guilin Li, Tao Wang, Jiashi Feng</h3>
<p>Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome
teacher model into a lightweight student model. Its success is generally
attributed to the privileged information on similarities among categories
provided by the teacher model, and in this sense, only strong teacher models
are deployed to teach weaker students in practice. In this work, we challenge
this common belief by following experimental observations: 1) beyond the
acknowledgment that the teacher can improve the student, the student can also
enhance the teacher significantly by reversing the KD procedure; 2) a
poorly-trained teacher with much lower accuracy than the student can still
improve the latter significantly. To explain these observations, we provide a
theoretical analysis of the relationships between KD and label smoothing
regularization. We prove that 1) KD is a type of learned label smoothing
regularization and 2) label smoothing regularization provides a virtual teacher
model for KD. From these results, we argue that the success of KD is not fully
due to the similarity information between categories from teachers, but also to
the regularization of soft targets, which is equally or even more important.

Based on these analyses, we further propose a novel Teacher-free Knowledge
Distillation (Tf-KD) framework, where a student model learns from itself or
manuallydesigned regularization distribution. The Tf-KD achieves comparable
performance with normal KD from a superior teacher, which is well applied when
a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be
directly deployed for training deep neural networks. Without any extra
computation cost, Tf-KD achieves up to 0.65\% improvement on ImageNet over
well-established baseline models, which is superior to label smoothing
regularization.
</p>
<a href="http://arxiv.org/abs/1909.11723" target="_blank">arXiv:1909.11723</a> [<a href="http://arxiv.org/pdf/1909.11723" target="_blank">pdf</a>]

<h2>W-PoseNet: Dense Correspondence Regularized Pixel Pair Pose Regression. (arXiv:1912.11888v2 [cs.CV] UPDATED)</h2>
<h3>Zelin Xu, Ke Chen, Kui Jia</h3>
<p>Solving 6D pose estimation is non-trivial to cope with intrinsic appearance
and shape variation and severe inter-object occlusion, and is made more
challenging in light of extrinsic large illumination changes and low quality of
the acquired data under an uncontrolled environment. This paper introduces a
novel pose estimation algorithm W-PoseNet, which densely regresses from input
data to 6D pose and also 3D coordinates in model space. In other words, local
features learned for pose regression in our deep network are regularized by
explicitly learning pixel-wise correspondence mapping onto 3D pose-sensitive
coordinates as an auxiliary task. Moreover, a sparse pair combination of
pixel-wise features and soft voting on pixel-pair pose predictions are designed
to improve robustness to inconsistent and sparse local features. Experiment
results on the popular YCB-Video and LineMOD benchmarks show that the proposed
W-PoseNet consistently achieves superior performance to the state-of-the-art
algorithms.
</p>
<a href="http://arxiv.org/abs/1912.11888" target="_blank">arXiv:1912.11888</a> [<a href="http://arxiv.org/pdf/1912.11888" target="_blank">pdf</a>]

<h2>Single-View 3D Object Reconstruction from Shape Priors in Memory. (arXiv:2003.03711v3 [cs.CV] UPDATED)</h2>
<h3>Shuo Yang, Min Xu, Haozhe Xie, Stuart Perry, Jiahao Xia</h3>
<p>Existing methods for single-view 3D object reconstruction directly learn to
transform image features into 3D representations. However, these methods are
vulnerable to images containing noisy backgrounds and heavy occlusions because
the extracted image features do not contain enough information to reconstruct
high-quality 3D shapes. Humans routinely use incomplete or noisy visual cues
from an image to retrieve similar 3D shapes from their memory and reconstruct
the 3D shape of an object. Inspired by this, we propose a novel method, named
Mem3D, that explicitly constructs shape priors to supplement the missing
information in the image. Specifically, the shape priors are in the forms of
"image-voxel" pairs in the memory network, which is stored by a well-designed
writing strategy during training. We also propose a voxel triplet loss function
that helps to retrieve the precise 3D shapes that are highly related to the
input image from shape priors. The LSTM-based shape encoder is introduced to
extract information from the retrieved 3D shapes, which are useful in
recovering the 3D shape of an object that is heavily occluded or in complex
environments. Experimental results demonstrate that Mem3D significantly
improves reconstruction quality and performs favorably against state-of-the-art
methods on the ShapeNet and Pix3D datasets.
</p>
<a href="http://arxiv.org/abs/2003.03711" target="_blank">arXiv:2003.03711</a> [<a href="http://arxiv.org/pdf/2003.03711" target="_blank">pdf</a>]

<h2>UniT: Unified Knowledge Transfer for Any-shot Object Detection and Segmentation. (arXiv:2006.07502v3 [cs.CV] UPDATED)</h2>
<h3>Siddhesh Khandelwal, Raghav Goyal, Leonid Sigal</h3>
<p>Methods for object detection and segmentation rely on large scale
instance-level annotations for training, which are difficult and time-consuming
to collect. Efforts to alleviate this look at varying degrees and quality of
supervision. Weakly-supervised approaches draw on image-level labels to build
detectors/segmentors, while zero/few-shot methods assume abundant
instance-level data for a set of base classes, and none to a few examples for
novel classes. This taxonomy has largely siloed algorithmic designs. In this
work, we aim to bridge this divide by proposing an intuitive and unified
semi-supervised model that is applicable to a range of supervision: from zero
to a few instance-level samples per novel class. For base classes, our model
learns a mapping from weakly-supervised to fully-supervised
detectors/segmentors. By learning and leveraging visual and lingual
similarities between the novel and base classes, we transfer those mappings to
obtain detectors/segmentors for novel classes; refining them with a few novel
class instance-level annotated samples, if available. The overall model is
end-to-end trainable and highly flexible. Through extensive experiments on
MS-COCO and Pascal VOC benchmark datasets we show improved performance in a
variety of settings.
</p>
<a href="http://arxiv.org/abs/2006.07502" target="_blank">arXiv:2006.07502</a> [<a href="http://arxiv.org/pdf/2006.07502" target="_blank">pdf</a>]

<h2>Failure-Resilient Coverage Maximization with Multiple Robots. (arXiv:2007.02204v3 [cs.RO] UPDATED)</h2>
<h3>Ishat E Rabban, Pratap Tokekar</h3>
<p>The task of maximizing coverage using multiple robots has several
applications such as surveillance, exploration, and environmental monitoring. A
major challenge of deploying such multi-robot systems in a practical scenario
is to ensure resilience against robot failures. A recent work introduced the
Resilient Coverage Maximization (RCM) problem where the goal is to maximize a
submodular coverage utility when the robots are subject to adversarial attacks
or failures. The RCM problem is known to be NP-hard. In this paper, we propose
two approximation algorithms for the RCM problem, namely, the Ordered Greedy
(OrG) and the Local Search (LS) algorithm. Both algorithms empirically
outperform the state-of-the-art solution in terms of accuracy and running time.
To demonstrate the effectiveness of our proposed solution, we empirically
compare our proposed algorithms with the existing solution and a brute force
optimal algorithm. We also perform a case study on the persistent monitoring
problem to show the applicability of our proposed algorithms in a practical
setting.
</p>
<a href="http://arxiv.org/abs/2007.02204" target="_blank">arXiv:2007.02204</a> [<a href="http://arxiv.org/pdf/2007.02204" target="_blank">pdf</a>]

<h2>Rethinking Recurrent Neural Networks and Other Improvements for Image Classification. (arXiv:2007.15161v3 [cs.CV] UPDATED)</h2>
<h3>Nguyen Huu Phong, Bernardete Ribeiro</h3>
<p>Over the long history of machine learning, which dates back several decades,
recurrent neural networks (RNNs) have been used mainly for sequential data and
time series and generally with 1D information. Even in some rare studies on 2D
images, these networks are used merely to learn and generate data sequentially
rather than for image recognition tasks. In this study, we propose integrating
an RNN as an additional layer when designing image recognition models. We also
develop end-to-end multimodel ensembles that produce expert predictions using
several models. In addition, we extend the training strategy so that our model
performs comparably to leading models and can even match the state-of-the-art
models on several challenging datasets (e.g., SVHN (0.99), Cifar-100 (0.9027)
and Cifar-10 (0.9852)). Moreover, our model sets a new record on the Surrey
dataset (0.949). The source code of the methods provided in this article is
available at https://github.com/leonlha/e2e-3m and this http URL
</p>
<a href="http://arxiv.org/abs/2007.15161" target="_blank">arXiv:2007.15161</a> [<a href="http://arxiv.org/pdf/2007.15161" target="_blank">pdf</a>]

<h2>Incomplete Descriptor Mining with Elastic Loss for Person Re-Identification. (arXiv:2008.04010v4 [cs.CV] UPDATED)</h2>
<h3>Hongchen Tan, Yuhao Bian, Huasheng Wang, Xiuping Liu, Baocai Yin</h3>
<p>In this paper, we propose a novel person Re-ID model, Consecutive Batch
DropBlock Network (CBDB-Net), to capture the attentive and robust person
descriptor for the person Re-ID task. The CBDB-Net contains two novel designs:
the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss (EL). In
the Consecutive Batch DropBlock Module (CBDBM), we firstly conduct uniform
partition on the feature maps. And then, we independently and continuously drop
each patch from top to bottom on the feature maps, which can output multiple
incomplete feature maps. In the training stage, these multiple incomplete
features can better encourage the Re-ID model to capture the robust person
descriptor for the Re-ID task. In the Elastic Loss (EL), we design a novel
weight control item to help the Re-ID model adaptively balance hard sample
pairs and easy sample pairs in the whole training process. Through an extensive
set of ablation studies, we verify that the Consecutive Batch DropBlock Module
(CBDBM) and the Elastic Loss (EL) each contribute to the performance boosts of
CBDB-Net. We demonstrate that our CBDB-Net can achieve the competitive
performance on the three standard person Re-ID datasets (the Market-1501, the
DukeMTMC-Re-ID, and the CUHK03 dataset), three occluded Person Re-ID datasets
(the Occluded DukeMTMC, the Partial-REID, and the Partial iLIDS dataset), and a
general image retrieval dataset (In-Shop Clothes Retrieval dataset).
</p>
<a href="http://arxiv.org/abs/2008.04010" target="_blank">arXiv:2008.04010</a> [<a href="http://arxiv.org/pdf/2008.04010" target="_blank">pdf</a>]

<h2>A Color Elastica Model for Vector-Valued Image Regularization. (arXiv:2008.08255v2 [cs.CV] UPDATED)</h2>
<h3>Hao Liu, Xue-Cheng Tai, Ron Kimmel, Roland Glowinski</h3>
<p>Models related to the Euler's elastica energy have proven to be useful for
many applications including image processing. Extending elastica models to
color images and multi-channel data is a challenging task, as stable and
consistent numerical solvers for these geometric models often involve high
order derivatives. Like the single channel Euler's elastica model and the total
variation (TV) models, geometric measures that involve high order derivatives
could help when considering image formation models that minimize elastic
properties. In the past, the Polyakov action from high energy physics has been
successfully applied to color image processing. Here, we introduce an addition
to the Polyakov action for color images that minimizes the color manifold
curvature. The color image curvature is computed by applying of the
Laplace-Beltrami operator to the color image channels. When reduced to
gray-scale images, while selecting appropriate scaling between space and color,
the proposed model minimizes the Euler's elastica operating on the image level
sets. Finding a minimizer for the proposed nonlinear geometric model is a
challenge we address in this paper. Specifically, we present an
operator-splitting method to minimize the proposed functional. The
non-linearity is decoupled by introducing three vector-valued and matrix-valued
variables. The problem is then converted into solving for the steady state of
an associated initial-value problem. The initial-value problem is time-split
into three fractional steps, such that each sub-problem has a closed form
solution, or can be solved by fast algorithms. The efficiency and robustness of
the proposed method are demonstrated by systematic numerical experiments.
</p>
<a href="http://arxiv.org/abs/2008.08255" target="_blank">arXiv:2008.08255</a> [<a href="http://arxiv.org/pdf/2008.08255" target="_blank">pdf</a>]

<h2>VarifocalNet: An IoU-aware Dense Object Detector. (arXiv:2008.13367v2 [cs.CV] UPDATED)</h2>
<h3>Haoyang Zhang, Ying Wang, Feras Dayoub, Niko S&#xfc;nderhauf</h3>
<p>Accurately ranking the vast number of candidate detections is crucial for
dense object detectors to achieve high performance. Prior work uses the
classification score or a combination of classification and predicted
localization scores to rank candidates. However, neither option results in a
reliable ranking, thus degrading detection performance. In this paper, we
propose to learn an Iou-aware Classification Score (IACS) as a joint
representation of object presence confidence and localization accuracy. We show
that dense object detectors can achieve a more accurate ranking of candidate
detections based on the IACS. We design a new loss function, named Varifocal
Loss, to train a dense object detector to predict the IACS, and propose a new
star-shaped bounding box feature representation for IACS prediction and
bounding box refinement. Combining these two new components and a bounding box
refinement branch, we build an IoU-aware dense object detector based on the
FCOS+ATSS architecture, that we call VarifocalNet or VFNet for short. Extensive
experiments on MS COCO show that our VFNet consistently surpasses the strong
baseline by $\sim$2.0 AP with different backbones. Our best model VFNet-X-1200
with Res2Net-101-DCN achieves a single-model single-scale AP of 55.1 on COCO
test-dev, which is state-of-the-art among various object detectors.Code is
available at https://github.com/hyz-xmaster/VarifocalNet .
</p>
<a href="http://arxiv.org/abs/2008.13367" target="_blank">arXiv:2008.13367</a> [<a href="http://arxiv.org/pdf/2008.13367" target="_blank">pdf</a>]

<h2>Information Bottleneck Constrained Latent Bidirectional Embedding for Zero-Shot Learning. (arXiv:2009.07451v3 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Lei Zhou, Xiao Bai, Lin Gu, Tatsuya Harada, Jun Zhou</h3>
<p>Zero-shot learning (ZSL) aims to recognize novel classes by transferring
semantic knowledge from seen classes to unseen classes. Though many ZSL methods
rely on a direct mapping between the visual and the semantic space, the
calibration deviation and hubness problem limit the generalization capability
to unseen classes. Recently emerged generative ZSL methods generate unseen
image features to transform ZSL into a supervised classification problem.
However, most generative models still suffer from the seen-unseen bias problem
as only seen data is used for training. To address these issues, we propose a
novel bidirectional embedding based generative model with a tight
visual-semantic coupling constraint. We learn a unified latent space that
calibrates the embedded parametric distributions of both visual and semantic
spaces. Since the embedding from high-dimensional visual features comprise much
non-semantic information, the alignment of visual and semantic in latent space
would inevitably been deviated. Therefore, we introduce information bottleneck
(IB) constraint to ZSL for the first time to preserve essential attribute
information during the mapping. Specifically, we utilize the uncertainty
estimation and the wake-sleep procedure to alleviate the feature noises and
improve model abstraction capability. In addition, our method can be easily
extended to transductive ZSL setting by generating labels for unseen images. We
then introduce a robust loss to solve this label noise problem. Extensive
experimental results show that our method outperforms the state-of-the-art
methods in different ZSL settings on most benchmark datasets. The code will be
available at https://github.com/osierboy/IBZSL.
</p>
<a href="http://arxiv.org/abs/2009.07451" target="_blank">arXiv:2009.07451</a> [<a href="http://arxiv.org/pdf/2009.07451" target="_blank">pdf</a>]

<h2>Semantic MapNet: Building Allocentric SemanticMaps and Representations from Egocentric Views. (arXiv:2010.01191v2 [cs.CV] UPDATED)</h2>
<h3>Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, Dhruv Batra</h3>
<p>We study the task of semantic mapping - specifically, an embodied agent (a
robot or an egocentric AI assistant) is given a tour of a new environment and
asked to build an allocentric top-down semantic map ("what is where?") from
egocentric observations of an RGB-D camera with known pose (via localization
sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists
of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame,
(2) a Feature Projector that projects egocentric features to appropriate
locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan
length x width x feature-dims that learns to accumulate projected egocentric
features, and (4) a Map Decoder that uses the memory tensor to produce semantic
top-down maps. SMNet combines the strengths of (known) projective camera
geometry and neural representation learning. On the task of semantic mapping in
the Matterport3D dataset, SMNet significantly outperforms competitive baselines
by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1
metrics. Moreover, we show how to use the neural episodic memories and
spatio-semantic allocentric representations build by SMNet for subsequent tasks
in the same space - navigating to objects seen during the tour("Find chair") or
answering questions about the space ("How many chairs did you see in the
house?").
</p>
<a href="http://arxiv.org/abs/2010.01191" target="_blank">arXiv:2010.01191</a> [<a href="http://arxiv.org/pdf/2010.01191" target="_blank">pdf</a>]

<h2>Fully Unsupervised Person Re-identification viaSelective Contrastive Learning. (arXiv:2010.07608v2 [cs.CV] UPDATED)</h2>
<h3>Bo Pang, Deming Zhai, Junjun Jiang, Xianming Liu</h3>
<p>Person re-identification (ReID) aims at searching the same identity person
among images captured by various cameras. Unsupervised person ReID attracts a
lot of attention recently, due to it works without intensive manual annotation
and thus shows great potential of adapting to new conditions. Representation
learning plays a critical role in unsupervised person ReID. In this work, we
propose a novel selective contrastive learning framework for unsupervised
feature learning. Specifically, different from traditional contrastive learning
strategies, we propose to use multiple positives and adaptively sampled
negatives for defining the contrastive loss, enabling to learn a feature
embedding model with stronger identity discriminative representation. Moreover,
we propose to jointly leverage global and local features to construct three
dynamic dictionaries, among which the global and local memory banks are used
for pairwise similarity computation and the mixture memory bank are used for
contrastive loss definition. Experimental results demonstrate the superiority
of our method in unsupervised person ReID compared with the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2010.07608" target="_blank">arXiv:2010.07608</a> [<a href="http://arxiv.org/pdf/2010.07608" target="_blank">pdf</a>]

<h2>PRIMAL2: Pathfinding via Reinforcement and Imitation Multi-Agent Learning -- Lifelong. (arXiv:2010.08184v3 [cs.RO] UPDATED)</h2>
<h3>Mehul Damani, Zhiyao Luo, Emerson Wenzel, Guillaume Sartoretti</h3>
<p>Multi-agent path finding (MAPF) is an indispensable component of large-scale
robot deployments in numerous domains ranging from airport management to
warehouse automation. In particular, this work addresses lifelong MAPF (LMAPF)
- an online variant of the problem where agents are immediately assigned a new
goal upon reaching their current one - in dense and highly structured
environments, typical of real-world warehouse operations. Effectively solving
LMAPF in such environments requires expensive coordination between agents as
well as frequent replanning abilities, a daunting task for existing coupled and
decoupled approaches alike. With the purpose of achieving considerable agent
coordination without any compromise on reactivity and scalability, we introduce
PRIMAL2, a distributed reinforcement learning framework for LMAPF where agents
learn fully decentralized policies to reactively plan paths online in a
partially observable world. We extend our previous work, which was effective in
low-density sparsely occupied worlds, to highly structured and constrained
worlds by identifying behaviors and conventions which improve implicit agent
coordination, and enable their learning through the construction of a novel
local agent observation and various training aids. We present extensive results
of PRIMAL2 in both MAPF and LMAPF environments and compare its performance to
state-of-the-art planners in terms of makespan and throughput. We show that
PRIMAL2 significantly surpasses our previous work and performs comparably to
these baselines, while allowing real-time re-planning and scaling up to 2048
agents.
</p>
<a href="http://arxiv.org/abs/2010.08184" target="_blank">arXiv:2010.08184</a> [<a href="http://arxiv.org/pdf/2010.08184" target="_blank">pdf</a>]

<h2>RigidFusion: Robot Localisation and Mapping in Environments with Large Dynamic Rigid Objects. (arXiv:2010.10841v2 [cs.RO] UPDATED)</h2>
<h3>Ran Long, Christian Rauch, Tianwei Zhang, Vladimir Ivan, Sethu Vijayakumar</h3>
<p>This work presents a novel RGB-D SLAM approach to simultaneously segment,
track and reconstruct the static background and large dynamic rigid objects
that can occlude major portions of the camera view. Previous approaches treat
dynamic parts of a scene as outliers and are thus limited to a small amount of
changes in the scene, or rely on prior information for all objects in the scene
to enable robust camera tracking. Here, we propose to treat all dynamic parts
as one rigid body and simultaneously segment and track both static and dynamic
components. We, therefore, enable simultaneous localisation and reconstruction
of both the static background and rigid dynamic components in environments
where dynamic objects cause large occlusion. We evaluate our approach on
multiple challenging scenes with large dynamic occlusion. The evaluation
demonstrates that our approach achieves better motion segmentation,
localisation and mapping without requiring prior knowledge of the dynamic
object's shape and appearance.
</p>
<a href="http://arxiv.org/abs/2010.10841" target="_blank">arXiv:2010.10841</a> [<a href="http://arxiv.org/pdf/2010.10841" target="_blank">pdf</a>]

<h2>EDNet: Efficient Disparity Estimation with Cost Volume Combination and Attention-based Spatial Residual. (arXiv:2010.13338v4 [cs.CV] UPDATED)</h2>
<h3>Songyan Zhang, Zhicheng Wang, Qiang Wang, Jinshuo Zhang, Gang Wei, Xiaowen Chu</h3>
<p>Existing state-of-the-art disparity estimation works mostly leverage the 4D
concatenation volume and construct a very deep 3D convolution neural network
(CNN) for disparity regression, which is inefficient due to the high memory
consumption and slow inference speed. In this paper, we propose a network named
EDNet for efficient disparity estimation. Firstly, we construct a combined
volume which incorporates contextual information from the squeezed
concatenation volume and feature similarity measurement from the correlation
volume. The combined volume can be next aggregated by 2D convolutions which are
faster and require less memory than 3D convolutions. Secondly, we propose an
attention-based spatial residual module to generate attention-aware residual
features. The attention mechanism is applied to provide intuitive spatial
evidence about inaccurate regions with the help of error maps at multiple
scales and thus improve the residual learning efficiency. Extensive experiments
on the Scene Flow and KITTI datasets show that EDNet outperforms the previous
3D CNN based works and achieves state-of-the-art performance with significantly
faster speed and less memory consumption.
</p>
<a href="http://arxiv.org/abs/2010.13338" target="_blank">arXiv:2010.13338</a> [<a href="http://arxiv.org/pdf/2010.13338" target="_blank">pdf</a>]

<h2>A Weakly-Supervised Semantic Segmentation Approach based on the Centroid Loss: Application to Quality Control and Inspection. (arXiv:2010.13433v3 [cs.CV] UPDATED)</h2>
<h3>Kai Yao, Alberto Ortiz, Francisco Bonnin-Pascual</h3>
<p>It is generally accepted that one of the critical parts of current vision
algorithms based on deep learning and convolutional neural networks is the
annotation of a sufficient number of images to achieve competitive performance.
This is particularly difficult for semantic segmentation tasks since the
annotation must be ideally generated at the pixel level. Weakly-supervised
semantic segmentation aims at reducing this cost by employing simpler
annotations that, hence, are easier, cheaper and quicker to produce. In this
paper, we propose and assess a new weakly-supervised semantic segmentation
approach making use of a novel loss function whose goal is to counteract the
effects of weak annotations. To this end, this loss function comprises several
terms based on partial cross-entropy losses, being one of them the Centroid
Loss. This term induces a clustering of the image pixels in the object classes
under consideration, whose aim is to improve the training of the segmentation
network by guiding the optimization. The performance of the approach is
evaluated against datasets from two different industry-related case studies:
while one involves the detection of instances of a number of different object
classes in the context of a quality control application, the other stems from
the visual inspection domain and deals with the localization of images areas
whose pixels correspond to scene surface points affected by a specific sort of
defect. The detection results that are reported for both cases show that,
despite the differences among them and the particular challenges, the use of
weak annotations do not prevent from achieving a competitive performance level
for both.
</p>
<a href="http://arxiv.org/abs/2010.13433" target="_blank">arXiv:2010.13433</a> [<a href="http://arxiv.org/pdf/2010.13433" target="_blank">pdf</a>]

<h2>Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule Networks. (arXiv:2011.08819v2 [cs.CV] UPDATED)</h2>
<h3>Nikhil Churamani, Sinan Kalkan, Hatice Gunes</h3>
<p>Most state-of-the-art approaches for Facial Action Unit (AU) detection rely
upon evaluating facial expressions from static frames, encoding a snapshot of
heightened facial activity. In real-world interactions, however, facial
expressions are usually more subtle and evolve in a temporal manner requiring
AU detection models to learn spatial as well as temporal information. In this
paper, we focus on both spatial and spatio-temporal features encoding the
temporal evolution of facial AU activation. For this purpose, we propose the
Action Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU
detection using both frame and sequence-level features. While at the
frame-level the capsule layers of AULA-Caps learn spatial feature primitives to
determine AU activations, at the sequence-level, it learns temporal
dependencies between contiguous frames by focusing on relevant spatio-temporal
segments in the sequence. The learnt feature capsules are routed together such
that the model learns to selectively focus more on spatial or spatio-temporal
information depending upon the AU lifecycle. The proposed model is evaluated on
the commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art
results on both the datasets.
</p>
<a href="http://arxiv.org/abs/2011.08819" target="_blank">arXiv:2011.08819</a> [<a href="http://arxiv.org/pdf/2011.08819" target="_blank">pdf</a>]

<h2>Creative Sketch Generation. (arXiv:2011.10039v2 [cs.CV] UPDATED)</h2>
<h3>Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick, Devi Parikh</h3>
<p>Sketching or doodling is a popular creative activity that people engage in.
However, most existing work in automatic sketch understanding or generation has
focused on sketches that are quite mundane. In this work, we introduce two
datasets of creative sketches -- Creative Birds and Creative Creatures --
containing 10k sketches each along with part annotations. We propose DoodlerGAN
-- a part-based Generative Adversarial Network (GAN) -- to generate unseen
compositions of novel part appearances. Quantitative evaluations as well as
human studies demonstrate that sketches generated by our approach are more
creative and of higher quality than existing approaches. In fact, in Creative
Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by
humans! Our code can be found at https://github.com/facebookresearch/DoodlerGAN
and a demo can be found at this http URL
</p>
<a href="http://arxiv.org/abs/2011.10039" target="_blank">arXiv:2011.10039</a> [<a href="http://arxiv.org/pdf/2011.10039" target="_blank">pdf</a>]

<h2>RIN: Textured Human Model Recovery and Imitation with a Single Image. (arXiv:2011.12024v3 [cs.CV] UPDATED)</h2>
<h3>Haoxi Ran, Guangfu Wang, Li Lu</h3>
<p>Human imitation has become topical recently, driven by GAN's ability to
disentangle human pose and body content. However, the latest methods hardly
focus on 3D information, and to avoid self-occlusion, a massive amount of input
images are needed. In this paper, we propose RIN, a novel volume-based
framework for reconstructing a textured 3D model from a single picture and
imitating a subject with the generated model. Specifically, to estimate most of
the human texture, we propose a U-Net-like front-to-back translation network.
With both front and back images input, the textured volume recovery module
allows us to color a volumetric human. A sequence of 3D poses then guides the
colored volume via Flowable Disentangle Networks as a volume-to-volume
translation task. To project volumes to a 2D plane during training, we design a
differentiable depth-aware renderer. Our experiments demonstrate that our
volume-based model is adequate for human imitation, and the back view can be
estimated reliably using our network. While prior works based on either 2D pose
or semantic map often fail for the unstable appearance of a human, our
framework can still produce concrete results, which are competitive to those
imagined from multi-view input.
</p>
<a href="http://arxiv.org/abs/2011.12024" target="_blank">arXiv:2011.12024</a> [<a href="http://arxiv.org/pdf/2011.12024" target="_blank">pdf</a>]

<h2>YieldNet: A Convolutional Neural Network for Simultaneous Corn and Soybean Yield Prediction Based on Remote Sensing Data. (arXiv:2012.03129v2 [cs.CV] UPDATED)</h2>
<h3>Saeed Khaki, Hieu Pham, Lizhi Wang</h3>
<p>Large scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout its growth state. Having this information allows stakeholders
the ability to make real-time decisions to maximize yield potential. Although
various models exist that predict yield from remote sensing data, there
currently does not exist an approach that can estimate yield for multiple crops
simultaneously, and thus leads to more accurate predictions. A model that
predicts yield of multiple crops and concurrently considers the interaction
between multiple crop's yield. We propose a new model called YieldNet which
utilizes a novel deep learning framework that uses transfer learning between
corn and soybean yield predictions by sharing the weights of the backbone
feature extractor. Additionally, to consider the multi-target response
variable, we propose a new loss function. Numerical results demonstrate that
our proposed method accurately predicts yield from one to four months before
the harvest, and is competitive to other state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.03129" target="_blank">arXiv:2012.03129</a> [<a href="http://arxiv.org/pdf/2012.03129" target="_blank">pdf</a>]

<h2>Efficient Semi-Supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency. (arXiv:2012.07042v3 [cs.CV] UPDATED)</h2>
<h3>Xiangde Luo, Wenjun Liao, Jieneng Chen, Tao Song, Yinan Chen, Shichuan Zhang, Nianyong Chen, Guotai Wang, Shaoting Zhang</h3>
<p>Gross Target Volume (GTV) segmentation plays an irreplaceable role in
radiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that
Convolutional Neural Networks (CNN) have achieved good performance for this
task, they rely on a large set of labeled images for training, which is
expensive and time-consuming to acquire. In this paper, we propose a novel
framework with Uncertainty Rectified Pyramid Consistency (URPC) regularization
for semi-supervised NPC GTV segmentation. Concretely, we extend a backbone
segmentation network to produce pyramid predictions at different scales. The
pyramid predictions network (PPNet) is supervised by the ground truth of
labeled images and a multi-scale consistency loss for unlabeled images,
motivated by the fact that prediction at different scales for the same input
should be similar and consistent. However, due to the different resolution of
these predictions, encouraging them to be consistent at each pixel directly has
low robustness and may lose some fine details. To address this problem, we
further design a novel uncertainty rectifying module to enable the framework to
gradually learn from meaningful and reliable consensual regions at different
scales. Experimental results on a dataset with 258 NPC MR images showed that
with only 10% or 20% images labeled, our method largely improved the
segmentation performance by leveraging the unlabeled images, and it also
outperformed five state-of-the-art semi-supervised segmentation methods.
Moreover, when only 50% images labeled, URPC achieved an average Dice score of
82.74% that was close to fully supervised learning.
</p>
<a href="http://arxiv.org/abs/2012.07042" target="_blank">arXiv:2012.07042</a> [<a href="http://arxiv.org/pdf/2012.07042" target="_blank">pdf</a>]

<h2>FPCC: Fast Point Cloud Clustering for Instance Segmentation. (arXiv:2012.14618v3 [cs.CV] UPDATED)</h2>
<h3>Yajun Xu, Shogo Arai, Diyi Liu, Fangzhou Lin, Kazuhiro Kosuge</h3>
<p>Instance segmentation is an important pre-processing task in numerous
real-world applications, such as robotics, autonomous vehicles, and
human-computer interaction. However, there has been little research on 3D point
cloud instance segmentation of bin-picking scenes in which multiple objects of
the same class are stacked together. Compared with the rapid development of
deep learning for two-dimensional (2D) image tasks, deep learning-based 3D
point cloud segmentation still has a lot of room for development. In such a
situation, distinguishing a large number of occluded objects of the same class
is a highly challenging problem. In a usual bin-picking scene, an object model
is known and the number of object type is one. Thus, the semantic information
can be ignored; instead, the focus is put on the segmentation of instances.
Based on this task requirement, we propose a network (FPCC-Net) that infers
feature centers of each instance and then clusters the remaining points to the
closest feature center in feature embedding space. FPCC-Net includes two
subnets, one for inferring the feature centers for clustering and the other for
describing features of each point. The proposed method is compared with
existing 3D point cloud and 2D segmentation methods in some bin-picking scenes.
It is shown that FPCC-Net improves average precision (AP) by about 40\% than
SGPN and can process about 60,000 points in about 0.8 [s].
</p>
<a href="http://arxiv.org/abs/2012.14618" target="_blank">arXiv:2012.14618</a> [<a href="http://arxiv.org/pdf/2012.14618" target="_blank">pdf</a>]

<h2>Temporal Contrastive Graph Learning for Video Action Recognition and Retrieval. (arXiv:2101.00820v7 [cs.CV] UPDATED)</h2>
<h3>Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin</h3>
<p>Attempt to fully discover the temporal diversity for self-supervised video
representation learning, this work takes advantage of the temporal dependencies
of videos and further proposes a novel self-supervised method named Temporal
Contrastive Graph Learning (TCGL). In contrast to the existing methods that
consider the temporal dependency from a single scale, our TCGL roots in a
hybrid graph contrastive learning strategy to regard the inter-snippet and
intra-snippet temporal dependencies as self-supervision signals for temporal
representation learning. To learn multi-scale temporal dependencies, the TCGL
integrates the prior knowledge about the frame and snippet orders into graph
structures, i.e., the intra-/inter- snippet temporal contrastive graph modules.
By randomly removing edges and masking node features of the intra-snippet
graphs or inter-snippet graphs, the TCGL can generate different correlated
graph views. Then, specific contrastive modules are designed to maximize the
agreement between node embeddings in different views. To learn the global
context representation and recalibrate the channel-wise features adaptively, we
introduce an adaptive video snippet order prediction module, which leverages
the relational knowledge among video snippets to predict the actual snippet
orders. Experimental results demonstrate the superiority of our TCGL over the
state-of-the-art methods on large-scale action recognition and video retrieval
benchmarks.
</p>
<a href="http://arxiv.org/abs/2101.00820" target="_blank">arXiv:2101.00820</a> [<a href="http://arxiv.org/pdf/2101.00820" target="_blank">pdf</a>]

<h2>DeepDT: Learning Geometry From Delaunay Triangulation for Surface Reconstruction. (arXiv:2101.10353v2 [cs.CV] UPDATED)</h2>
<h3>Yiming Luo, Zhenxing Mi, Wenbing Tao</h3>
<p>In this paper, a novel learning-based network, named DeepDT, is proposed to
reconstruct the surface from Delaunay triangulation of point cloud. DeepDT
learns to predict inside/outside labels of Delaunay tetrahedrons directly from
a point cloud and corresponding Delaunay triangulation. The local geometry
features are first extracted from the input point cloud and aggregated into a
graph deriving from the Delaunay triangulation. Then a graph filtering is
applied on the aggregated features in order to add structural regularization to
the label prediction of tetrahedrons. Due to the complicated spatial relations
between tetrahedrons and the triangles, it is impossible to directly generate
ground truth labels of tetrahedrons from ground truth surface. Therefore, we
propose a multilabel supervision strategy which votes for the label of a
tetrahedron with labels of sampling locations inside it. The proposed DeepDT
can maintain abundant geometry details without generating overly complex
surfaces , especially for inner surfaces of open scenes. Meanwhile, the
generalization ability and time consumption of the proposed method is
acceptable and competitive compared with the state-of-the-art methods.
Experiments demonstrate the superior performance of the proposed DeepDT.
</p>
<a href="http://arxiv.org/abs/2101.10353" target="_blank">arXiv:2101.10353</a> [<a href="http://arxiv.org/pdf/2101.10353" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filter and Smoothing for Inertial-Integrated Navigation. (arXiv:2102.12897v3 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Chi Guo, Shenyong You, Jianlang Hu, Jingnan Liu</h3>
<p>The error representation using the straight difference of two vectors in the
inertial navigation system may not be reasonable as it does not take the
direction difference into consideration. Therefore, we proposed to use the
$SE_2(3)$ matrix Lie group to represent the state of the inertial-integrated
navigation system which consequently leads to the common frame error
representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down local-level navigation frame and the earth-centered earth-fixed
frame, respectively, the corresponding extending Kalman filter (EKF), terms as
$SE_2(3)$-EKF has also been derived. It provides a new perspective on the
geometric EKF with a more sophisticated formula for the inertial-integrated
navigation system. Furthermore, we propose a $SE_2(3)$-based smoothing
algorithm based on the $SE_2(3)$-based EKF.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v2 [cs.CV] UPDATED)</h2>
<h3>Chengchao Shen, Youtan Yin, Xinchao Wang, Xubin LI, Jie Song, Mingli Song</h3>
<p>Generative Adversarial Networks (GANs) have demonstrated unprecedented
success in various image generation tasks. The encouraging results, however,
come at the price of a cumbersome training process, during which the generator
and discriminator are alternately updated in two stages. In this paper, we
investigate a general training scheme that enables training GANs efficiently in
only one stage. Based on the adversarial losses of the generator and
discriminator, we categorize GANs into two classes, Symmetric GANs and
Asymmetric GANs, and introduce a novel gradient decomposition method to unify
the two, allowing us to train both classes in one stage and hence alleviate the
training effort. Computational analysis and experimental results on several
datasets and various network architectures demonstrate that, the proposed
one-stage training scheme yields a solid 1.5$\times$ acceleration over
conventional training schemes, regardless of the network architectures of the
generator and discriminator. Furthermore, we show that the proposed method is
readily applicable to other adversarial-training scenarios, such as data-free
knowledge distillation. Our source code will be published soon.
</p>
<a href="http://arxiv.org/abs/2103.00430" target="_blank">arXiv:2103.00430</a> [<a href="http://arxiv.org/pdf/2103.00430" target="_blank">pdf</a>]

<h2>PENet: Towards Precise and Efficient Image Guided Depth Completion. (arXiv:2103.00783v2 [cs.CV] UPDATED)</h2>
<h3>Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, Xiaojin Gong</h3>
<p>Image guided depth completion is the task of generating a dense depth map
from a sparse depth map and a high quality image. In this task, how to fuse the
color and depth modalities plays an important role in achieving good
performance. This paper proposes a two-branch backbone that consists of a
color-dominant branch and a depth-dominant branch to exploit and fuse two
modalities thoroughly. More specifically, one branch inputs a color image and a
sparse depth map to predict a dense depth map. The other branch takes as inputs
the sparse depth map and the previously predicted depth map, and outputs a
dense depth map as well. The depth maps predicted from two branches are
complimentary to each other and therefore they are adaptively fused. In
addition, we also propose a simple geometric convolutional layer to encode 3D
geometric cues. The geometric encoded backbone conducts the fusion of different
modalities at multiple stages, leading to good depth completion results. We
further implement a dilated and accelerated CSPN++ to refine the fused depth
map efficiently. The proposed full model ranks 1st in the KITTI depth
completion online leaderboard at the time of submission. It also infers much
faster than most of the top ranked methods. The code of this work will be
available at https://github.com/JUGGHM/PENet_ICRA2021.
</p>
<a href="http://arxiv.org/abs/2103.00783" target="_blank">arXiv:2103.00783</a> [<a href="http://arxiv.org/pdf/2103.00783" target="_blank">pdf</a>]

<h2>Systematic Analysis and Removal of Circular Artifacts for StyleGAN. (arXiv:2103.01090v2 [cs.CV] UPDATED)</h2>
<h3>Way Tan, Bihan Wen, Xulei Yang</h3>
<p>StyleGAN is one of the state-of-the-art image generators which is well-known
for synthesizing high-resolution and hyper-realistic face images. Though images
generated by vanilla StyleGAN model are visually appealing, they sometimes
contain prominent circular artifacts which severely degrade the quality of
generated images. In this work, we provide a systematic investigation on how
those circular artifacts are formed by studying the functionalities of
different stages of vanilla StyleGAN architecture, with both mechanism analysis
and extensive experiments. The key modules of vanilla StyleGAN that promote
such undesired artifacts are highlighted. Our investigation also explains why
the artifacts are usually circular, relatively small and rarely split into 2 or
more parts. Besides, we propose a simple yet effective solution to remove the
prominent circular artifacts for vanilla StyleGAN, by applying a novel
pixel-instance normalization (PIN) layer.
</p>
<a href="http://arxiv.org/abs/2103.01090" target="_blank">arXiv:2103.01090</a> [<a href="http://arxiv.org/pdf/2103.01090" target="_blank">pdf</a>]

<h2>Scalable Scene Flow from Point Clouds in the Real World. (arXiv:2103.01306v2 [cs.CV] UPDATED)</h2>
<h3>Philipp Jund, Chris Sweeney, Nichola Abdo, Zhifeng Chen, Jonathon Shlens</h3>
<p>Autonomous vehicles operate in highly dynamic environments necessitating an
accurate assessment of which aspects of a scene are moving and where they are
moving to. A popular approach to 3D motion estimation -- termed scene flow --
is to employ 3D point cloud data from consecutive LiDAR scans, although such
approaches have been limited by the small size of real-world, annotated LiDAR
data. In this work, we introduce a new large scale benchmark for scene flow
based on the Waymo Open Dataset. The dataset is $\sim$1,000$\times$ larger than
previous real-world datasets in terms of the number of annotated frames and is
derived from the corresponding tracked 3D objects. We demonstrate how previous
works were bounded based on the amount of real LiDAR data available, suggesting
that larger datasets are required to achieve state-of-the-art predictive
performance. Furthermore, we show how previous heuristics for operating on
point clouds such as artificial down-sampling heavily degrade performance,
motivating a new class of models that are tractable on the full point cloud. To
address this issue, we introduce the model architecture FastFlow3D that
provides real time inference on the full point cloud. Finally, we demonstrate
that this problem is amenable to techniques from semi-supervised learning by
highlighting open problems for generalizing methods for predicting motion on
unlabeled objects. We hope that this dataset may provide new opportunities for
developing real world scene flow systems and motivate a new class of machine
learning problems.
</p>
<a href="http://arxiv.org/abs/2103.01306" target="_blank">arXiv:2103.01306</a> [<a href="http://arxiv.org/pdf/2103.01306" target="_blank">pdf</a>]

<h2>Augmentation Strategies for Learning with Noisy Labels. (arXiv:2103.02130v2 [cs.CV] UPDATED)</h2>
<h3>Kento Nishi, Yi Ding, Alex Rich, Tobias H&#xf6;llerer</h3>
<p>Imperfect labels are ubiquitous in real-world datasets. Several recent
successful methods for training deep neural networks (DNNs) robust to label
noise have used two primary techniques: filtering samples based on loss during
a warm-up phase to curate an initial set of cleanly labeled samples, and using
the output of a network as a pseudo-label for subsequent loss calculations. In
this paper, we evaluate different augmentation strategies for algorithms
tackling the "learning with noisy labels" problem. We propose and examine
multiple augmentation strategies and evaluate them using synthetic datasets
based on CIFAR-10 and CIFAR-100, as well as on the real-world dataset
Clothing1M. Due to several commonalities in these algorithms, we find that
using one set of augmentations for loss modeling tasks and another set for
learning is the most effective, improving results on the state-of-the-art and
other previous methods. Furthermore, we find that applying augmentation during
the warm-up period can negatively impact the loss convergence behavior of
correctly versus incorrectly labeled samples. We introduce this augmentation
strategy to the state-of-the-art technique and demonstrate that we can improve
performance across all evaluated noise levels. In particular, we improve
accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in
absolute accuracy and we also improve performance on the real-world dataset
Clothing1M.

(* equal contribution)
</p>
<a href="http://arxiv.org/abs/2103.02130" target="_blank">arXiv:2103.02130</a> [<a href="http://arxiv.org/pdf/2103.02130" target="_blank">pdf</a>]

<h2>Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control. (arXiv:2103.02142v2 [cs.RO] UPDATED)</h2>
<h3>Jacopo Panerati (1 and 2), Hehui Zheng (3), SiQi Zhou (1 and 2), James Xu (1), Amanda Prorok (3), Angela P. Schoellig (1 and 2) ((1) University of Toronto Institute for Aerospace Studies, (2) Vector Institute for Artificial Intelligence, (3) University of Cambridge)</h3>
<p>Robotic simulators are crucial for academic research and education as well as
the development of safety-critical applications. Reinforcement learning
environments -- simple simulations coupled with a problem specification in the
form of a reward function -- are also important to standardize the development
(and benchmarking) of learning algorithms. Yet, full-scale simulators typically
lack portability and parallelizability. Vice versa, many reinforcement learning
environments trade-off realism for high sample throughputs in toy-like
problems. While public data sets have greatly benefited deep learning and
computer vision, we still lack the software tools to simultaneously develop --
and fairly compare -- control theory and reinforcement learning approaches. In
this paper, we propose an open-source OpenAI Gym-like environment for multiple
quadcopters based on the Bullet physics engine. Its multi-agent and vision
based reinforcement learning interfaces, as well as the support of realistic
collisions and aerodynamic effects, make it, to the best of our knowledge, a
first of its kind. We demonstrate its use through several examples, either for
control (trajectory tracking with PID control, multi-robot flight with
downwash, etc.) or reinforcement learning (single and multi-agent stabilization
tasks), hoping to inspire future research that combines control theory and
machine learning.
</p>
<a href="http://arxiv.org/abs/2103.02142" target="_blank">arXiv:2103.02142</a> [<a href="http://arxiv.org/pdf/2103.02142" target="_blank">pdf</a>]

<h2>Inertial based Integration with Transformed INS Mechanization in Earth Frame. (arXiv:2103.02229v2 [cs.RO] UPDATED)</h2>
<h3>Lubin Chang, Jingbo Di, Fangjun Qin</h3>
<p>This paper proposes to use a newly-derived transformed inertial navigation
system (INS) mechanization to fuse INS with other complimentary sensors.
Through formulating the attitude, velocity and position as one group state of
group of double direct spatial isometries, the transformed INS mechanization
has proven to be group affine, which opens door to log-linearity and filtering
consistency. In order to make use of the transformed INS mechanization in
inertial based applications, both the right and left error state models are
derived. The INS/GPS and INS/Odometer integration are investigated as two
representatives of inertial based applications. Some application aspects of the
derived error state models in the two applications are presented, which include
how to select the error state model, initialization of the SE2(3) based error
state covariance and feedback correction corresponding to the error state
definitions. Land vehicle experiments are conducted to evaluate the performance
of the derived error state models. It is shown that the most striking
superiority of using the derived error state models is their ability to handle
the large initial attitude misalignments, which is just the result of
log-linearity property of the derived error state models. Therefore, the
derived error state models can be used in the so-called attitude alignment for
the two applications.
</p>
<a href="http://arxiv.org/abs/2103.02229" target="_blank">arXiv:2103.02229</a> [<a href="http://arxiv.org/pdf/2103.02229" target="_blank">pdf</a>]

<h2>MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions. (arXiv:2103.02243v2 [cs.CV] UPDATED)</h2>
<h3>Haixu Wu, Zhiyu Yao, Mingsheng Long, Jianmin Wang</h3>
<p>This paper tackles video prediction from a new dimension of predicting
spacetime-varying motions that are incessantly changing across both space and
time. Prior methods mainly capture the temporal state transitions but overlook
the complex spatiotemporal variations of the motion itself, making them
difficult to adapt to ever-changing motions. We observe that physical world
motions can be decomposed into transient variation and motion trend, while the
latter can be regarded as the accumulation of previous motions. Thus,
simultaneously capturing the transient variation and the motion trend is the
key to make spacetime-varying motions more predictable. Based on these
observations, we propose the MotionRNN framework, which can capture the complex
variations within motions and adapt to spacetime-varying scenarios. MotionRNN
has two main contributions. The first is that we design the MotionGRU unit,
which can model the transient variation and motion trend in a unified way. The
second is that we apply the MotionGRU to RNN-based predictive models and
indicate a new flexible video prediction architecture with a Motion Highway
that can significantly improve the ability to predict changeable motions and
avoid motion vanishing for stacked multiple-layer predictive models. With high
flexibility, this framework can adapt to a series of models for deterministic
spatiotemporal prediction. Our MotionRNN can yield significant improvements on
three challenging benchmarks for video prediction with spacetime-varying
motions.
</p>
<a href="http://arxiv.org/abs/2103.02243" target="_blank">arXiv:2103.02243</a> [<a href="http://arxiv.org/pdf/2103.02243" target="_blank">pdf</a>]

<h2>ID-Unet: Iterative Soft and Hard Deformation for View Synthesis. (arXiv:2103.02264v2 [cs.CV] UPDATED)</h2>
<h3>Mingyu Yin, Li Sun, Qingli Li</h3>
<p>View synthesis is usually done by an autoencoder, in which the encoder maps a
source view image into a latent content code, and the decoder transforms it
into a target view image according to the condition. However, the source
contents are often not well kept in this setting, which leads to unnecessary
changes during the view translation. Although adding skipped connections, like
Unet, alleviates the problem, but it often causes the failure on the view
conformity. This paper proposes a new architecture by performing the
source-to-target deformation in an iterative way. Instead of simply
incorporating the features from multiple layers of the encoder, we design soft
and hard deformation modules, which warp the encoder features to the target
view at different resolutions, and give results to the decoder to complement
the details. Particularly, the current warping flow is not only used to align
the feature of the same resolution, but also as an approximation to coarsely
deform the high resolution feature. Then the residual flow is estimated and
applied in the high resolution, so that the deformation is built up in the
coarse-to-fine fashion. To better constrain the model, we synthesize a rough
target view image based on the intermediate flows and their warped features.
The extensive ablation studies and the final results on two different data sets
show the effectiveness of the proposed model.
</p>
<a href="http://arxiv.org/abs/2103.02264" target="_blank">arXiv:2103.02264</a> [<a href="http://arxiv.org/pdf/2103.02264" target="_blank">pdf</a>]

<h2>Event-based Synthetic Aperture Imaging with a Hybrid Network. (arXiv:2103.02376v2 [cs.CV] UPDATED)</h2>
<h3>Xiang Zhang, Liao Wei, Lei Yu, Wen Yang, Gui-Song Xia</h3>
<p>Synthetic aperture imaging (SAI) is able to achieve the see through effect by
blurring out the off-focus foreground occlusions and reconstructing the
in-focus occluded targets from multi-view images. However, very dense
occlusions and extreme lighting conditions may bring significant disturbances
to SAI based on conventional frame-based cameras, leading to performance
degeneration. To address these problems, we propose a novel SAI system based on
the event camera which can produce asynchronous events with extremely low
latency and high dynamic range. Thus, it can eliminate the interference of
dense occlusions by measuring with almost continuous views, and simultaneously
tackle the over/under exposure problems. To reconstruct the occluded targets,
we propose a hybrid encoder-decoder network composed of spiking neural networks
(SNNs) and convolutional neural networks (CNNs). In the hybrid network, the
spatio-temporal information of the collected events is first encoded by SNN
layers, and then transformed to the visual image of the occluded targets by a
style-transfer CNN decoder. Through experiments, the proposed method shows
remarkable performance in dealing with very dense occlusions and extreme
lighting conditions, and high quality visual images can be reconstructed using
pure event data.
</p>
<a href="http://arxiv.org/abs/2103.02376" target="_blank">arXiv:2103.02376</a> [<a href="http://arxiv.org/pdf/2103.02376" target="_blank">pdf</a>]

<h2>Self-Distribution Binary Neural Networks. (arXiv:2103.02394v2 [cs.CV] UPDATED)</h2>
<h3>Ping Xue, Yang Lu, Jingfei Chang, Xing Wei, Zhen Wei</h3>
<p>In this work, we study the binary neural networks (BNNs) of which both the
weights and activations are binary (i.e., 1-bit representation). Feature
representation is critical for deep neural networks, while in BNNs, the
features only differ in signs. Prior work introduces scaling factors into
binary weights and activations to reduce the quantization error and effectively
improves the classification accuracy of BNNs. However, the scaling factors not
only increase the computational complexity of networks, but also make no sense
to the signs of binary features. To this end, Self-Distribution Binary Neural
Network (SD-BNN) is proposed. Firstly, we utilize Activation Self Distribution
(ASD) to adaptively adjust the sign distribution of activations, thereby
improve the sign differences of the outputs of the convolution. Secondly, we
adjust the sign distribution of weights through Weight Self Distribution (WSD)
and then fine-tune the sign distribution of the outputs of the convolution.
Extensive experiments on CIFAR-10 and ImageNet datasets with various network
structures show that the proposed SD-BNN consistently outperforms the
state-of-the-art (SOTA) BNNs (e.g., achieves 92.5% on CIFAR-10 and 66.5% on
ImageNet with ResNet-18) with less computation cost. Code is available at
https://github.com/ pingxue-hfut/SD-BNN.
</p>
<a href="http://arxiv.org/abs/2103.02394" target="_blank">arXiv:2103.02394</a> [<a href="http://arxiv.org/pdf/2103.02394" target="_blank">pdf</a>]

<h2>$S^3$: Learnable Sparse Signal Superdensity for Guided Depth Estimation. (arXiv:2103.02396v2 [cs.CV] UPDATED)</h2>
<h3>Yu-Kai Huang, Yueh-Cheng Liu, Tsung-Han Wu, Hung-Ting Su, Yu-Cheng Chang, Tsung-Lin Tsou, Yu-An Wang, Winston H. Hsu</h3>
<p>Dense Depth estimation plays a key role in multiple applications such as
robotics, 3D reconstruction, and augmented reality. While sparse signal, e.g.,
LiDAR and Radar, has been leveraged as guidance for enhancing dense depth
estimation, the improvement is limited due to its low density and imbalanced
distribution. To maximize the utility from the sparse source, we propose $S^3$
technique, which expands the depth value from sparse cues while estimating the
confidence of expanded region. The proposed $S^3$ can be applied to various
guided depth estimation approaches and trained end-to-end at different stages,
including input, cost volume and output. Extensive experiments demonstrate the
effectiveness, robustness, and flexibility of the $S^3$ technique on LiDAR and
Radar signal.
</p>
<a href="http://arxiv.org/abs/2103.02396" target="_blank">arXiv:2103.02396</a> [<a href="http://arxiv.org/pdf/2103.02396" target="_blank">pdf</a>]

<h2>Dynamic Fusion Module Evolves Drivable Area and Road Anomaly Detection: A Benchmark and Algorithms. (arXiv:2103.02433v2 [cs.CV] UPDATED)</h2>
<h3>Hengli Wang, Rui Fan, Yuxiang Sun, Ming Liu</h3>
<p>Joint detection of drivable areas and road anomalies is very important for
mobile robots. Recently, many semantic segmentation approaches based on
convolutional neural networks (CNNs) have been proposed for pixel-wise drivable
area and road anomaly detection. In addition, some benchmark datasets, such as
KITTI and Cityscapes, have been widely used. However, the existing benchmarks
are mostly designed for self-driving cars. There lacks a benchmark for ground
mobile robots, such as robotic wheelchairs. Therefore, in this paper, we first
build a drivable area and road anomaly detection benchmark for ground mobile
robots, evaluating the existing state-of-the-art single-modal and data-fusion
semantic segmentation CNNs using six modalities of visual features.
Furthermore, we propose a novel module, referred to as the dynamic fusion
module (DFM), which can be easily deployed in existing data-fusion networks to
fuse different types of visual features effectively and efficiently. The
experimental results show that the transformed disparity image is the most
informative visual feature and the proposed DFM-RTFNet outperforms the
state-of-the-arts. Additionally, our DFM-RTFNet achieves competitive
performance on the KITTI road benchmark. Our benchmark is publicly available at
https://sites.google.com/view/gmrb.
</p>
<a href="http://arxiv.org/abs/2103.02433" target="_blank">arXiv:2103.02433</a> [<a href="http://arxiv.org/pdf/2103.02433" target="_blank">pdf</a>]

