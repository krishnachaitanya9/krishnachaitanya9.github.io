---
title: Latest Deep Learning Papers
date: 2021-03-09 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (115 Articles)</h1>
<h2>Use square root affinity to regress labels in semantic segmentation. (arXiv:2103.04990v1 [cs.CV])</h2>
<h3>Lumeng Cao, Zhouwang Yang</h3>
<p>Semantic segmentation is a basic but non-trivial task in computer vision.
Many previous work focus on utilizing affinity patterns to enhance segmentation
networks. Most of these studies use the affinity matrix as a kind of feature
fusion weights, which is part of modules embedded in the network, such as
attention models and non-local models. In this paper, we associate affinity
matrix with labels, exploiting the affinity in a supervised way. Specifically,
we utilize the label to generate a multi-scale label affinity matrix as a
structural supervision, and we use a square root kernel to compute a non-local
affinity matrix on output layers. With such two affinities, we define a novel
loss called Affinity Regression loss (AR loss), which can be an auxiliary loss
providing pair-wise similarity penalty. Our model is easy to train and adds
little computational burden without run-time inference. Extensive experiments
on NYUv2 dataset and Cityscapes dataset demonstrate that our proposed method is
sufficient in promoting semantic segmentation networks.
</p>
<a href="http://arxiv.org/abs/2103.04990" target="_blank">arXiv:2103.04990</a> [<a href="http://arxiv.org/pdf/2103.04990" target="_blank">pdf</a>]

<h2>Advances in Inference and Representation for Simultaneous Localization and Mapping. (arXiv:2103.05041v1 [cs.RO])</h2>
<h3>David M. Rosen, Kevin J. Doherty, Antonio Teran Espinoza, John J. Leonard</h3>
<p>Simultaneous localization and mapping (SLAM) is the process of constructing a
global model of an environment from local observations of it; this is a
foundational capability for mobile robots, supporting such core functions as
planning, navigation, and control. This article reviews recent progress in
SLAM, focusing on advances in the expressive capacity of the environmental
models used in SLAM systems (representation) and the performance of the
algorithms used to estimate these models from data (inference). A prominent
theme of recent SLAM research is the pursuit of environmental representations
(including learned representations) that go beyond the classical attributes of
geometry and appearance to model properties such as hierarchical organization,
affordance, dynamics, and semantics; these advances equip autonomous agents
with a more comprehensive understanding of the world, enabling more versatile
and intelligent operation. A second major theme is a revitalized interest in
the mathematical properties of the SLAM estimation problem itself (including
its computational and information-theoretic performance limits); this work has
led to the development of novel classes of certifiable and robust inference
methods that dramatically improve the reliability of SLAM systems in real-world
operation. We survey these advances with an emphasis on their ramifications for
achieving robust, long-duration autonomy, and conclude with a discussion of
open challenges and a perspective on future research directions.
</p>
<a href="http://arxiv.org/abs/2103.05041" target="_blank">arXiv:2103.05041</a> [<a href="http://arxiv.org/pdf/2103.05041" target="_blank">pdf</a>]

<h2>LCDNet: Deep Loop Closure Detection for LiDAR SLAM based on Unbalanced Optimal Transport. (arXiv:2103.05056v1 [cs.RO])</h2>
<h3>Daniele Cattaneo, Matteo Vaghi, Abhinav Valada</h3>
<p>Loop closure detection is an essential component of Simultaneous Localization
and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over
the years, several deep learning approaches have been proposed to address this
task, however their performance has been subpar compared to handcrafted
techniques, especially while dealing with reverse loops. In this paper, we
introduce the novel LCDNet that effectively detects loop closures in LiDAR
point clouds by simultaneously identifying previously visited places and
estimating the 6-DoF relative transformation between the current scan and the
map. LCDNet is composed of a shared encoder, a place recognition head that
extracts global descriptors, and a relative pose head that estimates the
transformation between two point clouds. We introduce a novel relative pose
head based on the unbalanced optimal transport theory that we implement in a
differentiable manner to allow for end-to-end training. Extensive evaluations
of LCDNet on multiple real-world autonomous driving datasets show that our
approach outperforms state-of-the-art techniques by a large margin even while
dealing with reverse loops. Moreover, we integrate our proposed loop closure
detection approach into a LiDAR SLAM library to provide a complete mapping
system and demonstrate the generalization ability using different sensor setup
in an unseen city.
</p>
<a href="http://arxiv.org/abs/2103.05056" target="_blank">arXiv:2103.05056</a> [<a href="http://arxiv.org/pdf/2103.05056" target="_blank">pdf</a>]

<h2>Offboard 3D Object Detection from Point Cloud Sequences. (arXiv:2103.05073v1 [cs.CV])</h2>
<h3>Charles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, Dragomir Anguelov</h3>
<p>While current 3D object recognition research mostly focuses on the real-time,
onboard scenario, there are many offboard use cases of perception that are
largely under-explored, such as using machines to automatically generate
high-quality 3D labels. Existing 3D object detectors fail to satisfy the
high-quality requirement for offboard uses due to the limited input and speed
constraints. In this paper, we propose a novel offboard 3D object detection
pipeline using point cloud sequence data. Observing that different frames
capture complementary views of objects, we design the offboard detector to make
use of the temporal points through both multi-frame object detection and novel
object-centric refinement models. Evaluated on the Waymo Open Dataset, our
pipeline named 3D Auto Labeling shows significant gains compared to the
state-of-the-art onboard detectors and our offboard baselines. Its performance
is even on par with human labels verified through a human label study. Further
experiments demonstrate the application of auto labels for semi-supervised
learning and provide extensive analysis to validate various design choices.
</p>
<a href="http://arxiv.org/abs/2103.05073" target="_blank">arXiv:2103.05073</a> [<a href="http://arxiv.org/pdf/2103.05073" target="_blank">pdf</a>]

<h2>How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D Lines. (arXiv:2103.05086v1 [cs.CV])</h2>
<h3>Kunal Chelani, Fredrik Kahl, Torsten Sattler</h3>
<p>Visual localization is the problem of estimating the camera pose of a given
image with respect to a known scene. Visual localization algorithms are a
fundamental building block in advanced computer vision applications, including
Mixed and Virtual Reality systems. Many algorithms used in practice represent
the scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D
matches between a query image and the 3D points for camera pose estimation. As
recently shown, image details can be accurately recovered from SfM point clouds
by translating renderings of the sparse point clouds to images. To address the
resulting potential privacy risks for user-generated content, it was recently
proposed to lift point clouds to line clouds by replacing 3D points by randomly
oriented 3D lines passing through these points. The resulting representation is
unintelligible to humans and effectively prevents point cloud-to-image
translation. This paper shows that a significant amount of information about
the 3D scene geometry is preserved in these line clouds, allowing us to
(approximately) recover the 3D point positions and thus to (approximately)
recover image content. Our approach is based on the observation that the
closest points between lines can yield a good approximation to the original 3D
points. Code is available at https://github.com/kunalchelani/Line2Point.
</p>
<a href="http://arxiv.org/abs/2103.05086" target="_blank">arXiv:2103.05086</a> [<a href="http://arxiv.org/pdf/2103.05086" target="_blank">pdf</a>]

<h2>Learning Connectivity for Data Distribution in Robot Teams. (arXiv:2103.05091v1 [cs.RO])</h2>
<h3>Ekaterina Tolstaya, Landon Butler, Daniel Mox, James Paulos, Vijay Kumar, Alejandro Ribeiro</h3>
<p>Many algorithms for control of multi-robot teams operate under the assumption
that low-latency, global state information necessary to coordinate agent
actions can readily be disseminated among the team. However, in harsh
environments with no existing communication infrastructure, robots must form
ad-hoc networks, forcing the team to operate in a distributed fashion. To
overcome this challenge, we propose a task-agnostic, decentralized, low-latency
method for data distribution in ad-hoc networks using Graph Neural Networks
(GNN). Our approach enables multi-agent algorithms based on global state
information to function by ensuring it is available at each robot. To do this,
agents glean information about the topology of the network from packet
transmissions and feed it to a GNN running locally which instructs the agent
when and where to transmit the latest state information. We train the
distributed GNN communication policies via reinforcement learning using the
average Age of Information as the reward function and show that it improves
training stability compared to task-specific reward functions. Our approach
performs favorably compared to industry-standard methods for data distribution
such as random flooding and round robin. We also show that the trained policies
generalize to larger teams of both static and mobile agents.
</p>
<a href="http://arxiv.org/abs/2103.05091" target="_blank">arXiv:2103.05091</a> [<a href="http://arxiv.org/pdf/2103.05091" target="_blank">pdf</a>]

<h2>Subjective and Objective Quality Assessment of Mobile Gaming Video. (arXiv:2103.05099v1 [cs.CV])</h2>
<h3>Shaoguo Wen, Suiyi Ling, Junle Wang, Ximing Chen, Lizhi Fang, Yanqing Jing, Patrick Le Callet</h3>
<p>Nowadays, with the vigorous expansion and development of gaming video
streaming techniques and services, the expectation of users, especially the
mobile phone users, for higher quality of experience is also growing swiftly.
As most of the existing research focuses on traditional video streaming, there
is a clear lack of both subjective study and objective quality models that are
tailored for quality assessment of mobile gaming content. To this end, in this
study, we first present a brand new Tencent Gaming Video dataset containing
1293 mobile gaming sequences encoded with three different codecs. Second, we
propose an objective quality framework, namely Efficient hard-RAnk Quality
Estimator (ERAQUE), that is equipped with (1) a novel hard pairwise ranking
loss, which forces the model to put more emphasis on differentiating similar
pairs; (2) an adapted model distillation strategy, which could be utilized to
compress the proposed model efficiently without causing significant performance
drop. Extensive experiments demonstrate the efficiency and robustness of our
model.
</p>
<a href="http://arxiv.org/abs/2103.05099" target="_blank">arXiv:2103.05099</a> [<a href="http://arxiv.org/pdf/2103.05099" target="_blank">pdf</a>]

<h2>Learning Hierarchical Integration of Foveal and Peripheral Vision for Vergence Control by Active Efficient Coding. (arXiv:2103.05100v1 [cs.CV])</h2>
<h3>Zhetuo Zhao, Jochen Triesch, Bertram E. Shi</h3>
<p>The active efficient coding (AEC) framework parsimoniously explains the joint
development of visual processing and eye movements, e.g., the emergence of
binocular disparity selective neurons and fusional vergence, the disjunctive
eye movements that align left and right eye images. Vergence can be driven by
information in both the fovea and periphery, which play complementary roles.
The high resolution fovea can drive precise short range movements. The lower
resolution periphery supports coarser long range movements. The fovea and
periphery may also contain conflicting information, e.g. due to objects at
different depths. While past AEC models did integrate peripheral and foveal
information, they did not explicitly take into account these characteristics.
We propose here a two-level hierarchical approach that does. The bottom level
generates different vergence actions from foveal and peripheral regions. The
top level selects one. We demonstrate that the hierarchical approach performs
better than prior approaches in realistic environments, exhibiting better
alignment and less oscillation.
</p>
<a href="http://arxiv.org/abs/2103.05100" target="_blank">arXiv:2103.05100</a> [<a href="http://arxiv.org/pdf/2103.05100" target="_blank">pdf</a>]

<h2>Video Action Recognition Using spatio-temporal optical flow video frames. (arXiv:2103.05101v1 [cs.CV])</h2>
<h3>Aytekin Nebisoy, Saber Malekzadeh</h3>
<p>Recognizing human actions based on videos has became one of the most popular
areas of research in computer vision in recent years. This area has many
applications such as surveillance, robotics, health care, video search and
human-computer interaction. There are many problems associated with recognizing
human actions in videos such as cluttered backgrounds, obstructions, viewpoints
variation, execution speed and camera movement. A large number of methods have
been proposed to solve the problems. This paper focus on spatial and temporal
pattern recognition for the classification of videos using Deep Neural
Networks. This model takes RGB images and Optical Flow as input data and
outputs an action class number. The final recognition accuracy was about 94%.
</p>
<a href="http://arxiv.org/abs/2103.05101" target="_blank">arXiv:2103.05101</a> [<a href="http://arxiv.org/pdf/2103.05101" target="_blank">pdf</a>]

<h2>Self-supervised Multisensor Change Detection. (arXiv:2103.05102v1 [cs.CV])</h2>
<h3>Sudipan Saha, Patrick Ebel, Xiao Xiang Zhu</h3>
<p>Multimodal and multisensor data analysis is a long-standing goal in machine
learning research. In this paper we revisit multisensor analysis in context of
self-supervised change detection in bi-temporal satellite images. Most change
detection methods assume that pre-change and post-change images are acquired by
the same sensor. However, in many real-life scenarios, e.g., natural disaster,
it is more practical to use the latest available images before and after the
occurrence of incidence, which may be acquired using different sensors. In
particular, we are interested in the combination of the images acquired by
optical and Synthetic Aperture Radar (SAR) sensors. While optical images are
like the natural images dealt in computer vision, SAR images appear vastly
different even when capturing the same scene. Adding to this, change detection
methods are often constrained to use only target image-pair, no labeled data,
and no additional unlabeled data. Such constraints limit the scope of
traditional supervised machine learning and unsupervised generative approaches
for multi-sensor change detection. Recent rapid development of self-supervised
learning methods has shown that some of them can even work with only few
images. Motivated by this, in this work we propose a method for multi-sensor
change detection using only the unlabeled target bi-temporal images that are
used for training a network in self-supervised fashion by using deep clustering
and contrastive learning. The trained network is evaluated on multi-modal
satellite data showing change and the benefits of our self-supervised approach
are demonstrated.
</p>
<a href="http://arxiv.org/abs/2103.05102" target="_blank">arXiv:2103.05102</a> [<a href="http://arxiv.org/pdf/2103.05102" target="_blank">pdf</a>]

<h2>Image Captioning using Multiple Transformers for Self-Attention Mechanism. (arXiv:2103.05103v1 [cs.CV])</h2>
<h3>Farrukh Olimov, Shikha Dubey, Labina Shrestha, Tran Trung Tin, Moongu Jeon</h3>
<p>Real-time image captioning, along with adequate precision, is the main
challenge of this research field. The present work, Multiple Transformers for
Self-Attention Mechanism (MTSM), utilizes multiple transformers to address
these problems. The proposed algorithm, MTSM, acquires region proposals using a
transformer detector (DETR). Consequently, MTSM achieves the self-attention
mechanism by transferring these region proposals and their visual and
geometrical features through another transformer and learns the objects' local
and global interconnections. The qualitative and quantitative results of the
proposed algorithm, MTSM, are shown on the MSCOCO dataset.
</p>
<a href="http://arxiv.org/abs/2103.05103" target="_blank">arXiv:2103.05103</a> [<a href="http://arxiv.org/pdf/2103.05103" target="_blank">pdf</a>]

<h2>New Methods for Detecting Concentric Objects With High Accuracy. (arXiv:2103.05104v1 [cs.CV])</h2>
<h3>Ali A. Al-Sharadqah, Lorenzo Rull</h3>
<p>Fitting concentric geometric objects to digitized data is an important
problem in many areas such as iris detection, autonomous navigation, and
industrial robotics operations. There are two common approaches to fitting
geometric shapes to data: the geometric (iterative) approach and algebraic
(non-iterative) approach. The geometric approach is a nonlinear iterative
method that minimizes the sum of the squares of Euclidean distances of the
observed points to the ellipses and regarded as the most accurate method, but
it needs a good initial guess to improve the convergence rate. The algebraic
approach is based on minimizing the algebraic distances with some constraints
imposed on parametric space. Each algebraic method depends on the imposed
constraint, and it can be solved with the aid of the generalized eigenvalue
problem. Only a few methods in literature were developed to solve the problem
of concentric ellipses. Here we study the statistical properties of existing
methods by firstly establishing a general mathematical and statistical
framework for this problem. Using rigorous perturbation analysis, we derive the
variances and biasedness of each method under the small-sigma model. We also
develop new estimators, which can be used as reliable initial guesses for other
iterative methods. Then we compare the performance of each method according to
their theoretical accuracy. Not only do our methods described here outperform
other existing non-iterative methods, they are also quite robust against large
noise. These methods and their practical performances are assessed by a series
of numerical experiments on both synthetic and real data.
</p>
<a href="http://arxiv.org/abs/2103.05104" target="_blank">arXiv:2103.05104</a> [<a href="http://arxiv.org/pdf/2103.05104" target="_blank">pdf</a>]

<h2>Urdu Handwritten Text Recognition Using ResNet18. (arXiv:2103.05105v1 [cs.CV])</h2>
<h3>Muhammad Kashif</h3>
<p>Handwritten text recognition is an active research area in the field of deep
learning and artificial intelligence to convert handwritten text into
machine-understandable. A lot of work has been done for other languages,
especially for English, but work for the Urdu language is very minimal due to
the cursive nature of Urdu characters. The need for Urdu HCR systems is
increasing because of the advancement of technology. In this paper, we propose
a ResNet18 model for handwritten text recognition using Urdu Nastaliq
Handwritten Dataset (UNHD) which contains 3,12000 words written by 500
candidates.
</p>
<a href="http://arxiv.org/abs/2103.05105" target="_blank">arXiv:2103.05105</a> [<a href="http://arxiv.org/pdf/2103.05105" target="_blank">pdf</a>]

<h2>Risk Prediction on Traffic Accidents using a Compact Neural Model for Multimodal Information Fusion over Urban Big Data. (arXiv:2103.05107v1 [cs.CV])</h2>
<h3>Wenshan Wang, Su Yang, Weishan Zhang</h3>
<p>Predicting risk map of traffic accidents is vital for accident prevention and
early planning of emergency response. Here, the challenge lies in the
multimodal nature of urban big data. We propose a compact neural ensemble model
to alleviate overfitting in fusing multimodal features and develop some new
features such as fractal measure of road complexity in satellite images, taxi
flows, POIs, and road width and connectivity in OpenStreetMap. The solution is
more promising in performance than the baseline methods and the single-modality
data based solutions. After visualization from a micro view, the visual
patterns of the scenes related to high and low risk are revealed, providing
lessons for future road design. From city point of view, the predicted risk map
is close to the ground truth, and can act as the base in optimizing spatial
configuration of resources for emergency response, and alarming signs. To the
best of our knowledge, it is the first work to fuse visual and spatio-temporal
features in traffic accident prediction while advances to bridge the gap
between data mining based urban computing and computer vision based urban
perception.
</p>
<a href="http://arxiv.org/abs/2103.05107" target="_blank">arXiv:2103.05107</a> [<a href="http://arxiv.org/pdf/2103.05107" target="_blank">pdf</a>]

<h2>Believe The HiPe: Hierarchical Perturbation for Fast and Robust Explanation of Black Box Models. (arXiv:2103.05108v1 [cs.CV])</h2>
<h3>Jessica Cooper, Ognjen Arandjelovi&#x107;, David Harrison</h3>
<p>Understanding the predictions made by Artificial Intelligence (AI) systems is
becoming more and more important as deep learning models are used for
increasingly complex and high-stakes tasks. Saliency mapping - an easily
interpretable visual attribution method - is one important tool for this, but
existing formulations are limited by either computational cost or architectural
constraints. We therefore propose Hierarchical Perturbation, a very fast and
completely model-agnostic method for explaining model predictions with robust
saliency maps. Using standard benchmarks and datasets, we show that our
saliency maps are of competitive or superior quality to those generated by
existing black-box methods - and are over 20x faster to compute.
</p>
<a href="http://arxiv.org/abs/2103.05108" target="_blank">arXiv:2103.05108</a> [<a href="http://arxiv.org/pdf/2103.05108" target="_blank">pdf</a>]

<h2>Highly Efficient Representation and Active Learning Framework for Imbalanced Data and its Application to COVID-19 X-Ray Classification. (arXiv:2103.05109v1 [cs.CV])</h2>
<h3>Heng Hao, Sima Didari, Jae Oh Woo, Hankyu Moon, Patrick Bangert</h3>
<p>We propose a highly data-efficient classification and active learning
framework for classifying chest X-rays. It is based on (1) unsupervised
representation learning of a CNN (Convolutional Neural Network) and (2) the GP
(Gaussian Process) method. The unsupervised representation learning employs
self-supervision that does not require class labels, and the learned features
are proven to achieve label-efficient classification. GP is a kernel-based
Bayesian approach that also leads to data-efficient predictions with the added
benefit of estimating each decision's uncertainty. Our novel framework combines
these two elements in sequence to achieve highly data and label efficient
classifications. Moreover, both elements are less sensitive to the prevalent
and challenging class imbalance issue, thanks to the (1) feature learned
without labels and (2) the Bayesian nature of GP. The GP-provided uncertainty
estimates enable active learning by ranking samples based on the uncertainty
and selectively labeling samples showing higher uncertainty. We apply this
novel combination to the data-deficient and severely imbalanced case of
COVID-19 chest X-ray classification. We demonstrate that only $\sim 10\%$ of
the labeled data is needed to reach the accuracy from training all available
labels. Its application to the COVID-19 data in a fully supervised
classification scenario shows that our model, with a generic ResNet backbone,
outperforms (COVID-19 case by 4\%) the state-of-the-art model with a highly
tuned architecture. Our model architecture and proposed framework are general
and straightforward to apply to a broader class of datasets, with expected
success.
</p>
<a href="http://arxiv.org/abs/2103.05109" target="_blank">arXiv:2103.05109</a> [<a href="http://arxiv.org/pdf/2103.05109" target="_blank">pdf</a>]

<h2>Web Table Classification based on Visual Features. (arXiv:2103.05110v1 [cs.CV])</h2>
<h3>Babette B&#xfc;hler, Heiko Paulheim</h3>
<p>Tables on the web constitute a valuable data source for many applications,
like factual search and knowledge base augmentation. However, as genuine tables
containing relational knowledge only account for a small proportion of tables
on the web, reliable genuine web table classification is a crucial first step
of table extraction. Previous works usually rely on explicit feature
construction from the HTML code. In contrast, we propose an approach for web
table classification by exploiting the full visual appearance of a table, which
works purely by applying a convolutional neural network on the rendered image
of the web table. Since these visual features can be extracted automatically,
our approach circumvents the need for explicit feature construction. A new hand
labeled gold standard dataset containing HTML source code and images for 13,112
tables was generated for this task. Transfer learning techniques are applied to
well known VGG16 and ResNet50 architectures. The evaluation of CNN image
classification with fine tuned ResNet50 (F1 93.29%) shows that this approach
achieves results comparable to previous solutions using explicitly defined HTML
code based features. By combining visual and explicit features, an F-measure of
93.70% can be achieved by Random Forest classification, which beats current
state of the art methods.
</p>
<a href="http://arxiv.org/abs/2103.05110" target="_blank">arXiv:2103.05110</a> [<a href="http://arxiv.org/pdf/2103.05110" target="_blank">pdf</a>]

<h2>Application of Transfer Learning to Sign Language Recognition using an Inflated 3D Deep Convolutional Neural Network. (arXiv:2103.05111v1 [cs.CV])</h2>
<h3>Roman T&#xf6;ngi</h3>
<p>Sign language is the primary language for people with a hearing loss. Sign
language recognition (SLR) is the automatic recognition of sign language, which
represents a challenging problem for computers, though some progress has been
made recently using deep learning. Huge amounts of data are generally required
to train deep learning models. However, corresponding datasets are missing for
the majority of sign languages. Transfer learning is a technique to utilize a
related task with an abundance of data available to help solve a target task
lacking sufficient data. Transfer learning has been applied highly successfully
in computer vision and natural language processing. However, much less research
has been conducted in the field of SLR. This paper investigates how effectively
transfer learning can be applied to isolated SLR using an inflated 3D
convolutional neural network as the deep learning architecture. Transfer
learning is implemented by pre-training a network on the American Sign Language
dataset MS-ASL and subsequently fine-tuning it separately on three different
sizes of the German Sign Language dataset SIGNUM. The results of the
experiments give clear empirical evidence that transfer learning can be
effectively applied to isolated SLR. The accuracy performances of the networks
applying transfer learning increased substantially by up to 21% as compared to
the baseline models that were not pre-trained on the MS-ASL dataset.
</p>
<a href="http://arxiv.org/abs/2103.05111" target="_blank">arXiv:2103.05111</a> [<a href="http://arxiv.org/pdf/2103.05111" target="_blank">pdf</a>]

<h2>Deep Learning Based Decision Support for Medicine -- A Case Study on Skin Cancer Diagnosis. (arXiv:2103.05112v1 [cs.CV])</h2>
<h3>Adriano Lucieri, Andreas Dengel, Sheraz Ahmed</h3>
<p>Early detection of skin cancers like melanoma is crucial to ensure high
chances of survival for patients. Clinical application of Deep Learning
(DL)-based Decision Support Systems (DSS) for skin cancer screening has the
potential to improve the quality of patient care. The majority of work in the
medical AI community focuses on a diagnosis setting that is mainly relevant for
autonomous operation. Practical decision support should, however, go beyond
plain diagnosis and provide explanations. This paper provides an overview of
works towards explainable, DL-based decision support in medical applications
with the example of skin cancer diagnosis from clinical, dermoscopic and
histopathologic images. Analysis reveals that comparably little attention is
payed to the explanation of histopathologic skin images and that current work
is dominated by visual relevance maps as well as dermoscopic feature
identification. We conclude that future work should focus on meeting the
stakeholder's cognitive concepts, providing exhaustive explanations that
combine global and local approaches and leverage diverse modalities. Moreover,
the possibility to intervene and guide models in case of misbehaviour is
identified as a major step towards successful deployment of AI as DL-based DSS
and beyond.
</p>
<a href="http://arxiv.org/abs/2103.05112" target="_blank">arXiv:2103.05112</a> [<a href="http://arxiv.org/pdf/2103.05112" target="_blank">pdf</a>]

<h2>Deep Neural Networks for the Assessment of Surgical Skills: A Systematic Review. (arXiv:2103.05113v1 [cs.CV])</h2>
<h3>Erim Yanik, Xavier Intes, Uwe Kruger, Pingkun Yan, David Miller, Brian Van Voorst, Basiel Makled, Jack Norfleet, Suvranu De</h3>
<p>Surgical training in medical school residency programs has followed the
apprenticeship model. The learning and assessment process is inherently
subjective and time-consuming. Thus, there is a need for objective methods to
assess surgical skills. Here, we use the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines to systematically
survey the literature on the use of Deep Neural Networks for automated and
objective surgical skill assessment, with a focus on kinematic data as putative
markers of surgical competency. There is considerable recent interest in deep
neural networks (DNN) due to the availability of powerful algorithms, multiple
datasets, some of which are publicly available, as well as efficient
computational hardware to train and host them. We have reviewed 530 papers, of
which we selected 25 for this systematic review. Based on this review, we
concluded that DNNs are powerful tools for automated, objective surgical skill
assessment using both kinematic and video data. The field would benefit from
large, publicly available, annotated datasets that are representative of the
surgical trainee and expert demographics and multimodal data beyond kinematics
and videos.
</p>
<a href="http://arxiv.org/abs/2103.05113" target="_blank">arXiv:2103.05113</a> [<a href="http://arxiv.org/pdf/2103.05113" target="_blank">pdf</a>]

<h2>Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles. (arXiv:2103.05121v1 [cs.CV])</h2>
<h3>Jevgenij Gamper, Nasir Rajpoot</h3>
<p>We present ARCH, a computational pathology (CP) multiple instance captioning
dataset to facilitate dense supervision of CP tasks. Existing CP datasets focus
on narrow tasks; ARCH on the other hand contains dense diagnostic and
morphological descriptions for a range of stains, tissue types and pathologies.
Using intrinsic dimensionality estimation, we show that ARCH is the only CP
dataset to (ARCH-)rival its computer vision analog MS-COCO Captions. We
conjecture that an encoder pre-trained on dense image captions learns
transferable representations for most CP tasks. We support the conjecture with
evidence that ARCH representation transfers to a variety of pathology sub-tasks
better than ImageNet features or representations obtained via self-supervised
or multi-task learning on pathology images alone. We release our best model and
invite other researchers to test it on their CP tasks.
</p>
<a href="http://arxiv.org/abs/2103.05121" target="_blank">arXiv:2103.05121</a> [<a href="http://arxiv.org/pdf/2103.05121" target="_blank">pdf</a>]

<h2>Contemplating real-world object classification. (arXiv:2103.05137v1 [cs.CV])</h2>
<h3>Ali Borji</h3>
<p>Deep object recognition models have been very successful over benchmark
datasets such as ImageNet. How accurate and robust are they to distribution
shifts arising from natural and synthetic variations in datasets? Prior
research on this problem has primarily focused on ImageNet variations (e.g.,
ImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies,
we take a different approach. Specifically, we reanalyze the ObjectNet dataset
recently proposed by Barbu et al. containing objects in daily life situations.
They showed a dramatic performance drop of the state of the art object
recognition models on this dataset. Due to the importance and implications of
their results regarding the generalization ability of deep models, we take a
second look at their analysis. We find that applying deep models to the
isolated objects, rather than the entire scene as is done in the original
paper, results in around 20-30% performance improvement. Relative to the
numbers reported in Barbu et al., around 10-15% of the performance loss is
recovered, without any test time data augmentation. Despite this gain, however,
we conclude that deep models still suffer drastically on the ObjectNet dataset.
We also investigate the robustness of models against synthetic image
perturbations such as geometric transformations (e.g., scale, rotation,
translation), natural image distortions (e.g., impulse noise, blur) as well as
adversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting
the object area as much as possible (i.e., from the entire image to the
bounding box to the segmentation mask) leads to consistent improvement in
accuracy and robustness.
</p>
<a href="http://arxiv.org/abs/2103.05137" target="_blank">arXiv:2103.05137</a> [<a href="http://arxiv.org/pdf/2103.05137" target="_blank">pdf</a>]

<h2>Benchmarking Off-The-Shelf Solutions to Robotic Assembly Tasks. (arXiv:2103.05140v1 [cs.RO])</h2>
<h3>Wenzhao Lian, Tim Kelch, Dirk Holz, Adam Norton, Stefan Schaal</h3>
<p>In recent years, many learning based approaches have been studied to realize
robotic manipulation and assembly tasks, often including vision and
force/tactile feedback. However, it remains frequently unclear what is the
baseline state-of-the-art performance and what are the bottleneck problems. In
this work, we evaluate some off-the-shelf (OTS) industrial solutions on a
recently introduced benchmark, the National Institute of Standards and
Technology (NIST) Assembly Task Boards. A set of assembly tasks are introduced
and baseline methods are provided to understand their intrinsic difficulty.
Multiple sensor-based robotic solutions are then evaluated, including hybrid
force/motion control and 2D/3D pattern matching algorithms. An end-to-end
integrated solution that accomplishes the tasks is also provided. The results
and findings throughout the study reveal a few noticeable factors that impede
the adoptions of the OTS solutions: expertise dependent, limited applicability,
lack of interoperability, no scene awareness or error recovery mechanisms, and
high cost. This paper also provides a first attempt of an objective benchmark
performance on the NIST Assembly Task Boards as a reference comparison for
future works on this problem.
</p>
<a href="http://arxiv.org/abs/2103.05140" target="_blank">arXiv:2103.05140</a> [<a href="http://arxiv.org/pdf/2103.05140" target="_blank">pdf</a>]

<h2>Orientation to Pose: Continuum Robots Shape Sensing Based on Piecewise Polynomial Curvature Model. (arXiv:2103.05150v1 [cs.RO])</h2>
<h3>Hao Cheng, Hongji Shang, Bin Lan, Houde Liu, Xueqian Wang, Bin Liang</h3>
<p>Continuum robots are typically slender and flexible with infinite freedoms in
theory, which poses a challenge for their control and application. The shape
sensing of continuum robots is vital to realise accuracy control. This letter
proposed a novel general real-time shape sensing framework of continuum robots
based on the piecewise polynomial curvature (PPC) kinematics model. We
illustrate the coupling between orientation and position at any given location
of the continuum robots. Further, the coupling relation could be bridged by the
PPC kinematics. Therefore, we propose to estimate the shape of continuum robots
through orientation estimation, using the off-the-shelf orientation sensors,
e.g., IMUs, mounted on certain locations. The approach gives a valuable
framework to the shape sensing of continuum robots, universality, accuracy and
convenience. The accuracy of the general approach is verified in the
experiments of multi-type physical prototypes.
</p>
<a href="http://arxiv.org/abs/2103.05150" target="_blank">arXiv:2103.05150</a> [<a href="http://arxiv.org/pdf/2103.05150" target="_blank">pdf</a>]

<h2>Knowledge Evolution in Neural Networks. (arXiv:2103.05152v1 [cs.CV])</h2>
<h3>Ahmed Taha, Abhinav Shrivastava, Larry Davis</h3>
<p>Deep learning relies on the availability of a large corpus of data (labeled
or unlabeled). Thus, one challenging unsettled question is: how to train a deep
network on a relatively small dataset? To tackle this question, we propose an
evolution-inspired training approach to boost performance on relatively small
datasets. The knowledge evolution (KE) approach splits a deep network into two
hypotheses: the fit-hypothesis and the reset-hypothesis. We iteratively evolve
the knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for
multiple generations. This approach not only boosts performance, but also
learns a slim network with a smaller inference cost. KE integrates seamlessly
with both vanilla and residual convolutional networks. KE reduces both
overfitting and the burden for data collection.

We evaluate KE on various network architectures and loss functions. We
evaluate KE using relatively small datasets (e.g., CUB-200) and randomly
initialized deep networks. KE achieves an absolute 21% improvement margin on a
state-of-the-art baseline. This performance improvement is accompanied by a
relative 73% reduction in inference cost. KE achieves state-of-the-art results
on classification and metric learning benchmarks. Code available at
this http URL
</p>
<a href="http://arxiv.org/abs/2103.05152" target="_blank">arXiv:2103.05152</a> [<a href="http://arxiv.org/pdf/2103.05152" target="_blank">pdf</a>]

<h2>Deep Learning-based High-precision Depth Map Estimation from Missing Viewpoints for 360 Degree Digital Holography. (arXiv:2103.05158v1 [cs.CV])</h2>
<h3>Hakdong Kim, Heonyeong Lim, Minkyu Jee, Yurim Lee, Jisoo Jeong, Kyudam Choi, MinSung Yoon, Cheongwon Kim</h3>
<p>In this paper, we propose a novel, convolutional neural network model to
extract highly precise depth maps from missing viewpoints, especially well
applicable to generate holographic 3D contents. The depth map is an essential
element for phase extraction which is required for synthesis of
computer-generated hologram (CGH). The proposed model called the HDD Net uses
MSE for the better performance of depth map estimation as loss function, and
utilizes the bilinear interpolation in up sampling layer with the Relu as
activation function. We design and prepare a total of 8,192 multi-view images,
each resolution of 640 by 360 for the deep learning study. The proposed model
estimates depth maps through extracting features, up sampling. For quantitative
assessment, we compare the estimated depth maps with the ground truths by using
the PSNR, ACC, and RMSE. We also compare the CGH patterns made from estimated
depth maps with ones made from ground truths. Furthermore, we demonstrate the
experimental results to test the quality of estimated depth maps through
directly reconstructing holographic 3D image scenes from the CGHs.
</p>
<a href="http://arxiv.org/abs/2103.05158" target="_blank">arXiv:2103.05158</a> [<a href="http://arxiv.org/pdf/2103.05158" target="_blank">pdf</a>]

<h2>Anomalous entities detection using a cascade of deep learning models. (arXiv:2103.05164v1 [cs.CV])</h2>
<h3>Hamza Riaz, Muhammad Uzair, Habib Ullah</h3>
<p>Human actions that do not conform to usual behavior are considered as
anomalous and such actors are called anomalous entities. Detection of anomalous
entities using visual data is a challenging problem in computer vision. This
paper presents a new approach to detect anomalous entities in complex
situations of examination halls. The proposed method uses a cascade of deep
convolutional neural network models. In the first stage, we apply a pretrained
model of human pose estimation on frames of videos to extract key feature
points of body. Patches extracted from each key point are utilized in the
second stage to build a densely connected deep convolutional neural network
model for detecting anomalous entities. For experiments we collect a video
database of students undertaking examination in a hall. Our results show that
the proposed method can detect anomalous entities and warrant unusual behavior
with high accuracy.
</p>
<a href="http://arxiv.org/abs/2103.05164" target="_blank">arXiv:2103.05164</a> [<a href="http://arxiv.org/pdf/2103.05164" target="_blank">pdf</a>]

<h2>Sequential Learning on Liver Tumor Boundary Semantics and Prognostic Biomarker Mining. (arXiv:2103.05170v1 [cs.CV])</h2>
<h3>Jieneng Chen, Ke Yan, Yu-Dong Zhang, Youbao Tang, Xun Xu, Shuwen Sun, Qiuping Liu, Lingyun Huang, Jing Xiao, Alan L. Yuille, Ya Zhang, Le Lu</h3>
<p>The boundary of tumors (hepatocellular carcinoma, or HCC) contains rich
semantics: capsular invasion, visibility, smoothness, folding and protuberance,
etc. Capsular invasion on tumor boundary has proven to be clinically correlated
with the prognostic indicator, microvascular invasion (MVI). Investigating
tumor boundary semantics has tremendous clinical values. In this paper, we
propose the first and novel computational framework that disentangles the task
into two components: spatial vertex localization and sequential semantic
classification. (1) A HCC tumor segmentor is built for tumor mask boundary
extraction, followed by polar transform representing the boundary with radius
and angle. Vertex generator is used to produce fixed-length boundary vertices
where vertex features are sampled on the corresponding spatial locations. (2)
The sampled deep vertex features with positional embedding are mapped into a
sequential space and decoded by a multilayer perceptron (MLP) for semantic
classification. Extensive experiments on tumor capsule semantics demonstrate
the effectiveness of our framework. Mining the correlation between the boundary
semantics and MVI status proves the feasibility to integrate this boundary
semantics as a valid HCC prognostic biomarker.
</p>
<a href="http://arxiv.org/abs/2103.05170" target="_blank">arXiv:2103.05170</a> [<a href="http://arxiv.org/pdf/2103.05170" target="_blank">pdf</a>]

<h2>Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning. (arXiv:2103.05187v1 [cs.CV])</h2>
<h3>Mingjie Sun, Jimin Xiao, Eng Gee Lim</h3>
<p>In this paper, we are tackling the proposal-free referring expression
grounding task, aiming at localizing the target object according to a query
sentence, without relying on off-the-shelf object proposals. Existing
proposal-free methods employ a query-image matching branch to select the
highest-score point in the image feature map as the target box center, with its
width and height predicted by another branch. Such methods, however, fail to
utilize the contextual relation between the target and reference objects, and
lack interpretability on its reasoning procedure. To solve these problems, we
propose an iterative shrinking mechanism to localize the target, where the
shrinking direction is decided by a reinforcement learning agent, with all
contents within the current image patch comprehensively considered. Beside, the
sequential shrinking process enables to demonstrate the reasoning about how to
iteratively find the target. Experiments show that the proposed method boosts
the accuracy by 4.32% against the previous state-of-the-art (SOTA) method on
the RefCOCOg dataset, where query sentences are long and complex, with many
targets referred by other reference objects.
</p>
<a href="http://arxiv.org/abs/2103.05187" target="_blank">arXiv:2103.05187</a> [<a href="http://arxiv.org/pdf/2103.05187" target="_blank">pdf</a>]

<h2>Generative Transition Mechanism to Image-to-Image Translation via Encoded Transformation. (arXiv:2103.05193v1 [cs.CV])</h2>
<h3>Yaxin Shi, Xiaowei Zhou, Ping Liu, Ivor Tsang</h3>
<p>In this paper, we revisit the Image-to-Image (I2I) translation problem with
transition consistency, namely the consistency defined on the conditional data
mapping between each data pairs. Explicitly parameterizing each data mappings
with a transition variable $t$, i.e., $x \overset{t(x,y)}{\mapsto}y$, we
discover that existing I2I translation models mainly focus on maintaining
consistency on results, e.g., image reconstruction or attribute prediction,
named result consistency in our paper. This restricts their generalization
ability to generate satisfactory results with unseen transitions in the test
phase. Consequently, we propose to enforce both result consistency and
transition consistency for I2I translation, to benefit the problem with a
closer consistency between the input and output. To benefit the generalization
ability of the translation model, we propose transition encoding to facilitate
explicit regularization of these two {kinds} of consistencies on unseen
transitions. We further generalize such explicitly regularized consistencies to
distribution-level, thus facilitating a generalized overall consistency for I2I
translation problems. With the above design, our proposed model, named
Transition Encoding GAN (TEGAN), can poss superb generalization ability to
generate realistic and semantically consistent translation results with unseen
transitions in the test phase. It also provides a unified understanding of the
existing GAN-based I2I transition models with our explicitly modeling of the
data mapping, i.e., transition. Experiments on four different I2I translation
tasks demonstrate the efficacy and generality of TEGAN.
</p>
<a href="http://arxiv.org/abs/2103.05193" target="_blank">arXiv:2103.05193</a> [<a href="http://arxiv.org/pdf/2103.05193" target="_blank">pdf</a>]

<h2>Enhancing Medical Image Registration via Appearance Adjustment Networks. (arXiv:2103.05213v1 [cs.CV])</h2>
<h3>Mingyuan Meng, Lei Bi, Michael Fulham, David Dagan Feng, Jinman Kim</h3>
<p>Deformable image registration is fundamental for many medical image analyses.
A key obstacle for accurate image registration is the variations in image
appearance. Recently, deep learning-based registration methods (DLRs), using
deep neural networks, have computational efficiency that is several orders of
magnitude greater than traditional optimization-based registration methods
(ORs). A major drawback, however, of DLRs is a disregard for the
target-pair-specific optimization that is inherent in ORs and instead they rely
on a globally optimized network that is trained with a set of training samples
to achieve faster registration. Thus, DLRs inherently have degraded ability to
adapt to appearance variations and perform poorly, compared to ORs, when image
pairs (fixed/moving images) have large differences in appearance. Hence, we
propose an Appearance Adjustment Network (AAN) where we leverage anatomy edges,
through an anatomy-constrained loss function, to generate an anatomy-preserving
appearance transformation. We designed the AAN so that it can be readily
inserted into a wide range of DLRs, to reduce the appearance differences
between the fixed and moving images. Our AAN and DLR's network can be trained
cooperatively in an unsupervised and end-to-end manner. We evaluated our AAN
with two widely used DLRs - Voxelmorph (VM) and FAst IMage registration (FAIM)
- on three public 3D brain magnetic resonance (MR) image datasets - IBSR18,
Mindboggle101, and LPBA40. The results show that DLRs, using the AAN, improved
performance and achieved higher results than state-of-the-art ORs.
</p>
<a href="http://arxiv.org/abs/2103.05213" target="_blank">arXiv:2103.05213</a> [<a href="http://arxiv.org/pdf/2103.05213" target="_blank">pdf</a>]

<h2>Universal Undersampled MRI Reconstruction. (arXiv:2103.05214v1 [cs.CV])</h2>
<h3>Xinwen Liu, Jing Wang, Feng Liu, S.Kevin Zhou</h3>
<p>Deep neural networks have been extensively studied for undersampled MRI
reconstruction. While achieving state-of-the-art performance, they are trained
and deployed specifically for one anatomy with limited generalization ability
to another anatomy. Rather than building multiple models, a universal model
that reconstructs images across different anatomies is highly desirable for
efficient deployment and better generalization. Simply mixing images from
multiple anatomies for training a single network does not lead to an ideal
universal model due to the statistical shift among datasets of various
anatomies, the need to retrain from scratch on all datasets with the addition
of a new dataset, and the difficulty in dealing with imbalanced sampling when
the new dataset is further of a smaller size. In this paper, for the first
time, we propose a framework to learn a universal deep neural network for
undersampled MRI reconstruction. Specifically, anatomy-specific instance
normalization is proposed to compensate for statistical shift and allow easy
generalization to new datasets. Moreover, the universal model is trained by
distilling knowledge from available independent models to further exploit
representations across anatomies. Experimental results show the proposed
universal model can reconstruct both brain and knee images with high image
quality. Also, it is easy to adapt the trained model to new datasets of smaller
size, i.e., abdomen, cardiac and prostate, with little effort and superior
performance.
</p>
<a href="http://arxiv.org/abs/2103.05214" target="_blank">arXiv:2103.05214</a> [<a href="http://arxiv.org/pdf/2103.05214" target="_blank">pdf</a>]

<h2>A Data Augmentation Method by Mixing Up Negative Candidate Answers for Solving Raven's Progressive Matrices. (arXiv:2103.05222v1 [cs.CV])</h2>
<h3>Wentao He, Jialu Zhang, Chenglin Yao, Shihe Wang, Jianfeng Ren, Ruibin Bai</h3>
<p>Raven's Progressive Matrices (RPMs) are frequently-used in testing human's
visual reasoning ability. Recently developed RPM-like datasets and solution
models transfer this kind of problems from cognitive science to computer
science. In view of the poor generalization performance due to insufficient
samples in RPM datasets, we propose a data augmentation strategy by image
mix-up, which is generalizable to a variety of multiple-choice problems,
especially for image-based RPM-like problems. By focusing on potential
functionalities of negative candidate answers, the visual reasoning capability
of the model is enhanced. By applying the proposed data augmentation method, we
achieve significant and consistent improvement on various RPM-like datasets
compared with the state-of-the-art models.
</p>
<a href="http://arxiv.org/abs/2103.05222" target="_blank">arXiv:2103.05222</a> [<a href="http://arxiv.org/pdf/2103.05222" target="_blank">pdf</a>]

<h2>A Scavenger Hunt for Service Robots. (arXiv:2103.05225v1 [cs.RO])</h2>
<h3>Harel Yedidsion, Jennifer Suriadinata, Zifan Xu, Stefan Debruyn, Peter Stone</h3>
<p>Creating robots that can perform general-purpose service tasks in a
human-populated environment has been a longstanding grand challenge for AI and
Robotics research. One particularly valuable skill that is relevant to a wide
variety of tasks is the ability to locate and retrieve objects upon request.
This paper models this skill as a Scavenger Hunt (SH) game, which we formulate
as a variation of the NP-hard stochastic traveling purchaser problem. In this
problem, the goal is to find a set of objects as quickly as possible, given
probability distributions of where they may be found. We investigate the
performance of several solution algorithms for the SH problem, both in
simulation and on a real mobile robot. We use Reinforcement Learning (RL) to
train an agent to plan a minimal cost path, and show that the RL agent can
outperform a range of heuristic algorithms, achieving near optimal performance.
In order to stimulate research on this problem, we introduce a publicly
available software stack and associated website that enable users to upload
scavenger hunts which robots can download, perform, and learn from to
continually improve their performance on future hunts.
</p>
<a href="http://arxiv.org/abs/2103.05225" target="_blank">arXiv:2103.05225</a> [<a href="http://arxiv.org/pdf/2103.05225" target="_blank">pdf</a>]

<h2>DeepSeagrass Dataset. (arXiv:2103.05226v1 [cs.CV])</h2>
<h3>Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett Kettle, Brano Kusy</h3>
<p>We introduce a dataset of seagrass images collected by a biologist
snorkelling in Moreton Bay, Queensland, Australia, as described in our
publication: arXiv:2009.09924. The images are labelled at the image-level by
collecting images of the same morphotype in a folder hierarchy. We also release
pre-trained models and training codes for detection and classification of
seagrass species at the patch level at
https://github.com/csiro-robotics/deepseagrass.
</p>
<a href="http://arxiv.org/abs/2103.05226" target="_blank">arXiv:2103.05226</a> [<a href="http://arxiv.org/pdf/2103.05226" target="_blank">pdf</a>]

<h2>Uncertainty-aware Incremental Learning for Multi-organ Segmentation. (arXiv:2103.05227v1 [cs.CV])</h2>
<h3>Yuhang Zhou, Xiaoman Zhang, Shixiang Feng, Ya Zhang, Yanfeng</h3>
<p>Most existing approaches to train a unified multi-organ segmentation model
from several single-organ datasets require simultaneously access multiple
datasets during training. In the real scenarios, due to privacy and ethics
concerns, the training data of the organs of interest may not be publicly
available. To this end, we investigate a data-free incremental organ
segmentation scenario and propose a novel incremental training framework to
solve it. We use the pretrained model instead of its own training data for
privacy protection. Specifically, given a pretrained $K$ organ segmentation
model and a new single-organ dataset, we train a unified $K+1$ organ
segmentation model without accessing any data belonging to the previous
training stages. Our approach consists of two parts: the background label
alignment strategy and the uncertainty-aware guidance strategy. The first part
is used for knowledge transfer from the pretained model to the training model.
The second part is used to extract the uncertainty information from the
pretrained model to guide the whole knowledge transfer process. By combing
these two strategies, more reliable information is extracted from the
pretrained model without original training data. Experiments on multiple
publicly available pretrained models and a multi-organ dataset MOBA have
demonstrated the effectiveness of our framework.
</p>
<a href="http://arxiv.org/abs/2103.05227" target="_blank">arXiv:2103.05227</a> [<a href="http://arxiv.org/pdf/2103.05227" target="_blank">pdf</a>]

<h2>Stabilized Medical Image Attacks. (arXiv:2103.05232v1 [cs.CV])</h2>
<h3>Gege Qi, Lijun Gong, Yibing Song, Kai Ma, Yefeng Zheng</h3>
<p>Convolutional Neural Networks (CNNs) have advanced existing medical systems
for automatic disease diagnosis. However, a threat to these systems arises that
adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a
negative influence on human healthcare. There is a need to investigate
potential adversarial attacks to robustify deep medical diagnosis systems. On
the other side, there are several modalities of medical images (e.g., CT,
fundus, and endoscopic image) of which each type is significantly different
from others. It is more challenging to generate adversarial perturbations for
different types of medical images. In this paper, we propose an image-based
medical adversarial attack method to consistently produce adversarial
perturbations on medical images. The objective function of our method consists
of a loss deviation term and a loss stabilization term. The loss deviation term
increases the divergence between the CNN prediction of an adversarial example
and its ground truth label. Meanwhile, the loss stabilization term ensures
similar CNN predictions of this example and its smoothed input. From the
perspective of the whole iterations for perturbation generation, the proposed
loss stabilization term exhaustively searches the perturbation space to smooth
the single spot for local optimum escape. We further analyze the KL-divergence
of the proposed loss function and find that the loss stabilization term makes
the perturbations updated towards a fixed objective spot while deviating from
the ground truth. This stabilization ensures the proposed medical attack
effective for different types of medical images while producing perturbations
in small variance. Experiments on several medical image analysis benchmarks
including the recent COVID-19 dataset show the stability of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2103.05232" target="_blank">arXiv:2103.05232</a> [<a href="http://arxiv.org/pdf/2103.05232" target="_blank">pdf</a>]

<h2>Enhancing sensor resolution improves CNN accuracy given the same number of parameters or FLOPS. (arXiv:2103.05251v1 [cs.CV])</h2>
<h3>Ali Borji</h3>
<p>High image resolution is critical to obtain a good performance in many
computer vision applications. Computational complexity of CNNs, however, grows
significantly with the increase in input image size. Here, we show that it is
almost always possible to modify a network such that it achieves higher
accuracy at a higher input resolution while having the same number of
parameters or/and FLOPS. The idea is similar to the EfficientNet paper but
instead of optimizing network width, depth and resolution simultaneously, here
we focus only on input resolution. This makes the search space much smaller
which is more suitable for low computational budget regimes. More importantly,
by controlling for the number of model parameters (and hence model capacity),
we show that the additional benefit in accuracy is indeed due to the higher
input resolution. Preliminary empirical investigation over MNIST, Fashion
MNIST, and CIFAR10 datasets demonstrates the efficiency of the proposed
approach.
</p>
<a href="http://arxiv.org/abs/2103.05251" target="_blank">arXiv:2103.05251</a> [<a href="http://arxiv.org/pdf/2103.05251" target="_blank">pdf</a>]

<h2>MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2103.05254v1 [cs.CV])</h2>
<h3>Xiaoqing Guo, Chen Yang, Baopu Li, Yixuan Yuan</h3>
<p>Unsupervised domain adaptation (UDA) aims to transfer the knowledge from the
labeled source domain to the unlabeled target domain. Existing self-training
based UDA approaches assign pseudo labels for target data and treat them as
ground truth labels to fully leverage unlabeled target data for model
adaptation. However, the generated pseudo labels from the model optimized on
the source domain inevitably contain noise due to the domain gap. To tackle
this issue, we advance a MetaCorrection framework, where a Domain-aware
Meta-learning strategy is devised to benefit Loss Correction (DMLC) for UDA
semantic segmentation. In particular, we model the noise distribution of pseudo
labels in target domain by introducing a noise transition matrix (NTM) and
construct meta data set with domain-invariant source data to guide the
estimation of NTM. Through the risk minimization on the meta data set, the
optimized NTM thus can correct the noisy issues in pseudo labels and enhance
the generalization ability of the model on the target data. Considering the
capacity gap between shallow and deep features, we further employ the proposed
DMLC strategy to provide matched and compatible supervision signals for
different level features, thereby ensuring deep adaptation. Extensive
experimental results highlight the effectiveness of our method against existing
state-of-the-art methods on three benchmarks.
</p>
<a href="http://arxiv.org/abs/2103.05254" target="_blank">arXiv:2103.05254</a> [<a href="http://arxiv.org/pdf/2103.05254" target="_blank">pdf</a>]

<h2>BASAR:Black-box Attack on Skeletal Action Recognition. (arXiv:2103.05266v1 [cs.CV])</h2>
<h3>Yunfeng Diao, Tianjia Shao, Yong-Liang Yang, Kun Zhou, He Wang</h3>
<p>Skeletal motion plays a vital role in human activity recognition as either an
independent data source or a complement. The robustness of skeleton-based
activity recognizers has been questioned recently, which shows that they are
vulnerable to adversarial attacks when the full-knowledge of the recognizer is
accessible to the attacker. However, this white-box requirement is overly
restrictive in most scenarios and the attack is not truly threatening. In this
paper, we show that such threats do exist under black-box settings too. To this
end, we propose the first black-box adversarial attack method BASAR. Through
BASAR, we show that adversarial attack is not only truly a threat but also can
be extremely deceitful, because on-manifold adversarial samples are rather
common in skeletal motions, in contrast to the common belief that adversarial
samples only exist off-manifold. Through exhaustive evaluation and comparison,
we show that BASAR can deliver successful attacks across models, data, and
attack modes. Through harsh perceptual studies, we show that it achieves
effective yet imperceptible attacks. By analyzing the attack on different
activity recognizers, BASAR helps identify the potential causes of their
vulnerability and provides insights on what classifiers are likely to be more
robust against attack.
</p>
<a href="http://arxiv.org/abs/2103.05266" target="_blank">arXiv:2103.05266</a> [<a href="http://arxiv.org/pdf/2103.05266" target="_blank">pdf</a>]

<h2>PcmNet: Position-Sensitive Context Modeling Network for Temporal Action Localization. (arXiv:2103.05270v1 [cs.CV])</h2>
<h3>Xin Qin, Hanbin Zhao, Guangchen Lin, Hao Zeng, Songcen Xu, Xi Li</h3>
<p>Temporal action localization is an important and challenging task that aims
to locate temporal regions in real-world untrimmed videos where actions occur
and recognize their classes. It is widely acknowledged that video context is a
critical cue for video understanding, and exploiting the context has become an
important strategy to boost localization performance. However, previous
state-of-the-art methods focus more on exploring semantic context which
captures the feature similarity among frames or proposals, and neglect
positional context which is vital for temporal localization. In this paper, we
propose a temporal-position-sensitive context modeling approach to incorporate
both positional and semantic information for more precise action localization.
Specifically, we first augment feature representations with directed temporal
positional encoding, and then conduct attention-based information propagation,
in both frame-level and proposal-level. Consequently, the generated feature
representations are significantly empowered with the discriminative capability
of encoding the position-aware context information, and thus benefit boundary
detection and proposal evaluation. We achieve state-of-the-art performance on
both two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the
effectiveness and generalization ability of our method.
</p>
<a href="http://arxiv.org/abs/2103.05270" target="_blank">arXiv:2103.05270</a> [<a href="http://arxiv.org/pdf/2103.05270" target="_blank">pdf</a>]

<h2>Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation. (arXiv:2103.05271v1 [cs.CV])</h2>
<h3>Gengcong Yang, Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yujiu Yang</h3>
<p>To generate "accurate" scene graphs, almost all existing methods predict
pairwise relationships in a deterministic manner. However, we argue that visual
relationships are often semantically ambiguous. Specifically, inspired by
linguistic knowledge, we classify the ambiguity into three types: Synonymy
Ambiguity, Hyponymy Ambiguity, and Multi-view Ambiguity. The ambiguity
naturally leads to the issue of \emph{implicit multi-label}, motivating the
need for diverse predictions. In this work, we propose a novel plug-and-play
Probabilistic Uncertainty Modeling (PUM) module. It models each union region as
a Gaussian distribution, whose variance measures the uncertainty of the
corresponding visual content. Compared to the conventional deterministic
methods, such uncertainty modeling brings stochasticity of feature
representation, which naturally enables diverse predictions. As a byproduct,
PUM also manages to cover more fine-grained relationships and thus alleviates
the issue of bias towards frequent relationships. Extensive experiments on the
large-scale Visual Genome benchmark show that combining PUM with newly proposed
ResCAGCN can achieve state-of-the-art performances, especially under the mean
recall metric. Furthermore, we prove the universal effectiveness of PUM by
plugging it into some existing models and provide insightful analysis of its
ability to generate diverse yet plausible visual relationships.
</p>
<a href="http://arxiv.org/abs/2103.05271" target="_blank">arXiv:2103.05271</a> [<a href="http://arxiv.org/pdf/2103.05271" target="_blank">pdf</a>]

<h2>Open-book Video Captioning with Retrieve-Copy-Generate Network. (arXiv:2103.05284v1 [cs.CV])</h2>
<h3>Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng, Weiming Hu</h3>
<p>Due to the rapid emergence of short videos and the requirement for content
understanding and creation, the video captioning task has received increasing
attention in recent years. In this paper, we convert traditional video
captioning task into a new paradigm, \ie, Open-book Video Captioning, which
generates natural language under the prompts of video-content-relevant
sentences, not limited to the video itself. To address the open-book video
captioning problem, we propose a novel Retrieve-Copy-Generate network, where a
pluggable video-to-text retriever is constructed to retrieve sentences as hints
from the training corpus effectively, and a copy-mechanism generator is
introduced to extract expressions from multi-retrieved sentences dynamically.
The two modules can be trained end-to-end or separately, which is flexible and
extensible. Our framework coordinates the conventional retrieval-based methods
with orthodox encoder-decoder methods, which can not only draw on the diverse
expressions in the retrieved sentences but also generate natural and accurate
content of the video. Extensive experiments on several benchmark datasets show
that our proposed approach surpasses the state-of-the-art performance,
indicating the effectiveness and promising of the proposed paradigm in the task
of video captioning.
</p>
<a href="http://arxiv.org/abs/2103.05284" target="_blank">arXiv:2103.05284</a> [<a href="http://arxiv.org/pdf/2103.05284" target="_blank">pdf</a>]

<h2>Decentralized Circle Formation Control for Fish-like Robots in the Real-world via Reinforcement Learning. (arXiv:2103.05293v1 [cs.RO])</h2>
<h3>Tianhao Zhang, Yueheng Li, Shuai Li, Qiwei Ye, Chen Wang, Guangming Xie</h3>
<p>In this paper, the circle formation control problem is addressed for a group
of cooperative underactuated fish-like robots involving unknown nonlinear
dynamics and disturbances. Based on the reinforcement learning and cognitive
consistency theory, we propose a decentralized controller without the knowledge
of the dynamics of the fish-like robots. The proposed controller can be
transferred from simulation to reality. It is only trained in our established
simulation environment, and the trained controller can be deployed to real
robots without any manual tuning. Simulation results confirm that the proposed
model-free robust formation control method is scalable with respect to the
group size of the robots and outperforms other representative RL algorithms.
Several experiments in the real world verify the effectiveness of our RL-based
approach for circle formation control.
</p>
<a href="http://arxiv.org/abs/2103.05293" target="_blank">arXiv:2103.05293</a> [<a href="http://arxiv.org/pdf/2103.05293" target="_blank">pdf</a>]

<h2>Bio-Inspired Representation Learning for Visual Attention Prediction. (arXiv:2103.05310v1 [cs.CV])</h2>
<h3>Yuan Yuan, Hailong Ning, Xiaoqiang Lu</h3>
<p>Visual Attention Prediction (VAP) is a significant and imperative issue in
the field of computer vision. Most of existing VAP methods are based on deep
learning. However, they do not fully take advantage of the low-level contrast
features while generating the visual attention map. In this paper, a novel VAP
method is proposed to generate visual attention map via bio-inspired
representation learning. The bio-inspired representation learning combines both
low-level contrast and high-level semantic features simultaneously, which are
developed by the fact that human eye is sensitive to the patches with high
contrast and objects with high semantics. The proposed method is composed of
three main steps: 1) feature extraction, 2) bio-inspired representation
learning and 3) visual attention map generation. Firstly, the high-level
semantic feature is extracted from the refined VGG16, while the low-level
contrast feature is extracted by the proposed contrast feature extraction block
in a deep network. Secondly, during bio-inspired representation learning, both
the extracted low-level contrast and high-level semantic features are combined
by the designed densely connected block, which is proposed to concatenate
various features scale by scale. Finally, the weighted-fusion layer is
exploited to generate the ultimate visual attention map based on the obtained
representations after bio-inspired representation learning. Extensive
experiments are performed to demonstrate the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2103.05310" target="_blank">arXiv:2103.05310</a> [<a href="http://arxiv.org/pdf/2103.05310" target="_blank">pdf</a>]

<h2>Passive Flow Control for Series Inflatable Actuators: Application on a Wearable Soft-Robot for Posture Assistance. (arXiv:2103.05332v1 [cs.RO])</h2>
<h3>Diego Paez-Granados, Takehiro Yamamoto, Hideki Kadone, Kenji Suzuki</h3>
<p>This paper presents a passive control method for multiple degrees of freedom
in a soft pneumatic robot through the combination of flow resistor tubes with
series inflatable actuators. We designed and developed these 3D printed
resistors based on the pressure drop principle of multiple capillary orifices,
which allows a passive control of its sequential activation from a single
source of pressure. Our design fits in standard tube connectors, making it easy
to adopt it on any other type of actuator with pneumatic inlets. We present its
characterization of pressure drop and evaluation of the activation sequence for
series and parallel circuits of actuators. Moreover, we present an application
for the assistance of postural transition from lying to sitting. We embedded it
in a wearable garment robot-suit designed for infants with cerebral palsy.
Then, we performed the test with a dummy baby for emulating the upper-body
motion control. The results show a sequential motion control of the sitting and
lying transitions validating the proposed system for flow control and its
application on the robot-suit.
</p>
<a href="http://arxiv.org/abs/2103.05332" target="_blank">arXiv:2103.05332</a> [<a href="http://arxiv.org/pdf/2103.05332" target="_blank">pdf</a>]

<h2>Thumbnail: A Novel Data Augmentation for Convolutional Neural Network. (arXiv:2103.05342v1 [cs.CV])</h2>
<h3>Tianshu Xie, Xuan Cheng, Minghui Liu, Jiali Deng, Xiaomin Wang, Ming Liu</h3>
<p>In this paper, we propose a new data augmentation strategy named Thumbnail,
which aims to strengthen the network's capture of global features. We get a
generated image by reducing an image to a certain size, which is called as the
thumbnail, and pasting it in the random position of the original image. The
generated image not only retains most of the original image information but
also has the global information in the thumbnail. Furthermore, we find that the
idea of thumbnail can be perfectly integrated with Mixed Sample Data
Augmentation, so we paste the thumbnail in another image where the ground truth
labels are also mixed with a certain weight, which makes great achievements on
various computer vision tasks. Extensive experiments show that Thumbnail works
better than the state-of-the-art augmentation strategies across classification,
fine-grained image classification, and object detection. On ImageNet
classification, ResNet50 architecture with our method achieves 79.21% accuracy,
which is more than 2.89% improvement on the baseline.
</p>
<a href="http://arxiv.org/abs/2103.05342" target="_blank">arXiv:2103.05342</a> [<a href="http://arxiv.org/pdf/2103.05342" target="_blank">pdf</a>]

<h2>A model-based framework for learning transparent swarm behaviors. (arXiv:2103.05343v1 [cs.RO])</h2>
<h3>Mario Coppola, Jian Guo, Eberhard Gill, Guido C. H. E. de Croon</h3>
<p>This paper proposes a model-based framework to automatically and efficiently
design understandable and verifiable behaviors for swarms of robots. The
framework is based on the automatic extraction of two distinct models: 1) a
neural network model trained to estimate the relationship between the robots'
sensor readings and the global performance of the swarm, and 2) a probabilistic
state transition model that explicitly models the local state transitions
(i.e., transitions in observations from the perspective of a single robot in
the swarm) given a policy. The models can be trained from a data set of
simulated runs featuring random policies. The first model is used to
automatically extract a set of local states that are expected to maximize the
global performance. These local states are referred to as desired local states.
The second model is used to optimize a stochastic policy so as to increase the
probability that the robots in the swarm observe one of the desired local
states. Following these steps, the framework proposed in this paper can
efficiently lead to effective controllers. This is tested on four case studies,
featuring aggregation and foraging tasks. Importantly, thanks to the models,
the framework allows us to understand and inspect a swarm's behavior. To this
end, we propose verification checks to identify some potential issues that may
prevent the swarm from achieving the desired global objective. In addition, we
explore how the framework can be used in combination with a "standard"
evolutionary robotics strategy (i.e., where performance is measured via
simulation), or with online learning.
</p>
<a href="http://arxiv.org/abs/2103.05343" target="_blank">arXiv:2103.05343</a> [<a href="http://arxiv.org/pdf/2103.05343" target="_blank">pdf</a>]

<h2>ST3D: Self-training for Unsupervised Domain Adaptation on 3D ObjectDetection. (arXiv:2103.05346v1 [cs.CV])</h2>
<h3>Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, Xiaojuan Qi</h3>
<p>We present a new domain adaptive self-training pipeline, named ST3D, for
unsupervised domain adaptation on 3D object detection from point clouds. First,
we pre-train the 3D detector on the source domain with our proposed random
object scaling strategy for mitigating the negative effects of source domain
bias. Then, the detector is iteratively improved on the target domain by
alternatively conducting two steps, which are the pseudo label updating with
the developed quality-aware triplet memory bank and the model training with
curriculum data augmentation. These specific designs for 3D object detection
enable the detector to be trained with consistent and high-quality pseudo
labels and to avoid overfitting to the large number of easy examples in pseudo
labeled data. Our ST3D achieves state-of-the-art performance on all evaluated
datasets and even surpasses fully supervised results on KITTI 3D object
detection benchmark. Code will be available at
https://github.com/CVMI-Lab/ST3D.
</p>
<a href="http://arxiv.org/abs/2103.05346" target="_blank">arXiv:2103.05346</a> [<a href="http://arxiv.org/pdf/2103.05346" target="_blank">pdf</a>]

<h2>Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack. (arXiv:2103.05347v1 [cs.CV])</h2>
<h3>He Wang, Feixiang He, Zhexi Peng, Yong-Liang Yang, Tianjia Shao, Kun Zhou, David Hogg</h3>
<p>Action recognition has been heavily employed in many applications such as
autonomous vehicles, surveillance, etc, where its robustness is a primary
concern. In this paper, we examine the robustness of state-of-the-art action
recognizers against adversarial attack, which has been rarely investigated so
far. To this end, we propose a new method to attack action recognizers that
rely on 3D skeletal motion. Our method involves an innovative perceptual loss
that ensures the imperceptibility of the attack. Empirical studies demonstrate
that our method is effective in both white-box and black-box scenarios. Its
generalizability is evidenced on a variety of action recognizers and datasets.
Its versatility is shown in different attacking strategies. Its deceitfulness
is proven in extensive perceptual studies. Our method shows that adversarial
attack on 3D skeletal motions, one type of time-series data, is significantly
different from traditional adversarial attack problems. Its success raises
serious concern on the robustness of action recognizers and provides insights
on potential improvements.
</p>
<a href="http://arxiv.org/abs/2103.05347" target="_blank">arXiv:2103.05347</a> [<a href="http://arxiv.org/pdf/2103.05347" target="_blank">pdf</a>]

<h2>I am Robot: Neuromuscular Reinforcement Learning to Actuate Human Limbs through Functional Electrical Stimulation. (arXiv:2103.05349v1 [cs.RO])</h2>
<h3>Nat Wannawas, Ali Shafti, A. Aldo Faisal</h3>
<p>Human movement disorders or paralysis lead to the loss of control of muscle
activation and thus motor control. Functional Electrical Stimulation (FES) is
an established and safe technique for contracting muscles by stimulating the
skin above a muscle to induce its contraction. However, an open challenge
remains on how to restore motor abilities to human limbs through FES, as the
problem of controlling the stimulation is unclear. We are taking a robotics
perspective on this problem, by developing robot learning algorithms that
control the ultimate humanoid robot, the human body, through electrical muscle
stimulation. Human muscles are not trivial to control as actuators due to their
force production being non-stationary as a result of fatigue and other internal
state changes, in contrast to robot actuators which are well-understood and
stationary over broad operation ranges. We present our Deep Reinforcement
Learning approach to the control of human muscles with FES, using a recurrent
neural network for dynamic state representation, to overcome the unobserved
elements of the behaviour of human muscles under external stimulation. We
demonstrate our technique both in neuromuscular simulations but also
experimentally on a human. Our results show that our controller can learn to
manipulate human muscles, applying appropriate levels of stimulation to achieve
the given tasks while compensating for advancing muscle fatigue which arises
throughout the tasks. Additionally, our technique can learn quickly enough to
be implemented in real-world human-in-the-loop settings.
</p>
<a href="http://arxiv.org/abs/2103.05349" target="_blank">arXiv:2103.05349</a> [<a href="http://arxiv.org/pdf/2103.05349" target="_blank">pdf</a>]

<h2>A Riemannian Metric for Geometry-Aware Singularity Avoidance by Articulated Robots. (arXiv:2103.05362v1 [cs.RO])</h2>
<h3>Filip Mari&#x107;, Luka Petrovi&#x107;, Marko Guberina, Jonathan Kelly, Ivan Petrovi&#x107;</h3>
<p>Articulated robots such as manipulators increasingly must operate in
uncertain and dynamic environments where interaction (with human coworkers, for
example) is necessary. In these situations, the capacity to quickly adapt to
unexpected changes in operational space constraints is essential. At certain
points in a manipulator's configuration space, termed singularities, the robot
loses one or more degrees of freedom (DoF) and is unable to move in specific
operational space directions. The inability to move in arbitrary directions in
operational space compromises adaptivity and, potentially, safety. We introduce
a geometry-aware singularity index,defined using a Riemannian metric on the
manifold of symmetric positive definite matrices, to provide a measure of
proximity to singular configurations. We demonstrate that our index avoids some
of the failure modes and difficulties inherent to other common indices.
Further, we show that this index can be differentiated easily, making it
compatible with local optimization approaches used for operational space
control. Our experimental results establish that, for reaching and path
following tasks, optimization based on our index outperforms a common
manipulability maximization technique, ensuring singularity-robust motions.
</p>
<a href="http://arxiv.org/abs/2103.05362" target="_blank">arXiv:2103.05362</a> [<a href="http://arxiv.org/pdf/2103.05362" target="_blank">pdf</a>]

<h2>MWQ: Multiscale Wavelet Quantized Neural Networks. (arXiv:2103.05363v1 [cs.CV])</h2>
<h3>Qigong Sun, Yan Ren, Licheng Jiao, Xiufang Li, Fanhua Shang, Fang Liu</h3>
<p>Model quantization can reduce the model size and computational latency, it
has become an essential technique for the deployment of deep neural networks on
resourceconstrained hardware (e.g., mobile phones and embedded devices). The
existing quantization methods mainly consider the numerical elements of the
weights and activation values, ignoring the relationship between elements. The
decline of representation ability and information loss usually lead to the
performance degradation. Inspired by the characteristics of images in the
frequency domain, we propose a novel multiscale wavelet quantization (MWQ)
method. This method decomposes original data into multiscale frequency
components by wavelet transform, and then quantizes the components of different
scales, respectively. It exploits the multiscale frequency and spatial
information to alleviate the information loss caused by quantization in the
spatial domain. Because of the flexibility of MWQ, we demonstrate three
applications (e.g., model compression, quantized network optimization, and
information enhancement) on the ImageNet and COCO datasets. Experimental
results show that our method has stronger representation ability and can play
an effective role in quantized neural networks.
</p>
<a href="http://arxiv.org/abs/2103.05363" target="_blank">arXiv:2103.05363</a> [<a href="http://arxiv.org/pdf/2103.05363" target="_blank">pdf</a>]

<h2>ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments. (arXiv:2103.05368v1 [cs.CV])</h2>
<h3>Jin-Man Park, Jae-Hyuk Jang, Sahng-Min Yoo, Sun-Kyung Lee, Ue-Hwan Kim, Jong-Hwan Kim</h3>
<p>We present a challenging dataset, ChangeSim, aimed at online scene change
detection (SCD) and more. The data is collected in photo-realistic simulation
environments with the presence of environmental non-targeted variations, such
as air turbidity and light condition changes, as well as targeted object
changes in industrial indoor environments. By collecting data in simulations,
multi-modal sensor data and precise ground truth labels are obtainable such as
the RGB image, depth image, semantic segmentation, change segmentation, camera
poses, and 3D reconstructions. While the previous online SCD datasets evaluate
models given well-aligned image pairs, ChangeSim also provides raw unpaired
sequences that present an opportunity to develop an online SCD model in an
end-to-end manner, considering both pairing and detection. Experiments show
that even the latest pair-based SCD models suffer from the bottleneck of the
pairing process, and it gets worse when the environment contains the
non-targeted variations. Our dataset is available at
this http URL
</p>
<a href="http://arxiv.org/abs/2103.05368" target="_blank">arXiv:2103.05368</a> [<a href="http://arxiv.org/pdf/2103.05368" target="_blank">pdf</a>]

<h2>Pluggable Weakly-Supervised Cross-View Learning for Accurate Vehicle Re-Identification. (arXiv:2103.05376v1 [cs.CV])</h2>
<h3>Lu Yang, Hongbang Liu, Jinghao Zhou, Lingqiao Liu, Lei Zhang, Peng Wang, Yanning Zhang</h3>
<p>Learning cross-view consistent feature representation is the key for accurate
vehicle Re-identification (ReID), since the visual appearance of vehicles
changes significantly under different viewpoints. To this end, most existing
approaches resort to the supervised cross-view learning using extensive extra
viewpoints annotations, which however, is difficult to deploy in real
applications due to the expensive labelling cost and the continous viewpoint
variation that makes it hard to define discrete viewpoint labels. In this
study, we present a pluggable Weakly-supervised Cross-View Learning (WCVL)
module for vehicle ReID. Through hallucinating the cross-view samples as the
hardest positive counterparts in feature domain, we can learn the consistent
feature representation via minimizing the cross-view feature distance based on
vehicle IDs only without using any viewpoint annotation. More importantly, the
proposed method can be seamlessly plugged into most existing vehicle ReID
baselines for cross-view learning without re-training the baselines. To
demonstrate its efficacy, we plug the proposed method into a bunch of
off-the-shelf baselines and obtain significant performance improvement on four
public benchmark datasets, i.e., VeRi-776, VehicleID, VRIC and VRAI.
</p>
<a href="http://arxiv.org/abs/2103.05376" target="_blank">arXiv:2103.05376</a> [<a href="http://arxiv.org/pdf/2103.05376" target="_blank">pdf</a>]

<h2>Are We Ready for Unmanned Surface Vehicles in Inland Waterways? The USVInland Multisensor Dataset and Benchmark. (arXiv:2103.05383v1 [cs.RO])</h2>
<h3>Yuwei Cheng, Mengxin Jiang, Jiannan Zhu, Yimin Liu</h3>
<p>Unmanned surface vehicles (USVs) have great value with their ability to
execute hazardous and time-consuming missions over water surfaces. Recently,
USVs for inland waterways have attracted increasing attention for their
potential application in autonomous monitoring, transportation, and cleaning.
However, unlike sailing in open water, the challenges posed by scenes of inland
waterways, such as the complex distribution of obstacles, the global
positioning system (GPS) signal denial environment, the reflection of bank-side
structures, and the fog over the water surface, all impede USV application in
inland waterways. To address these problems and stimulate relevant research, we
introduce USVInland, a multisensor dataset for USVs in inland waterways. The
collection of USVInland spans a trajectory of more than 26 km in diverse
real-world scenes of inland waterways using various modalities, including
lidar, stereo cameras, millimeter-wave radar, GPS, and inertial measurement
units (IMUs). Based on the requirements and challenges in the perception and
navigation of USVs for inland waterways, we build benchmarks for simultaneous
localization and mapping (SLAM), stereo matching, and water segmentation. We
evaluate common algorithms for the above tasks to determine the influence of
unique inland waterway scenes on algorithm performance. Our dataset and the
development tools are available online at
https://www.orca-tech.cn/datasets.html.
</p>
<a href="http://arxiv.org/abs/2103.05383" target="_blank">arXiv:2103.05383</a> [<a href="http://arxiv.org/pdf/2103.05383" target="_blank">pdf</a>]

<h2>NaroNet: Objective-based learning of the tumor microenvironment from highly multiplexed immunostained images. (arXiv:2103.05385v1 [cs.CV])</h2>
<h3>Daniel Jim&#xe9;nez-S&#xe1;nchez, Mikel Ariz, Hang Chang, Xavier Matias-Guiu, Carlos E. de Andrea, Carlos Ortiz-de-Sol&#xf3;rzano</h3>
<p>We present NaroNet, a Machine Learning framework that integrates the
multiscale spatial, in situ analysis of the tumor microenvironment (TME) with
patient-level predictions into a seamless end-to-end learning pipeline. Trained
only with patient-level labels, NaroNet quantifies the phenotypes,
neighborhoods, and neighborhood interactions that have the highest influence on
the predictive task. We validate NaroNet using synthetic data simulating
multiplex-immunostained images with adjustable probabilistic incidence of
different TMEs. Then we apply our model to two real sets of patient tumors, one
consisting of 336 seven-color multiplex-immunostained images from 12 high-grade
endometrial cancers, and the other consisting of 372 35-plex mass cytometry
images from 283 breast cancer patients. In both synthetic and real datasets,
NaroNet provides outstanding predictions while associating those predictions to
the presence of specific TMEs. This inherent interpretability could be of great
value both in a clinical setting and as a tool to discover novel biomarker
signatures.
</p>
<a href="http://arxiv.org/abs/2103.05385" target="_blank">arXiv:2103.05385</a> [<a href="http://arxiv.org/pdf/2103.05385" target="_blank">pdf</a>]

<h2>Instance and Pair-Aware Dynamic Networks for Re-Identification. (arXiv:2103.05395v1 [cs.CV])</h2>
<h3>Bingliang Jiao, Xin Tan, Lu Yang, Yunlong Wang, Peng Wang</h3>
<p>Re-identification (ReID) is to identify the same instance across different
cameras. Existing ReID methods mostly utilize alignment-based or
attention-based strategies to generate effective feature representations.
However, most of these methods only extract general feature by employing single
input image itself, overlooking the exploration of relevance between comparing
images. To fill this gap, we propose a novel end-to-end trainable dynamic
convolution framework named Instance and Pair-Aware Dynamic Networks in this
paper. The proposed model is composed of three main branches where a
self-guided dynamic branch is constructed to strengthen instance-specific
features, focusing on every single image. Furthermore, we also design a
mutual-guided dynamic branch to generate pair-aware features for each pair of
images to be compared. Extensive experiments are conducted in order to verify
the effectiveness of our proposed algorithm. We evaluate our algorithm in
several mainstream person and vehicle ReID datasets including CUHK03,
DukeMTMCreID, Market-1501, VeRi776 and VehicleID. In some datasets our
algorithm outperforms state-of-the-art methods and in others, our algorithm
achieves a comparable performance.
</p>
<a href="http://arxiv.org/abs/2103.05395" target="_blank">arXiv:2103.05395</a> [<a href="http://arxiv.org/pdf/2103.05395" target="_blank">pdf</a>]

<h2>QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information. (arXiv:2103.05399v1 [cs.CV])</h2>
<h3>Masato Tamura, Hiroki Ohashi, Tomoaki Yoshinaga</h3>
<p>We propose a simple, intuitive yet powerful method for human-object
interaction (HOI) detection. HOIs are so diverse in spatial distribution in an
image that existing CNN-based methods face the following three major drawbacks;
they cannot leverage image-wide features due to CNN's locality, they rely on a
manually defined location-of-interest for the feature aggregation, which
sometimes does not cover contextually important regions, and they cannot help
but mix up the features for multiple HOI instances if they are located closely.
To overcome these drawbacks, we propose a transformer-based feature extractor,
in which an attention mechanism and query-based detection play key roles. The
attention mechanism is effective in aggregating contextually important
information image-wide, while the queries, which we design in such a way that
each query captures at most one human-object pair, can avoid mixing up the
features from multiple instances. This transformer-based feature extractor
produces so effective embeddings that the subsequent detection heads may be
fairly simple and intuitive. The extensive analysis reveals that the proposed
method successfully extracts contextually important features, and thus
outperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.7
mAP on V-COCO). The source codes are available at
$\href{https://github.com/hitachi-rd-cv/qpic}{\text{this https URL}}$.
</p>
<a href="http://arxiv.org/abs/2103.05399" target="_blank">arXiv:2103.05399</a> [<a href="http://arxiv.org/pdf/2103.05399" target="_blank">pdf</a>]

<h2>Deep 6-DoF Tracking of Unknown Objects for Reactive Grasping. (arXiv:2103.05401v1 [cs.RO])</h2>
<h3>Marc Tuscher, Julian H&#xf6;rz, Danny Driess, Marc Toussaint</h3>
<p>Robotic manipulation of unknown objects is an important field of research.
Practical applications occur in many real-world settings where robots need to
interact with an unknown environment. We tackle the problem of reactive
grasping by proposing a method for unknown object tracking, grasp point
sampling and dynamic trajectory planning. Our object tracking method combines
Siamese Networks with an Iterative Closest Point approach for pointcloud
registration into a method for 6-DoF unknown object tracking. The method does
not require further training and is robust to noise and occlusion. We propose a
robotic manipulation system, which is able to grasp a wide variety of formerly
unseen objects and is robust against object perturbations and inferior grasping
points.
</p>
<a href="http://arxiv.org/abs/2103.05401" target="_blank">arXiv:2103.05401</a> [<a href="http://arxiv.org/pdf/2103.05401" target="_blank">pdf</a>]

<h2>Efficient learning of goal-oriented push-grasping synergy in clutter. (arXiv:2103.05405v1 [cs.RO])</h2>
<h3>Kechun Xu, Hongxiang Yu, Qianen Lai, Yue Wang, Rong Xiong</h3>
<p>We focus on the task of goal-oriented grasping, in which a robot is supposed
to grasp a pre-assigned goal object in clutter and needs some pre-grasp actions
such as pushes to enable stable grasps. However, sample inefficiency remains a
main challenge. In this paper, a goal-conditioned hierarchical reinforcement
learning formulation with high sample efficiency is proposed to learn a
push-grasping policy for grasping a specific object in clutter. In our work,
sample efficiency is improved by two means. First, we use a goal-conditioned
mechanism by goal relabeling to enrich the replay buffer. Second, the pushing
and grasping policies are respectively regarded as a generator and a
discriminator and the pushing policy is trained with supervision of the
grasping discriminator, thus densifying pushing rewards. To deal with the
problem of distribution mismatch caused by different training settings of two
policies, an alternating training stage is added to learn pushing and grasping
in turn. A series of experiments carried out in simulation and real world
indicate that our method can quickly learn effective pushing and grasping
policies and outperforms existing methods in task completion rate and goal
grasp success rate by less times of motion. Furthermore, we validate that our
system can also adapt to goal-agnostic conditions with better performance. Note
that our system can be transferred to the real world without any fine-tuning.
Our code is available at https://github.com/xukechun/Efficient goal-oriented
push-grasping synergy
</p>
<a href="http://arxiv.org/abs/2103.05405" target="_blank">arXiv:2103.05405</a> [<a href="http://arxiv.org/pdf/2103.05405" target="_blank">pdf</a>]

<h2>Weather GAN: Multi-Domain Weather Translation Using Generative Adversarial Networks. (arXiv:2103.05422v1 [cs.CV])</h2>
<h3>Xuelong Li, Kai Kou, Bin Zhao</h3>
<p>In this paper, a new task is proposed, namely, weather translation, which
refers to transferring weather conditions of the image from one category to
another. It is important for photographic style transfer. Although lots of
approaches have been proposed in traditional image translation tasks, few of
them can handle the multi-category weather translation task, since weather
conditions have rich categories and highly complex semantic structures. To
address this problem, we develop a multi-domain weather translation approach
based on generative adversarial networks (GAN), denoted as Weather GAN, which
can achieve the transferring of weather conditions among sunny, cloudy, foggy,
rainy and snowy. Specifically, the weather conditions in the image are
determined by various weather-cues, such as cloud, blue sky, wet ground, etc.
Therefore, it is essential for weather translation to focus the main attention
on weather-cues. To this end, the generator of Weather GAN is composed of an
initial translation module, an attention module and a weather-cue segmentation
module. The initial translation module performs global translation during
generation procedure. The weather-cue segmentation module identifies the
structure and exact distribution of weather-cues. The attention module learns
to focus on the interesting areas of the image while keeping other areas
unaltered. The final generated result is synthesized by these three parts. This
approach suppresses the distortion and deformation caused by weather
translation. our approach outperforms the state-of-the-arts has been shown by a
large number of experiments and evaluations.
</p>
<a href="http://arxiv.org/abs/2103.05422" target="_blank">arXiv:2103.05422</a> [<a href="http://arxiv.org/pdf/2103.05422" target="_blank">pdf</a>]

<h2>Deep Learning based 3D Segmentation: A Survey. (arXiv:2103.05423v1 [cs.CV])</h2>
<h3>Yong He, Hongshan Yu, Xiaoyan Liu, Zhengeng Yang, Wei Sun, Yaonan Wang, Qiang Fu, Yanmei Zou, Ajmal Main</h3>
<p>3D object segmentation is a fundamental and challenging problem in computer
vision with applications in autonomous driving, robotics, augmented reality and
medical image analysis. It has received significant attention from the computer
vision, graphics and machine learning communities. Traditionally, 3D
segmentation was performed with hand-crafted features and engineered methods
which failed to achieve acceptable accuracy and could not generalize to
large-scale data. Driven by their great success in 2D computer vision, deep
learning techniques have recently become the tool of choice for 3D segmentation
tasks as well. This has led to an influx of a large number of methods in the
literature that have been evaluated on different benchmark datasets. This paper
provides a comprehensive survey of recent progress in deep learning based 3D
segmentation covering over 150 papers. It summarizes the most commonly used
pipelines, discusses their highlights and shortcomings, and analyzes the
competitive results of these segmentation methods. Based on the analysis, it
also provides promising research directions for the future.
</p>
<a href="http://arxiv.org/abs/2103.05423" target="_blank">arXiv:2103.05423</a> [<a href="http://arxiv.org/pdf/2103.05423" target="_blank">pdf</a>]

<h2>Automatic Borescope Damage Assessments for Gas Turbine Blades via Deep Learning. (arXiv:2103.05430v1 [cs.CV])</h2>
<h3>Chun Yui Wong, Pranay Seshadri, Geoffrey T. Parks</h3>
<p>To maximise fuel economy, bladed components in aero-engines operate close to
material limits. The severe operating environment leads to in-service damage on
compressor and turbine blades, having a profound and immediate impact on the
performance of the engine. Current methods of blade visual inspection are
mainly based on borescope imaging. During these inspections, the sentencing of
components under inspection requires significant manual effort, with a lack of
systematic approaches to avoid human biases. To perform fast and accurate
sentencing, we propose an automatic workflow based on deep learning for
detecting damage present on rotor blades using borescope videos. Building upon
state-of-the-art methods from computer vision, we show that damage statistics
can be presented for each blade in a blade row separately, and demonstrate the
workflow on two borescope videos.
</p>
<a href="http://arxiv.org/abs/2103.05430" target="_blank">arXiv:2103.05430</a> [<a href="http://arxiv.org/pdf/2103.05430" target="_blank">pdf</a>]

<h2>The MICCAI Hackathon on reproducibility, diversity, and selection of papers at the MICCAI conference. (arXiv:2103.05437v1 [cs.CV])</h2>
<h3>Fabian Balsiger, Alain Jungo, Naren Akash R J, Jianan Chen, Ivan Ezhov, Shengnan Liu, Jun Ma, Johannes C. Paetzold, Vishva Saravanan R, Anjany Sekuboyina, Suprosanna Shit, Yannick Suter, Moshood Yekini, Guodong Zeng, Markus Rempfler</h3>
<p>The MICCAI conference has encountered tremendous growth over the last years
in terms of the size of the community, as well as the number of contributions
and their technical success. With this growth, however, come new challenges for
the community. Methods are more difficult to reproduce and the ever-increasing
number of paper submissions to the MICCAI conference poses new questions
regarding the selection process and the diversity of topics. To exchange,
discuss, and find novel and creative solutions to these challenges, a new
format of a hackathon was initiated as a satellite event at the MICCAI 2020
conference: The MICCAI Hackathon. The first edition of the MICCAI Hackathon
covered the topics reproducibility, diversity, and selection of MICCAI papers.
In the manner of a small think-tank, participants collaborated to find
solutions to these challenges. In this report, we summarize the insights from
the MICCAI Hackathon into immediate and long-term measures to address these
challenges. The proposed measures can be seen as starting points and guidelines
for discussions and actions to possibly improve the MICCAI conference with
regards to reproducibility, diversity, and selection of papers.
</p>
<a href="http://arxiv.org/abs/2103.05437" target="_blank">arXiv:2103.05437</a> [<a href="http://arxiv.org/pdf/2103.05437" target="_blank">pdf</a>]

<h2>Pixel-wise Anomaly Detection in Complex Driving Scenes. (arXiv:2103.05445v1 [cs.CV])</h2>
<h3>Giancarlo Di Biase, Hermann Blum, Roland Siegwart, Cesar Cadena</h3>
<p>The inability of state-of-the-art semantic segmentation methods to detect
anomaly instances hinders them from being deployed in safety-critical and
complex applications, such as autonomous driving. Recent approaches have
focused on either leveraging segmentation uncertainty to identify anomalous
areas or re-synthesizing the image from the semantic label map to find
dissimilarities with the input image. In this work, we demonstrate that these
two methodologies contain complementary information and can be combined to
produce robust predictions for anomaly segmentation. We present a pixel-wise
anomaly detection framework that uses uncertainty maps to improve over existing
re-synthesis methods in finding dissimilarities between the input and generated
images. Our approach works as a general framework around already trained
segmentation networks, which ensures anomaly detection without compromising
segmentation accuracy, while significantly outperforming all similar methods.
Top-2 performance across a range of different anomaly datasets shows the
robustness of our approach to handling different anomaly instances.
</p>
<a href="http://arxiv.org/abs/2103.05445" target="_blank">arXiv:2103.05445</a> [<a href="http://arxiv.org/pdf/2103.05445" target="_blank">pdf</a>]

<h2>Extended Task and Motion Planning of Long-horizon Robot Manipulation. (arXiv:2103.05456v1 [cs.RO])</h2>
<h3>Tianyu Ren, Georgia Chalvatzaki, Jan Peters</h3>
<p>Task and Motion Planning (TAMP) requires the integration of symbolic
reasoning with metric motion planning that accounts for the robot's actions'
geometric feasibility. This hierarchical structure inevitably prevents the
symbolic planners from accessing the environment's low-level geometric
description, vital to the problem's solution. Most TAMP approaches fail to
provide feasible solutions when there is missing knowledge about the
environment at the symbolic level. The incapability of devising alternative
high-level plans leads existing planners to a dead end. We propose a novel
approach for decision-making on extended decision spaces over plan skeletons
and action parameters. We integrate top-k planning for constructing an explicit
skeleton space, where a skeleton planner generates a variety of candidate
skeleton plans. Moreover, we effectively combine this skeleton space with the
resultant motion parameter spaces into a single extended decision space.
Accordingly, we use Monte-Carlo Tree Search (MCTS) to ensure an
exploration-exploitation balance at each decision node and optimize globally to
produce minimum-cost solutions. The proposed seamless combination of symbolic
top-k planning with streams, with the proved optimality of MCTS, leads to a
powerful planning algorithm that can handle the combinatorial complexity of
long-horizon manipulation tasks. We empirically evaluate our proposed algorithm
in challenging manipulation tasks with different domains that require
multi-stage decisions and show how our method can overcome dead-ends through
its effective alternate plans compared to its most competitive baseline method.
</p>
<a href="http://arxiv.org/abs/2103.05456" target="_blank">arXiv:2103.05456</a> [<a href="http://arxiv.org/pdf/2103.05456" target="_blank">pdf</a>]

<h2>Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation. (arXiv:2103.05463v1 [cs.CV])</h2>
<h3>Chaohao Xie, Dongwei Ren, Lei Wang, Qinghua Hu, Liang Lin, Wangmeng Zuo</h3>
<p>Recently, several weakly supervised learning methods have been devoted to
utilize bounding box supervision for training deep semantic segmentation
models. Most existing methods usually leverage the generic proposal generators
(\eg, dense CRF and MCG) to produce enhanced segmentation masks for further
training segmentation models. These proposal generators, however, are generic
and not specifically designed for box-supervised semantic segmentation, thereby
leaving some leeway for improving segmentation performance. In this paper, we
aim at seeking for a more accurate learning-based class-agnostic pseudo mask
generator tailored to box-supervised semantic segmentation. To this end, we
resort to a pixel-level annotated auxiliary dataset where the class labels are
non-overlapped with those of the box-annotated dataset. For learning pseudo
mask generator from the auxiliary dataset, we present a bi-level optimization
formulation. In particular, the lower subproblem is used to learn
box-supervised semantic segmentation, while the upper subproblem is used to
learn an optimal class-agnostic pseudo mask generator. The learned pseudo
segmentation mask generator can then be deployed to the box-annotated dataset
for improving weakly supervised semantic segmentation. Experiments on PASCAL
VOC 2012 dataset show that the learned pseudo mask generator is effective in
boosting segmentation performance, and our method can further close the
performance gap between box-supervised and fully-supervised models. Our code
will be made publicly available at
https://github.com/Vious/LPG_BBox_Segmentation .
</p>
<a href="http://arxiv.org/abs/2103.05463" target="_blank">arXiv:2103.05463</a> [<a href="http://arxiv.org/pdf/2103.05463" target="_blank">pdf</a>]

<h2>PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency. (arXiv:2103.05465v1 [cs.CV])</h2>
<h3>Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo Fu, Chiew-Lan Tai</h3>
<p>Removing outlier correspondences is one of the critical steps for successful
feature-based point cloud registration. Despite the increasing popularity of
introducing deep learning methods in this field, spatial consistency, which is
essentially established by a Euclidean transformation between point clouds, has
received almost no individual attention in existing learning frameworks. In
this paper, we present PointDSC, a novel deep neural network that explicitly
incorporates spatial consistency for pruning outlier correspondences. First, we
propose a nonlocal feature aggregation module, weighted by both feature and
spatial coherence, for feature embedding of the input correspondences. Second,
we formulate a differentiable spectral matching module, supervised by pairwise
spatial compatibility, to estimate the inlier confidence of each correspondence
from the embedded features. With modest computation cost, our method
outperforms the state-of-the-art hand-crafted and learning-based outlier
rejection approaches on several real-world datasets by a significant margin. We
also show its wide applicability by combining PointDSC with different 3D local
descriptors.
</p>
<a href="http://arxiv.org/abs/2103.05465" target="_blank">arXiv:2103.05465</a> [<a href="http://arxiv.org/pdf/2103.05465" target="_blank">pdf</a>]

<h2>Optimized Object Tracking Technique Using Kalman Filter. (arXiv:2103.05467v1 [cs.CV])</h2>
<h3>Liana Ellen Taylor, Midriem Mirdanies, Roni Permana Saputra</h3>
<p>This paper focused on the design of an optimized object tracking technique
which would minimize the processing time required in the object detection
process while maintaining accuracy in detecting the desired moving object in a
cluttered scene. A Kalman filter based cropped image is used for the image
detection process as the processing time is significantly less to detect the
object when a search window is used that is smaller than the entire video
frame. This technique was tested with various sizes of the window in the
cropping process. MATLAB was used to design and test the proposed method. This
paper found that using a cropped image with 2.16 multiplied by the largest
dimension of the object resulted in significantly faster processing time while
still providing a high success rate of detection and a detected center of the
object that was reasonably close to the actual center.
</p>
<a href="http://arxiv.org/abs/2103.05467" target="_blank">arXiv:2103.05467</a> [<a href="http://arxiv.org/pdf/2103.05467" target="_blank">pdf</a>]

<h2>Contrastive Neural Architecture Search with Neural Architecture Comparators. (arXiv:2103.05471v1 [cs.CV])</h2>
<h3>Yaofo Chen, Yong Guo, Qi Chen, Minli Li, Yaowei Wang, Wei Zeng, Mingkui Tan</h3>
<p>One of the key steps in Neural Architecture Search (NAS) is to estimate the
performance of candidate architectures. Existing methods either directly use
the validation performance or learn a predictor to estimate the performance.
However, these methods can be either computationally expensive or very
inaccurate, which may severely affect the search efficiency and performance.
Moreover, as it is very difficult to annotate architectures with accurate
performance on specific tasks, learning a promising performance predictor is
often non-trivial due to the lack of labeled data. In this paper, we argue that
it may not be necessary to estimate the absolute performance for NAS. On the
contrary, we may need only to understand whether an architecture is better than
a baseline one. However, how to exploit this comparison information as the
reward and how to well use the limited labeled data remains two great
challenges. In this paper, we propose a novel Contrastive Neural Architecture
Search (CTNAS) method which performs architecture search by taking the
comparison results between architectures as the reward. Specifically, we design
and learn a Neural Architecture Comparator (NAC) to compute the probability of
candidate architectures being better than a baseline one. Moreover, we present
a baseline updating scheme to improve the baseline iteratively in a curriculum
learning manner. More critically, we theoretically show that learning NAC is
equivalent to optimizing the ranking over architectures. Extensive experiments
in three search spaces demonstrate the superiority of our CTNAS over existing
methods.
</p>
<a href="http://arxiv.org/abs/2103.05471" target="_blank">arXiv:2103.05471</a> [<a href="http://arxiv.org/pdf/2103.05471" target="_blank">pdf</a>]

<h2>Differentially Private Imaging via Latent Space Manipulation. (arXiv:2103.05472v1 [cs.CV])</h2>
<h3>Tao Li, Chris Clifton</h3>
<p>There is growing concern about image privacy due to the popularity of social
media and photo devices, along with increasing use of face recognition systems.
However, established image de-identification techniques are either too subject
to re-identification, produce photos that are insufficiently realistic, or
both. To tackle this, we present a novel approach for image obfuscation by
manipulating latent spaces of an unconditionally trained generative model that
is able to synthesize photo-realistic facial images of high resolution. This
manipulation is done in a way that satisfies the formal privacy standard of
local differential privacy. To our knowledge, this is the first approach to
image privacy that satisfies $\varepsilon$-differential privacy \emph{for the
person.}
</p>
<a href="http://arxiv.org/abs/2103.05472" target="_blank">arXiv:2103.05472</a> [<a href="http://arxiv.org/pdf/2103.05472" target="_blank">pdf</a>]

<h2>Doubly Contrastive Deep Clustering. (arXiv:2103.05484v1 [cs.CV])</h2>
<h3>Zhiyuan Dang, Cheng Deng, Xu Yang, Heng Huang</h3>
<p>Deep clustering successfully provides more effective features than
conventional ones and thus becomes an important technique in current
unsupervised learning. However, most deep clustering methods ignore the vital
positive and negative pairs introduced by data augmentation and further the
significance of contrastive learning, which leads to suboptimal performance. In
this paper, we present a novel Doubly Contrastive Deep Clustering (DCDC)
framework, which constructs contrastive loss over both sample and class views
to obtain more discriminative features and competitive results. Specifically,
for the sample view, we set the class distribution of the original sample and
its augmented version as positive sample pairs and set one of the other
augmented samples as negative sample pairs. After that, we can adopt the
sample-wise contrastive loss to pull positive sample pairs together and push
negative sample pairs apart. Similarly, for the class view, we build the
positive and negative pairs from the sample distribution of the class. In this
way, two contrastive losses successfully constrain the clustering results of
mini-batch samples in both sample and class level. Extensive experimental
results on six benchmark datasets demonstrate the superiority of our proposed
model against state-of-the-art methods. Particularly in the challenging dataset
Tiny-ImageNet, our method leads 5.6\% against the latest comparison method. Our
code will be available at \url{https://github.com/ZhiyuanDang/DCDC}.
</p>
<a href="http://arxiv.org/abs/2103.05484" target="_blank">arXiv:2103.05484</a> [<a href="http://arxiv.org/pdf/2103.05484" target="_blank">pdf</a>]

<h2>TS-Net: OCR Trained to Switch Between Text Transcription Styles. (arXiv:2103.05489v1 [cs.CV])</h2>
<h3>Jan Koh&#xfa;t, Michal Hradi&#x161;</h3>
<p>Users of OCR systems, from different institutions and scientific disciplines,
prefer and produce different transcription styles. This presents a problem for
training of consistent text recognition neural networks on real-world data. We
propose to extend existing text recognition networks with a Transcription Style
Block (TSB) which can learn from data to switch between multiple transcription
styles without any explicit knowledge of transcription rules. TSB is an
adaptive instance normalization conditioned by identifiers representing
consistently transcribed documents (e.g. single document, documents by a single
transcriber, or an institution). We show that TSB is able to learn completely
different transcription styles in controlled experiments on artificial data, it
improves text recognition accuracy on large-scale real-world data, and it
learns semantically meaningful transcription style embedding. We also show how
TSB can efficiently adapt to transcription styles of new documents from
transcriptions of only a few text lines.
</p>
<a href="http://arxiv.org/abs/2103.05489" target="_blank">arXiv:2103.05489</a> [<a href="http://arxiv.org/pdf/2103.05489" target="_blank">pdf</a>]

<h2>Integrating Fast Regional Optimization into Sampling-based Kinodynamic Planning for Multirotor Flight. (arXiv:2103.05519v1 [cs.RO])</h2>
<h3>Hongkai Ye, Tianyu Liu, Chao Xu, Fei Gao</h3>
<p>For real-time multirotor kinodynamic motion planning, the efficiency of
sampling-based methods is usually hindered by difficult-to-sample homotopy
classes like narrow passages. In this paper, we address this issue by a hybrid
scheme. We firstly propose a fast regional optimizer exploiting the information
of local environments and then integrate it into a global sampling process to
ensure faster convergence. The incorporation of local optimization on different
sampling-based methods shows significantly improved success rates and less
planning time in various types of challenging environments. We also present a
refinement module that fully investigates the resulting trajectory of the
global sampling and greatly improves its smoothness with negligible computation
effort. Benchmark results illustrate that compared to the state-of-the-art
ones, our proposed method can better exploit a previous trajectory. The
planning methods are applied to generate trajectories for a simulated quadrotor
system, and its capability is validated in real-time applications.
</p>
<a href="http://arxiv.org/abs/2103.05519" target="_blank">arXiv:2103.05519</a> [<a href="http://arxiv.org/pdf/2103.05519" target="_blank">pdf</a>]

<h2>Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis. (arXiv:2103.05558v1 [cs.CV])</h2>
<h3>Chaoyi Zhang, Jianhui Yu, Yang Song, Weidong Cai</h3>
<p>Scene understanding is a critical problem in computer vision. In this paper,
we propose a 3D point-based scene graph generation ($\mathbf{SGG_{point}}$)
framework to effectively bridge perception and reasoning to achieve scene
understanding via three sequential stages, namely scene graph construction,
reasoning, and inference. Within the reasoning stage, an EDGE-oriented Graph
Convolutional Network ($\texttt{EdgeGCN}$) is created to exploit
multi-dimensional edge features for explicit relationship modeling, together
with the exploration of two associated twinning interaction mechanisms between
nodes and edges for the independent evolution of scene graph representations.
Overall, our integrated $\mathbf{SGG_{point}}$ framework is established to seek
and infer scene structures of interest from both real-world and synthetic 3D
point-based scenes. Our experimental results show promising edge-oriented
reasoning effects on scene graph generation studies. We also demonstrate our
method advantage on several traditional graph representation learning benchmark
datasets, including the node-wise classification on citation networks and
whole-graph recognition problems for molecular analysis.
</p>
<a href="http://arxiv.org/abs/2103.05558" target="_blank">arXiv:2103.05558</a> [<a href="http://arxiv.org/pdf/2103.05558" target="_blank">pdf</a>]

<h2>Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v1 [cs.CV])</h2>
<h3>Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, Soumen Chakrabarti</h3>
<p>Multimodal IR, spanning text corpus, knowledge graph and images, called
outside knowledge visual question answering (OKVQA), is of much recent
interest. However, the popular data set has serious limitations. A surprisingly
large fraction of queries do not assess the ability to integrate cross-modal
information. Instead, some are independent of the image, some depend on
speculation, some require OCR or are otherwise answerable from the image alone.
To add to the above limitations, frequency-based guessing is very effective
because of (unintended) widespread answer overlaps between the train and test
folds. Overall, it is hard to determine when state-of-the-art systems exploit
these weaknesses rather than really infer the answers, because they are opaque
and their 'reasoning' process is uninterpretable. An equally important
limitation is that the dataset is designed for the quantitative assessment only
of the end-to-end answer retrieval task, with no provision for assessing the
correct(semantic) interpretation of the input query. In response, we identify a
key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and
build a new data set and challenge around it. Specifically, the questioner
identifies an entity in the image and asks a question involving that entity
which can be answered only by consulting a knowledge graph or corpus passage
mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA
annotated based on the structural idiom and (ii)S3VQA, a new dataset built from
scratch. We also present a neural but structurally transparent OKVQA system,
S3, that explicitly addresses our challenge dataset, and outperforms recent
competitive baselines.
</p>
<a href="http://arxiv.org/abs/2103.05568" target="_blank">arXiv:2103.05568</a> [<a href="http://arxiv.org/pdf/2103.05568" target="_blank">pdf</a>]

<h2>FAIR1M: A Benchmark Dataset for Fine-grained Object Recognition in High-Resolution Remote Sensing Imagery. (arXiv:2103.05569v1 [cs.CV])</h2>
<h3>Xian Sun, Peijin Wang, Zhiyuan Yan, Cheng Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Kun Fu</h3>
<p>With the rapid development of deep learning, many deep learning based
approaches have made great achievements in object detection task. It is
generally known that deep learning is a data-driven method. Data directly
impact the performance of object detectors to some extent. Although existing
datasets have included common objects in remote sensing images, they still have
some limitations in terms of scale, categories, and images. Therefore, there is
a strong requirement for establishing a large-scale benchmark on object
detection in high-resolution remote sensing images. In this paper, we propose a
novel benchmark dataset with more than 1 million instances and more than 15,000
images for Fine-grAined object recognItion in high-Resolution remote sensing
imagery which is named as FAIR1M. All objects in the FAIR1M dataset are
annotated with respect to 5 categories and 37 sub-categories by oriented
bounding boxes. Compared with existing detection datasets dedicated to object
detection, the FAIR1M dataset has 4 particular characteristics: (1) it is much
larger than other existing object detection datasets both in terms of the
quantity of instances and the quantity of images, (2) it provides more rich
fine-grained category information for objects in remote sensing images, (3) it
contains geographic information such as latitude, longitude and resolution, (4)
it provides better image quality owing to a careful data cleaning procedure. To
establish a baseline for fine-grained object recognition, we propose a novel
evaluation method and benchmark fine-grained object detection tasks and a
visual classification task using several State-Of-The-Art (SOTA) deep learning
based models on our FAIR1M dataset. Experimental results strongly indicate that
the FAIR1M dataset is closer to practical application and it is considerably
more challenging than existing datasets.
</p>
<a href="http://arxiv.org/abs/2103.05569" target="_blank">arXiv:2103.05569</a> [<a href="http://arxiv.org/pdf/2103.05569" target="_blank">pdf</a>]

<h2>Risk-Averse RRT* Planning with Nonlinear Steering and Tracking Controllers for Nonlinear Robotic Systems Under Uncertainty. (arXiv:2103.05572v1 [cs.RO])</h2>
<h3>Sleiman Safaoui, Benjamin J. Gravell, Venkatraman Renganathan, Tyler H. Summers</h3>
<p>We propose a two-phase risk-averse architecture for controlling stochastic
nonlinear robotic systems. We present Risk-Averse Nonlinear Steering RRT*
(RANS-RRT*) as an RRT* variant that incorporates nonlinear dynamics by solving
a nonlinear program (NLP) and accounts for risk by approximating the state
distribution and performing a distributionally robust (DR) collision check to
promote safe planning.The generated plan is used as a reference for a low-level
tracking controller. We demonstrate three controllers: finite horizon linear
quadratic regulator (LQR) with linearized dynamics around the reference
trajectory, LQR with robustness-promoting multiplicative noise terms, and a
nonlinear model predictive control law (NMPC). We demonstrate the effectiveness
of our algorithm using unicycle dynamics under heavy-tailed Laplace process
noise in a cluttered environment.
</p>
<a href="http://arxiv.org/abs/2103.05572" target="_blank">arXiv:2103.05572</a> [<a href="http://arxiv.org/pdf/2103.05572" target="_blank">pdf</a>]

<h2>SimTriplet: Simple Triplet Representation Learning with a Single GPU. (arXiv:2103.05585v1 [cs.CV])</h2>
<h3>Quan Liu, Peter C. Louis, Yuzhe Lu, Aadarsh Jha, Mengyang Zhao, Ruining Deng, Tianyuan Yao, Joseph T. Roland, Haichun Yang, Shilin Zhao, Lee E. Wheless, Yuankai Huo</h3>
<p>Contrastive learning is a key technique of modern self-supervised learning.
The broader accessibility of earlier approaches is hindered by the need of
heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which
accommodate for large-scale negative samples or momentum. The more recent
SimSiam approach addresses such key limitations via stop-gradient without
momentum encoders. In medical image analysis, multiple instances can be
achieved from the same patient or tissue. Inspired by these advances, we
propose a simple triplet representation learning (SimTriplet) approach on
pathological images. The contribution of the paper is three-fold: (1) The
proposed SimTriplet method takes advantage of the multi-view nature of medical
images beyond self-augmentation; (2) The method maximizes both intra-sample and
inter-sample similarities via triplets from positive pairs, without using
negative samples; and (3) The recent mix precision training is employed to
advance the training by only using a single GPU with 16GB memory. By learning
from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58%
better performance compared with supervised learning. It also achieved 2.13%
better performance compared with SimSiam. Our proposed SimTriplet can achieve
decent performance using only 1% labeled data. The code and data are available
at https://github.com/hrlblab/SimTriple.
</p>
<a href="http://arxiv.org/abs/2103.05585" target="_blank">arXiv:2103.05585</a> [<a href="http://arxiv.org/pdf/2103.05585" target="_blank">pdf</a>]

<h2>unzipFPGA: Enhancing FPGA-based CNN Engines with On-the-Fly Weights Generation. (arXiv:2103.05600v1 [cs.CV])</h2>
<h3>Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane</h3>
<p>Single computation engines have become a popular design choice for FPGA-based
convolutional neural networks (CNNs) enabling the deployment of diverse models
without fabric reconfiguration. This flexibility, however, often comes with
significantly reduced performance on memory-bound layers and resource
underutilisation due to suboptimal mapping of certain layers on the engine's
fixed configuration. In this work, we investigate the implications in terms of
CNN engine design for a class of models that introduce a pre-convolution stage
to decompress the weights at run time. We refer to these approaches as
on-the-fly. To minimise the negative impact of limited bandwidth on
memory-bound layers, we present a novel hardware component that enables the
on-chip on-the-fly generation of weights. We further introduce an input
selective processing element (PE) design that balances the load between PEs on
suboptimally mapped layers. Finally, we present unzipFPGA, a framework to train
on-the-fly models and traverse the design space to select the highest
performing CNN engine configuration. Quantitative evaluation shows that
unzipFPGA yields an average speedup of 2.14x and 71% over optimised status-quo
and pruned CNN engines under constrained bandwidth and up to 3.69x higher
performance density over the state-of-the-art FPGA-based CNN accelerators.
</p>
<a href="http://arxiv.org/abs/2103.05600" target="_blank">arXiv:2103.05600</a> [<a href="http://arxiv.org/pdf/2103.05600" target="_blank">pdf</a>]

<h2>NeX: Real-time View Synthesis with Neural Basis Expansion. (arXiv:2103.05606v1 [cs.CV])</h2>
<h3>Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn</h3>
<p>We present NeX, a new approach to novel view synthesis based on enhancements
of multiplane image (MPI) that can reproduce next-level view-dependent effects
-- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$
planes, our technique models view-dependent effects by instead parameterizing
each pixel as a linear combination of basis functions learned from a neural
network. Moreover, we propose a hybrid implicit-explicit modeling strategy that
improves upon fine detail and produces state-of-the-art results. Our method is
evaluated on benchmark forward-facing datasets as well as our newly-introduced
dataset designed to test the limit of view-dependent modeling with
significantly more challenging effects such as rainbow reflections on a CD. Our
method achieves the best overall scores across all major metrics on these
datasets with more than 1000$\times$ faster rendering time than the state of
the art. For real-time demos, visit https://nex-mpi.github.io/
</p>
<a href="http://arxiv.org/abs/2103.05606" target="_blank">arXiv:2103.05606</a> [<a href="http://arxiv.org/pdf/2103.05606" target="_blank">pdf</a>]

<h2>Point-supervised Segmentation of Microscopy Images and Volumes via Objectness Regularization. (arXiv:2103.05617v1 [cs.CV])</h2>
<h3>Shijie Li, Neel Dey, Katharina Bermond, Leon von der Emde, Christine A. Curcio, Thomas Ach, Guido Gerig</h3>
<p>Annotation is a major hurdle in the semantic segmentation of microscopy
images and volumes due to its prerequisite expertise and effort. This work
enables the training of semantic segmentation networks on images with only a
single point for training per instance, an extreme case of weak supervision
which drastically reduces the burden of annotation. Our approach has two key
aspects: (1) we construct a graph-theoretic soft-segmentation using individual
seeds to be used within a regularizer during training and (2) we use an
objective function that enables learning from the constructed soft-labels. We
achieve competitive results against the state-of-the-art in point-supervised
semantic segmentation on challenging datasets in digital pathology. Finally, we
scale our methodology to point-supervised segmentation in 3D fluorescence
microscopy volumes, obviating the need for arduous manual volumetric
delineation. Our code is freely available.
</p>
<a href="http://arxiv.org/abs/2103.05617" target="_blank">arXiv:2103.05617</a> [<a href="http://arxiv.org/pdf/2103.05617" target="_blank">pdf</a>]

<h2>ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v1 [cs.CV])</h2>
<h3>Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, Ziwei Liu</h3>
<p>The rapid progress of photorealistic synthesis techniques has reached at a
critical point where the boundary between real and manipulated images starts to
blur. Thus, benchmarking and advancing digital forgery analysis have become a
pressing issue. However, existing face forgery datasets either have limited
diversity or only support coarse-grained analysis. To counter this emerging
threat, we construct the ForgeryNet dataset, an extremely large face forgery
dataset with unified annotations in image- and video-level data across four
tasks: 1) Image Forgery Classification, including two-way (real / fake),
three-way (real / fake with identity-replaced forgery approaches / fake with
identity-remained forgery approaches), and n-way (real and 15 respective
forgery approaches) classification. 2) Spatial Forgery Localization, which
segments the manipulated area of fake images compared to their corresponding
source real images. 3) Video Forgery Classification, which re-defines the
video-level forgery classification with manipulated frames in random positions.
This task is important because attackers in real world are free to manipulate
any target frame. and 4) Temporal Forgery Localization, to localize the
temporal segments which are manipulated. ForgeryNet is by far the largest
publicly available deep face forgery dataset in terms of data-scale (2.9
million images, 221,247 videos), manipulations (7 image-level approaches, 8
video-level approaches), perturbations (36 independent and more mixed
perturbations) and annotations (6.3 million classification labels, 2.9 million
manipulated area annotations and 221,247 temporal forgery segment labels). We
perform extensive benchmarking and studies of existing face forensics methods
and obtain several valuable observations.
</p>
<a href="http://arxiv.org/abs/2103.05630" target="_blank">arXiv:2103.05630</a> [<a href="http://arxiv.org/pdf/2103.05630" target="_blank">pdf</a>]

<h2>3D Face From X: Learning Face Shape from Diverse Sources. (arXiv:1808.05323v3 [cs.CV] UPDATED)</h2>
<h3>Yudong Guo, Lin Cai, Juyong Zhang</h3>
<p>We present a novel method to jointly learn a 3D face parametric model and 3D
face reconstruction from diverse sources. Previous methods usually learn 3D
face modeling from one kind of source, such as scanned data or in-the-wild
images. Although 3D scanned data contain accurate geometric information of face
shapes, the capture system is expensive and such datasets usually contain a
small number of subjects. On the other hand, in-the-wild face images are easily
obtained and there are a large number of facial images. However, facial images
do not contain explicit geometric information. In this paper, we propose a
method to learn a unified face model from diverse sources. Besides scanned face
data and face images, we also utilize a large number of RGB-D images captured
with an iPhone X to bridge the gap between the two sources. Experimental
results demonstrate that with training data from more sources, we can learn a
more powerful face model.
</p>
<a href="http://arxiv.org/abs/1808.05323" target="_blank">arXiv:1808.05323</a> [<a href="http://arxiv.org/pdf/1808.05323" target="_blank">pdf</a>]

<h2>An Efficient and Layout-Independent Automatic License Plate Recognition System Based on the YOLO detector. (arXiv:1909.01754v4 [cs.CV] UPDATED)</h2>
<h3>Rayson Laroca, Luiz A. Zanlorensi, Gabriel R. Gon&#xe7;alves, Eduardo Todt, William Robson Schwartz, David Menotti</h3>
<p>This paper presents an efficient and layout-independent Automatic License
Plate Recognition (ALPR) system based on the state-of-the-art YOLO object
detector that contains a unified approach for license plate (LP) detection and
layout classification to improve the recognition results using post-processing
rules. The system is conceived by evaluating and optimizing different models,
aiming at achieving the best speed/accuracy trade-off at each stage. The
networks are trained using images from several datasets, with the addition of
various data augmentation techniques, so that they are robust under different
conditions. The proposed system achieved an average end-to-end recognition rate
of 96.9% across eight public datasets (from five different regions) used in the
experiments, outperforming both previous works and commercial systems in the
ChineseLP, OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets. In the other
datasets, the proposed approach achieved competitive results to those attained
by the baselines. Our system also achieved impressive frames per second (FPS)
rates on a high-end GPU, being able to perform in real time even when there are
four vehicles in the scene. An additional contribution is that we manually
labeled 38,351 bounding boxes on 6,239 images from public datasets and made the
annotations publicly available to the research community.
</p>
<a href="http://arxiv.org/abs/1909.01754" target="_blank">arXiv:1909.01754</a> [<a href="http://arxiv.org/pdf/1909.01754" target="_blank">pdf</a>]

<h2>Discovering Latent Classes for Semi-Supervised Semantic Segmentation. (arXiv:1912.12936v4 [cs.CV] UPDATED)</h2>
<h3>Olga Zatsarynna, Johann Sawatzky, Juergen Gall</h3>
<p>High annotation costs are a major bottleneck for the training of semantic
segmentation systems. Therefore, methods working with less annotation effort
are of special interest. This paper studies the problem of semi-supervised
semantic segmentation. This means that only a small subset of the training
images is annotated while the other training images do not contain any
annotation. In order to leverage the information present in the unlabeled
images, we propose to learn a second task that is related to semantic
segmentation but easier. On labeled images, we learn latent classes consistent
with semantic classes so that the variety of semantic classes assigned to a
latent class is as low as possible. On unlabeled images, we predict a
probability map for latent classes and use it as a supervision signal to learn
semantic segmentation. The latent classes, as well as the semantic classes, are
simultaneously predicted by a two-branch network. In our experiments on Pascal
VOC and Cityscapes, we show that the latent classes learned this way have an
intuitive meaning and that the proposed method achieves state of the art
results for semi-supervised semantic segmentation.
</p>
<a href="http://arxiv.org/abs/1912.12936" target="_blank">arXiv:1912.12936</a> [<a href="http://arxiv.org/pdf/1912.12936" target="_blank">pdf</a>]

<h2>Compensation for undefined behaviors during robot task execution by switching controllers depending on embedded dynamics in RNN. (arXiv:2003.04862v2 [cs.RO] UPDATED)</h2>
<h3>Kanata Suzuki, Hiroki Mori, Tetsuya Ogata</h3>
<p>Robotic applications require both correct task performance and compensation
for undefined behaviors. Although deep learning is a promising approach to
perform complex tasks, the response to undefined behaviors that are not
reflected in the training dataset remains challenging. In a human-robot
collaborative task, the robot may adopt an unexpected posture due to collisions
and other unexpected events. Therefore, robots should be able to recover from
disturbances for completing the execution of the intended task. We propose a
compensation method for undefined behaviors by switching between two
controllers. Specifically, the proposed method switches between learning-based
and model-based controllers depending on the internal representation of a
recurrent neural network that learns task dynamics. We applied the proposed
method to a pick-and-place task and evaluated the compensation for undefined
behaviors. Experimental results from simulations and on a real robot
demonstrate the effectiveness and high performance of the proposed method.
</p>
<a href="http://arxiv.org/abs/2003.04862" target="_blank">arXiv:2003.04862</a> [<a href="http://arxiv.org/pdf/2003.04862" target="_blank">pdf</a>]

<h2>Three-Filters-to-Normal: An Accurate and Ultrafast Surface Normal Estimator. (arXiv:2005.08165v3 [cs.CV] UPDATED)</h2>
<h3>Rui Fan, Hengli Wang, Bohuan Xue, Huaiyang Huang, Yuan Wang, Ming Liu, Ioannis Pitas</h3>
<p>This paper proposes three-filters-to-normal (3F2N), an accurate and ultrafast
surface normal estimator (SNE), which is designed for structured range sensor
data, e.g., depth/disparity images. 3F2N SNE computes surface normals by simply
performing three filtering operations (two image gradient filters in horizontal
and vertical directions, respectively, and a mean/median filter) on an inverse
depth image or a disparity image. Despite the simplicity of 3F2N SNE, no
similar method already exists in the literature. To evaluate the performance of
our proposed SNE, we created three large-scale synthetic datasets (easy, medium
and hard) using 24 3D mesh models, each of which is used to generate 1800--2500
pairs of depth images (resolution: 480X640 pixels) and the corresponding
ground-truth surface normal maps from different views. 3F2N SNE demonstrates
the state-of-the-art performance, outperforming all other existing
geometry-based SNEs, where the average angular errors with respect to the easy,
medium and hard datasets are 1.66 degrees, 5.69 degrees and 15.31 degrees,
respectively. Furthermore, our C++ and CUDA implementations achieve a
processing speed of over 260 Hz and 21 kHz, respectively. Our datasets and
source code are publicly available at sites.google.com/view/3f2n.
</p>
<a href="http://arxiv.org/abs/2005.08165" target="_blank">arXiv:2005.08165</a> [<a href="http://arxiv.org/pdf/2005.08165" target="_blank">pdf</a>]

<h2>ResKD: Residual-Guided Knowledge Distillation. (arXiv:2006.04719v4 [cs.CV] UPDATED)</h2>
<h3>Xuewei Li, Songyuan Li, Bourahla Omar, Fei Wu, Xi Li</h3>
<p>Knowledge distillation, aimed at transferring the knowledge from a heavy
teacher network to a lightweight student network, has emerged as a promising
technique for compressing neural networks. However, due to the capacity gap
between the heavy teacher and the lightweight student, there still exists a
significant performance gap between them. In this paper, we see knowledge
distillation in a fresh light, using the knowledge gap, or the residual,
between a teacher and a student as guidance to train a much more lightweight
student, called a res-student. We combine the student and the res-student into
a new student, where the res-student rectifies the errors of the former
student. Such a residual-guided process can be repeated until the user strikes
the balance between accuracy and cost. At inference time, we propose a
sample-adaptive strategy to decide which res-students are not necessary for
each sample, which can save computational cost. Experimental results show that
we achieve competitive performance with 18.04$\%$, 23.14$\%$, 53.59$\%$, and
56.86$\%$ of the teachers' computational costs on the CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet datasets. Finally, we do thorough theoretical and
empirical analysis for our method.
</p>
<a href="http://arxiv.org/abs/2006.04719" target="_blank">arXiv:2006.04719</a> [<a href="http://arxiv.org/pdf/2006.04719" target="_blank">pdf</a>]

<h2>Simple and effective localized attribute representations for zero-shot learning. (arXiv:2006.05938v3 [cs.CV] UPDATED)</h2>
<h3>Shiqi Yang, Kai Wang, Luis Herranz, Joost van de Weijer</h3>
<p>Zero-shot learning (ZSL) aims to discriminate images from unseen classes by
exploiting relations to seen classes via their semantic descriptions. Some
recent papers have shown the importance of localized features together with
fine-tuning the feature extractor to obtain discriminative and transferable
features. However, these methods require complex attention or part detection
modules to perform explicit localization in the visual space. In contrast, in
this paper we propose localizing representations in the semantic/attribute
space, with a simple but effective pipeline where localization is implicit.
Focusing on attribute representations, we show that our method obtains
state-of-the-art performance on CUB and SUN datasets, and also achieves
competitive results on AWA2 dataset, outperforming generally more complex
methods with explicit localization in the visual space. Our method can be
implemented easily, which can be used as a new baseline for zero shot-learning.
In addition, our localized representations are highly interpretable as
attribute-specific heatmaps.
</p>
<a href="http://arxiv.org/abs/2006.05938" target="_blank">arXiv:2006.05938</a> [<a href="http://arxiv.org/pdf/2006.05938" target="_blank">pdf</a>]

<h2>Learning to Generate Novel Domains for Domain Generalization. (arXiv:2007.03304v3 [cs.CV] UPDATED)</h2>
<h3>Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang</h3>
<p>This paper focuses on domain generalization (DG), the task of learning from
multiple source domains a model that generalizes well to unseen domains. A main
challenge for DG is that the available source domains often exhibit limited
diversity, hampering the model's ability to learn to generalize. We therefore
employ a data generator to synthesize data from pseudo-novel domains to augment
the source domains. This explicitly increases the diversity of available
training domains and leads to a more generalizable model. To train the
generator, we model the distribution divergence between source and synthesized
pseudo-novel domains using optimal transport, and maximize the divergence. To
ensure that semantics are preserved in the synthesized data, we further impose
cycle-consistency and classification losses on the generator. Our method,
L2A-OT (Learning to Augment by Optimal Transport) outperforms current
state-of-the-art DG methods on four benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2007.03304" target="_blank">arXiv:2007.03304</a> [<a href="http://arxiv.org/pdf/2007.03304" target="_blank">pdf</a>]

<h2>Safe Active Dynamics Learning and Control: A Sequential Exploration-Exploitation Framework. (arXiv:2008.11700v2 [cs.RO] UPDATED)</h2>
<h3>Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, Marco Pavone</h3>
<p>To safely deploy learning-based systems in highly uncertain environments, one
must ensure that they always satisfy constraints. In this work, we propose a
practical and theoretically justified approach to maintaining safety in the
presence of dynamics uncertainty. Our approach leverages Bayesian meta-learning
with last-layer adaptation: the expressiveness of neural-network features
trained offline, paired with efficient last-layer online adaptation, enables
the derivation of tight confidence sets which contract around the true dynamics
as the model adapts online. We exploit these confidence sets to plan
trajectories that guarantee the safety of the system. Our approach handles
problems with high dynamics uncertainty where reaching the goal safely is
initially infeasible by first exploring to gather data and reduce uncertainty,
before autonomously exploiting the acquired information to safely perform the
task. Under reasonable assumptions, we prove that our framework provides safety
guarantees in the form of a single joint chance constraint. Furthermore, we use
this theoretical analysis to motivate regularization of the model to improve
performance. We extensively demonstrate our approach in simulation and on
hardware.
</p>
<a href="http://arxiv.org/abs/2008.11700" target="_blank">arXiv:2008.11700</a> [<a href="http://arxiv.org/pdf/2008.11700" target="_blank">pdf</a>]

<h2>Vision-Based Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v3 [cs.CV] UPDATED)</h2>
<h3>Ekta U. Samani, Xingjian Yang, Ashis G. Banerjee</h3>
<p>Object recognition in unseen indoor environments remains a challenging
problem for visual perception of mobile robots. In this letter, we propose the
use of topologically persistent features, which rely on the shape information
of the objects, to address this challenge. In particular, we extract two kinds
of features, namely, sparse persistence image (PI) and amplitude, by applying
persistent homology to multi-directional height function-based filtrations of
the cubical complexes representing the object segmentation maps. The features
are then used to train a fully connected network for recognition. For
performance evaluation, in addition to a widely used shape dataset, we collect
a new dataset comprising scene images from two different environments, namely,
a living room and a mock warehouse. The scenes in both the environments include
up to five different objects that are chosen from a given set of fourteen
objects. The objects have varying poses and arrangements, and are imaged under
different illumination conditions and camera poses. The recognition performance
of our methods, which are trained using the living room images, remains
relatively unaffected on the unseen warehouse images. In contrast, the
performance of the widely used Faster R-CNN and its state-of-the-art variant,
Domain Adaptive Faster R-CNN, drops significantly. Moreover, the use of sparse
PI features yields slightly higher overall recall and accuracy in the unseen
warehouse environment. We also implement the proposed method on a real-world
robot to demonstrate its usefulness.
</p>
<a href="http://arxiv.org/abs/2010.03196" target="_blank">arXiv:2010.03196</a> [<a href="http://arxiv.org/pdf/2010.03196" target="_blank">pdf</a>]

<h2>Uncertainty-aware Contact-safe Model-based Reinforcement Learning. (arXiv:2010.08169v3 [cs.RO] UPDATED)</h2>
<h3>Cheng-Yu Kuo, Andreas Schaarschmidt, Yunduan Cui, Tamim Asfour, Takamitsu Matsubara</h3>
<p>This letter presents contact-safe Model-based Reinforcement Learning (MBRL)
for robot applications that achieves contact-safe behaviors in the learning
process. In typical MBRL, we cannot expect the data-driven model to generate
accurate and reliable policies to the intended robotic tasks during the
learning process due to sample scarcity. Operating these unreliable policies in
a contact-rich environment could cause damage to the robot and its
surroundings. To alleviate the risk of causing damage through unexpected
intensive physical contacts, we present the contact-safe MBRL that associates
the probabilistic Model Predictive Control's (pMPC) control limits with the
model uncertainty so that the allowed acceleration of controlled behavior is
adjusted according to learning progress. Control planning with such
uncertainty-aware control limits is formulated as a deterministic MPC problem
using a computation-efficient approximated GP dynamics and an approximated
inference technique. Our approach's effectiveness is evaluated through bowl
mixing tasks with simulated and real robots, scooping tasks with a real robot
as examples of contact-rich manipulation skills. (video:
https://youtu.be/sdhHP3NhYi0)
</p>
<a href="http://arxiv.org/abs/2010.08169" target="_blank">arXiv:2010.08169</a> [<a href="http://arxiv.org/pdf/2010.08169" target="_blank">pdf</a>]

<h2>Fast and Robust Bio-inspired Teach and Repeat Navigation. (arXiv:2010.11326v2 [cs.RO] UPDATED)</h2>
<h3>Dominic Dall&#x27;Osto, Tobias Fischer, Michael Milford</h3>
<p>Fully autonomous mobile robots have a multitude of potential applications,
but guaranteeing robust navigation performance remains an open research
problem. For many tasks such as repeated infrastructure inspection, item
delivery or inventory transport, a route repeating capability can be sufficient
and offers potential practical advantages over a full navigation stack.
Previous teach and repeat research has achieved high performance in difficult
conditions generally by using sophisticated, often expensive sensors, and has
often had high computational requirements. Biological systems, such as small
animals and insects like seeing ants, offer a proof of concept that robust and
generalisable navigation can be achieved with extremely limited visual systems
and computing power. In this work we create a novel asynchronous formulation
for teach and repeat navigation that fully utilises odometry information,
paired with a correction signal driven by much more computationally lightweight
visual processing than is typically required. This correction signal is also
decoupled from the robot's motor control, allowing its rate to be modulated by
the available computing capacity. We evaluate this approach with extensive
experimentation on two different robotic platforms, the Consequential Robotics
Miro and the Clearpath Jackal robots, across navigation trials totalling more
than 6000 metres in a range of challenging indoor and outdoor environments. Our
approach continues to succeed when multiple state-of-the-art systems fail due
to low resolution images, unreliable odometry, or lighting change, while
requiring significantly less compute. We also -- for the first time --
demonstrate versatile cross-platform teach and repeat without changing
parameters, in which we learn to navigate a route with one robot and repeat
that route using another robot with different camera.
</p>
<a href="http://arxiv.org/abs/2010.11326" target="_blank">arXiv:2010.11326</a> [<a href="http://arxiv.org/pdf/2010.11326" target="_blank">pdf</a>]

<h2>Occlusion-Aware Search for Object Retrieval in Clutter. (arXiv:2011.03334v3 [cs.RO] UPDATED)</h2>
<h3>Wissam Bejjani, Wisdom C. Agboh, Mehmet R. Dogar, Matteo Leonetti</h3>
<p>We address the manipulation task of retrieving a target object from a
cluttered shelf. When the target object is hidden, the robot must search
through the clutter for retrieving it. Solving this task requires reasoning
over the likely locations of the target object. It also requires physics
reasoning over multi-object interactions and future occlusions. In this work,
we present a data-driven approach for generating occlusion-aware actions in
closed-loop. We present a hybrid planner that explores likely states generated
from a learned distribution over the location of the target object. The search
is guided by a heuristic trained with reinforcement learning to evaluate
observations with occlusions. We evaluate our approach in different simulation
and real-world settings. The results validate that our approach can search and
retrieve a target object in near real time in the real world while only being
trained in simulation.
</p>
<a href="http://arxiv.org/abs/2011.03334" target="_blank">arXiv:2011.03334</a> [<a href="http://arxiv.org/pdf/2011.03334" target="_blank">pdf</a>]

<h2>RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v4 [cs.CV] UPDATED)</h2>
<h3>Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou</h3>
<p>We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video
Frame Interpolation (VFI). Most existing flow-based methods first estimate the
bi-directional optical flows, then scale and reverse them to approximate
intermediate flows, leading to artifacts on motion boundaries. RIFE uses a
neural network named IFNet that can directly estimate the intermediate flows
from images with much better speed. Based on our proposed leakage distillation
loss, RIFE can be trained in an end-to-end fashion. Experiments demonstrate
that our method is flexible and can achieve impressive performance on several
public benchmarks. The code is available at
https://github.com/hzwer/arXiv2020-RIFE.
</p>
<a href="http://arxiv.org/abs/2011.06294" target="_blank">arXiv:2011.06294</a> [<a href="http://arxiv.org/pdf/2011.06294" target="_blank">pdf</a>]

<h2>Online Monitoring of Object Detection Performance During Deployment. (arXiv:2011.07750v2 [cs.CV] UPDATED)</h2>
<h3>Quazi Marufur Rahman, Niko S&#xfc;nderhauf, Feras Dayoub</h3>
<p>During deployment, an object detector is expected to operate at a similar
performance level reported on its testing dataset. However, when deployed
onboard mobile robots that operate under varying and complex environmental
conditions, the detector's performance can fluctuate and occasionally degrade
severely without warning. Undetected, this can lead the robot to take unsafe
and risky actions based on low-quality and unreliable object detections. We
address this problem and introduce a cascaded neural network that monitors the
performance of the object detector by predicting the quality of its mean
average precision (mAP) on a sliding window of the input frames. The proposed
cascaded network exploits the internal features from the deep neural network of
the object detector. We evaluate our proposed approach using different
combinations of autonomous driving datasets and object detectors.
</p>
<a href="http://arxiv.org/abs/2011.07750" target="_blank">arXiv:2011.07750</a> [<a href="http://arxiv.org/pdf/2011.07750" target="_blank">pdf</a>]

<h2>Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning. (arXiv:2011.10043v2 [cs.CV] UPDATED)</h2>
<h3>Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, Han Hu</h3>
<p>Contrastive learning methods for unsupervised visual representation learning
have reached remarkable levels of transfer performance. We argue that the power
of contrastive learning has yet to be fully unleashed, as current methods are
trained only on instance-level pretext tasks, leading to representations that
may be sub-optimal for downstream tasks requiring dense pixel predictions. In
this paper, we introduce pixel-level pretext tasks for learning dense feature
representations. The first task directly applies contrastive learning at the
pixel level. We additionally propose a pixel-to-propagation consistency task
that produces better results, even surpassing the state-of-the-art approaches
by a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2
mIoU when transferred to Pascal VOC object detection (C4), COCO object
detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50
backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the
previous best methods built on instance-level contrastive learning. Moreover,
the pixel-level pretext tasks are found to be effective for pre-training not
only regular backbone networks but also head networks used for dense downstream
tasks, and are complementary to instance-level contrastive methods. These
results demonstrate the strong potential of defining pretext tasks at the pixel
level, and suggest a new path forward in unsupervised visual representation
learning. Code is available at \url{https://github.com/zdaxie/PixPro}.
</p>
<a href="http://arxiv.org/abs/2011.10043" target="_blank">arXiv:2011.10043</a> [<a href="http://arxiv.org/pdf/2011.10043" target="_blank">pdf</a>]

<h2>Unsupervised Path Regression Networks. (arXiv:2011.14787v2 [cs.RO] UPDATED)</h2>
<h3>Michal P&#xe1;ndy, Daniel Lenton, Ronald Clark</h3>
<p>We demonstrate that challenging shortest path problems can be solved via
direct spline regression from a neural network, trained in an unsupervised
manner (i.e. without requiring ground truth optimal paths for training). To
achieve this, we derive a geometry-dependent optimal cost function whose minima
guarantees collision-free solutions. Our method beats state-of-the-art
supervised learning baselines for shortest path planning, with a much more
scalable training pipeline, and a significant speedup in inference time.
</p>
<a href="http://arxiv.org/abs/2011.14787" target="_blank">arXiv:2011.14787</a> [<a href="http://arxiv.org/pdf/2011.14787" target="_blank">pdf</a>]

<h2>Vehicle trajectory prediction in top-view image sequences based on deep learning method. (arXiv:2102.01749v2 [cs.CV] UPDATED)</h2>
<h3>Zahra Salahshoori Nejad, Hamed Heravi, Ali Rahimpour Jounghani, Abdollah Shahrezaie, Afshin Ebrahimi</h3>
<p>Annually, a large number of injuries and deaths around the world are related
to motor vehicle accidents. This value has recently been reduced to some
extent, via the use of driver-assistance systems. Developing driver-assistance
systems (i.e., automated driving systems) can play a crucial role in reducing
this number. Estimating and predicting surrounding vehicles' movement is
essential for an automated vehicle and advanced safety systems. Moreover,
predicting the trajectory is influenced by numerous factors, such as drivers'
behavior during accidents, history of the vehicle's movement and the
surrounding vehicles, and their position on the traffic scene. The vehicle must
move over a safe path in traffic and react to other drivers' unpredictable
behaviors in the shortest time. Herein, to predict automated vehicles' path, a
model with low computational complexity is proposed, which is trained by images
taken from the road's aerial image. Our method is based on an encoder-decoder
model that utilizes a social tensor to model the effect of the surrounding
vehicles' movement on the target vehicle. The proposed model can predict the
vehicle's future path in any freeway only by viewing the images related to the
history of the target vehicle's movement and its neighbors. Deep learning was
used as a tool for extracting the features of these images. Using the HighD
database, an image dataset of the road's aerial image was created, and the
model's performance was evaluated on this new database. We achieved the RMSE of
1.91 for the next 5 seconds and found that the proposed method had less error
than the best path-prediction methods in previous studies.
</p>
<a href="http://arxiv.org/abs/2102.01749" target="_blank">arXiv:2102.01749</a> [<a href="http://arxiv.org/pdf/2102.01749" target="_blank">pdf</a>]

<h2>Regional Attention with Architecture-Rebuilt 3D Network for RGB-D Gesture Recognition. (arXiv:2102.05348v2 [cs.CV] UPDATED)</h2>
<h3>Benjia Zhou, Yunan Li, Jun Wan</h3>
<p>Human gesture recognition has drawn much attention in the area of computer
vision. However, the performance of gesture recognition is always influenced by
some gesture-irrelevant factors like the background and the clothes of
performers. Therefore, focusing on the regions of hand/arm is important to the
gesture recognition. Meanwhile, a more adaptive architecture-searched network
structure can also perform better than the block-fixed ones like Resnet since
it increases the diversity of features in different stages of the network
better. In this paper, we propose a regional attention with
architecture-rebuilt 3D network (RAAR3DNet) for gesture recognition. We replace
the fixed Inception modules with the automatically rebuilt structure through
the network via Neural Architecture Search (NAS), owing to the different shape
and representation ability of features in the early, middle, and late stage of
the network. It enables the network to capture different levels of feature
representations at different layers more adaptively. Meanwhile, we also design
a stackable regional attention module called dynamic-static Attention (DSA),
which derives a Gaussian guidance heatmap and dynamic motion map to highlight
the hand/arm regions and the motion information in the spatial and temporal
domains, respectively. Extensive experiments on two recent large-scale RGB-D
gesture datasets validate the effectiveness of the proposed method and show it
outperforms state-of-the-art methods. The codes of our method are available at:
https://github.com/zhoubenjia/RAAR3DNet.
</p>
<a href="http://arxiv.org/abs/2102.05348" target="_blank">arXiv:2102.05348</a> [<a href="http://arxiv.org/pdf/2102.05348" target="_blank">pdf</a>]

<h2>Minimizing false negative rate in melanoma detection and providing insight into the causes of classification. (arXiv:2102.09199v3 [cs.CV] UPDATED)</h2>
<h3>Ell&#xe1;k Somfai, Benj&#xe1;min Baffy, Kristian Fenech, Changlu Guo, Rita Hossz&#xfa;, Dorina Kor&#xf3;zs, Fabrizio Nunnari, Marcell P&#xf3;lik, Daniel Sonntag, Attila Ulbert, Andr&#xe1;s L&#x151;rincz</h3>
<p>Our goal is to bridge human and machine intelligence in melanoma detection.
We develop a classification system exploiting a combination of visual
pre-processing, deep learning, and ensembling for providing explanations to
experts and to minimize false negative rate while maintaining high accuracy in
melanoma detection. Source images are first automatically segmented using a
U-net CNN. The result of the segmentation is then used to extract image
sub-areas and specific parameters relevant in human evaluation, namely center,
border, and asymmetry measures. These data are then processed by tailored
neural networks which include structure searching algorithms. Partial results
are then ensembled by a committee machine. Our evaluation on the largest skin
lesion dataset which is publicly available today, ISIC-2019, shows improvement
in all evaluated metrics over a baseline using the original images only. We
also showed that indicative scores computed by the feature classifiers can
provide useful insight into the various features on which the decision can be
based.
</p>
<a href="http://arxiv.org/abs/2102.09199" target="_blank">arXiv:2102.09199</a> [<a href="http://arxiv.org/pdf/2102.09199" target="_blank">pdf</a>]

<h2>GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation. (arXiv:2102.12145v3 [cs.CV] UPDATED)</h2>
<h3>Gu Wang, Fabian Manhardt, Federico Tombari, Xiangyang Ji</h3>
<p>6D pose estimation from a single RGB image is a fundamental task in computer
vision. The current top-performing deep learning-based methods rely on an
indirect strategy, i.e., first establishing 2D-3D correspondences between the
coordinates in the image plane and object coordinate system, and then applying
a variant of the P$n$P/RANSAC algorithm. However, this two-stage pipeline is
not end-to-end trainable, thus is hard to be employed for many tasks requiring
differentiable poses. On the other hand, methods based on direct regression are
currently inferior to geometry-based methods. In this work, we perform an
in-depth investigation on both direct and indirect methods, and propose a
simple yet effective Geometry-guided Direct Regression Network (GDR-Net) to
learn the 6D pose in an end-to-end manner from dense correspondence-based
intermediate geometric representations. Extensive experiments show that our
approach remarkably outperforms state-of-the-art methods on LM, LM-O and YCB-V
datasets. Code is available at https://git.io/GDR-Net.
</p>
<a href="http://arxiv.org/abs/2102.12145" target="_blank">arXiv:2102.12145</a> [<a href="http://arxiv.org/pdf/2102.12145" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filter and Smoothing for Inertial-Integrated Navigation. (arXiv:2102.12897v5 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Chi Guo, Shenyong You, Jianlang Hu, Jingnan Liu</h3>
<p>The error representation using the straight difference of two vectors in the
inertial navigation system may not be reasonable as it does not take the
direction difference into consideration. Therefore, we proposed to use the
$SE_2(3)$ matrix Lie group to represent the state of the inertial-integrated
navigation system which consequently leads to the common frame error
representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down local-level navigation frame and the earth-centered earth-fixed
frame, respectively, the corresponding extending Kalman filter (EKF), terms as
$SE_2(3)$-EKF has also been derived. It provides a new perspective on the
geometric EKF with a more sophisticated formula for the inertial-integrated
navigation system. Furthermore, we propose a $SE_2(3)$-based smoothing
algorithm based on the $SE_2(3)$-based EKF.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>Open-set Intersection Intention Prediction for Autonomous Driving. (arXiv:2103.00140v2 [cs.RO] UPDATED)</h2>
<h3>Fei Li, Xiangxu Li, Jun Luo, Shiwei Fan, Hongbo Zhang</h3>
<p>Intention prediction is a crucial task for Autonomous Driving (AD). Due to
the variety of size and layout of intersections, it is challenging to predict
intention of human driver at different intersections, especially unseen and
irregular intersections. In this paper, we formulate the prediction of
intention at intersections as an open-set prediction problem that requires
context specific matching of the target vehicle state and the diverse
intersection configurations that are in principle unbounded. We capture
map-centric features that correspond to intersection structures under a
spatial-temporal graph representation, and use two MAAMs (mutually auxiliary
attention module) that cover respectively lane-level and exitlevel intentions
to predict a target that best matches intersection elements in map-centric
feature space. Under our model, attention scores estimate the probability
distribution of the openset intentions that are contextually defined by the
structure of the current intersection. The proposed model is trained and
evaluated on simulated dataset. Furthermore, the model, trained on simulated
dataset and without any fine tuning, is directly validated on in-house
real-world dataset collected at 98 realworld intersections and exhibits
satisfactory performance,demonstrating the practical viability of our approach.
</p>
<a href="http://arxiv.org/abs/2103.00140" target="_blank">arXiv:2103.00140</a> [<a href="http://arxiv.org/pdf/2103.00140" target="_blank">pdf</a>]

<h2>Self-Supervised Longitudinal Neighbourhood Embedding. (arXiv:2103.03840v2 [cs.CV] UPDATED)</h2>
<h3>Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Edith V Sullivan, Adolf Pfefferbaum, Greg Zaharchuk, Kilian M Pohl</h3>
<p>Longitudinal MRIs are often used to capture the gradual deterioration of
brain structure and function caused by aging or neurological diseases.
Analyzing this data via machine learning generally requires a large number of
ground-truth labels, which are often missing or expensive to obtain. Reducing
the need for labels, we propose a self-supervised strategy for representation
learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts
in contrastive learning, LNE explicitly models the similarity between
trajectory vectors across different subjects. We do so by building a graph in
each training iteration defining neighborhoods in the latent space so that the
progression direction of a subject follows the direction of its neighbors. This
results in a smooth trajectory field that captures the global morphological
change of the brain while maintaining the local continuity. We apply LNE to
longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274
healthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,
N=632). The visualization of the smooth trajectory vector field and superior
performance on downstream tasks demonstrate the strength of the proposed method
over existing self-supervised methods in extracting information associated with
normal aging and in revealing the impact of neurodegenerative disorders. The
code is available at
\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.
</p>
<a href="http://arxiv.org/abs/2103.03840" target="_blank">arXiv:2103.03840</a> [<a href="http://arxiv.org/pdf/2103.03840" target="_blank">pdf</a>]

<h2>Bilateral Control-Based Imitation Learning for Velocity-Controlled Robot. (arXiv:2103.04004v2 [cs.RO] UPDATED)</h2>
<h3>Sho Sakaino</h3>
<p>Machine learning is now playing important role in robotic object
manipulation. In addition, force control is necessary for manipulating various
objects to achieve robustness against perturbations of configurations and
stiffness. The author's group revealed that fast and dynamic object
manipulation with force control can be obtained by bilateral control-based
imitation learning. However, the method is applicable only in robots that can
control torque, while it is not applicable in robots that can only follow
position and velocity commands like many commercially available robots. Then,
in this research, a way to implement bilateral control-based imitation learning
to velocity-controlled robots is proposed. The validity of the proposed method
is experimentally verified by a mopping task.
</p>
<a href="http://arxiv.org/abs/2103.04004" target="_blank">arXiv:2103.04004</a> [<a href="http://arxiv.org/pdf/2103.04004" target="_blank">pdf</a>]

<h2>Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction. (arXiv:2103.04216v2 [cs.CV] UPDATED)</h2>
<h3>Wei Yin, Yifan Liu, Chunhua Shen</h3>
<p>Monocular depth prediction plays a crucial role in understanding 3D scene
geometry. Although recent methods have achieved impressive progress in terms of
evaluation metrics such as the pixel-wise relative error, most methods neglect
the geometric constraints in the 3D space. In this work, we show the importance
of the high-order 3D geometric constraints for depth prediction. By designing a
loss term that enforces a simple geometric constraint, namely, virtual normal
directions determined by randomly sampled three points in the reconstructed 3D
space, we significantly improve the accuracy and robustness of monocular depth
estimation. Significantly, the virtual normal loss can not only improve the
performance of learning metric depth, but also disentangle the scale
information and enrich the model with better shape information. Therefore, when
not having access to absolute metric depth training data, we can use virtual
normal to learn a robust affine-invariant depth generated on diverse scenes. In
experiments, We show state-of-the-art results of learning metric depth on NYU
Depth-V2 and KITTI. From the high-quality predicted depth, we are now able to
recover good 3D structures of the scene such as the point cloud and surface
normal directly, eliminating the necessity of relying on additional models as
was previously done. To demonstrate the excellent generalizability of learning
affine-invariant depth on diverse data with the virtual normal loss, we
construct a large-scale and diverse dataset for training affine-invariant
depth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five
datasets with the zero-shot test setting. Code is available at:
https://git.io/Depth
</p>
<a href="http://arxiv.org/abs/2103.04216" target="_blank">arXiv:2103.04216</a> [<a href="http://arxiv.org/pdf/2103.04216" target="_blank">pdf</a>]

<h2>Repurposing GANs for One-shot Semantic Part Segmentation. (arXiv:2103.04379v2 [cs.CV] UPDATED)</h2>
<h3>Nontawat Tritrong, Pitchaporn Rewatbowornwong, Supasorn Suwajanakorn</h3>
<p>While GANs have shown success in realistic image generation, the idea of
using GANs for other tasks unrelated to synthesis is underexplored. Do GANs
learn meaningful structural parts of objects during their attempt to reproduce
those objects? In this work, we test this hypothesis and propose a simple and
effective approach based on GANs for semantic part segmentation that requires
as few as one label example along with an unlabeled dataset. Our key idea is to
leverage a trained GAN to extract pixel-wise representation from the input
image and use it as feature vectors for a segmentation network. Our experiments
demonstrate that GANs representation is "readily discriminative" and produces
surprisingly good results that are comparable to those from supervised
baselines trained with significantly more labels. We believe this novel
repurposing of GANs underlies a new class of unsupervised representation
learning that is applicable to many other tasks. More results are available at
https://repurposegans.github.io/.
</p>
<a href="http://arxiv.org/abs/2103.04379" target="_blank">arXiv:2103.04379</a> [<a href="http://arxiv.org/pdf/2103.04379" target="_blank">pdf</a>]

<h2>Parser-Free Virtual Try-on via Distilling Appearance Flows. (arXiv:2103.04559v2 [cs.CV] UPDATED)</h2>
<h3>Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, Ping Luo</h3>
<p>Image virtual try-on aims to fit a garment image (target clothes) to a person
image. Prior methods are heavily based on human parsing. However,
slightly-wrong segmentation results would lead to unrealistic try-on images
with large artifacts. Inaccurate parsing misleads parser-based methods to
produce visually unrealistic results where artifacts usually occur. A recent
pioneering work employed knowledge distillation to reduce the dependency of
human parsing, where the try-on images produced by a parser-based method are
used as supervisions to train a "student" network without relying on
segmentation, making the student mimic the try-on ability of the parser-based
model. However, the image quality of the student is bounded by the parser-based
model. To address this problem, we propose a novel approach,
"teacher-tutor-student" knowledge distillation, which is able to produce highly
photo-realistic images without human parsing, possessing several appealing
advantages compared to prior arts. (1) Unlike existing work, our approach
treats the fake images produced by the parser-based method as "tutor
knowledge", where the artifacts can be corrected by real "teacher knowledge",
which is extracted from the real person images in a self-supervised way. (2)
Other than using real images as supervisions, we formulate knowledge
distillation in the try-on problem as distilling the appearance flows between
the person image and the garment image, enabling us to find accurate dense
correspondences between them to produce high-quality results. (3) Extensive
evaluations show large superiority of our method (see Fig. 1).
</p>
<a href="http://arxiv.org/abs/2103.04559" target="_blank">arXiv:2103.04559</a> [<a href="http://arxiv.org/pdf/2103.04559" target="_blank">pdf</a>]

<h2>Interpretable Attention Guided Network for Fine-grained Visual Classification. (arXiv:2103.04701v2 [cs.CV] UPDATED)</h2>
<h3>Zhenhuan Huang, Xiaoyue Duan, Bo Zhao, Jinhu L&#xfc;, Baochang Zhang</h3>
<p>Fine-grained visual classification (FGVC) is challenging but more critical
than traditional classification tasks. It requires distinguishing different
subcategories with the inherently subtle intra-class object variations.
Previous works focus on enhancing the feature representation ability using
multiple granularities and discriminative regions based on the attention
strategy or bounding boxes. However, these methods highly rely on deep neural
networks which lack interpretability. We propose an Interpretable Attention
Guided Network (IAGN) for fine-grained visual classification. The contributions
of our method include: i) an attention guided framework which can guide the
network to extract discriminitive regions in an interpretable way; ii) a
progressive training mechanism obtained to distill knowledge stage by stage to
fuse features of various granularities; iii) the first interpretable FGVC
method with a competitive performance on several standard FGVC benchmark
datasets.
</p>
<a href="http://arxiv.org/abs/2103.04701" target="_blank">arXiv:2103.04701</a> [<a href="http://arxiv.org/pdf/2103.04701" target="_blank">pdf</a>]

<h2>The Weakly-Labeled Rand Index. (arXiv:2103.04872v2 [cs.CV] UPDATED)</h2>
<h3>Dylan Stewart, Anna Hampton, Alina Zare, Jeff Dale, James Keller</h3>
<p>Synthetic Aperture Sonar (SAS) surveys produce imagery with large regions of
transition between seabed types. Due to these regions, it is difficult to label
and segment the imagery and, furthermore, challenging to score the image
segmentations appropriately. While there are many approaches to quantify
performance in standard crisp segmentation schemes, drawing hard boundaries in
remote sensing imagery where gradients and regions of uncertainty exist is
inappropriate. These cases warrant weak labels and an associated appropriate
scoring approach. In this paper, a labeling approach and associated modified
version of the Rand index for weakly-labeled data is introduced to address
these issues. Results are evaluated with the new index and compared to
traditional segmentation evaluation methods. Experimental results on a SAS data
set containing must-link and cannot-link labels show that our Weakly-Labeled
Rand index scores segmentations appropriately in reference to qualitative
performance and is more suitable than traditional quantitative metrics for
scoring weakly-labeled data.
</p>
<a href="http://arxiv.org/abs/2103.04872" target="_blank">arXiv:2103.04872</a> [<a href="http://arxiv.org/pdf/2103.04872" target="_blank">pdf</a>]

<h2>Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction. (arXiv:2103.04174v1 [cs.CV] CROSS LISTED)</h2>
<h3>Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, Chelsea Finn</h3>
<p>A video prediction model that generalizes to diverse scenes would enable
intelligent agents such as robots to perform a variety of tasks via planning
with the model. However, while existing video prediction models have produced
promising results on small datasets, they suffer from severe underfitting when
trained on large and diverse datasets. To address this underfitting challenge,
we first observe that the ability to train larger video prediction models is
often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep
hierarchical latent variable models can produce higher quality predictions by
capturing the multi-level stochasticity of future observations, but end-to-end
optimization of such models is notably difficult. Our key insight is that
greedy and modular optimization of hierarchical autoencoders can simultaneously
address both the memory constraints and the optimization challenges of
large-scale video prediction. We introduce Greedy Hierarchical Variational
Autoencoders (GHVAEs), a method that learns high-fidelity video predictions by
greedily training each level of a hierarchical autoencoder. In comparison to
state-of-the-art models, GHVAEs provide 17-55% gains in prediction performance
on four video datasets, a 35-40% higher success rate on real robot tasks, and
can improve performance monotonically by simply adding more modules.
</p>
<a href="http://arxiv.org/abs/2103.04174" target="_blank">arXiv:2103.04174</a> [<a href="http://arxiv.org/pdf/2103.04174" target="_blank">pdf</a>]

