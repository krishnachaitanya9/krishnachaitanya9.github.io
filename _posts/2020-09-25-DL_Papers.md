---
title: Latest Deep Learning Papers
date: 2021-03-14 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (74 Articles)</h1>
<h2>HumanGAN: A Generative Model of Humans Images. (arXiv:2103.06902v1 [cs.CV])</h2>
<h3>Kripasindhu Sarkar, Lingjie Liu, Vladislav Golyanik, Christian Theobalt</h3>
<p>Generative adversarial networks achieve great performance in photorealistic
image synthesis in various domains, including human images. However, they
usually employ latent vectors that encode the sampled outputs globally. This
does not allow convenient control of semantically-relevant individual parts of
the image, and is not able to draw samples that only differ in partial aspects,
such as clothing style. We address these limitations and present a generative
model for images of dressed humans offering control over pose, local body part
appearance and garment style. This is the first method to solve various aspects
of human image generation such as global appearance sampling, pose transfer,
parts and garment transfer, and parts sampling jointly in a unified framework.
As our model encodes part-based latent appearance vectors in a normalized
pose-independent space and warps them to different poses, it preserves body and
clothing appearance under varying posture. Experiments show that our flexible
and general generative method outperforms task-specific baselines for
pose-conditioned image generation, pose transfer and part sampling in terms of
realism and output resolution.
</p>
<a href="http://arxiv.org/abs/2103.06902" target="_blank">arXiv:2103.06902</a> [<a href="http://arxiv.org/pdf/2103.06902" target="_blank">pdf</a>]

<h2>Impact Invariant Control with Applications to Bipedal Locomotion. (arXiv:2103.06907v1 [cs.RO])</h2>
<h3>William Yang, Michael Posa</h3>
<p>When legged robots impact their environment, they undergo large changes in
their velocities in a small amount of time. Measuring and applying feedback to
these velocities is challenging, and is further complicated due to uncertainty
in the impact model and impact timing. This work proposes a general framework
for adapting feedback control during impact by projecting the control
objectives to a subspace that is invariant to the impact event. The resultant
controller is robust to uncertainties in the impact event while maintaining
maximum control authority over the impact invariant subspace. We demonstrate
the utility of the projection on a walking controller for a planar
five-link-biped and on a jumping controller for a compliant 3D bipedal robot,
Cassie. The effectiveness of our method is shown to translate well on hardware.
</p>
<a href="http://arxiv.org/abs/2103.06907" target="_blank">arXiv:2103.06907</a> [<a href="http://arxiv.org/pdf/2103.06907" target="_blank">pdf</a>]

<h2>CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v1 [cs.CV])</h2>
<h3>Tianyu Zhao, Qiaojun Feng, Sai Jadhav, Nikolay Atanasov</h3>
<p>This paper considers online object-level mapping using partial point-cloud
observations obtained online in an unknown environment. We develop and approach
for fully Convolutional Object Retrieval and Symmetry-AIded Registration
(CORSAIR). Our model extends the Fully Convolutional Geometric Features model
to learn a global object-shape embedding in addition to local point-wise
features from the point-cloud observations. The global feature is used to
retrieve a similar object from a category database, and the local features are
used for robust pose registration between the observed and the retrieved
object. Our formulation also leverages symmetries, present in the object
shapes, to obtain promising local-feature pairs from different symmetry classes
for matching. We present results from synthetic and real-world datasets with
different object categories to verify the robustness of our method.
</p>
<a href="http://arxiv.org/abs/2103.06911" target="_blank">arXiv:2103.06911</a> [<a href="http://arxiv.org/pdf/2103.06911" target="_blank">pdf</a>]

<h2>DefakeHop: A Light-Weight High-Performance Deepfake Detector. (arXiv:2103.06929v1 [cs.CV])</h2>
<h3>Hong-Shuo Chen, Mozhdeh Rouhsedaghat, Hamza Ghani, Shuowen Hu, Suya You, C.-C. Jay Kuo</h3>
<p>A light-weight high-performance Deepfake detection method, called DefakeHop,
is proposed in this work. State-of-the-art Deepfake detection methods are built
upon deep neural networks. DefakeHop extracts features automatically using the
successive subspace learning (SSL) principle from various parts of face images.
The features are extracted by c/w Saab transform and further processed by our
feature distillation module using spatial dimension reduction and soft
classification for each channel to get a more concise description of the face.
Extensive experiments are conducted to demonstrate the effectiveness of the
proposed DefakeHop method. With a small model size of 42,845 parameters,
DefakeHop achieves state-of-the-art performance with the area under the ROC
curve (AUC) of 100%, 94.95%, and 90.56% on UADFV, Celeb-DF v1 and Celeb-DF v2
datasets, respectively.
</p>
<a href="http://arxiv.org/abs/2103.06929" target="_blank">arXiv:2103.06929</a> [<a href="http://arxiv.org/pdf/2103.06929" target="_blank">pdf</a>]

<h2>The Semi-Supervised iNaturalist-Aves Challenge at FGVC7 Workshop. (arXiv:2103.06937v1 [cs.CV])</h2>
<h3>Jong-Chyi Su, Subhransu Maji</h3>
<p>This document describes the details and the motivation behind a new dataset
we collected for the semi-supervised recognition challenge~\cite{semi-aves} at
the FGVC7 workshop at CVPR 2020. The dataset contains 1000 species of birds
sampled from the iNat-2018 dataset for a total of nearly 150k images. From this
collection, we sample a subset of classes and their labels, while adding the
images from the remaining classes to the unlabeled set of images. The presence
of out-of-domain data (novel classes), high class-imbalance, and fine-grained
similarity between classes poses significant challenges for existing
semi-supervised recognition techniques in the literature. The dataset is
available here: \url{https://github.com/cvl-umass/semi-inat-2020}
</p>
<a href="http://arxiv.org/abs/2103.06937" target="_blank">arXiv:2103.06937</a> [<a href="http://arxiv.org/pdf/2103.06937" target="_blank">pdf</a>]

<h2>Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and 3D Keypoint Sets. (arXiv:2103.06966v1 [cs.CV])</h2>
<h3>Laurent Chauvin, Kuldeep Kumar, Christian Desrosiers, William Wells III, Matthew Toews</h3>
<p>We propose a novel pairwise distance measure between variable sized sets of
image keypoints for the purpose of large-scale medical image indexing. Our
measure generalizes the Jaccard distance to account for soft set equivalence
(SSE) between set elements, via an adaptive kernel framework accounting for
uncertainty in keypoint appearance and geometry. Novel kernels are proposed to
quantify variability of keypoint geometry in location and scale. Our distance
measure may be estimated between $N^2$ image pairs in $O(N~log~N)$ operations
via keypoint indexing. Experiments validate our method in predicting 509,545
pairwise relationships from T1-weighted MRI brain volumes of monozygotic and
dizygotic twins, siblings and half-siblings sharing 100%-25% of their
polymorphic genes. Soft set equivalence and keypoint geometry kernels
outperform standard hard set equivalence (HSE) in predicting family
relationships. High accuracy is achieved, with monozygotic twin identification
near 100% and several cases of unknown family labels, due to errors in the
genotyping process, are correctly paired with family members. Software is
provided for efficient fine-grained curation of large, generic image datasets.
</p>
<a href="http://arxiv.org/abs/2103.06966" target="_blank">arXiv:2103.06966</a> [<a href="http://arxiv.org/pdf/2103.06966" target="_blank">pdf</a>]

<h2>Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks. (arXiv:2103.06982v1 [cs.CV])</h2>
<h3>Ben Saunders, Necati Cihan Camgoz, Richard Bowden</h3>
<p>Sign languages are multi-channel visual languages, where signers use a
continuous 3D space to communicate.Sign Language Production (SLP), the
automatic translation from spoken to sign languages, must embody both the
continuous articulation and full morphology of sign to be truly understandable
by the Deaf community. Previous deep learning-based SLP works have produced
only a concatenation of isolated signs focusing primarily on the manual
features, leading to a robotic and non-expressive production.

In this work, we propose a novel Progressive Transformer architecture, the
first SLP model to translate from spoken language sentences to continuous 3D
multi-channel sign pose sequences in an end-to-end manner. Our transformer
network architecture introduces a counter decoding that enables variable length
continuous sequence generation by tracking the production progress over time
and predicting the end of sequence. We present extensive data augmentation
techniques to reduce prediction drift, alongside an adversarial training regime
and a Mixture Density Network (MDN) formulation to produce realistic and
expressive sign pose sequences.

We propose a back translation evaluation mechanism for SLP, presenting
benchmark quantitative results on the challenging PHOENIX14T dataset and
setting baselines for future research. We further provide a user evaluation of
our SLP model, to understand the Deaf reception of our sign pose productions.
</p>
<a href="http://arxiv.org/abs/2103.06982" target="_blank">arXiv:2103.06982</a> [<a href="http://arxiv.org/pdf/2103.06982" target="_blank">pdf</a>]

<h2>Robofleet: Secure Open Source Communication and Management for Fleets of Autonomous Robots. (arXiv:2103.06993v1 [cs.RO])</h2>
<h3>Kavan Singh Sikand, Logan Zartman, Sadegh Rabiee, Joydeep Biswas</h3>
<p>Safe long-term deployment of a fleet of mobile robots requires reliable and
secure two-way communication channels between individual robots and remote
human operators for supervision and tasking. Existing open-source solutions to
this problem degrade in performance in challenging real-world situations such
as intermittent and low-bandwidth connectivity, do not provide security control
options, and can be computationally expensive on hardware-constrained mobile
robot platforms. In this paper, we present Robofleet, a lightweight open-source
system which provides inter-robot communication, remote monitoring, and remote
tasking for a fleet of ROS-enabled service-mobile robots that is designed with
the practical goals of resilience to network variance and security control in
mind.

Robofleet supports multi-user, multi-robot communication via a central
server. This architecture deduplicates network traffic between robots,
significantly reducing overall network load when compared with native ROS
communication. This server also functions as a single entrypoint into the
system, enabling security control and user authentication. Individual robots
run the lightweight Robofleet client, which is responsible for exchanging
messages with the Robofleet server. It automatically adapts to adverse network
conditions through backpressure monitoring as well as topic-level priority
control, ensuring that safety-critical messages are successfully transmitted.
Finally, the system includes a web-based visualization tool that can be run on
any internet-connected, browser-enabled device to monitor and control the
fleet.

We compare Robofleet to existing methods of robotic communication, and
demonstrate that it provides superior resilience to network variance while
maintaining performance that exceeds that of widely-used systems.
</p>
<a href="http://arxiv.org/abs/2103.06993" target="_blank">arXiv:2103.06993</a> [<a href="http://arxiv.org/pdf/2103.06993" target="_blank">pdf</a>]

<h2>The Location of Optimal Object Colors with More Than Two Transitions. (arXiv:2103.06997v1 [cs.CV])</h2>
<h3>Scott A. Burns</h3>
<p>The chromaticity diagram associated with the CIE 1931 color matching
functions is shown to be slightly non-convex. While having no impact on
practical colorimetric computations, the non-convexity does have a significant
impact on the shape of some optimal object color reflectance distributions
associated with the outer surface of the object color solid. Instead of the
usual two-transition Schr\"odinger form, many optimal colors exhibit higher
transition counts. A linear programming formulation is developed and is used to
locate where these higher-transition optimal object colors reside on the object
color solid surface.
</p>
<a href="http://arxiv.org/abs/2103.06997" target="_blank">arXiv:2103.06997</a> [<a href="http://arxiv.org/pdf/2103.06997" target="_blank">pdf</a>]

<h2>An Efficient Hypergraph Approach to Robust Point Cloud Resampling. (arXiv:2103.06999v1 [cs.CV])</h2>
<h3>Qinwen Deng, Songyang Zhang, Zhi Ding</h3>
<p>Efficient processing and feature extraction of largescale point clouds are
important in related computer vision and cyber-physical systems. This work
investigates point cloud resampling based on hypergraph signal processing
(HGSP) to better explore the underlying relationship among different cloud
points and to extract contour-enhanced features. Specifically, we design
hypergraph spectral filters to capture multi-lateral interactions among the
signal nodes of point clouds and to better preserve their surface outlines.
Without the need and the computation to first construct the underlying
hypergraph, our low complexity approach directly estimates hypergraph spectrum
of point clouds by leveraging hypergraph stationary processes from the observed
3D coordinates. Evaluating the proposed resampling methods with several
metrics, our test results validate the high efficacy of hypergraph
characterization of point clouds and demonstrate the robustness of
hypergraph-based resampling under noisy observations.
</p>
<a href="http://arxiv.org/abs/2103.06999" target="_blank">arXiv:2103.06999</a> [<a href="http://arxiv.org/pdf/2103.06999" target="_blank">pdf</a>]

<h2>CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement. (arXiv:2103.07017v1 [cs.CV])</h2>
<h3>Noranart Vesdapunt, Baoyuan Wang</h3>
<p>Face detection is a fundamental problem for many downstream face
applications, and there is a rising demand for faster, more accurate yet
support for higher resolution face detectors. Recent smartphones can record a
video in 8K resolution, but many of the existing face detectors still fail due
to the anchor size and training data. We analyze the failure cases and observe
a large number of correct predicted boxes with incorrect confidences. To
calibrate these confidences, we propose a confidence ranking network with a
pairwise ranking loss to re-rank the predicted confidences locally within the
same image. Our confidence ranker is model-agnostic, so we can augment the data
by choosing the pairs from multiple face detectors during the training, and
generalize to a wide range of face detectors during the testing. On WiderFace,
we achieve the highest AP on the single-scale, and our AP is competitive with
the previous multi-scale methods while being significantly faster. On 8K
resolution, our method solves the GPU memory issue and allows us to indirectly
train on 8K. We collect 8K resolution test set to show the improvement, and we
will release our test set as a new benchmark for future research.
</p>
<a href="http://arxiv.org/abs/2103.07017" target="_blank">arXiv:2103.07017</a> [<a href="http://arxiv.org/pdf/2103.07017" target="_blank">pdf</a>]

<h2>Sensor selection for detecting deviations from a planned itinerary. (arXiv:2103.07029v1 [cs.RO])</h2>
<h3>Hazhar Rahmani, Dylan A. Shell, Jason M. O&#x27;Kane</h3>
<p>Suppose an agent asserts that it will move through an environment in some
way. When the agent executes its motion, how does one verify the claim? The
problem arises in a range of contexts including in validating safety claims
about robot behavior, applications in security and surveillance, and for both
the conception and the (physical) design and logistics of scientific
experiments. Given a set of feasible sensors to select from, we ask how to
choose sensors optimally in order to ensure that the agent's execution does
indeed fit its pre-disclosed itinerary. Our treatment is distinguished from
prior work in sensor selection by two aspects: the form the itinerary takes (a
regular language of transitions) and that families of sensor choices can be
grouped as a single choice. Both are intimately tied together, permitting
construction of a product automaton because the same physical sensors (i.e.,
the same choice) can appear multiple times. This paper establishes the hardness
of sensor selection for itinerary validation within this treatment, and
proposes an exact algorithm based on an ILP formulation that is capable of
solving problem instances of moderate size. We demonstrate its efficacy on
small-scale case studies, including one motivated by wildlife tracking.
</p>
<a href="http://arxiv.org/abs/2103.07029" target="_blank">arXiv:2103.07029</a> [<a href="http://arxiv.org/pdf/2103.07029" target="_blank">pdf</a>]

<h2>Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop Removal. (arXiv:2103.07051v1 [cs.CV])</h2>
<h3>Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren, Lin Ma, Hongdong Li</h3>
<p>Rain streaks and rain drops are two natural phenomena, which degrade image
capture in different ways. Currently, most existing deep deraining networks
take them as two distinct problems and individually address one, and thus
cannot deal adequately with both simultaneously. To address this, we propose a
Dual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing
both rain streaks and raindrops. Inside the DAM, there are two attentive maps -
each of which attends to the heavy and light rainy regions, respectively, to
guide the deraining process differently for applicable regions. In addition, to
further refine the result, a Differential-driven Dual Attention-in-Attention
Model (D-DAiAM) is proposed with a "heavy-to-light" scheme to remove rain via
addressing the unsatisfying deraining regions. Extensive experiments on one
public raindrop dataset, one public rain streak and our synthesized joint rain
streak and raindrop (JRSRD) dataset have demonstrated that the proposed method
not only is capable of removing rain streaks and raindrops simultaneously, but
also achieves the state-of-the-art performance on both tasks.
</p>
<a href="http://arxiv.org/abs/2103.07051" target="_blank">arXiv:2103.07051</a> [<a href="http://arxiv.org/pdf/2103.07051" target="_blank">pdf</a>]

<h2>FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism. (arXiv:2103.07054v1 [cs.CV])</h2>
<h3>Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Linlin Shen, Ales Leonardis</h3>
<p>In this paper, we focus on category-level 6D pose and size estimation from
monocular RGB-D image. Previous methods suffer from inefficient category-level
pose feature extraction which leads to low accuracy and inference speed. To
tackle this problem, we propose a fast shape-based network (FS-Net) with
efficient category-level feature extraction for 6D pose estimation. First, we
design an orientation aware autoencoder with 3D graph convolution for latent
feature extraction. The learned latent feature is insensitive to point shift
and object size thanks to the shift and scale-invariance properties of the 3D
graph convolution. Then, to efficiently decode category-level rotation
information from the latent feature, we propose a novel decoupled rotation
mechanism that employs two decoders to complementarily access the rotation
information. Meanwhile, we estimate translation and size by two residuals,
which are the difference between the mean of object points and ground truth
translation, and the difference between the mean size of the category and
ground truth size, respectively. Finally, to increase the generalization
ability of FS-Net, we propose an online box-cage based 3D deformation mechanism
to augment the training data. Extensive experiments on two benchmark datasets
show that the proposed method achieves state-of-the-art performance in both
category- and instance-level 6D object pose estimation. Especially in
category-level pose estimation, without extra synthetic data, our method
outperforms existing methods by 6.3% on the NOCS-REAL dataset.
</p>
<a href="http://arxiv.org/abs/2103.07054" target="_blank">arXiv:2103.07054</a> [<a href="http://arxiv.org/pdf/2103.07054" target="_blank">pdf</a>]

<h2>Advanced Multiple Linear Regression Based Dark Channel Prior Applied on Dehazing Image and Generating Synthetic Haze. (arXiv:2103.07065v1 [cs.CV])</h2>
<h3>Binghan Li, Yindong Hua, Mi Lu</h3>
<p>Haze removal is an extremely challenging task, and object detection in the
hazy environment has recently gained much attention due to the popularity of
autonomous driving and traffic surveillance. In this work, the authors propose
a multiple linear regression haze removal model based on a widely adopted
dehazing algorithm named Dark Channel Prior. Training this model with a
synthetic hazy dataset, the proposed model can reduce the unanticipated
deviations generated from the rough estimations of transmission map and
atmospheric light in Dark Channel Prior. To increase object detection accuracy
in the hazy environment, the authors further present an algorithm to build a
synthetic hazy COCO training dataset by generating the artificial haze to the
MS COCO training dataset. The experimental results demonstrate that the
proposed model obtains higher image quality and shares more similarity with
ground truth images than most conventional pixel-based dehazing algorithms and
neural network based haze-removal models. The authors also evaluate the mean
average precision of Mask R-CNN when training the network with synthetic hazy
COCO training dataset and preprocessing test hazy dataset by removing the haze
with the proposed dehazing model. It turns out that both approaches can
increase the object detection accuracy significantly and outperform most
existing object detection models over hazy images.
</p>
<a href="http://arxiv.org/abs/2103.07065" target="_blank">arXiv:2103.07065</a> [<a href="http://arxiv.org/pdf/2103.07065" target="_blank">pdf</a>]

<h2>Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion. (arXiv:2103.07074v1 [cs.CV])</h2>
<h3>Shi Qiu, Saeed Anwar, Nick Barnes</h3>
<p>Given the prominence of current 3D sensors, a fine-grained analysis on the
basic point cloud data is worthy of further investigation. Particularly, real
point cloud scenes can intuitively capture complex surroundings in the real
world, but due to 3D data's raw nature, it is very challenging for machine
perception. In this work, we concentrate on the essential visual task, semantic
segmentation, for large-scale point cloud data collected in reality. On the one
hand, to reduce the ambiguity in nearby points, we augment their local context
by fully utilizing both geometric and semantic features in a bilateral
structure. On the other hand, we comprehensively interpret the distinctness of
the points from multiple resolutions and represent the feature map following an
adaptive fusion method at point-level for accurate semantic segmentation.
Further, we provide specific ablation studies and intuitive visualizations to
validate our key modules. By comparing with state-of-the-art networks on three
different benchmarks, we demonstrate the effectiveness of our network.
</p>
<a href="http://arxiv.org/abs/2103.07074" target="_blank">arXiv:2103.07074</a> [<a href="http://arxiv.org/pdf/2103.07074" target="_blank">pdf</a>]

<h2>iToF2dToF: A Robust and Flexible Representation for Data-Driven Time-of-Flight Imaging. (arXiv:2103.07087v1 [cs.CV])</h2>
<h3>Felipe Gutierrez-Barragan, Huaijin Chen, Mohit Gupta, Andreas Velten, Jinwei Gu</h3>
<p>Indirect Time-of-Flight (iToF) cameras are a promising depth sensing
technology. However, they are prone to errors caused by multi-path interference
(MPI) and low signal-to-noise ratio (SNR). Traditional methods, after
denoising, mitigate MPI by estimating a transient image that encodes depths.
Recently, data-driven methods that jointly denoise and mitigate MPI have become
state-of-the-art without using the intermediate transient representation. In
this paper, we propose to revisit the transient representation. Using
data-driven priors, we interpolate/extrapolate iToF frequencies and use them to
estimate the transient image. Given direct ToF (dToF) sensors capture transient
images, we name our method iToF2dToF. The transient representation is flexible.
It can be integrated with different rule-based depth sensing algorithms that
are robust to low SNR and can deal with ambiguous scenarios that arise in
practice (e.g., specular MPI, optical cross-talk). We demonstrate the benefits
of iToF2dToF over previous methods in real depth sensing scenarios.
</p>
<a href="http://arxiv.org/abs/2103.07087" target="_blank">arXiv:2103.07087</a> [<a href="http://arxiv.org/pdf/2103.07087" target="_blank">pdf</a>]

<h2>PVStereo: Pyramid Voting Module for End-to-End Self-Supervised Stereo Matching. (arXiv:2103.07094v1 [cs.CV])</h2>
<h3>Hengli Wang, Rui Fan, Peide Cai, Ming Liu</h3>
<p>Supervised learning with deep convolutional neural networks (DCNNs) has seen
huge adoption in stereo matching. However, the acquisition of large-scale
datasets with well-labeled ground truth is cumbersome and labor-intensive,
making supervised learning-based approaches often hard to implement in
practice. To overcome this drawback, we propose a robust and effective
self-supervised stereo matching approach, consisting of a pyramid voting module
(PVM) and a novel DCNN architecture, referred to as OptStereo. Specifically,
our OptStereo first builds multi-scale cost volumes, and then adopts a
recurrent unit to iteratively update disparity estimations at high resolution;
while our PVM can generate reliable semi-dense disparity images, which can be
employed to supervise OptStereo training. Furthermore, we publish the
HKUST-Drive dataset, a large-scale synthetic stereo dataset, collected under
different illumination and weather conditions for research purposes. Extensive
experimental results demonstrate the effectiveness and efficiency of our
self-supervised stereo matching approach on the KITTI Stereo benchmarks and our
HKUST-Drive dataset. PVStereo, our best-performing implementation, greatly
outperforms all other state-of-the-art self-supervised stereo matching
approaches. Our project page is available at sites.google.com/view/pvstereo.
</p>
<a href="http://arxiv.org/abs/2103.07094" target="_blank">arXiv:2103.07094</a> [<a href="http://arxiv.org/pdf/2103.07094" target="_blank">pdf</a>]

<h2>Goal-Driven Autonomous Mapping Through Deep Reinforcement Learning and Planning-Based Navigation. (arXiv:2103.07119v1 [cs.RO])</h2>
<h3>Reinis Cimurs, Il Hong Suh, Jin Han Lee</h3>
<p>In this paper, we present a goal-driven autonomous mapping and exploration
system that combines reactive and planned robot navigation. First, a navigation
policy is learned through a deep reinforcement learning (DRL) framework in a
simulated environment. This policy guides an autonomous agent towards a goal
while avoiding obstacles. We develop a navigation system where this learned
policy is integrated into a motion planning stack as the local navigation layer
to move the robot towards the intermediate goals. A global path planner is used
to mitigate the local optimum problem and guide the robot towards the global
goal. Possible intermediate goal locations are extracted from the environment
and used as local goals according to the navigation system heuristics. The
fully autonomous navigation is performed without any prior knowledge while
mapping is performed as the robot moves through the environment. Experiments
show the capability of the system navigating in previously unknown surroundings
and arriving at the designated goal.
</p>
<a href="http://arxiv.org/abs/2103.07119" target="_blank">arXiv:2103.07119</a> [<a href="http://arxiv.org/pdf/2103.07119" target="_blank">pdf</a>]

<h2>Thousand to One: Semantic Prior Modeling for Conceptual Coding. (arXiv:2103.07131v1 [cs.CV])</h2>
<h3>Jianhui Chang, Zhenghui Zhao, Lingbo Yang, Chuanmin Jia, Jian Zhang, Siwei Ma</h3>
<p>Conceptual coding has been an emerging research topic recently, which encodes
natural images into disentangled conceptual representations for compression.
However, the compression performance of the existing methods is still
sub-optimal due to the lack of comprehensive consideration of rate constraint
and reconstruction quality. To this end, we propose a novel end-to-end semantic
prior modeling-based conceptual coding scheme towards extremely low bitrate
image compression, which leverages semantic-wise deep representations as a
unified prior for entropy estimation and texture synthesis. Specifically, we
employ semantic segmentation maps as structural guidance for extracting deep
semantic prior, which provides fine-grained texture distribution modeling for
better detail construction and higher flexibility in subsequent high-level
vision tasks. Moreover, a cross-channel entropy model is proposed to further
exploit the inter-channel correlation of the spatially independent semantic
prior, leading to more accurate entropy estimation for rate-constrained
training. The proposed scheme achieves an ultra-high 1000x compression ratio,
while still enjoying high visual reconstruction quality and versatility towards
visual processing and analysis tasks.
</p>
<a href="http://arxiv.org/abs/2103.07131" target="_blank">arXiv:2103.07131</a> [<a href="http://arxiv.org/pdf/2103.07131" target="_blank">pdf</a>]

<h2>UIEC^2-Net: CNN-based Underwater Image Enhancement Using Two Color Space. (arXiv:2103.07138v1 [cs.CV])</h2>
<h3>Yudong Wang, Jichang Guo, Huan Gao, Huihui Yue</h3>
<p>Underwater image enhancement has attracted much attention due to the rise of
marine resource development in recent years. Benefit from the powerful
representation capabilities of Convolution Neural Networks(CNNs), multiple
underwater image enhancement algorithms based on CNNs have been proposed in the
last few years. However, almost all of these algorithms employ RGB color space
setting, which is insensitive to image properties such as luminance and
saturation. To address this problem, we proposed Underwater Image Enhancement
Convolution Neural Network using 2 Color Space (UICE^2-Net) that efficiently
and effectively integrate both RGB Color Space and HSV Color Space in one
single CNN. To our best knowledge, this method is the first to use HSV color
space for underwater image enhancement based on deep learning. UIEC^2-Net is an
end-to-end trainable network, consisting of three blocks as follow: a RGB
pixel-level block implements fundamental operations such as denoising and
removing color cast, a HSV global-adjust block for globally adjusting
underwater image luminance, color and saturation by adopting a novel neural
curve layer, and an attention map block for combining the advantages of RGB and
HSV block output images by distributing weight to each pixel. Experimental
results on synthetic and real-world underwater images show the good performance
of our proposed method in both subjective comparisons and objective metrics.
</p>
<a href="http://arxiv.org/abs/2103.07138" target="_blank">arXiv:2103.07138</a> [<a href="http://arxiv.org/pdf/2103.07138" target="_blank">pdf</a>]

<h2>Neural Reprojection Error: Merging Feature Learning and Camera Pose Estimation. (arXiv:2103.07153v1 [cs.CV])</h2>
<h3>Hugo Germain, Vincent Lepetit, Guillaume Bourmaud</h3>
<p>Absolute camera pose estimation is usually addressed by sequentially solving
two distinct subproblems: First a feature matching problem that seeks to
establish putative 2D-3D correspondences, and then a Perspective-n-Point
problem that minimizes, with respect to the camera pose, the sum of so-called
Reprojection Errors (RE). We argue that generating putative 2D-3D
correspondences 1) leads to an important loss of information that needs to be
compensated as far as possible, within RE, through the choice of a robust loss
and the tuning of its hyperparameters and 2) may lead to an RE that conveys
erroneous data to the pose estimator. In this paper, we introduce the Neural
Reprojection Error (NRE) as a substitute for RE. NRE allows to rethink the
camera pose estimation problem by merging it with the feature learning problem,
hence leveraging richer information than 2D-3D correspondences and eliminating
the need for choosing a robust loss and its hyperparameters. Thus NRE can be
used as training loss to learn image descriptors tailored for pose estimation.
We also propose a coarse-to-fine optimization method able to very efficiently
minimize a sum of NRE terms with respect to the camera pose. We experimentally
demonstrate that NRE is a good substitute for RE as it significantly improves
both the robustness and the accuracy of the camera pose estimate while being
computationally and memory highly efficient. From a broader point of view, we
believe this new way of merging deep learning and 3D geometry may be useful in
other computer vision applications.
</p>
<a href="http://arxiv.org/abs/2103.07153" target="_blank">arXiv:2103.07153</a> [<a href="http://arxiv.org/pdf/2103.07153" target="_blank">pdf</a>]

<h2>Learnable Companding Quantization for Accurate Low-bit Neural Networks. (arXiv:2103.07156v1 [cs.CV])</h2>
<h3>Kohei Yamamoto</h3>
<p>Quantizing deep neural networks is an effective method for reducing memory
consumption and improving inference speed, and is thus useful for
implementation in resource-constrained devices. However, it is still hard for
extremely low-bit models to achieve accuracy comparable with that of
full-precision models. To address this issue, we propose learnable companding
quantization (LCQ) as a novel non-uniform quantization method for 2-, 3-, and
4-bit models. LCQ jointly optimizes model weights and learnable companding
functions that can flexibly and non-uniformly control the quantization levels
of weights and activations. We also present a new weight normalization
technique that allows more stable training for quantization. Experimental
results show that LCQ outperforms conventional state-of-the-art methods and
narrows the gap between quantized and full-precision models for image
classification and object detection tasks. Notably, the 2-bit ResNet-50 model
on ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%,
allowing LCQ to further exploit the potential of non-uniform quantization.
</p>
<a href="http://arxiv.org/abs/2103.07156" target="_blank">arXiv:2103.07156</a> [<a href="http://arxiv.org/pdf/2103.07156" target="_blank">pdf</a>]

<h2>Facial emotion expressions in human-robot interaction: A survey. (arXiv:2103.07169v1 [cs.RO])</h2>
<h3>Niyati Rawal, Ruth Maria Stock-Homburg</h3>
<p>Facial expressions are an ideal means of communicating one's emotions or
intentions to others. This overview will focus on human facial expression
recognition as well as robotic facial expression generation. In case of human
facial expression recognition, both facial expression recognition on predefined
datasets as well as in real time will be covered. For robotic facial expression
generation, hand coded and automated methods i.e., facial expressions of a
robot are generated by moving the features (eyes, mouth) of the robot by hand
coding or automatically using machine learning techniques, will also be
covered. There are already plenty of studies that achieve high accuracy for
emotion expression recognition on predefined datasets, but the accuracy for
facial expression recognition in real time is comparatively lower. In case of
expression generation in robots, while most of the robots are capable of making
basic facial expressions, there are not many studies that enable robots to do
so automatically.
</p>
<a href="http://arxiv.org/abs/2103.07169" target="_blank">arXiv:2103.07169</a> [<a href="http://arxiv.org/pdf/2103.07169" target="_blank">pdf</a>]

<h2>Agile Actions with a Centaur-Type Humanoid: A Decoupled Approach. (arXiv:2103.07183v1 [cs.RO])</h2>
<h3>Matteo Parigi Polverini, Enrico Mingo Hoffman, Arturo Laurenzi, Nikos G. Tsagarakis</h3>
<p>The kinematic features of a centaur-type humanoid platform, combined with a
powerful actuation, enable the experimentation of a variety of agile and
dynamic motions. However, the higher number of degrees-of-freedom and the
increased weight of the system, compared to the bipedal and quadrupedal
counterparts, pose significant research challenges in terms of computational
load and real implementation. To this end, this work presents a control
architecture to perform agile actions, conceived for torque-controlled
platforms, which decouples for computational purposes offline optimal control
planning of lower-body primitives, based on a template kinematic model, and
online control of the upper-body motion to maintain balance. Three stabilizing
strategies are presented, whose performance is compared in two types of
simulated jumps, while experimental validation is performed on a half-squat
jump using the CENTAURO robot.
</p>
<a href="http://arxiv.org/abs/2103.07183" target="_blank">arXiv:2103.07183</a> [<a href="http://arxiv.org/pdf/2103.07183" target="_blank">pdf</a>]

<h2>Urban Surface Reconstruction in SAR Tomography by Graph-Cuts. (arXiv:2103.07202v1 [cs.CV])</h2>
<h3>Cl&#xe9;ment Rambour, Lo&#xef;c Denis, Florence Tupin, H&#xe9;l&#xe8;ne Oriot, Yue Huang, Laurent Ferro-Famil</h3>
<p>SAR (Synthetic Aperture Radar) tomography reconstructs 3-D volumes from
stacks of SAR images. High-resolution satellites such as TerraSAR-X provide
images that can be combined to produce 3-D models. In urban areas, sparsity
priors are generally enforced during the tomographic inversion process in order
to retrieve the location of scatterers seen within a given radar resolution
cell. However, such priors often miss parts of the urban surfaces. Those
missing parts are typically regions of flat areas such as ground or rooftops.
This paper introduces a surface segmentation algorithm based on the computation
of the optimal cut in a flow network. This segmentation process can be included
within the 3-D reconstruction framework in order to improve the recovery of
urban surfaces. Illustrations on a TerraSAR-X tomographic dataset demonstrate
the potential of the approach to produce a 3-D model of urban surfaces such as
ground, fa\c{c}ades and rooftops.
</p>
<a href="http://arxiv.org/abs/2103.07202" target="_blank">arXiv:2103.07202</a> [<a href="http://arxiv.org/pdf/2103.07202" target="_blank">pdf</a>]

<h2>In the light of feature distributions: moment matching for Neural Style Transfer. (arXiv:2103.07208v1 [cs.CV])</h2>
<h3>Nikolai Kalischek, Jan Dirk Wegner, Konrad Schindler</h3>
<p>Style transfer aims to render the content of a given image in the
graphical/artistic style of another image. The fundamental concept underlying
NeuralStyle Transfer (NST) is to interpret style as a distribution in the
feature space of a Convolutional Neural Network, such that a desired style can
be achieved by matching its feature distribution. We show that most current
implementations of that concept have important theoretical and practical
limitations, as they only partially align the feature distributions. We propose
a novel approach that matches the distributions more precisely, thus
reproducing the desired style more faithfully, while still being
computationally efficient. Specifically, we adapt the dual form of Central
Moment Discrepancy (CMD), as recently proposed for domain adaptation, to
minimize the difference between the target style and the feature distribution
of the output image. The dual interpretation of this metric explicitly matches
all higher-order centralized moments and is therefore a natural extension of
existing NST methods that only take into account the first and second moments.
Our experiments confirm that the strong theoretical properties also translate
to visually better style transfer, and better disentangle style from semantic
image content.
</p>
<a href="http://arxiv.org/abs/2103.07208" target="_blank">arXiv:2103.07208</a> [<a href="http://arxiv.org/pdf/2103.07208" target="_blank">pdf</a>]

<h2>Sequential Random Network for Fine-grained Image Classification. (arXiv:2103.07230v1 [cs.CV])</h2>
<h3>Chaorong Li, Malu Zhang, Wei Huang, Fengqing Qin, Anping Zeng, Yuanyuan Huang</h3>
<p>Deep Convolutional Neural Network (DCNN) and Transformer have achieved
remarkable successes in image recognition. However, their performance in
fine-grained image recognition is still difficult to meet the requirements of
actual needs. This paper proposes a Sequence Random Network (SRN) to enhance
the performance of DCNN. The output of DCNN is one-dimensional features. This
one-dimensional feature abstractly represents image information, but it does
not express well the detailed information of image. To address this issue, we
use the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks
(called BiLSTM-TDN), to further process DCNN one-dimensional features for
highlighting the detail information of image. After the feature transform by
BiLSTM-TDN, the recognition performance has been greatly improved. We conducted
the experiments on six fine-grained image datasets. Except for FGVC-Aircraft,
the accuracies of the proposed methods on the other datasets exceeded 99%.
Experimental results show that BiLSTM-TDN is far superior to the existing
state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended
to other models, such as Transformer.
</p>
<a href="http://arxiv.org/abs/2103.07230" target="_blank">arXiv:2103.07230</a> [<a href="http://arxiv.org/pdf/2103.07230" target="_blank">pdf</a>]

<h2>Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation. (arXiv:2103.07246v1 [cs.CV])</h2>
<h3>Beomyoung Kim, Sangeun Han. Junmo Kim</h3>
<p>Weakly-supervised semantic segmentation (WSSS) using image-level labels has
recently attracted much attention for reducing annotation costs. Existing WSSS
methods utilize localization maps from the classification network to generate
pseudo segmentation labels. However, since localization maps obtained from the
classifier focus only on sparse discriminative object regions, it is difficult
to generate high-quality segmentation labels. To address this issue, we
introduce discriminative region suppression (DRS) module that is a simple yet
effective method to expand object activation regions. DRS suppresses the
attention on discriminative regions and spreads it to adjacent
non-discriminative regions, generating dense localization maps. DRS requires
few or no additional parameters and can be plugged into any network.
Furthermore, we introduce an additional learning strategy to give a
self-enhancement of localization maps, named localization map refinement
learning. Benefiting from this refinement learning, localization maps are
refined and enhanced by recovering some missing parts or removing noise itself.
Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on
the PASCAL VOC 2012 segmentation benchmark using only image-level labels.
Extensive experiments demonstrate the effectiveness of our approach. The code
is available at https://github.com/qjadud1994/DRS.
</p>
<a href="http://arxiv.org/abs/2103.07246" target="_blank">arXiv:2103.07246</a> [<a href="http://arxiv.org/pdf/2103.07246" target="_blank">pdf</a>]

<h2>Deep Dual Consecutive Network for Human Pose Estimation. (arXiv:2103.07254v1 [cs.CV])</h2>
<h3>Zhenguang Liu, Haoming Chen, Runyang Feng, Shuang Wu, Shouling Ji, Bailin Yang, Xun Wang</h3>
<p>Multi-frame human pose estimation in complicated situations is challenging.
Although state-of-the-art human joints detectors have demonstrated remarkable
results for static images, their performances come short when we apply these
models to video sequences. Prevalent shortcomings include the failure to handle
motion blur, video defocus, or pose occlusions, arising from the inability in
capturing the temporal dependency among video frames. On the other hand,
directly employing conventional recurrent neural networks incurs empirical
difficulties in modeling spatial contexts, especially for dealing with pose
occlusions. In this paper, we propose a novel multi-frame human pose estimation
framework, leveraging abundant temporal cues between video frames to facilitate
keypoint detection. Three modular components are designed in our framework. A
Pose Temporal Merger encodes keypoint spatiotemporal context to generate
effective searching scopes while a Pose Residual Fusion module computes
weighted pose residuals in dual directions. These are then processed via our
Pose Correction Network for efficient refining of pose estimations. Our method
ranks No.1 in the Multi-frame Person Pose Estimation Challenge on the
large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have
released our code, hoping to inspire future research.
</p>
<a href="http://arxiv.org/abs/2103.07254" target="_blank">arXiv:2103.07254</a> [<a href="http://arxiv.org/pdf/2103.07254" target="_blank">pdf</a>]

<h2>Learning Long-Term Style-Preserving Blind Video Temporal Consistency. (arXiv:2103.07278v1 [cs.CV])</h2>
<h3>Hugo Thimonier, Julien Despois, Robin Kips, Matthieu Perrot</h3>
<p>When trying to independently apply image-trained algorithms to successive
frames in videos, noxious flickering tends to appear. State-of-the-art
post-processing techniques that aim at fostering temporal consistency, generate
other temporal artifacts and visually alter the style of videos. We propose a
postprocessing model, agnostic to the transformation applied to videos (e.g.
style transfer, image manipulation using GANs, etc.), in the form of a
recurrent neural network. Our model is trained using a Ping Pong procedure and
its corresponding loss, recently introduced for GAN video generation, as well
as a novel style preserving perceptual loss. The former improves long-term
temporal consistency learning, while the latter fosters style preservation. We
evaluate our model on the DAVIS and videvo.net datasets and show that our
approach offers state-of-the-art results concerning flicker removal, and better
keeps the overall style of the videos than previous approaches.
</p>
<a href="http://arxiv.org/abs/2103.07278" target="_blank">arXiv:2103.07278</a> [<a href="http://arxiv.org/pdf/2103.07278" target="_blank">pdf</a>]

<h2>Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator. (arXiv:2103.07289v1 [cs.CV])</h2>
<h3>Sian-Yao Huang, Wei-Ta Chu</h3>
<p>In one-shot NAS, sub-networks need to be searched from the supernet to meet
different hardware constraints. However, the search cost is high and $N$ times
of searches are needed for $N$ different constraints. In this work, we propose
a novel search strategy called architecture generator to search sub-networks by
generating them, so that the search process can be much more efficient and
flexible. With the trained architecture generator, given target hardware
constraints as the input, $N$ good architectures can be generated for $N$
constraints by just one forward pass without re-searching and supernet
retraining. Moreover, we propose a novel single-path supernet, called unified
supernet, to further improve search efficiency and reduce GPU memory
consumption of the architecture generator. With the architecture generator and
the unified supernet, we propose a flexible and efficient one-shot NAS
framework, called Searching by Generating NAS (SGNAS). With the pre-trained
supernt, the search time of SGNAS for $N$ different hardware constraints is
only 5 GPU hours, which is $4N$ times faster than previous SOTA single-path
methods. After training from scratch, the top1-accuracy of SGNAS on ImageNet is
77.1%, which is comparable with the SOTAs. The code is available at:
https://github.com/eric8607242/SGNAS.
</p>
<a href="http://arxiv.org/abs/2103.07289" target="_blank">arXiv:2103.07289</a> [<a href="http://arxiv.org/pdf/2103.07289" target="_blank">pdf</a>]

<h2>VDSM: Unsupervised Video Disentanglement with State-Space Modeling and Deep Mixtures of Experts. (arXiv:2103.07292v1 [cs.CV])</h2>
<h3>Matthew J. Vowels, Necati Cihan Camgoz, Richard Bowden</h3>
<p>Disentangled representations support a range of downstream tasks including
causal reasoning, generative modeling, and fair machine learning.
Unfortunately, disentanglement has been shown to be impossible without the
incorporation of supervision or inductive bias. Given that supervision is often
expensive or infeasible to acquire, we choose to incorporate structural
inductive bias and present an unsupervised, deep State-Space-Model for Video
Disentanglement (VDSM). The model disentangles latent time-varying and dynamic
factors via the incorporation of hierarchical structure with a dynamic prior
and a Mixture of Experts decoder. VDSM learns separate disentangled
representations for the identity of the object or person in the video, and for
the action being performed. We evaluate VDSM across a range of qualitative and
quantitative tasks including identity and dynamics transfer, sequence
generation, Fr\'echet Inception Distance, and factor classification. VDSM
provides state-of-the-art performance and exceeds adversarial methods, even
when the methods use additional supervision.
</p>
<a href="http://arxiv.org/abs/2103.07292" target="_blank">arXiv:2103.07292</a> [<a href="http://arxiv.org/pdf/2103.07292" target="_blank">pdf</a>]

<h2>Seeking the Shape of Sound: An Adaptive Framework for Learning Voice-Face Association. (arXiv:2103.07293v1 [cs.CV])</h2>
<h3>Peisong Wen, Qianqian Xu, Yangbangyan Jiang, Zhiyong Yang, Yuan He, Qingming Huang</h3>
<p>Nowadays, we have witnessed the early progress on learning the association
between voice and face automatically, which brings a new wave of studies to the
computer vision community. However, most of the prior arts along this line (a)
merely adopt local information to perform modality alignment and (b) ignore the
diversity of learning difficulty across different subjects. In this paper, we
propose a novel framework to jointly address the above-mentioned issues.
Targeting at (a), we propose a two-level modality alignment loss where both
global and local information are considered. Compared with the existing
methods, we introduce a global loss into the modality alignment process. The
global component of the loss is driven by the identity classification.
Theoretically, we show that minimizing the loss could maximize the distance
between embeddings across different identities while minimizing the distance
between embeddings belonging to the same identity, in a global sense (instead
of a mini-batch). Targeting at (b), we propose a dynamic reweighting scheme to
better explore the hard but valuable identities while filtering out the
unlearnable identities. Experiments show that the proposed method outperforms
the previous methods in multiple settings, including voice-face matching,
verification and retrieval.
</p>
<a href="http://arxiv.org/abs/2103.07293" target="_blank">arXiv:2103.07293</a> [<a href="http://arxiv.org/pdf/2103.07293" target="_blank">pdf</a>]

<h2>Augmented Environment Representations with Complete Object Models. (arXiv:2103.07298v1 [cs.RO])</h2>
<h3>Krishnananda Prabhu Sivananda, Francesco Verdoja, Ville Kyrki</h3>
<p>While 2D occupancy maps commonly used in mobile robotics enable safe
navigation in indoor environments, in order for robots to understand their
environment to the level required for them to perform more advanced tasks,
representing 3D geometry and semantic environment information is required. We
propose a pipeline that can generate a multi-layer representation of indoor
environments for robotic applications. The proposed representation includes 3D
metric-semantic layers, a 2D occupancy layer, and an object instance layer
where known objects are replaced with an approximate model obtained through a
novel model-matching approach. The metric-semantic layer and the object
instance layer are combined to form an augmented representation of the
environment. Experiments show that the proposed shape matching method
outperforms a state-of-the-art deep learning method when tasked to complete
unseen parts of objects in the scene. The pipeline performance translates well
from simulation to real world as shown by F1-score analysis, with semantic
segmentation accuracy using Mask R-CNN acting as the major bottleneck. Finally,
we also demonstrate on a real robotic platform how the multi-layer map can be
used to improve navigation safety.
</p>
<a href="http://arxiv.org/abs/2103.07298" target="_blank">arXiv:2103.07298</a> [<a href="http://arxiv.org/pdf/2103.07298" target="_blank">pdf</a>]

<h2>Siamese Infrared and Visible Light Fusion Network for RGB-T Tracking. (arXiv:2103.07302v1 [cs.CV])</h2>
<h3>Peng Jingchao, Zhao Haitao, Hu Zhengwei, Zhuang Yi, Wang Bofan</h3>
<p>Due to the different photosensitive properties of infrared and visible light,
the registered RGB-T image pairs shot in the same scene exhibit quite different
characteristics. This paper proposes a siamese infrared and visible light
fusion Network (SiamIVFN) for RBG-T image-based tracking. SiamIVFN contains two
main subnetworks: a complementary-feature-fusion network (CFFN) and a
contribution-aggregation network (CAN). CFFN utilizes a two-stream multilayer
convolutional structure whose filters for each layer are partially coupled to
fuse the features extracted from infrared images and visible light images. CFFN
is a feature-level fusion network, which can cope with the misalignment of the
RGB-T image pairs. Through adaptively calculating the contributions of infrared
and visible light features obtained from CFFN, CAN makes the tracker robust
under various light conditions. Experiments on two RGB-T tracking benchmark
datasets demonstrate that the proposed SiamIVFN has achieved state-of-the-art
performance. The tracking speed of SiamIVFN is 147.6FPS, the current fastest
RGB-T fusion tracker.
</p>
<a href="http://arxiv.org/abs/2103.07302" target="_blank">arXiv:2103.07302</a> [<a href="http://arxiv.org/pdf/2103.07302" target="_blank">pdf</a>]

<h2>Vectorial Parameterizations of Pose. (arXiv:2103.07309v1 [cs.RO])</h2>
<h3>Timothy D. Barfoot, James R. Forbes, Gabriele M. T. D&#x27;Eleuterio</h3>
<p>Robotics and computer vision problems commonly require handling rigid-body
motions comprising translation and rotation - together referred to as pose. In
some situations, a vectorial parameterization of pose can be useful, where
elements of a vector space are surjectively mapped to a matrix Lie group. For
example, these vectorial representations can be employed for optimization as
well as uncertainty representation on groups. The most common mapping is the
matrix exponential, which maps elements of a Lie algebra onto the associated
Lie group. However, this choice is not unique. It has been previously shown how
to characterize all such vectorial parameterizations for SO(3), the group of
rotations. We extend this result to SE(3), the group of poses (translation and
rotation), showing how to build a family of mappings that includes the matrix
exponential as well as the Cayley transformation. While our main contribution
is the theory, we also demonstrate three different applications of the proposed
pose mappings: (i) pose interpolation, (ii) pose servoing control, and (iii)
pose estimation in a pointcloud alignment problem. In the pointcloud alignment
problem our results lead to a new algorithm based on the Cayley transformation,
which we call CayPer.
</p>
<a href="http://arxiv.org/abs/2103.07309" target="_blank">arXiv:2103.07309</a> [<a href="http://arxiv.org/pdf/2103.07309" target="_blank">pdf</a>]

<h2>Juggling With Representations: On the Information Transfer Between Imagery, Point Clouds, and Meshes for Multi-Modal Semantics. (arXiv:2103.07348v1 [cs.CV])</h2>
<h3>Dominik Laupheimer, Norbert Haala</h3>
<p>The automatic semantic segmentation of the huge amount of acquired remote
sensing data has become an important task in the last decade. Images and Point
Clouds (PCs) are fundamental data representations, particularly in urban
mapping applications. Textured 3D meshes integrate both data representations
geometrically by wiring the PC and texturing the surface elements with
available imagery. We present a mesh-centered holistic geometry-driven
methodology that explicitly integrates entities of imagery, PC and mesh. Due to
its integrative character, we choose the mesh as the core representation that
also helps to solve the visibility problem for points in imagery. Utilizing the
proposed multi-modal fusion as the backbone and considering the established
entity relationships, we enable the sharing of information across the
modalities imagery, PC and mesh in a two-fold manner: (i) feature transfer and
(ii) label transfer. By these means, we achieve to enrich feature vectors to
multi-modal feature vectors for each representation. Concurrently, we achieve
to label all representations consistently while reducing the manual label
effort to a single representation. Consequently, we facilitate to train machine
learning algorithms and to semantically segment any of these data
representations - both in a multi-modal and single-modal sense. The paper
presents the association mechanism and the subsequent information transfer,
which we believe are cornerstones for multi-modal scene analysis. Furthermore,
we discuss the preconditions and limitations of the presented approach in
detail. We demonstrate the effectiveness of our methodology on the ISPRS 3D
semantic labeling contest (Vaihingen 3D) and a proprietary data set (Hessigheim
3D).
</p>
<a href="http://arxiv.org/abs/2103.07348" target="_blank">arXiv:2103.07348</a> [<a href="http://arxiv.org/pdf/2103.07348" target="_blank">pdf</a>]

<h2>Monocular Quasi-Dense 3D Object Tracking. (arXiv:2103.07351v1 [cs.CV])</h2>
<h3>Hou-Ning Hu, Yung-Hsu Yang, Tobias Fischer, Trevor Darrell, Fisher Yu, Min Sun</h3>
<p>A reliable and accurate 3D tracking framework is essential for predicting
future locations of surrounding objects and planning the observer's actions in
numerous applications such as autonomous driving. We propose a framework that
can effectively associate moving objects over time and estimate their full 3D
bounding box information from a sequence of 2D images captured on a moving
platform. The object association leverages quasi-dense similarity learning to
identify objects in various poses and viewpoints with appearance cues only.
After initial 2D association, we further utilize 3D bounding boxes
depth-ordering heuristics for robust instance association and motion-based 3D
trajectory prediction for re-identification of occluded vehicles. In the end,
an LSTM-based object velocity learning module aggregates the long-term
trajectory information for more accurate motion extrapolation. Experiments on
our proposed simulation data and real-world benchmarks, including KITTI,
nuScenes, and Waymo datasets, show that our tracking framework offers robust
object association and tracking on urban-driving scenarios. On the Waymo Open
benchmark, we establish the first camera-only baseline in the 3D tracking and
3D detection challenges. Our quasi-dense 3D tracking pipeline achieves
impressive improvements on the nuScenes 3D tracking benchmark with near five
times tracking accuracy of the best vision-only submission among all published
methods. Our code, data and trained models are available at
https://github.com/SysCV/qd-3dt.
</p>
<a href="http://arxiv.org/abs/2103.07351" target="_blank">arXiv:2103.07351</a> [<a href="http://arxiv.org/pdf/2103.07351" target="_blank">pdf</a>]

<h2>PLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View Depth Estimation with Neural Positional Encoding and Distilled Matting Loss. (arXiv:2103.07362v1 [cs.CV])</h2>
<h3>Juan Luis Gonzalez Bello, Munchurl Kim</h3>
<p>In this paper, we propose a self-supervised single-view pixel-level accurate
depth estimation network, called PLADE-Net. The PLADE-Net is the first work
that shows unprecedented accuracy levels, exceeding 95\% in terms of the
$\delta^1$ metric on the challenging KITTI dataset. Our PLADE-Net is based on a
new network architecture with neural positional encoding and a novel loss
function that borrows from the closed-form solution of the matting Laplacian to
learn pixel-level accurate depth estimation from stereo images. Neural
positional encoding allows our PLADE-Net to obtain more consistent depth
estimates by letting the network reason about location-specific image
properties such as lens and projection distortions. Our novel distilled matting
Laplacian loss allows our network to predict sharp depths at object boundaries
and more consistent depths in highly homogeneous regions. Our proposed method
outperforms all previous self-supervised single-view depth estimation methods
by a large margin on the challenging KITTI dataset, with unprecedented levels
of accuracy. Furthermore, our PLADE-Net, naively extended for stereo inputs,
outperforms the most recent self-supervised stereo methods, even without any
advanced blocks like 1D correlations, 3D convolutions, or spatial pyramid
pooling. We present extensive ablation studies and experiments that support our
method's effectiveness on the KITTI, CityScapes, and Make3D datasets.
</p>
<a href="http://arxiv.org/abs/2103.07362" target="_blank">arXiv:2103.07362</a> [<a href="http://arxiv.org/pdf/2103.07362" target="_blank">pdf</a>]

<h2>Information Maximization Clustering via Multi-View Self-Labelling. (arXiv:2103.07368v1 [cs.CV])</h2>
<h3>Foivos Ntelemis, Yaochu Jin, Spencer A. Thomas</h3>
<p>Image clustering is a particularly challenging computer vision task, which
aims to generate annotations without human supervision. Recent advances focus
on the use of self-supervised learning strategies in image clustering, by first
learning valuable semantics and then clustering the image representations.
These multiple-phase algorithms, however, increase the computational time and
their final performance is reliant on the first stage. By extending the
self-supervised approach, we propose a novel single-phase clustering method
that simultaneously learns meaningful representations and assigns the
corresponding annotations. This is achieved by integrating a discrete
representation into the self-supervised paradigm through a classifier net.
Specifically, the proposed clustering objective employs mutual information, and
maximizes the dependency between the integrated discrete representation and a
discrete probability distribution. The discrete probability distribution is
derived though the self-supervised process by comparing the learnt latent
representation with a set of trainable prototypes. To enhance the learning
performance of the classifier, we jointly apply the mutual information across
multi-crop views. Our empirical results show that the proposed framework
outperforms state-of-the-art techniques with the average accuracy of 89.1% and
49.0%, respectively, on CIFAR-10 and CIFAR-100/20 datasets. Finally, the
proposed method also demonstrates attractive robustness to parameter settings,
making it ready to be applicable to other datasets.
</p>
<a href="http://arxiv.org/abs/2103.07368" target="_blank">arXiv:2103.07368</a> [<a href="http://arxiv.org/pdf/2103.07368" target="_blank">pdf</a>]

<h2>PatchNet -- Short-range Template Matching for Efficient Video Processing. (arXiv:2103.07371v1 [cs.CV])</h2>
<h3>Huizi Mao, Sibo Zhu, Song Han, William J. Dally</h3>
<p>Object recognition is a fundamental problem in many video processing tasks,
accurately locating seen objects at low computation cost paves the way for
on-device video recognition. We propose PatchNet, an efficient convolutional
neural network to match objects in adjacent video frames. It learns the
patchwise correlation features instead of pixel features. PatchNet is very
compact, running at just 58MFLOPs, $5\times$ simpler than MobileNetV2. We
demonstrate its application on two tasks, video object detection and visual
object tracking. On ImageNet VID, PatchNet reduces the flops of R-FCN
ResNet-101 by 5x and EfficientDet-D0 by 3.4x with less than 1% mAP loss. On
OTB2015, PatchNet reduces SiamFC and SiamRPN by 2.5x with no accuracy loss.
Experiments on Jetson Nano further demonstrate 2.8x to 4.3x speed-ups
associated with flops reduction. Code is open sourced at
https://github.com/RalphMao/PatchNet.
</p>
<a href="http://arxiv.org/abs/2103.07371" target="_blank">arXiv:2103.07371</a> [<a href="http://arxiv.org/pdf/2103.07371" target="_blank">pdf</a>]

<h2>ACTION-Net: Multipath Excitation for Action Recognition. (arXiv:2103.07372v1 [cs.CV])</h2>
<h3>Zhengwei Wang, Qi She, Aljosa Smolic</h3>
<p>Spatial-temporal, channel-wise, and motion patterns are three complementary
and crucial types of information for video action recognition. Conventional 2D
CNNs are computationally cheap but cannot catch temporal relationships; 3D CNNs
can achieve good performance but are computationally intensive. In this work,
we tackle this dilemma by designing a generic and effective module that can be
embedded into 2D CNNs. To this end, we propose a spAtio-temporal, Channel and
moTion excitatION (ACTION) module consisting of three paths: Spatio-Temporal
Excitation (STE) path, Channel Excitation (CE) path, and Motion Excitation (ME)
path. The STE path employs one channel 3D convolution to characterize
spatio-temporal representation. The CE path adaptively recalibrates
channel-wise feature responses by explicitly modeling interdependencies between
channels in terms of the temporal aspect. The ME path calculates feature-level
temporal differences, which is then utilized to excite motion-sensitive
channels. We equip 2D CNNs with the proposed ACTION module to form a simple yet
effective ACTION-Net with very limited extra computational cost. ACTION-Net is
demonstrated by consistently outperforming 2D CNN counterparts on three
backbones (i.e., ResNet-50, MobileNet V2 and BNInception) employing three
datasets (i.e., Something-Something V2, Jester, and EgoGesture). Codes are
available at \url{https://github.com/V-Sense/ACTION-Net}.
</p>
<a href="http://arxiv.org/abs/2103.07372" target="_blank">arXiv:2103.07372</a> [<a href="http://arxiv.org/pdf/2103.07372" target="_blank">pdf</a>]

<h2>Offset-free Model Predictive Control: A Ball Catching Application with a Spherical Soft Robotic Arm. (arXiv:2103.07379v1 [cs.RO])</h2>
<h3>Yaohui Huang, Matthias Hofer, Raffaello D&#x27;Andrea</h3>
<p>This paper presents an offset-free model predictive controller for fast and
accurate control of a spherical soft robotic arm. In this control scheme, a
linear model is combined with an online disturbance estimation technique to
systematically compensate model deviations. Dynamic effects such as material
relaxation resulting from the use of soft materials can be addressed to achieve
offset-free tracking. The tracking error can be reduced by 35% when compared to
a standard model predictive controller without a disturbance compensation
scheme. The improved tracking performance enables the realization of a ball
catching application, where the spherical soft robotic arm can catch a ball
thrown by a human.
</p>
<a href="http://arxiv.org/abs/2103.07379" target="_blank">arXiv:2103.07379</a> [<a href="http://arxiv.org/pdf/2103.07379" target="_blank">pdf</a>]

<h2>Generating and Characterizing Scenarios for Safety Testing of Autonomous Vehicles. (arXiv:2103.07403v1 [cs.RO])</h2>
<h3>Zahra Ghodsi, Siva Kumar Sastry Hari, Iuri Frosio, Timothy Tsai, Alejandro Troccoli, Stephen W. Keckler, Siddharth Garg, Anima Anandkumar</h3>
<p>Extracting interesting scenarios from real-world data as well as generating
failure cases is important for the development and testing of autonomous
systems. We propose efficient mechanisms to both characterize and generate
testing scenarios using a state-of-the-art driving simulator. For any scenario,
our method generates a set of possible driving paths and identifies all the
possible safe driving trajectories that can be taken starting at different
times, to compute metrics that quantify the complexity of the scenario. We use
our method to characterize real driving data from the Next Generation
Simulation (NGSIM) project, as well as adversarial scenarios generated in
simulation. We rank the scenarios by defining metrics based on the complexity
of avoiding accidents and provide insights into how the AV could have minimized
the probability of incurring an accident. We demonstrate a strong correlation
between the proposed metrics and human intuition.
</p>
<a href="http://arxiv.org/abs/2103.07403" target="_blank">arXiv:2103.07403</a> [<a href="http://arxiv.org/pdf/2103.07403" target="_blank">pdf</a>]

<h2>Real-time Nonrigid Mosaicking of Laparoscopy Images. (arXiv:2103.07414v1 [cs.CV])</h2>
<h3>Haoyin Zhou, Jagadeesan Jayender</h3>
<p>The ability to extend the field of view of laparoscopy images can help the
surgeons to obtain a better understanding of the anatomical context. However,
due to tissue deformation, complex camera motion and significant
three-dimensional (3D) anatomical surface, image pixels may have non-rigid
deformation and traditional mosaicking methods cannot work robustly for
laparoscopy images in real-time. To solve this problem, a novel two-dimensional
(2D) non-rigid simultaneous localization and mapping (SLAM) system is proposed
in this paper, which is able to compensate for the deformation of pixels and
perform image mosaicking in real-time. The key algorithm of this 2D non-rigid
SLAM system is the expectation maximization and dual quaternion (EMDQ)
algorithm, which can generate smooth and dense deformation field from sparse
and noisy image feature matches in real-time. An uncertainty-based loop closing
method has been proposed to reduce the accumulative errors. To achieve
real-time performance, both CPU and GPU parallel computation technologies are
used for dense mosaicking of all pixels. Experimental results on \textit{in
vivo} and synthetic data demonstrate the feasibility and accuracy of our
non-rigid mosaicking method.
</p>
<a href="http://arxiv.org/abs/2103.07414" target="_blank">arXiv:2103.07414</a> [<a href="http://arxiv.org/pdf/2103.07414" target="_blank">pdf</a>]

<h2>Probabilistic two-stage detection. (arXiv:2103.07461v1 [cs.CV])</h2>
<h3>Xingyi Zhou, Vladlen Koltun, Philipp Kr&#xe4;henb&#xfc;hl</h3>
<p>We develop a probabilistic interpretation of two-stage object detection. We
show that this probabilistic interpretation motivates a number of common
empirical training practices. It also suggests changes to two-stage detection
pipelines. Specifically, the first stage should infer proper
object-vs-background likelihoods, which should then inform the overall score of
the detector. A standard region proposal network (RPN) cannot infer this
likelihood sufficiently well, but many one-stage detectors can. We show how to
build a probabilistic two-stage detector from any state-of-the-art one-stage
detector. The resulting detectors are faster and more accurate than both their
one- and two-stage precursors. Our detector achieves 56.4 mAP on COCO test-dev
with single-scale testing, outperforming all published results. Using a
lightweight backbone, our detector achieves 49.2 mAP on COCO at 33 fps on a
Titan Xp, outperforming the popular YOLOv4 model.
</p>
<a href="http://arxiv.org/abs/2103.07461" target="_blank">arXiv:2103.07461</a> [<a href="http://arxiv.org/pdf/2103.07461" target="_blank">pdf</a>]

<h2>3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v1 [cs.CV])</h2>
<h3>Luis Roldao, Raoul de Charette, Anne Verroust-Blondet</h3>
<p>Semantic Scene Completion (SSC) aims to jointly estimate the complete
geometry and semantics of a scene, assuming partial sparse input. In the last
years following the multiplication of large-scale 3D datasets, SSC has gained
significant momentum in the research community because it holds unresolved
challenges. Specifically, SSC lies in the ambiguous completion of large
unobserved areas and the weak supervision signal of the ground truth. This led
to a substantially increasing number of papers on the matter. This survey aims
to identify, compare and analyze the techniques providing a critical analysis
of the SSC literature on both methods and datasets. Throughout the paper, we
provide an in-depth analysis of the existing works covering all choices made by
the authors while highlighting the remaining avenues of research. SSC
performance of the SoA on the most popular datasets is also evaluated and
analyzed.
</p>
<a href="http://arxiv.org/abs/2103.07466" target="_blank">arXiv:2103.07466</a> [<a href="http://arxiv.org/pdf/2103.07466" target="_blank">pdf</a>]

<h2>Imitation learning-based framework for learning 6-D linear compliant motions. (arXiv:1809.01561v3 [cs.RO] UPDATED)</h2>
<h3>Markku Suomalainen, Fares J. Abu-Dakka, Ville Kyrki</h3>
<p>We present a novel method for learning from demonstration 6-D tasks that can
be modeled as a sequence of linear motions and compliances. The focus of this
paper is the learning of a single linear primitive, many of which can be
sequenced to perform more complex tasks. The presented method learns from
demonstrations only, without any prior information, how to take advantage of
mechanical gradients in in-contact tasks, such as assembly, both for
translations and rotations. The method assumes there exists a desired linear
direction in 6-D which, if followed by the manipulator, leads the robot's
end-effector to the goal area shown in the demonstration, either in free space
or by leveraging contact through compliance. First, demonstrations are gathered
where the teacher explicitly shows the robot how the mechanical gradients can
be used as guidance towards the goal. From the demonstrations, a set of
directions is computed which would result in the observed motion at each
timestep during a demonstration of a single primitive. By observing which
direction is included in all these sets, we find a single desired direction
which can reproduce the demonstrated motion. Finding the number of compliant
axes and their directions in both rotation and translation is based on the
assumption that in the presence of a desired direction of motion, all other
observed motion is caused by the contact force of the environment, signalling
the need for compliance. We evaluate the method on a KUKA LWR4+ robot with test
setups imitating typical tasks where a human would use compliance to cope with
positional uncertainty. Results show that the method can successfully learn and
reproduce compliant motions by taking advantage of the geometry of the task,
therefore reducing the need for localization accuracy.
</p>
<a href="http://arxiv.org/abs/1809.01561" target="_blank">arXiv:1809.01561</a> [<a href="http://arxiv.org/pdf/1809.01561" target="_blank">pdf</a>]

<h2>Show, Price and Negotiate: A Negotiator with Online Value Look-Ahead. (arXiv:1905.03721v2 [cs.CV] UPDATED)</h2>
<h3>Amin Parvaneh, Ehsan Abbasnejad, Qi Wu, Javen Qinfeng Shi, Anton van den Hengel</h3>
<p>Negotiation, as an essential and complicated aspect of online shopping, is
still challenging for an intelligent agent. To that end, we propose the Price
Negotiator, a modular deep neural network that addresses the unsolved problems
in recent studies by (1) considering images of the items as a crucial, though
neglected, source of information in a negotiation, (2) heuristically finding
the most similar items from an external online source to predict the potential
value and an acceptable agreement price, (3) predicting a general price-based
action at each turn which is fed into the language generator to output the
supporting natural language, and (4) adjusting the prices based on the
predicted actions. Empirically, we show that our model, that is trained in both
supervised and reinforcement learning setting, significantly improves
negotiation on the CraigslistBargain dataset, in terms of the agreement price,
price consistency, and dialogue quality.
</p>
<a href="http://arxiv.org/abs/1905.03721" target="_blank">arXiv:1905.03721</a> [<a href="http://arxiv.org/pdf/1905.03721" target="_blank">pdf</a>]

<h2>ACE: Adaptive Confusion Energy for Natural World Data Distribution. (arXiv:1910.12423v3 [cs.CV] UPDATED)</h2>
<h3>Yen-Chi Hsu, Cheng-Yao Hong, Wan-Cyuan Fan, Ming-Sui Lee, Davi Geiger, Tyng-Luh Liu</h3>
<p>With the development of deep learning, standard classification problems have
achieved good results. However, conventional classification problems are often
too idealistic. Most data in the natural world usually have imbalanced
distribution and fine-grained characteristics. Recently, many state-of-the-art
approaches tend to focus on one or another separately, but rarely on both. In
this paper, we introduce a novel and adaptive batch-wise regularization based
on the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature
world distribution, which usually involves fine-grained and long-tailed
properties at the same time. ACE increases the difficulty of the training
process and further alleviates the overfitting problem. Through the datasets
with the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed
(ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows
that the ACE is not only competitive to some state-of-the-art on performance
but also demonstrates the effectiveness of training.
</p>
<a href="http://arxiv.org/abs/1910.12423" target="_blank">arXiv:1910.12423</a> [<a href="http://arxiv.org/pdf/1910.12423" target="_blank">pdf</a>]

<h2>Show, Recall, and Tell: Image Captioning with Recall Mechanism. (arXiv:2001.05876v3 [cs.CV] UPDATED)</h2>
<h3>Li Wang, Zechen Bai, Yonghua Zhang, Hongtao Lu</h3>
<p>Generating natural and accurate descriptions in image cap-tioning has always
been a challenge. In this paper, we pro-pose a novel recall mechanism to
imitate the way human con-duct captioning. There are three parts in our recall
mecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS).
Recall unit is a text-retrieval module designedto retrieve recalled words for
images. SG and RWS are de-signed for the best use of recalled words. SG branch
cangenerate a recalled context, which can guide the process ofgenerating
caption. RWS branch is responsible for copyingrecalled words to the caption.
Inspired by pointing mecha-nism in text summarization, we adopt a soft switch
to balancethe generated-word probabilities between SG and RWS. Inthe CIDEr
optimization step, we also introduce an individualrecalled-word reward (WR) to
boost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr /
SPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 /
22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the
results of other state-of-the-artmethods.
</p>
<a href="http://arxiv.org/abs/2001.05876" target="_blank">arXiv:2001.05876</a> [<a href="http://arxiv.org/pdf/2001.05876" target="_blank">pdf</a>]

<h2>Fully Convolutional Online Tracking. (arXiv:2004.07109v4 [cs.CV] UPDATED)</h2>
<h3>Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu</h3>
<p>Online learning has turned out to be effective for improving tracking
performance. However, it could be simply applied for classification branch, but
still remains challenging for adapting to regression branch due to the complex
design. To tackle this issue, we present the first fully convolutional online
tracking framework (FCOT), with a focus on enabling online learning for both
classification and regression branches. Our key contribution is to introduce an
online regression model generator (RMG) based on the carefully designed
anchor-free box regression branch, which enables our FCOT to be more effective
in handling target deformation during tracking procedure. In addition, to deal
with the confusion of similar objects, we devise a simple yet effective
multi-scale classification branch to improve both accuracy and robustness of
FCOT. Due to its simplicity in design, our FCOT could be trained and deployed
in a fully convolutional manner with a running speed of 45FPS. The proposed
FCOT sets a new state-of-the-art results on six benchmarks including VOT2018,
LaSOT, TrackingNet, GOT-10k, UAV123, and NFS. Particularly, among real-time
trackers, our FCOT achieves EAO of 0.456 on VOT2018, NP of 0.678 on LaSOT, NP
of 0.828 on TrackingNet, and AO of 0.640 on GOT-10k. The code and models will
be made available at https://github.com/MCG-NJU/FCOT.
</p>
<a href="http://arxiv.org/abs/2004.07109" target="_blank">arXiv:2004.07109</a> [<a href="http://arxiv.org/pdf/2004.07109" target="_blank">pdf</a>]

<h2>Network Bending: Expressive Manipulation of Deep Generative Models. (arXiv:2005.12420v2 [cs.CV] UPDATED)</h2>
<h3>Terence Broad, Frederic Fol Leymarie, Mick Grierson</h3>
<p>We introduce a new framework for manipulating and interacting with deep
generative models that we call network bending. We present a comprehensive set
of deterministic transformations that can be inserted as distinct layers into
the computational graph of a trained generative neural network and applied
during inference. In addition, we present a novel algorithm for analysing the
deep generative model and clustering features based on their spatial activation
maps. This allows features to be grouped together based on spatial similarity
in an unsupervised fashion. This results in the meaningful manipulation of sets
of features that correspond to the generation of a broad array of semantically
significant features of the generated images. We outline this framework,
demonstrating our results on state-of-the-art deep generative models trained on
several image datasets. We show how it allows for the direct manipulation of
semantically meaningful aspects of the generative process as well as allowing
for a broad range of expressive outcomes.
</p>
<a href="http://arxiv.org/abs/2005.12420" target="_blank">arXiv:2005.12420</a> [<a href="http://arxiv.org/pdf/2005.12420" target="_blank">pdf</a>]

<h2>CANet: Context Aware Network for 3D Brain Tumor Segmentation. (arXiv:2007.07788v2 [cs.CV] UPDATED)</h2>
<h3>Zhihua Liu, Lei Tong, Long Chen, Feixiang Zhou, Zheheng Jiang, Qianni Zhang, Yinhai Wang, Caifeng Shan, Ling Li, Huiyu Zhou</h3>
<p>Automated segmentation of brain glioma plays an active role in diagnosis
decision, progression monitoring and surgery planning. Based on deep neural
networks, previous studies have shown promising technologies for brain glioma
segmentation. However, these approaches lack powerful strategies to incorporate
contextual information of tumor cells and their surrounding, which has been
proven as a fundamental cue to deal with local ambiguity. In this work, we
propose a novel approach named Context-Aware Network (CANet) for brain glioma
segmentation. CANet captures high dimensional and discriminative features with
contexts from both the convolutional space and feature interaction graphs. We
further propose context guided attentive conditional random fields which can
selectively aggregate features. We evaluate our method using publicly
accessible brain glioma segmentation datasets BRATS2017, BRATS2018 and
BRATS2019. The experimental results show that the proposed algorithm has better
or competitive performance against several State-of-The-Art approaches under
different segmentation metrics on the training and validation sets.
</p>
<a href="http://arxiv.org/abs/2007.07788" target="_blank">arXiv:2007.07788</a> [<a href="http://arxiv.org/pdf/2007.07788" target="_blank">pdf</a>]

<h2>Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination. (arXiv:2008.01068v2 [cs.CV] UPDATED)</h2>
<h3>Peng-Shuai Wang, Yu-Qi Yang, Qian-Fang Zou, Zhirong Wu, Yang Liu, Xin Tong</h3>
<p>Although unsupervised feature learning has demonstrated its advantages to
reducing the workload of data labeling and network design in many fields,
existing unsupervised 3D learning methods still cannot offer a generic network
for various shape analysis tasks with competitive performance to supervised
methods. In this paper, we propose an unsupervised method for learning a
generic and efficient shape encoding network for different shape analysis
tasks. The key idea of our method is to jointly encode and learn shape and
point features from unlabeled 3D point clouds. For this purpose, we adapt
HR-Net to octree-based convolutional neural networks for jointly encoding shape
and point features with fused multiresolution subnetworks and design a
simple-yet-efficient Multiresolution Instance Discrimination (MID) loss for
jointly learning the shape and point features. Our network takes a 3D point
cloud as input and output both shape and point features. After training, the
network is concatenated with simple task-specific back-end layers and
fine-tuned for different shape analysis tasks. We evaluate the efficacy and
generality of our method and validate our network and loss design with a set of
shape analysis tasks, including shape classification, semantic shape
segmentation, as well as shape registration tasks. With simple back-ends, our
network demonstrates the best performance among all unsupervised methods and
achieves competitive performance to supervised methods, especially in tasks
with a small labeled dataset. For fine-grained shape segmentation, our method
even surpasses existing supervised methods by a large margin.
</p>
<a href="http://arxiv.org/abs/2008.01068" target="_blank">arXiv:2008.01068</a> [<a href="http://arxiv.org/pdf/2008.01068" target="_blank">pdf</a>]

<h2>Regularized Densely-connected Pyramid Network for Salient Instance Segmentation. (arXiv:2008.12416v2 [cs.CV] UPDATED)</h2>
<h3>Yu-Huan Wu, Yun Liu, Le Zhang, Wang Gao, Ming-Ming Cheng</h3>
<p>Much of the recent efforts on salient object detection (SOD) have been
devoted to producing accurate saliency maps without being aware of their
instance labels. To this end, we propose a new pipeline for end-to-end salient
instance segmentation (SIS) that predicts a class-agnostic mask for each
detected salient instance. To better use the rich feature hierarchies in deep
networks and enhance the side predictions, we propose the regularized dense
connections, which attentively promote informative features and suppress
non-informative ones from all feature pyramids. A novel multi-level RoIAlign
based decoder is introduced to adaptively aggregate multi-level features for
better mask predictions. Such strategies can be well-encapsulated into the Mask
R-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that
our design significantly outperforms existing \sArt competitors by 6.3\%
(58.6\% vs. 52.3\%) in terms of the AP metric.The code is available at
https://github.com/yuhuan-wu/RDPNet.
</p>
<a href="http://arxiv.org/abs/2008.12416" target="_blank">arXiv:2008.12416</a> [<a href="http://arxiv.org/pdf/2008.12416" target="_blank">pdf</a>]

<h2>Probabilistic Surface Friction Estimation Based on Visual and Haptic Measurements. (arXiv:2010.08277v3 [cs.RO] UPDATED)</h2>
<h3>Tran Nguyen Le, Francesco Verdoja, Fares J. Abu-Dakka, Ville Kyrki</h3>
<p>Accurately modeling local surface properties of objects is crucial to many
robotic applications, from grasping to material recognition. Surface properties
like friction are however difficult to estimate, as visual observation of the
object does not convey enough information over these properties. In contrast,
haptic exploration is time consuming as it only provides information relevant
to the explored parts of the object. In this work, we propose a joint
visuo-haptic object model that enables the estimation of surface friction
coefficient over an entire object by exploiting the correlation of visual and
haptic information, together with a limited haptic exploration by a robotic
arm. We demonstrate the validity of the proposed method by showing its ability
to estimate varying friction coefficients on a range of real multi-material
objects. Furthermore, we illustrate how the estimated friction coefficients can
improve grasping success rate by guiding a grasp planner toward high friction
areas.
</p>
<a href="http://arxiv.org/abs/2010.08277" target="_blank">arXiv:2010.08277</a> [<a href="http://arxiv.org/pdf/2010.08277" target="_blank">pdf</a>]

<h2>Learning Spring Mass Locomotion: Guiding Policies with a Reduced-Order Model. (arXiv:2010.11234v2 [cs.RO] UPDATED)</h2>
<h3>Kevin Green, Yesh Godse, Jeremy Dao, Ross L. Hatton, Alan Fern, Jonathan Hurst</h3>
<p>In this paper, we describe an approach to achieve dynamic legged locomotion
on physical robots which combines existing methods for control with
reinforcement learning. Specifically, our goal is a control hierarchy in which
highest-level behaviors are planned through reduced-order models, which
describe the fundamental physics of legged locomotion, and lower level
controllers utilize a learned policy that can bridge the gap between the
idealized, simple model and the complex, full order robot. The high-level
planner can use a model of the environment and be task specific, while the
low-level learned controller can execute a wide range of motions so that it
applies to many different tasks. In this letter we describe this learned
dynamic walking controller and show that a range of walking motions from
reduced-order models can be used as the command and primary training signal for
learned policies. The resulting policies do not attempt to naively track the
motion (as a traditional trajectory tracking controller would) but instead
balance immediate motion tracking with long term stability. The resulting
controller is demonstrated on a human scale, unconstrained, untethered bipedal
robot at speeds up to 1.2 m/s. This letter builds the foundation of a generic,
dynamic learned walking controller that can be applied to many different tasks.
</p>
<a href="http://arxiv.org/abs/2010.11234" target="_blank">arXiv:2010.11234</a> [<a href="http://arxiv.org/pdf/2010.11234" target="_blank">pdf</a>]

<h2>ChoiRbot: A ROS 2 Toolbox for Cooperative Robotics. (arXiv:2010.13431v2 [cs.RO] UPDATED)</h2>
<h3>Andrea Testa, Andrea Camisa, Giuseppe Notarstefano</h3>
<p>In this paper, we introduce ChoiRbot, a toolbox for distributed cooperative
robotics based on the novel Robot Operating System (ROS) 2. ChoiRbot provides a
fully-functional toolset to execute complex distributed multi-robot tasks,
either in simulation or experimentally, with a particular focus on networks of
heterogeneous robots without a central coordinator. Thanks to its modular
structure, ChoiRbot allows for a highly straight implementation of
optimization-based distributed control schemes, such as distributed optimal
control, model predictive control, task assignment, in which local computation
and communication with neighboring robots are alternated. To this end, the
toolbox provides functionalities for the solution of distributed optimization
problems. The package can be also used to implement distributed feedback laws
that do not need optimization features but do require the exchange of
information among robots. The potential of the toolbox is illustrated with
simulations and experiments on distributed robotics scenarios with mobile
ground robots. The ChoiRbot toolbox is available at
https://github.com/OPT4SMART/choirbot.
</p>
<a href="http://arxiv.org/abs/2010.13431" target="_blank">arXiv:2010.13431</a> [<a href="http://arxiv.org/pdf/2010.13431" target="_blank">pdf</a>]

<h2>Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs. (arXiv:2011.00844v4 [cs.CV] UPDATED)</h2>
<h3>Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo</h3>
<p>Natural images are projections of 3D objects on a 2D image plane. While
state-of-the-art 2D generative models like GANs show unprecedented quality in
modeling the natural image manifold, it is unclear whether they implicitly
capture the underlying 3D object structures. And if so, how could we exploit
such knowledge to recover the 3D shapes of objects in the images? To answer
these questions, in this work, we present the first attempt to directly mine 3D
geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only.
Through our investigation, we found that such a pre-trained GAN indeed contains
rich 3D knowledge and thus can be used to recover 3D shape from a single 2D
image in an unsupervised manner. The core of our framework is an iterative
strategy that explores and exploits diverse viewpoint and lighting variations
in the GAN image manifold. The framework does not require 2D keypoint or 3D
annotations, or strong assumptions on object shapes (e.g. shapes are
symmetric), yet it successfully recovers 3D shapes with high precision for
human faces, cats, cars, and buildings. The recovered 3D shapes immediately
allow high-quality image editing like relighting and object rotation. We
quantitatively demonstrate the effectiveness of our approach compared to
previous methods in both 3D shape reconstruction and face rotation. Our code is
available at https://github.com/XingangPan/GAN2Shape.
</p>
<a href="http://arxiv.org/abs/2011.00844" target="_blank">arXiv:2011.00844</a> [<a href="http://arxiv.org/pdf/2011.00844" target="_blank">pdf</a>]

<h2>Iterative Surface Mapping Using Local Geometry Approximation with Sparse Measurements During Robotic Tooling Tasks. (arXiv:2011.06375v2 [cs.RO] UPDATED)</h2>
<h3>Manuel Amersdorfer, Thomas Meurer</h3>
<p>We present a method to map an unknown 3D freeform surface using only sparse
measurements while the end-effector of a robotic manipulator moves along the
surface. The geometry is locally approximated by a plane, which is defined by
measured points on the surface. The method relies on linear Kalman filters,
estimating the height of each point on a 2D grid. Therefore, the approximation
covariance for each grid point is determined by the projected distance to the
measured points' positions. We propose different update strategies for the grid
points, where the approximation is valid to consider the locality of the planar
approximation. We experimentally validate the approach by tracking the surface
with a robotic manipulator. Three laser distance sensors mounted on the
end-effector continuously measure points on the surface during the motion.
These points determine the approximation plane, which updates the mapping. It
is shown that the surface geometry can be mapped reasonably accurate with a
mean absolute error below 1 mm. The mapping error mainly depends on the size of
the approximation area and the curvature of the surface.
</p>
<a href="http://arxiv.org/abs/2011.06375" target="_blank">arXiv:2011.06375</a> [<a href="http://arxiv.org/pdf/2011.06375" target="_blank">pdf</a>]

<h2>DenserNet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation. (arXiv:2012.02366v4 [cs.CV] UPDATED)</h2>
<h3>Dongfang Liu, Yiming Cui, Liqi Yan, Christos Mousas, Baijian Yang, Yingjie Chen</h3>
<p>In this work, we introduce a Denser Feature Network (DenserNet) for visual
localization. Our work provides three principal contributions. First, we
develop a convolutional neural network (CNN) architecture which aggregates
feature maps at different semantic levels for image representations. Using
denser feature maps, our method can produce more keypoint features and increase
image retrieval accuracy. Second, our model is trained end-to-end without
pixel-level annotation other than positive and negative GPS-tagged image pairs.
We use a weakly supervised triplet ranking loss to learn discriminative
features and encourage keypoint feature repeatability for image representation.
Finally, our method is computationally efficient as our architecture has shared
features and parameters during computation. Our method can perform accurate
large-scale localization under challenging conditions while remaining the
computational constraint. Extensive experiment results indicate that our method
sets a new state-of-the-art on four challenging large-scale localization
benchmarks and three image retrieval benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.02366" target="_blank">arXiv:2012.02366</a> [<a href="http://arxiv.org/pdf/2012.02366" target="_blank">pdf</a>]

<h2>SWA Object Detection. (arXiv:2012.12645v3 [cs.CV] UPDATED)</h2>
<h3>Haoyang Zhang, Ying Wang, Feras Dayoub, Niko S&#xfc;nderhauf</h3>
<p>Do you want to improve 1.0 AP for your object detector without any inference
cost and any change to your detector? Let us tell you such a recipe. It is
surprisingly simple: train your detector for an extra 12 epochs using cyclical
learning rates and then average these 12 checkpoints as your final detection
model}. This potent recipe is inspired by Stochastic Weights Averaging (SWA),
which is proposed in arXiv:1803.05407 for improving generalization in deep
neural networks. We found it also very effective in object detection. In this
technique report, we systematically investigate the effects of applying SWA to
object detection as well as instance segmentation. Through extensive
experiments, we discover the aforementioned workable policy of performing SWA
in object detection, and we consistently achieve $\sim$1.0 AP improvement over
various popular detectors on the challenging COCO benchmark, including Mask
RCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will
make more researchers in object detection know this technique and help them
train better object detectors. Code is available at:
https://github.com/hyz-xmaster/swa_object_detection .
</p>
<a href="http://arxiv.org/abs/2012.12645" target="_blank">arXiv:2012.12645</a> [<a href="http://arxiv.org/pdf/2012.12645" target="_blank">pdf</a>]

<h2>Improving the Generalization of End-to-End Driving through Procedural Generation. (arXiv:2012.13681v2 [cs.RO] UPDATED)</h2>
<h3>Quanyi Li, Zhenghao Peng, Qihang Zhang, Chunxiao Liu, Bolei Zhou</h3>
<p>Over the past few years there is a growing interest in the learning-based
self driving system. To ensure safety, such systems are first developed and
validated in simulators before being deployed in the real world. However, most
of the existing driving simulators only contain a fixed set of scenes and a
limited number of configurable settings. That might easily cause the
overfitting issue for the learning-based driving systems as well as the lack of
their generalization ability to unseen scenarios. To better evaluate and
improve the generalization of end-to-end driving, we introduce an open-ended
and highly configurable driving simulator called PGDrive, following a key
feature of procedural generation. Diverse road networks are first generated by
the proposed generation algorithm via sampling from elementary road blocks.
Then they are turned into interactive training environments where traffic flows
of nearby vehicles with realistic kinematics are rendered. We validate that
training with the increasing number of procedurally generated scenes
significantly improves the generalization of the agent across scenarios of
different traffic densities and road networks. Many applications such as
multi-agent traffic simulation and safe driving benchmark can be further built
upon the simulator. To facilitate the joint research effort of end-to-end
driving, we release the simulator and pretrained models at
https://decisionforce.github.io/pgdrive
</p>
<a href="http://arxiv.org/abs/2012.13681" target="_blank">arXiv:2012.13681</a> [<a href="http://arxiv.org/pdf/2012.13681" target="_blank">pdf</a>]

<h2>Navigation Framework for a Hybrid Steel Bridge Inspection Robot. (arXiv:2101.02282v3 [cs.RO] UPDATED)</h2>
<h3>Hoang-Dung Bui, Hung M. La</h3>
<p>Autonomous navigation is essential for steel bridge inspection robot to
monitor and maintain the working condition of steel bridges. Majority of
existing robotic solutions requires human support to navigate the robot doing
the inspection. In this paper, a navigation framework is proposed for ARA robot
[1], [2] to run on mobile mode. In this mode, the robot needs to cross and
inspect all the available steel bars. The most significant contributions of
this research are four algorithms, which can process the depth data, segment it
into clusters, estimate the boundaries, construct a graph to represent the
structure, generate a shortest inspection path with any starting and ending
points, and determine available robot configuration for path planning.
Experiments on steel bridge structures setup highlight the effective
performance of the algorithms, and the potential to apply to the ARA robot to
run on real bridge structures. We released our source code in Github for the
research community to use.
</p>
<a href="http://arxiv.org/abs/2101.02282" target="_blank">arXiv:2101.02282</a> [<a href="http://arxiv.org/pdf/2101.02282" target="_blank">pdf</a>]

<h2>LLA: Loss-aware Label Assignment for Dense Pedestrian Detection. (arXiv:2101.04307v3 [cs.CV] UPDATED)</h2>
<h3>Zheng Ge, Jianfeng Wang, Xin Huang, Songtao Liu, Osamu Yoshie</h3>
<p>Label assignment has been widely studied in general object detection because
of its great impact on detectors' performance. However, none of these works
focus on label assignment in dense pedestrian detection. In this paper, we
propose a simple yet effective assigning strategy called Loss-aware Label
Assignment (LLA) to boost the performance of pedestrian detectors in crowd
scenarios. LLA first calculates classification (cls) and regression (reg)
losses between each anchor and ground-truth (GT) pair. A joint loss is then
defined as the weighted summation of cls and reg losses as the assigning
indicator. Finally, anchors with top K minimum joint losses for a certain GT
box are assigned as its positive anchors. Anchors that are not assigned to any
GT box are considered negative. Loss-aware label assignment is based on an
observation that anchors with lower joint loss usually contain richer semantic
information and thus can better represent their corresponding GT boxes.
Experiments on CrowdHuman and CityPersons show that such a simple label
assigning strategy can boost MR by 9.53% and 5.47% on two famous one-stage
detectors - RetinaNet and FCOS, respectively, demonstrating the effectiveness
of LLA.
</p>
<a href="http://arxiv.org/abs/2101.04307" target="_blank">arXiv:2101.04307</a> [<a href="http://arxiv.org/pdf/2101.04307" target="_blank">pdf</a>]

<h2>Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors. (arXiv:2101.05036v3 [cs.CV] UPDATED)</h2>
<h3>Ali Harakeh, Steven L. Waslander</h3>
<p>Predictive uncertainty estimation is an essential next step for the reliable
deployment of deep object detectors in safety-critical tasks. In this work, we
focus on estimating predictive distributions for bounding box regression output
with variance networks. We show that in the context of object detection,
training variance networks with negative log likelihood (NLL) can lead to high
entropy predictive distributions regardless of the correctness of the output
mean. We propose to use the energy score as a non-local proper scoring rule and
find that when used for training, the energy score leads to better calibrated
and lower entropy predictive distributions than NLL. We also address the
widespread use of non-proper scoring metrics for evaluating predictive
distributions from deep object detectors by proposing an alternate evaluation
approach founded on proper scoring rules. Using the proposed evaluation tools,
we show that although variance networks can be used to produce high quality
predictive distributions, ad-hoc approaches used by seminal object detectors
for choosing regression targets during training do not provide wide enough data
support for reliable variance learning. We hope that our work helps shift
evaluation in probabilistic object detection to better align with predictive
uncertainty evaluation in other machine learning domains. Code for all models,
evaluation, and datasets is available at:
https://github.com/asharakeh/probdet.git.
</p>
<a href="http://arxiv.org/abs/2101.05036" target="_blank">arXiv:2101.05036</a> [<a href="http://arxiv.org/pdf/2101.05036" target="_blank">pdf</a>]

<h2>Auto4D: Learning to Label 4D Objects from Sequential Point Clouds. (arXiv:2101.06586v2 [cs.CV] UPDATED)</h2>
<h3>Bin Yang, Min Bai, Ming Liang, Wenyuan Zeng, Raquel Urtasun</h3>
<p>In the past few years we have seen great advances in object perception
(particularly in 4D space-time dimensions) thanks to deep learning methods.
However, they typically rely on large amounts of high-quality labels to achieve
good performance, which often require time-consuming and expensive work by
human annotators. To address this we propose an automatic annotation pipeline
that generates accurate object trajectories in 3D space (i.e., 4D labels) from
LiDAR point clouds. The key idea is to decompose the 4D object label into two
parts: the object size in 3D that's fixed through time for rigid objects, and
the motion path describing the evolution of the object's pose through time.
Instead of generating a series of labels in one shot, we adopt an iterative
refinement process where online generated object detections are tracked through
time as the initialization. Given the cheap but noisy input, our model produces
higher quality 4D labels by re-estimating the object size and smoothing the
motion path, where the improvement is achieved by exploiting aggregated
observations and motion cues over the entire trajectory. We validate the
proposed method on a large-scale driving dataset and show a 25% reduction of
human annotation efforts. We also showcase the benefits of our approach in the
annotator-in-the-loop setting.
</p>
<a href="http://arxiv.org/abs/2101.06586" target="_blank">arXiv:2101.06586</a> [<a href="http://arxiv.org/pdf/2101.06586" target="_blank">pdf</a>]

<h2>PLUME: Efficient 3D Object Detection from Stereo Images. (arXiv:2101.06594v2 [cs.CV] UPDATED)</h2>
<h3>Yan Wang, Bin Yang, Rui Hu, Ming Liang, Raquel Urtasun</h3>
<p>3D object detection plays a significant role in various robotic applications
including self-driving. While many approaches rely on expensive 3D sensors like
LiDAR to produce accurate 3D estimates, stereo-based methods have recently
shown promising results at a lower cost. Existing methods tackle the problem in
two steps: first depth estimation is performed, a pseudo LiDAR point cloud
representation is computed from the depth estimates, and then object detection
is performed in 3D space. However, because the two separate tasks are optimized
in different metric spaces, the depth estimation is biased towards nearby
objects and may cause sub-optimal performance of 3D detection. In this paper we
propose a model that unifies these two tasks in the same metric space.
Specifically, our model directly constructs a pseudo LiDAR feature volume
(PLUME) in 3D space, which is used to solve both occupancy estimation and
object detection tasks. Our approach achieves state-of-the-art performance on
the challenging KITTI benchmark, with significantly reduced inference time
compared with existing methods.
</p>
<a href="http://arxiv.org/abs/2101.06594" target="_blank">arXiv:2101.06594</a> [<a href="http://arxiv.org/pdf/2101.06594" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filtering and Smoothing Framework for Inertial-Integrated Navigation. (arXiv:2102.12897v7 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Chi Guo, Shengyong You, Jianlang Hu, Jingnan Liu</h3>
<p>This paper proposes an $SE_2(3)$ based extended Kalman filtering (EKF)
framework for the inertial-integrated state estimation problem. The error
representation using the straight difference of two vectors in the inertial
navigation system may not be reasonable as it does not take the direction
difference into consideration.

Therefore, we choose to use the $SE_2(3)$ matrix Lie group to represent the
state of the inertial-integrated navigation system which consequently leads to
the common frame error representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down (NED) navigation frame and the earth-centered earth-fixed
(ECEF) frame, respectively, the corresponding EKF, terms as $SE_2(3)$ based EKF
has also been derived. It provides a new perspective on the geometric EKF with
a more sophisticated formula for the inertial-integrated navigation system.
Furthermore, we design two new modified error dynamics on the NED frame and the
ECEF frame respectively by introducing new auxiliary vectors. Finally the
equivalence of the left-invariant EKF and left $SE_2(3)$ based EKF have been
shown in navigation frame and ECEF frame.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>EKMP: Generalized Imitation Learning with Adaptation, Nonlinear Hard Constraints and Obstacle Avoidance. (arXiv:2103.00452v2 [cs.RO] UPDATED)</h2>
<h3>Yanlong Huang</h3>
<p>As a user-friendly and straightforward solution for robot trajectory
generation, imitation learning has been viewed as a vital direction in the
context of robot skill learning. In contrast to unconstrained imitation
learning which ignores possible internal and external constraints arising from
environments and robot kinematics/dynamics, recent works on constrained
imitation learning allow for transferring human skills to unstructured
scenarios, further enlarging the application domain of imitation learning.
While various constraints have been studied, e.g., joint limits, obstacle
avoidance and plane constraints, the problem of nonlinear hard constraints has
not been well-addressed. In this paper, we propose extended kernelized movement
primitives (EKMP) to cope with most of the key problems in imitation learning,
including nonlinear hard constraints. Specifically, EKMP is capable of learning
the probabilistic features of multiple demonstrations, adapting the learned
skills towards arbitrary desired points in terms of joint position and
velocity, avoiding obstacles at the level of robot links, as well as satisfying
arbitrary linear and nonlinear, equality and inequality hard constraints.
Besides, the connections between EKMP and state-of-the-art motion planning
approaches are discussed. Several evaluations including the planning of joint
trajectories for a 7-DoF robotic arm are provided to verify the effectiveness
of our framework.
</p>
<a href="http://arxiv.org/abs/2103.00452" target="_blank">arXiv:2103.00452</a> [<a href="http://arxiv.org/pdf/2103.00452" target="_blank">pdf</a>]

<h2>AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation. (arXiv:2103.05863v2 [cs.CV] UPDATED)</h2>
<h3>Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, Sotaro Tsukizawa</h3>
<p>AutoAugment has sparked an interest in automated augmentation methods for
deep learning models. These methods estimate image transformation policies for
train data that improve generalization to test data. While recent papers
evolved in the direction of decreasing policy search complexity, we show that
those methods are not robust when applied to biased and noisy data. To overcome
these limitations, we reformulate AutoAugment as a generalized automated
dataset optimization (AutoDO) task that minimizes the distribution shift
between test data and distorted train dataset. In our AutoDO model, we
explicitly estimate a set of per-point hyperparameters to flexibly change
distribution of train data. In particular, we include hyperparameters for
augmentation, loss weights, and soft-labels that are jointly estimated using
implicit differentiation. We develop a theoretical probabilistic interpretation
of this framework using Fisher information and show that its complexity scales
linearly with the dataset size. Our experiments on SVHN, CIFAR-10/100, and
ImageNet classification show up to 9.3% improvement for biased datasets with
label noise compared to prior methods and, importantly, up to 36.6% gain for
underrepresented SVHN classes.
</p>
<a href="http://arxiv.org/abs/2103.05863" target="_blank">arXiv:2103.05863</a> [<a href="http://arxiv.org/pdf/2103.05863" target="_blank">pdf</a>]

<h2>Duplex Contextual Relation Network for Polyp Segmentation. (arXiv:2103.06725v2 [cs.CV] UPDATED)</h2>
<h3>Zijin Yin, Kongming Liang, Zhanyu Ma, Jun Guo</h3>
<p>Polyp segmentation is of great importance in the early diagnosis and
treatment of colorectal cancer. Since polyps vary in their shape, size, color,
and texture, accurate polyp segmentation is very challenging. One promising way
to mitigate the diversity of polyps is to model the contextual relation for
each pixel such as using attention mechanism. However, previous methods only
focus on learning the dependencies between the position within an individual
image and ignore the contextual relation across different images. In this
paper, we propose Duplex Contextual Relation Network (DCRNet) to capture both
within-image and cross-image contextual relations. Specifically, we first
design Interior Contextual-Relation Module to estimate the similarity between
each position and all the positions within the same image. Then Exterior
Contextual-Relation Module is incorporated to estimate the similarity between
each position and the positions across different images. Based on the above two
types of similarity, the feature at one position can be further enhanced by the
contextual region embedding within and across images. To store the
characteristic region embedding from all the images, a memory bank is designed
and operates as a queue. Therefore, the proposed method can relate similar
features even though they come from different images. We evaluate the proposed
method on the EndoScene, Kvasir-SEG and the recently released large-scale
PICCOLO dataset. Experimental results show that the proposed DCRNet outperforms
the state-of-the-art methods in terms of the widely-used evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2103.06725" target="_blank">arXiv:2103.06725</a> [<a href="http://arxiv.org/pdf/2103.06725" target="_blank">pdf</a>]

