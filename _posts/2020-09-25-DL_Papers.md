---
title: Latest Deep Learning Papers
date: 2020-11-24 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (223 Articles)</h1>
<h2>Explainable Multivariate Time Series Classification: A Deep Neural Network Which Learns To Attend To Important Variables As Well As Informative Time Intervals. (arXiv:2011.11631v1 [cs.LG])</h2>
<h3>Tsung-Yu Hsieh, Suhang Wang, Yiwei Sun, Vasant Honavar</h3>
<p>Time series data is prevalent in a wide variety of real-world applications
and it calls for trustworthy and explainable models for people to understand
and fully trust decisions made by AI solutions. We consider the problem of
building explainable classifiers from multi-variate time series data. A key
criterion to understand such predictive models involves elucidating and
quantifying the contribution of time varying input variables to the
classification. Hence, we introduce a novel, modular, convolution-based feature
extraction and attention mechanism that simultaneously identifies the variables
as well as time intervals which determine the classifier output. We present
results of extensive experiments with several benchmark data sets that show
that the proposed method outperforms the state-of-the-art baseline methods on
multi-variate time series classification task. The results of our case studies
demonstrate that the variables and time intervals identified by the proposed
method make sense relative to available domain knowledge.
</p>
<a href="http://arxiv.org/abs/2011.11631" target="_blank">arXiv:2011.11631</a> [<a href="http://arxiv.org/pdf/2011.11631" target="_blank">pdf</a>]

<h2>Differentially Private Learning Needs Better Features (or Much More Data). (arXiv:2011.11660v1 [cs.LG])</h2>
<h3>Florian Tram&#xe8;r, Dan Boneh</h3>
<p>We demonstrate that differentially private machine learning has not yet
reached its "AlexNet moment" on many canonical vision tasks: linear models
trained on handcrafted features significantly outperform end-to-end deep neural
networks for moderate privacy budgets. To exceed the performance of handcrafted
features, we show that private learning requires either much more private data,
or access to features learned on public data from a similar domain. Our work
introduces simple yet strong baselines for differentially private learning that
can inform the evaluation of future progress in this area.
</p>
<a href="http://arxiv.org/abs/2011.11660" target="_blank">arXiv:2011.11660</a> [<a href="http://arxiv.org/pdf/2011.11660" target="_blank">pdf</a>]

<h2>Low-Resolution Face Recognition In Resource-Constrained Environments. (arXiv:2011.11674v1 [cs.CV])</h2>
<h3>Mozhdeh Rouhsedaghat, Yifan Wang, Shuowen Hu, Suya You, C.-C. Jay Kuo</h3>
<p>A non-parametric low-resolution face recognition model for
resource-constrained environments with limited networking and computing is
proposed in this work. Such environments often demand a small model capable of
being effectively trained on a small number of labeled data samples, with low
training complexity, and low-resolution input images. To address these
challenges, we adopt an emerging explainable machine learning methodology
called successive subspace learning (SSL).SSL offers an explainable
non-parametric model that flexibly trades the model size for verification
performance. Its training complexity is significantly lower since its model is
trained in a one-pass feedforward manner without backpropagation. Furthermore,
active learning can be conveniently incorporated to reduce the labeling cost.
The effectiveness of the proposed model is demonstrated by experiments on the
LFW and the CMU Multi-PIE datasets.
</p>
<a href="http://arxiv.org/abs/2011.11674" target="_blank">arXiv:2011.11674</a> [<a href="http://arxiv.org/pdf/2011.11674" target="_blank">pdf</a>]

<h2>Scaling Wide Residual Networks for Panoptic Segmentation. (arXiv:2011.11675v1 [cs.CV])</h2>
<h3>Liang-Chieh Chen, Huiyu Wang, Siyuan Qiao</h3>
<p>The Wide Residual Networks (Wide-ResNets), a shallow but wide model variant
of the Residual Networks (ResNets) by stacking a small number of residual
blocks with large channel sizes, have demonstrated outstanding performance on
multiple dense prediction tasks. However, since proposed, the Wide-ResNet
architecture has barely evolved over the years. In this work, we revisit its
architecture design for the recent challenging panoptic segmentation task,
which aims to unify semantic segmentation and instance segmentation. A baseline
model is obtained by incorporating the simple and effective
Squeeze-and-Excitation and Switchable Atrous Convolution to the Wide-ResNets.
Its network capacity is further scaled up or down by adjusting the width (i.e.,
channel size) and depth (i.e., number of layers), resulting in a family of
SWideRNets (short for Scaling Wide Residual Networks). We demonstrate that such
a simple scaling scheme, coupled with grid search, identifies several
SWideRNets that significantly advance state-of-the-art performance on panoptic
segmentation datasets in both the fast model regime and strong model regime.
</p>
<a href="http://arxiv.org/abs/2011.11675" target="_blank">arXiv:2011.11675</a> [<a href="http://arxiv.org/pdf/2011.11675" target="_blank">pdf</a>]

<h2>Ensemble- and Distance-Based Feature Ranking for Unsupervised Learning. (arXiv:2011.11679v1 [cs.LG])</h2>
<h3>Matej Petkovi&#x107;, Dragi Kocev, Bla&#x17e; &#x160;krlj, Sa&#x161;o D&#x17e;eroski</h3>
<p>In this work, we propose two novel (groups of) methods for unsupervised
feature ranking and selection. The first group includes feature ranking scores
(Genie3 score, RandomForest score) that are computed from ensembles of
predictive clustering trees. The second method is URelief, the unsupervised
extension of the Relief family of feature ranking algorithms. Using 26
benchmark data sets and 5 baselines, we show that both the Genie3 score
(computed from the ensemble of extra trees) and the URelief method outperform
the existing methods and that Genie3 performs best overall, in terms of
predictive power of the top-ranked features. Additionally, we analyze the
influence of the hyper-parameters of the proposed methods on their performance,
and show that for the Genie3 score the highest quality is achieved by the most
efficient parameter configuration. Finally, we propose a way of discovering the
location of the features in the ranking, which are the most relevant in
reality.
</p>
<a href="http://arxiv.org/abs/2011.11679" target="_blank">arXiv:2011.11679</a> [<a href="http://arxiv.org/pdf/2011.11679" target="_blank">pdf</a>]

<h2>Efficient Construction of Nonlinear Models overNormalized Data. (arXiv:2011.11682v1 [cs.LG])</h2>
<h3>Zhaoyue Chen, Nick Koudas, Zhe Zhang, Xiaohui Yu</h3>
<p>Machine Learning (ML) applications are proliferating in the enterprise.
Relational data which are prevalent in enterprise applications are typically
normalized; as a result, data has to be denormalized via primary/foreign-key
joins to be provided as input to ML algorithms. In this paper, we study the
implementation of popular nonlinear ML models, Gaussian Mixture Models (GMM)
and Neural Networks (NN), over normalized data addressing both cases of binary
and multi-way joins over normalized relations.

For the case of GMM, we show how it is possible to decompose computation in a
systematic way both for binary joins and for multi-way joins to construct
mixture models. We demonstrate that by factoring the computation, one can
conduct the training of the models much faster compared to other applicable
approaches, without any loss in accuracy.

For the case of NN, we propose algorithms to train the network taking
normalized data as the input. Similarly, we present algorithms that can conduct
the training of the network in a factorized way and offer performance
advantages. The redundancy introduced by denormalization can be exploited for
certain types of activation functions. However, we demonstrate that attempting
to explore this redundancy is helpful up to a certain point; exploring
redundancy at higher layers of the network will always result in increased
costs and is not recommended.

We present the results of a thorough experimental evaluation, varying several
parameters of the input relations involved and demonstrate that our proposals
for the training of GMM and NN yield drastic performance improvements typically
starting at 100%, which become increasingly higher as parameters of the
underlying data vary, without any loss in accuracy.
</p>
<a href="http://arxiv.org/abs/2011.11682" target="_blank">arXiv:2011.11682</a> [<a href="http://arxiv.org/pdf/2011.11682" target="_blank">pdf</a>]

<h2>Sequential Topological Representations for Predictive Models of Deformable Objects. (arXiv:2011.11693v1 [cs.RO])</h2>
<h3>Rika Antonova, Anastasiia Varava, Peiyang Shi, J. Frederico Carvalho, Danica Kragic</h3>
<p>Deformable objects present a formidable challenge for robotic manipulation
due to the lack of canonical low-dimensional representations and the difficulty
of capturing, predicting, and controlling such objects. We construct compact
topological representations to capture the state of highly deformable objects
that are topologically nontrivial. We develop an approach that tracks the
evolution of this topological state through time. Under several mild
assumptions, we prove that the topology of the scene and its evolution can be
recovered from point clouds representing the scene. Our further contribution is
a method to learn predictive models that take a sequence of past point cloud
observations as input and predict a sequence of topological states, conditioned
on target/future control actions. Our experiments with highly deformable
objects in simulation show that the proposed multistep predictive models yield
more precise results than those obtained from computational topology libraries.
These models can leverage patterns inferred across various objects and offer
fast multistep predictions suitable for real-time applications.
</p>
<a href="http://arxiv.org/abs/2011.11693" target="_blank">arXiv:2011.11693</a> [<a href="http://arxiv.org/pdf/2011.11693" target="_blank">pdf</a>]

<h2>Mechanical Search on Shelves using Lateral Access X-RAY. (arXiv:2011.11696v1 [cs.RO])</h2>
<h3>Huang Huang, Marcus Dominguez-Kuhne, Jeffrey Ichnowski, Vishal Satish, Michael Danielczuk, Kate Sanders, Andrew Lee, Anelia Angelova, Vincent Vanhoucke, Ken Goldberg</h3>
<p>Efficiently finding an occluded object with lateral access arises in many
contexts such as warehouses, retail, healthcare, shipping, and homes. We
introduce LAX-RAY (Lateral Access maXimal Reduction of occupancY support Area),
a system to automate the mechanical search for occluded objects on shelves. For
such lateral access environments, LAX-RAY couples a perception pipeline
predicting a target object occupancy support distribution with a mechanical
search policy that sequentially selects occluding objects to push to the side
to reveal the target as efficiently as possible. Within the context of extruded
polygonal objects and a stationary target with a known aspect ratio, we explore
three lateral access search policies: Distribution Area Reduction (DAR),
Distribution Entropy Reduction (DER), and Distribution Entropy Reduction over
Multiple Time Steps (DER-MT) utilizing the support distribution and prior
information. We evaluate these policies using the First-Order Shelf Simulator
(FOSS) in which we simulate 800 random shelf environments of varying
difficulty, and in a physical shelf environment with a Fetch robot and an
embedded PrimeSense RGBD Camera. Average simulation results of 87.3% success
rate demonstrate better performance of DER-MT with 2 prediction steps. When
deployed on the robot, results show a success rate of at least 80% for all
policies, suggesting that LAX-RAY can efficiently reveal the target object in
reality. Both results show significantly better performance of the three
proposed policies compared to a baseline policy with uniform probability
distribution assumption in non-trivial cases, showing the importance of
distribution prediction. Code, videos, and supplementary material can be found
at https://sites.google.com/berkeley.edu/lax-ray.
</p>
<a href="http://arxiv.org/abs/2011.11696" target="_blank">arXiv:2011.11696</a> [<a href="http://arxiv.org/pdf/2011.11696" target="_blank">pdf</a>]

<h2>A Use of Even Activation Functions in Neural Networks. (arXiv:2011.11713v1 [cs.LG])</h2>
<h3>Fuchang Gao, Boyu Zhang</h3>
<p>Despite broad interest in applying deep learning techniques to scientific
discovery, learning interpretable formulas that accurately describe scientific
data is very challenging because of the vast landscape of possible functions
and the "black box" nature of deep neural networks. The key to success is to
effectively integrate existing knowledge or hypotheses about the underlying
structure of the data into the architecture of deep learning models to guide
machine learning. Currently, such integration is commonly done through
customization of the loss functions. Here we propose an alternative approach to
integrate existing knowledge or hypotheses of data structure by constructing
custom activation functions that reflect this structure. Specifically, we study
a common case when the multivariate target function $f$ to be learned from the
data is partially exchangeable, \emph{i.e.} $f(u,v,w)=f(v,u,w)$ for $u,v\in
\mathbb{R}^d$. For instance, these conditions are satisfied for the
classification of images that is invariant under left-right flipping. Through
theoretical proof and experimental verification, we show that using an even
activation function in one of the fully connected layers improves neural
network performance. In our experimental 9-dimensional regression problems,
replacing one of the non-symmetric activation functions with the designated
"Seagull" activation function $\log(1+x^2)$ results in substantial improvement
in network performance. Surprisingly, even activation functions are seldom used
in neural networks. Our results suggest that customized activation functions
have great potential in neural networks.
</p>
<a href="http://arxiv.org/abs/2011.11713" target="_blank">arXiv:2011.11713</a> [<a href="http://arxiv.org/pdf/2011.11713" target="_blank">pdf</a>]

<h2>Siamese Tracking with Lingual Object Constraints. (arXiv:2011.11721v1 [cs.CV])</h2>
<h3>Maximilian Filtenborg, Efstratios Gavves, Deepak Gupta</h3>
<p>Classically, visual object tracking involves following a target object
throughout a given video, and it provides us the motion trajectory of the
object. However, for many practical applications, this output is often
insufficient since additional semantic information is required to act on the
video material. Example applications of this are surveillance and
target-specific video summarization, where the target needs to be monitored
with respect to certain predefined constraints, e.g., 'when standing near a
yellow car'. This paper explores, tracking visual objects subjected to
additional lingual constraints. Differently from Li et al., we impose
additional lingual constraints upon tracking, which enables new applications of
tracking. Whereas in their work the goal is to improve and extend upon tracking
itself. To perform benchmarks and experiments, we contribute two datasets:
c-MOT16 and c-LaSOT, curated through appending additional constraints to the
frames of the original LaSOT and MOT16 datasets. We also experiment with two
deep models SiamCT-DFG and SiamCT-CA, obtained through extending a recent
state-of-the-art Siamese tracking method and adding modules inspired from the
fields of natural language processing and visual question answering. Through
experimental results, we show that the proposed model SiamCT-CA can
significantly outperform its counterparts. Furthermore, our method enables the
selective compression of videos, based on the validity of the constraint.
</p>
<a href="http://arxiv.org/abs/2011.11721" target="_blank">arXiv:2011.11721</a> [<a href="http://arxiv.org/pdf/2011.11721" target="_blank">pdf</a>]

<h2>From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion. (arXiv:2011.11722v1 [cs.RO])</h2>
<h3>Deepali Jain, Atil Iscen, Ken Caluwaerts</h3>
<p>Legged robots navigating crowded scenes and complex terrains in the real
world are required to execute dynamic leg movements while processing visual
input for obstacle avoidance and path planning. We show that a quadruped robot
can acquire both of these skills by means of hierarchical reinforcement
learning (HRL). By virtue of their hierarchical structure, our policies learn
to implicitly break down this joint problem by concurrently learning High Level
(HL) and Low Level (LL) neural network policies. These two levels are connected
by a low dimensional hidden layer, which we call latent command. HL receives a
first-person camera view, whereas LL receives the latent command from HL and
the robot's on-board sensors to control its actuators. We train policies to
walk in two different environments: a curved cliff and a maze. We show that
hierarchical policies can concurrently learn to locomote and navigate in these
environments, and show they are more efficient than non-hierarchical neural
network policies. This architecture also allows for knowledge reuse across
tasks. LL networks trained on one task can be transferred to a new task in a
new environment. Finally HL, which processes camera images, can be evaluated at
much lower and varying frequencies compared to LL, thus reducing computation
times and bandwidth requirements.
</p>
<a href="http://arxiv.org/abs/2011.11722" target="_blank">arXiv:2011.11722</a> [<a href="http://arxiv.org/pdf/2011.11722" target="_blank">pdf</a>]

<h2>Rotation-Only Bundle Adjustment. (arXiv:2011.11724v1 [cs.CV])</h2>
<h3>Seong Hun Lee, Javier Civera</h3>
<p>We propose a novel method for estimating the global rotations of the cameras
independently of their positions and the scene structure. When two calibrated
cameras observe five or more of the same points, their relative rotation can be
recovered independently of the translation. We extend this idea to multiple
views, thereby decoupling the rotation estimation from the translation and
structure estimation. Our approach provides several benefits such as complete
immunity to inaccurate translations and structure, and the accuracy improvement
when used with rotation averaging. We perform extensive evaluations on both
synthetic and real datasets, demonstrating consistent and significant gains in
accuracy when used with the state-of-the-art rotation averaging method.
</p>
<a href="http://arxiv.org/abs/2011.11724" target="_blank">arXiv:2011.11724</a> [<a href="http://arxiv.org/pdf/2011.11724" target="_blank">pdf</a>]

<h2>End-to-End Framework for Efficient Deep Learning Using Metasurfaces Optics. (arXiv:2011.11728v1 [cs.CV])</h2>
<h3>Carlos Mauricio Villegas Burgos, Tianqi Yang, Nick Vamivakas, Yuhao Zhu</h3>
<p>Deep learning using Convolutional Neural Networks (CNNs) has been shown to
significantly out-performed many conventional vision algorithms. Despite
efforts to increase the CNN efficiency both algorithmically and with
specialized hardware, deep learning remains difficult to deploy in
resource-constrained environments. In this paper, we propose an end-to-end
framework to explore optically compute the CNNs in free-space, much like a
computational camera. Compared to existing free-space optics-based approaches
which are limited to processing single-channel (i.e., grayscale) inputs, we
propose the first general approach, based on nanoscale meta-surface optics,
that can process RGB data directly from the natural scenes. Our system achieves
up to an order of magnitude energy saving, simplifies the sensor design, all
the while sacrificing little network accuracy.
</p>
<a href="http://arxiv.org/abs/2011.11728" target="_blank">arXiv:2011.11728</a> [<a href="http://arxiv.org/pdf/2011.11728" target="_blank">pdf</a>]

<h2>RISE-SLAM: A Resource-aware Inverse Schmidt Estimator for SLAM. (arXiv:2011.11730v1 [cs.RO])</h2>
<h3>Tong Ke, Kejian J. Wu, Stergios I. Roumeliotis</h3>
<p>In this paper, we present the RISE-SLAM algorithm for performing
visual-inertial simultaneous localization and mapping (SLAM), while improving
estimation consistency. Specifically, in order to achieve real-time operation,
existing approaches often assume previously-estimated states to be perfectly
known, which leads to inconsistent estimates. Instead, based on the idea of the
Schmidt-Kalman filter, which has processing cost linear in the size of the
state vector but quadratic memory requirements, we derive a new consistent
approximate method in the information domain, which has linear memory
requirements and adjustable (constant to linear) processing cost. In
particular, this method, the resource-aware inverse Schmidt estimator (RISE),
allows trading estimation accuracy for computational efficiency. Furthermore,
and in order to better address the requirements of a SLAM system during an
exploration vs. a relocalization phase, we employ different configurations of
RISE (in terms of the number and order of states updated) to maximize accuracy
while preserving efficiency. Lastly, we evaluate the proposed RISE-SLAM
algorithm on publicly-available datasets and demonstrate its superiority, both
in terms of accuracy and efficiency, as compared to alternative visual-inertial
SLAM systems.
</p>
<a href="http://arxiv.org/abs/2011.11730" target="_blank">arXiv:2011.11730</a> [<a href="http://arxiv.org/pdf/2011.11730" target="_blank">pdf</a>]

<h2>HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms. (arXiv:2011.11731v1 [cs.CV])</h2>
<h3>Mahmoud Afifi, Marcus A. Brubaker, Michael S. Brown</h3>
<p>While generative adversarial networks (GANs) can successfully produce
high-quality images, they can be challenging to control. Simplifying GAN-based
image generation is critical for their adoption in graphic design and artistic
work. This goal has led to significant interest in methods that can intuitively
control the appearance of images generated by GANs. In this paper, we present
HistoGAN, a color histogram-based method for controlling GAN-generated images'
colors. We focus on color histograms as they provide an intuitive way to
describe image color while remaining decoupled from domain-specific semantics.
Specifically, we introduce an effective modification of the recent StyleGAN
architecture to control the colors of GAN-generated images specified by a
target color histogram feature. We then describe how to expand HistoGAN to
recolor real images. For image recoloring, we jointly train an encoder network
along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised
approach trained to encourage the network to keep the original image's content
while changing the colors based on the given target histogram. We show that
this histogram-based approach offers a better way to control GAN-generated and
real images' colors while producing more compelling results compared to
existing alternative strategies.
</p>
<a href="http://arxiv.org/abs/2011.11731" target="_blank">arXiv:2011.11731</a> [<a href="http://arxiv.org/pdf/2011.11731" target="_blank">pdf</a>]

<h2>Learnable Gabor modulated complex-valued networks for orientation robustness. (arXiv:2011.11734v1 [cs.CV])</h2>
<h3>Felix Richards, Adeline Paiement, Xianghua Xie, Pierre-Alain Duc</h3>
<p>Robustness to transformation is desirable in many computer vision tasks,
given that input data often exhibits pose variance within classes. While
translation invariance and equivariance is a documented phenomenon of CNNs,
sensitivity to other transformations is typically encouraged through data
augmentation. We investigate the modulation of complex valued convolutional
weights with learned Gabor filters to enable orientation robustness. With Gabor
modulation, the designed network is able to generate orientation dependent
features free of interpolation with a single set of rotation-governing
parameters. Moreover, by learning rotation parameters alongside traditional
convolutional weights, the representation space is not constrained and may
adapt to the exact input transformation. We present Learnable Convolutional
Gabor Networks (LCGNs), that are parameter-efficient and offer increased model
complexity while keeping backpropagation simple. We demonstrate that learned
Gabor modulation utilising an end-to-end complex architecture enables rotation
invariance and equivariance on MNIST and a new dataset of simulated images of
galactic cirri.
</p>
<a href="http://arxiv.org/abs/2011.11734" target="_blank">arXiv:2011.11734</a> [<a href="http://arxiv.org/pdf/2011.11734" target="_blank">pdf</a>]

<h2>Large Scale Multimodal Classification Using an Ensemble of Transformer Models and Co-Attention. (arXiv:2011.11735v1 [cs.AI])</h2>
<h3>Varnith Chordia, Vijay Kumar BG</h3>
<p>Accurate and efficient product classification is significant for E-commerce
applications, as it enables various downstream tasks such as recommendation,
retrieval, and pricing. Items often contain textual and visual information, and
utilizing both modalities usually outperforms classification utilizing either
mode alone. In this paper we describe our methodology and results for the SIGIR
eCom Rakuten Data Challenge. We employ a dual attention technique to model
image-text relationships using pretrained language and image embeddings. While
dual attention has been widely used for Visual Question Answering(VQA) tasks,
ours is the first attempt to apply the concept for multimodal classification.
</p>
<a href="http://arxiv.org/abs/2011.11735" target="_blank">arXiv:2011.11735</a> [<a href="http://arxiv.org/pdf/2011.11735" target="_blank">pdf</a>]

<h2>Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets. (arXiv:2011.11740v1 [cs.LG])</h2>
<h3>Charilaos Mylonas, Eleni Chatzi</h3>
<p>In this work, a novel approach for the construction and training of time
series models is presented that deals with the problem of learning on large
time series with non-equispaced observations, which at the same time may
possess features of interest that span multiple scales. The proposed method is
appropriate for constructing predictive models for non-stationary stochastic
time series.The efficacy of the method is demonstrated on a simulated
stochastic degradation dataset and on a real-world accelerated life testing
dataset for ball-bearings. The proposed method, which is based on GraphNets,
implicitly learns a model that describes the evolution of the system at the
level of a state-vector rather than of a raw observation. The proposed approach
is compared to a recurrent network with a temporal convolutional feature
extractor head (RNN-tCNN) which forms a known viable alternative for the
problem context considered. Finally, by taking advantage of recent advances in
the computation of reparametrization gradients for learning probability
distributions, a simple yet effective technique for representing prediction
uncertainty as a Gamma distribution over remaining useful life predictions is
employed.
</p>
<a href="http://arxiv.org/abs/2011.11740" target="_blank">arXiv:2011.11740</a> [<a href="http://arxiv.org/pdf/2011.11740" target="_blank">pdf</a>]

<h2>Learnable and Instance-Robust Predictions for Online Matching, Flows and Load Balancing. (arXiv:2011.11743v1 [cs.LG])</h2>
<h3>Thomas Lavastida, Benjamin Moseley, R. Ravi, Chenyang Xu</h3>
<p>This paper proposes a new model for augmenting algorithms with useful
predictions that go beyond worst-case bounds on the algorithm performance. By
refining existing models, our model ensures predictions are formally learnable
and instance robust. Learnability guarantees that predictions can be
efficiently constructed from past data. Instance robustness formally ensures a
prediction is robust to modest changes in the problem input. Further, the
robustness model ensures two different predictions can be objectively compared,
addressing a shortcoming in prior models. This paper establishes the existence
of predictions which satisfy these properties. The paper considers online
algorithms with predictions for a network flow allocation problem and the
restricted assignment makespan minimization problem. For both problems, three
key properties are established: existence of useful predictions that give near
optimal solutions, robustness of these predictions to errors that smoothly
degrade as the underlying problem instance changes, and we prove high quality
predictions can be learned from a small sample of prior instances.
</p>
<a href="http://arxiv.org/abs/2011.11743" target="_blank">arXiv:2011.11743</a> [<a href="http://arxiv.org/pdf/2011.11743" target="_blank">pdf</a>]

<h2>Path Design and Resource Management for NOMA enhanced Indoor Intelligent Robots. (arXiv:2011.11745v1 [cs.AI])</h2>
<h3>Ruikang Zhong, Xiao Liu, Yuanwei Liu, Yue Chen, Xianbin Wang</h3>
<p>A communication enabled indoor intelligent robots (IRs) service framework is
proposed, where non-orthogonal multiple access (NOMA) technique is adopted to
enable highly reliable communications. In cooperation with the ultramodern
indoor channel model recently proposed by the International Telecommunication
Union (ITU), the Lego modeling method is proposed, which can deterministically
describe the indoor layout and channel state in order to construct the radio
map. The investigated radio map is invoked as a virtual environment to train
the reinforcement learning agent, which can save training time and hardware
costs. Build on the proposed communication model, motions of IRs who need to
reach designated mission destinations and their corresponding down-link power
allocation policy are jointly optimized to maximize the mission efficiency and
communication reliability of IRs. In an effort to solve this optimization
problem, a novel reinforcement learning approach named deep transfer
deterministic policy gradient (DT-DPG) algorithm is proposed. Our simulation
results demonstrate that 1) With the aid of NOMA techniques, the communication
reliability of IRs is effectively improved; 2) The radio map is qualified to be
a virtual training environment, and its statistical channel state information
improves training efficiency by about 30%; 3) The proposed DT-DPG algorithm is
superior to the conventional deep deterministic policy gradient (DDPG)
algorithm in terms of optimization performance, training time, and anti-local
optimum ability.
</p>
<a href="http://arxiv.org/abs/2011.11745" target="_blank">arXiv:2011.11745</a> [<a href="http://arxiv.org/pdf/2011.11745" target="_blank">pdf</a>]

<h2>Multimodal dynamics modeling for off-road autonomous vehicles. (arXiv:2011.11751v1 [cs.RO])</h2>
<h3>Jean-Fran&#xe7;ois Tremblay, Travis Manderson, Aur&#xe9;lio Noca, Gregory Dudek, David Meger</h3>
<p>Dynamics modeling in outdoor and unstructured environments is difficult
because different elements in the environment interact with the robot in ways
that can be hard to predict. Leveraging multiple sensors to perceive maximal
information about the robot's environment is thus crucial when building a model
to perform predictions about the robot's dynamics with the goal of doing motion
planning. We design a model capable of long-horizon motion predictions,
leveraging vision, lidar and proprioception, which is robust to arbitrarily
missing modalities at test time. We demonstrate in simulation that our model is
able to leverage vision to predict traction changes. We then test our model
using a real-world challenging dataset of a robot navigating through a forest,
performing predictions in trajectories unseen during training. We try different
modality combinations at test time and show that, while our model performs best
when all modalities are present, it is still able to perform better than the
baseline even when receiving only raw vision input and no proprioception, as
well as when only receiving proprioception. Overall, our study demonstrates the
importance of leveraging multiple sensors when doing dynamics modeling in
outdoor conditions.
</p>
<a href="http://arxiv.org/abs/2011.11751" target="_blank">arXiv:2011.11751</a> [<a href="http://arxiv.org/pdf/2011.11751" target="_blank">pdf</a>]

<h2>Learning Translation Invariance in CNNs. (arXiv:2011.11757v1 [cs.CV])</h2>
<h3>Valerio Biscione, Jeffrey Bowers</h3>
<p>When seeing a new object, humans can immediately recognize it across
different retinal locations: we say that the internal object representation is
invariant to translation. It is commonly believed that Convolutional Neural
Networks (CNNs) are architecturally invariant to translation thanks to the
convolution and/or pooling operations they are endowed with. In fact, several
works have found that these networks systematically fail to recognise new
objects on untrained locations. In this work we show how, even though CNNs are
not 'architecturally invariant' to translation, they can indeed 'learn' to be
invariant to translation. We verified that this can be achieved by pretraining
on ImageNet, and we found that it is also possible with much simpler datasets
in which the items are fully translated across the input canvas. We
investigated how this pretraining affected the internal network
representations, finding that the invariance was almost always acquired, even
though it was some times disrupted by further training due to catastrophic
forgetting/interference. These experiments show how pretraining a network on an
environment with the right 'latent' characteristics (a more naturalistic
environment) can result in the network learning deep perceptual rules which
would dramatically improve subsequent generalization.
</p>
<a href="http://arxiv.org/abs/2011.11757" target="_blank">arXiv:2011.11757</a> [<a href="http://arxiv.org/pdf/2011.11757" target="_blank">pdf</a>]

<h2>Multi-regime analysis for computer vision-based traffic surveillance using a change-point detection algorithm. (arXiv:2011.11758v1 [cs.CV])</h2>
<h3>Seungyun Jeong, Keemin Sohn</h3>
<p>As a result of significant advances in deep learning, computer vision
technology has been widely adopted in the field of traffic surveillance.
Nonetheless, it is difficult to find a universal model that can measure traffic
parameters irrespective of ambient conditions such as times of the day,
weather, or shadows. These conditions vary recurrently, but the exact points of
change are inconsistent and unpredictable. Thus, the application of a
multi-regime method would be problematic, even when separate sets of model
parameters are prepared in advance. In the present study we devised a robust
approach that facilitates multi-regime analysis. This approach employs an
online parametric algorithm to determine the change-points for ambient
conditions. An autoencoder was used to reduce the dimensions of input images,
and reduced feature vectors were used to implement the online change-point
algorithm. Seven separate periods were tagged with typical times in a given
day. Multi-regime analysis was then performed so that the traffic density could
be separately measured for each period. To train and test models for vehicle
counting, 1,100 video images were randomly chosen for each period and labeled
with traffic counts. The measurement accuracy of multi-regime analysis was much
higher than that of an integrated model trained on all data.
</p>
<a href="http://arxiv.org/abs/2011.11758" target="_blank">arXiv:2011.11758</a> [<a href="http://arxiv.org/pdf/2011.11758" target="_blank">pdf</a>]

<h2>Multimodal Pretraining for Dense Video Captioning. (arXiv:2011.11760v1 [cs.CV])</h2>
<h3>Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, Radu Soricut</h3>
<p>Learning specific hands-on skills such as cooking, car maintenance, and home
repairs increasingly happens via instructional videos. The user experience with
such videos is known to be improved by meta-information such as time-stamped
annotations for the main steps involved. Generating such annotations
automatically is challenging, and we describe here two relevant contributions.
First, we construct and release a new dense video captioning dataset, Video
Timeline Tags (ViTT), featuring a variety of instructional videos together with
time-stamped annotations. Second, we explore several multimodal
sequence-to-sequence pretraining strategies that leverage large unsupervised
datasets of videos and caption-like texts. We pretrain and subsequently
finetune dense video captioning models using both YouCook2 and ViTT. We show
that such models generalize well and are robust over a wide variety of
instructional videos.
</p>
<a href="http://arxiv.org/abs/2011.11760" target="_blank">arXiv:2011.11760</a> [<a href="http://arxiv.org/pdf/2011.11760" target="_blank">pdf</a>]

<h2>A robust solution of a statistical inverse problem in multiscale computational mechanics using an artificial neural network. (arXiv:2011.11761v1 [cs.LG])</h2>
<h3>Florent Pled (MSME), Christophe Desceliers (MSME), Tianyu Zhang (MSME)</h3>
<p>This work addresses the inverse identification of apparent elastic properties
of random heterogeneous materials using machine learning based on artificial
neural networks. The proposed neural network-based identification method
requires the construction of a database from which an artificial neural network
can be trained to learn the nonlinear relationship between the hyperparameters
of a prior stochastic model of the random compliance field and some relevant
quantities of interest of an ad hoc multiscale computational model. An initial
database made up with input and target data is first generated from the
computational model, from which a processed database is deduced by conditioning
the input data with respect to the target data using the nonparametric
statistics. Two-and three-layer feedforward artificial neural networks are then
trained from each of the initial and processed databases to construct an
algebraic representation of the nonlinear mapping between the hyperparameters
(network outputs) and the quantities of interest (network inputs). The
performances of the trained artificial neural networks are analyzed in terms of
mean squared error, linear regression fit and probability distribution between
network outputs and targets for both databases. An ad hoc probabilistic model
of the input random vector is finally proposed in order to take into account
uncertainties on the network input and to perform a robustness analysis of the
network output with respect to the input uncertainties level. The capability of
the proposed neural network-based identification method to efficiently solve
the underlying statistical inverse problem is illustrated through two numerical
examples developed within the framework of 2D plane stress linear elasticity,
namely a first validation example on synthetic data obtained through
computational simulations and a second application example on real experimental
data obtained through a physical experiment monitored by digital image
correlation on a real heterogeneous biological material (beef cortical bone).
</p>
<a href="http://arxiv.org/abs/2011.11761" target="_blank">arXiv:2011.11761</a> [<a href="http://arxiv.org/pdf/2011.11761" target="_blank">pdf</a>]

<h2>Boosting Contrastive Self-Supervised Learning with False Negative Cancellation. (arXiv:2011.11765v1 [cs.CV])</h2>
<h3>Tri Huynh, Simon Kornblith, Matthew R. Walter, Michael Maire, Maryam Khademi</h3>
<p>Self-supervised representation learning has witnessed significant leaps
fueled by recent progress in Contrastive learning, which seeks to learn
transformations that embed positive input pairs nearby, while pushing negative
pairs far apart. While positive pairs can be generated reliably (e.g., as
different views of the same image), it is difficult to accurately establish
negative pairs, defined as samples from different images regardless of their
semantic content or visual features. A fundamental problem in contrastive
learning is mitigating the effects of false negatives. Contrasting false
negatives induces two critical issues in representation learning: discarding
semantic information and slow convergence. In this paper, we study this problem
in detail and propose novel approaches to mitigate the effects of false
negatives. The proposed methods exhibit consistent and significant improvements
over existing contrastive learning-based models. They achieve new
state-of-the-art performance on ImageNet evaluations, achieving 5.8% absolute
improvement in top-1 accuracy over the previous state-of-the-art when
finetuning with 1% labels, as well as transferring to downstream tasks.
</p>
<a href="http://arxiv.org/abs/2011.11765" target="_blank">arXiv:2011.11765</a> [<a href="http://arxiv.org/pdf/2011.11765" target="_blank">pdf</a>]

<h2>KeepAugment: A Simple Information-Preserving Data Augmentation Approach. (arXiv:2011.11778v1 [cs.CV])</h2>
<h3>Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, Qiang Liu</h3>
<p>Data augmentation (DA) is an essential technique for training
state-of-the-art deep learning systems. In this paper, we empirically show data
augmentation might introduce noisy augmented examples and consequently hurt the
performance on unaugmented data during inference. To alleviate this issue, we
propose a simple yet highly effective approach, dubbed \emph{KeepAugment}, to
increase augmented images fidelity. The idea is first to use the saliency map
to detect important regions on the original images and then preserve these
informative regions during augmentation. This information-preserving strategy
allows us to generate more faithful training examples. Empirically, we
demonstrate our method significantly improves on a number of prior art data
augmentation schemes, e.g. AutoAugment, Cutout, random erasing, achieving
promising results on image classification, semi-supervised image
classification, multi-view multi-camera tracking and object detection.
</p>
<a href="http://arxiv.org/abs/2011.11778" target="_blank">arXiv:2011.11778</a> [<a href="http://arxiv.org/pdf/2011.11778" target="_blank">pdf</a>]

<h2>AlphaMatch: Improving Consistency for Semi-supervised Learning with Alpha-divergence. (arXiv:2011.11779v1 [cs.LG])</h2>
<h3>Chengyue Gong, Dilin Wang, Qiang Liu</h3>
<p>Semi-supervised learning (SSL) is a key approach toward more data-efficient
machine learning by jointly leverage both labeled and unlabeled data. We
propose AlphaMatch, an efficient SSL method that leverages data augmentations,
by efficiently enforcing the label consistency between the data points and the
augmented data derived from them. Our key technical contribution lies on: 1)
using alpha-divergence to prioritize the regularization on data with high
confidence, achieving a similar effect as FixMatch but in a more flexible
fashion, and 2) proposing an optimization-based, EM-like algorithm to enforce
the consistency, which enjoys better convergence than iterative regularization
procedures used in recent SSL methods such as FixMatch, UDA, and MixMatch.
AlphaMatch is simple and easy to implement, and consistently outperforms prior
arts on standard benchmarks, e.g. CIFAR-10, SVHN, CIFAR-100, STL-10.
Specifically, we achieve 91.3% test accuracy on CIFAR-10 with just 4 labelled
data per class, substantially improving over the previously best 88.7% accuracy
achieved by FixMatch.
</p>
<a href="http://arxiv.org/abs/2011.11779" target="_blank">arXiv:2011.11779</a> [<a href="http://arxiv.org/pdf/2011.11779" target="_blank">pdf</a>]

<h2>Robust image stitching with multiple registrations. (arXiv:2011.11784v1 [cs.CV])</h2>
<h3>Charles Herrmann, Chen Wang, Richard Strong Bowen, Emil Keyder, Michael Krainin, Ce Liu, Ramin Zabih</h3>
<p>Panorama creation is one of the most widely deployed techniques in computer
vision. In addition to industry applications such as Google Street View, it is
also used by millions of consumers in smartphones and other cameras.
Traditionally, the problem is decomposed into three phases: registration, which
picks a single transformation of each source image to align it to the other
inputs, seam finding, which selects a source image for each pixel in the final
result, and blending, which fixes minor visual artifacts. Here, we observe that
the use of a single registration often leads to errors, especially in scenes
with significant depth variation or object motion. We propose instead the use
of multiple registrations, permitting regions of the image at different depths
to be captured with greater accuracy. MRF inference techniques naturally extend
to seam finding over multiple registrations, and we show here that their energy
functions can be readily modified with new terms that discourage duplication
and tearing, common problems that are exacerbated by the use of multiple
registrations. Our techniques are closely related to layer-based stereo, and
move image stitching closer to explicit scene modeling. Experimental evidence
demonstrates that our techniques often generate significantly better panoramas
when there is substantial motion or parallax.
</p>
<a href="http://arxiv.org/abs/2011.11784" target="_blank">arXiv:2011.11784</a> [<a href="http://arxiv.org/pdf/2011.11784" target="_blank">pdf</a>]

<h2>An analysis of Reinforcement Learning applied to Coach task in IEEE Very Small Size Soccer. (arXiv:2011.11785v1 [cs.RO])</h2>
<h3>Carlos H. C. Pena, Mateus G. Machado, Mariana S. Barros, Jos&#xe9; D. P. Silva, Lucas D. Maciel, Tsang Ing Ren, Edna N. S. Barros, Pedro H. M. Braga, Hansenclever F. Bassani</h3>
<p>The IEEE Very Small Size Soccer (VSSS) is a robot soccer competition in which
two teams of three small robots play against each other. Traditionally, a
deterministic coach agent will choose the most suitable strategy and formation
for each adversary's strategy. Therefore, the role of a coach is of great
importance to the game. In this sense, this paper proposes an end-to-end
approach for the coaching task based on Reinforcement Learning (RL). The
proposed system processes the information during the simulated matches to learn
an optimal policy that chooses the current formation, depending on the opponent
and game conditions. We trained two RL policies against three different teams
(balanced, offensive, and heavily offensive) in a simulated environment. Our
results were assessed against one of the top teams of the VSSS league, showing
promising results after achieving a win/loss ratio of approximately 2.0.
</p>
<a href="http://arxiv.org/abs/2011.11785" target="_blank">arXiv:2011.11785</a> [<a href="http://arxiv.org/pdf/2011.11785" target="_blank">pdf</a>]

<h2>Prior to Segment: Foreground Cues for Novel Objects in Partially Supervised Instance Segmentation. (arXiv:2011.11787v1 [cs.CV])</h2>
<h3>David Biertimpel, Sindi Shkodrani, Anil S. Baslamisli, N&#xf3;ra Baka</h3>
<p>Instance segmentation methods require large datasets with expensive
instance-level mask labels. This makes partially supervised learning appealing
in settings where abundant box and limited mask labels are available. To
improve mask predictions with limited labels, we modify a Mask R-CNN by
introducing an object mask prior (OMP) for the mask head. We show that a
conventional class-agnostic mask head has difficulties learning foreground for
classes with box-supervision only. Our OMP resolves this by providing the mask
head with the general concept of foreground implicitly learned by the box
classification head under the supervision of all classes. This helps the
class-agnostic mask head to focus on the primary object in a region of interest
(RoI) and improves generalization to novel classes. We test our approach on the
COCO dataset using different splits of strongly and weakly supervised classes.
Our approach significantly improves over the Mask R-CNN baseline and obtains
competitive performance with the state-of-the-art, while offering a much
simpler architecture.
</p>
<a href="http://arxiv.org/abs/2011.11787" target="_blank">arXiv:2011.11787</a> [<a href="http://arxiv.org/pdf/2011.11787" target="_blank">pdf</a>]

<h2>Object-centered image stitching. (arXiv:2011.11789v1 [cs.CV])</h2>
<h3>Charles Herrmann, Chen Wang, Richard Strong Bowen, Emil Keyder, Ramin Zabih</h3>
<p>Image stitching is typically decomposed into three phases: registration,
which aligns the source images with a common target image; seam finding, which
determines for each target pixel the source image it should come from; and
blending, which smooths transitions over the seams. As described in [1], the
seam finding phase attempts to place seams between pixels where the transition
between source images is not noticeable. Here, we observe that the most
problematic failures of this approach occur when objects are cropped, omitted,
or duplicated. We therefore take an object-centered approach to the problem,
leveraging recent advances in object detection [2,3,4]. We penalize candidate
solutions with this class of error by modifying the energy function used in the
seam finding stage. This produces substantially more realistic stitching
results on challenging imagery. In addition, these methods can be used to
determine when there is non-recoverable occlusion in the input data, and also
suggest a simple evaluation metric that can be used to evaluate the output of
stitching algorithms.
</p>
<a href="http://arxiv.org/abs/2011.11789" target="_blank">arXiv:2011.11789</a> [<a href="http://arxiv.org/pdf/2011.11789" target="_blank">pdf</a>]

<h2>Who killed Lilly Kane? A case study in applying knowledge graphs to crime fiction. (arXiv:2011.11804v1 [cs.LG])</h2>
<h3>Mariam Alaverdian, William Gilroy, Veronica Kirgios, Xia Li, Carolina Matuk, Daniel Mckenzie, Tachin Ruangkriengsin, Andrea Bertozzi, Jeffrey Brantingham</h3>
<p>We present a preliminary study of a knowledge graph created from season one
of the television show Veronica Mars, which follows the eponymous young private
investigator as she attempts to solve the murder of her best friend Lilly Kane.
We discuss various techniques for mining the knowledge graph for clues and
potential suspects. We also discuss best practice for collaboratively
constructing knowledge graphs from television shows.
</p>
<a href="http://arxiv.org/abs/2011.11804" target="_blank">arXiv:2011.11804</a> [<a href="http://arxiv.org/pdf/2011.11804" target="_blank">pdf</a>]

<h2>The Interpretable Dictionary in Sparse Coding. (arXiv:2011.11805v1 [cs.LG])</h2>
<h3>Edward Kim, Connor Onweller, Andrew O&#x27;Brien, Kathleen McCoy</h3>
<p>Artificial neural networks (ANNs), specifically deep learning networks, have
often been labeled as black boxes due to the fact that the internal
representation of the data is not easily interpretable. In our work, we
illustrate that an ANN, trained using sparse coding under specific sparsity
constraints, yields a more interpretable model than the standard deep learning
model. The dictionary learned by sparse coding can be more easily understood
and the activations of these elements creates a selective feature output. We
compare and contrast our sparse coding model with an equivalent feed forward
convolutional autoencoder trained on the same data. Our results show both
qualitative and quantitative benefits in the interpretation of the learned
sparse coding dictionary as well as the internal activation representations.
</p>
<a href="http://arxiv.org/abs/2011.11805" target="_blank">arXiv:2011.11805</a> [<a href="http://arxiv.org/pdf/2011.11805" target="_blank">pdf</a>]

<h2>MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera. (arXiv:2011.11814v1 [cs.CV])</h2>
<h3>Felix Wimbauer, Nan Yang, Lukas von Stumberg, Niclas Zeller, Daniel Cremers</h3>
<p>In this paper, we propose MonoRec, a semi-supervised monocular dense
reconstruction architecture that predicts depth maps from a single moving
camera in dynamic environments. MonoRec is based on a MVS setting which encodes
the information of multiple consecutive images in a cost volume. To deal with
dynamic objects in the scene, we introduce a MaskModule that predicts moving
object masks by leveraging the photometric inconsistencies encoded in the cost
volumes. Unlike other MVS methods, MonoRec is able to predict accurate depths
for both static and moving objects by leveraging the predicted masks.
Furthermore, we present a novel multi-stage training scheme with a
semi-supervised loss formulation that does not require LiDAR depth values. We
carefully evaluate MonoRec on the KITTI dataset and show that it achieves
state-of-the-art performance compared to both multi-view and single-view
methods. With the model trained on KITTI, we further demonstrate that MonoRec
is able to generalize well to both the Oxford RobotCar dataset and the more
challenging TUM-Mono dataset recorded by a handheld camera. Training code and
pre-trained model will be published soon.
</p>
<a href="http://arxiv.org/abs/2011.11814" target="_blank">arXiv:2011.11814</a> [<a href="http://arxiv.org/pdf/2011.11814" target="_blank">pdf</a>]

<h2>When Machine Learning Meets Privacy: A Survey and Outlook. (arXiv:2011.11819v1 [cs.LG])</h2>
<h3>Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin</h3>
<p>The newly emerged machine learning (e.g. deep learning) methods have become a
strong driving force to revolutionize a wide range of industries, such as smart
healthcare, financial technology, and surveillance systems. Meanwhile, privacy
has emerged as a big concern in this machine learning-based artificial
intelligence era. It is important to note that the problem of privacy
preservation in the context of machine learning is quite different from that in
traditional data privacy protection, as machine learning can act as both friend
and foe. Currently, the work on the preservation of privacy and machine
learning (ML) is still in an infancy stage, as most existing solutions only
focus on privacy problems during the machine learning process. Therefore, a
comprehensive study on the privacy preservation problems and machine learning
is required. This paper surveys the state of the art in privacy issues and
solutions for machine learning. The survey covers three categories of
interactions between privacy and machine learning: (i) private machine
learning, (ii) machine learning aided privacy protection, and (iii) machine
learning-based privacy attack and corresponding protection schemes. The current
research progress in each category is reviewed and the key challenges are
identified. Finally, based on our in-depth analysis of the area of privacy and
machine learning, we point out future research directions in this field.
</p>
<a href="http://arxiv.org/abs/2011.11819" target="_blank">arXiv:2011.11819</a> [<a href="http://arxiv.org/pdf/2011.11819" target="_blank">pdf</a>]

<h2>Delayed Feedback Modeling for the Entire Space Conversion Rate Prediction. (arXiv:2011.11826v1 [cs.LG])</h2>
<h3>Yanshi Wang, Jie Zhang, Qing Da, Anxiang Zeng</h3>
<p>Estimating post-click conversion rate (CVR) accurately is crucial in
E-commerce. However, CVR prediction usually suffers from three major challenges
in practice: i) data sparsity: compared with impressions, conversion samples
are often extremely scarce; ii) sample selection bias: conventional CVR models
are trained with clicked impressions while making inference on the entire space
of all impressions; iii) delayed feedback: many conversions can only be
observed after a relatively long and random delay since clicks happened,
resulting in many false negative labels during training. Previous studies
mainly focus on one or two issues while ignoring the others. In this paper, we
propose a novel neural network framework ESDF to tackle the above three
challenges simultaneously. Unlike existing methods, ESDF models the CVR
prediction from a perspective of entire space, and combines the advantage of
user sequential behavior pattern and the time delay factor. Specifically, ESDF
utilizes sequential behavior of user actions on the entire space with all
impressions to alleviate the sample selection bias problem. By sharing the
embedding parameters between CTR and CVR networks, data sparsity problem is
greatly relieved. Different from conventional delayed feedback methods, ESDF
does not make any special assumption about the delay distribution. We
discretize the delay time by day slot and model the probability based on
survival analysis with deep neural network, which is more practical and
suitable for industrial situations. Extensive experiments are conducted to
evaluate the effectiveness of our method. To the best of our knowledge, ESDF is
the first attempt to unitedly solve the above three challenges in CVR
prediction area.
</p>
<a href="http://arxiv.org/abs/2011.11826" target="_blank">arXiv:2011.11826</a> [<a href="http://arxiv.org/pdf/2011.11826" target="_blank">pdf</a>]

<h2>REPAINT: Knowledge Transfer in Deep Actor-Critic Reinforcement Learning. (arXiv:2011.11827v1 [cs.LG])</h2>
<h3>Yunzhe Tao, Sahika Genc, Tao Sun, Sunil Mallya</h3>
<p>Accelerating the learning processes for complex tasks by leveraging
previously learned tasks has been one of the most challenging problems in
reinforcement learning, especially when the similarity between source and
target tasks is low or unknown. In this work, we propose a
REPresentation-And-INstance Transfer algorithm (REPAINT) for deep actor-critic
reinforcement learning paradigm. In representation transfer, we adopt a
kickstarted training method using a pre-trained teacher policy by introducing
an auxiliary cross-entropy loss. In instance transfer, we develop a sampling
approach, i.e., advantage-based experience replay, on transitions collected
following the teacher policy, where only the samples with high advantage
estimates are retained for policy update. We consider both learning an unseen
target task by transferring from previously learned teacher tasks and learning
a partially unseen task composed of multiple sub-tasks by transferring from a
pre-learned teacher sub-task. In several benchmark experiments, REPAINT
significantly reduces the total training time and improves the asymptotic
performance compared to training with no prior knowledge and other baselines.
</p>
<a href="http://arxiv.org/abs/2011.11827" target="_blank">arXiv:2011.11827</a> [<a href="http://arxiv.org/pdf/2011.11827" target="_blank">pdf</a>]

<h2>RTFN: A Robust Temporal Feature Network for Time Series Classification. (arXiv:2011.11829v1 [cs.LG])</h2>
<h3>Zhiwen Xiao, Xin Xu, Huanlai Xing, Shouxi Luo, Penglin Dai, Dawei Zhan</h3>
<p>Time series data usually contains local and global patterns. Most of the
existing feature networks pay more attention to local features rather than the
relationships among them. The latter is, however, also important yet more
difficult to explore. To obtain sufficient representations by a feature network
is still challenging. To this end, we propose a novel robust temporal feature
network (RTFN) for feature extraction in time series classification, containing
a temporal feature network (TFN) and an LSTM-based attention network (LSTMaN).
TFN is a residual structure with multiple convolutional layers. It functions as
a local-feature extraction network to mine sufficient local features from data.
LSTMaN is composed of two identical layers, where attention and long short-term
memory (LSTM) networks are hybridized. This network acts as a relation
extraction network to discover the intrinsic relationships among the extracted
features at different positions in sequential data. In experiments, we embed
RTFN into a supervised structure as a feature extractor and into an
unsupervised structure as an encoder, respectively. The results show that the
RTFN-based structures achieve excellent supervised and unsupervised performance
on a large number of UCR2018 and UEA2018 datasets.
</p>
<a href="http://arxiv.org/abs/2011.11829" target="_blank">arXiv:2011.11829</a> [<a href="http://arxiv.org/pdf/2011.11829" target="_blank">pdf</a>]

<h2>Dissecting Image Crops. (arXiv:2011.11831v1 [cs.CV])</h2>
<h3>Basile Van Hoorick, Carl Vondrick</h3>
<p>The elementary operation of cropping underpins nearly every computer vision
system, ranging from data augmentation and translation invariance to
computational photography and representation learning. This paper investigates
the subtle traces introduced by this operation. For example, despite
refinements to camera optics, lenses will leave behind certain clues, notably
chromatic aberration and vignetting. Photographers also leave behind other
clues relating to image aesthetics and scene composition. We study how to
detect these traces, and investigate the impact that cropping has on the image
distribution. While our aim is to dissect the fundamental impact of spatial
crops, there are also a number of practical implications to our work, such as
detecting image manipulations and equipping neural network researchers with a
better understanding of shortcut learning. Code is available at
https://github.com/basilevh/dissecting-image-crops.
</p>
<a href="http://arxiv.org/abs/2011.11831" target="_blank">arXiv:2011.11831</a> [<a href="http://arxiv.org/pdf/2011.11831" target="_blank">pdf</a>]

<h2>Comparisons among different stochastic selection of activation layers for convolutional neural networks for healthcare. (arXiv:2011.11834v1 [cs.CV])</h2>
<h3>Loris Nanni, Alessandra Lumini, Stefano Ghidoni, Gianluca Maguolo</h3>
<p>Classification of biological images is an important task with crucial
application in many fields, such as cell phenotypes recognition, detection of
cell organelles and histopathological classification, and it might help in
early medical diagnosis, allowing automatic disease classification without the
need of a human expert. In this paper we classify biomedical images using
ensembles of neural networks. We create this ensemble using a ResNet50
architecture and modifying its activation layers by substituting ReLUs with
other functions. We select our activations among the following ones: ReLU,
leaky ReLU, Parametric ReLU, ELU, Adaptive Piecewice Linear Unit, S-Shaped
ReLU, Swish , Mish, Mexican Linear Unit, Gaussian Linear Unit, Parametric
Deformable Linear Unit, Soft Root Sign (SRS) and others.

As a baseline, we used an ensemble of neural networks that only use ReLU
activations. We tested our networks on several small and medium sized
biomedical image datasets. Our results prove that our best ensemble obtains a
better performance than the ones of the naive approaches. In order to encourage
the reproducibility of this work, the MATLAB code of all the experiments will
be shared at https://github.com/LorisNanni.
</p>
<a href="http://arxiv.org/abs/2011.11834" target="_blank">arXiv:2011.11834</a> [<a href="http://arxiv.org/pdf/2011.11834" target="_blank">pdf</a>]

<h2>Stochastic Motion Planning under Partial Observability for Mobile Robots with Continuous Range Measurements. (arXiv:2011.11836v1 [cs.RO])</h2>
<h3>Ke Sun, Brent Schlotfeldt George Pappas, Vijay Kumar</h3>
<p>In this paper, we address the problem of stochastic motion planning under
partial observability, more specifically, how to navigate a mobile robot
equipped with continuous range sensors such as LIDAR. In contrast to many
existing robotic motion planning methods, we explicitly consider the
uncertainty of the robot state by modeling the system as a POMDP. Recent work
on general purpose POMDP solvers is typically limited to discrete observation
spaces, and does not readily apply to the proposed problem due to the
continuous measurements from LIDAR. In this work, we build upon an existing
Monte Carlo Tree Search method, POMCP, and propose a new algorithm POMCP++. Our
algorithm can handle continuous observation spaces with a novel measurement
selection strategy. The POMCP++ algorithm overcomes over-optimism in the value
estimation of a rollout policy by removing the implicit perfect state
assumption at the rollout phase. We validate POMCP++ in theory by proving it is
a Monte Carlo Tree Search algorithm. Through comparisons with other methods
that can also be applied to the proposed problem, we show that POMCP++ yields
significantly higher success rate and total reward.
</p>
<a href="http://arxiv.org/abs/2011.11836" target="_blank">arXiv:2011.11836</a> [<a href="http://arxiv.org/pdf/2011.11836" target="_blank">pdf</a>]

<h2>Benchmarking Inference Performance of Deep Learning Models on Analog Devices. (arXiv:2011.11840v1 [cs.LG])</h2>
<h3>Omobayode Fagbohungbe, Lijun Qian</h3>
<p>Analog hardware implemented deep learning models are promising for
computation and energy constrained systems such as edge computing devices.
However, the analog nature of the device and the associated many noise sources
will cause changes to the value of the weights in the trained deep learning
models deployed on such devices. In this study, systematic evaluation of the
inference performance of trained popular deep learning models for image
classification deployed on analog devices has been carried out, where additive
white Gaussian noise has been added to the weights of the trained models during
inference. It is observed that deeper models and models with more redundancy in
design such as VGG are more robust to the noise in general. However, the
performance is also affected by the design philosophy of the model, the
detailed structure of the model, the exact machine learning task, as well as
the datasets.
</p>
<a href="http://arxiv.org/abs/2011.11840" target="_blank">arXiv:2011.11840</a> [<a href="http://arxiv.org/pdf/2011.11840" target="_blank">pdf</a>]

<h2>Unsupervised Discovery of DisentangledManifolds in GANs. (arXiv:2011.11842v1 [cs.CV])</h2>
<h3>Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang</h3>
<p>As recent generative models can generate photo-realistic images, people seek
to understand the mechanism behind the generation process. Interpretable
generation process is beneficial to various image editing applications. In this
work, we propose a framework to discover interpretable directions in the latent
space given arbitrary pre-trained generative adversarial networks. We propose
to learn the transformation from prior one-hot vectors representing different
attributes to the latent space used by pre-trained models. Furthermore, we
apply a centroid loss function to improve consistency and smoothness while
traversing through different directions. We demonstrate the efficacy of the
proposed framework on a wide range of datasets. The discovered direction
vectors are shown to be visually corresponding to various distinct attributes
and thus enable attribute editing.
</p>
<a href="http://arxiv.org/abs/2011.11842" target="_blank">arXiv:2011.11842</a> [<a href="http://arxiv.org/pdf/2011.11842" target="_blank">pdf</a>]

<h2>Densely connected multidilated convolutional networks for dense prediction tasks. (arXiv:2011.11844v1 [cs.CV])</h2>
<h3>Naoya Takahashi, Yuki Mitsufuji</h3>
<p>Tasks that involve high-resolution dense prediction require a modeling of
both local and global patterns in a large input field. Although the local and
global structures often depend on each other and their simultaneous modeling is
important, many convolutional neural network (CNN)-based approaches interchange
representations in different resolutions only a few times. In this paper, we
claim the importance of a dense simultaneous modeling of multiresolution
representation and propose a novel CNN architecture called densely connected
multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution
that has different dilation factors in a single layer to model different
resolutions simultaneously. By combining the multidilated convolution with the
DenseNet architecture, D3Net incorporates multiresolution learning with an
exponentially growing receptive field in almost all layers, while avoiding the
aliasing problem that occurs when we naively incorporate the dilated
convolution in DenseNet. Experiments on the image semantic segmentation task
using Cityscapes and the audio source separation task using MUSDB18 show that
the proposed method has superior performance over state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.11844" target="_blank">arXiv:2011.11844</a> [<a href="http://arxiv.org/pdf/2011.11844" target="_blank">pdf</a>]

<h2>AutoWeka4MCPS-AVATAR: Accelerating Automated Machine Learning Pipeline Composition and Optimisation. (arXiv:2011.11846v1 [cs.LG])</h2>
<h3>Tien-Dung Nguyen, Bogdan Gabrys, Katarzyna Musial</h3>
<p>Automated machine learning pipeline (ML) composition and optimisation aim at
automating the process of finding the most promising ML pipelines within
allocated resources (i.e., time, CPU and memory). Existing methods, such as
Bayesian-based and genetic-based optimisation, which are implemented in
Auto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them.
Therefore, the pipeline composition and optimisation of these methods
frequently require a tremendous amount of time that prevents them from
exploring complex pipelines to find better predictive models. To further
explore this research challenge, we have conducted experiments showing that
many of the generated pipelines are invalid in the first place, and attempting
to execute them is a waste of time and resources. To address this issue, we
propose a novel method to evaluate the validity of ML pipelines, without their
execution, using a surrogate model (AVATAR). The AVATAR generates a knowledge
base by automatically learning the capabilities and effects of ML algorithms on
datasets' characteristics. This knowledge base is used for a simplified mapping
from an original ML pipeline to a surrogate model which is a Petri net based
pipeline. Instead of executing the original ML pipeline to evaluate its
validity, the AVATAR evaluates its surrogate model constructed by capabilities
and effects of the ML pipeline components and input/output simplified mappings.
Evaluating this surrogate model is less resource-intensive than the execution
of the original pipeline. As a result, the AVATAR enables the pipeline
composition and optimisation methods to evaluate more pipelines by quickly
rejecting invalid pipelines. We integrate the AVATAR into the sequential
model-based algorithm configuration (SMAC). Our experiments show that when SMAC
employs AVATAR, it finds better solutions than on its own.
</p>
<a href="http://arxiv.org/abs/2011.11846" target="_blank">arXiv:2011.11846</a> [<a href="http://arxiv.org/pdf/2011.11846" target="_blank">pdf</a>]

<h2>Solving The Lunar Lander Problem under Uncertainty using Reinforcement Learning. (arXiv:2011.11850v1 [cs.LG])</h2>
<h3>Soham Gadgil, Yunfeng Xin, Chengzhe Xu</h3>
<p>Reinforcement Learning (RL) is an area of machine learning concerned with
enabling an agent to navigate an environment with uncertainty in order to
maximize some notion of cumulative long-term reward. In this paper, we
implement and analyze two different RL techniques, Sarsa and Deep QLearning, on
OpenAI Gym's LunarLander-v2 environment. We then introduce additional
uncertainty to the original problem to test the robustness of the mentioned
techniques. With our best models, we are able to achieve average rewards of
170+ with the Sarsa agent and 200+ with the Deep Q-Learning agent on the
original problem. We also show that these techniques are able to overcome the
additional uncertainities and achieve positive average rewards of 100+ with
both agents. We then perform a comparative analysis of the two techniques to
conclude which agent peforms better.
</p>
<a href="http://arxiv.org/abs/2011.11850" target="_blank">arXiv:2011.11850</a> [<a href="http://arxiv.org/pdf/2011.11850" target="_blank">pdf</a>]

<h2>A Robotic Dating Coaching System Leveraging Online Communities Posts. (arXiv:2011.11855v1 [cs.RO])</h2>
<h3>Sihyeon Jo, Donghwi Jung, Keonwoo Kim, Eun Gyo Joung, Giulia Nespoli, Seungryong Yoo, Minseob So, Seung-Woo Seo, Seong-Woo Kim</h3>
<p>Can a robot be a personal dating coach? Even with the increasing amount of
conversational data on the internet, the implementation of conversational
robots remains a challenge. In particular, a detailed and professional
counseling log is expensive and not publicly accessible. In this paper, we
develop a robot dating coaching system leveraging corpus from online
communities. We examine people's perceptions of the dating coaching robot with
a dialogue module. 97 participants joined to have a conversation with the
robot, and 30 of them evaluated the robot. The results indicate that
participants thought the robot could become a dating coach while considering
the robot is entertaining rather than helpful.
</p>
<a href="http://arxiv.org/abs/2011.11855" target="_blank">arXiv:2011.11855</a> [<a href="http://arxiv.org/pdf/2011.11855" target="_blank">pdf</a>]

<h2>Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v1 [cs.LG])</h2>
<h3>J&#xe9;r&#xf4;me Rony, Eric Granger, Marco Pedersoli, Ismail Ben Ayed</h3>
<p>Adversarial attack algorithms are dominated by penalty methods, which are
slow in practice, or more efficient distance-customized methods, which are
heavily tailored to the properties of the considered distance. We propose a
white-box attack algorithm to generate minimally perturbed adversarial examples
based on Augmented Lagrangian principles. We bring several non-trivial
algorithmic modifications, which have a crucial effect on performance. Our
attack enjoys the generality of penalty methods and the computational
efficiency of distance-customized algorithms, and can be readily used for a
wide set of distances. We compare our attack to state-of-the-art methods on
three datasets and several models, and consistently obtain competitive
performances with similar or lower computational complexity.
</p>
<a href="http://arxiv.org/abs/2011.11857" target="_blank">arXiv:2011.11857</a> [<a href="http://arxiv.org/pdf/2011.11857" target="_blank">pdf</a>]

<h2>GMOT-40: A Benchmark for Generic Multiple Object Tracking. (arXiv:2011.11858v1 [cs.CV])</h2>
<h3>Hexin Bai, Wensheng Cheng, Peng Chu, Juehuan Liu, Kai Zhang, Haibin Ling</h3>
<p>Multiple Object Tracking (MOT) has witnessed remarkable advances in recent
years. However, existing studies dominantly request prior knowledge of the
tracking target, and hence may not generalize well to unseen categories. In
contrast, Generic Multiple Object Tracking (GMOT), which requires little prior
information about the target, is largely under-explored. In this paper, we make
contributions to boost the study of GMOT in three aspects. First, we construct
the first public GMOT dataset, dubbed GMOT-40, which contains 40 carefully
annotated sequences evenly distributed among 10 object categories. In addition,
two tracking protocols are adopted to evaluate different characteristics of
tracking algorithms. Second, by noting the lack of devoted tracking algorithms,
we have designed a series of baseline GMOT algorithms. Third, we perform a
thorough evaluation on GMOT-40, involving popular MOT algorithms (with
necessary modifications) and the proposed baselines. We will release the
GMOT-40 benchmark, the evaluation results, as well as the baseline algorithm to
the public upon the publication of the paper.
</p>
<a href="http://arxiv.org/abs/2011.11858" target="_blank">arXiv:2011.11858</a> [<a href="http://arxiv.org/pdf/2011.11858" target="_blank">pdf</a>]

<h2>Cyclic Label Propagation for Graph Semi-supervised Learning. (arXiv:2011.11860v1 [cs.LG])</h2>
<h3>Zhao Li, Yixin Liu, Zhen Zhang, Shirui Pan, Jianliang Gao, Jiajun Bu</h3>
<p>Graph neural networks (GNNs) have emerged as effective approaches for graph
analysis, especially in the scenario of semi-supervised learning. Despite its
success, GNN often suffers from over-smoothing and over-fitting problems, which
affects its performance on node classification tasks. We analyze that an
alternative method, the label propagation algorithm (LPA), avoids the
aforementioned problems thus it is a promising choice for graph semi-supervised
learning. Nevertheless, the intrinsic limitations of LPA on feature
exploitation and relation modeling make propagating labels become less
effective. To overcome these limitations, we introduce a novel framework for
graph semi-supervised learning termed as Cyclic Label Propagation (CycProp for
abbreviation), which integrates GNNs into the process of label propagation in a
cyclic and mutually reinforcing manner to exploit the advantages of both GNNs
and LPA. In particular, our proposed CycProp updates the node embeddings
learned by GNN module with the augmented information by label propagation,
while fine-tunes the weighted graph of label propagation with the help of node
embedding in turn. After the model converges, reliably predicted labels and
informative node embeddings are obtained with the LPA and GNN modules
respectively. Extensive experiments on various real-world datasets are
conducted, and the experimental results empirically demonstrate that the
proposed CycProp model can achieve relatively significant gains over the
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.11860" target="_blank">arXiv:2011.11860</a> [<a href="http://arxiv.org/pdf/2011.11860" target="_blank">pdf</a>]

<h2>Multi-Scale Progressive Fusion Learning for Depth Map Super-Resolution. (arXiv:2011.11865v1 [cs.CV])</h2>
<h3>Chuhua Xian, Kun Qian, Zitian Zhang, Charlie C.L. Wang</h3>
<p>Limited by the cost and technology, the resolution of depth map collected by
depth camera is often lower than that of its associated RGB camera. Although
there have been many researches on RGB image super-resolution (SR), a major
problem with depth map super-resolution is that there will be obvious jagged
edges and excessive loss of details. To tackle these difficulties, in this
work, we propose a multi-scale progressive fusion network for depth map SR,
which possess an asymptotic structure to integrate hierarchical features in
different domains. Given a low-resolution (LR) depth map and its associated
high-resolution (HR) color image, We utilize two different branches to achieve
multi-scale feature learning. Next, we propose a step-wise fusion strategy to
restore the HR depth map. Finally, a multi-dimensional loss is introduced to
constrain clear boundaries and details. Extensive experiments show that our
proposed method produces improved results against state-of-the-art methods both
qualitatively and quantitatively.
</p>
<a href="http://arxiv.org/abs/2011.11865" target="_blank">arXiv:2011.11865</a> [<a href="http://arxiv.org/pdf/2011.11865" target="_blank">pdf</a>]

<h2>Persistent Mixture Model Networks for Few-Shot Image Classification. (arXiv:2011.11872v1 [cs.CV])</h2>
<h3>Arman Afrasiyabi, Jean-Fran&#xe7;ois Lalonde, Christian Gagn&#xe9;</h3>
<p>We introduce Persistent Mixture Model (PMM) networks for representation
learning in the few-shot image classification context. While previous methods
represent classes with a single centroid or rely on post hoc clustering
methods, our method learns a mixture model for each base class jointly with the
data representation in an end-to-end manner. The PMM training algorithm is
organized into two main stages: 1) initial training and 2) progressive
following. First, the initial estimate for multi-component mixtures is learned
for each class in the base domain using a combination of two loss functions
(competitive and collaborative). The resulting network is then progressively
refined through a leader-follower learning procedure, which uses the current
estimate of the learner as a fixed "target" network. This target network is
used to make a consistent assignment of instances to mixture components, in
order to increase performance while stabilizing the training. The effectiveness
of our joint representation/mixture learning approach is demonstrated with
extensive experiments on four standard datasets and four backbones. In
particular, we demonstrate that when we combine our robust representation with
recent alignment- and margin-based approaches, we achieve new state-of-the-art
results in the inductive setting, with an absolute accuracy for 5-shot
classification of 82.45% on miniImageNet, 88.20% with tieredImageNet, and
60.70% in FC100, all using the ResNet-12 backbone.
</p>
<a href="http://arxiv.org/abs/2011.11872" target="_blank">arXiv:2011.11872</a> [<a href="http://arxiv.org/pdf/2011.11872" target="_blank">pdf</a>]

<h2>InstaHide's Sample Complexity When Mixing Two Private Images. (arXiv:2011.11877v1 [cs.LG])</h2>
<h3>Baihe Huang, Zhao Song, Runzhou Tao, Ruizhe Zhang, Danyang Zhuo</h3>
<p>Inspired by InstaHide challenge [Huang, Song, Li and Arora'20], [Chen, Song
and Zhuo'20] recently provides one mathematical formulation of InstaHide attack
problem under Gaussian images distribution. They show that it suffices to use
$O(n_{\mathsf{priv}}^{k_{\mathsf{priv}} - 2/(k_{\mathsf{priv}} + 1)})$ samples
to recover one private image in $n_{\mathsf{priv}}^{O(k_{\mathsf{priv}})} +
\mathrm{poly}(n_{\mathsf{pub}})$ time for any integer $k_{\mathsf{priv}}$,
where $n_{\mathsf{priv}}$ and $n_{\mathsf{pub}}$ denote the number of images
used in the private and the public dataset to generate a mixed image sample.

Under the current setup for the InstaHide challenge of mixing two private
images ($k_{\mathsf{priv}} = 2$), this means $n_{\mathsf{priv}}^{4/3}$ samples
are sufficient to recover a private image. In this work, we show that
$n_{\mathsf{priv}} \log ( n_{\mathsf{priv}} )$ samples are sufficient
(information-theoretically) for recovering all the private images.
</p>
<a href="http://arxiv.org/abs/2011.11877" target="_blank">arXiv:2011.11877</a> [<a href="http://arxiv.org/pdf/2011.11877" target="_blank">pdf</a>]

<h2>Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder. (arXiv:2011.11878v1 [cs.LG])</h2>
<h3>Hyemi Kim, Seungjae Shin, JoonHo Jang, Kyungwoo Song, Weonyoung Joo, Wanmo Kang, Il-Chul Moon</h3>
<p>The problem of fair classification can be mollified if we develop a method to
remove the embedded sensitive information from the classification features.
This line of separating the sensitive information is developed through the
causal inference, and the causal inference enables the counterfactual
generations to contrast the what-if case of the opposite sensitive attribute.
Along with this separation with the causality, a frequent assumption in the
deep latent causal model defines a single latent variable to absorb the entire
exogenous uncertainty of the causal graph. However, we claim that such
structure cannot distinguish the 1) information caused by the intervention
(i.e., sensitive variable) and 2) information correlated with the intervention
from the data. Therefore, this paper proposes Disentangled Causal Effect
Variational Autoencoder (DCEVAE) to resolve this limitation by disentangling
the exogenous uncertainty into two latent variables: either 1) independent to
interventions or 2) correlated to interventions without causality.
Particularly, our disentangling approach preserves the latent variable
correlated to interventions in generating counterfactual examples. We show that
our method estimates the total effect and the counterfactual effect without a
complete causal graph. By adding a fairness regularization, DCEVAE generates a
counterfactual fair dataset while losing less original information. Also,
DCEVAE generates natural counterfactual images by only flipping sensitive
information. Additionally, we theoretically show the differences in the
covariance structures of DCEVAE and prior works from the perspective of the
latent disentanglement.
</p>
<a href="http://arxiv.org/abs/2011.11878" target="_blank">arXiv:2011.11878</a> [<a href="http://arxiv.org/pdf/2011.11878" target="_blank">pdf</a>]

<h2>Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v1 [cs.CV])</h2>
<h3>Mahmoud Afifi, Jonathan T. Barron, Chloe LeGendre, Yun-Ta Tsai, Francois Bleibel</h3>
<p>We present "Cross-Camera Convolutional Color Constancy" (C5), a
learning-based method, trained on images from multiple cameras, that accurately
estimates a scene's illuminant color from raw images captured by a new camera
previously unseen during training. C5 is a hypernetwork-like extension of the
convolutional color constancy (CCC) approach: C5 learns to generate the weights
of a CCC model that is then evaluated on the input image, with the CCC weights
dynamically adapted to different input content. Unlike prior cross-camera color
constancy models, which are usually designed to be agnostic to the spectral
properties of test-set images from unobserved cameras, C5 approaches this
problem through the lens of transductive inference: additional unlabeled images
are provided as input to the model at test time, which allows the model to
calibrate itself to the spectral properties of the test-set camera during
inference. C5 achieves state-of-the-art accuracy for cross-camera color
constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on
a GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is
a practical solution to the problem of calibration-free automatic white balance
for mobile photography.
</p>
<a href="http://arxiv.org/abs/2011.11890" target="_blank">arXiv:2011.11890</a> [<a href="http://arxiv.org/pdf/2011.11890" target="_blank">pdf</a>]

<h2>Learning Principle of Least Action with Reinforcement Learning. (arXiv:2011.11891v1 [cs.LG])</h2>
<h3>Zehao Jin, Joshua Yao-Yu Lin, Siao-Fong Li</h3>
<p>Nature provides a way to understand physics with reinforcement learning since
nature favors the economical way for an object to propagate. In the case of
classical mechanics, nature favors the object to move along the path according
to the integral of the Lagrangian, called the action $\mathcal{S}$. We consider
setting the reward/penalty as a function of $\mathcal{S}$, so the agent could
learn the physical trajectory of particles in various kinds of environments
with reinforcement learning. In this work, we verified the idea by using a
Q-Learning based algorithm on learning how light propagates in materials with
different refraction indices, and show that the agent could recover the
minimal-time path equivalent to the solution obtained by Snell's law or
Fermat's Principle. We also discuss the similarity of our reinforcement
learning approach to the path integral formalism.
</p>
<a href="http://arxiv.org/abs/2011.11891" target="_blank">arXiv:2011.11891</a> [<a href="http://arxiv.org/pdf/2011.11891" target="_blank">pdf</a>]

<h2>Temporal Action Detection with Multi-level Supervision. (arXiv:2011.11893v1 [cs.CV])</h2>
<h3>Baifeng Shi, Qi Dai, Judy Hoffman, Kate Saenko, Trevor Darrell, Huijuan Xu</h3>
<p>Training temporal action detection in videos requires large amounts of
labeled data, yet such annotation is expensive to collect. Incorporating
unlabeled or weakly-labeled data to train action detection model could help
reduce annotation cost. In this work, we first introduce the Semi-supervised
Action Detection (SSAD) task with a mixture of labeled and unlabeled data and
analyze different types of errors in the proposed SSAD baselines which are
directly adapted from the semi-supervised classification task. To alleviate the
main error of action incompleteness (i.e., missing parts of actions) in SSAD
baselines, we further design an unsupervised foreground attention (UFA) module
utilizing the "independence" between foreground and background motion. Then we
incorporate weakly-labeled data into SSAD and propose Omni-supervised Action
Detection (OSAD) with three levels of supervision. An information bottleneck
(IB) suppressing the scene information in non-action frames while preserving
the action information is designed to help overcome the accompanying
action-context confusion problem in OSAD baselines. We extensively benchmark
against the baselines for SSAD and OSAD on our created data splits in THUMOS14
and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and
IB methods. Lastly, the benefit of our full OSAD-IB model under limited
annotation budgets is shown by exploring the optimal annotation strategy for
labeled, unlabeled and weakly-labeled data.
</p>
<a href="http://arxiv.org/abs/2011.11893" target="_blank">arXiv:2011.11893</a> [<a href="http://arxiv.org/pdf/2011.11893" target="_blank">pdf</a>]

<h2>CAFE-GAN: Arbitrary Face Attribute Editing with Complementary Attention Feature. (arXiv:2011.11900v1 [cs.CV])</h2>
<h3>Jeong-gi Kwak, David K. Han, Hanseok Ko</h3>
<p>The goal of face attribute editing is altering a facial image according to
given target attributes such as hair color, mustache, gender, etc. It belongs
to the image-to-image domain transfer problem with a set of attributes
considered as a distinctive domain. There have been some works in multi-domain
transfer problem focusing on facial attribute editing employing Generative
Adversarial Network (GAN). These methods have reported some successes but they
also result in unintended changes in facial regions - meaning the generator
alters regions unrelated to the specified attributes. To address this
unintended altering problem, we propose a novel GAN model which is designed to
edit only the parts of a face pertinent to the target attributes by the concept
of Complementary Attention Feature (CAFE). CAFE identifies the facial regions
to be transformed by considering both target attributes as well as
complementary attributes, which we define as those attributes absent in the
input facial image. In addition, we introduce a complementary feature matching
to help in training the generator for utilizing the spatial information of
attributes. Effectiveness of the proposed method is demonstrated by analysis
and comparison study with state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.11900" target="_blank">arXiv:2011.11900</a> [<a href="http://arxiv.org/pdf/2011.11900" target="_blank">pdf</a>]

<h2>Latent Group Structured Multi-task Learning. (arXiv:2011.11904v1 [cs.LG])</h2>
<h3>Xiangyu Niu, Yifan Sun, Jinyuan Sun</h3>
<p>In multi-task learning (MTL), we improve the performance of key machine
learning algorithms by training various tasks jointly. When the number of tasks
is large, modeling task structure can further refine the task relationship
model. For example, often tasks can be grouped based on metadata, or via simple
preprocessing steps like K-means. In this paper, we present our group
structured latent-space multi-task learning model, which encourages group
structured tasks defined by prior information. We use an alternating
minimization method to learn the model parameters. Experiments are conducted on
both synthetic and real-world datasets, showing competitive performance over
single-task learning (where each group is trained separately) and other MTL
baselines.
</p>
<a href="http://arxiv.org/abs/2011.11904" target="_blank">arXiv:2011.11904</a> [<a href="http://arxiv.org/pdf/2011.11904" target="_blank">pdf</a>]

<h2>Rotational Error Metrics for Quadrotor Control. (arXiv:2011.11909v1 [cs.RO])</h2>
<h3>Alexander Spitzer, Nathan Michael</h3>
<p>We analyze and experimentally compare various rotational error metrics for
use in quadrotor controllers. Traditional quadrotor attitude controllers have
used Euler angles or the full rotation to compute an attitude error and scale
that to compute a control response. Recently, several works have shown that
prioritizing quadrotor tilt, or thrust vector error, in the attitude controller
leads to improved position control, especially in situations with large yaw
error. We provide a catalog of proposed rotational metrics, place them into the
same framework, and show that we can independently reason about and design the
magnitude of the response and the direction of the response. Existing
approaches mainly fall into two categories: (1) metrics that induce a response
in the shortest direction to correct the full rotation error and (2) metrics
that combine a response in the shortest direction to correct tilt error with
the shortest direction to correct yaw error. We show experimental results to
highlight the salient differences between the rotational error metrics. See
https://alspitz.github.io/roterrormetrics.html for an interactive simulation
visualizing the experiments performed.
</p>
<a href="http://arxiv.org/abs/2011.11909" target="_blank">arXiv:2011.11909</a> [<a href="http://arxiv.org/pdf/2011.11909" target="_blank">pdf</a>]

<h2>Variational Monocular Depth Estimation for Reliability Prediction. (arXiv:2011.11912v1 [cs.CV])</h2>
<h3>Noriaki Hirose, Shun Taguchi, Keisuke Kawano, Satoshi Koide</h3>
<p>Self-supervised learning for monocular depth estimation is widely
investigated as an alternative to supervised learning approach, that requires a
lot of ground truths. Previous works have successfully improved the accuracy of
depth estimation by modifying the model structure, adding objectives, and
masking dynamic objects and occluded area. However, when using such estimated
depth image in applications, such as autonomous vehicles, and robots, we have
to uniformly believe the estimated depth at each pixel position. This could
lead to fatal errors in performing the tasks, because estimated depth at some
pixels may make a bigger mistake. In this paper, we theoretically formulate a
variational model for the monocular depth estimation to predict the reliability
of the estimated depth image. Based on the results, we can exclude the
estimated depths with low reliability or refine them for actual use. The
effectiveness of the proposed method is quantitatively and qualitatively
demonstrated using the KITTI benchmark and Make3D dataset.
</p>
<a href="http://arxiv.org/abs/2011.11912" target="_blank">arXiv:2011.11912</a> [<a href="http://arxiv.org/pdf/2011.11912" target="_blank">pdf</a>]

<h2>Semi-supervised Gated Recurrent Neural Networks for Robotic Terrain Classification. (arXiv:2011.11913v1 [cs.RO])</h2>
<h3>Ahmadreza Ahmadi, T&#xf8;nnes Nygaard, Navinda Kottege, David Howard, Nicolas Hudson</h3>
<p>Legged robots are popular candidates for missions in challenging terrains due
to the wide variety of locomotion strategies they can employ. Terrain
classification is a key enabling technology for autonomous legged robots, as it
allows the robot to harness their innate flexibility to adapt their behaviour
to the demands of their operating environment. In this paper, we show how
highly capable machine learning techniques, namely gated recurrent neural
networks, allow our target legged robot to correctly classify the terrain it
traverses in both supervised and semi-supervised fashions. Tests on a benchmark
data set shows that our time-domain classifiers are well capable of dealing
with raw and variable-length data with small amount of labels and perform to a
level far exceeding the frequency-domain classifiers. The classification
results on our own extended data set opens up a range of high-performance
behaviours that are specific to those environments. Furthermore, we show how
raw unlabelled data is used to improve significantly the classification results
in a semi-supervised model.
</p>
<a href="http://arxiv.org/abs/2011.11913" target="_blank">arXiv:2011.11913</a> [<a href="http://arxiv.org/pdf/2011.11913" target="_blank">pdf</a>]

<h2>On the Adversarial Robustness of 3D Point Cloud Classification. (arXiv:2011.11922v1 [cs.LG])</h2>
<h3>Jiachen Sun, Karl Koenig, Yulong Cao, Qi Alfred Chen, Z. Morley Mao</h3>
<p>3D point clouds play pivotal roles in various safety-critical fields, such as
autonomous driving, which desires the corresponding deep neural networks to be
robust to adversarial perturbations. Though a few defenses against adversarial
point cloud classification have been proposed, it remains unknown whether they
can provide real robustness. To this end, we perform the first security
analysis of state-of-the-art defenses and design adaptive attacks on them. Our
100% adaptive attack success rates demonstrate that current defense designs are
still vulnerable. Since adversarial training (AT) is believed to be the most
effective defense, we present the first in-depth study showing how AT behaves
in point cloud classification and identify that the required symmetric function
(pooling operation) is paramount to the model's robustness under AT. Through
our systematic analysis, we find that the default used fixed pooling operations
(e.g., MAX pooling) generally weaken AT's performance in point cloud
classification. Still, sorting-based parametric pooling operations can
significantly improve the models' robustness. Based on the above insights, we
further propose DeepSym, a deep symmetric pooling operation, to architecturally
advance the adversarial robustness under AT to 47.0% without sacrificing
nominal accuracy, outperforming the original design and a strong baseline by
28.5% ($\sim 2.6 \times$) and 6.5%, respectively, in PointNet.
</p>
<a href="http://arxiv.org/abs/2011.11922" target="_blank">arXiv:2011.11922</a> [<a href="http://arxiv.org/pdf/2011.11922" target="_blank">pdf</a>]

<h2>Automatic Clustering for Unsupervised Risk Diagnosis of Vehicle Driving for Smart Road. (arXiv:2011.11933v1 [cs.LG])</h2>
<h3>Xiupeng Shi, Yiik Diew Wong, Chen Chai, Michael Zhi-Feng Li, Tianyi Chen, Zeng Zeng</h3>
<p>Early risk diagnosis and driving anomaly detection from vehicle stream are of
great benefits in a range of advanced solutions towards Smart Road and crash
prevention, although there are intrinsic challenges, especially lack of ground
truth, definition of multiple risk exposures. This study proposes a
domain-specific automatic clustering (termed Autocluster) to self-learn the
optimal models for unsupervised risk assessment, which integrates key steps of
risk clustering into an auto-optimisable pipeline, including feature and
algorithm selection, hyperparameter auto-tuning. Firstly, based on surrogate
conflict measures, indicator-guided feature extraction is conducted to
construct temporal-spatial and kinematical risk features. Then we develop an
elimination-based model reliance importance (EMRI) method to
unsupervised-select the useful features. Secondly, we propose balanced
Silhouette Index (bSI) to evaluate the internal quality of imbalanced
clustering. A loss function is designed that considers the clustering
performance in terms of internal quality, inter-cluster variation, and model
stability. Thirdly, based on Bayesian optimisation, the algorithm selection and
hyperparameter auto-tuning are self-learned to generate the best clustering
partitions. Various algorithms are comprehensively investigated. Herein, NGSIM
vehicle trajectory data is used for test-bedding. Findings show that
Autocluster is reliable and promising to diagnose multiple distinct risk
exposures inherent to generalised driving behaviour. Besides, we also delve
into risk clustering, such as, algorithms heterogeneity, Silhouette analysis,
hierarchical clustering flows, etc. Meanwhile, the Autocluster is also a method
for unsupervised multi-risk data labelling and indicator threshold calibration.
Furthermore, Autocluster is useful to tackle the challenges in imbalanced
clustering without ground truth or priori knowledge
</p>
<a href="http://arxiv.org/abs/2011.11933" target="_blank">arXiv:2011.11933</a> [<a href="http://arxiv.org/pdf/2011.11933" target="_blank">pdf</a>]

<h2>DADNN: Multi-Scene CTR Prediction via Domain-Aware Deep Neural Network. (arXiv:2011.11938v1 [cs.AI])</h2>
<h3>Junyou He, Guibao Mei, Feng Xing, Xiaorui Yang, Yongjun Bao, Weipeng Yan</h3>
<p>Click through rate(CTR) prediction is a core task in advertising systems. The
booming e-commerce business in our company, results in a growing number of
scenes. Most of them are so-called long-tail scenes, which means that the
traffic of a single scene is limited, but the overall traffic is considerable.
Typical studies mainly focus on serving a single scene with a well designed
model. However, this method brings excessive resource consumption both on
offline training and online serving. Besides, simply training a single model
with data from multiple scenes ignores the characteristics of their own. To
address these challenges, we propose a novel but practical model named
Domain-Aware Deep Neural Network(DADNN) by serving multiple scenes with only
one model. Specifically, shared bottom block among all scenes is applied to
learn a common representation, while domain-specific heads maintain the
characteristics of every scene. Besides, knowledge transfer is introduced to
enhance the opportunity of knowledge sharing among different scenes. In this
paper, we study two instances of DADNN where its shared bottom block is
multilayer perceptron(MLP) and Multi-gate Mixture-of-Experts(MMoE)
respectively, for which we denote as DADNN-MLP and DADNN-MMoE.Comprehensive
offline experiments on a real production dataset from our company show that
DADNN outperforms several state-of-the-art methods for multi-scene CTR
prediction. Extensive online A/B tests reveal that DADNN-MLP contributes up to
6.7% CTR and 3.0% CPM(Cost Per Mille) promotion compared with a well-engineered
DCN model. Furthermore, DADNN-MMoE outperforms DADNN-MLP with a relative
improvement of 2.2% and 2.7% on CTR and CPM respectively. More importantly,
DADNN utilizes a single model for multiple scenes which saves a lot of offline
training and online serving resources.
</p>
<a href="http://arxiv.org/abs/2011.11938" target="_blank">arXiv:2011.11938</a> [<a href="http://arxiv.org/pdf/2011.11938" target="_blank">pdf</a>]

<h2>Hyper parameter estimation method with particle swarm optimization. (arXiv:2011.11944v1 [cs.LG])</h2>
<h3>Yaru Li, Yulai Zhang</h3>
<p>Particle swarm optimization (PSO) method cannot be directly used in the
problem of hyper-parameter estimation since the mathematical formulation of the
mapping from hyper-parameters to loss function or generalization accuracy is
unclear. Bayesian optimization (BO) framework is capable of converting the
optimization of the hyper-parameters into the optimization of an acquisition
function. The acquisition function is non-convex and multi-peak. So the problem
can be better solved by the PSO. The proposed method in this paper uses the
particle swarm method to optimize the acquisition function in the BO framework
to get better hyper-parameters. The performances of proposed method in both of
the classification and regression models are evaluated and demonstrated. The
results on several benchmark problems are improved.
</p>
<a href="http://arxiv.org/abs/2011.11944" target="_blank">arXiv:2011.11944</a> [<a href="http://arxiv.org/pdf/2011.11944" target="_blank">pdf</a>]

<h2>Benchmarking Image Retrieval for Visual Localization. (arXiv:2011.11946v1 [cs.CV])</h2>
<h3>No&#xe9; Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, Torsten Sattler</h3>
<p>Visual localization, i.e., camera pose estimation in a known scene, is a core
component of technologies such as autonomous driving and augmented reality.
State-of-the-art localization approaches often rely on image retrieval
techniques for one of two tasks: (1) provide an approximate pose estimate or
(2) determine which parts of the scene are potentially visible in a given query
image. It is common practice to use state-of-the-art image retrieval algorithms
for these tasks. These algorithms are often trained for the goal of retrieving
the same landmark under a large range of viewpoint changes. However, robustness
to viewpoint changes is not necessarily desirable in the context of visual
localization. This paper focuses on understanding the role of image retrieval
for multiple visual localization tasks. We introduce a benchmark setup and
compare state-of-the-art retrieval representations on multiple datasets. We
show that retrieval performance on classical landmark retrieval/recognition
tasks correlates only for some but not all tasks to localization performance.
This indicates a need for retrieval approaches specifically designed for
localization tasks. Our benchmark and evaluation protocols are available at
https://github.com/naver/kapture-localization.
</p>
<a href="http://arxiv.org/abs/2011.11946" target="_blank">arXiv:2011.11946</a> [<a href="http://arxiv.org/pdf/2011.11946" target="_blank">pdf</a>]

<h2>Path Planning with Automatic Seam Extraction over Point Cloud Models for Robotic Arc Welding. (arXiv:2011.11951v1 [cs.RO])</h2>
<h3>Peng Zhou, Rui Peng, Maggie Xu, Victor Wu, David Navarro-Alarcon</h3>
<p>This paper presents a point cloud based robotic system for arc welding. Using
hand gesture controls, the system scans partial point cloud views of workpiece
and reconstructs them into a complete 3D model by a linear iterative closest
point algorithm. Then, a bilateral filter is extended to denoise the workpiece
model and preserve important geometrical information. To extract the welding
seam from the model, a novel intensity-based algorithm is proposed that detects
edge points and generates a smooth 6-DOF welding path. The methods are tested
on multiple workpieces with different joint types and poses. Experimental
results prove the robustness and efficiency of this robotic system on automatic
path planning for welding applications.
</p>
<a href="http://arxiv.org/abs/2011.11951" target="_blank">arXiv:2011.11951</a> [<a href="http://arxiv.org/pdf/2011.11951" target="_blank">pdf</a>]

<h2>DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations. (arXiv:2011.11953v1 [cs.CV])</h2>
<h3>Wenhao Wang, Shengcai Liao, Fang Zhao, Cuicui Kang, Ling Shao</h3>
<p>Existing person re-identification methods often have low generalization
capability, which is mostly due to the limited availability of large-scale
labeled training data. However, labeling large-scale training data is very
expensive and time-consuming. To address this, this paper presents a solution,
called DomainMix, which can learn a person re-identification model from both
synthetic and real-world data, for the first time, completely without human
annotations. This way, the proposed method enjoys the cheap availability of
large-scale training data, and benefiting from its scalability and diversity,
the learned model is able to generalize well on unseen domains. Specifically,
inspired from a recent work generating large-scale synthetic data for effective
person re-identification training, the proposed method firstly applies
unsupervised domain adaptation from labeled synthetic data to unlabeled
real-world data to generate pseudo labels. Then, the two sources of data are
directly mixed together for supervised training. However, a large domain gap
still exists between them. To address this, a domain-invariant feature learning
method is proposed, which designs an adversarial learning between
domain-invariant feature learning and domain discrimination, and meanwhile
learns a discriminant feature for person re-identification. This way, the
domain gap between synthetic and real-world data is much reduced, and the
learned feature is generalizable thanks to the large-scale and diverse training
data. Experimental results show that the proposed annotation-free method is
more or less comparable to the counterpart trained with full human annotations,
which is quite promising. In addition, it achieves the current state of the art
on several popular person re-identification datasets under direct cross-dataset
evaluation.
</p>
<a href="http://arxiv.org/abs/2011.11953" target="_blank">arXiv:2011.11953</a> [<a href="http://arxiv.org/pdf/2011.11953" target="_blank">pdf</a>]

<h2>Towards Imperceptible Universal Attacks on Texture Recognition. (arXiv:2011.11957v1 [cs.CV])</h2>
<h3>Yingpeng Deng, Lina J. Karam</h3>
<p>Although deep neural networks (DNNs) have been shown to be susceptible to
image-agnostic adversarial attacks on natural image classification problems,
the effects of such attacks on DNN-based texture recognition have yet to be
explored. As part of our work, we find that limiting the perturbation's $l_p$
norm in the spatial domain may not be a suitable way to restrict the
perceptibility of universal adversarial perturbations for texture images. Based
on the fact that human perception is affected by local visual frequency
characteristics, we propose a frequency-tuned universal attack method to
compute universal perturbations in the frequency domain. Our experiments
indicate that our proposed method can produce less perceptible perturbations
yet with a similar or higher white-box fooling rates on various DNN texture
classifiers and texture datasets as compared to existing universal attack
techniques. We also demonstrate that our approach can improve the attack
robustness against defended models as well as the cross-dataset transferability
for texture recognition problems.
</p>
<a href="http://arxiv.org/abs/2011.11957" target="_blank">arXiv:2011.11957</a> [<a href="http://arxiv.org/pdf/2011.11957" target="_blank">pdf</a>]

<h2>Provably-Robust Runtime Monitoring of Neuron Activation Patterns. (arXiv:2011.11959v1 [cs.LG])</h2>
<h3>Chih-Hong Cheng</h3>
<p>For deep neural networks (DNNs) to be used in safety-critical autonomous
driving tasks, it is desirable to monitor in operation time if the input for
the DNN is similar to the data used in DNN training. While recent results in
monitoring DNN activation patterns provide a sound guarantee due to building an
abstraction out of the training data set, reducing false positives due to
slight input perturbation has been an issue towards successfully adapting the
techniques. We address this challenge by integrating formal symbolic reasoning
inside the monitor construction process. The algorithm performs a sound
worst-case estimate of neuron values with inputs (or features) subject to
perturbation, before the abstraction function is applied to build the monitor.
The provable robustness is further generalized to cases where monitoring a
single neuron can use more than one bit, implying that one can record
activation patterns with a fine-grained decision on the neuron value interval.
</p>
<a href="http://arxiv.org/abs/2011.11959" target="_blank">arXiv:2011.11959</a> [<a href="http://arxiv.org/pdf/2011.11959" target="_blank">pdf</a>]

<h2>Is a Green Screen Really Necessary for Real-Time Human Matting?. (arXiv:2011.11961v1 [cs.CV])</h2>
<h3>Zhanghan Ke, Kaican Li, Yurou Zhou, Qiuhua Wu, Xiangyu Mao, Qiong Yan, Rynson W.H. Lau</h3>
<p>For human matting without the green screen, existing works either require
auxiliary inputs that are costly to obtain or use multiple models that are
computationally expensive. Consequently, they are unavailable in real-time
applications. In contrast, we present a light-weight matting objective
decomposition network (MODNet), which can process human matting from a single
input image in real time. The design of MODNet benefits from optimizing a
series of correlated sub-objectives simultaneously via explicit constraints.
Moreover, since trimap-free methods usually suffer from the domain shift
problem in practice, we introduce (1) a self-supervised strategy based on
sub-objectives consistency to adapt MODNet to real-world data and (2) a
one-frame delay trick to smooth the results when applying MODNet to video human
matting.

MODNet is easy to be trained in an end-to-end style. It is much faster than
contemporaneous matting methods and runs at 63 frames per second. On a
carefully designed human matting benchmark newly proposed in this work, MODNet
greatly outperforms prior trimap-free methods. More importantly, our method
achieves remarkable results in daily photos and videos. Now, do you really need
a green screen for real-time human matting?
</p>
<a href="http://arxiv.org/abs/2011.11961" target="_blank">arXiv:2011.11961</a> [<a href="http://arxiv.org/pdf/2011.11961" target="_blank">pdf</a>]

<h2>LiDAR-based Panoptic Segmentation via Dynamic Shifting Network. (arXiv:2011.11964v1 [cs.CV])</h2>
<h3>Fangzhou Hong, Hui Zhou, Xinge Zhu, Hongsheng Li, Ziwei Liu</h3>
<p>With the rapid advances of autonomous driving, it becomes critical to equip
its sensing system with more holistic 3D perception. However, existing works
focus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g.
trees and buildings) from the LiDAR sensor. In this work, we address the task
of LiDAR-based panoptic segmentation, which aims to parse both objects and
scenes in a unified manner. As one of the first endeavors towards this new
challenging task, we propose the Dynamic Shifting Network (DS-Net), which
serves as an effective panoptic segmentation framework in the point cloud
realm. In particular, DS-Net has three appealing properties: 1) strong backbone
design. DS-Net adopts the cylinder convolution that is specifically designed
for LiDAR point clouds. The extracted features are shared by the semantic
branch and the instance branch which operates in a bottom-up clustering style.
2) Dynamic Shifting for complex point distributions. We observe that
commonly-used clustering algorithms like BFS or DBSCAN are incapable of
handling complex autonomous driving scenes with non-uniform point cloud
distributions and varying instance sizes. Thus, we present an efficient
learnable clustering module, dynamic shifting, which adapts kernel functions
on-the-fly for different instances. 3) Consensus-driven Fusion. Finally,
consensus-driven fusion is used to deal with the disagreement between semantic
and instance predictions. To comprehensively evaluate the performance of
LiDAR-based panoptic segmentation, we construct and curate benchmarks from two
large-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes.
Extensive experiments demonstrate that our proposed DS-Net achieves superior
accuracies over current state-of-the-art methods. Notably, we achieve 1st place
on the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6% in
terms of the PQ metric.
</p>
<a href="http://arxiv.org/abs/2011.11964" target="_blank">arXiv:2011.11964</a> [<a href="http://arxiv.org/pdf/2011.11964" target="_blank">pdf</a>]

<h2>Foundations of the Socio-physical Model of Activities (SOMA) for Autonomous Robotic Agents. (arXiv:2011.11972v1 [cs.RO])</h2>
<h3>Daniel Be&#xdf;ler, Robert Porzel, Mihai Pomarlan, Abhijit Vyas, Sebastian H&#xf6;ffner, Michael Beetz, Rainer Malaka, John Bateman</h3>
<p>In this paper, we present foundations of the Socio-physical Model of
Activities (SOMA). SOMA represents both the physical as well as the social
context of everyday activities. Such tasks seem to be trivial for humans,
however, they pose severe problems for artificial agents. For starters, a
natural language command requesting something will leave many pieces of
information necessary for performing the task unspecified. Humans can solve
such problems fast as we reduce the search space by recourse to prior knowledge
such as a connected collection of plans that describe how certain goals can be
achieved at various levels of abstraction. Rather than enumerating fine-grained
physical contexts SOMA sets out to include socially constructed knowledge about
the functions of actions to achieve a variety of goals or the roles objects can
play in a given situation. As the human cognition system is capable of
generalizing experiences into abstract knowledge pieces applicable to novel
situations, we argue that both physical and social context need be modeled to
tackle these challenges in a general manner. This is represented by the link
between the physical and social context in SOMA where relationships are
established between occurrences and generalizations of them, which has been
demonstrated in several use cases that validate SOMA.
</p>
<a href="http://arxiv.org/abs/2011.11972" target="_blank">arXiv:2011.11972</a> [<a href="http://arxiv.org/pdf/2011.11972" target="_blank">pdf</a>]

<h2>UKPGAN: Unsupervised KeyPoint GANeration. (arXiv:2011.11974v1 [cs.CV])</h2>
<h3>Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu</h3>
<p>Keypoint detection is an essential component for the object registration and
alignment. However, previous works mainly focused on how to register keypoints
under arbitrary rigid transformations. Differently, in this work, we reckon
keypoints under an information compression scheme to represent the whole
object. Based on this, we propose UKPGAN, an unsupervised 3D keypoint detector
where keypoints are detected so that they could reconstruct the original object
shape. Two modules: GAN-based keypoint sparsity control and salient information
distillation modules are proposed to locate those important keypoints.
Extensive experiments show that our keypoints preserve the semantic information
of objects and align well with human annotated part and keypoint labels.
Furthermore, we show that UKPGAN can be applied to either rigid objects or
non-rigid SMPL human bodies under arbitrary pose deformations. As a keypoint
detector, our model is stable under both rigid and non-rigid transformations,
with local reference frame estimation. Our code is available on
https://github.com/qq456cvb/UKPGAN.
</p>
<a href="http://arxiv.org/abs/2011.11974" target="_blank">arXiv:2011.11974</a> [<a href="http://arxiv.org/pdf/2011.11974" target="_blank">pdf</a>]

<h2>Deep-learning based discovery of partial differential equations in integral form from sparse and noisy data. (arXiv:2011.11981v1 [cs.LG])</h2>
<h3>Hao Xu, Dongxiao Zhang, Nanzhe Wang</h3>
<p>Data-driven discovery of partial differential equations (PDEs) has attracted
increasing attention in recent years. Although significant progress has been
made, certain unresolved issues remain. For example, for PDEs with high-order
derivatives, the performance of existing methods is unsatisfactory, especially
when the data are sparse and noisy. It is also difficult to discover
heterogeneous parametric PDEs where heterogeneous parameters are embedded in
the partial differential operators. In this work, a new framework combining
deep-learning and integral form is proposed to handle the above-mentioned
problems simultaneously, and improve the accuracy and stability of PDE
discovery. In the framework, a deep neural network is firstly trained with
observation data to generate meta-data and calculate derivatives. Then, a
unified integral form is defined, and the genetic algorithm is employed to
discover the best structure. Finally, the value of parameters is calculated,
and whether the parameters are constants or variables is identified. Numerical
experiments proved that our proposed algorithm is more robust to noise and more
accurate compared with existing methods due to the utilization of integral
form. Our proposed algorithm is also able to discover PDEs with high-order
derivatives or heterogeneous parameters accurately with sparse and noisy data.
</p>
<a href="http://arxiv.org/abs/2011.11981" target="_blank">arXiv:2011.11981</a> [<a href="http://arxiv.org/pdf/2011.11981" target="_blank">pdf</a>]

<h2>WeiPS: a symmetric fusion model framework for large-scale online learning. (arXiv:2011.11983v1 [cs.LG])</h2>
<h3>Xiang Yu, Fuping Chu, Junqi Wu, Bo Huang</h3>
<p>The recommendation system is an important commercial application of machine
learning, where billions of feed views in the information flow every day. In
reality, the interaction between user and item usually makes user's interest
changing over time, thus many companies (e.g. ByteDance, Baidu, Alibaba, and
Weibo) employ online learning as an effective way to quickly capture user
interests. However, hundreds of billions of model parameters present online
learning with challenges for real-time model deployment. Besides, model
stability is another key point for online learning. To this end, we design and
implement a symmetric fusion online learning system framework called WeiPS,
which integrates model training and model inference. Specifically, WeiPS
carries out second level model deployment by streaming update mechanism to
satisfy the consistency requirement. Moreover, it uses multi-level fault
tolerance and real-time domino degradation to achieve high availability
requirement.
</p>
<a href="http://arxiv.org/abs/2011.11983" target="_blank">arXiv:2011.11983</a> [<a href="http://arxiv.org/pdf/2011.11983" target="_blank">pdf</a>]

<h2>Adam$^+$: A Stochastic Method with Adaptive Variance Reduction. (arXiv:2011.11985v1 [cs.LG])</h2>
<h3>Mingrui Liu, Wei Zhang, Francesco Orabona, Tianbao Yang</h3>
<p>Adam is a widely used stochastic optimization method for deep learning
applications. While practitioners prefer Adam because it requires less
parameter tuning, its use is problematic from a theoretical point of view since
it may not converge. Variants of Adam have been proposed with provable
convergence guarantee, but they tend not be competitive with Adam on the
practical performance. In this paper, we propose a new method named Adam$^+$
(pronounced as Adam-plus). Adam$^+$ retains some of the key components of Adam
but it also has several noticeable differences: (i) it does not maintain the
moving average of second moment estimate but instead computes the moving
average of first moment estimate at extrapolated points; (ii) its adaptive step
size is formed not by dividing the square root of second moment estimate but
instead by dividing the root of the norm of first moment estimate. As a result,
Adam$^+$ requires few parameter tuning, as Adam, but it enjoys a provable
convergence guarantee. Our analysis further shows that Adam$^+$ enjoys adaptive
variance reduction, i.e., the variance of the stochastic gradient estimator
reduces as the algorithm converges, hence enjoying an adaptive convergence. We
also propose a more general variant of Adam$^+$ with different adaptive step
sizes and establish their fast convergence rate. Our empirical studies on
various deep learning tasks, including image classification, language modeling,
and automatic speech recognition, demonstrate that Adam$^+$ significantly
outperforms Adam and achieves comparable performance with best-tuned SGD and
momentum SGD.
</p>
<a href="http://arxiv.org/abs/2011.11985" target="_blank">arXiv:2011.11985</a> [<a href="http://arxiv.org/pdf/2011.11985" target="_blank">pdf</a>]

<h2>Efficient Initial Pose-graph Generation for Global SfM. (arXiv:2011.11986v1 [cs.CV])</h2>
<h3>Daniel Barath, Dmytro Mishkin, Ivan Eichhardt, Ilia Shipachev, Jiri Matas</h3>
<p>We propose ways to speed up the initial pose-graph generation for global
Structure-from-Motion algorithms. To avoid forming tentative point
correspondences by FLANN and geometric verification by RANSAC, which are the
most time-consuming steps of the pose-graph creation, we propose two new
methods - built on the fact that image pairs usually are matched consecutively.
Thus, candidate relative poses can be recovered from paths in the partly-built
pose-graph. We propose a heuristic for the A* traversal, considering global
similarity of images and the quality of the pose-graph edges. Given a relative
pose from a path, descriptor-based feature matching is made "light-weight" by
exploiting the known epipolar geometry. To speed up PROSAC-based sampling when
RANSAC is applied, we propose a third method to order the correspondences by
their inlier probabilities from previous estimations. The algorithms are tested
on 402130 image pairs from the 1DSfM dataset and they speed up the feature
matching 17 times and pose estimation 5 times.
</p>
<a href="http://arxiv.org/abs/2011.11986" target="_blank">arXiv:2011.11986</a> [<a href="http://arxiv.org/pdf/2011.11986" target="_blank">pdf</a>]

<h2>Discovering Avoidable Planner Failures of Autonomous Vehicles using Counterfactual Analysis in Behaviorally Diverse Simulation. (arXiv:2011.11991v1 [cs.LG])</h2>
<h3>Daisuke Nishiyama, Mario Ynocente Castro, Shirou Maruyama, Shinya Shiroshita, Karim Hamzaoui, Yi Ouyang, Guy Rosman, Jonathan DeCastro, Kuan-Hui Lee, Adrien Gaidon</h3>
<p>Automated Vehicles require exhaustive testing in simulation to detect as many
safety-critical failures as possible before deployment on public roads. In this
work, we focus on the core decision-making component of autonomous robots:
their planning algorithm. We introduce a planner testing framework that
leverages recent progress in simulating behaviorally diverse traffic
participants. Using large scale search, we generate, detect, and characterize
dynamic scenarios leading to collisions. In particular, we propose methods to
distinguish between unavoidable and avoidable accidents, focusing especially on
automatically finding planner-specific defects that must be corrected before
deployment. Through experiments in complex multi-agent intersection scenarios,
we show that our method can indeed find a wide range of critical planner
failures.
</p>
<a href="http://arxiv.org/abs/2011.11991" target="_blank">arXiv:2011.11991</a> [<a href="http://arxiv.org/pdf/2011.11991" target="_blank">pdf</a>]

<h2>Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes. (arXiv:2011.12001v1 [cs.CV])</h2>
<h3>Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu</h3>
<p>3D object detection has attracted much attention thanks to the advances in
sensors and deep learning methods for point clouds. Current state-of-the-art
methods like VoteNet regress direct offset towards object centers and box
orientations with an additional Multi-Layer-Perceptron network. Both their
offset and orientation predictions are not accurate due to the fundamental
difficulty in rotation classification. In the work, we disentangle the direct
offset into Local Canonical Coordinates (LCC), box scales and box orientations.
Only LCC and box scales are regressed while box orientations are generated by a
canonical voting scheme. Finally, a LCC-aware back-projection checking
algorithm iteratively cuts out bounding boxes from the generated vote maps,
with the elimination of false positives. Our model achieves state-of-the-art
performance on challenging large-scale datasets of real point cloud scans:
ScanNet, SceneNN with 11.4 and 5.3 mAP improvement respectively. Code is
available on https://github.com/qq456cvb/CanonicalVoting.
</p>
<a href="http://arxiv.org/abs/2011.12001" target="_blank">arXiv:2011.12001</a> [<a href="http://arxiv.org/pdf/2011.12001" target="_blank">pdf</a>]

<h2>KShapeNet: Riemannian network on Kendall shape space for Skeleton based Action Recognition. (arXiv:2011.12004v1 [cs.CV])</h2>
<h3>Racha Friji, Hassen Drira, Faten Chaieb, Sebastian Kurtek, Hamza Kchok</h3>
<p>Deep Learning architectures, albeit successful in most computer vision tasks,
were designed for data with an underlying Euclidean structure, which is not
usually fulfilled since pre-processed data may lie on a non-linear space. In
this paper, we propose a geometry aware deep learning approach for
skeleton-based action recognition. Skeleton sequences are first modeled as
trajectories on Kendall's shape space and then mapped to the linear tangent
space. The resulting structured data are then fed to a deep learning
architecture, which includes a layer that optimizes over rigid and non rigid
transformations of the 3D skeletons, followed by a CNN-LSTM network. The
assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D
120, has proven that proposed approach outperforms existing geometric deep
learning methods and is competitive with respect to recently published
approaches.
</p>
<a href="http://arxiv.org/abs/2011.12004" target="_blank">arXiv:2011.12004</a> [<a href="http://arxiv.org/pdf/2011.12004" target="_blank">pdf</a>]

<h2>Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs. (arXiv:2011.12010v1 [cs.LG])</h2>
<h3>Cheng Wang, Carolin Lawrence, Mathias Niepert</h3>
<p>Uncertainty quantification is crucial for building reliable and trustable
machine learning systems. We propose to estimate uncertainty in recurrent
neural networks (RNNs) via stochastic discrete state transitions over recurrent
timesteps. The uncertainty of the model can be quantified by running a
prediction several times, each time sampling from the recurrent state
transition distribution, leading to potentially different results if the model
is uncertain. Alongside uncertainty quantification, our proposed method offers
several advantages in different settings. The proposed method can (1) learn
deterministic and probabilistic automata from data, (2) learn well-calibrated
models on real-world classification tasks, (3) improve the performance of
out-of-distribution detection, and (4) control the exploration-exploitation
trade-off in reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2011.12010" target="_blank">arXiv:2011.12010</a> [<a href="http://arxiv.org/pdf/2011.12010" target="_blank">pdf</a>]

<h2>RIN: Textured Human Model Recovery and Imitation with a Single Image. (arXiv:2011.12024v1 [cs.CV])</h2>
<h3>Haoxi Ran, Guangfu Wang, Li Lu</h3>
<p>Human imitation has become topical recently, driven by GAN's ability to
disentangle human pose and body content. However, the latest methods hardly
focus on 3D information, and to avoid self-occlusion, a massive amount of input
images are needed. In this paper, we propose RIN, a novel volume-based
framework for reconstructing a textured 3D model from a single picture and
imitating a subject with the generated model. Specifically, to estimate most of
the human texture, we propose a U-Net-like front-to-back translation network.
With both front and back images input, the textured volume recovery module
allows us to color a volumetric human. A sequence of 3D poses then guides the
colored volume via Flowable Disentangle Networks as a volume-to-volume
translation task. To project volumes to a 2D plane during training, we design a
differentiable depth-aware renderer. Our experiments demonstrate that our
volume-based model is adequate for human imitation, and the back view can be
estimated reliably using our network. While prior works based on either 2D pose
or semantic map often fail for the unstable appearance of a human, our
framework can still produce concrete results, which are competitive to those
imagined from multi-view input.
</p>
<a href="http://arxiv.org/abs/2011.12024" target="_blank">arXiv:2011.12024</a> [<a href="http://arxiv.org/pdf/2011.12024" target="_blank">pdf</a>]

<h2>SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation. (arXiv:2011.12025v1 [cs.CV])</h2>
<h3>Thomas Verelst, Tinne Tuytelaars</h3>
<p>SegBlocks reduces the computational cost of existing neural networks, by
dynamically adjusting the processing resolution of image regions based on their
complexity. Our method splits an image into blocks and downsamples blocks of
low complexity, reducing the number of operations and memory consumption. A
lightweight policy network, selecting the complex regions, is trained using
reinforcement learning. In addition, we introduce several modules implemented
in CUDA to process images in blocks. Most important, our novel BlockPad module
prevents the feature discontinuities at block borders of which existing methods
suffer, while keeping memory consumption under control. Our experiments on
Cityscapes and Mapillary Vistas semantic segmentation show that dynamically
processing images offers a better accuracy versus complexity trade-off compared
to static baselines of similar complexity. For instance, our method reduces the
number of floating-point operations of SwiftNet-RN18 by 60% and increases the
inference speed by 50%, with only 0.3% decrease in mIoU accuracy on Cityscapes.
</p>
<a href="http://arxiv.org/abs/2011.12025" target="_blank">arXiv:2011.12025</a> [<a href="http://arxiv.org/pdf/2011.12025" target="_blank">pdf</a>]

<h2>Adversarial Generation of Continuous Images. (arXiv:2011.12026v1 [cs.CV])</h2>
<h3>Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny</h3>
<p>In most existing learning systems, images are typically viewed as 2D pixel
arrays. However, in another paradigm gaining popularity, a 2D image is
represented as an implicit neural representation (INR) -- an MLP that predicts
an RGB pixel value given its (x,y) coordinate. In this paper, we propose two
novel architectural techniques for building INR-based image decoders:
factorized multiplicative modulation and multi-scale INRs, and use them to
build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs
for image generation were limited to MNIST-like datasets and do not scale to
complex real-world data. Our proposed architectural design improves the
performance of continuous image generators by x6-40 times and reaches FID
scores of 6.27 on LSUN bedroom 256x256 and 16.32 on FFHQ 1024x1024, greatly
reducing the gap between continuous image GANs and pixel-based ones. To the
best of our knowledge, these are the highest reported scores for an image
generator, that consists entirely of fully-connected layers. Apart from that,
we explore several exciting properties of INR-based decoders, like
out-of-the-box superresolution, meaningful image-space interpolation,
accelerated inference of low-resolution images, an ability to extrapolate
outside of image boundaries and strong geometric prior. The source code is
available at https://github.com/universome/inr-gan
</p>
<a href="http://arxiv.org/abs/2011.12026" target="_blank">arXiv:2011.12026</a> [<a href="http://arxiv.org/pdf/2011.12026" target="_blank">pdf</a>]

<h2>Revisiting Pixel-Wise Supervision for Face Anti-Spoofing. (arXiv:2011.12032v1 [cs.CV])</h2>
<h3>Zitong Yu, Xiaobai Li, Jingang Shi, Zhaoqiang Xia, Guoying Zhao</h3>
<p>Face anti-spoofing (FAS) plays a vital role in securing face recognition
systems from the presentation attacks (PAs). As more and more realistic PAs
with novel types spring up, it is necessary to develop robust algorithms for
detecting unknown attacks even in unseen scenarios. However, deep models
supervised by traditional binary loss (e.g., `0' for bonafide vs. `1' for PAs)
are weak in describing intrinsic and discriminative spoofing patterns.
Recently, pixel-wise supervision has been proposed for the FAS task, intending
to provide more fine-grained pixel/patch-level cues. In this paper, we firstly
give a comprehensive review and analysis about the existing pixel-wise
supervision methods for FAS. Then we propose a novel pyramid supervision, which
guides deep models to learn both local details and global semantics from
multi-scale spatial context. Extensive experiments are performed on five FAS
benchmark datasets to show that, without bells and whistles, the proposed
pyramid supervision could not only improve the performance beyond existing
pixel-wise supervision frameworks, but also enhance the model's
interpretability (i.e., locating the patch-level positions of PAs more
reasonably). Furthermore, elaborate studies are conducted for exploring the
efficacy of different architecture configurations with two kinds of pixel-wise
supervisions (binary mask and depth map supervisions), which provides
inspirable insights for future architecture/supervision design.
</p>
<a href="http://arxiv.org/abs/2011.12032" target="_blank">arXiv:2011.12032</a> [<a href="http://arxiv.org/pdf/2011.12032" target="_blank">pdf</a>]

<h2>Efficient Sampling for Predictor-Based Neural Architecture Search. (arXiv:2011.12043v1 [cs.LG])</h2>
<h3>Lukas Mauch, Stephen Tiedemann, Javier Alonso Garcia, Bac Nguyen Cong, Kazuki Yoshiyama, Fabien Cardinaux, Thomas Kemp</h3>
<p>Recently, predictor-based algorithms emerged as a promising approach for
neural architecture search (NAS). For NAS, we typically have to calculate the
validation accuracy of a large number of Deep Neural Networks (DNNs), what is
computationally complex. Predictor-based NAS algorithms address this problem.
They train a proxy model that can infer the validation accuracy of DNNs
directly from their network structure. During optimization, the proxy can be
used to narrow down the number of architectures for which the true validation
accuracy must be computed, what makes predictor-based algorithms sample
efficient. Usually, we compute the proxy for all DNNs in the network search
space and pick those that maximize the proxy as candidates for optimization.
However, that is intractable in practice, because the search spaces are often
very large and contain billions of network architectures. The contributions of
this paper are threefold: 1) We define a sample efficiency gain to compare
different predictor-based NAS algorithms. 2) We conduct experiments on the
NASBench-101 dataset and show that the sample efficiency of predictor-based
algorithms decreases dramatically if the proxy is only computed for a subset of
the search space. 3) We show that if we choose the subset of the search space
on which the proxy is evaluated in a smart way, the sample efficiency of the
original predictor-based algorithm that has access to the full search space can
be regained. This is an important step to make predictor-based NAS algorithms
useful, in practice.
</p>
<a href="http://arxiv.org/abs/2011.12043" target="_blank">arXiv:2011.12043</a> [<a href="http://arxiv.org/pdf/2011.12043" target="_blank">pdf</a>]

<h2>Effective and Sparse Count-Sketch via k-means clustering. (arXiv:2011.12046v1 [cs.LG])</h2>
<h3>Yuhan Wang (1), Zijian Lei (2), Liang Lan (3) (Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China)</h3>
<p>Count-sketch is a popular matrix sketching algorithm that can produce a
sketch of an input data matrix X in O(nnz(X))time where nnz(X) denotes the
number of non-zero entries in X. The sketched matrix will be much smaller than
X while preserving most of its properties. Therefore, count-sketch is widely
used for addressing high-dimensionality challenge in machine learning. However,
there are two main limitations of count-sketch: (1) The sketching matrix used
count-sketch is generated randomly which does not consider any intrinsic data
properties of X. This data-oblivious matrix sketching method could produce a
bad sketched matrix which will result in low accuracy for subsequent machine
learning tasks (e.g.classification); (2) For highly sparse input data,
count-sketch could produce a dense sketched data matrix. This dense sketch
matrix could make the subsequent machine learning tasks more computationally
expensive than on the original sparse data X. To address these two limitations,
we first show an interesting connection between count-sketch and k-means
clustering by analyzing the reconstruction error of the count-sketch method.
Based on our analysis, we propose to reduce the reconstruction error of
count-sketch by using k-means clustering algorithm to obtain the
low-dimensional sketched matrix. In addition, we propose to solve k-mean
clustering using gradient descent with -L1 ball projection to produce a sparse
sketched matrix. Our experimental results based on six real-life classification
datasets have demonstrated that our proposed method achieves higher accuracy
than the original count-sketch and other popular matrix sketching algorithms.
Our results also demonstrate that our method produces a sparser sketched data
matrix than other methods and therefore the prediction cost of our method will
be smaller than other matrix sketching methods.
</p>
<a href="http://arxiv.org/abs/2011.12046" target="_blank">arXiv:2011.12046</a> [<a href="http://arxiv.org/pdf/2011.12046" target="_blank">pdf</a>]

<h2>Infrared small target detection based on isotropic constraint under complex background. (arXiv:2011.12059v1 [cs.CV])</h2>
<h3>Fan Wang</h3>
<p>Infrared search and tracking (IRST) system has been widely concerned and
applied in the area of national defence. Small target detection under complex
background is a very challenging task in the development of system algorithm.
Low signal-to-clutter ratio (SCR) of target and the interference caused by
irregular background clutter make it difficult to get an accurate result. In
this paper, small targets are considered to have two characteristics of high
contrast and isotropy, and we propose a multilayer gray difference (MGD) method
constrained by isotropy. Firstly, the suspected regions are obtained through
MGD, and then the eigenvalues of the original image's Hessian matrix are
calculated to obtain the isotropy parameter of each region. Finally, those
regions do not meet the isotropic constraint condition are suppressed.
Experiments show that the proposed method is effective and superior to several
common methods in terms of signal-to-clutter ratio gain (SCRG) and receiver
operating characteristic (ROC) curve.
</p>
<a href="http://arxiv.org/abs/2011.12059" target="_blank">arXiv:2011.12059</a> [<a href="http://arxiv.org/pdf/2011.12059" target="_blank">pdf</a>]

<h2>Fuzzy Stochastic Timed Petri Nets for Causal properties representation. (arXiv:2011.12075v1 [cs.AI])</h2>
<h3>Alejandro Sobrino, Eduardo C. Garrido-Merchan, Cristina Puente</h3>
<p>Imagery is frequently used to model, represent and communicate knowledge. In
particular, graphs are one of the most powerful tools, being able to represent
relations between objects. Causal relations are frequently represented by
directed graphs, with nodes denoting causes and links denoting causal
influence. A causal graph is a skeletal picture, showing causal associations
and impact between entities. Common methods used for graphically representing
causal scenarios are neurons, truth tables, causal Bayesian networks, cognitive
maps and Petri Nets. Causality is often defined in terms of precedence (the
cause precedes the effect), concurrency (often, an effect is provoked
simultaneously by two or more causes), circularity (a cause provokes the effect
and the effect reinforces the cause) and imprecision (the presence of the cause
favors the effect, but not necessarily causes it). We will show that, even
though the traditional graphical models are able to represent separately some
of the properties aforementioned, they fail trying to illustrate indistinctly
all of them. To approach that gap, we will introduce Fuzzy Stochastic Timed
Petri Nets as a graphical tool able to represent time, co-occurrence, looping
and imprecision in causal flow.
</p>
<a href="http://arxiv.org/abs/2011.12075" target="_blank">arXiv:2011.12075</a> [<a href="http://arxiv.org/pdf/2011.12075" target="_blank">pdf</a>]

<h2>CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v1 [cs.CV])</h2>
<h3>Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee</h3>
<p>Learning to detect real-world anomalous events through video-level labels is
a challenging task due to the rare occurrence of anomalies as well as noise in
the labels. In this work, we propose a weakly supervised anomaly detection
method which has manifold contributions including1) a random batch based
training procedure to reduce inter-batch correlation, 2) a normalcy suppression
mechanism to minimize anomaly scores of the normal regions of a video by taking
into account the overall information available in one training batch, and 3) a
clustering distance based loss to contribute towards mitigating the label noise
and to produce better anomaly representations by encouraging our model to
generate distinct normal and anomalous clusters. The proposed method
obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and
ShanghaiTech datasets respectively, demonstrating its superiority over the
existing state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/2011.12077" target="_blank">arXiv:2011.12077</a> [<a href="http://arxiv.org/pdf/2011.12077" target="_blank">pdf</a>]

<h2>Multi-Features Guidance Network for partial-to-partial point cloud registration. (arXiv:2011.12079v1 [cs.CV])</h2>
<h3>Hongyuan Wang, Xiang Liu, Wen Kang, Zhiqiang Yan, Bingwen Wang, Qianhao Ning</h3>
<p>To eliminate the problems of large dimensional differences, big semantic gap,
and mutual interference caused by hybrid features, in this paper, we propose a
novel Multi-Features Guidance Network for partial-to-partial point cloud
registration(MFG). The proposed network mainly includes four parts: keypoints'
feature extraction, correspondences searching, correspondences credibility
computation, and SVD, among which correspondences searching and correspondence
credibility computation are the cores of the network. Unlike the previous work,
we utilize the shape features and the spatial coordinates to guide
correspondences search independently and fusing the matching results to obtain
the final matching matrix. In the correspondences credibility computation
module, based on the conflicted relationship between the features matching
matrix and the coordinates matching matrix, we score the reliability for each
correspondence, which can reduce the impact of mismatched or non-matched
points. Experimental results show that our network outperforms the current
state-of-the-art while maintaining computational efficiency.
</p>
<a href="http://arxiv.org/abs/2011.12079" target="_blank">arXiv:2011.12079</a> [<a href="http://arxiv.org/pdf/2011.12079" target="_blank">pdf</a>]

<h2>Computational efficient deep neural network with differential attention maps for facial action unit detection. (arXiv:2011.12082v1 [cs.CV])</h2>
<h3>Jing Chen, Chenhui Wang, Kejun Wang, Meichen Liu</h3>
<p>In this paper, we propose a computational efficient end-to-end training deep
neural network (CEDNN) model and spatial attention maps based on difference
images. Firstly, the difference image is generated by image processing. Then
five binary images of difference images are obtained using different
thresholds, which are used as spatial attention maps. We use group convolution
to reduce model complexity. Skip connection and $\text{1}\times \text{1}$
convolution are used to ensure good performance even if the network model is
not deep. As an input, spatial attention map can be selectively fed into the
input of each block. The feature maps tend to focus on the parts that are
related to the target task better. In addition, we only need to adjust the
parameters of classifier to train different numbers of AU. It can be easily
extended to varying datasets without increasing too much computation. A large
number of experimental results show that the proposed CEDNN is obviously better
than the traditional deep learning method on DISFA+ and CK+ datasets. After
adding spatial attention maps, the result is better than the most advanced AU
detection method. At the same time, the scale of the network is small, the
running speed is fast, and the requirement for experimental equipment is low.
</p>
<a href="http://arxiv.org/abs/2011.12082" target="_blank">arXiv:2011.12082</a> [<a href="http://arxiv.org/pdf/2011.12082" target="_blank">pdf</a>]

<h2>A Convenient Infinite Dimensional Framework for Generative Adversarial Learning. (arXiv:2011.12087v1 [cs.LG])</h2>
<h3>Hayk Asatryan, Hanno Gottschalk, Marieke Lippert, Matthias Rottmann</h3>
<p>In recent years, generative adversarial networks (GANs) have demonstrated
impressive experimental results while there are only a few works that foster
statistical learning theory for GANs. In this work, we propose an infinite
dimensional theoretical framework for generative adversarial learning. Assuming
the class of uniformly bounded $k$-times $\alpha$-H\"older differentiable and
uniformly positive densities, we show that the Rosenblatt transformation
induces an optimal generator, which is realizable in the hypothesis space of
$\alpha$-H\"older differentiable generators. With a consistent definition of
the hypothesis space of discriminators, we further show that in our framework
the Jensen-Shannon divergence between the distribution induced by the generator
from the adversarial learning procedure and the data generating distribution
converges to zero. Under sufficiently strict regularity assumptions on the
density of the data generating process, we also provide rates of convergence
based on concentration and chaining.
</p>
<a href="http://arxiv.org/abs/2011.12087" target="_blank">arXiv:2011.12087</a> [<a href="http://arxiv.org/pdf/2011.12087" target="_blank">pdf</a>]

<h2>AI Discovering a Coordinate System of Chemical Elements: Dual Representation by Variational Autoencoders. (arXiv:2011.12090v1 [cs.LG])</h2>
<h3>Alex Glushkovsky</h3>
<p>The periodic table is a fundamental representation of chemical elements that
plays essential theoretical and practical roles. The research article discusses
the experiences of unsupervised training of neural networks to represent
elements on the 2D latent space based on their electron configurations while
forcing disentanglement. To emphasize chemical properties of the elements, the
original data of electron configurations has been realigned towards the
outermost valence orbitals. Recognizing seven shells and four subshells, the
input data has been arranged as (7x4) images. Latent space representation has
been performed using a convolutional beta variational autoencoder (beta-VAE).
Despite discrete and sparse input data, the beta-VAE disentangles elements of
different periods, blocks, groups, and types, while retaining the order along
atomic numbers. In addition, it isolates outliers on the latent space that
turned out to be known cases of Madelung's rule violations for lanthanide and
actinide elements. Considering the generative capabilities of beta-VAE and
discrete input data, the supervised machine learning has been set to find out
if there are insightful patterns distinguishing electron configurations between
real elements and decoded artificial ones. Also, the article addresses the
capability of dual representation by autoencoders. Conventionally, autoencoders
represent observations of input data on the latent space. However, by
transposing and duplicating original input data, it is possible to represent
variables on the latent space as well. The latest can lead to the discovery of
meaningful patterns among input variables. Applying that unsupervised learning
for transposed data of electron configurations, the order of input variables
that has been arranged by the encoder on the latent space has turned out to
exactly match the sequence of Madelung's rule.
</p>
<a href="http://arxiv.org/abs/2011.12090" target="_blank">arXiv:2011.12090</a> [<a href="http://arxiv.org/pdf/2011.12090" target="_blank">pdf</a>]

<h2>SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries. (arXiv:2011.12091v1 [cs.CV])</h2>
<h3>Xirong Li, Fangming Zhou, Chaoxi Xu, Jiaqi Ji, Gang Yang</h3>
<p>Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search
(AVS), is a core theme in multimedia data management and retrieval. The success
of AVS counts on cross-modal representation learning that encodes both query
sentences and videos into common spaces for semantic similarity computation.
Inspired by the initial success of previously few works in combining multiple
sentence encoders, this paper takes a step forward by developing a new and
general method for effectively exploiting diverse sentence encoders. The
novelty of the proposed method, which we term Sentence Encoder Assembly (SEA),
is two-fold. First, different from prior art that use only a single common
space, SEA supports text-video matching in multiple encoder-specific common
spaces. Such a property prevents the matching from being dominated by a
specific encoder that produces an encoding vector much longer than other
encoders. Second, in order to explore complementarities among the individual
common spaces, we propose multi-space multi-loss learning. As extensive
experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)
show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to
implement. All this makes SEA an appealing solution for AVS and promising for
continuously advancing the task by harvesting new sentence encoders.
</p>
<a href="http://arxiv.org/abs/2011.12091" target="_blank">arXiv:2011.12091</a> [<a href="http://arxiv.org/pdf/2011.12091" target="_blank">pdf</a>]

<h2>Learning to Sample the Most Useful Training Patches from Images. (arXiv:2011.12097v1 [cs.CV])</h2>
<h3>Shuyang Sun, Liang Chen, Gregory Slabaugh, Philip Torr</h3>
<p>Some image restoration tasks like demosaicing require difficult training
samples to learn effective models. Existing methods attempt to address this
data training problem by manually collecting a new training dataset that
contains adequate hard samples, however, there are still hard and simple areas
even within one single image. In this paper, we present a data-driven approach
called PatchNet that learns to select the most useful patches from an image to
construct a new training set instead of manual or random selection. We show
that our simple idea automatically selects informative samples out from a
large-scale dataset, leading to a surprising 2.35dB generalisation gain in
terms of PSNR. In addition to its remarkable effectiveness, PatchNet is also
resource-friendly as it is applied only during training and therefore does not
require any additional computational cost during inference.
</p>
<a href="http://arxiv.org/abs/2011.12097" target="_blank">arXiv:2011.12097</a> [<a href="http://arxiv.org/pdf/2011.12097" target="_blank">pdf</a>]

<h2>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields. (arXiv:2011.12100v1 [cs.CV])</h2>
<h3>Michael Niemeyer, Andreas Geiger</h3>
<p>Deep generative models allow for photorealistic image synthesis at high
resolutions. But for many applications, this is not enough: content creation
also needs to be controllable. While several recent works investigate how to
disentangle underlying factors of variation in the data, most of them operate
in 2D and hence ignore that our world is three-dimensional. Further, only few
works consider the compositional nature of scenes. Our key hypothesis is that
incorporating a compositional 3D scene representation into the generative model
leads to more controllable image synthesis. Representing scenes as
compositional generative neural feature fields allows us to disentangle one or
multiple objects from the background as well as individual objects' shapes and
appearances while learning from unstructured and unposed image collections
without any additional supervision. Combining this scene representation with a
neural rendering pipeline yields a fast and realistic image synthesis model. As
evidenced by our experiments, our model is able to disentangle individual
objects and allows for translating and rotating them in the scene as well as
changing the camera pose.
</p>
<a href="http://arxiv.org/abs/2011.12100" target="_blank">arXiv:2011.12100</a> [<a href="http://arxiv.org/pdf/2011.12100" target="_blank">pdf</a>]

<h2>Do You Live a Healthy Life? Analyzing Lifestyle by Visual Life Logging. (arXiv:2011.12102v1 [cs.CV])</h2>
<h3>Qing Gao, Mingtao Pei, Hongyu Shen</h3>
<p>A healthy lifestyle is the key to better health and happiness and has a
considerable effect on quality of life and disease prevention. Current
lifelogging/egocentric datasets are not suitable for lifestyle analysis;
consequently, there is no research on lifestyle analysis in the field of
computer vision. In this work, we investigate the problem of lifestyle analysis
and build a visual lifelogging dataset for lifestyle analysis (VLDLA). The
VLDLA contains images captured by a wearable camera every 3 seconds from 8:00
am to 6:00 pm for seven days. In contrast to current lifelogging/egocentric
datasets, our dataset is suitable for lifestyle analysis as images are taken
with short intervals to capture activities of short duration; moreover, images
are taken continuously from morning to evening to record all the activities
performed by a user. Based on our dataset, we classify the user activities in
each frame and use three latent fluents of the user, which change over time and
are associated with activities, to measure the healthy degree of the user's
lifestyle. The scores for the three latent fluents are computed based on
recognized activities, and the healthy degree of the lifestyle for the day is
determined based on the scores for the latent fluents. Experimental results
show that our method can be used to analyze the healthiness of users'
lifestyles.
</p>
<a href="http://arxiv.org/abs/2011.12102" target="_blank">arXiv:2011.12102</a> [<a href="http://arxiv.org/pdf/2011.12102" target="_blank">pdf</a>]

<h2>Recurrent Multi-view Alignment Network for Unsupervised Surface Registration. (arXiv:2011.12104v1 [cs.CV])</h2>
<h3>Wanquan Feng, Juyong Zhang, Hongrui Cai, Haofei Xu, Junhui Hou, Hujun Bao</h3>
<p>Learning non-rigid registration in an end-to-end manner is challenging due to
the inherent high degrees of freedom and the lack of labeled training data. In
this paper, we resolve these two challenges simultaneously. First, we propose
to represent the non-rigid transformation with a point-wise combination of
several rigid transformations. This representation not only makes the solution
space well-constrained but also enables our method to be solved iteratively
with a recurrent framework, which greatly reduces the difficulty of learning.
Second, we introduce a differentiable loss function that measures the 3D shape
similarity on the projected multi-view 2D depth images so that our full
framework can be trained end-to-end without ground truth supervision. Extensive
experiments on several different datasets demonstrate that our proposed method
outperforms the previous state-of-the-art by a large margin.
</p>
<a href="http://arxiv.org/abs/2011.12104" target="_blank">arXiv:2011.12104</a> [<a href="http://arxiv.org/pdf/2011.12104" target="_blank">pdf</a>]

<h2>Achieving Sample-Efficient and Online-Training-Safe Deep Reinforcement Learning with Base Controllers. (arXiv:2011.12105v1 [cs.RO])</h2>
<h3>Minjian Xin, Guangming Wang, Zhe Liu, Hesheng Wang</h3>
<p>Application of Deep Reinforcement Learning (DRL) algorithms in real-world
robotic tasks faces many challenges. On the one hand, reward-shaping for
complex tasks is difficult and may result in sub-optimal performances. On the
other hand, a sparse-reward setting renders exploration inefficient, and
exploration using physical robots is of high-cost and unsafe. In this paper we
propose a method of learning challenging sparse-reward tasks utilizing existing
controllers. Built upon Deep Deterministic Policy Gradients (DDPG), our
algorithm incorporates the controllers into stages of exploration, Q-value
estimation as well as policy update. Through experiments ranging from stacking
blocks to cups, we present a straightforward way of synthesizing these
controllers, and show that the learned state-based or image-based policies
steadily outperform them. Compared to previous works of learning from
demonstrations, our method improves sample efficiency by orders of magnitude
and can learn online in a safe manner. Overall, our method bears the potential
of leveraging existing industrial robot manipulation systems to build more
flexible and intelligent controllers.
</p>
<a href="http://arxiv.org/abs/2011.12105" target="_blank">arXiv:2011.12105</a> [<a href="http://arxiv.org/pdf/2011.12105" target="_blank">pdf</a>]

<h2>Wide-angle Image Rectification: A Survey. (arXiv:2011.12108v1 [cs.CV])</h2>
<h3>Jinlong Fan, Jing Zhang, Stephen J. Maybank, Dacheng Tao</h3>
<p>Wide field-of-view (FOV) cameras, which capture a larger scene area than
narrow FOV cameras, are used in many applications including 3D reconstruction,
autonomous driving, and video surveillance. However, wide-angle images contain
distortions that violate the assumptions underlying pinhole camera models,
resulting in object distortion, difficulties in estimating scene distance,
area, and direction, and preventing the use of off-the-shelf deep models
trained on undistorted images for downstream computer vision tasks. Image
rectification, which aims to correct these distortions, can solve these
problems. In this paper, we comprehensively survey progress in wide-angle image
rectification from transformation models to rectification methods.
Specifically, we first present a detailed description and discussion of the
camera models used in different approaches. Then, we summarize several
distortion models including radial distortion and projection distortion. Next,
we review both traditional geometry-based image rectification methods and deep
learning-based methods, where the former formulate distortion parameter
estimation as an optimization problem and the latter treat it as a regression
problem by leveraging the power of deep neural networks. We evaluate the
performance of state-of-the-art methods on public datasets and show that
although both kinds of methods can achieve good results, these methods only
work well for specific camera models and distortion types. We also provide a
strong baseline model and carry out an empirical study of different distortion
models on synthetic datasets and real-world wide-angle images. Finally, we
discuss several potential research directions that are expected to further
advance this area in the future.
</p>
<a href="http://arxiv.org/abs/2011.12108" target="_blank">arXiv:2011.12108</a> [<a href="http://arxiv.org/pdf/2011.12108" target="_blank">pdf</a>]

<h2>Lipophilicity Prediction with Multitask Learning and Molecular Substructures Representation. (arXiv:2011.12117v1 [cs.LG])</h2>
<h3>Nina Lukashina, Alisa Alenicheva, Elizaveta Vlasova, Artem Kondiukov, Aigul Khakimova, Emil Magerramov, Nikita Churikov, Aleksei Shpilman</h3>
<p>Lipophilicity is one of the factors determining the permeability of the cell
membrane to a drug molecule. Hence, accurate lipophilicity prediction is an
essential step in the development of new drugs. In this paper, we introduce a
novel approach to encoding additional graph information by extracting molecular
substructures. By adding a set of generalized atomic features of these
substructures to an established Direct Message Passing Neural Network (D-MPNN)
we were able to achieve a new state-of-the-art result at the task of prediction
of two main lipophilicity coefficients, namely logP and logD descriptors. We
further improve our approach by employing a multitask approach to predict logP
and logD values simultaneously. Additionally, we present a study of the model
performance on symmetric and asymmetric molecules, that may yield insight for
further research.
</p>
<a href="http://arxiv.org/abs/2011.12117" target="_blank">arXiv:2011.12117</a> [<a href="http://arxiv.org/pdf/2011.12117" target="_blank">pdf</a>]

<h2>Deep learning for video game genre classification. (arXiv:2011.12143v1 [cs.CV])</h2>
<h3>Yuhang Jiang, Lukun Zheng</h3>
<p>Video game genre classification based on its cover and textual description
would be utterly beneficial to many modern identification, collocation, and
retrieval systems. At the same time, it is also an extremely challenging task
due to the following reasons: First, there exists a wide variety of video game
genres, many of which are not concretely defined. Second, video game covers
vary in many different ways such as colors, styles, textual information, etc,
even for games of the same genre. Third, cover designs and textual descriptions
may vary due to many external factors such as country, culture, target reader
populations, etc. With the growing competitiveness in the video game industry,
the cover designers and typographers push the cover designs to its limit in the
hope of attracting sales. The computer-based automatic video game genre
classification systems become a particularly exciting research topic in recent
years. In this paper, we propose a multi-modal deep learning framework to solve
this problem. The contribution of this paper is four-fold. First, we compiles a
large dataset consisting of 50,000 video games from 21 genres made of cover
images, description text, and title text and the genre information. Second,
image-based and text-based, state-of-the-art models are evaluated thoroughly
for the task of genre classification for video games. Third, we developed an
efficient and salable multi-modal framework based on both images and texts.
Fourth, a thorough analysis of the experimental results is given and future
works to improve the performance is suggested. The results show that the
multi-modal framework outperforms the current state-of-the-art image-based or
text-based models. Several challenges are outlined for this task. More efforts
and resources are needed for this classification task in order to reach a
satisfactory level.
</p>
<a href="http://arxiv.org/abs/2011.12143" target="_blank">arXiv:2011.12143</a> [<a href="http://arxiv.org/pdf/2011.12143" target="_blank">pdf</a>]

<h2>SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration. (arXiv:2011.12149v1 [cs.CV])</h2>
<h3>Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, Yulan Guo</h3>
<p>Extracting robust and general 3D local features is key to downstream tasks
such as point cloud registration and reconstruction. Existing learning-based
local descriptors are either sensitive to rotation transformations, or rely on
classical handcrafted features which are neither general nor representative. In
this paper, we introduce a new, yet conceptually simple, neural architecture,
termed SpinNet, to extract local features which are rotationally invariant
whilst sufficiently informative to enable accurate registration. A Spatial
Point Transformer is first introduced to map the input local surface into a
carefully designed cylindrical space, enabling end-to-end optimization with
SO(2) equivariant representation. A Neural Feature Extractor which leverages
the powerful point-based and 3D cylindrical convolutional neural layers is then
utilized to derive a compact and representative descriptor for matching.
Extensive experiments on both indoor and outdoor datasets demonstrate that
SpinNet outperforms existing state-of-the-art techniques by a large margin.
More critically, it has the best generalization ability across unseen scenarios
with different sensor modalities. The code is available at
https://github.com/QingyongHu/SpinNet.
</p>
<a href="http://arxiv.org/abs/2011.12149" target="_blank">arXiv:2011.12149</a> [<a href="http://arxiv.org/pdf/2011.12149" target="_blank">pdf</a>]

<h2>Tensor Kernel Recovery for Spatio-Temporal Hawkes Processes. (arXiv:2011.12151v1 [stat.ML])</h2>
<h3>Heejune Sheen, Xiaonan Zhu, Yao Xie</h3>
<p>We estimate the general influence functions for spatio-temporal Hawkes
processes using a tensor recovery approach by formulating the location
dependent influence function that captures the influence of historical events
as a tensor kernel. We assume a low-rank structure for the tensor kernel and
cast the estimation problem as a convex optimization problem using the Fourier
transformed nuclear norm (TNN). We provide theoretical performance guarantees
for our approach and present an algorithm to solve the optimization problem. We
demonstrate the efficiency of our estimation with numerical simulations.
</p>
<a href="http://arxiv.org/abs/2011.12151" target="_blank">arXiv:2011.12151</a> [<a href="http://arxiv.org/pdf/2011.12151" target="_blank">pdf</a>]

<h2>VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval. (arXiv:2011.12172v1 [cs.CV])</h2>
<h3>Sijie Zhu, Taojiannan Yang, Chen Chen</h3>
<p>Cross-view image geo-localization aims to determine the locations of
street-view query images by matching with GPS-tagged reference images from
aerial view. Recent works have achieved surprisingly high retrieval accuracy on
city-scale datasets. However, these results rely on the assumption that there
exists a reference image exactly centered at the location of any query image,
which is not applicable for practical scenarios. In this paper, we redefine
this problem with a more realistic assumption that the query image can be
arbitrary in the area of interest and the reference images are captured before
the queries emerge. This assumption breaks the one-to-one retrieval setting of
existing datasets as the queries and reference images are not perfectly aligned
pairs, and there may be multiple reference images covering one query location.
To bridge the gap between this realistic setting and existing datasets, we
propose a new large-scale benchmark -- VIGOR -- for cross-View Image
Geo-localization beyond One-to-one Retrieval. We benchmark existing
state-of-the-art methods and propose a novel end-to-end framework to localize
the query in a coarse-to-fine manner. Apart from the image-level retrieval
accuracy, we also evaluate the localization accuracy in terms of the actual
distance (meters) using the raw GPS data. Extensive experiments are conducted
under different application scenarios to validate the effectiveness of the
proposed method. The results indicate that cross-view geo-localization in this
realistic setting is still challenging, fostering new research in this
direction. Our dataset and code will be publicly available.
</p>
<a href="http://arxiv.org/abs/2011.12172" target="_blank">arXiv:2011.12172</a> [<a href="http://arxiv.org/pdf/2011.12172" target="_blank">pdf</a>]

<h2>xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs. (arXiv:2011.12193v1 [cs.LG])</h2>
<h3>Susie Xi Rao, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Zhiyao Chen, Yinan Shan, Yang Zhao, Ce Zhang</h3>
<p>At online retail platforms, it is crucial to actively detect risks of
fraudulent transactions to improve our customer experience, minimize loss, and
prevent unauthorized chargebacks. Traditional rule-based methods and simple
feature-based models are either inefficient or brittle and uninterpretable. The
graph structure that exists among the heterogeneous typed entities of the
transaction logs is informative and difficult to fake. To utilize the
heterogeneous graph relationships and enrich the explainability, we present
xFraud, an explainable Fraud transaction prediction system. xFraud is composed
of a predictor which learns expressive representations for malicious
transaction detection from the heterogeneous transaction graph via a
self-attentive heterogeneous graph neural network, and an explainer that
generates meaningful and human understandable explanations from graphs to
facilitate further process in business unit. In our experiments with xFraud on
two real transaction networks with up to ten millions transactions, we are able
to achieve an area under a curve (AUC) score that outperforms baseline models
and graph embedding methods. In addition, we show how the explainer could
benefit the understanding towards model predictions and enhance model
trustworthiness for real-world fraud transaction cases.
</p>
<a href="http://arxiv.org/abs/2011.12193" target="_blank">arXiv:2011.12193</a> [<a href="http://arxiv.org/pdf/2011.12193" target="_blank">pdf</a>]

<h2>Making Graph Neural Networks Worth It for Low-Data Molecular Machine Learning. (arXiv:2011.12203v1 [cs.LG])</h2>
<h3>Aneesh Pappu, Brooks Paige</h3>
<p>Graph neural networks have become very popular for machine learning on
molecules due to the expressive power of their learnt representations. However,
molecular machine learning is a classically low-data regime and it isn't clear
that graph neural networks can avoid overfitting in low-resource settings. In
contrast, fingerprint methods are the traditional standard for low-data
environments due to their reduced number of parameters and manually engineered
features. In this work, we investigate whether graph neural networks are
competitive in small data settings compared to the parametrically 'cheaper'
alternative of fingerprint methods. When we find that they are not, we explore
pretraining and the meta-learning method MAML (and variants FO-MAML and ANIL)
for improving graph neural network performance by transfer learning from
related tasks. We find that MAML and FO-MAML do enable the graph neural network
to outperform models based on fingerprints, providing a path to using graph
neural networks even in settings with severely restricted data availability. In
contrast to previous work, we find ANIL performs worse that other meta-learning
approaches in this molecule setting. Our results suggest two reasons: molecular
machine learning tasks may require significant task-specific adaptation, and
distribution shifts in test tasks relative to train tasks may contribute to
worse ANIL performance.
</p>
<a href="http://arxiv.org/abs/2011.12203" target="_blank">arXiv:2011.12203</a> [<a href="http://arxiv.org/pdf/2011.12203" target="_blank">pdf</a>]

<h2>Minimum Variance Embedded Auto-associative Kernel Extreme Learning Machine for One-class Classification. (arXiv:2011.12208v1 [cs.LG])</h2>
<h3>Pratik K. Mishra, Chandan Gautam, Aruna Tiwari</h3>
<p>One-class classification (OCC) needs samples from only a single class to
train the classifier. Recently, an auto-associative kernel extreme learning
machine was developed for the OCC task. This paper introduces a novel extension
of this classifier by embedding minimum variance information within its
architecture and is referred to as VAAKELM. The minimum variance embedding
forces the network output weights to focus in regions of low variance and
reduces the intra-class variance. This leads to a better separation of target
samples and outliers, resulting in an improvement in the generalization
performance of the classifier. The proposed classifier follows a
reconstruction-based approach to OCC and minimizes the reconstruction error by
using the kernel extreme learning machine as the base classifier. It uses the
deviation in reconstruction error to identify the outliers. We perform
experiments on 15 small-size and 10 medium-size one-class benchmark datasets to
demonstrate the efficiency of the proposed classifier. We compare the results
with 13 existing one-class classifiers by considering the mean F1 score as the
comparison metric. The experimental results show that VAAKELM consistently
performs better than the existing classifiers, making it a viable alternative
for the OCC task.
</p>
<a href="http://arxiv.org/abs/2011.12208" target="_blank">arXiv:2011.12208</a> [<a href="http://arxiv.org/pdf/2011.12208" target="_blank">pdf</a>]

<h2>Searching for Interactions: Why the Laplace Kernel is your Friend. (arXiv:2011.12215v1 [cs.LG])</h2>
<h3>Keli Liu, Feng Ruan</h3>
<p>We tackle the problem of nonparametric variable selection with a focus on
discovering interactions between variables. With $p$ variables there are
$O(p^s)$ possible order-$s$ interactions making exhaustive search infeasible.
It is nonetheless possible to identify the variables involved in interactions
with only linear computation cost, $O(p)$. The trick is to maximize a class of
parametrized nonparametric dependence measures which we call \emph{metric
learning objectives}; the landscape of these nonconvex objective functions is
sensitive to interactions but the objectives themselves do not explicitly model
interactions. Three properties make metric learning objectives highly
attractive:

(a) The stationary points of the objective are automatically sparse (i.e.
performs selection)---no explicit $\ell_1$ penalization is needed.

(b) All stationary points of the objective exclude noise variables with high
probability.

(c) Guaranteed recovery of all signal variables without needing to reach the
objective's global maxima or special stationary points.

The second and third properties mean that all our theoretical results apply
in the practical case where one uses gradient ascent to maximize the metric
learning objective. While not all metric learning objectives enjoy good
statistical power, we design an objective based on $\ell_1$ kernels that does
exhibit favorable power: it recovers (i) main effects with $n \sim \log p$
samples, (ii) hierarchical interactions with $n \sim \log p$ samples and (iii)
order-$s$ pure interactions with $n \sim p^{2(s-1)}\log p$ samples.
</p>
<a href="http://arxiv.org/abs/2011.12215" target="_blank">arXiv:2011.12215</a> [<a href="http://arxiv.org/pdf/2011.12215" target="_blank">pdf</a>]

<h2>Energy-Based Models for Continual Learning. (arXiv:2011.12216v1 [cs.LG])</h2>
<h3>Shuang Li, Yilun Du, Gido M. van de Ven, Antonio Torralba, Igor Mordatch</h3>
<p>We motivate Energy-Based Models (EBMs) as a promising model class for
continual learning problems. Instead of tackling continual learning via the use
of external memory, growing models, or regularization, EBMs have a natural way
to support a dynamically-growing number of tasks or classes that causes less
interference with previously learned information. We find that EBMs outperform
the baseline methods by a large margin on several continual learning
benchmarks. We also show that EBMs are adaptable to a more general continual
learning setting where the data distribution changes without the notion of
explicitly delineated tasks. These observations point towards EBMs as a class
of models naturally inclined towards the continual learning regime.
</p>
<a href="http://arxiv.org/abs/2011.12216" target="_blank">arXiv:2011.12216</a> [<a href="http://arxiv.org/pdf/2011.12216" target="_blank">pdf</a>]

<h2>Generative Adversarial Stacked Autoencoders. (arXiv:2011.12236v1 [cs.LG])</h2>
<h3>Ariel Ruiz-Garcia, Ibrahim Almakky, Vasile Palade, Luke Hicks</h3>
<p>Generative Adversarial Networks (GANs) have become predominant in image
generation tasks. Their success is attributed to the training regime which
employs two models: a generator G and discriminator D that compete in a minimax
zero sum game. Nonetheless, GANs are difficult to train due to their
sensitivity to hyperparameter and parameter initialisation, which often leads
to vanishing gradients, non-convergence, or mode collapse, where the generator
is unable to create samples with different variations. In this work, we propose
a novel Generative Adversarial Stacked Convolutional Autoencoder(GASCA) model
and a generative adversarial gradual greedy layer-wise learning algorithm
de-signed to train Adversarial Autoencoders in an efficient and incremental
manner. Our training approach produces images with significantly lower
reconstruction error than vanilla joint training.
</p>
<a href="http://arxiv.org/abs/2011.12236" target="_blank">arXiv:2011.12236</a> [<a href="http://arxiv.org/pdf/2011.12236" target="_blank">pdf</a>]

<h2>Classification supporting COVID-19 diagnostics based on patient survey data. (arXiv:2011.12247v1 [cs.LG])</h2>
<h3>Joanna Henzel, Joanna Tobiasz, Micha&#x142; Kozielski, Ma&#x142;gorzata Bach, Pawe&#x142; Foszner, Aleksandra Gruca, Mateusz Kania, Justyna Mika, Anna Papiez, Aleksandra Werner, Joanna Zyla, Jerzy Jaroszewicz, Joanna Polanska, Marek Sikora</h3>
<p>Distinguishing COVID-19 from other flu-like illnesses can be difficult due to
ambiguous symptoms and still an initial experience of doctors. Whereas, it is
crucial to filter out those sick patients who do not need to be tested for
SARS-CoV-2 infection, especially in the event of the overwhelming increase in
disease. As a part of the presented research, logistic regression and XGBoost
classifiers, that allow for effective screening of patients for COVID-19, were
generated. Each of the methods was tuned to achieve an assumed acceptable
threshold of negative predictive values during classification. Additionally, an
explanation of the obtained classification models was presented. The
explanation enables the users to understand what was the basis of the decision
made by the model. The obtained classification models provided the basis for
the DECODE service (decode.polsl.pl), which can serve as support in screening
patients with COVID-19 disease. Moreover, the data set constituting the basis
for the analyses performed is made available to the research community. This
data set consisting of more than 3,000 examples is based on questionnaires
collected at a hospital in Poland.
</p>
<a href="http://arxiv.org/abs/2011.12247" target="_blank">arXiv:2011.12247</a> [<a href="http://arxiv.org/pdf/2011.12247" target="_blank">pdf</a>]

<h2>Learning Navigation Skills for Legged Robots with Learned Robot Embeddings. (arXiv:2011.12255v1 [cs.RO])</h2>
<h3>Joanne Truong, Denis Yarats, Tianyu Li, Franziska Meier, Sonia Chernova, Dhruv Batra, Akshara Rai</h3>
<p>Navigation policies are commonly learned on idealized cylinder agents in
simulation, without modelling complex dynamics, like contact dynamics, arising
from the interaction between the robot and the environment. Such policies
perform poorly when deployed on complex and dynamic robots, such as legged
robots. In this work, we learn hierarchical navigation policies that account
for the low-level dynamics of legged robots, such as maximum speed, slipping,
and achieve good performance at navigating cluttered indoor environments. Once
such a policy is learned on one legged robot, it does not directly generalize
to a different robot due to dynamical differences, which increases the cost of
learning such a policy on new robots. To overcome this challenge, we learn
dynamics-aware navigation policies across multiple robots with robot-specific
embeddings, which enable generalization to new unseen robots. We train our
policies across three legged robots - 2 quadrupeds (A1, AlienGo) and a hexapod
(Daisy). At test time, we study the performance of our learned policy on two
new legged robots (Laikago, 4-legged Daisy) and show that our learned policy
can sample-efficiently generalize to previously unseen robots.
</p>
<a href="http://arxiv.org/abs/2011.12255" target="_blank">arXiv:2011.12255</a> [<a href="http://arxiv.org/pdf/2011.12255" target="_blank">pdf</a>]

<h2>Multi-Stage CNN-Based Monocular 3D Vehicle Localization and Orientation Estimation. (arXiv:2011.12256v1 [cs.CV])</h2>
<h3>Ali Babolhavaeji, Mohammad Fanaei</h3>
<p>This paper aims to design a 3D object detection model from 2D images taken by
monocular cameras by combining the estimated bird's-eye view elevation map and
the deep representation of object features. The proposed model has a
pre-trained ResNet-50 network as its backend network and three more branches.
The model first builds a bird's-eye view elevation map to estimate the depth of
the object in the scene and by using that estimates the object's 3D bounding
boxes. We have trained and evaluate it on two major datasets: a syntactic
dataset and the KIITI dataset.
</p>
<a href="http://arxiv.org/abs/2011.12256" target="_blank">arXiv:2011.12256</a> [<a href="http://arxiv.org/pdf/2011.12256" target="_blank">pdf</a>]

<h2>Model Elicitation through Direct Questioning. (arXiv:2011.12262v1 [cs.AI])</h2>
<h3>Sachin Grover, David Smith, Subbarao Kambhampati</h3>
<p>The future will be replete with scenarios where humans are robots will be
working together in complex environments. Teammates interact, and the robot's
interaction has to be about getting useful information about the human's
(teammate's) model. There are many challenges before a robot can interact, such
as incorporating the structural differences in the human's model, ensuring
simpler responses, etc. In this paper, we investigate how a robot can interact
to localize the human model from a set of models. We show how to generate
questions to refine the robot's understanding of the teammate's model. We
evaluate the method in various planning domains. The evaluation shows that
these questions can be generated offline, and can help refine the model through
simple answers.
</p>
<a href="http://arxiv.org/abs/2011.12262" target="_blank">arXiv:2011.12262</a> [<a href="http://arxiv.org/pdf/2011.12262" target="_blank">pdf</a>]

<h2>Is First Person Vision Challenging for Object Tracking? The TREK-100 Benchmark Dataset. (arXiv:2011.12263v1 [cs.CV])</h2>
<h3>Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella, Christian Micheloni</h3>
<p>Understanding human-object interactions is fundamental in First Person Vision
(FPV). Tracking algorithms which follow the objects manipulated by the camera
wearer can provide useful information to effectively model such interactions.
Despite a few previous attempts to exploit trackers in FPV applications, a
systematic analysis of the performance of state-of-the-art trackers in this
domain is still missing. On the other hand, the visual tracking solutions
available in the computer vision literature have significantly improved their
performance in the last years for a large variety of target objects and
tracking scenarios. To fill the gap, in this paper, we present TREK-100, the
first benchmark dataset for visual object tracking in FPV. The dataset is
composed of 100 video sequences densely annotated with 60K bounding boxes, 17
sequence attributes, 13 action verb attributes and 29 target object attributes.
Along with the dataset, we present an extensive analysis of the performance of
30 among the best and most recent visual trackers. Our results show that object
tracking in FPV is challenging, which suggests that more research efforts
should be devoted to this problem.
</p>
<a href="http://arxiv.org/abs/2011.12263" target="_blank">arXiv:2011.12263</a> [<a href="http://arxiv.org/pdf/2011.12263" target="_blank">pdf</a>]

<h2>Constraint Based Refinement of Optical Flow. (arXiv:2011.12267v1 [cs.CV])</h2>
<h3>Hirak Doshi, N. Uday Kiran</h3>
<p>The goal of this paper is to formulate a general framework for a
constraint-based refinement of the optical flow using variational methods. We
demonstrate that for a particular choice of the constraint, formulated as a
minimization problem with the quadratic regularization, our results are close
to the continuity equation based fluid flow. This closeness to the continuity
model is theoretically justified through a modified augmented Lagrangian method
and validated numerically. Further, along with the continuity constraint, our
model can include geometric constraints as well. The correctness of our process
is studied in the Hilbert space setting. Moreover, a special feature of our
system is the possibility of a diagonalization by the Cauchy-Riemann operator
and transforming it to a diffusion process on the curl and the divergence of
the flow. Using the theory of semigroups on the decoupled system, we show that
our process preserves the spatial characteristics of the divergence and the
vorticities. We perform several numerical experiments and show the results on
different datasets.
</p>
<a href="http://arxiv.org/abs/2011.12267" target="_blank">arXiv:2011.12267</a> [<a href="http://arxiv.org/pdf/2011.12267" target="_blank">pdf</a>]

<h2>Insights From A Large-Scale Database of Material Depictions In Paintings. (arXiv:2011.12276v1 [cs.CV])</h2>
<h3>Hubert Lin, Mitchell Van Zuijlen, Maarten W.A. Wijntjes, Sylvia C. Pont, Kavita Bala</h3>
<p>Deep learning has paved the way for strong recognition systems which are
often both trained on and applied to natural images. In this paper, we examine
the give-and-take relationship between such visual recognition systems and the
rich information available in the fine arts. First, we find that visual
recognition systems designed for natural images can work surprisingly well on
paintings. In particular, we find that interactive segmentation tools can be
used to cleanly annotate polygonal segments within paintings, a task which is
time consuming to undertake by hand. We also find that FasterRCNN, a model
which has been designed for object recognition in natural scenes, can be
quickly repurposed for detection of materials in paintings. Second, we show
that learning from paintings can be beneficial for neural networks that are
intended to be used on natural images. We find that training on paintings
instead of natural images can improve the quality of learned features and we
further find that a large number of paintings can be a valuable source of test
data for evaluating domain adaptation algorithms. Our experiments are based on
a novel large-scale annotated database of material depictions in paintings
which we detail in a separate manuscript.
</p>
<a href="http://arxiv.org/abs/2011.12276" target="_blank">arXiv:2011.12276</a> [<a href="http://arxiv.org/pdf/2011.12276" target="_blank">pdf</a>]

<h2>MicroNet: Towards Image Recognition with Extremely Low FLOPs. (arXiv:2011.12289v1 [cs.CV])</h2>
<h3>Yunsheng Li, Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Lu Yuan, Zicheng Liu, Lei Zhang, Nuno Vasconcelos</h3>
<p>In this paper, we present MicroNet, which is an efficient convolutional
neural network using extremely low computational cost (e.g. 6 MFLOPs on
ImageNet classification). Such a low cost network is highly desired on edge
devices, yet usually suffers from a significant performance degradation. We
handle the extremely low FLOPs based upon two design principles: (a) avoiding
the reduction of network width by lowering the node connectivity, and (b)
compensating for the reduction of network depth by introducing more complex
non-linearity per layer. Firstly, we propose Micro-Factorized convolution to
factorize both pointwise and depthwise convolutions into low rank matrices for
a good tradeoff between the number of channels and input/output connectivity.
Secondly, we propose a new activation function, named Dynamic Shift-Max, to
improve the non-linearity via maxing out multiple dynamic fusions between an
input feature map and its circular channel shift. The fusions are dynamic as
their parameters are adapted to the input. Building upon Micro-Factorized
convolution and dynamic Shift-Max, a family of MicroNets achieve a significant
performance gain over the state-of-the-art in the low FLOP regime. For
instance, MicroNet-M1 achieves 61.1% top-1 accuracy on ImageNet classification
with 12 MFLOPs, outperforming MobileNetV3 by 11.3%.
</p>
<a href="http://arxiv.org/abs/2011.12289" target="_blank">arXiv:2011.12289</a> [<a href="http://arxiv.org/pdf/2011.12289" target="_blank">pdf</a>]

<h2>Speech Recognition: Keyword Spotting Through Image Recognition. (arXiv:1803.03759v2 [stat.ML] UPDATED)</h2>
<h3>Sanjay Krishna Gouda, Salil Kanetkar, David Harrison, Manfred K Warmuth</h3>
<p>The problem of identifying voice commands has always been a challenge due to
the presence of noise and variability in speed, pitch, etc. We will compare the
efficacies of several neural network architectures for the speech recognition
problem. In particular, we will build a model to determine whether a one second
audio clip contains a particular word (out of a set of 10), an unknown word, or
silence. The models to be implemented are a CNN recommended by the Tensorflow
Speech Recognition tutorial, a low-latency CNN, and an adversarially trained
CNN. The result is a demonstration of how to convert a problem in audio
recognition to the better-studied domain of image classification, where the
powerful techniques of convolutional neural networks are fully developed.
Additionally, we demonstrate the applicability of the technique of Virtual
Adversarial Training (VAT) to this problem domain, functioning as a powerful
regularizer with promising potential future applications.
</p>
<a href="http://arxiv.org/abs/1803.03759" target="_blank">arXiv:1803.03759</a> [<a href="http://arxiv.org/pdf/1803.03759" target="_blank">pdf</a>]

<h2>Task dependent Deep LDA pruning of neural networks. (arXiv:1803.08134v6 [cs.CV] UPDATED)</h2>
<h3>Qing Tian, Tal Arbel, James J. Clark</h3>
<p>With deep learning's success, a limited number of popular deep nets have been
widely adopted for various vision tasks. However, this usually results in
unnecessarily high complexities and possibly many features of low task utility.
In this paper, we address this problem by introducing a task-dependent deep
pruning framework based on Fisher's Linear Discriminant Analysis (LDA). The
approach can be applied to convolutional, fully-connected, and module-based
deep network structures, in all cases leveraging the high decorrelation of
neuron motifs found in the pre-decision space and cross-layer deconv
dependency. Moreover, we examine our approach's potential in network
architecture search for specific tasks and analyze the influence of our pruning
on model robustness to noises and adversarial attacks. Experimental results on
datasets of generic objects (ImageNet, CIFAR100) as well as domain specific
tasks (Adience, and LFWA) illustrate our framework's superior performance over
state-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet,
MobileNet). The proposed method successfully maintains comparable accuracies
even after discarding most parameters (98%-99% for VGG16, up to 82% for the
already compact InceptionNet) and with significant FLOP reductions (83% for
VGG16, up to 64% for InceptionNet). Through pruning, we can also derive
smaller, but more accurate and more robust models suitable for the task.
</p>
<a href="http://arxiv.org/abs/1803.08134" target="_blank">arXiv:1803.08134</a> [<a href="http://arxiv.org/pdf/1803.08134" target="_blank">pdf</a>]

<h2>Channel selection using Gumbel Softmax. (arXiv:1812.04180v4 [cs.CV] UPDATED)</h2>
<h3>Charles Herrmann, Richard Strong Bowen, Ramin Zabih</h3>
<p>Important applications such as mobile computing require reducing the
computational costs of neural network inference. Ideally, applications would
specify their preferred tradeoff between accuracy and speed, and the network
would optimize this end-to-end, using classification error to remove parts of
the network. Increasing speed can be done either during training - e.g.,
pruning filters - or during inference - e.g., conditionally executing a subset
of the layers. We propose a single end-to-end framework that can improve
inference efficiency in both settings. We use a combination of batch activation
loss and classification loss, and Gumbel reparameterization to learn network
structure. We train end-to-end, and the same technique supports pruning as well
as conditional computation. We obtain promising experimental results for
ImageNet classification with ResNet (45-52% less computation).
</p>
<a href="http://arxiv.org/abs/1812.04180" target="_blank">arXiv:1812.04180</a> [<a href="http://arxiv.org/pdf/1812.04180" target="_blank">pdf</a>]

<h2>Interactively shaping robot behaviour with unlabeled human instructions. (arXiv:1902.01670v2 [cs.LG] UPDATED)</h2>
<h3>Anis Najar, Olivier Sigaud, Mohamed Chetouani</h3>
<p>In this paper, we propose a framework that enables a human teacher to shape a
robot behaviour by interactively providing it with unlabeled instructions. We
ground the meaning of instruction signals in the task-learning process, and use
them simultaneously for guiding the latter. We implement our framework as a
modular architecture, named TICS (Task-Instruction-Contingency-Shaping) that
combines different information sources: a predefined reward function, human
evaluative feedback and unlabeled instructions. This approach provides a novel
perspective for robotic task learning that lies between Reinforcement Learning
and Supervised Learning paradigms. We evaluate our framework both in simulation
and with a real robot. The experimental results demonstrate the effectiveness
of our framework in accelerating the task-learning process and in reducing the
number of required teaching signals.
</p>
<a href="http://arxiv.org/abs/1902.01670" target="_blank">arXiv:1902.01670</a> [<a href="http://arxiv.org/pdf/1902.01670" target="_blank">pdf</a>]

<h2>BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames. (arXiv:1903.11779v2 [cs.CV] UPDATED)</h2>
<h3>Brent A. Griffin, Jason J. Corso</h3>
<p>Semi-supervised video object segmentation has made significant progress on
real and challenging videos in recent years. The current paradigm for
segmentation methods and benchmark datasets is to segment objects in video
provided a single annotation in the first frame. However, we find that
segmentation performance across the entire video varies dramatically when
selecting an alternative frame for annotation. This paper address the problem
of learning to suggest the single best frame across the video for user
annotation-this is, in fact, never the first frame of video. We achieve this by
introducing BubbleNets, a novel deep sorting network that learns to select
frames using a performance-based loss function that enables the conversion of
expansive amounts of training examples from already existing datasets. Using
BubbleNets, we are able to achieve an 11% relative improvement in segmentation
performance on the DAVIS benchmark without any changes to the underlying method
of segmentation.
</p>
<a href="http://arxiv.org/abs/1903.11779" target="_blank">arXiv:1903.11779</a> [<a href="http://arxiv.org/pdf/1903.11779" target="_blank">pdf</a>]

<h2>EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association. (arXiv:1904.11781v2 [cs.CV] UPDATED)</h2>
<h3>Michael Strecke, J&#xf6;rg St&#xfc;ckler</h3>
<p>The majority of approaches for acquiring dense 3D environment maps with RGB-D
cameras assumes static environments or rejects moving objects as outliers. The
representation and tracking of moving objects, however, has significant
potential for applications in robotics or augmented reality. In this paper, we
propose a novel approach to dynamic SLAM with dense object-level
representations. We represent rigid objects in local volumetric signed distance
function (SDF) maps, and formulate multi-object tracking as direct alignment of
RGB-D images with the SDF representations. Our main novelty is a probabilistic
formulation which naturally leads to strategies for data association and
occlusion handling. We analyze our approach in experiments and demonstrate that
our approach compares favorably with the state-of-the-art methods in terms of
robustness and accuracy.
</p>
<a href="http://arxiv.org/abs/1904.11781" target="_blank">arXiv:1904.11781</a> [<a href="http://arxiv.org/pdf/1904.11781" target="_blank">pdf</a>]

<h2>Shapley Q-value: A Local Reward Approach to Solve Global Reward Games. (arXiv:1907.05707v5 [cs.LG] UPDATED)</h2>
<h3>Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu</h3>
<p>Cooperative game is a critical research area in the multi-agent reinforcement
learning (MARL). Global reward game is a subclass of cooperative games, where
all agents aim to maximize the global reward. Credit assignment is an important
problem studied in the global reward game. Most of previous works stood by the
view of non-cooperative-game theoretical framework with the shared reward
approach, i.e., each agent being assigned a shared global reward directly.
This, however, may give each agent an inaccurate reward on its contribution to
the group, which could cause inefficient learning. To deal with this problem,
we i) introduce a cooperative-game theoretical framework called extended convex
game (ECG) that is a superset of global reward game, and ii) propose a local
reward approach called Shapley Q-value. Shapley Q-value is able to distribute
the global reward, reflecting each agent's own contribution in contrast to the
shared reward approach. Moreover, we derive an MARL algorithm called Shapley
Q-value deep deterministic policy gradient (SQDDPG), using Shapley Q-value as
the critic for each agent. We evaluate SQDDPG on Cooperative Navigation,
Prey-and-Predator and Traffic Junction, compared with the state-of-the-art
algorithms, e.g., MADDPG, COMA, Independent DDPG and Independent A2C. In the
experiments, SQDDPG shows a significant improvement on the convergence rate.
Finally, we plot Shapley Q-value and validate the property of fair credit
assignment.
</p>
<a href="http://arxiv.org/abs/1907.05707" target="_blank">arXiv:1907.05707</a> [<a href="http://arxiv.org/pdf/1907.05707" target="_blank">pdf</a>]

<h2>Deep Evidential Regression. (arXiv:1910.02600v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Amini, Wilko Schwarting, Ava Soleimany, Daniela Rus</h3>
<p>Deterministic neural networks (NNs) are increasingly being deployed in safety
critical domains, where calibrated, robust, and efficient measures of
uncertainty are crucial. In this paper, we propose a novel method for training
non-Bayesian NNs to estimate a continuous target as well as its associated
evidence in order to learn both aleatoric and epistemic uncertainty. We
accomplish this by placing evidential priors over the original Gaussian
likelihood function and training the NN to infer the hyperparameters of the
evidential distribution. We additionally impose priors during training such
that the model is regularized when its predicted evidence is not aligned with
the correct output. Our method does not rely on sampling during inference or on
out-of-distribution (OOD) examples for training, thus enabling efficient and
scalable uncertainty learning. We demonstrate learning well-calibrated measures
of uncertainty on various benchmarks, scaling to complex computer vision tasks,
as well as robustness to adversarial and OOD test samples.
</p>
<a href="http://arxiv.org/abs/1910.02600" target="_blank">arXiv:1910.02600</a> [<a href="http://arxiv.org/pdf/1910.02600" target="_blank">pdf</a>]

<h2>Gradient penalty from a maximum margin perspective. (arXiv:1910.06922v2 [cs.LG] UPDATED)</h2>
<h3>Alexia Jolicoeur-Martineau, Ioannis Mitliagkas</h3>
<p>A popular heuristic for improved performance in Generative adversarial
networks (GANs) is to use some form of gradient penalty on the discriminator.
This gradient penalty was originally motivated by a Wasserstein distance
formulation. However, the use of gradient penalty in other GAN formulations is
not well motivated. We present a unifying framework of expected margin
maximization and show that a wide range of gradient-penalized GANs (e.g.,
Wasserstein, Standard, Least-Squares, and Hinge GANs) can be derived from this
framework. Our results imply that employing gradient penalties induces a
large-margin classifier (thus, a large-margin discriminator in GANs). We
describe how expected margin maximization helps reduce vanishing gradients at
fake (generated) samples, a known problem in GANs. From this framework, we
derive a new $L^\infty$ gradient norm penalty with Hinge loss which generally
produces equally good (or better) generated output in GANs than $L^2$-norm
penalties (based on the Fr\'echet Inception Distance).
</p>
<a href="http://arxiv.org/abs/1910.06922" target="_blank">arXiv:1910.06922</a> [<a href="http://arxiv.org/pdf/1910.06922" target="_blank">pdf</a>]

<h2>Reduced-Order Modeling of Deep Neural Networks. (arXiv:1910.06995v4 [cs.LG] UPDATED)</h2>
<h3>Talgat Daulbaev, Julia Gusak, Evgeny Ponomarev, Andrzej Cichocki, Ivan Oseledets</h3>
<p>We introduce a new method for speeding up the inference of deep neural
networks. It is somewhat inspired by the reduced-order modeling techniques for
dynamical systems.The cornerstone of the proposed method is the maximum volume
algorithm. We demonstrate efficiency on neural networks pre-trained on
different datasets. We show that in many practical cases it is possible to
replace convolutional layers with much smaller fully-connected layers with a
relatively small drop in accuracy.
</p>
<a href="http://arxiv.org/abs/1910.06995" target="_blank">arXiv:1910.06995</a> [<a href="http://arxiv.org/pdf/1910.06995" target="_blank">pdf</a>]

<h2>Provably Convergent Two-Timescale Off-Policy Actor-Critic with Function Approximation. (arXiv:1911.04384v9 [cs.LG] UPDATED)</h2>
<h3>Shangtong Zhang, Bo Liu, Hengshuai Yao, Shimon Whiteson</h3>
<p>We present the first provably convergent two-timescale off-policy
actor-critic algorithm (COF-PAC) with function approximation. Key to COF-PAC is
the introduction of a new critic, the emphasis critic, which is trained via
Gradient Emphasis Learning (GEM), a novel combination of the key ideas of
Gradient Temporal Difference Learning and Emphatic Temporal Difference
Learning. With the help of the emphasis critic and the canonical value function
critic, we show convergence for COF-PAC, where the critics are linear and the
actor can be nonlinear.
</p>
<a href="http://arxiv.org/abs/1911.04384" target="_blank">arXiv:1911.04384</a> [<a href="http://arxiv.org/pdf/1911.04384" target="_blank">pdf</a>]

<h2>BSP-Net: Generating Compact Meshes via Binary Space Partitioning. (arXiv:1911.06971v5 [cs.CV] UPDATED)</h2>
<h3>Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang</h3>
<p>Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only
played a minor role in the deep learning revolution. Leading methods for
learning generative models of shapes rely on implicit functions, and generate
meshes only after expensive iso-surfacing routines. To overcome these
challenges, we are inspired by a classical spatial data structure from computer
graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core
ingredient of BSP is an operation for recursive subdivision of space to obtain
convex sets. By exploiting this property, we devise BSP-Net, a network that
learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net
is unsupervised since no convex shape decompositions are needed for training.
The network is trained to reconstruct a shape using a set of convexes obtained
from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can
be easily extracted to form a polygon mesh, without any need for iso-surfacing.
The generated meshes are compact (i.e., low-poly) and well suited to represent
sharp geometry; they are guaranteed to be watertight and can be easily
parameterized. We also show that the reconstruction quality by BSP-Net is
competitive with state-of-the-art methods while using much fewer primitives.
Code is available at https://github.com/czq142857/BSP-NET-original.
</p>
<a href="http://arxiv.org/abs/1911.06971" target="_blank">arXiv:1911.06971</a> [<a href="http://arxiv.org/pdf/1911.06971" target="_blank">pdf</a>]

<h2>Unsupervised Many-to-Many Image-to-Image Translation Across Multiple Domains. (arXiv:1911.12552v2 [cs.CV] UPDATED)</h2>
<h3>Ye Lin, Keren Fu, Shenggui Ling, Cheng Peng</h3>
<p>Unsupervised multi-domain image-to-image translation aims to synthesis images
among multiple domains without labeled data, which is more general and
complicated than one-to-one image mapping. However, existing methods mainly
focus on reducing the large costs of modeling and do not pay enough attention
to the quality of generated images. In some target domains, their translation
results may not be expected or even it has model collapse. To improve the image
quality, we propose an effective many-to-many mapping framework for
unsupervised multi-domain image-to-image translation. There are two key aspects
in our method. The first is a proposed many-to-many architecture with only one
domain-shared encoder and several domain-specialized decoders to effectively
and simultaneously translate images across multiple domains. The second is two
proposed constraints extended from one-to-one mappings to further help improve
the generation. All the evaluations demonstrate our framework is superior to
existing methods and provides an effective solution for multi-domain
image-to-image translation.
</p>
<a href="http://arxiv.org/abs/1911.12552" target="_blank">arXiv:1911.12552</a> [<a href="http://arxiv.org/pdf/1911.12552" target="_blank">pdf</a>]

<h2>Progressive Learning Algorithm for Efficient Person Re-Identification. (arXiv:1912.07447v2 [cs.CV] UPDATED)</h2>
<h3>Zhen Li, Hanyang Shao, Nian Xue, Liang Niu, LiangLiang Cao</h3>
<p>This paper studies the problem of Person Re-Identification (ReID)for
large-scale applications. Recent research efforts have been devoted to building
complicated part models, which introduce considerably high computational cost
and memory consumption, inhibiting its practicability in large-scale
applications. This paper aims to develop a novel learning strategy to find
efficient feature embeddings while maintaining the balance of accuracy and
model complexity. More specifically, we find by enhancing the classical triplet
loss together with cross-entropy loss, our method can explore the hard examples
and build a discriminant feature embedding yet compact enough for large-scale
applications. Our method is carried out progressively using Bayesian
optimization, and we call it the Progressive Learning Algorithm (PLA).
Extensive experiments on three large-scale datasets show that our PLA is
comparable or better than the-state-of-the-arts. Especially, on the challenging
Market-1501 dataset, we achieve Rank-1=94.7\%/mAP=89.4\% while saving at least
30\% parameters than strong part models.
</p>
<a href="http://arxiv.org/abs/1912.07447" target="_blank">arXiv:1912.07447</a> [<a href="http://arxiv.org/pdf/1912.07447" target="_blank">pdf</a>]

<h2>Causal query in observational data with hidden variables. (arXiv:2001.10269v4 [cs.AI] UPDATED)</h2>
<h3>Debo Cheng (1), Jiuyong Li (1), Lin Liu (1), Jixue Liu (1), Kui Yu (2), Thuc Duy Le (1) ((1) School of Information Technology and Mathematical Sciences, University of South Australia (2) School of Computer Science and Information Engineering, Hefei University of Technology)</h3>
<p>This paper discusses the problem of causal query in observational data with
hidden variables, with the aim of seeking the change of an outcome when
"manipulating" a variable while given a set of plausible confounding variables
which affect the manipulated variable and the outcome. Such an "experiment on
data" to estimate the causal effect of the manipulated variable is useful for
validating an experiment design using historical data or for exploring
confounders when studying a new relationship. However, existing data-driven
methods for causal effect estimation face some major challenges, including poor
scalability with high dimensional data, low estimation accuracy due to
heuristics used by the global causal structure learning algorithms, and the
assumption of causal sufficiency when hidden variables are inevitable in data.
In this paper, we develop a theorem for using local search to find a superset
of the adjustment (or confounding) variables for causal effect estimation from
observational data under a realistic pretreatment assumption. The theorem
ensures that the unbiased estimate of causal effect is included in the set of
causal effects estimated by the superset of adjustment variables. Based on the
developed theorem, we propose a data-driven algorithm for causal query.
Experiments show that the proposed algorithm is faster and produces better
causal effect estimation than an existing data-driven causal effect estimation
method with hidden variables. The causal effects estimated by the proposed
algorithm are as accurate as those by the state-of-the-art methods using domain
knowledge.
</p>
<a href="http://arxiv.org/abs/2001.10269" target="_blank">arXiv:2001.10269</a> [<a href="http://arxiv.org/pdf/2001.10269" target="_blank">pdf</a>]

<h2>A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima. (arXiv:2002.03495v13 [cs.LG] UPDATED)</h2>
<h3>Zeke Xie, Issei Sato, Masashi Sugiyama</h3>
<p>Stochastic Gradient Descent (SGD) and its variants are mainstream methods for
training deep networks in practice. SGD is known to find a flat minimum that
often generalizes well. However, it is mathematically unclear how deep learning
can select a flat minimum among so many minima. To answer the question
quantitatively, we develop a density diffusion theory (DDT) to reveal how
minima selection quantitatively depends on the minima sharpness and the
hyperparameters. To the best of our knowledge, we are the first to
theoretically and empirically prove that, benefited from the Hessian-dependent
covariance of stochastic gradient noise, SGD favors flat minima exponentially
more than sharp minima, while Gradient Descent (GD) with injected white noise
favors flat minima only polynomially more than sharp minima. We also reveal
that either a small learning rate or large-batch training requires
exponentially many iterations to escape from minima in terms of the ratio of
the batch size and learning rate. Thus, large-batch training cannot search flat
minima efficiently in a realistic computational time.
</p>
<a href="http://arxiv.org/abs/2002.03495" target="_blank">arXiv:2002.03495</a> [<a href="http://arxiv.org/pdf/2002.03495" target="_blank">pdf</a>]

<h2>No Regret Sample Selection with Noisy Labels. (arXiv:2003.03179v3 [cs.LG] UPDATED)</h2>
<h3>H. Song, N. Mitsuo, S. Uchida, D. Suehiro</h3>
<p>The Deep Neural Network (DNN) suffers from noisy labeled data because of the
risk of overfitting. To avoid the risk, in this paper, we propose a novel
sample selection framework for learning noisy samples. The core idea is to
employ a "regret" minimization approach. The proposed sample selection method
adaptively selects a subset of noisy labeled training samples to minimize the
regret of selecting noise samples. The algorithm works efficiently and performs
with theoretical support. Moreover, unlike the typical approaches, the
algorithm does not require any side information or learning information that
depends on the training settings of the DNN. The experimental results
demonstrate that the proposed method improves the performance of a black-box
DNN with noisy labeled data.
</p>
<a href="http://arxiv.org/abs/2003.03179" target="_blank">arXiv:2003.03179</a> [<a href="http://arxiv.org/pdf/2003.03179" target="_blank">pdf</a>]

<h2>On the Texture Bias for Few-Shot CNN Segmentation. (arXiv:2003.04052v2 [cs.CV] UPDATED)</h2>
<h3>Reza Azad, Abdur R Fayjie, Claude Kauffman, Ismail Ben Ayed, Marco Pedersoli, Jose Dolz</h3>
<p>Despite the initial belief that Convolutional Neural Networks (CNNs) are
driven by shapes to perform visual recognition tasks, recent evidence suggests
that texture bias in CNNs provides higher performing models when learning on
large labeled training datasets. This contrasts with the perceptual bias in the
human visual cortex, which has a stronger preference towards shape components.
Perceptual differences may explain why CNNs achieve human-level performance
when large labeled datasets are available, but their performance significantly
degrades in lowlabeled data scenarios, such as few-shot semantic segmentation.
To remove the texture bias in the context of few-shot learning, we propose a
novel architecture that integrates a set of Difference of Gaussians (DoG) to
attenuate high-frequency local components in the feature space. This produces a
set of modified feature maps, whose high-frequency components are diminished at
different standard deviation values of the Gaussian distribution in the spatial
domain. As this results in multiple feature maps for a single image, we employ
a bi-directional convolutional long-short-term-memory to efficiently merge the
multi scale-space representations. We perform extensive experiments on three
well-known few-shot segmentation benchmarks -- Pascal i5, COCO-20i and FSS-1000
-- and demonstrate that our method outperforms state-of-the-art approaches in
two datasets under the same conditions. The code is available at:
https://github.com/rezazad68/fewshot-segmentation
</p>
<a href="http://arxiv.org/abs/2003.04052" target="_blank">arXiv:2003.04052</a> [<a href="http://arxiv.org/pdf/2003.04052" target="_blank">pdf</a>]

<h2>Multiscale Sparsifying Transform Learning for Image Denoising. (arXiv:2003.11265v4 [cs.CV] UPDATED)</h2>
<h3>Ashkan Abbasi, Amirhassan Monadjemi, Leyuan Fang, Hossein Rabbani, Neda Noormohammadi, Yi Zhang</h3>
<p>The data-driven sparse methods such as synthesis dictionary learning and
sparsifying transform learning have been proven to be effective in image
denoising. However, these methods are intrinsically single-scale, which ignores
the multiscale nature of images. This often leads to suboptimal results. In
this paper, we propose several strategies to exploit multiscale information in
image denoising through the sparsifying transform learning denoising (TLD)
method. To this end, we first employ a simple method of denoising each wavelet
subband independently via TLD. Then, we show that this method can be greatly
enhanced using wavelet subbands mixing, which is a cheap fusion technique, to
combine the results of single-scale and multiscale methods. Finally, we remove
the need for denoising detail subbands. This simplification leads to an
efficient multiscale denoising method with competitive performance to its
baseline. The effectiveness of the proposed methods are experimentally shown
over two datasets: 1) classic test images corrupted with Gaussian noise, and 2)
fluorescence microscopy images corrupted with real Poisson-Gaussian noise. The
proposed multiscale methods improve over the single-scale baseline method by an
average of about 0.2 dB (in terms of PSNR) for removing synthetic Gaussian
noise form classic test images and real Poisson-Gaussian noise from microscopy
images, respectively. Interestingly, the proposed multiscale methods keep their
superiority over the baseline even when noise is relatively weak. More
importantly, we show that the proposed methods lead to visually pleasing
results, in which edges and textures are better recovered. Extensive
experiments over these two different datasets show that the proposed methods
offer a good trade-off between performance and complexity.
</p>
<a href="http://arxiv.org/abs/2003.11265" target="_blank">arXiv:2003.11265</a> [<a href="http://arxiv.org/pdf/2003.11265" target="_blank">pdf</a>]

<h2>FKAConv: Feature-Kernel Alignment for Point Cloud Convolution. (arXiv:2004.04462v3 [cs.CV] UPDATED)</h2>
<h3>Alexandre Boulch, Gilles Puy, Renaud Marlet</h3>
<p>Recent state-of-the-art methods for point cloud processing are based on the
notion of point convolution, for which several approaches have been proposed.
In this paper, inspired by discrete convolution in image processing, we provide
a formulation to relate and analyze a number of point convolution methods. We
also propose our own convolution variant, that separates the estimation of
geometry-less kernel weights and their alignment to the spatial support of
features. Additionally, we define a point sampling strategy for convolution
that is both effective and fast. Finally, using our convolution and sampling
strategy, we show competitive results on classification and semantic
segmentation benchmarks while being time and memory efficient.
</p>
<a href="http://arxiv.org/abs/2004.04462" target="_blank">arXiv:2004.04462</a> [<a href="http://arxiv.org/pdf/2004.04462" target="_blank">pdf</a>]

<h2>Where Does It End? -- Reasoning About Hidden Surfaces by Object Intersection Constraints. (arXiv:2004.04630v3 [cs.CV] UPDATED)</h2>
<h3>Michael Strecke, Joerg Stueckler</h3>
<p>Dynamic scene understanding is an essential capability in robotics and VR/AR.
In this paper we propose Co-Section, an optimization-based approach to 3D
dynamic scene reconstruction, which infers hidden shape information from
intersection constraints. An object-level dynamic SLAM frontend detects,
segments, tracks and maps dynamic objects in the scene. Our optimization
backend completes the shapes using hull and intersection constraints between
the objects. In experiments, we demonstrate our approach on real and synthetic
dynamic scene datasets. We also assess the shape completion performance of our
method quantitatively. To the best of our knowledge, our approach is the first
method to incorporate such physical plausibility constraints on object
intersections for shape completion of dynamic objects in an energy minimization
framework.
</p>
<a href="http://arxiv.org/abs/2004.04630" target="_blank">arXiv:2004.04630</a> [<a href="http://arxiv.org/pdf/2004.04630" target="_blank">pdf</a>]

<h2>Exemplar VAE: Linking Generative Models, Nearest Neighbor Retrieval, and Data Augmentation. (arXiv:2004.04795v3 [cs.LG] UPDATED)</h2>
<h3>Sajad Norouzi, David J. Fleet, Mohammad Norouzi</h3>
<p>We introduce Exemplar VAEs, a family of generative models that bridge the gap
between parametric and non-parametric, exemplar based generative models.
Exemplar VAE is a variant of VAE with a non-parametric prior in the latent
space based on a Parzen window estimator. To sample from it, one first draws a
random exemplar from a training set, then stochastically transforms that
exemplar into a latent code and a new observation. We propose retrieval
augmented training (RAT) as a way to speed up Exemplar VAE training by using
approximate nearest neighbor search in the latent space to define a lower bound
on log marginal likelihood. To enhance generalization, model parameters are
learned using exemplar leave-one-out and subsampling. Experiments demonstrate
the effectiveness of Exemplar VAEs on density estimation and representation
learning. Importantly, generative data augmentation using Exemplar VAEs on
permutation invariant MNIST and Fashion MNIST reduces classification error from
1.17% to 0.69% and from 8.56% to 8.16%.
</p>
<a href="http://arxiv.org/abs/2004.04795" target="_blank">arXiv:2004.04795</a> [<a href="http://arxiv.org/pdf/2004.04795" target="_blank">pdf</a>]

<h2>Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios. (arXiv:2004.06531v2 [cs.RO] UPDATED)</h2>
<h3>Baiming Chen, Xiang Chen, Wu Qiong, Liang Li</h3>
<p>Autonomous vehicles must be comprehensively evaluated before deployed in
cities and highways. However, most existing evaluation approaches for
autonomous vehicles are static and lack adaptability, so they are usually
inefficient in generating challenging scenarios for tested vehicles. In this
paper, we propose an adaptive evaluation framework to efficiently evaluate
autonomous vehicles in adversarial environments generated by deep reinforcement
learning. Considering the multimodal nature of dangerous scenarios, we use
ensemble models to represent different local optimums for diversity. We then
utilize a nonparametric Bayesian method to cluster the adversarial policies.
The proposed method is validated in a typical lane-change scenario that
involves frequent interactions between the ego vehicle and the surrounding
vehicles. Results show that the adversarial scenarios generated by our method
significantly degrade the performance of the tested vehicles. We also
illustrate different patterns of generated adversarial environments, which can
be used to infer the weaknesses of the tested vehicles.
</p>
<a href="http://arxiv.org/abs/2004.06531" target="_blank">arXiv:2004.06531</a> [<a href="http://arxiv.org/pdf/2004.06531" target="_blank">pdf</a>]

<h2>Incorporating Multiple Cluster Centers for Multi-Label Learning. (arXiv:2004.08113v2 [cs.LG] UPDATED)</h2>
<h3>Senlin Shu, Fengmao Lv, Lei Feng, Yan Yan, Shuo He, Jun He, Li Li</h3>
<p>Multi-label learning deals with the problem that each instance is associated
with multiple labels simultaneously. Most of the existing approaches aim to
improve the performance of multi-label learning by exploiting label
correlations. Although the data augmentation technique is widely used in many
machine learning tasks, it is still unclear whether data augmentation is
helpful to multi-label learning. In this paper, (to the best of our knowledge)
we provide the first attempt to leverage the data augmentation technique to
improve the performance of multi-label learning. Specifically, we first propose
a novel data augmentation approach that performs clustering on the real
examples and treats the cluster centers as virtual examples, and these virtual
examples naturally embody the local label correlations and label importances.
Then, motivated by the cluster assumption that examples in the same cluster
should have the same label, we propose a novel regularization term to bridge
the gap between the real examples and virtual examples, which can promote the
local smoothness of the learning function. Extensive experimental results on a
number of real-world multi-label data sets clearly demonstrate that our
proposed approach outperforms the state-of-the-art counterparts.
</p>
<a href="http://arxiv.org/abs/2004.08113" target="_blank">arXiv:2004.08113</a> [<a href="http://arxiv.org/pdf/2004.08113" target="_blank">pdf</a>]

<h2>Knowledge Refactoring for Inductive Program Synthesis. (arXiv:2004.09931v3 [cs.AI] UPDATED)</h2>
<h3>Sebastijan Dumancic, Tias Guns, Andrew Cropper</h3>
<p>Humans constantly restructure knowledge to use it more efficiently. Our goal
is to give a machine learning system similar abilities so that it can learn
more efficiently. We introduce the \textit{knowledge refactoring} problem,
where the goal is to restructure a learner's knowledge base to reduce its size
and to minimise redundancy in it. We focus on inductive logic programming,
where the knowledge base is a logic program. We introduce Knorf, a system which
solves the refactoring problem using constraint optimisation. We evaluate our
approach on two program induction domains: real-world string transformations
and building Lego structures. Our experiments show that learning from
refactored knowledge can improve predictive accuracies fourfold and reduce
learning times by half.
</p>
<a href="http://arxiv.org/abs/2004.09931" target="_blank">arXiv:2004.09931</a> [<a href="http://arxiv.org/pdf/2004.09931" target="_blank">pdf</a>]

<h2>Reinforcement learning with human advice: a survey. (arXiv:2005.11016v2 [cs.AI] UPDATED)</h2>
<h3>Anis Najar, Mohamed Chetouani</h3>
<p>In this paper, we provide an overview of the existing methods for integrating
human advice into a Reinforcement Learning process. We first propose a taxonomy
of the different forms of advice that can be provided to a learning agent. We
then describe the methods that can be used for interpreting advice when its
meaning is not determined beforehand. Finally, we review different approaches
for integrating advice into the learning process.
</p>
<a href="http://arxiv.org/abs/2005.11016" target="_blank">arXiv:2005.11016</a> [<a href="http://arxiv.org/pdf/2005.11016" target="_blank">pdf</a>]

<h2>Equivariant Maps for Hierarchical Structures. (arXiv:2006.03627v2 [cs.LG] UPDATED)</h2>
<h3>Renhao Wang, Marjan Albooyeh, Siamak Ravanbakhsh</h3>
<p>While using invariant and equivariant maps, it is possible to apply deep
learning to a range of primitive data structures, a formalism for dealing with
hierarchy is lacking. This is a significant issue because many practical
structures are hierarchies of simple building blocks; some examples include
sequences of sets, graphs of graphs, or multiresolution images. Observing that
the symmetry of a hierarchical structure is the "wreath product" of symmetries
of the building blocks, we express the equivariant map for the hierarchy using
an intuitive combination of the equivariant linear layers of the building
blocks. More generally, we show that any equivariant map for the hierarchy has
this form. To demonstrate the effectiveness of this approach to model design,
we consider its application in the semantic segmentation of point-cloud data.
By voxelizing the point cloud, we impose a hierarchy of translation and
permutation symmetries on the data and report state-of-the-art on Semantic3D,
S3DIS, and vKITTI, that include some of the largest real-world point-cloud
benchmarks.
</p>
<a href="http://arxiv.org/abs/2006.03627" target="_blank">arXiv:2006.03627</a> [<a href="http://arxiv.org/pdf/2006.03627" target="_blank">pdf</a>]

<h2>A Sliced Wasserstein Loss for Neural Texture Synthesis. (arXiv:2006.07229v3 [cs.CV] UPDATED)</h2>
<h3>Eric Heitz, Kenneth Vanhoey, Thomas Chambon, Laurent Belcour</h3>
<p>We address the problem of computing a textural loss based on the statistics
extracted from the feature activations of a convolutional neural network
optimized for object recognition (e.g. VGG-19). The underlying mathematical
problem is the measure of the distance between two distributions in feature
space. The Gram-matrix loss is the ubiquitous approximation for this problem
but it is subject to several shortcomings. Our goal is to promote the Sliced
Wasserstein Distance as a replacement for it. It is theoretically
proven,practical, simple to implement, and achieves results that are visually
superior for texture synthesis by optimization or training generative neural
networks.
</p>
<a href="http://arxiv.org/abs/2006.07229" target="_blank">arXiv:2006.07229</a> [<a href="http://arxiv.org/pdf/2006.07229" target="_blank">pdf</a>]

<h2>Multiscale Deep Equilibrium Models. (arXiv:2006.08656v2 [cs.LG] UPDATED)</h2>
<h3>Shaojie Bai, Vladlen Koltun, J. Zico Kolter</h3>
<p>We propose a new class of implicit networks, the multiscale deep equilibrium
model (MDEQ), suited to large-scale and highly hierarchical pattern recognition
domains. An MDEQ directly solves for and backpropagates through the equilibrium
points of multiple feature resolutions simultaneously, using implicit
differentiation to avoid storing intermediate states (and thus requiring only
$O(1)$ memory consumption). These simultaneously-learned multi-resolution
features allow us to train a single model on a diverse set of tasks and loss
functions, such as using a single MDEQ to perform both image classification and
semantic segmentation. We illustrate the effectiveness of this approach on two
large-scale vision tasks: ImageNet classification and semantic segmentation on
high-resolution images from the Cityscapes dataset. In both settings, MDEQs are
able to match or exceed the performance of recent competitive computer vision
models: the first time such performance and scale have been achieved by an
implicit deep learning approach. The code and pre-trained models are at
https://github.com/locuslab/mdeq .
</p>
<a href="http://arxiv.org/abs/2006.08656" target="_blank">arXiv:2006.08656</a> [<a href="http://arxiv.org/pdf/2006.08656" target="_blank">pdf</a>]

<h2>TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly Representations. (arXiv:2006.10187v2 [cs.CV] UPDATED)</h2>
<h3>Jiahao Pang, Duanshun Li, Dong Tian</h3>
<p>Topology matters. Despite the recent success of point cloud processing with
geometric deep learning, it remains arduous to capture the complex topologies
of point cloud data with a learning model. Given a point cloud dataset
containing objects with various genera, or scenes with multiple objects, we
propose an autoencoder, TearingNet, which tackles the challenging task of
representing the point clouds using a fixed-length descriptor. Unlike existing
works directly deforming predefined primitives of genus zero (e.g., a 2D square
patch) to an object-level point cloud, our TearingNet is characterized by a
proposed Tearing network module and a Folding network module interacting with
each other iteratively. Particularly, the Tearing network module learns the
point cloud topology explicitly. By breaking the edges of a primitive graph, it
tears the graph into patches or with holes to emulate the topology of a target
point cloud, leading to faithful reconstructions. Experimentation shows the
superiority of our proposal in terms of reconstructing point clouds as well as
generating more topology-friendly representations than benchmarks.
</p>
<a href="http://arxiv.org/abs/2006.10187" target="_blank">arXiv:2006.10187</a> [<a href="http://arxiv.org/pdf/2006.10187" target="_blank">pdf</a>]

<h2>SXL: Spatially explicit learning of geographic processes with auxiliary tasks. (arXiv:2006.10461v2 [cs.LG] UPDATED)</h2>
<h3>Konstantin Klemmer, Daniel B. Neill</h3>
<p>From earth system sciences to climate modeling and ecology, many of the
greatest empirical modeling challenges are geographic in nature. As these
processes are characterized by spatial dynamics, we can exploit their
autoregressive nature to inform learning algorithms. We introduce SXL, a method
for learning with geospatial data using explicitly spatial auxiliary tasks. We
embed the local Moran's I, a well-established measure of local spatial
autocorrelation, into the training process, "nudging" the model to learn the
direction and magnitude of local autoregressive effects in parallel with the
primary task. Further, we propose an expansion of Moran's I to multiple
resolutions to capture effects at different spatial granularities and over
varying distance scales. We show the superiority of this method for training
neural networks using experiments with real-world geospatial data in both
generative and predictive modeling tasks. The Moran's I embedding can be
constructed easily for any spatial, numerical input and our approach can be
used with arbitrary network architectures, consistently improving their
performance as shown by our experiments. We also outperform appropriate,
domain-specific spatial interpolation benchmarks. Our work highlights how
integrating the geographic information sciences and spatial statistics into
neural network models can address the specific challenges of spatial data. The
code for our experiments is available on Github:
https://github.com/konstantinklemmer/sxl.
</p>
<a href="http://arxiv.org/abs/2006.10461" target="_blank">arXiv:2006.10461</a> [<a href="http://arxiv.org/pdf/2006.10461" target="_blank">pdf</a>]

<h2>Task-agnostic Unsupervised Out-of-Distribution Detection Using Kernel Density Estimation. (arXiv:2006.10712v3 [cs.CV] UPDATED)</h2>
<h3>Ertunc Erdil, Krishna Chaitanya, Neerav Karani, Ender Konukoglu</h3>
<p>In the recent years, researchers proposed a number of successful methods to
perform out-of-distribution (OOD) detection in deep neural networks (DNNs). So
far the scope of the highly accurate methods has been limited to classification
tasks. Attempts for generally applicable methods beyond classification did not
attain similar performance. In this paper, we propose a task-agnostic
unsupervised OOD detection method using kernel density estimation (KDE) that
addresses this limitation. We estimate the probability density functions (pdfs)
of intermediate features of an already trained network, by performing KDE on
the training dataset. As direct application of KDE to feature maps is hindered
by their high dimensionality, we use a set of channel-wise marginalized KDE
models instead of a single high-dimensional one. At test time, we evaluate the
pdfs on a test sample and combine the resulting channel-wise scores with a
logistic regression into a final confidence score that indicates the sample is
OOD. Crucially, the proposed method is task agnostic as we only use
intermediate features without requiring information on class labels nor the
structure of the output, and attains high accuracy thanks to the flexibility of
KDE. We performed experiments on DNNs trained for segmentation, detection and
classification tasks, using benchmark datasets for OOD detection. The proposed
method substantially outperformed existing works for non-classification
networks while achieving on-par accuracy with the state-of-the-art for
classification networks. The results demonstrate that the proposed method
attains high OOD detection accuracy across different tasks, offering a larger
scope of applications than existing task-specific methods and improving
state-of-the-art for task-agnostic methods. The code will be made available.
</p>
<a href="http://arxiv.org/abs/2006.10712" target="_blank">arXiv:2006.10712</a> [<a href="http://arxiv.org/pdf/2006.10712" target="_blank">pdf</a>]

<h2>Telescoping Density-Ratio Estimation. (arXiv:2006.12204v2 [stat.ML] UPDATED)</h2>
<h3>Benjamin Rhodes, Kai Xu, Michael U. Gutmann</h3>
<p>Density-ratio estimation via classification is a cornerstone of unsupervised
learning. It has provided the foundation for state-of-the-art methods in
representation learning and generative modelling, with the number of use-cases
continuing to proliferate. However, it suffers from a critical limitation: it
fails to accurately estimate ratios p/q for which the two densities differ
significantly. Empirically, we find this occurs whenever the KL divergence
between p and q exceeds tens of nats. To resolve this limitation, we introduce
a new framework, telescoping density-ratio estimation (TRE), that enables the
estimation of ratios between highly dissimilar densities in high-dimensional
spaces. Our experiments demonstrate that TRE can yield substantial improvements
over existing single-ratio methods for mutual information estimation,
representation learning and energy-based modelling.
</p>
<a href="http://arxiv.org/abs/2006.12204" target="_blank">arXiv:2006.12204</a> [<a href="http://arxiv.org/pdf/2006.12204" target="_blank">pdf</a>]

<h2>Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v3 [cs.CV] UPDATED)</h2>
<h3>Youngwan Lee, Joong-won Hwang, Hyung-Il Kim, Kimin Yun, Yongjin Kwon</h3>
<p>Since many safety-critical systems, such as surgical robots and autonomous
driving cars, are in unstable environments with sensor noise and incomplete
data, it is desirable for object detectors to take into account the confidence
of localization prediction. There are three limitations of the prior
uncertainty estimation methods for anchor-based object detection. 1) They model
the uncertainty based on object properties having different characteristics,
such as location (center point) and scale (width, height). 2) they model a box
offset and ground-truth as Gaussian distribution and Dirac delta distribution,
which leads to the model misspecification problem. Because the Dirac delta
distribution is not exactly represented as Gaussian, i.e., for any $\mu$ and
$\Sigma$. 3) Since anchor-based methods are sensitive to hyper-parameters of
anchor, the localization uncertainty modeling is also sensitive to these
parameters. Therefore, we propose a new localization uncertainty estimation
method called Gaussian-FCOS for anchor-free object detection. Our method
captures the uncertainty based on four directions of box offsets~(left, right,
top, bottom) that have similar properties, which enables to capture which
direction is uncertain and provide a quantitative value in range~[0, 1]. To
this end, we design a new uncertainty loss, negative power log-likelihood loss,
to measure uncertainty by weighting IoU to the likelihood loss, which
alleviates the model misspecification problem. Experiments on COCO datasets
demonstrate that our Gaussian-FCOS reduces false positives and finds more
missing-objects by mitigating over-confidence scores with the estimated
uncertainty. We hope Gaussian-FCOS serves as a crucial component for the
reliability-required task.
</p>
<a href="http://arxiv.org/abs/2006.15607" target="_blank">arXiv:2006.15607</a> [<a href="http://arxiv.org/pdf/2006.15607" target="_blank">pdf</a>]

<h2>Adai: Separating the Effects of Adaptive Learning Rate and Momentum Inertia. (arXiv:2006.15815v8 [cs.LG] UPDATED)</h2>
<h3>Zeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, Masashi Sugiyama</h3>
<p>Adaptive Momentum Estimation (Adam), which combines Adaptive Learning Rate
and Momentum, is the most popular stochastic optimizer for accelerating
training of deep neural networks. But Adam often generalizes significantly
worse than Stochastic Gradient Descent (SGD). It is still mathematically
unclear how Adaptive Learning Rate and Momentum affect saddle-point escaping
and minima selection. Based on the diffusion theoretical framework, we decouple
the effects of Adaptive Learning Rate and Momentum on saddle-point escaping and
minima selection. We prove that Adaptive Learning Rate can escape saddle points
efficiently, but cannot select flat minima as SGD does. In contrast, Momentum
provides a momentum drift effect to help passing through saddle points, and
almost does not affect flat minima selection. This mathematically explains why
SGD (with Momentum) generalizes better, while Adam generalizes worse but
converges faster. We design a novel adaptive optimizer named Adaptive Inertia
Estimation (Adai), which uses parameter-wise adaptive inertia to accelerate
training and provably favors flat minima as much as SGD. Our real-world
experiments demonstrate that Adai can significantly outperform SGD and existing
Adam variants.
</p>
<a href="http://arxiv.org/abs/2006.15815" target="_blank">arXiv:2006.15815</a> [<a href="http://arxiv.org/pdf/2006.15815" target="_blank">pdf</a>]

<h2>Robust Processing-In-Memory Neural Networks via Noise-Aware Normalization. (arXiv:2007.03230v2 [cs.CV] UPDATED)</h2>
<h3>Li-Huang Tsai, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Cheng Juan</h3>
<p>Analog computing hardwares, such as Processing-in-memory (PIM) accelerators,
have gradually received more attention for accelerating the neural network
computations. However, PIM accelerators often suffer from intrinsic noise in
the physical components, making it challenging for neural network models to
achieve the same performance as on the digital hardware. Previous works in
mitigating intrinsic noise assumed the knowledge of the noise model, and
retraining the neural networks accordingly was required. In this paper, we
propose a noise-agnostic method to achieve robust neural network performance
against any noise setting. Our key observation is that the degradation of
performance is due to the distribution shifts in network activations, which are
caused by the noise. To properly track the shifts and calibrate the biased
distributions, we propose a "noise-aware" batch normalization layer, which is
able to align the distributions of the activations under variational noise
inherent in the analog environments. Our method is simple, easy to implement,
general to various noise settings, and does not need to retrain the models. We
conduct experiments on several tasks in computer vision, including
classification, object detection and semantic segmentation. The results
demonstrate the effectiveness of our method, achieving robust performance under
a wide range of noise settings, more reliable than existing methods. We believe
that our simple yet general method can facilitate the adoption of analog
computing devices for neural networks.
</p>
<a href="http://arxiv.org/abs/2007.03230" target="_blank">arXiv:2007.03230</a> [<a href="http://arxiv.org/pdf/2007.03230" target="_blank">pdf</a>]

<h2>EAGLE: Large-scale Vehicle Detection Dataset in Real-World Scenarios using Aerial Imagery. (arXiv:2007.06124v3 [cs.CV] UPDATED)</h2>
<h3>Seyed Majid Azimi, Reza Bahmanyar, Corenin Henry, Franz Kurz</h3>
<p>Multi-class vehicle detection from airborne imagery with orientation
estimation is an important task in the near and remote vision domains with
applications in traffic monitoring and disaster management. In the last decade,
we have witnessed significant progress in object detection in ground imagery,
but it is still in its infancy in airborne imagery, mostly due to the scarcity
of diverse and large-scale datasets. Despite being a useful tool for different
applications, current airborne datasets only partially reflect the challenges
of real-world scenarios. To address this issue, we introduce EAGLE (oriEnted
vehicle detection using Aerial imaGery in real-worLd scEnarios), a large-scale
dataset for multi-class vehicle detection with object orientation information
in aerial imagery. It features high-resolution aerial images composed of
different real-world situations with a wide variety of camera sensor,
resolution, flight altitude, weather, illumination, haze, shadow, time, city,
country, occlusion, and camera angle. The annotation was done by airborne
imagery experts with small- and large-vehicle classes. EAGLE contains 215,986
instances annotated with oriented bounding boxes defined by four points and
orientation, making it by far the largest dataset to date in this task. It also
supports researches on the haze and shadow removal as well as super-resolution
and in-painting applications. We define three tasks: detection by (1)
horizontal bounding boxes, (2) rotated bounding boxes, and (3) oriented
bounding boxes. We carried out several experiments to evaluate several
state-of-the-art methods in object detection on our dataset to form a baseline.
Experiments show that the EAGLE dataset accurately reflects real-world
situations and correspondingly challenging applications.
</p>
<a href="http://arxiv.org/abs/2007.06124" target="_blank">arXiv:2007.06124</a> [<a href="http://arxiv.org/pdf/2007.06124" target="_blank">pdf</a>]

<h2>Reconstruction Bottlenecks in Object-Centric Generative Models. (arXiv:2007.06245v2 [cs.LG] UPDATED)</h2>
<h3>Martin Engelcke, Oiwi Parker Jones, Ingmar Posner</h3>
<p>A range of methods with suitable inductive biases exist to learn
interpretable object-centric representations of images without supervision.
However, these are largely restricted to visually simple images; robust object
discovery in real-world sensory datasets remains elusive. To increase the
understanding of such inductive biases, we empirically investigate the role of
"reconstruction bottlenecks" for scene decomposition in GENESIS, a recent
VAE-based model. We show such bottlenecks determine reconstruction and
segmentation quality and critically influence model behaviour.
</p>
<a href="http://arxiv.org/abs/2007.06245" target="_blank">arXiv:2007.06245</a> [<a href="http://arxiv.org/pdf/2007.06245" target="_blank">pdf</a>]

<h2>On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v2 [cs.LG] UPDATED)</h2>
<h3>Chaitanya Devaguptapu, Devansh Agarwal, Gaurav Mittal, Vineeth N Balasubramanian</h3>
<p>Adversarial robustness of deep learning models has gained much traction in
the last few years. Various attacks and defenses are proposed to improve the
adversarial robustness of modern-day deep learning architectures. While all
these approaches help improve the robustness, one promising direction for
improving adversarial robustness is un-explored, i.e., the complex topology of
the neural network architecture. In this work, we answer the following
question: "Can the complex topology of a neural network give adversarial
robustness without any form of adversarial training?" empirically by
experimenting with different hand-crafted and NAS based architectures. Our
findings show that, for small-scale attacks, NAS-based architectures are more
robust for small-scale datasets and simple tasks than hand-crafted
architectures. However, as the dataset's size or the task's complexity
increase, hand-crafted architectures are more robust than NAS-based
architectures. We perform the first large scale study to understand adversarial
robustness purely from an architectural perspective. Our results show that
random sampling in the search space of DARTS (a popular NAS method) with simple
ensembling can improve the robustness to PGD attack by nearly ~12\%. We show
that NAS, which is popular for SoTA accuracy, can provide adversarial accuracy
as a free add-on without any form of adversarial training. Our results show
that leveraging the power of neural network topology with methods like
ensembles can be an excellent way to achieve adversarial robustness without any
form of adversarial training. We also introduce a metric that can be used to
calculate the trade-off between clean accuracy and adversarial robustness.
</p>
<a href="http://arxiv.org/abs/2007.08428" target="_blank">arXiv:2007.08428</a> [<a href="http://arxiv.org/pdf/2007.08428" target="_blank">pdf</a>]

<h2>Discovering Reinforcement Learning Algorithms. (arXiv:2007.08794v2 [cs.LG] UPDATED)</h2>
<h3>Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, David Silver</h3>
<p>Reinforcement learning (RL) algorithms update an agent's parameters according
to one of several possible rules, discovered manually through years of
research. Automating the discovery of update rules from data could lead to more
efficient algorithms, or algorithms that are better adapted to specific
environments. Although there have been prior attempts at addressing this
significant scientific challenge, it remains an open question whether it is
feasible to discover alternatives to fundamental concepts of RL such as value
functions and temporal-difference learning. This paper introduces a new
meta-learning approach that discovers an entire update rule which includes both
'what to predict' (e.g. value functions) and 'how to learn from it' (e.g.
bootstrapping) by interacting with a set of environments. The output of this
method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical
results show that our method discovers its own alternative to the concept of
value functions. Furthermore it discovers a bootstrapping mechanism to maintain
and use its predictions. Surprisingly, when trained solely on toy environments,
LPG generalises effectively to complex Atari games and achieves non-trivial
performance. This shows the potential to discover general RL algorithms from
data.
</p>
<a href="http://arxiv.org/abs/2007.08794" target="_blank">arXiv:2007.08794</a> [<a href="http://arxiv.org/pdf/2007.08794" target="_blank">pdf</a>]

<h2>Improving the HardNet Descriptor. (arXiv:2007.09699v2 [cs.CV] UPDATED)</h2>
<h3>Milan Pultar</h3>
<p>In the thesis we consider the problem of local feature descriptor learning
for wide baseline stereo focusing on the HardNet descriptor, which is close to
state-of-the-art. AMOS Patches dataset is introduced, which improves robustness
to illumination and appearance changes. It is based on registered images from
selected cameras from the AMOS dataset. We provide recommendations on the patch
dataset creation process and evaluate HardNet trained on data of different
modalities. We also introduce a dataset combination and reduction methods, that
allow comparable performance on a significantly smaller dataset.

HardNet8, consistently outperforming the original HardNet, benefits from the
architectural choices made: connectivity pattern, final pooling, receptive
field, CNN building blocks found by manual or automatic search algorithms --
DARTS. We show impact of overlooked hyperparameters such as batch size and
length of training on the descriptor quality. PCA dimensionality reduction
further boosts performance and also reduces memory footprint.

Finally, the insights gained lead to two HardNet8 descriptors: one performing
well on a variety of benchmarks -- HPatches, AMOS Patches and IMW Phototourism,
the other is optimized for IMW Phototourism.
</p>
<a href="http://arxiv.org/abs/2007.09699" target="_blank">arXiv:2007.09699</a> [<a href="http://arxiv.org/pdf/2007.09699" target="_blank">pdf</a>]

<h2>Nonclosedness of Sets of Neural Networks in Sobolev Spaces. (arXiv:2007.11730v2 [stat.ML] UPDATED)</h2>
<h3>Scott Mahan, Emily King, Alex Cloninger</h3>
<p>We examine the closedness of sets of realized neural networks of a fixed
architecture in Sobolev spaces. For an exactly $m$-times differentiable
activation function $\rho$, we construct a sequence of neural networks
$(\Phi_n)_{n \in \mathbb{N}}$ whose realizations converge in order-$(m-1)$
Sobolev norm to a function that cannot be realized exactly by a neural network.
Thus, sets of realized neural networks are not closed in order-$(m-1)$ Sobolev
spaces $W^{m-1,p}$ for $p \in [1,\infty)$. We further show that these sets are
not closed in $W^{m,p}$ under slightly stronger conditions on the $m$-th
derivative of $\rho$. For a real analytic activation function, we show that
sets of realized neural networks are not closed in $W^{k,p}$ for \textit{any}
$k \in \mathbb{N}$. The nonclosedness allows for approximation of non-network
target functions with unbounded parameter growth. We partially characterize the
rate of parameter growth for most activation functions by showing that a
specific sequence of realized neural networks can approximate the activation
function's derivative with weights increasing inversely proportional to the
$L^p$ approximation error. Finally, we present experimental results showing
that networks are capable of closely approximating non-network target functions
with increasing parameters via training.
</p>
<a href="http://arxiv.org/abs/2007.11730" target="_blank">arXiv:2007.11730</a> [<a href="http://arxiv.org/pdf/2007.11730" target="_blank">pdf</a>]

<h2>UNIPoint: Universally Approximating Point Processes Intensities. (arXiv:2007.14082v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Soen (1), Alexander Mathews (1), Daniel Grixti-Cheng (1), Lexing Xie (1) ((1) Australian National University)</h3>
<p>Point processes have been a preferred mathematical tool for describing events
over time, and there are many recent approaches for representing and learning
them. One notable open question is in precisely describing the flexibility of
the various models, and whether there exists a general model that can represent
{\em all} point processes. Our work bridges this gap. Focusing on the widely
used event intensity function representation of point processes, we provide a
constructive proof that a class of learnable functions can universally
approximate any valid intensity function. The proof connects the well known
Stone-Weierstrass Theorem for function approximation, the uniform density of
positive transfer functions, formulating parameters of piece-wise continuous
functions as a dynamic system, and recurrent neural networks for capturing the
dynamics. Using these insights, we design and implement UNIPoint, a novel
neural point process model, using recurrent neural networks to parameterise
sums of basis function upon each event. Evaluations on synthetic and real
datasets show that this simpler representation performs better than Hawkes
process variants, and as well as more complex neural network-based approaches.
We expect this result will provide a basis for practically selecting and tuning
models, as well as further theoretical work on fine-grained characterisation of
representational complexity versus expressiveness.
</p>
<a href="http://arxiv.org/abs/2007.14082" target="_blank">arXiv:2007.14082</a> [<a href="http://arxiv.org/pdf/2007.14082" target="_blank">pdf</a>]

<h2>How Trustworthy are the Existing Performance Evaluations for Basic Vision Tasks?. (arXiv:2008.03533v2 [cs.CV] UPDATED)</h2>
<h3>Hamid Rezatofighi, Tran Thien Dat Nguyen, Ba-Ngu Vo, Ba-Tuong Vo, Silvio Savarese, Ian Reid</h3>
<p>This paper examines performance evaluation criteria for basic vision tasks
involving sets of objects namely, object detection, instance-level segmentation
and multi-object tracking. The rankings of algorithms by current criteria
fluctuate with different choices of parameters, e.g. Intersection over Union
(IoU) threshold, making their evaluations unreliable. More importantly, there
is no means to even verify whether we can trust the evaluations of a criterion.
This work advocates a notion of trustworthiness for criteria, which requires
(i) robustness to parameters for reliability, (ii) contextual meaningfulness in
sanity tests, and (iii) consistency with mathematical requirements such as the
metric properties. We show that such requirements were overlooked by many
widely-used criteria. We also explore alternative criteria using metrics for
sets of shapes, and assess them against these requirements to find trustworthy
criteria.
</p>
<a href="http://arxiv.org/abs/2008.03533" target="_blank">arXiv:2008.03533</a> [<a href="http://arxiv.org/pdf/2008.03533" target="_blank">pdf</a>]

<h2>Self-adapting confidence estimation for stereo. (arXiv:2008.06447v2 [cs.CV] UPDATED)</h2>
<h3>Matteo Poggi, Filippo Aleotti, Fabio Tosi, Giulio Zaccaroni, Stefano Mattoccia</h3>
<p>Estimating the confidence of disparity maps inferred by a stereo algorithm
has become a very relevant task in the years, due to the increasing number of
applications leveraging such cue. Although self-supervised learning has
recently spread across many computer vision tasks, it has been barely
considered in the field of confidence estimation. In this paper, we propose a
flexible and lightweight solution enabling self-adapting confidence estimation
agnostic to the stereo algorithm or network. Our approach relies on the minimum
information available in any stereo setup (i.e., the input stereo pair and the
output disparity map) to learn an effective confidence measure. This strategy
allows us not only a seamless integration with any stereo system, including
consumer and industrial devices equipped with undisclosed stereo perception
methods, but also, due to its self-adapting capability, for its out-of-the-box
deployment in the field. Exhaustive experimental results with different
standard datasets support our claims, showing how our solution is the
first-ever enabling online learning of accurate confidence estimation for any
stereo system and without any requirement for the end-user.
</p>
<a href="http://arxiv.org/abs/2008.06447" target="_blank">arXiv:2008.06447</a> [<a href="http://arxiv.org/pdf/2008.06447" target="_blank">pdf</a>]

<h2>Unsupervised Transfer Learning for Anomaly Detection: Application to Complementary Operating Condition Transfer. (arXiv:2008.07815v2 [cs.LG] UPDATED)</h2>
<h3>Gabriel Michau, Olga Fink</h3>
<p>Anomaly Detectors are trained on healthy operating condition data and raise
an alarm when the measured samples deviate from the training data distribution.
This means that the samples used to train the model should be sufficient in
quantity and representative of the healthy operating conditions. But for
industrial systems subject to changing operating conditions, acquiring such
comprehensive sets of samples requires a long collection period and delay the
point at which the anomaly detector can be trained and put in operation.

A solution to this problem is to perform unsupervised transfer learning
(UTL), to transfer complementary data between different units. In the
literature however, UTL aims at finding common structure between the datasets,
to perform clustering or dimensionality reduction. Yet, the task of
transferring and combining complementary training data has not been studied.

Our proposed framework is designed to transfer complementary operating
conditions between different units in a completely unsupervised way to train
more robust anomaly detectors. It differs, thereby, from other unsupervised
transfer learning works as it focuses on a one-class classification problem.
The proposed methodology enables to detect anomalies in operating conditions
only experienced by other units. The proposed end-to-end framework uses
adversarial deep learning to ensure alignment of the different units'
distributions. The framework introduces a new loss, inspired by a
dimensionality reduction tool, to enforce the conservation of the inherent
variability of each dataset, and uses state-of-the art once-class approach to
detect anomalies. We demonstrate the benefit of the proposed framework using
three open source datasets.
</p>
<a href="http://arxiv.org/abs/2008.07815" target="_blank">arXiv:2008.07815</a> [<a href="http://arxiv.org/pdf/2008.07815" target="_blank">pdf</a>]

<h2>Relevance of Rotationally Equivariant Convolutions for Predicting Molecular Properties. (arXiv:2008.08461v4 [cs.LG] UPDATED)</h2>
<h3>Benjamin Kurt Miller, Mario Geiger, Tess E. Smidt, Frank No&#xe9;</h3>
<p>Equivariant neural networks (ENNs) are graph neural networks embedded in
$\mathbb{R}^3$ and are well suited for predicting molecular properties. The ENN
library e3nn has customizable convolutions, which can be designed to depend
only on distances between points, or also on angular features, making them
rotationally invariant, or equivariant, respectively. This paper studies the
practical value of including angular dependencies for molecular property
prediction directly via an ablation study with \texttt{e3nn} and the QM9 data
set. We find that, for fixed network depth and parameter count, adding angular
features decreased test error by an average of 23%. Meanwhile, increasing
network depth decreased test error by only 4% on average, implying that
rotationally equivariant layers are comparatively parameter efficient. We
present an explanation of the accuracy improvement on the dipole moment, the
target which benefited most from the introduction of angular features.
</p>
<a href="http://arxiv.org/abs/2008.08461" target="_blank">arXiv:2008.08461</a> [<a href="http://arxiv.org/pdf/2008.08461" target="_blank">pdf</a>]

<h2>No-reference Screen Content Image Quality Assessment with Unsupervised Domain Adaptation. (arXiv:2008.08561v2 [cs.CV] UPDATED)</h2>
<h3>Baoliang Chen, Haoliang Li, Hongfei Fan, Shiqi Wang</h3>
<p>In this paper, we quest the capability of transferring the quality of natural
scene images to the images that are not acquired by optical cameras (e.g.,
screen content images, SCIs), rooted in the widely accepted view that the human
visual system has adapted and evolved through the perception of natural
environment. Here, we develop the first unsupervised domain adaptation based no
reference quality assessment method for SCIs, leveraging rich subjective
ratings of the natural images (NIs). In general, it is a non-trivial task to
directly transfer the quality prediction model from NIs to a new type of
content (i.e., SCIs) that holds dramatically different statistical
characteristics. Inspired by the transferability of pair-wise relationship, the
proposed quality measure operates based on the philosophy of improving the
transferability and discriminability simultaneously. In particular, we
introduce three types of losses which complementarily and explicitly regularize
the feature space of ranking in a progressive manner. Regarding feature
discriminatory capability enhancement, we propose a center based loss to
rectify the classifier and improve its prediction capability not only for
source domain (NI) but also the target domain (SCI). For feature discrepancy
minimization, the maximum mean discrepancy (MMD) is imposed on the extracted
ranking features of NIs and SCIs. Furthermore, to further enhance the feature
diversity, we introduce the correlation penalization between different feature
dimensions, leading to the features with lower rank and higher diversity.
Experiments show that our method can achieve higher performance on different
source-target settings based on a light-weight convolution neural network. The
proposed method also sheds light on learning quality assessment measures for
unseen application-specific content without the cumbersome and costing
subjective evaluations.
</p>
<a href="http://arxiv.org/abs/2008.08561" target="_blank">arXiv:2008.08561</a> [<a href="http://arxiv.org/pdf/2008.08561" target="_blank">pdf</a>]

<h2>Active Deep Densely Connected Convolutional Network for Hyperspectral Image Classification. (arXiv:2009.00320v2 [cs.CV] UPDATED)</h2>
<h3>Bing Liu, Anzhu Yu, Pengqiang Zhang, Lei Ding, Wenyue Guo, Kuiliang Gao, Xibing Zuo</h3>
<p>Deep learning based methods have seen a massive rise in popularity for
hyperspectral image classification over the past few years. However, the
success of deep learning is attributed greatly to numerous labeled samples. It
is still very challenging to use only a few labeled samples to train deep
learning models to reach a high classification accuracy. An active
deep-learning framework trained by an end-to-end manner is, therefore, proposed
by this paper in order to minimize the hyperspectral image classification
costs. First, a deep densely connected convolutional network is considered for
hyperspectral image classification. Different from the traditional active
learning methods, an additional network is added to the designed deep densely
connected convolutional network to predict the loss of input samples. Then, the
additional network could be used to suggest unlabeled samples that the deep
densely connected convolutional network is more likely to produce a wrong
label. Note that the additional network uses the intermediate features of the
deep densely connected convolutional network as input. Therefore, the proposed
method is an end-to-end framework. Subsequently, a few of the selected samples
are labelled manually and added to the training samples. The deep densely
connected convolutional network is therefore trained using the new training
set. Finally, the steps above are repeated to train the whole framework
iteratively. Extensive experiments illustrates that the method proposed could
reach a high accuracy in classification after selecting just a few samples.
</p>
<a href="http://arxiv.org/abs/2009.00320" target="_blank">arXiv:2009.00320</a> [<a href="http://arxiv.org/pdf/2009.00320" target="_blank">pdf</a>]

<h2>Conditional Uncorrelation and Efficient Non-approximate Subset Selection in Sparse Regression. (arXiv:2009.03986v2 [cs.LG] UPDATED)</h2>
<h3>Jianji Wang, Qi Liu, Shupei Zhang, Nanning Zheng, Fei-Yue Wang</h3>
<p>Given $m$ $d$-dimensional responsors and $n$ $d$-dimensional predictors,
sparse regression finds at most $k$ predictors for each responsor for linear
approximation, $1\leq k \leq d-1$. The key problem in sparse regression is
subset selection, which usually suffers from high computational cost. Recent
years, many improved approximate methods of subset selection have been
published. However, less attention has been paid on the non-approximate method
of subset selection, which is very necessary for many questions in data
analysis. Here we consider sparse regression from the view of correlation, and
propose the formula of conditional uncorrelation. Then an efficient
non-approximate method of subset selection is proposed in which we do not need
to calculate any coefficients in regression equation for candidate predictors.
By the proposed method, the computational complexity is reduced from
$O(\frac{1}{6}{k^3}+mk^2+mkd)$ to $O(\frac{1}{6}{k^3}+\frac{1}{2}mk^2)$ for
each candidate subset in sparse regression. Because the dimension $d$ is
generally the number of observations or experiments and large enough, the
proposed method can greatly improve the efficiency of non-approximate subset
selection.
</p>
<a href="http://arxiv.org/abs/2009.03986" target="_blank">arXiv:2009.03986</a> [<a href="http://arxiv.org/pdf/2009.03986" target="_blank">pdf</a>]

<h2>SelfAugment: Automatic Augmentation Policies for Self-Supervised Learning. (arXiv:2009.07724v2 [cs.CV] UPDATED)</h2>
<h3>Colorado Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, Kurt Keutzer</h3>
<p>A common practice in unsupervised representation learning is to use labeled
data to evaluate the quality of the learned representations. This supervised
evaluation is then used to guide critical aspects of the training process such
as selecting the data augmentation policy. However, guiding an unsupervised
training process through supervised evaluations is not possible for real-world
data that does not actually contain labels (which may be the case, for example,
in privacy sensitive fields such as medical imaging). Therefore, in this work
we show that evaluating the learned representations with a self-supervised
image rotation task is highly correlated with a standard set of supervised
evaluations (rank correlation &gt; 0.94). We establish this correlation across
hundreds of augmentation policies, training settings, and network architectures
and provide an algorithm (SelfAugment) to automatically and efficiently select
augmentation policies without using supervised evaluations. Despite not using
any labeled data, the learned augmentation policies perform comparably with
augmentation policies that were determined using exhaustive supervised
evaluations.
</p>
<a href="http://arxiv.org/abs/2009.07724" target="_blank">arXiv:2009.07724</a> [<a href="http://arxiv.org/pdf/2009.07724" target="_blank">pdf</a>]

<h2>Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v2 [cs.LG] UPDATED)</h2>
<h3>Youwei Liang, Dong Huang</h3>
<p>Since the Lipschitz properties of convolutional neural network (CNN) are
widely considered to be related to adversarial robustness, we theoretically
characterize the $\ell_1$ norm and $\ell_\infty$ norm of 2D multi-channel
convolutional layers and provide efficient methods to compute the exact
$\ell_1$ norm and $\ell_\infty$ norm. Based on our theorem, we propose a novel
regularization method termed norm decay, which can effectively reduce the norms
of CNN layers. Experiments show that norm-regularization methods, including
norm decay, weight decay, and singular value clipping, can improve
generalization of CNNs. However, we are surprised to find that they can
slightly hurt adversarial robustness. Furthermore, we compute the norms of
layers in the CNNs trained with three different adversarial training frameworks
and find that adversarially robust CNNs have comparable or even larger norms
than their non-adversarially robust counterparts. Moreover, we prove that under
a mild assumption, adversarially robust classifiers can be achieved with neural
networks and an adversarially robust neural network can have arbitrarily large
Lipschitz constant. For these reasons, enforcing small norms of CNN layers may
be neither effective nor necessary in achieving adversarial robustness. Our
code is available at https://github.com/youweiliang/norm_robustness.
</p>
<a href="http://arxiv.org/abs/2009.08435" target="_blank">arXiv:2009.08435</a> [<a href="http://arxiv.org/pdf/2009.08435" target="_blank">pdf</a>]

<h2>Learning a Contact-Adaptive Controller for Robust, Efficient Legged Locomotion. (arXiv:2009.10019v4 [cs.RO] UPDATED)</h2>
<h3>Xingye Da, Zhaoming Xie, David Hoeller, Byron Boots, Animashree Anandkumar, Yuke Zhu, Buck Babich, Animesh Garg</h3>
<p>We present a hierarchical framework that combines model-based control and
reinforcement learning (RL) to synthesize robust controllers for a quadruped
(the Unitree Laikago). The system consists of a high-level controller that
learns to choose from a set of primitives in response to changes in the
environment and a low-level controller that utilizes an established control
method to robustly execute the primitives. Our framework learns a controller
that can adapt to challenging environmental changes on the fly, including novel
scenarios not seen during training. The learned controller is up to 85~percent
more energy efficient and is more robust compared to baseline methods. We also
deploy the controller on a physical robot without any randomization or
adaptation scheme.
</p>
<a href="http://arxiv.org/abs/2009.10019" target="_blank">arXiv:2009.10019</a> [<a href="http://arxiv.org/pdf/2009.10019" target="_blank">pdf</a>]

<h2>Group Whitening: Balancing Learning Efficiency and Representational Capacity. (arXiv:2009.13333v3 [cs.LG] UPDATED)</h2>
<h3>Lei Huang, Yi Zhou, Li Liu, Fan Zhu, Ling Shao</h3>
<p>Batch normalization (BN) is an important technique commonly incorporated into
deep learning models to perform standardization within mini-batches. The merits
of BN in improving a model's learning efficiency can be further amplified by
applying whitening, while its drawbacks in estimating population statistics for
inference can be avoided through group normalization (GN). This paper proposes
group whitening (GW), which exploits the advantages of the whitening operation
and avoids the disadvantages of normalization within mini-batches. In addition,
we analyze the constraints imposed on features by normalization, and show how
the batch size (group number) affects the performance of batch (group)
normalized networks, from the perspective of model's representational capacity.
This analysis provides theoretical guidance for applying GW in practice.
Finally, we apply the proposed GW to ResNet and ResNeXt architectures and
conduct experiments on the ImageNet and COCO benchmarks. Results show that GW
consistently improves the performance of different architectures, with absolute
gains of $1.02\%$ $\sim$ $1.49\%$ in top-1 accuracy on ImageNet and $1.82\%$
$\sim$ $3.21\%$ in bounding box AP on COCO.
</p>
<a href="http://arxiv.org/abs/2009.13333" target="_blank">arXiv:2009.13333</a> [<a href="http://arxiv.org/pdf/2009.13333" target="_blank">pdf</a>]

<h2>Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization. (arXiv:2009.13586v3 [cs.LG] UPDATED)</h2>
<h3>Xuezhe Ma</h3>
<p>In this paper, we introduce Apollo, a quasi-Newton method for nonconvex
stochastic optimization, which dynamically incorporates the curvature of the
loss function by approximating the Hessian via a diagonal matrix. Importantly,
the update and storage of the diagonal approximation of Hessian is as efficient
as adaptive first-order optimization methods with linear complexity for both
time and memory. To handle nonconvexity, we replace the Hessian with its
rectified absolute value, which is guaranteed to be positive-definite.
Experiments on three tasks of vision and language show that Apollo achieves
significant improvements over other stochastic optimization methods, including
SGD and variants of Adam, in term of both convergence speed and generalization
performance. The implementation of the algorithm is available at
https://github.com/XuezheMax/apollo.
</p>
<a href="http://arxiv.org/abs/2009.13586" target="_blank">arXiv:2009.13586</a> [<a href="http://arxiv.org/pdf/2009.13586" target="_blank">pdf</a>]

<h2>Conformance Checking over Uncertain Event Data. (arXiv:2009.14452v2 [cs.AI] UPDATED)</h2>
<h3>Marco Pegoraro, Merih Seran Uysal, Wil M.P. van der Aalst</h3>
<p>The strong impulse to digitize processes and operations in companies and
enterprises have resulted in the creation and automatic recording of an
increasingly large amount of process data in information systems. These are
made available in the form of event logs. Process mining techniques enable the
process-centric analysis of data, including automatically discovering process
models and checking if event data conform to a given model. In this paper, we
analyze the previously unexplored setting of uncertain event logs. In such
event logs uncertainty is recorded explicitly, i.e., the time, activity and
case of an event may be unclear or imprecise. In this work, we define a
taxonomy of uncertain event logs and models, and we examine the challenges that
uncertainty poses on process discovery and conformance checking. Finally, we
show how upper and lower bounds for conformance can be obtained by aligning an
uncertain trace onto a regular process model.
</p>
<a href="http://arxiv.org/abs/2009.14452" target="_blank">arXiv:2009.14452</a> [<a href="http://arxiv.org/pdf/2009.14452" target="_blank">pdf</a>]

<h2>Mini-DDSM: Mammography-based Automatic Age Estimation. (arXiv:2010.00494v3 [cs.CV] UPDATED)</h2>
<h3>Charitha Dissanayake Lekamlage, Fabia Afzal, Erik Westerberg, Abbas Cheddad</h3>
<p>Age estimation has attracted attention for its various medical applications.
There are many studies on human age estimation from biomedical images. However,
there is no research done on mammograms for age estimation, as far as we know.
The purpose of this study is to devise an AI-based model for estimating age
from mammogram images. Due to lack of public mammography data sets that have
the age attribute, we resort to using a web crawler to download thumbnail
mammographic images and their age fields from the public data set; the Digital
Database for Screening Mammography. The original images in this data set
unfortunately can only be retrieved by a software which is broken.
Subsequently, we extracted deep learning features from the collected data set,
by which we built a model using Random Forests regressor to estimate the age
automatically. The performance assessment was measured using the mean absolute
error values. The average error value out of 10 tests on random selection of
samples was around 8 years. In this paper, we show the merits of this approach
to fill up missing age values. We ran logistic and linear regression models on
another independent data set to further validate the advantage of our proposed
work. This paper also introduces the free-access Mini-DDSM data set.
</p>
<a href="http://arxiv.org/abs/2010.00494" target="_blank">arXiv:2010.00494</a> [<a href="http://arxiv.org/pdf/2010.00494" target="_blank">pdf</a>]

<h2>Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch. (arXiv:2010.01729v3 [cs.CV] UPDATED)</h2>
<h3>Youngeun Kim, Priyadarshini Panda</h3>
<p>Spiking Neural Networks (SNNs) have recently emerged as an alternative to
deep learning owing to sparse, asynchronous and binary event (or spike) driven
processing, that can yield huge energy efficiency benefits on neuromorphic
hardware. However, training high-accuracy and low-latency SNNs from scratch
suffers from non-differentiable nature of a spiking neuron. To address this
training issue in SNNs, we revisit batch normalization and propose a temporal
Batch Normalization Through Time (BNTT) technique. Most prior SNN works till
now have disregarded batch normalization deeming it ineffective for training
temporal SNNs. Different from previous works, our proposed BNTT decouples the
parameters in a BNTT layer along the time axis to capture the temporal dynamics
of spikes. The temporally evolving learnable parameters in BNTT allow a neuron
to control its spike rate through different time-steps, enabling low-latency
and low-energy training from scratch. We conduct experiments on CIFAR-10,
CIFAR-100, Tiny-ImageNet and event-driven DVS-CIFAR10 datasets. BNTT allows us
to train deep SNN architectures from scratch, for the first time, on complex
datasets with just few 25-30 time-steps. We also propose an early exit
algorithm using the distribution of parameters in BNTT to reduce the latency at
inference, that further improves the energy-efficiency.
</p>
<a href="http://arxiv.org/abs/2010.01729" target="_blank">arXiv:2010.01729</a> [<a href="http://arxiv.org/pdf/2010.01729" target="_blank">pdf</a>]

<h2>Disentangle-based Continual Graph Representation Learning. (arXiv:2010.02565v4 [cs.LG] UPDATED)</h2>
<h3>Xiaoyu Kou, Yankai Lin, Shaobo Liu, Peng Li, Jie Zhou, Yan Zhang</h3>
<p>Graph embedding (GE) methods embed nodes (and/or edges) in graph into a
low-dimensional semantic space, and have shown its effectiveness in modeling
multi-relational data. However, existing GE models are not practical in
real-world applications since it overlooked the streaming nature of incoming
data. To address this issue, we study the problem of continual graph
representation learning which aims to continually train a GE model on new data
to learn incessantly emerging multi-relational data while avoiding
catastrophically forgetting old learned knowledge. Moreover, we propose a
disentangle-based continual graph representation learning (DiCGRL) framework
inspired by the human's ability to learn procedural knowledge. The experimental
results show that DiCGRL could effectively alleviate the catastrophic
forgetting problem and outperform state-of-the-art continual learning models.
</p>
<a href="http://arxiv.org/abs/2010.02565" target="_blank">arXiv:2010.02565</a> [<a href="http://arxiv.org/pdf/2010.02565" target="_blank">pdf</a>]

<h2>Real-time Uncertainty Decomposition for Online Learning Control. (arXiv:2010.02613v2 [cs.LG] UPDATED)</h2>
<h3>Jonas Umlauft, Armin Lederer, Thomas Beckers, Sandra Hirche</h3>
<p>Safety-critical decisions based on machine learning models require a clear
understanding of the involved uncertainties to avoid hazardous or risky
situations. While aleatoric uncertainty can be explicitly modeled given a
parametric description, epistemic uncertainty rather describes the presence or
absence of training data. This paper proposes a novel generic method for
modeling epistemic uncertainty and shows its advantages over existing
approaches for neural networks on various data sets. It can be directly
combined with aleatoric uncertainty estimates and allows for prediction in
real-time as the inference is sample-free. We exploit this property in a
model-based quadcopter control setting and demonstrate how the controller
benefits from a differentiation between aleatoric and epistemic uncertainty in
online learning of thermal disturbances.
</p>
<a href="http://arxiv.org/abs/2010.02613" target="_blank">arXiv:2010.02613</a> [<a href="http://arxiv.org/pdf/2010.02613" target="_blank">pdf</a>]

<h2>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. (arXiv:2010.04296v2 [cs.RO] UPDATED)</h2>
<h3>Ossama Ahmed, Frederik Tr&#xe4;uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch&#xf6;lkopf, Manuel W&#xfc;thrich, Stefan Bauer</h3>
<p>Despite recent successes of reinforcement learning (RL), it remains a
challenge for agents to transfer learned skills to related environments. To
facilitate research addressing this problem, we propose CausalWorld, a
benchmark for causal structure and transfer learning in a robotic manipulation
environment. The environment is a simulation of an open-source robotic
platform, hence offering the possibility of sim-to-real transfer. Tasks consist
of constructing 3D shapes from a given set of blocks - inspired by how children
learn to build complex structures. The key strength of CausalWorld is that it
provides a combinatorial family of such tasks with common causal structure and
underlying factors (including, e.g., robot and object masses, colors, sizes).
The user (or the agent) may intervene on all causal variables, which allows for
fine-grained control over how similar different tasks (or task distributions)
are. One can thus easily define training and evaluation distributions of a
desired difficulty level, targeting a specific form of generalization (e.g.,
only changes in appearance or object mass). Further, this common
parametrization facilitates defining curricula by interpolating between an
initial and a target task. While users may define their own task distributions,
we present eight meaningful distributions as concrete benchmarks, ranging from
simple to very challenging, all of which require long-horizon planning as well
as precise low-level motor control. Finally, we provide baseline results for a
subset of these tasks on distinct training curricula and corresponding
evaluation protocols, verifying the feasibility of the tasks in this benchmark.
</p>
<a href="http://arxiv.org/abs/2010.04296" target="_blank">arXiv:2010.04296</a> [<a href="http://arxiv.org/pdf/2010.04296" target="_blank">pdf</a>]

<h2>Refining Semantic Segmentation with Superpixel by Transparent Initialization and Sparse Encoder. (arXiv:2010.04363v3 [cs.CV] UPDATED)</h2>
<h3>Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley</h3>
<p>Although deep learning greatly improves the performance of semantic
segmentation, its success mainly lies in object central areas without accurate
edges. As superpixels are a popular and effective auxiliary to preserve object
edges, in this paper, we jointly learn semantic segmentation with trainable
superpixels. We achieve it with fully-connected layers with Transparent
Initialization (TI) and efficient logit consistency using a sparse encoder. The
proposed TI preserves the effects of learned parameters of pretrained networks.
This avoids a significant increase of the loss of pretrained networks, which
otherwise may be caused by inappropriate parameter initialization of the
additional layers. Meanwhile, consistent pixel labels in each superpixel are
guaranteed by logit consistency. The sparse encoder with sparse matrix
operations substantially reduces both the memory requirement and the
computational complexity. We demonstrated the superiority of TI over other
parameter initialization methods and tested its numerical stability. The
effectiveness of our proposal was validated on PASCAL VOC 2012, ADE20K, and
PASCAL Context showing enhanced semantic segmentation edges. With quantitative
evaluations on segmentation edges using performance ratio and F-measure, our
method outperforms the state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2010.04363" target="_blank">arXiv:2010.04363</a> [<a href="http://arxiv.org/pdf/2010.04363" target="_blank">pdf</a>]

<h2>Gini in a Bottleneck: Sparse Molecular Representations for Graph Convolutional Neural Networks. (arXiv:2010.04535v3 [cs.LG] UPDATED)</h2>
<h3>Ryan Henderson, Djork-Arn&#xe9; Clevert, Floriane Montanari</h3>
<p>Due to the nature of deep learning approaches, it is inherently difficult to
understand which aspects of a molecular graph drive the predictions of the
network. As a mitigation strategy, we constrain certain weights in a multi-task
graph convolutional neural network according to the Gini index to maximize the
"inequality" of the learned representations. We show that this constraint does
not degrade evaluation metrics for some targets, and allows us to combine the
outputs of the graph convolutional operation in a visually interpretable way.
We then perform a proof-of-concept experiment on quantum chemistry targets on
the public QM9 dataset, and a larger experiment on ADMET targets on proprietary
drug-like molecules. Since a benchmark of explainability in the latter case is
difficult, we informally surveyed medicinal chemists within our organization to
check for agreement between regions of the molecule they and the model
identified as relevant to the properties in question.
</p>
<a href="http://arxiv.org/abs/2010.04535" target="_blank">arXiv:2010.04535</a> [<a href="http://arxiv.org/pdf/2010.04535" target="_blank">pdf</a>]

<h2>Permutation invariant networks to learn Wasserstein metrics. (arXiv:2010.05820v3 [cs.LG] UPDATED)</h2>
<h3>Arijit Sehanobish, Neal Ravindra, David van Dijk</h3>
<p>Understanding the space of probability measures on a metric space equipped
with a Wasserstein distance is one of the fundamental questions in mathematical
analysis. The Wasserstein metric has received a lot of attention in the machine
learning community especially for its principled way of comparing
distributions. In this work, we use a permutation invariant network to map
samples from probability measures into a low-dimensional space such that the
Euclidean distance between the encoded samples reflects the Wasserstein
distance between probability measures. We show that our network can generalize
to correctly compute distances between unseen densities. We also show that
these networks can learn the first and the second moments of probability
distributions.
</p>
<a href="http://arxiv.org/abs/2010.05820" target="_blank">arXiv:2010.05820</a> [<a href="http://arxiv.org/pdf/2010.05820" target="_blank">pdf</a>]

<h2>Can Federated Learning Save The Planet ?. (arXiv:2010.06537v3 [cs.LG] UPDATED)</h2>
<h3>Xinchi Qiu, Titouan Parcolle, Daniel J. Beutel, Taner Topal, Akhil Mathur, Nicholas D. Lane</h3>
<p>Despite impressive results, deep learning-based technologies also raise
severe privacy and environmental concerns induced by the training procedure
often conducted in data centers. In response, alternatives to centralized
training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL
in particular is starting to be deployed at a global scale by companies that
must adhere to new legal demands and policies originating from governments and
the civil society for privacy protection. However, the potential environmental
impact related to FL remains unclear and unexplored. This paper offers the
first-ever systematic study of the carbon footprint of FL. First, we propose a
rigorous model to quantify the carbon footprint, hence facilitating the
investigation of the relationship between FL design and carbon emissions. Then,
we compare the carbon footprint of FL to traditional centralized learning.
Finally, we highlight and connect the reported results to the future challenges
and trends in FL to reduce its environmental impact, including algorithms
efficiency, hardware capabilities, and stronger industry transparency.
</p>
<a href="http://arxiv.org/abs/2010.06537" target="_blank">arXiv:2010.06537</a> [<a href="http://arxiv.org/pdf/2010.06537" target="_blank">pdf</a>]

<h2>LiDAM: Semi-Supervised Learning with Localized Domain Adaptation and Iterative Matching. (arXiv:2010.06668v2 [cs.LG] UPDATED)</h2>
<h3>Qun Liu, Matthew Shreve, Raja Bala</h3>
<p>Although data is abundant, data labeling is expensive. Semi-supervised
learning methods combine a few labeled samples with a large corpus of unlabeled
data to effectively train models. This paper introduces our proposed method
LiDAM, a semi-supervised learning approach rooted in both domain adaptation and
self-paced learning. LiDAM first performs localized domain shifts to extract
better domain-invariant features for the model that results in more accurate
clusters and pseudo-labels. These pseudo-labels are then aligned with real
class labels in a self-paced fashion using a novel iterative matching technique
that is based on majority consistency over high-confidence predictions.
Simultaneously, a final classifier is trained to predict ground-truth labels
until convergence. LiDAM achieves state-of-the-art performance on the CIFAR-100
dataset, outperforming FixMatch (73.50% vs. 71.82%) when using 2500 labels.
</p>
<a href="http://arxiv.org/abs/2010.06668" target="_blank">arXiv:2010.06668</a> [<a href="http://arxiv.org/pdf/2010.06668" target="_blank">pdf</a>]

<h2>Facial Emotion Recognition with Noisy Multi-task Annotations. (arXiv:2010.09849v2 [cs.CV] UPDATED)</h2>
<h3>Siwei Zhang, Zhiwu Huang, Danda Pani Paudel, Luc Van Gool</h3>
<p>Human emotions can be inferred from facial expressions. However, the
annotations of facial expressions are often highly noisy in common emotion
coding models, including categorical and dimensional ones. To reduce human
labelling effort on multi-task labels, we introduce a new problem of facial
emotion recognition with noisy multi-task annotations. For this new problem, we
suggest a formulation from the point of joint distribution match view, which
aims at learning more reliable correlations among raw facial images and
multi-task labels, resulting in the reduction of noise influence. In our
formulation, we exploit a new method to enable the emotion prediction and the
joint distribution learning in a unified adversarial learning game. Evaluation
throughout extensive experiments studies the real setups of the suggested new
problem, as well as the clear superiority of the proposed method over the
state-of-the-art competing methods on either the synthetic noisy labeled
CIFAR-10 or practical noisy multi-task labeled RAF and AffectNet. The code is
available at https://github.com/sanweiliti/noisyFER.
</p>
<a href="http://arxiv.org/abs/2010.09849" target="_blank">arXiv:2010.09849</a> [<a href="http://arxiv.org/pdf/2010.09849" target="_blank">pdf</a>]

<h2>Generative Model-Enhanced Human Motion Prediction. (arXiv:2010.11699v2 [cs.CV] UPDATED)</h2>
<h3>Anthony Bourached, Ryan-Rhys Griffiths, Robert Gray, Ashwani Jha, Parashkev Nachev</h3>
<p>The task of predicting human motion is complicated by the natural
heterogeneity and compositionality of actions, necessitating robustness to
distributional shifts as far as out-of-distribution (OoD). Here we formulate a
new OoD benchmark based on the Human3.6M and CMU motion capture datasets, and
introduce a hybrid framework for hardening discriminative architectures to OoD
failure by augmenting them with a generative model. When applied to current
state-of-the-art discriminative models, we show that the proposed approach
improves OoD robustness without sacrificing in-distribution performance, and
can theoretically facilitate model interpretability. We suggest human motion
predictors ought to be constructed with OoD challenges in mind, and provide an
extensible general framework for hardening diverse discriminative architectures
to extreme distributional shift. The code is available at
https://github.com/bouracha/OoDMotion.
</p>
<a href="http://arxiv.org/abs/2010.11699" target="_blank">arXiv:2010.11699</a> [<a href="http://arxiv.org/pdf/2010.11699" target="_blank">pdf</a>]

<h2>Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation. (arXiv:2010.12184v2 [cs.CV] UPDATED)</h2>
<h3>Taotao Jing, Bingrong Xu, Jingjing Li, Zhengming Ding</h3>
<p>Domain adaptation (DA) becomes an up-and-coming technique to address the
insufficient or no annotation issue by exploiting external source knowledge.
Existing DA algorithms mainly focus on practical knowledge transfer through
domain alignment. Unfortunately, they ignore the fairness issue when the
auxiliary source is extremely imbalanced across different categories, which
results in severe under-presented knowledge adaptation of minority source set.
To this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to
handle the fairness challenge in imbalanced cross-domain learning.
Specifically, a novel cross-domain mixup generation is exploited to augment the
minority source set with target information to enhance fairness. Moreover, dual
distinct classifiers and cross-domain prototype alignment are developed to seek
a more robust classifier boundary and mitigate the domain shift. Such three
strategies are formulated into a unified framework to address the fairness
issue and domain shift challenge. Extensive experiments over two popular
benchmarks have verified the effectiveness of our proposed model by comparing
to existing state-of-the-art DA models, and especially our model significantly
improves over 20% on two benchmarks in terms of the overall accuracy.
</p>
<a href="http://arxiv.org/abs/2010.12184" target="_blank">arXiv:2010.12184</a> [<a href="http://arxiv.org/pdf/2010.12184" target="_blank">pdf</a>]

<h2>Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning. (arXiv:2010.12914v2 [cs.LG] UPDATED)</h2>
<h3>Xiyao Wang, Junge Zhang, Wenzhen Huang, Qiyue Yin</h3>
<p>Model-based reinforcement learning is a framework in which an agent learns an
environment model, makes planning and decision-making in this model, and
finally interacts with the real environment. Model-based reinforcement learning
has high sample efficiency compared with model-free reinforcement learning, and
shows great potential in the real-world application. However, model-based
reinforcement learning has been plagued by dynamics bottleneck. Dynamics
bottleneck is the phenomenon that when the timestep to interact with the
environment increases, the reward of the agent falls into the local optimum
instead of increasing. In this paper, we analyze and explain how the coupling
relationship between model and policy causes the dynamics bottleneck and shows
improving the exploration ability of the agent can alleviate this issue. We
then propose a new planning algorithm called Maximum Entropy Cross-Entropy
Method (MECEM). MECEM can improve the agent's exploration ability by maximizing
the distribution of action entropy in the planning process. We conduct
experiments on fourteen well-recognized benchmark environments such as
HalfCheetah, Ant and Swimmer. The results verify that our approach obtains the
state-of-the-art performance on eleven benchmark environments and can
effectively alleviate dynamics bottleneck on HalfCheetah, Ant and Walker2D.
</p>
<a href="http://arxiv.org/abs/2010.12914" target="_blank">arXiv:2010.12914</a> [<a href="http://arxiv.org/pdf/2010.12914" target="_blank">pdf</a>]

<h2>Examining Deep Learning Models with Multiple Data Sources for COVID-19 Forecasting. (arXiv:2010.14491v2 [cs.LG] UPDATED)</h2>
<h3>Lijing Wang, Aniruddha Adiga, Srinivasan Venkatramanan, Jiangzhuo Chen, Bryan Lewis, Madhav Marathe</h3>
<p>The COVID-19 pandemic represents the most significant public health disaster
since the 1918 influenza pandemic. During pandemics such as COVID-19, timely
and reliable spatio-temporal forecasting of epidemic dynamics is crucial. Deep
learning-based time series models for forecasting have recently gained
popularity and have been successfully used for epidemic forecasting. Here we
focus on the design and analysis of deep learning-based models for COVID-19
forecasting. We implement multiple recurrent neural network-based deep learning
models and combine them using the stacking ensemble technique. In order to
incorporate the effects of multiple factors in COVID-19 spread, we consider
multiple sources such as COVID-19 confirmed and death case count data and
testing data for better predictions. To overcome the sparsity of training data
and to address the dynamic correlation of the disease, we propose
clustering-based training for high-resolution forecasting. The methods help us
to identify the similar trends of certain groups of regions due to various
spatio-temporal effects. We examine the proposed method for forecasting weekly
COVID-19 new confirmed cases at county-, state-, and country-level. A
comprehensive comparison between different time series models in COVID-19
context is conducted and analyzed. The results show that simple deep learning
models can achieve comparable or better performance when compared with more
complicated models. We are currently integrating our methods as a part of our
weekly forecasts that we provide state and federal authorities.
</p>
<a href="http://arxiv.org/abs/2010.14491" target="_blank">arXiv:2010.14491</a> [<a href="http://arxiv.org/pdf/2010.14491" target="_blank">pdf</a>]

<h2>Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search. (arXiv:2010.15821v2 [cs.CV] UPDATED)</h2>
<h3>Houwen Peng, Hao Du, Hongyuan Yu, Qi Li, Jing Liao, Jianlong Fu</h3>
<p>One-shot weight sharing methods have recently drawn great attention in neural
architecture search due to high efficiency and competitive performance.
However, weight sharing across models has an inherent deficiency, i.e.,
insufficient training of subnetworks in hypernetworks. To alleviate this
problem, we present a simple yet effective architecture distillation method.
The central idea is that subnetworks can learn collaboratively and teach each
other throughout the training process, aiming to boost the convergence of
individual models. We introduce the concept of prioritized path, which refers
to the architecture candidates exhibiting superior performance during training.
Distilling knowledge from the prioritized paths is able to boost the training
of subnetworks. Since the prioritized paths are changed on the fly depending on
their performance and complexity, the final obtained paths are the cream of the
crop. We directly select the most promising one from the prioritized paths as
the final architecture, without using other complex search methods, such as
reinforcement learning or evolution algorithms. The experiments on ImageNet
verify such path distillation method can improve the convergence ratio and
performance of the hypernetwork, as well as boosting the training of
subnetworks. The discovered architectures achieve superior performance compared
to the recent MobileNetV3 and EfficientNet families under aligned settings.
Moreover, the experiments on object detection and more challenging search space
show the generality and robustness of the proposed method. Code and models are
available at https://github.com/microsoft/cream.git.
</p>
<a href="http://arxiv.org/abs/2010.15821" target="_blank">arXiv:2010.15821</a> [<a href="http://arxiv.org/pdf/2010.15821" target="_blank">pdf</a>]

<h2>Learning Deformable Tetrahedral Meshes for 3D Reconstruction. (arXiv:2011.01437v2 [cs.CV] UPDATED)</h2>
<h3>Jun Gao, Wenzheng Chen, Tommy Xiang, Clement Fuji Tsang, Alec Jacobson, Morgan McGuire, Sanja Fidler</h3>
<p>3D shape representations that accommodate learning-based 3D reconstruction
are an open problem in machine learning and computer graphics. Previous work on
neural 3D reconstruction demonstrated benefits, but also limitations, of point
cloud, voxel, surface mesh, and implicit function representations. We introduce
Deformable Tetrahedral Meshes (DefTet) as a particular parameterization that
utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike
existing volumetric approaches, DefTet optimizes for both vertex placement and
occupancy, and is differentiable with respect to standard 3D reconstruction
loss functions. It is thus simultaneously high-precision, volumetric, and
amenable to learning-based neural architectures. We show that it can represent
arbitrary, complex topology, is both memory and computationally efficient, and
can produce high-fidelity reconstructions with a significantly smaller grid
size than alternative volumetric approaches. The predicted surfaces are also
inherently defined as tetrahedral meshes, thus do not require post-processing.
We demonstrate that DefTet matches or exceeds both the quality of the previous
best approaches and the performance of the fastest ones. Our approach obtains
high-quality tetrahedral meshes computed directly from noisy point clouds, and
is the first to showcase high-quality 3D tet-mesh results using only a single
image as input. Our project webpage: https://nv-tlabs.github.io/DefTet/
</p>
<a href="http://arxiv.org/abs/2011.01437" target="_blank">arXiv:2011.01437</a> [<a href="http://arxiv.org/pdf/2011.01437" target="_blank">pdf</a>]

<h2>Policy Transfer via Kinematic Domain Randomization and Adaptation. (arXiv:2011.01891v2 [cs.RO] UPDATED)</h2>
<h3>Ioannis Exarchos, Yifeng Jiang, Wenhao Yu, C. Karen Liu</h3>
<p>Transferring reinforcement learning policies trained in physics simulation to
the real hardware remains a challenge, known as the "sim-to-real" gap. Domain
randomization is a simple yet effective technique to address dynamics
discrepancies across source and target domains, but its success generally
depends on heuristics and trial-and-error. In this work we investigate the
impact of randomized parameter selection on policy transferability across
different types of domain discrepancies. Contrary to common practice in which
kinematic parameters are carefully measured while dynamic parameters are
randomized, we found that virtually randomizing kinematic parameters (e.g.,
link lengths) during training in simulation generally outperforms dynamic
randomization. Based on this finding, we introduce a new domain adaptation
algorithm that utilizes simulated kinematic parameters variation. Our
algorithm, Multi-Policy Bayesian Optimization, trains an ensemble of universal
policies conditioned on virtual kinematic parameters and efficiently adapts to
the target environment using a limited number of target domain rollouts. We
showcase our findings on a simulated quadruped robot in five different target
environments covering different aspects of domain discrepancies.
</p>
<a href="http://arxiv.org/abs/2011.01891" target="_blank">arXiv:2011.01891</a> [<a href="http://arxiv.org/pdf/2011.01891" target="_blank">pdf</a>]

<h2>An analysis of the transfer learning of convolutional neural networks for artistic images. (arXiv:2011.02727v2 [cs.CV] UPDATED)</h2>
<h3>Nicolas Gonthier, Yann Gousseau, Sa&#xef;d Ladjal</h3>
<p>Transfer learning from huge natural image datasets, fine-tuning of deep
neural networks and the use of the corresponding pre-trained networks have
become de facto the core of art analysis applications. Nevertheless, the
effects of transfer learning are still poorly understood. In this paper, we
first use techniques for visualizing the network internal representations in
order to provide clues to the understanding of what the network has learned on
artistic images. Then, we provide a quantitative analysis of the changes
introduced by the learning process thanks to metrics in both the feature and
parameter spaces, as well as metrics computed on the set of maximal activation
images. These analyses are performed on several variations of the transfer
learning procedure. In particular, we observed that the network could
specialize some pre-trained filters to the new image modality and also that
higher layers tend to concentrate classes. Finally, we have shown that a double
fine-tuning involving a medium-size artistic dataset can improve the
classification on smaller datasets, even when the task changes.
</p>
<a href="http://arxiv.org/abs/2011.02727" target="_blank">arXiv:2011.02727</a> [<a href="http://arxiv.org/pdf/2011.02727" target="_blank">pdf</a>]

<h2>A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs. (arXiv:2011.03083v2 [cs.CV] UPDATED)</h2>
<h3>Souvik Kundu, Mahdi Nazemi, Peter A. Beerel, Massoud Pedram</h3>
<p>This paper presents a dynamic network rewiring (DNR) method to generate
pruned deep neural network (DNN) models that are robust against adversarial
attacks yet maintain high accuracy on clean images. In particular, the
disclosed DNR method is based on a unified constrained optimization formulation
using a hybrid loss function that merges ultra-high model compression with
robust adversarial training. This training strategy dynamically adjusts
inter-layer connectivity based on per-layer normalized momentum computed from
the hybrid loss function. In contrast to existing robust pruning frameworks
that require multiple training iterations, the proposed learning strategy
achieves an overall target pruning ratio with only a single training iteration
and can be tuned to support both irregular and structured channel pruning. To
evaluate the merits of DNR, experiments were performed with two widely accepted
models, namely VGG16 and ResNet-18, on CIFAR-10, CIFAR-100 as well as with
VGG16 on Tiny-ImageNet. Compared to the baseline uncompressed models, DNR
provides over20x compression on all the datasets with no significant drop in
either clean or adversarial classification accuracy. Moreover, our experiments
show that DNR consistently finds compressed models with better clean and
adversarial image classification performance than what is achievable through
state-of-the-art alternatives.
</p>
<a href="http://arxiv.org/abs/2011.03083" target="_blank">arXiv:2011.03083</a> [<a href="http://arxiv.org/pdf/2011.03083" target="_blank">pdf</a>]

<h2>A decision-making tool to fine-tune abnormal levels in the complete blood count tests. (arXiv:2011.05900v2 [stat.ML] UPDATED)</h2>
<h3>Marta Avalos-Fernandez, Helene Touchais, Marcela Henriquez-Henriquez</h3>
<p>The complete blood count (CBC) performed by automated hematology analyzers is
one of the most ordered laboratory tests. It is a first-line tool for assessing
a patient's general health status, or diagnosing and monitoring disease
progression. When the analysis does not fit an expected setting, technologists
manually review a blood smear using a microscope. The International Consensus
Group for Hematology Review published in 2005 a set of criteria for reviewing
CBCs. Commonly, adjustments are locally needed to account for laboratory
resources and populations characteristics. Our objective is to provide a
decision support tool to identify which CBC variables are associated with
higher risks of abnormal smear and at which cutoff values. We propose a
cost-sensitive Lasso-penalized additive logistic regression combined with
stability selection. Using simulated and real CBC data, we demonstrate that our
tool correctly identify the true cutoff values, provided that there is enough
available data in their neighbourhood.
</p>
<a href="http://arxiv.org/abs/2011.05900" target="_blank">arXiv:2011.05900</a> [<a href="http://arxiv.org/pdf/2011.05900" target="_blank">pdf</a>]

<h2>Artificial Neural Variability for Deep Learning: On Overfitting, Noise Memorization, and Catastrophic Forgetting. (arXiv:2011.06220v2 [cs.LG] UPDATED)</h2>
<h3>Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng Tao, Masashi Sugiyama</h3>
<p>Deep learning is often criticized by two serious issues which rarely exist in
natural nervous systems: overfitting and catastrophic forgetting. It can even
memorize randomly labelled data, which has little knowledge behind the
instance-label pairs. When a deep network continually learns over time by
accommodating new tasks, it usually quickly overwrites the knowledge learned
from previous tasks. Referred to as the neural variability, it is well-known in
neuroscience that human brain reactions exhibit substantial variability even in
response to the same stimulus. This mechanism balances accuracy and
plasticity/flexibility in the motor learning of natural nervous systems. Thus
it motivates us to design a similar mechanism named artificial neural
variability (ANV), which helps artificial neural networks learn some advantages
from "natural" neural networks. We rigorously prove that ANV plays as an
implicit regularizer of the mutual information between the training data and
the learned model. This result theoretically guarantees ANV a strictly improved
generalizability, robustness to label noise, and robustness to catastrophic
forgetting. We then devise a neural variable risk minimization (NVRM) framework
and neural variable optimizers to achieve ANV for conventional network
architectures in practice. The empirical studies demonstrate that NVRM can
effectively relieve overfitting, label noise memorization, and catastrophic
forgetting at negligible costs.
</p>
<a href="http://arxiv.org/abs/2011.06220" target="_blank">arXiv:2011.06220</a> [<a href="http://arxiv.org/pdf/2011.06220" target="_blank">pdf</a>]

<h2>Foundations of Bayesian Learning from Synthetic Data. (arXiv:2011.08299v2 [cs.LG] UPDATED)</h2>
<h3>Harrison Wilde, Jack Jewson, Sebastian Vollmer, Chris Holmes</h3>
<p>There is significant growth and interest in the use of synthetic data as an
enabler for machine learning in environments where the release of real data is
restricted due to privacy or availability constraints. Despite a large number
of methods for synthetic data generation, there are comparatively few results
on the statistical properties of models learnt on synthetic data, and fewer
still for situations where a researcher wishes to augment real data with
another party's synthesised data. We use a Bayesian paradigm to characterise
the updating of model parameters when learning in these settings, demonstrating
that caution should be taken when applying conventional learning algorithms
without appropriate consideration of the synthetic data generating process and
learning task. Recent results from general Bayesian updating support a novel
and robust approach to Bayesian synthetic-learning founded on decision theory
that outperforms standard approaches across repeated experiments on supervised
learning and inference problems.
</p>
<a href="http://arxiv.org/abs/2011.08299" target="_blank">arXiv:2011.08299</a> [<a href="http://arxiv.org/pdf/2011.08299" target="_blank">pdf</a>]

<h2>AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries. (arXiv:2011.08435v2 [cs.LG] UPDATED)</h2>
<h3>Qianjiang Hu, Xiao Wang, Wei Hu, Guo-Jun Qi</h3>
<p>Contrastive learning relies on constructing a collection of negative examples
that are sufficiently hard to discriminate against positive queries when their
representations are self-trained. Existing contrastive learning methods either
maintain a queue of negative samples over minibatches while only a small
portion of them are updated in an iteration, or only use the other examples
from the current minibatch as negatives. They could not closely track the
change of the learned representation over iterations by updating the entire
queue as a whole, or discard the useful information from the past minibatches.
Alternatively, we present to directly learn a set of negative adversaries
playing against the self-trained representation. Two players, the
representation network and negative adversaries, are alternately updated to
obtain the most challenging negative examples against which the representation
of positive queries will be trained to discriminate. We further show that the
negative adversaries are updated towards a weighted combination of positive
queries by maximizing the adversarial contrastive loss, thereby allowing them
to closely track the change of representations over time. Experiment results
demonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves
superior performances with little computational overhead to the
state-of-the-art contrast models, but also can be pretrained more rapidly with
fewer epochs.
</p>
<a href="http://arxiv.org/abs/2011.08435" target="_blank">arXiv:2011.08435</a> [<a href="http://arxiv.org/pdf/2011.08435" target="_blank">pdf</a>]

<h2>A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus. (arXiv:2011.09046v2 [cs.CV] UPDATED)</h2>
<h3>Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, Fei Sha</h3>
<p>Identifying a short segment in a long video that semantically matches a text
query is a challenging task that has important application potentials in
language-based video search, browsing, and navigation. Typical retrieval
systems respond to a query with either a whole video or a pre-defined video
segment, but it is challenging to localize undefined segments in untrimmed and
unsegmented videos where exhaustively searching over all possible segments is
intractable. The outstanding challenge is that the representation of a video
must account for different levels of granularity in the temporal domain. To
tackle this problem, we propose the HierArchical Multi-Modal EncodeR (HAMMER)
that encodes a video at both the coarse-grained clip level and the fine-grained
frame level to extract information at different scales based on multiple
subtasks, namely, video retrieval, segment temporal localization, and masked
language modeling. We conduct extensive experiments to evaluate our model on
moment localization in video corpus on ActivityNet Captions and TVR datasets.
Our approach outperforms the previous methods as well as strong baselines,
establishing new state-of-the-art for this task.
</p>
<a href="http://arxiv.org/abs/2011.09046" target="_blank">arXiv:2011.09046</a> [<a href="http://arxiv.org/pdf/2011.09046" target="_blank">pdf</a>]

<h2>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v2 [cs.LG] UPDATED)</h2>
<h3>Hao-Zhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, Wei Chen</h3>
<p>Conventional unsupervised multi-source domain adaptation (UMDA) methods
assume all source domains can be accessed directly. This neglects the
privacy-preserving policy, that is, all the data and computations must be kept
decentralized. There exists three problems in this scenario: (1) Minimizing the
domain distance requires the pairwise calculation of the data from source and
target domains, which is not accessible. (2) The communication cost and privacy
security limit the application of UMDA methods (e.g., the domain adversarial
training). (3) Since users have no authority to check the data quality, the
irrelevant or malicious source domains are more likely to appear, which causes
negative transfer. In this study, we propose a privacy-preserving UMDA paradigm
named Knowledge Distillation based Decentralized Domain Adaptation (KD3A),
which performs domain adaptation through the knowledge distillation on models
from different source domains. KD3A solves the above problems with three
components: (1) A multi-source knowledge distillation method named Knowledge
Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting
strategy named Consensus Focus to identify both the malicious and irrelevant
domains. (3) A decentralized optimization strategy for domain distance named
BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is
robust to the negative transfer and brings a 100x reduction of communication
cost compared with other decentralized UMDA methods. Moreover, our KD3A
significantly outperforms state-of-the-art UMDA approaches.
</p>
<a href="http://arxiv.org/abs/2011.09757" target="_blank">arXiv:2011.09757</a> [<a href="http://arxiv.org/pdf/2011.09757" target="_blank">pdf</a>]

<h2>Towards Learning Controllable Representations of Physical Systems. (arXiv:2011.09906v2 [cs.LG] UPDATED)</h2>
<h3>Kevin Haninger, Raul Vicente Garcia, Joerg Krueger</h3>
<p>Learned representations of dynamical systems reduce dimensionality,
potentially supporting downstream reinforcement learning (RL). However, no
established methods predict a representation's suitability for control and
evaluation is largely done via downstream RL performance, slowing
representation design. Towards a principled evaluation of representations for
control, we consider the relationship between the true state and the
corresponding representations, proposing that ideally each representation
corresponds to a unique true state. This motivates two metrics: temporal
smoothness and high mutual information between true state/representation. These
metrics are related to established representation objectives, and studied on
Lagrangian systems where true state, information requirements, and statistical
properties of the state can be formalized for a broad class of systems. These
metrics are shown to predict reinforcement learning performance in a simulated
peg-in-hole task when comparing variants of autoencoder-based representations.
</p>
<a href="http://arxiv.org/abs/2011.09906" target="_blank">arXiv:2011.09906</a> [<a href="http://arxiv.org/pdf/2011.09906" target="_blank">pdf</a>]

<h2>Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v3 [cs.CV] UPDATED)</h2>
<h3>Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to remote sensing, where unlabeled data is often
abundant but labeled data is scarce. We first show that due to their different
characteristics, a non-trivial gap persists between contrastive and supervised
learning on standard benchmarks. To close the gap, we propose novel training
methods that exploit the spatiotemporal structure of remote sensing data. We
leverage spatially aligned images over time to construct temporal positive
pairs in contrastive learning and geo-location to design pre-text tasks. Our
experiments show that our proposed method closes the gap between contrastive
and supervised learning on image classification, object detection and semantic
segmentation for remote sensing and other geo-tagged image datasets
</p>
<a href="http://arxiv.org/abs/2011.09980" target="_blank">arXiv:2011.09980</a> [<a href="http://arxiv.org/pdf/2011.09980" target="_blank">pdf</a>]

<h2>Inverse Constrained Reinforcement Learning. (arXiv:2011.09999v2 [cs.LG] UPDATED)</h2>
<h3>Usman Anwar, Shehryar Malik, Alireza Aghasi, Ali Ahmed</h3>
<p>Standard reinforcement learning (RL) algorithms train agents to maximize
given reward functions. However, many real-world applications of RL require
agents to also satisfy certain constraints which may, for example, be motivated
by safety concerns. Constrained RL algorithms approach this problem by training
agents to maximize given reward functions while respecting \textit{explicitly}
defined constraints. However, in many cases, manually designing accurate
constraints is a challenging task. In this work, given a reward function and a
set of demonstrations from an expert that maximizes this reward function while
respecting \textit{unknown} constraints, we propose a framework to learn the
most likely constraints that the expert respects. We then train agents to
maximize the given reward function subject to the learned constraints. Previous
works in this regard have either mainly been restricted to tabular settings or
specific types of constraints or assume knowledge of transition dynamics of the
environment. In contrast, we empirically show that our framework is able to
learn arbitrary \textit{Markovian} constraints in high-dimensions in a
model-free setting.
</p>
<a href="http://arxiv.org/abs/2011.09999" target="_blank">arXiv:2011.09999</a> [<a href="http://arxiv.org/pdf/2011.09999" target="_blank">pdf</a>]

<h2>Born Identity Network: Multi-way Counterfactual Map Generation to Explain a Classifier's Decision. (arXiv:2011.10381v2 [cs.CV] UPDATED)</h2>
<h3>Kwanseok Oh, Jee Seok Yoon, Heung-Il Suk</h3>
<p>There exists an apparent negative correlation between performance and
interpretability of deep learning models. In an effort to reduce this negative
correlation, we propose Born Identity Network (BIN), which is a post-hoc
approach for producing multi-way counterfactual maps. A counterfactual map
transforms an input sample to be classified as a target label, which is similar
to how humans process knowledge through counterfactual thinking. Thus,
producing a better counterfactual map may be a step towards explanation at the
level of human knowledge. For example, a counterfactual map can localize
hypothetical abnormalities from a normal brain image that may cause it to be
diagnosed with a disease. Specifically, our proposed BIN consists of two core
components: Counterfactual Map Generator and Target Attribution Network. The
Counterfactual Map Generator is a variation of conditional GAN which can
synthesize a counterfactual map conditioned on an arbitrary target label. The
Target Attribution Network works in a complementary manner to enforce target
label attribution to the synthesized map. We have validated our proposed BIN in
qualitative, quantitative analysis on MNIST, 3D Shapes, and ADNI datasets, and
show the comprehensibility and fidelity of our method from various ablation
studies.
</p>
<a href="http://arxiv.org/abs/2011.10381" target="_blank">arXiv:2011.10381</a> [<a href="http://arxiv.org/pdf/2011.10381" target="_blank">pdf</a>]

<h2>Boundary-sensitive Pre-training for Temporal Localization in Videos. (arXiv:2011.10830v2 [cs.CV] UPDATED)</h2>
<h3>Mengmeng Xu, Juan-Manuel Perez-Rua, Victor Escorcia, Brais Martinez, Xiatian Zhu, Li Zhang, Bernard Ghanem, Tao Xiang</h3>
<p>Many video analysis tasks require temporal localization thus detection of
content changes. However, most existing models developed for these tasks are
pre-trained on general video action classification tasks. This is because large
scale annotation of temporal boundaries in untrimmed videos is expensive.
Therefore no suitable datasets exist for temporal boundary-sensitive
pre-training. In this paper for the first time, we investigate model
pre-training for temporal localization by introducing a novel
boundary-sensitive pretext (BSP) task. Instead of relying on costly manual
annotations of temporal boundaries, we propose to synthesize temporal
boundaries in existing video action classification datasets. With the
synthesized boundaries, BSP can be simply conducted via classifying the
boundary types. This enables the learning of video representations that are
much more transferable to downstream temporal localization tasks. Extensive
experiments show that the proposed BSP is superior and complementary to the
existing action classification based pre-training counterpart, and achieves new
state-of-the-art performance on several temporal localization tasks.
</p>
<a href="http://arxiv.org/abs/2011.10830" target="_blank">arXiv:2011.10830</a> [<a href="http://arxiv.org/pdf/2011.10830" target="_blank">pdf</a>]

<h2>FP-NAS: Fast Probabilistic Neural Architecture Search. (arXiv:2011.10949v2 [cs.CV] UPDATED)</h2>
<h3>Zhicheng Yan, Xiaoliang Dai, Peizhao Zhang, Yuandong Tian, Bichen Wu, Matt Feiszli</h3>
<p>Differential Neural Architecture Search (NAS) requires all layer choices to
be held in memory simultaneously; this limits the size of both search space and
final architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a
distribution over high-performing architectures, and uses only as much memory
as needed to train a single model. Nevertheless, it needs to sample many
architectures, making it computationally expensive for searching in an
extensive space. To solve these problems, we propose a sampling method adaptive
to the distribution entropy, drawing more samples to encourage explorations at
the beginning, and reducing samples as learning proceeds. Furthermore, to
search fast in the multi-variate space, we propose a coarse-to-fine strategy by
using a factorized distribution at the beginning which can reduce the number of
architecture parameters by over an order of magnitude.We call this method Fast
Probabilistic NAS (FP-NAS). Compared with PARSEC, it can sample 64% fewer
architectures and search 2.1x faster. Compared with FBNetV2, FP-NAS is 1.9x -
3.6x faster, and the searched models outperform FBNetV2 models on ImageNet.
FP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger
channel choices) and deeper (i.e. more blocks), while adding Split-Attention
block and enabling the search over the number of splits. When searching a model
of size 0.4G FLOPS, FP-NAS is 132x faster than EfficientNet, and the searched
FP-NAS-L0 model outperforms EfficientNet-B0 by 0.6% accuracy. Without using any
architecture surrogate or scaling tricks, we directly search large models up to
1.0G FLOPS. Our FP-NAS-L2 model with simple distillation outperforms BigNAS-XL
with advanced inplace distillation by 0.7% accuracy with less FLOPS.
</p>
<a href="http://arxiv.org/abs/2011.10949" target="_blank">arXiv:2011.10949</a> [<a href="http://arxiv.org/pdf/2011.10949" target="_blank">pdf</a>]

<h2>Time series classification for predictive maintenance on event logs. (arXiv:2011.10996v2 [cs.LG] UPDATED)</h2>
<h3>Antoine Guillaume, Christel Vrain, Elloumi Wael</h3>
<p>Time series classification (TSC) gained a lot of attention in the past decade
and number of methods for representing and classifying time series have been
proposed. Nowadays, methods based on convolutional networks and ensemble
techniques represent the state of the art for time series classification.
Techniques transforming time series to image or text also provide reliable ways
to extract meaningful features or representations of time series. We compare
the state-of-the-art representation and classification methods on a specific
application, that is predictive maintenance from sequences of event logs. The
contributions of this paper are twofold: introducing a new data set for
predictive maintenance on automated teller machines (ATMs) log data and
comparing the performance of different representation methods for predicting
the occurrence of a breakdown. The problem is difficult since unlike the
classic case of predictive maintenance via signals from sensors, we have
sequences of discrete event logs occurring at any time and the lengths of the
sequences, corresponding to life cycles, vary a lot.
</p>
<a href="http://arxiv.org/abs/2011.10996" target="_blank">arXiv:2011.10996</a> [<a href="http://arxiv.org/pdf/2011.10996" target="_blank">pdf</a>]

<h2>A Homotopy-based Algorithm for Sparse Multiple Right-hand Sides Nonnegative Least Squares. (arXiv:2011.11066v2 [cs.LG] UPDATED)</h2>
<h3>Nicolas Nadisic, Arnaud Vandaele, Nicolas Gillis</h3>
<p>Nonnegative least squares (NNLS) problems arise in models that rely on
additive linear combinations. In particular, they are at the core of
nonnegative matrix factorization (NMF) algorithms. The nonnegativity constraint
is known to naturally favor sparsity, that is, solutions with few non-zero
entries. However, it is often useful to further enhance this sparsity, as it
improves the interpretability of the results and helps reducing noise. While
the $\ell_0$-"norm", equal to the number of non-zeros entries in a vector, is a
natural sparsity measure, its combinatorial nature makes it difficult to use in
practical optimization schemes. Most existing approaches thus rely either on
its convex surrogate, the $\ell_1$-norm, or on heuristics such as greedy
algorithms. In the case of multiple right-hand sides NNLS (MNNLS), which are
used within NMF algorithms, sparsity is often enforced column- or row-wise, and
the fact that the solution is a matrix is not exploited. In this paper, we
first introduce a novel formulation for sparse MNNLS, with a matrix-wise
$\ell_0$ sparsity constraint. Then, we present a two-step algorithm to tackle
this problem. The first step uses a homotopy algorithm to produce the whole
regularization path for all the $\ell_1$-penalized NNLS problems arising in
MNNLS, that is, to produce a set of solutions representing different tradeoffs
between reconstruction error and sparsity. The second step selects solutions
among these paths in order to build a sparsity-constrained matrix that
minimizes the reconstruction error. We illustrate the advantages of our
proposed algorithm for the unmixing of facial and hyperspectral images.
</p>
<a href="http://arxiv.org/abs/2011.11066" target="_blank">arXiv:2011.11066</a> [<a href="http://arxiv.org/pdf/2011.11066" target="_blank">pdf</a>]

<h2>Stable Weight Decay Regularization. (arXiv:2011.11152v2 [cs.LG] UPDATED)</h2>
<h3>Zeke Xie, Issei Sato, Masashi Sugiyama</h3>
<p>Weight decay is a popular regularization technique for training of deep
neural networks. Modern deep learning libraries mainly use $L_{2}$
regularization as the default implementation of weight decay.
\citet{loshchilov2018decoupled} demonstrated that $L_{2}$ regularization is not
identical to weight decay for adaptive gradient methods, such as Adaptive
Momentum Estimation (Adam), and proposed Adam with Decoupled Weight Decay
(AdamW). However, we found that the popular implementations of weight decay,
including $L_{2}$ regularization and decoupled weight decay, in modern deep
learning libraries usually damage performance. First, the $L_{2}$
regularization is unstable weight decay for all optimizers that use Momentum,
such as stochastic gradient descent (SGD). Second, decoupled weight decay is
highly unstable for all adaptive gradient methods. We further propose the
Stable Weight Decay (SWD) method to fix the unstable weight decay problem from
a dynamical perspective. The proposed SWD method makes significant improvements
over $L_{2}$ regularization and decoupled weight decay in our experiments.
Simply fixing weight decay in Adam by SWD, with no extra hyperparameter, can
usually outperform complex Adam variants, which have more hyperparameters.
</p>
<a href="http://arxiv.org/abs/2011.11152" target="_blank">arXiv:2011.11152</a> [<a href="http://arxiv.org/pdf/2011.11152" target="_blank">pdf</a>]

<h2>Structure-Aware Completion of Photogrammetric Meshes in Urban Road Environment. (arXiv:2011.11210v2 [cs.CV] UPDATED)</h2>
<h3>Qing Zhu, Qishen Shang, Han Hu, Haojia Yu, Ruofei Zhong</h3>
<p>Photogrammetric mesh models obtained from aerial oblique images have been
widely used for urban reconstruction. However, the photogrammetric meshes also
suffer from severe texture problems, especially on the road areas due to
occlusion. This paper proposes a structure-aware completion approach to improve
the quality of meshes by removing undesired vehicles on the road seamlessly.
Specifically, the discontinuous texture atlas is first integrated to a
continuous screen space through rendering by the graphics pipeline; the
rendering also records necessary mapping for deintegration to the original
texture atlas after editing. Vehicle regions are masked by a standard object
detection approach, e.g. Faster RCNN. Then, the masked regions are completed
guided by the linear structures and regularities in the road region, which is
implemented based on Patch Match. Finally, the completed rendered image is
deintegrated to the original texture atlas and the triangles for the vehicles
are also flattened for improved meshes. Experimental evaluations and analyses
are conducted against three datasets, which are captured with different sensors
and ground sample distances. The results reveal that the proposed method can
quite realistic meshes after removing the vehicles. The structure-aware
completion approach for road regions outperforms popular image completion
methods and ablation study further confirms the effectiveness of the linear
guidance. It should be noted that the proposed method is also capable to handle
tiled mesh models for large-scale scenes. Dataset and code are available at
vrlab.org.cn/~hanhu/projects/mesh.
</p>
<a href="http://arxiv.org/abs/2011.11210" target="_blank">arXiv:2011.11210</a> [<a href="http://arxiv.org/pdf/2011.11210" target="_blank">pdf</a>]

<h2>Adversarial Refinement Network for Human Motion Prediction. (arXiv:2011.11221v2 [cs.CV] UPDATED)</h2>
<h3>Xianjin Chao, Yanrui Bin, Wenqing Chu, Xuan Cao, Yanhao Ge, Chengjie Wang, Jilin Li, Feiyue Huang, Howard Leung</h3>
<p>Human motion prediction aims to predict future 3D skeletal sequences by
giving a limited human motion as inputs. Two popular methods, recurrent neural
networks and feed-forward deep networks, are able to predict rough motion
trend, but motion details such as limb movement may be lost. To predict more
accurate future human motion, we propose an Adversarial Refinement Network
(ARNet) following a simple yet effective coarse-to-fine mechanism with novel
adversarial error augmentation. Specifically, we take both the historical
motion sequences and coarse prediction as input of our cascaded refinement
network to predict refined human motion and strengthen the refinement network
with adversarial error augmentation. During training, we deliberately introduce
the error distribution by learning through the adversarial mechanism among
different subjects. In testing, our cascaded refinement network alleviates the
prediction error from the coarse predictor resulting in a finer prediction
robustly. This adversarial error augmentation provides rich error cases as
input to our refinement network, leading to better generalization performance
on the testing dataset. We conduct extensive experiments on three standard
benchmark datasets and show that our proposed ARNet outperforms other
state-of-the-art methods, especially on challenging aperiodic actions in both
short-term and long-term predictions.
</p>
<a href="http://arxiv.org/abs/2011.11221" target="_blank">arXiv:2011.11221</a> [<a href="http://arxiv.org/pdf/2011.11221" target="_blank">pdf</a>]

<h2>Legacy Photo Editing with Learned Noise Prior. (arXiv:2011.11309v2 [cs.CV] UPDATED)</h2>
<h3>Zhao Yuzhi, Po Lai-Man, Wang Xuehui, Liu Kangcheng, Zhang Yujia, Yu Wing-Yin, Xian Pengfei, Xiong Jingjing</h3>
<p>There are quite a number of photographs captured under undesirable conditions
in the last century. Thus, they are often noisy, regionally incomplete, and
grayscale formatted. Conventional approaches mainly focus on one point so that
those restoration results are not perceptually sharp or clean enough. To solve
these problems, we propose a noise prior learner NEGAN to simulate the noise
distribution of real legacy photos using unpaired images. It mainly focuses on
matching high-frequency parts of noisy images through discrete wavelet
transform (DWT) since they include most of noise statistics. We also create a
large legacy photo dataset for learning noise prior. Using learned noise prior,
we can easily build valid training pairs by degrading clean images. Then, we
propose an IEGAN framework performing image editing including joint denoising,
inpainting and colorization based on the estimated noise prior. We evaluate the
proposed system and compare it with state-of-the-art image enhancement methods.
The experimental results demonstrate that it achieves the best perceptual
quality.
https://github.com/zhaoyuzhi/Legacy-Photo-Editing-with-Learned-Noise-Prior for
the codes and the proposed LP dataset.
</p>
<a href="http://arxiv.org/abs/2011.11309" target="_blank">arXiv:2011.11309</a> [<a href="http://arxiv.org/pdf/2011.11309" target="_blank">pdf</a>]

<h2>PLOP: Learning without Forgetting for Continual Semantic Segmentation. (arXiv:2011.11390v2 [cs.CV] UPDATED)</h2>
<h3>Arthur Douillard, Yifu Chen, Arnaud Dapogny, Matthieu Cord</h3>
<p>Deep learning approaches are nowadays ubiquitously used to tackle computer
vision tasks such as semantic segmentation, requiring large datasets and
substantial computational power. Continual learning for semantic segmentation
(CSS) is an emerging trend that consists in updating an old model by
sequentially adding new classes. However, continual learning methods are
usually prone to catastrophic forgetting. This issue is further aggravated in
CSS where, at each step, old classes from previous iterations are collapsed
into the background. In this paper, we propose Local POD, a multi-scale pooling
distillation scheme that preserves long- and short-range spatial relationships
at feature level. Furthermore, we design an entropy-based pseudo-labelling of
the background w.r.t. classes predicted by the old model to deal with
background shift and avoid catastrophic forgetting of the old classes. Our
approach, called PLOP, significantly outperforms state-of-the-art methods in
existing CSS scenarios, as well as in newly proposed challenging benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.11390" target="_blank">arXiv:2011.11390</a> [<a href="http://arxiv.org/pdf/2011.11390" target="_blank">pdf</a>]

<h2>Imagination-enabled Robot Perception. (arXiv:2011.11397v2 [cs.RO] UPDATED)</h2>
<h3>Patrick Mania, Franklin Kenghagho Kenfack, Michael Neumann, Michael Beetz</h3>
<p>Many of today's robot perception systems aim at accomplishing perception
tasks that are too simplistic and too hard. They are too simplistic because
they do not require the perception systems to provide all the information
needed to accomplish manipulation tasks. Typically the perception results do
not include information about the part structure of objects, articulation
mechanisms and other attributes needed for adapting manipulation behavior. On
the other hand, the perception problems stated are also too hard because --
unlike humans -- the perception systems cannot leverage the expectations about
what they will see to their full potential. Therefore, we investigate a
variation of robot perception tasks suitable for robots accomplishing everyday
manipulation tasks, such as household robots or a robot in a retail store. In
such settings it is reasonable to assume that robots know most objects and have
detailed models of them.

We propose a perception system that maintains its beliefs about its
environment as a scene graph with physics simulation and visual rendering. When
detecting objects, the perception system retrieves the model of the object and
places it at the corresponding place in a VR-based environment model. The
physics simulation ensures that object detections that are physically not
possible are rejected and scenes can be rendered to generate expectations at
the image level. The result is a perception system that can provide useful
information for manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2011.11397" target="_blank">arXiv:2011.11397</a> [<a href="http://arxiv.org/pdf/2011.11397" target="_blank">pdf</a>]

<h2>Pareto-efficient Acquisition Functions for Cost-Aware Bayesian Optimization. (arXiv:2011.11456v2 [cs.LG] UPDATED)</h2>
<h3>Gauthier Guinet, Valerio Perrone, C&#xe9;dric Archambeau</h3>
<p>Bayesian optimization (BO) is a popular method to optimize expensive
black-box functions. It efficiently tunes machine learning algorithms under the
implicit assumption that hyperparameter evaluations cost approximately the
same. In reality, the cost of evaluating different hyperparameters, be it in
terms of time, dollars or energy, can span several orders of magnitude of
difference. While a number of heuristics have been proposed to make BO
cost-aware, none of these have been proven to work robustly. In this work, we
reformulate cost-aware BO in terms of Pareto efficiency and introduce the cost
Pareto Front, a mathematical object allowing us to highlight the shortcomings
of commonly used acquisition functions. Based on this, we propose a novel
Pareto-efficient adaptation of the expected improvement. On 144 real-world
black-box function optimization problems we show that our Pareto-efficient
acquisition functions significantly outperform previous solutions, bringing up
to 50% speed-ups while providing finer control over the cost-accuracy
trade-off. We also revisit the common choice of Gaussian process cost models,
showing that simple, low-variance cost models predict training times
effectively.
</p>
<a href="http://arxiv.org/abs/2011.11456" target="_blank">arXiv:2011.11456</a> [<a href="http://arxiv.org/pdf/2011.11456" target="_blank">pdf</a>]

<h2>HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features. (arXiv:2011.11498v2 [cs.CV] UPDATED)</h2>
<h3>Cheng Sun, Min Sun, Hwann-Tzong Chen</h3>
<p>We present HoHoNet, a versatile and efficient framework for holistic
understanding of an indoor 360-degree panorama using a Latent Horizontal
Feature (LHFeat). The compact LHFeat flattens the features along the vertical
direction and has shown success in modeling per-column modality for room layout
reconstruction. HoHoNet advances in two important aspects. First, the deep
architecture is redesigned to run faster with improved accuracy. Second, we
propose a novel horizon-to-dense module, which relaxes the per-column output
shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is
fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones
respectively, for modeling dense modalities from a high-resolution $512 \times
1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and
semantic segmentation, HoHoNet achieves results on par with current
state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior
arts by a large margin.
</p>
<a href="http://arxiv.org/abs/2011.11498" target="_blank">arXiv:2011.11498</a> [<a href="http://arxiv.org/pdf/2011.11498" target="_blank">pdf</a>]

<h2>RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers. (arXiv:2011.11572v2 [cs.CV] UPDATED)</h2>
<h3>Saeid Asgari Taghanaki, Jieliang Luo, Ran Zhang, Ye Wang, Pradeep Kumar Jayaraman, Krishna Murthy Jatavallabhula</h3>
<p>The 3D deep learning community has seen significant strides in pointcloud
processing over the last few years. However, the datasets on which deep models
have been trained have largely remained the same. Most datasets comprise clean,
clutter-free pointclouds canonicalized for pose. Models trained on these
datasets fail in uninterpretible and unintuitive ways when presented with data
that contains transformations "unseen" at train time. While data augmentation
enables models to be robust to "previously seen" input transformations, 1) we
show that this does not work for unseen transformations during inference, and
2) data augmentation makes it difficult to analyze a model's inherent
robustness to transformations. To this end, we create a publicly available
dataset for robustness analysis of point cloud classification models
(independent of data augmentation) to input transformations, called
\textbf{RobustPointSet}. Our experiments indicate that despite all the progress
in the point cloud classification, PointNet (the very first multi-layered
perceptron-based approach) outperforms other methods (e.g., graph and neighbor
based methods) when evaluated on transformed test sets. We also find that most
of the current point cloud models are not robust to unseen transformations even
if they are trained with extensive data augmentation. RobustPointSet can be
accessed through https://github.com/AutodeskAILab/RobustPointSet.
</p>
<a href="http://arxiv.org/abs/2011.11572" target="_blank">arXiv:2011.11572</a> [<a href="http://arxiv.org/pdf/2011.11572" target="_blank">pdf</a>]

<h2>Object Rearrangement Using Learned Implicit Collision Functions. (arXiv:2011.10726v1 [cs.RO] CROSS LISTED)</h2>
<h3>Michael Danielczuk, Arsalan Mousavian, Clemens Eppner, Dieter Fox</h3>
<p>Robotic object rearrangement combines the skills of picking and placing
objects. When object models are unavailable, typical collision-checking models
may be unable to predict collisions in partial point clouds with occlusions,
making generation of collision-free grasping or placement trajectories
challenging. We propose a learned collision model that accepts scene and query
object point clouds and predicts collisions for 6DOF object poses within the
scene. We train the model on a synthetic set of 1 million scene/object point
cloud pairs and 2 billion collision queries. We leverage the learned collision
model as part of a model predictive path integral (MPPI) policy in a tabletop
rearrangement task and show that the policy can plan collision-free grasps and
placements for objects unseen in training in both simulated and physical
cluttered scenes with a Franka Panda robot. The learned model outperforms both
traditional pipelines and learned ablations by 9.8% in accuracy on a dataset of
simulated collision queries and is 75x faster than the best-performing
baseline. Videos and supplementary material are available at
https://sites.google.com/nvidia.com/scenecollisionnet.
</p>
<a href="http://arxiv.org/abs/2011.10726" target="_blank">arXiv:2011.10726</a> [<a href="http://arxiv.org/pdf/2011.10726" target="_blank">pdf</a>]

