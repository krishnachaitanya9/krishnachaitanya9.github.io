---
title: Latest Deep Learning Papers
date: 2021-03-16 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (82 Articles)</h1>
<h2>Autonomous Drone Racing with Deep Reinforcement Learning. (arXiv:2103.08624v1 [cs.RO])</h2>
<h3>Yunlong Song, Mats Steinweg, Elia Kaufmann, Davide Scaramuzza</h3>
<p>In many robotic tasks, such as drone racing, the goal is to travel through a
set of waypoints as fast as possible. A key challenge for this task is planning
the minimum-time trajectory, which is typically solved by assuming perfect
knowledge of the waypoints to pass in advance. The resulting solutions are
either highly specialized for a single-track layout, or suboptimal due to
simplifying assumptions about the platform dynamics. In this work, a new
approach to minimum-time trajectory generation for quadrotors is presented.
Leveraging deep reinforcement learning and relative gate observations, this
approach can adaptively compute near-time-optimal trajectories for random track
layouts. Our method exhibits a significant computational advantage over
approaches based on trajectory optimization for non-trivial track
configurations. The proposed approach is evaluated on a set of race tracks in
simulation and the real world, achieving speeds of up to 17 m/s with a physical
quadrotor.
</p>
<a href="http://arxiv.org/abs/2103.08624" target="_blank">arXiv:2103.08624</a> [<a href="http://arxiv.org/pdf/2103.08624" target="_blank">pdf</a>]

<h2>Domain-Incremental Continual Learning for Mitigating Bias in Facial Expression and Action Unit Recognition. (arXiv:2103.08637v1 [cs.CV])</h2>
<h3>Nikhil Churamani, Ozgur Kara, Hatice Gunes</h3>
<p>As Facial Expression Recognition (FER) systems become integrated into our
daily lives, these systems need to prioritise making fair decisions instead of
aiming at higher individual accuracy scores. Ranging from surveillance systems
to diagnosing mental and emotional health conditions of individuals, these
systems need to balance the accuracy vs fairness trade-off to make decisions
that do not unjustly discriminate against specific under-represented
demographic groups. Identifying bias as a critical problem in facial analysis
systems, different methods have been proposed that aim to mitigate bias both at
data and algorithmic levels. In this work, we propose the novel usage of
Continual Learning (CL), in particular, using Domain-Incremental Learning
(Domain-IL) settings, as a potent bias mitigation method to enhance the
fairness of FER systems while guarding against biases arising from skewed data
distributions. We compare different non-CL-based and CL-based methods for their
classification accuracy and fairness scores on expression recognition and
Action Unit (AU) detection tasks using two popular benchmarks, the RAF-DB and
BP4D datasets, respectively. Our experimental results show that CL-based
methods, on average, outperform other popular bias mitigation techniques on
both accuracy and fairness metrics.
</p>
<a href="http://arxiv.org/abs/2103.08637" target="_blank">arXiv:2103.08637</a> [<a href="http://arxiv.org/pdf/2103.08637" target="_blank">pdf</a>]

<h2>UPANets: Learning from the Universal Pixel Attention Networks. (arXiv:2103.08640v1 [cs.CV])</h2>
<h3>Ching-Hsun Tseng, Shin-Jye Lee, Jia-Nan Feng, Shengzhong Mao, Yu-Ping Wu, Jia-Yu Shang, Mou-Chung Tseng, Xiao-Jun Zeng</h3>
<p>Among image classification, skip and densely-connection-based networks have
dominated most leaderboards. Recently, from the successful development of
multi-head attention in natural language processing, it is sure that now is a
time of either using a Transformer-like model or hybrid CNNs with attention.
However, the former need a tremendous resource to train, and the latter is in
the perfect balance in this direction. In this work, to make CNNs handle global
and local information, we proposed UPANets, which equips channel-wise attention
with a hybrid skip-densely-connection structure. Also, the extreme-connection
structure makes UPANets robust with a smoother loss landscape. In experiments,
UPANets surpassed most well-known and widely-used SOTAs with an accuracy of
96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most
importantly, these performances have high parameters efficiency and only
trained in one customer-based GPU. We share implementing code of UPANets in
https://github.com/hanktseng131415go/UPANets.
</p>
<a href="http://arxiv.org/abs/2103.08640" target="_blank">arXiv:2103.08640</a> [<a href="http://arxiv.org/pdf/2103.08640" target="_blank">pdf</a>]

<h2>Robotics During a Pandemic: The 2020 NSF CPS Virtual Challenge -- SoilScope, Mars Edition. (arXiv:2103.08684v1 [cs.RO])</h2>
<h3>Darwin Mick, K. Srikar Siddarth, Swastik Nandan, Harish Anand, Stephen A. Rees, Jnaneshwar Das</h3>
<p>Remote sample recovery is a rapidly evolving application of Small Unmanned
Aircraft Systems (sUAS) for planetary sciences and space exploration.
Development of cyber-physical systems (CPS) for autonomous deployment and
recovery of sensor probes for sample caching is already in progress with NASA's
MARS 2020 mission. To challenge student teams to develop autonomy for sample
recovery settings, the 2020 NSF CPS Challenge was positioned around the launch
of the MARS 2020 rover and sUAS duo. This paper discusses perception and
trajectory planning for sample recovery by sUAS in a simulation environment.
Out of a total of five teams that participated, the results of the top two
teams have been discussed. The OpenUAV cloud simulation framework deployed on
the Cyber-Physical Systems Virtual Organization (CPS-VO) allowed the teams to
work remotely over a month during the COVID-19 pandemic to develop and simulate
autonomous exploration algorithms. Remote simulation enabled teams across the
globe to collaborate in experiments. The two teams approached the task of probe
search, probe recovery, and landing on a moving target differently. This paper
is a summary of teams' insights and lessons learned, as they chose from a wide
range of perception sensors and algorithms.
</p>
<a href="http://arxiv.org/abs/2103.08684" target="_blank">arXiv:2103.08684</a> [<a href="http://arxiv.org/pdf/2103.08684" target="_blank">pdf</a>]

<h2>Variable compliance and geometry regulation of Soft-Bubble grippers with active pressure control. (arXiv:2103.08710v1 [cs.RO])</h2>
<h3>Sihah Joonhigh, Naveen Kuppuswamy, Andrew Beaulieu, Alex Alspach, Russ Tedrake</h3>
<p>While compliant grippers have become increasingly commonplace in robot
manipulation, finding the right stiffness and geometry for grasping the widest
variety of objects remains a key challenge. Adjusting both stiffness and
gripper geometry on the fly may provide the versatility needed to manipulate
the large range of objects found in domestic environments. We present a system
for actively controlling the geometry (inflation level) and compliance of
Soft-bubble grippers - air filled, highly compliant parallel gripper fingers
incorporating visuotactile sensing. The proposed system enables large,
controlled changes in gripper finger geometry and grasp stiffness, as well as
simple in-hand manipulation. We also demonstrate, despite these changes, the
continued viability of advanced perception capabilities such as dense geometry
and shear force measurement - we present a straightforward extension of our
previously presented approach for measuring shear induced displacements using
the internal imaging sensor and taking into account pressure and geometry
changes. We quantify the controlled variation of grasp-free geometry, grasp
stiffness and contact patch geometry resulting from pressure regulation and we
demonstrate new capabilities for the gripper in the home by grasping in
constrained spaces, manipulating tools requiring lower and higher stiffness
grasps, as well as contact patch modulation.
</p>
<a href="http://arxiv.org/abs/2103.08710" target="_blank">arXiv:2103.08710</a> [<a href="http://arxiv.org/pdf/2103.08710" target="_blank">pdf</a>]

<h2>S3Net: 3D LiDAR Sparse Semantic Segmentation Network. (arXiv:2103.08745v1 [cs.CV])</h2>
<h3>Ran Cheng, Ryan Razani, Yuan Ren, Liu Bingbing</h3>
<p>Semantic Segmentation is a crucial component in the perception systems of
many applications, such as robotics and autonomous driving that rely on
accurate environmental perception and understanding. In literature, several
approaches are introduced to attempt LiDAR semantic segmentation task, such as
projection-based (range-view or birds-eye-view), and voxel-based approaches.
However, they either abandon the valuable 3D topology and geometric relations
and suffer from information loss introduced in the projection process or are
inefficient. Therefore, there is a need for accurate models capable of
processing the 3D driving-scene point cloud in 3D space. In this paper, we
propose S3Net, a novel convolutional neural network for LiDAR point cloud
semantic segmentation. It adopts an encoder-decoder backbone that consists of
Sparse Intra-channel Attention Module (SIntraAM), and Sparse Inter-channel
Attention Module (SInterAM) to emphasize the fine details of both within each
feature map and among nearby feature maps. To extract the global contexts in
deeper layers, we introduce Sparse Residual Tower based upon sparse convolution
that suits varying sparsity of LiDAR point cloud. In addition, geo-aware
anisotrophic loss is leveraged to emphasize the semantic boundaries and
penalize the noise within each predicted regions, leading to a robust
prediction. Our experimental results show that the proposed method leads to a
large improvement (12\%) compared to its baseline counterpart (MinkNet42
\cite{choy20194d}) on SemanticKITTI \cite{DBLP:conf/iccv/BehleyGMQBSG19} test
set and achieves state-of-the-art mIoU accuracy of semantic segmentation
approaches.
</p>
<a href="http://arxiv.org/abs/2103.08745" target="_blank">arXiv:2103.08745</a> [<a href="http://arxiv.org/pdf/2103.08745" target="_blank">pdf</a>]

<h2>Revisiting Dynamic Convolution via Matrix Decomposition. (arXiv:2103.08756v1 [cs.CV])</h2>
<h3>Yunsheng Li, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Ye Yu, Lu Yuan, Zicheng Liu, Mei Chen, Nuno Vasconcelos</h3>
<p>Recent research in dynamic convolution shows substantial performance boost
for efficient CNNs, due to the adaptive aggregation of K static convolution
kernels. It has two limitations: (a) it increases the number of convolutional
weights by K-times, and (b) the joint optimization of dynamic attention and
static convolution kernels is challenging. In this paper, we revisit it from a
new perspective of matrix decomposition and reveal the key issue is that
dynamic convolution applies dynamic attention over channel groups after
projecting into a higher dimensional latent space. To address this issue, we
propose dynamic channel fusion to replace dynamic attention over channel
groups. Dynamic channel fusion not only enables significant dimension reduction
of the latent space, but also mitigates the joint optimization difficulty. As a
result, our method is easier to train and requires significantly fewer
parameters without sacrificing accuracy. Source code is at
https://github.com/liyunsheng13/dcd.
</p>
<a href="http://arxiv.org/abs/2103.08756" target="_blank">arXiv:2103.08756</a> [<a href="http://arxiv.org/pdf/2103.08756" target="_blank">pdf</a>]

<h2>A LiDAR-Guided Framework for Video Enhancement. (arXiv:2103.08764v1 [cs.CV])</h2>
<h3>Yu Feng, Patrick Hansen, Paul N. Whatmough, Guoyu Lu, Yuhao Zhu</h3>
<p>This paper presents a general framework that simultaneously improves the
quality and the execution speed of a range of video enhancement tasks, such as
super-sampling, deblurring, and denoising. The key to our framework is a pixel
motion estimation algorithm that generates accurate motion from low-quality
videos while being computationally very lightweight. Our motion estimation
algorithm leverages point cloud information, which is readily available in
today's autonomous devices and will only become more common in the future. We
demonstrate a generic framework that leverages the motion information to guide
high-quality image reconstruction. Experiments show that our framework
consistently outperforms the state-of-the-art video enhancement algorithms
while improving the execution speed by an order of magnitude.
</p>
<a href="http://arxiv.org/abs/2103.08764" target="_blank">arXiv:2103.08764</a> [<a href="http://arxiv.org/pdf/2103.08764" target="_blank">pdf</a>]

<h2>A Computer Vision System to Help Prevent the Transmission of COVID-19. (arXiv:2103.08773v1 [cs.CV])</h2>
<h3>Fevziye Irem Eyiokur, Haz&#x131;m Kemal Ekenel, Alexander Waibel</h3>
<p>The COVID-19 pandemic affects every area of daily life globally. To avoid the
spread of coronavirus and retrieve the daily normal worldwide, health
organizations advise social distancing, wearing face mask, and avoiding
touching face. Based on these recommended protective measures, we developed a
deep learning-based computer vision system to help prevent the transmission of
COVID-19. Specifically, the developed system performs face mask detection,
face-hand interaction detection, and measures social distance. For these
purposes, we collected and annotated images that represent face mask usage and
face-hand interaction in the real world. We presented two different face
datasets, namely Unconstrained Face Mask Dataset (UFMD) and Unconstrained Face
Hand Dataset (UFHD). We trained the proposed models on our own datasets and
evaluated them on both our datasets and already existing datasets in the
literature without performing any adaptation on these target datasets. Besides,
we proposed a distance measurement module to track social distance between
people. Experimental results indicate that UFMD and UFHD represent the
real-world's diversity well. The proposed system achieved very high performance
and generalization capacity in a real-world scenario for unseen data from
outside the training data to detect face mask usage and face-hand interaction,
and satisfactory performance in the case of tracking social distance. Presented
UFMD and UFHD datasets will be available at
https://github.com/iremeyiokur/COVID-19-Preventions-Control-System.
</p>
<a href="http://arxiv.org/abs/2103.08773" target="_blank">arXiv:2103.08773</a> [<a href="http://arxiv.org/pdf/2103.08773" target="_blank">pdf</a>]

<h2>Track to Detect and Segment: An Online Multi-Object Tracker. (arXiv:2103.08808v1 [cs.CV])</h2>
<h3>Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, Junsong Yuan</h3>
<p>Most online multi-object trackers perform object detection stand-alone in a
neural net without any input from tracking. In this paper, we present a new
online joint detection and tracking model, TraDeS (TRAck to DEtect and
Segment), exploiting tracking clues to assist detection end-to-end. TraDeS
infers object tracking offset by a cost volume, which is used to propagate
previous object features for improving current object detection and
segmentation. Effectiveness and superiority of TraDeS are shown on 4 datasets,
including MOT (2D tracking), nuScenes (3D tracking), MOTS and Youtube-VIS
(instance segmentation tracking). Project page:
https://jialianwu.com/projects/TraDeS.html.
</p>
<a href="http://arxiv.org/abs/2103.08808" target="_blank">arXiv:2103.08808</a> [<a href="http://arxiv.org/pdf/2103.08808" target="_blank">pdf</a>]

<h2>Towards Indirect Top-Down Road Transport Emissions Estimation. (arXiv:2103.08829v1 [cs.CV])</h2>
<h3>Ryan Mukherjee, Derek Rollend, Gordon Christie, Armin Hadzic, Sally Matson, Anshu Saksena, Marisa Hughes</h3>
<p>Road transportation is one of the largest sectors of greenhouse gas (GHG)
emissions affecting climate change. Tackling climate change as a global
community will require new capabilities to measure and inventory road transport
emissions. However, the large scale and distributed nature of vehicle emissions
make this sector especially challenging for existing inventory methods. In this
work, we develop machine learning models that use satellite imagery to perform
indirect top-down estimation of road transport emissions. Our initial
experiments focus on the United States, where a bottom-up inventory was
available for training our models. We achieved a mean absolute error (MAE) of
39.5 kg CO$_{2}$ of annual road transport emissions, calculated on a
pixel-by-pixel (100 m$^{2}$) basis in Sentinel-2 imagery. We also discuss key
model assumptions and challenges that need to be addressed to develop models
capable of generalizing to global geography. We believe this work is the first
published approach for automated indirect top-down estimation of road transport
sector emissions using visual imagery and represents a critical step towards
scalable, global, near-real-time road transportation emissions inventories that
are measured both independently and objectively.
</p>
<a href="http://arxiv.org/abs/2103.08829" target="_blank">arXiv:2103.08829</a> [<a href="http://arxiv.org/pdf/2103.08829" target="_blank">pdf</a>]

<h2>Skeleton Based Sign Language Recognition Using Whole-body Keypoints. (arXiv:2103.08833v1 [cs.CV])</h2>
<h3>Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, Yun Fu</h3>
<p>Sign language is a visual language that is used by deaf or speech impaired
people to communicate with each other. Sign language is always performed by
fast transitions of hand gestures and body postures, requiring a great amount
of knowledge and training to understand it. Sign language recognition becomes a
useful yet challenging task in computer vision. Skeleton-based action
recognition is becoming popular that it can be further ensembled with RGB-D
based method to achieve state-of-the-art performance. However, skeleton-based
recognition can hardly be applied to sign language recognition tasks, majorly
because skeleton data contains no indication of hand gestures or facial
expressions. Inspired by the recent development of whole-body pose estimation
\cite{jin2020whole}, we propose recognizing sign language based on the
whole-body key points and features. The recognition results are further
ensembled with other modalities of RGB and optical flows to improve the
accuracy further. In the challenge about isolated sign language recognition
hosted by ChaLearn using a new large-scale multi-modal Turkish Sign Language
dataset (AUTSL). Our method achieved leading accuracy in both the development
phase and test phase. This manuscript is a fact sheet version. Our workshop
paper version will be released soon. Our code has been made available at
https://github.com/jackyjsy/CVPR21Chal-SLR
</p>
<a href="http://arxiv.org/abs/2103.08833" target="_blank">arXiv:2103.08833</a> [<a href="http://arxiv.org/pdf/2103.08833" target="_blank">pdf</a>]

<h2>GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video. (arXiv:2103.08834v1 [cs.CV])</h2>
<h3>Shih-Po Lee, Si-Cun Chen, Wen-Hsiao Peng</h3>
<p>This paper addresses fast semantic segmentation on video.Video segmentation
often calls for real-time, or even fasterthan real-time, processing. One common
recipe for conserving computation arising from feature extraction is to
propagate features of few selected keyframes. However, recent advances in fast
image segmentation make these solutions less attractive. To leverage fast image
segmentation for furthering video segmentation, we propose a simple yet
efficient propagation framework. Specifically, we perform lightweight flow
estimation in 1/8-downscaled image space for temporal warping in segmentation
outpace space. Moreover, we introduce a guided spatially-varying convolution
for fusing segmentations derived from the previous and current frames, to
mitigate propagation error and enable lightweight feature extraction on
non-keyframes. Experimental results on Cityscapes and CamVid show that our
scheme achieves the state-of-the-art accuracy-throughput trade-off on video
segmentation.
</p>
<a href="http://arxiv.org/abs/2103.08834" target="_blank">arXiv:2103.08834</a> [<a href="http://arxiv.org/pdf/2103.08834" target="_blank">pdf</a>]

<h2>Multi-Robot Routing with Time Windows: A Column Generation Approach. (arXiv:2103.08835v1 [cs.RO])</h2>
<h3>Naveed Haghani, Jiaoyang Li, Sven Koenig, Gautam Kunapuli, Claudio Contardo, Amelia Regan, Julian Yarkony</h3>
<p>Robots performing tasks in warehouses provide the first example of
wide-spread adoption of autonomous vehicles in transportation and logistics.
The efficiency of these operations, which can vary widely in practice, are a
key factor in the success of supply chains. In this work we consider the
problem of coordinating a fleet of robots performing picking operations in a
warehouse so as to maximize the net profit achieved within a time period while
respecting problem- and robot-specific constraints. We formulate the problem as
a weighted set packing problem where the elements in consideration are items on
the warehouse floor that can be picked up and delivered within specified time
windows. We enforce the constraint that robots must not collide, that each item
is picked up and delivered by at most one robot, and that the number of robots
active at any time does not exceed the total number available. Since the set of
routes is exponential in the size of the input, we attack optimization of the
resulting integer linear program using column generation, where pricing amounts
to solving an elementary resource-constrained shortest-path problem. We propose
an efficient optimization scheme that avoids consideration of every increment
within the time windows. We also propose a heuristic pricing algorithm that can
efficiently solve the pricing subproblem. While this itself is an important
problem, the insights gained from solving these problems effectively can lead
to new advances in other time-widow constrained vehicle routing problems.
</p>
<a href="http://arxiv.org/abs/2103.08835" target="_blank">arXiv:2103.08835</a> [<a href="http://arxiv.org/pdf/2103.08835" target="_blank">pdf</a>]

<h2>Closed-Loop Error Learning Control for Uncertain Nonlinear Systems With Experimental Validation on a Mobile Robot. (arXiv:2103.08845v1 [cs.RO])</h2>
<h3>Erkan Kayacan</h3>
<p>This paper develops a Closed-Loop Error Learning Control (CLELC) algorithm
for feedback linearizable systems with experimental validation on a mobile
robot. Traditional feedback and feedforward controllers are designed based on
the nominal model by using Feedback Linearization Control (FLC) method. Then,
an intelligent controller is designed based on sliding mode learning algorithm
that utilizes closed-loop error dynamics to learn the system behavior. The
controllers are working in parallel, and the intelligent controller can
gradually replace the feedback controller from the control of the system. In
addition to the stability of the sliding mode learning algorithm, the
closed-loop stability of an $n$th order feedback linearizable system is proven.
The simulation results demonstrate that CLELC algorithm can improve control
performance (e.g., smaller rise time, settling time and overshoot) in the
absence of uncertainties, and also provides robust control performance in the
presence of uncertainties as compared to traditional FLC method. To test the
efficiency and efficacy of CLELC algorithm, the trajectory tracking problem of
a tracked mobile robot is studied in real-time. The experimental results
demonstrate that CLELC algorithm ensures high-accurate trajectory tracking
performance than traditional FLC method.
</p>
<a href="http://arxiv.org/abs/2103.08845" target="_blank">arXiv:2103.08845</a> [<a href="http://arxiv.org/pdf/2103.08845" target="_blank">pdf</a>]

<h2>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models. (arXiv:2103.08849v1 [cs.CV])</h2>
<h3>Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, Alexander Hauptmann</h3>
<p>This paper studies zero-shot cross-lingual transfer of vision-language
models. Specifically, we focus on multilingual text-to-video search and propose
a Transformer-based model that learns contextualized multilingual multimodal
embeddings. Under a zero-shot setting, we empirically demonstrate that
performance degrades significantly when we query the multilingual text-video
model with non-English sentences. To address this problem, we introduce a
multilingual multimodal pre-training strategy, and collect a new multilingual
instructional video dataset (MultiHowTo100M) for pre-training. Experiments on
VTT show that our method significantly improves video search in non-English
languages without additional annotations. Furthermore, when multilingual
annotations are available, our method outperforms recent baselines by a large
margin in multilingual text-to-video search on VTT and VATEX; as well as in
multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is
available at this http URL
</p>
<a href="http://arxiv.org/abs/2103.08849" target="_blank">arXiv:2103.08849</a> [<a href="http://arxiv.org/pdf/2103.08849" target="_blank">pdf</a>]

<h2>Few Shot System Identification for Reinforcement Learning. (arXiv:2103.08850v1 [cs.RO])</h2>
<h3>Karim Farid, Nourhan Sakr</h3>
<p>Learning by interaction is the key to skill acquisition for most living
organisms, which is formally called Reinforcement Learning (RL). RL is
efficient in finding optimal policies for endowing complex systems with
sophisticated behavior. All paradigms of RL utilize a system model for finding
the optimal policy. Modeling dynamics can be done by formulating a mathematical
model or system identification. Dynamic models are usually exposed to aleatoric
and epistemic uncertainties that can divert the model from the one acquired and
cause the RL algorithm to exhibit erroneous behavior. Accordingly, the RL
process sensitive to operating conditions and changes in model parameters and
lose its generality. To address these problems, Intensive system identification
for modeling purposes is needed for each system even if the model dynamics
structure is the same, as the slight deviation in the model parameters can
render the model useless in RL. The existence of an oracle that can adaptively
predict the rest of the trajectory regardless of the uncertainties can help
resolve the issue. The target of this work is to present a framework for
facilitating the system identification of different instances of the same
dynamics class by learning a probability distribution of the dynamics
conditioned on observed data with variational inference and show its
reliability in robustly solving different instances of control problems with
the same model in model-based RL with maximum sample efficiency.
</p>
<a href="http://arxiv.org/abs/2103.08850" target="_blank">arXiv:2103.08850</a> [<a href="http://arxiv.org/pdf/2103.08850" target="_blank">pdf</a>]

<h2>Lite-HDSeg: LiDAR Semantic Segmentation Using Lite Harmonic Dense Convolutions. (arXiv:2103.08852v1 [cs.CV])</h2>
<h3>Ryan Razani, Ran Cheng, Ehsan Taghavi, Liu Bingbing</h3>
<p>Autonomous driving vehicles and robotic systems rely on accurate perception
of their surroundings. Scene understanding is one of the crucial components of
perception modules. Among all available sensors, LiDARs are one of the
essential sensing modalities of autonomous driving systems due to their active
sensing nature with high resolution of sensor readings. Accurate and fast
semantic segmentation methods are needed to fully utilize LiDAR sensors for
scene understanding. In this paper, we present Lite-HDSeg, a novel real-time
convolutional neural network for semantic segmentation of full $3$D LiDAR point
clouds. Lite-HDSeg can achieve the best accuracy vs. computational complexity
trade-off in SemanticKitti benchmark and is designed on the basis of a new
encoder-decoder architecture with light-weight harmonic dense convolutions as
its core. Moreover, we introduce ICM, an improved global contextual module to
capture multi-scale contextual features, and MCSPN, a multi-class Spatial
Propagation Network to further refine the semantic boundaries. Our experimental
results show that the proposed method outperforms state-of-the-art semantic
segmentation approaches which can run real-time, thus is suitable for robotic
and autonomous driving applications.
</p>
<a href="http://arxiv.org/abs/2103.08852" target="_blank">arXiv:2103.08852</a> [<a href="http://arxiv.org/pdf/2103.08852" target="_blank">pdf</a>]

<h2>Adversarial YOLO: Defense Human Detection Patch Attacks via Detecting Adversarial Patches. (arXiv:2103.08860v1 [cs.CV])</h2>
<h3>Nan Ji, YanFei Feng, Haidong Xie, Xueshuang Xiang, Naijin Liu</h3>
<p>The security of object detection systems has attracted increasing attention,
especially when facing adversarial patch attacks. Since patch attacks change
the pixels in a restricted area on objects, they are easy to implement in the
physical world, especially for attacking human detection systems. The existing
defenses against patch attacks are mostly applied for image classification
problems and have difficulty resisting human detection attacks. Towards this
critical issue, we propose an efficient and effective plug-in defense component
on the YOLO detection system, which we name Ad-YOLO. The main idea is to add a
patch class on the YOLO architecture, which has a negligible inference
increment. Thus, Ad-YOLO is expected to directly detect both the objects of
interest and adversarial patches. To the best of our knowledge, our approach is
the first defense strategy against human detection attacks.

We investigate Ad-YOLO's performance on the YOLOv2 baseline. To improve the
ability of Ad-YOLO to detect variety patches, we first use an adversarial
training process to develop a patch dataset based on the Inria dataset, which
we name Inria-Patch. Then, we train Ad-YOLO by a combination of Pascal VOC,
Inria, and Inria-Patch datasets. With a slight drop of $0.70\%$ mAP on VOC 2007
test set, Ad-YOLO achieves $80.31\%$ AP of persons, which highly outperforms
$33.93\%$ AP for YOLOv2 when facing white-box patch attacks. Furthermore,
compared with YOLOv2, the results facing a physical-world attack are also
included to demonstrate Ad-YOLO's excellent generalization ability.
</p>
<a href="http://arxiv.org/abs/2103.08860" target="_blank">arXiv:2103.08860</a> [<a href="http://arxiv.org/pdf/2103.08860" target="_blank">pdf</a>]

<h2>Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar. (arXiv:2103.08863v1 [cs.CV])</h2>
<h3>Peike Li, Xin Yu, Yi Yang</h3>
<p>Conventional face super-resolution methods usually assume testing
low-resolution (LR) images lie in the same domain as the training ones. Due to
different lighting conditions and imaging hardware, domain gaps between
training and testing images inevitably occur in many real-world scenarios.
Neglecting those domain gaps would lead to inferior face super-resolution (FSR)
performance. However, how to transfer a trained FSR model to a target domain
efficiently and effectively has not been investigated. To tackle this problem,
we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named
DAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces
from a target domain by exploiting only a pair of high-resolution (HR) and LR
exemplar in the target domain. To be specific, our DAP-FSR firstly employs its
encoder to extract the multi-scale latent representations of the input LR face.
Considering only one target domain example is available, we propose to augment
the target domain data by mixing the latent representations of the target
domain face and source domain ones, and then feed the mixed representations to
the decoder of our DAP-FSR. The decoder will generate new face images
resembling the target domain image style. The generated HR faces in turn are
used to optimize our decoder to reduce the domain gap. By iteratively updating
the latent representations and our decoder, our DAP-FSR will be adapted to the
target domain, thus achieving authentic and high-quality upsampled HR faces.
Extensive experiments on three newly constructed benchmarks validate the
effectiveness and superior performance of our DAP-FSR compared to the
state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.08863" target="_blank">arXiv:2103.08863</a> [<a href="http://arxiv.org/pdf/2103.08863" target="_blank">pdf</a>]

<h2>Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling. (arXiv:2103.08877v1 [cs.CV])</h2>
<h3>&#x110;or&#x111;e Miladinovi&#x107;, Aleksandar Stani&#x107;, Stefan Bauer, J&#xfc;rgen Schmidhuber, Joachim M. Buhmann</h3>
<p>How to improve generative modeling by better exploiting spatial regularities
and coherence in images? We introduce a novel neural network for building image
generators (decoders) and apply it to variational autoencoders (VAEs). In our
spatial dependency networks (SDNs), feature maps at each level of a deep neural
net are computed in a spatially coherent way, using a sequential gating-based
mechanism that distributes contextual information across 2-D space. We show
that augmenting the decoder of a hierarchical VAE by spatial dependency layers
considerably improves density estimation over baseline convolutional
architectures and the state-of-the-art among the models within the same class.
Furthermore, we demonstrate that SDN can be applied to large images by
synthesizing samples of high quality and coherence. In a vanilla VAE setting,
we find that a powerful SDN decoder also improves learning disentangled
representations, indicating that neural architectures play an important role in
this task. Our results suggest favoring spatial dependency over convolutional
layers in various VAE settings. The accompanying source code is given at
https://github.com/djordjemila/sdn.
</p>
<a href="http://arxiv.org/abs/2103.08877" target="_blank">arXiv:2103.08877</a> [<a href="http://arxiv.org/pdf/2103.08877" target="_blank">pdf</a>]

<h2>A New Autoregressive Neural Network Model with Command Compensation for Imitation Learning Based on Bilateral Control. (arXiv:2103.08879v1 [cs.RO])</h2>
<h3>Kazuki Hayashi, Ayumu Sasagawa, Sho Sakaino, Toshiaki Tsuji</h3>
<p>In the near future, robots are expected to work with humans or operate alone
and may replace human workers in various fields such as homes and factories. In
a previous study, we proposed bilateral control-based imitation learning that
enables robots to utilize force information and operate almost simultaneously
with an expert's demonstration. In addition, we recently proposed an
autoregressive neural network model (SM2SM) for bilateral control-based
imitation learning to obtain long-term inferences. In the SM2SM model, both
master and slave states must be input, but the master states are obtained from
the previous outputs of the SM2SM model, resulting in destabilized estimation
under large environmental variations. Hence, a new autoregressive neural
network model (S2SM) is proposed in this study. This model requires only the
slave state as input and its outputs are the next slave and master states,
thereby improving the task success rates. In addition, a new feedback
controller that utilizes the error between the responses and estimates of the
slave is proposed, which shows better reproducibility.
</p>
<a href="http://arxiv.org/abs/2103.08879" target="_blank">arXiv:2103.08879</a> [<a href="http://arxiv.org/pdf/2103.08879" target="_blank">pdf</a>]

<h2>Human-Robot Motion Retargeting via Neural Latent Optimization. (arXiv:2103.08882v1 [cs.RO])</h2>
<h3>Haodong Zhang, Weijie Li, Yuwei Liang, Zexi Chen, Yuxiang Cui, Yue Wang, Rong Xiong</h3>
<p>Motion retargeting from human to robot remains a very challenging task due to
variations in the structure of humans and robots. Most traditional
optimization-based algorithms solve this problem by minimizing an objective
function, which is usually time-consuming and heavily dependent on good
initialization. In contrast, methods with feedforward neural networks can learn
prior knowledge from training data and quickly infer the results, but these
methods also suffer from the generalization problem on unseen actions, leading
to some infeasible results. In this paper, we propose a novel neural
optimization approach taking advantages of both kinds of methods. A graph-based
neural network is utilized to establish a mapping between the latent space and
the robot motion space. Afterward, the retargeting results can be obtained by
searching for the optimal vector in this latent space. In addition, a deep
encoder also provides a promising initialization for better and faster
convergence. We perform experiments on retargeting Chinese sign language to
three different kinds of robots in the simulation environment, including ABB's
YuMi dual-arm collaborative robot, NAO and Pepper. A real-world experiment is
also conducted on the Yumi robot. Experimental results show that our method can
retarget motion from human to robot with both efficiency and accuracy.
</p>
<a href="http://arxiv.org/abs/2103.08882" target="_blank">arXiv:2103.08882</a> [<a href="http://arxiv.org/pdf/2103.08882" target="_blank">pdf</a>]

<h2>Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation. (arXiv:2103.08896v1 [cs.CV])</h2>
<h3>Jungbeom Lee, Eunji Kim, Sungroh Yoon</h3>
<p>Weakly supervised semantic segmentation produces a pixel-level localization
from a classifier, but it is likely to restrict its focus to a small
discriminative region of the target object. AdvCAM is an attribution map of an
image that is manipulated to increase the classification score. This
manipulation is realized in an anti-adversarial manner, which perturbs the
images along pixel gradients in the opposite direction from those used in an
adversarial attack. It forces regions initially considered not to be
discriminative to become involved in subsequent classifications, and produces
attribution maps that successively identify more regions of the target object.
In addition, we introduce a new regularization procedure that inhibits the
incorrect attribution of regions unrelated to the target object and limits the
attributions of the regions that already have high scores. On PASCAL VOC 2012
test images, we achieve mIoUs of 68.0 and 76.9 for weakly and semi-supervised
semantic segmentation respectively, which represent a new state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2103.08896" target="_blank">arXiv:2103.08896</a> [<a href="http://arxiv.org/pdf/2103.08896" target="_blank">pdf</a>]

<h2>BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation. (arXiv:2103.08907v1 [cs.CV])</h2>
<h3>Jungbeom Lee, Jihun Yi, Chaehun Shin, Sungroh Yoon</h3>
<p>Weakly supervised segmentation methods using bounding box annotations focus
on obtaining a pixel-level mask from each box containing an object. Existing
methods typically depend on a class-agnostic mask generator, which operates on
the low-level information intrinsic to an image. In this work, we utilize
higher-level information from the behavior of a trained object detector, by
seeking the smallest areas of the image from which the object detector produces
almost the same result as it does from the whole image. These areas constitute
a bounding-box attribution map (BBAM), which identifies the target object in
its bounding box and thus serves as pseudo ground-truth for weakly supervised
semantic and instance segmentation. This approach significantly outperforms
recent comparable techniques on both the PASCAL VOC and MS COCO benchmarks in
weakly supervised semantic and instance segmentation. In addition, we provide a
detailed analysis of our method, offering deeper insight into the behavior of
the BBAM.
</p>
<a href="http://arxiv.org/abs/2103.08907" target="_blank">arXiv:2103.08907</a> [<a href="http://arxiv.org/pdf/2103.08907" target="_blank">pdf</a>]

<h2>EADNet: Efficient Asymmetric Dilated Network for Semantic Segmentation. (arXiv:2103.08914v1 [cs.CV])</h2>
<h3>Qihang Yang, Tao Chen, Jiayuan Fan, Ye Lu, Chongyan Zuo, Qinghua Chi</h3>
<p>Due to real-time image semantic segmentation needs on power constrained edge
devices, there has been an increasing desire to design lightweight semantic
segmentation neural network, to simultaneously reduce computational cost and
increase inference speed. In this paper, we propose an efficient asymmetric
dilated semantic segmentation network, named EADNet, which consists of multiple
developed asymmetric convolution branches with different dilation rates to
capture the variable shapes and scales information of an image. Specially, a
multi-scale multi-shape receptive field convolution (MMRFC) block with only a
few parameters is designed to capture such information. Experimental results on
the Cityscapes dataset demonstrate that our proposed EADNet achieves
segmentation mIoU of 67.1 with smallest number of parameters (only 0.35M) among
mainstream lightweight semantic segmentation networks.
</p>
<a href="http://arxiv.org/abs/2103.08914" target="_blank">arXiv:2103.08914</a> [<a href="http://arxiv.org/pdf/2103.08914" target="_blank">pdf</a>]

<h2>Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v1 [cs.CV])</h2>
<h3>Pit Schneider</h3>
<p>Text line segmentation is one of the pre-stages of modern optical character
recognition systems. The algorithmic approach proposed by this paper has been
designed for this exact purpose. Its main characteristic is the combination of
two different techniques, morphological image operations and horizontal
histogram projections. The method was developed to be applied on a historic
data collection that commonly features quality issues, such as degraded paper,
blurred text, or curved text lines. For that reason, the segmenter in question
could be of particular interest for cultural institutions, such as libraries,
archives, museums, ..., that want access to robust line bounding boxes for a
given historic document. Because of the promising segmentation results that are
joined by low computational cost, the algorithm was incorporated into the OCR
pipeline of the National Library of Luxembourg, in the context of the
initiative of reprocessing their historic newspaper collection. The general
contribution of this paper is to outline the approach and to evaluate the gains
in terms of accuracy and speed, comparing it to the segmentation algorithm
bundled with the used open source OCR software.
</p>
<a href="http://arxiv.org/abs/2103.08922" target="_blank">arXiv:2103.08922</a> [<a href="http://arxiv.org/pdf/2103.08922" target="_blank">pdf</a>]

<h2>Modulating Localization and Classification for Harmonized Object Detection. (arXiv:2103.08958v1 [cs.CV])</h2>
<h3>Taiheng Zhang, Qiaoyong Zhong, Shiliang Pu, Di Xie</h3>
<p>Object detection involves two sub-tasks, i.e. localizing objects in an image
and classifying them into various categories. For existing CNN-based detectors,
we notice the widespread divergence between localization and classification,
which leads to degradation in performance. In this work, we propose a mutual
learning framework to modulate the two tasks. In particular, the two tasks are
forced to learn from each other with a novel mutual labeling strategy. Besides,
we introduce a simple yet effective IoU rescoring scheme, which further reduces
the divergence. Moreover, we define a Spearman rank correlation-based metric to
quantify the divergence, which correlates well with the detection performance.
The proposed approach is general-purpose and can be easily injected into
existing detectors such as FCOS and RetinaNet. We achieve a significant
performance gain over the baseline detectors on the COCO dataset.
</p>
<a href="http://arxiv.org/abs/2103.08958" target="_blank">arXiv:2103.08958</a> [<a href="http://arxiv.org/pdf/2103.08958" target="_blank">pdf</a>]

<h2>Hebbian Semi-Supervised Learning in a Sample Efficiency Setting. (arXiv:2103.09002v1 [cs.CV])</h2>
<h3>Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</h3>
<p>We propose to address the issue of sample efficiency, in Deep Convolutional
Neural Networks (DCNN), with a semisupervised training strategy that combines
Hebbian learning with gradient descent: all internal layers (both convolutional
and fully connected) are pre-trained using an unsupervised approach based on
Hebbian learning, and the last fully connected layer (the classification layer)
is using Stochastic Gradient Descent (SGD). In fact, as Hebbian learning is an
unsupervised learning method, its potential lies in the possibility of training
the internal layers of a DCNN without labeled examples. Only the final fully
connected layer has to be trained with labeled examples. We performed
experiments on various object recognition datasets, in different regimes of
sample efficiency, comparing our semi-supervised (Hebbian for internal layers +
SGD for the final fully layer) approach with end-to-end supervised
backpropagation training. The results show that, in regimes where the number of
available labeled samples is low, our semi-supervised approach outperforms full
backpropagation in almost all the cases.
</p>
<a href="http://arxiv.org/abs/2103.09002" target="_blank">arXiv:2103.09002</a> [<a href="http://arxiv.org/pdf/2103.09002" target="_blank">pdf</a>]

<h2>PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos. (arXiv:2103.09009v1 [cs.CV])</h2>
<h3>Tianyu Luan, Yali Wang, Junhao Zhang, Zhe Wang, Zhipeng Zhou, Yu Qiao</h3>
<p>The end-to-end Human Mesh Recovery (HMR) approach has been successfully used
for 3D body reconstruction. However, most HMR-based frameworks reconstruct
human body by directly learning mesh parameters from images or videos, while
lacking explicit guidance of 3D human pose in visual data. As a result, the
generated mesh often exhibits incorrect pose for complex activities. To tackle
this problem, we propose to exploit 3D pose to calibrate human mesh.
Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial
PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in
a serial or parallel manner, these two frameworks can effectively correct human
mesh with guidance of a concise pose calibration module. Furthermore, since the
calibration module is designed via non-rigid pose transformation, our PC-HMR
frameworks can flexibly tackle bone length variations to alleviate misplacement
in the calibrated mesh. Finally, our frameworks are based on generic and
complementary integration of data-driven learning and geometrical modeling. Via
plug-and-play modules, they can be efficiently adapted for both
image/video-based human mesh recovery. Additionally, they have no requirement
of extra 3D pose annotations in the testing phase, which releases inference
difficulties in practice. We perform extensive experiments on the popular
bench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks
achieve the SOTA results.
</p>
<a href="http://arxiv.org/abs/2103.09009" target="_blank">arXiv:2103.09009</a> [<a href="http://arxiv.org/pdf/2103.09009" target="_blank">pdf</a>]

<h2>Dense Interaction Learning for Video-based Person Re-identification. (arXiv:2103.09013v1 [cs.CV])</h2>
<h3>Tianyu He, Xin Jin, Xu Shen, Jianqiang Huang, Zhibo Chen, Xian-Sheng Hua</h3>
<p>Video-based person re-identification (re-ID) aims at matching the same person
across video clips. Efficiently exploiting multi-scale fine-grained features
while building the structural interaction among them is pivotal for its
success. In this paper, we propose a hybrid framework, Dense Interaction
Learning (DenseIL), that takes the principal advantages of both CNN-based and
Attention-based architectures to tackle video-based person re-ID difficulties.
DenseIL contains a CNN Encoder and a Transformer Decoder. The CNN Encoder is
responsible for efficiently extracting discriminative spatial features while
the Transformer Decoder is designed to deliberately model spatial-temporal
inherent interaction across frames. Different from the vanilla Transformer, we
additionally let the Transformer Decoder densely attends to intermediate
fine-grained CNN features and that naturally yields multi-scale
spatial-temporal feature representation for each video clip. Moreover, we
introduce Spatio-TEmporal Positional Embedding (STEP-Emb) into the Transformer
Decoder to investigate the positional relation among the spatial-temporal
inputs. Our experiments consistently and significantly outperform all the
state-of-the-art methods on multiple standard video-based re-ID datasets.
</p>
<a href="http://arxiv.org/abs/2103.09013" target="_blank">arXiv:2103.09013</a> [<a href="http://arxiv.org/pdf/2103.09013" target="_blank">pdf</a>]

<h2>Manipulator-Independent Representations for Visual Imitation. (arXiv:2103.09016v1 [cs.RO])</h2>
<h3>Yuxiang Zhou, Yusuf Aytar, Konstantinos Bousmalis</h3>
<p>Imitation learning is an effective tool for robotic learning tasks where
specifying a reinforcement learning (RL) reward is not feasible or where the
exploration problem is particularly difficult. Imitation, typically behavior
cloning or inverse RL, derive a policy from a collection of first-person
action-state trajectories. This is contrary to how humans and other animals
imitate: we observe a behavior, even from other species, understand its
perceived effect on the state of the environment, and figure out what actions
our body can perform to reach a similar outcome. In this work, we explore the
possibility of third-person visual imitation of manipulation trajectories, only
from vision and without access to actions, demonstrated by embodiments
different to the ones of our imitating agent. Specifically, we investigate what
would be an appropriate representation method with which an RL agent can
visually track trajectories of complex manipulation behavior -- non-planar with
multiple-object interactions -- demonstrated by experts with different
embodiments. We present a way to train manipulator-independent representations
(MIR) that primarily focus on the change in the environment and have all the
characteristics that make them suitable for cross-embodiment visual imitation
with RL: cross-domain alignment, temporal smoothness, and being actionable. We
show that with our proposed method our agents are able to imitate, with complex
robot control, trajectories from a variety of embodiments and with significant
visual and dynamics differences, e.g. simulation-to-reality gap.
</p>
<a href="http://arxiv.org/abs/2103.09016" target="_blank">arXiv:2103.09016</a> [<a href="http://arxiv.org/pdf/2103.09016" target="_blank">pdf</a>]

<h2>A Large-Scale Dataset for Benchmarking Elevator Button Segmentation and Character Recognition. (arXiv:2103.09030v1 [cs.CV])</h2>
<h3>Jianbang Liu, Yuqi Fang, Delong Zhu, Nachuan Ma, Jin Pan, Max Q.-H. Meng</h3>
<p>Human activities are hugely restricted by COVID-19, recently. Robots that can
conduct inter-floor navigation attract much public attention, since they can
substitute human workers to conduct the service work. However, current robots
either depend on human assistance or elevator retrofitting, and fully
autonomous inter-floor navigation is still not available. As the very first
step of inter-floor navigation, elevator button segmentation and recognition
hold an important position. Therefore, we release the first large-scale
publicly available elevator panel dataset in this work, containing 3,718 panel
images with 35,100 button labels, to facilitate more powerful algorithms on
autonomous elevator operation. Together with the dataset, a number of deep
learning based implementations for button segmentation and recognition are also
released to benchmark future methods in the community. The dataset will be
available at \url{https://github.com/zhudelong/elevator_button_recognition
</p>
<a href="http://arxiv.org/abs/2103.09030" target="_blank">arXiv:2103.09030</a> [<a href="http://arxiv.org/pdf/2103.09030" target="_blank">pdf</a>]

<h2>Combining Planning and Learning of Behavior Trees for Robotic Assembly. (arXiv:2103.09036v1 [cs.RO])</h2>
<h3>Jonathan Styrud, Matteo Iovino, Mikael Norrl&#xf6;f, M&#xe5;rten Bj&#xf6;rkman, Christian Smith</h3>
<p>Industrial robots can solve very complex tasks in controlled environments,
but modern applications require robots able to operate in unpredictable
surroundings as well. An increasingly popular reactive policy architecture in
robotics is Behavior Trees but as with other architectures, programming time
still drives cost and limits flexibility. There are two main branches of
algorithms to generate policies automatically, automated planning and machine
learning, both with their own drawbacks. We propose a method for generating
Behavior Trees using a Genetic Programming algorithm and combining the two
branches by taking the result of an automated planner and inserting it into the
population. Experimental results confirm that the proposed method of combining
planning and learning performs well on a variety of robotic assembly problems
and outperforms both of the base methods used separately. We also show that
this type of high level learning of Behavior Trees can be transferred to a real
system without further training.
</p>
<a href="http://arxiv.org/abs/2103.09036" target="_blank">arXiv:2103.09036</a> [<a href="http://arxiv.org/pdf/2103.09036" target="_blank">pdf</a>]

<h2>Analysis of a 3-RUU Parallel Manipulator. (arXiv:2103.09037v1 [cs.RO])</h2>
<h3>Thomas Stigger, Johannes Siegele, Daniel F. Scharler, Martin Pfurner, Manfred L. Husty</h3>
<p>The aim of this paper is to give a detailed examination of the input and
output singularities of a 3-RUU parallel manipulator in the translational
operation mode. This task is achieved by using algebraic constraint equations.
For this type of manipulator a complete workspace representation in Study
coordinates is presented after elimination of the input parameters. Both, input
and output singularities are mapped into a Study subspace as well as into the
joint space. Therewith a detailed singularity investigation of the
translational operation mode of a 3-RUU parallel manipulator is provided. This
paper is an extended version of a previous publication. The addendum comprises
the discovery of a possible transition between two operation modes as well as a
self motion and an examination of another component of the output singularity
surface, most of them for arbitrary design parameters.
</p>
<a href="http://arxiv.org/abs/2103.09037" target="_blank">arXiv:2103.09037</a> [<a href="http://arxiv.org/pdf/2103.09037" target="_blank">pdf</a>]

<h2>Inclined Quadrotor Landing using Deep Reinforcement Learning. (arXiv:2103.09043v1 [cs.RO])</h2>
<h3>Jacob E. Kooi, Robert Babu&#x161;ka</h3>
<p>Landing a quadrotor on an inclined surface is a challenging manoeuvre. The
final state of any inclined landing trajectory is not an equilibrium, which
precludes the use of most conventional control methods. We propose a deep
reinforcement learning approach to design an autonomous landing controller for
inclined surfaces. Using the proximal policy optimization (PPO) algorithm with
sparse rewards and a tailored curriculum learning approach, a robust policy can
be trained in simulation in less than 90 minutes on a standard laptop. The
policy then directly runs on a real Crazyflie 2.1 quadrotor and successfully
performs real inclined landings in a flying arena. A single policy evaluation
takes approximately 2.5 ms, which makes it suitable for a future embedded
implementation on the quadrotor.
</p>
<a href="http://arxiv.org/abs/2103.09043" target="_blank">arXiv:2103.09043</a> [<a href="http://arxiv.org/pdf/2103.09043" target="_blank">pdf</a>]

<h2>Map completion from partial observation using the global structure of multiple environmental maps. (arXiv:2103.09071v1 [cs.RO])</h2>
<h3>Yuki Katsumata, Akinori Kanechika, Akira Taniguchi, Lotfi El Hafi, Yoshinobu Hagiwara, Tadahiro Taniguchi</h3>
<p>Using the spatial structure of various indoor environments as prior
knowledge, the robot would construct the map more efficiently. Autonomous
mobile robots generally apply simultaneous localization and mapping (SLAM)
methods to understand the reachable area in newly visited environments.
However, conventional mapping approaches are limited by only considering sensor
observation and control signals to estimate the current environment map. This
paper proposes a novel SLAM method, map completion network-based SLAM
(MCN-SLAM), based on a probabilistic generative model incorporating deep neural
networks for map completion. These map completion networks are primarily
trained in the framework of generative adversarial networks (GANs) to extract
the global structure of large amounts of existing map data. We show in
experiments that the proposed method can estimate the environment map 1.3 times
better than the previous SLAM methods in the situation of partial observation.
</p>
<a href="http://arxiv.org/abs/2103.09071" target="_blank">arXiv:2103.09071</a> [<a href="http://arxiv.org/pdf/2103.09071" target="_blank">pdf</a>]

<h2>Cognitive architecture aided by working-memory for self-supervised multi-modal humans recognition. (arXiv:2103.09072v1 [cs.RO])</h2>
<h3>Jonas Gonzalez-Billandon, Giulia Belgiovine, Alessandra Sciutti, Giulio Sandini, Francesco Rea</h3>
<p>The ability to recognize human partners is an important social skill to build
personalized and long-term human-robot interactions, especially in scenarios
like education, care-giving, and rehabilitation. Faces and voices constitute
two important sources of information to enable artificial systems to reliably
recognize individuals. Deep learning networks have achieved state-of-the-art
results and demonstrated to be suitable tools to address such a task. However,
when those networks are applied to different and unprecedented scenarios not
included in the training set, they can suffer a drop in performance. For
example, with robotic platforms in ever-changing and realistic environments,
where always new sensory evidence is acquired, the performance of those models
degrades. One solution is to make robots learn from their first-hand sensory
data with self-supervision. This allows coping with the inherent variability of
the data gathered in realistic and interactive contexts. To this aim, we
propose a cognitive architecture integrating low-level perceptual processes
with a spatial working memory mechanism. The architecture autonomously
organizes the robot's sensory experience into a structured dataset suitable for
human recognition. Our results demonstrate the effectiveness of our
architecture and show that it is a promising solution in the quest of making
robots more autonomous in their learning process.
</p>
<a href="http://arxiv.org/abs/2103.09072" target="_blank">arXiv:2103.09072</a> [<a href="http://arxiv.org/pdf/2103.09072" target="_blank">pdf</a>]

<h2>Frequency-aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection. (arXiv:2103.09096v1 [cs.CV])</h2>
<h3>Jiaming Li, Hongtao Xie, Jiahong Li, Zhongyuan Wang, Yongdong Zhang</h3>
<p>Face forgery detection is raising ever-increasing interest in computer vision
since facial manipulation technologies cause serious worries. Though recent
works have reached sound achievements, there are still unignorable problems: a)
learned features supervised by softmax loss are separable but not
discriminative enough, since softmax loss does not explicitly encourage
intra-class compactness and interclass separability; and b) fixed filter banks
and hand-crafted features are insufficient to capture forgery patterns of
frequency from diverse inputs. To compensate for such limitations, a novel
frequency-aware discriminative feature learning framework is proposed in this
paper. Specifically, we design a novel single-center loss (SCL) that only
compresses intra-class variations of natural faces while boosting inter-class
differences in the embedding space. In such a case, the network can learn more
discriminative features with less optimization difficulty. Besides, an adaptive
frequency feature generation module is developed to mine frequency clues in a
completely data-driven fashion. With the above two modules, the whole framework
can learn more discriminative features in an end-to-end manner. Extensive
experiments demonstrate the effectiveness and superiority of our framework on
three versions of the FF++ dataset.
</p>
<a href="http://arxiv.org/abs/2103.09096" target="_blank">arXiv:2103.09096</a> [<a href="http://arxiv.org/pdf/2103.09096" target="_blank">pdf</a>]

<h2>Consistent Posterior Distributions under Vessel-Mixing: A Regularization for Cross-Domain Retinal Artery/Vein Classification. (arXiv:2103.09097v1 [cs.CV])</h2>
<h3>Chenxin Li, Yunlong Zhang, Zhehan Liang, Wenao Ma, Yue Huang, Xinghao Ding</h3>
<p>Retinal artery/vein (A/V) classification is a critical technique for
diagnosing diabetes and cardiovascular diseases. Although deep learning based
methods achieve impressive results in A/V classification, their performances
usually degrade severely when being directly applied to another database, due
to the domain shift, e.g., caused by the variations in imaging protocols. In
this paper, we propose a novel vessel-mixing based consistency regularization
framework, for cross-domain learning in retinal A/V classification. Specially,
to alleviate the severe bias to source domain, based on the label smooth prior,
the model is regularized to give consistent predictions for unlabeled
target-domain inputs that are under perturbation. This consistency
regularization implicitly introduces a mechanism where the model and the
perturbation is opponent to each other, where the model is pushed to be robust
enough to cope with the perturbation. Thus, we investigate a more difficult
opponent to further inspire the robustness of model, in the scenario of retinal
A/V, called vessel-mixing perturbation. Specially, it effectively disturbs the
fundus images especially the vessel structures by mixing two images regionally.
We conduct extensive experiments on cross-domain A/V classification using four
public datasets, which are collected by diverse institutions and imaging
devices. The results demonstrate that our method achieves the state-of-the-art
cross-domain performance, which is also close to the upper bound obtained by
fully supervised learning on target domain.
</p>
<a href="http://arxiv.org/abs/2103.09097" target="_blank">arXiv:2103.09097</a> [<a href="http://arxiv.org/pdf/2103.09097" target="_blank">pdf</a>]

<h2>Is it Enough to Optimize CNN Architectures on ImageNet?. (arXiv:2103.09108v1 [cs.CV])</h2>
<h3>Lukas Tuggener, J&#xfc;rgen Schmidhuber, Thilo Stadelmann</h3>
<p>An implicit but pervasive hypothesis of modern computer vision research is
that convolutional neural network (CNN) architectures that perform better on
ImageNet will also perform better on other vision datasets. We challenge this
hypothesis through an extensive empirical study for which we train 500 sampled
CNN architectures on ImageNet as well as 8 other image classification datasets
from a wide array of application domains. The relationship between architecture
and performance varies wildly, depending on the datasets. For some of them, the
performance correlation with ImageNet is even negative. Clearly, it is not
enough to optimize architectures solely for ImageNet when aiming for progress
that is relevant for all applications. Therefore, we identify two
dataset-specific performance indicators: the cumulative width across layers as
well as the total depth of the network. Lastly, we show that the range of
dataset variability covered by ImageNet can be significantly extended by adding
ImageNet subsets restricted to few classes.
</p>
<a href="http://arxiv.org/abs/2103.09108" target="_blank">arXiv:2103.09108</a> [<a href="http://arxiv.org/pdf/2103.09108" target="_blank">pdf</a>]

<h2>Distributed motion coordination for multi-robot systems under LTL specifications. (arXiv:2103.09111v1 [cs.RO])</h2>
<h3>Pian Yu, Dimos V. Dimarogonas</h3>
<p>This paper investigates the online motion coordination problem for a group of
mobile robots moving in a shared workspace, each of which is assigned a linear
temporal logic specification. Based on the realistic assumptions that each
robot is subject to both state and input constraints and can have only local
view and local information, a fully distributed multi-robot motion coordination
strategy is proposed. For each robot, the motion coordination strategy consists
of three layers. An offline layer pre-computes the braking area for each region
in the workspace, the controlled transition system, and a so-called potential
function. An initialization layer outputs an initially safely satisfying
trajectory. An online coordination layer resolves conflicts when one occurs.
The online coordination layer is further decomposed into three steps. Firstly,
a conflict detection algorithm is implemented, which detects conflicts with
neighboring robots. Whenever conflicts are detected, a rule is designed to
assign dynamically a planning order to each pair of neighboring robots.
Finally, a sampling-based algorithm is designed to generate local
collision-free trajectories for the robot which at the same time guarantees the
feasibility of the specification. Safety is proven to be guaranteed for all
robots at any time. The effectiveness and the computational tractability of the
resulting solution is verified numerically by two case studies.
</p>
<a href="http://arxiv.org/abs/2103.09111" target="_blank">arXiv:2103.09111</a> [<a href="http://arxiv.org/pdf/2103.09111" target="_blank">pdf</a>]

<h2>Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v1 [cs.CV])</h2>
<h3>Joseph P Robinson, Can Qin, Yann Henon, Samson Timoner, Yun Fu</h3>
<p>There are demographic biases in the SOTA CNN used for FR. Our BFW dataset
serves as a proxy to measure bias across ethnicity and gender subgroups,
allowing us to characterize FR performances per subgroup. We show performances
are non-optimal when a single score threshold is used to determine whether
sample pairs are genuine or imposter. Furthermore, actual performance ratings
vary greatly from the reported across subgroups. Thus, claims of specific error
rates only hold true for populations matching that of the validation data. We
mitigate the imbalanced performances using a novel domain adaptation learning
scheme on the facial encodings extracted using SOTA deep nets. Not only does
this technique balance performance, but it also boosts the overall performance.
A benefit of the proposed is to preserve identity information in facial
features while removing demographic knowledge in the lower dimensional
features. The removal of demographic knowledge prevents future potential biases
from being injected into decision-making. Additionally, privacy concerns are
satisfied by this removal. We explore why this works qualitatively with hard
samples. We also show quantitatively that subgroup classifiers can no longer
learn from the encodings mapped by the proposed.
</p>
<a href="http://arxiv.org/abs/2103.09118" target="_blank">arXiv:2103.09118</a> [<a href="http://arxiv.org/pdf/2103.09118" target="_blank">pdf</a>]

<h2>QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection. (arXiv:2103.09136v1 [cs.CV])</h2>
<h3>Chenhongyi Yang, Zehao Huang, Naiyan Wang</h3>
<p>While general object detection with deep learning has achieved great success
in the past few years, the performance and efficiency of detecting small
objects are far from satisfactory. The most common and effective way to promote
small object detection is to use high-resolution images or feature maps.
However, both approaches induce costly computation since the computational cost
grows squarely as the size of images and features increases. To get the best of
two worlds, we propose QueryDet that uses a novel query mechanism to accelerate
the inference speed of feature-pyramid based object detectors. The pipeline
composes two steps: it first predicts the coarse locations of small objects on
low-resolution features and then computes the accurate detection results using
high-resolution features sparsely guided by those coarse positions. In this
way, we can not only harvest the benefit of high-resolution feature maps but
also avoid useless computation for the background area. On the popular COCO
dataset, the proposed method improves the detection mAP by 1.0 and mAP-small by
2.0, and the high-resolution inference speed is improved to 3.0x on average. On
VisDrone dataset, which contains more small objects, we create a new
state-of-the-art while gaining a 2.3x high-resolution acceleration on average.
Code is available at: https://github.com/ChenhongyiYang/QueryDet-PyTorch
</p>
<a href="http://arxiv.org/abs/2103.09136" target="_blank">arXiv:2103.09136</a> [<a href="http://arxiv.org/pdf/2103.09136" target="_blank">pdf</a>]

<h2>Simultaneous Multi-View Camera Pose Estimation and Object Tracking with Square Planar Markers. (arXiv:2103.09141v1 [cs.CV])</h2>
<h3>Hamid Sarmadi, Rafael Mu&#xf1;oz-Salinas, M.A. Berb&#xed;s, R. Medina-Carnicer</h3>
<p>Object tracking is a key aspect in many applications such as augmented
reality in medicine (e.g. tracking a surgical instrument) or robotics. Squared
planar markers have become popular tools for tracking since their pose can be
estimated from their four corners. While using a single marker and a single
camera limits the working area considerably, using multiple markers attached to
an object requires estimating their relative position, which is not trivial,
for high accuracy tracking. Likewise, using multiple cameras requires
estimating their extrinsic parameters, also a tedious process that must be
repeated whenever a camera is moved.

This work proposes a novel method to simultaneously solve the above-mentioned
problems. From a video sequence showing a rigid set of planar markers recorded
from multiple cameras, the proposed method is able to automatically obtain the
three-dimensional configuration of the markers, the extrinsic parameters of the
cameras, and the relative pose between the markers and the cameras at each
frame. Our experiments show that our approach can obtain highly accurate
results for estimating these parameters using low resolution cameras.

Once the parameters are obtained, tracking of the object can be done in real
time with a low computational cost. The proposed method is a step forward in
the development of cost-effective solutions for object tracking.
</p>
<a href="http://arxiv.org/abs/2103.09141" target="_blank">arXiv:2103.09141</a> [<a href="http://arxiv.org/pdf/2103.09141" target="_blank">pdf</a>]

<h2>Adversarial Driving: Attacking End-to-End Autonomous Driving Systems. (arXiv:2103.09151v1 [cs.CV])</h2>
<h3>Han Wu, Wenjie Ruan</h3>
<p>As the research in deep neural networks advances, deep convolutional networks
become feasible for automated driving tasks. There is an emerging trend of
employing end-to-end models in the automation of driving tasks. However,
previous research unveils that deep neural networks are vulnerable to
adversarial attacks in classification tasks. While for regression tasks such as
autonomous driving, the effect of these attacks remains uncertain. In this
research, we devise two white-box targeted attacks against end-to-end
autonomous driving systems. The driving model takes an image as input and
outputs the steering angle. Our attacks can manipulate the behaviour of the
autonomous driving system only by changing the input image. The implementation
of both attacks can achieve real-time performance on CPUs. This demo aims to
raise concerns over applications of end-to-end models in safety-critical
systems.
</p>
<a href="http://arxiv.org/abs/2103.09151" target="_blank">arXiv:2103.09151</a> [<a href="http://arxiv.org/pdf/2103.09151" target="_blank">pdf</a>]

<h2>Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion Recognition. (arXiv:2103.09154v1 [cs.CV])</h2>
<h3>Liam Schoneveld, Alice Othmani, Hazem Abdelkawy</h3>
<p>Emotional expressions are the behaviors that communicate our emotional state
or attitude to others. They are expressed through verbal and non-verbal
communication. Complex human behavior can be understood by studying physical
features from multiple modalities; mainly facial, vocal and physical gestures.
Recently, spontaneous multi-modal emotion recognition has been extensively
studied for human behavior analysis. In this paper, we propose a new deep
learning-based approach for audio-visual emotion recognition. Our approach
leverages recent advances in deep learning like knowledge distillation and
high-performing deep architectures. The deep feature representations of the
audio and visual modalities are fused based on a model-level fusion strategy. A
recurrent neural network is then used to capture the temporal dynamics. Our
proposed approach substantially outperforms state-of-the-art approaches in
predicting valence on the RECOLA dataset. Moreover, our proposed visual facial
expression feature extraction network outperforms state-of-the-art results on
the AffectNet and Google Facial Expression Comparison datasets.
</p>
<a href="http://arxiv.org/abs/2103.09154" target="_blank">arXiv:2103.09154</a> [<a href="http://arxiv.org/pdf/2103.09154" target="_blank">pdf</a>]

<h2>LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud Segmentation. (arXiv:2103.09160v1 [cs.CV])</h2>
<h3>Jingdao Chen, Zsolt Kira, Yong K. Cho</h3>
<p>3D point cloud segmentation is an important function that helps robots
understand the layout of their surrounding environment and perform tasks such
as grasping objects, avoiding obstacles, and finding landmarks. Current
segmentation methods are mostly class-specific, many of which are tuned to work
with specific object categories and may not be generalizable to different types
of scenes. This research proposes a learnable region growing method for
class-agnostic point cloud segmentation, specifically for the task of instance
label prediction. The proposed method is able to segment any class of objects
using a single deep neural network without any assumptions about their shapes
and sizes. The deep neural network is trained to predict how to add or remove
points from a point cloud region to morph it into incrementally more complete
regions of an object instance. Segmentation results on the S3DIS and ScanNet
datasets show that the proposed method outperforms competing methods by 1%-9%
on 6 different evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2103.09160" target="_blank">arXiv:2103.09160</a> [<a href="http://arxiv.org/pdf/2103.09160" target="_blank">pdf</a>]

<h2>Behavior-Tree-Based Person Search for Symbiotic Autonomous Mobile Robot Tasks. (arXiv:2103.09162v1 [cs.RO])</h2>
<h3>Marvin Stuede, Timo Lerche, Martin Alexander Petersen, Svenja Spindeldreier</h3>
<p>We consider the problem of people search by a mobile social robot in case of
a situation that cannot be solved by the robot alone. Examples are physically
opening a closed door or operating an elevator. Based on the Behavior Tree
framework, we create a modular and easily extendable action sequence with the
goal of finding a person to assist the robot. By decomposing the Behavior Tree
as a Discrete Time Markov Chain, we obtain an estimate of the probability and
rate of success of the options for action, especially where the robot should
wait or search for people.In a real-world experiment, the presented method is
compared with other common approaches in a total of 588 test runs over the
course of one week, starting at two different locations in a university
building. We show our method to be superior to other approaches in terms of
success rate and duration until a finding person and returning to the start
location.
</p>
<a href="http://arxiv.org/abs/2103.09162" target="_blank">arXiv:2103.09162</a> [<a href="http://arxiv.org/pdf/2103.09162" target="_blank">pdf</a>]

<h2>RackLay: Multi-Layer Layout Estimation for Warehouse Racks. (arXiv:2103.09174v1 [cs.CV])</h2>
<h3>Meher Shashwat Nigam, Avinash Prabhu, Anurag Sahu, Puru Gupta, Tanvi Karandikar, N. Sai Shankar, Ravi Kiran Sarvadevabhatla, K. Madhava Krishna</h3>
<p>Given a monocular colour image of a warehouse rack, we aim to predict the
bird's-eye view layout for each shelf in the rack, which we term as multi-layer
layout prediction. To this end, we present RackLay, a deep neural network for
real-time shelf layout estimation from a single image. Unlike previous layout
estimation methods, which provide a single layout for the dominant ground plane
alone, \textit{RackLay} estimates the top-view \underline{and} front-view
layout for each shelf in the considered rack populated with objects. RackLay's
architecture and its variants are versatile and estimate accurate layouts for
diverse scenes characterized by varying number of visible shelves in an image,
large range in shelf occupancy factor and varied background clutter. Given the
extreme paucity of datasets in this space and the difficulty involved in
acquiring real data from warehouses, we additionally release a flexible
synthetic dataset generation pipeline \textit{WareSynth} which allows users to
control the generation process and tailor the dataset according to contingent
application. The ablations across architectural variants and comparison with
strong prior baselines vindicate the efficacy of \textit{RackLay} as an apt
architecture for the novel problem of multi-layered layout estimation. We also
show that fusing the top-view and front-view enables 3D reasoning applications
such as metric free space estimation for the considered rack.
</p>
<a href="http://arxiv.org/abs/2103.09174" target="_blank">arXiv:2103.09174</a> [<a href="http://arxiv.org/pdf/2103.09174" target="_blank">pdf</a>]

<h2>Conceptual Text Region Network: Cognition-Inspired Accurate Scene Text Detection. (arXiv:2103.09179v1 [cs.CV])</h2>
<h3>Chenwei Cui, Liangfu Lu, Zhiyuan Tan, Amir Hussain</h3>
<p>Segmentation-based methods are widely used for scene text detection due to
their superiority in describing arbitrary-shaped text instances. However, two
major problems still exist: 1) current label generation techniques are mostly
empirical and lack theoretical support, discouraging elaborate label design; 2)
as a result, most methods rely heavily on text kernel segmentation which is
unstable and requires deliberate tuning. To address these challenges, we
propose a human cognition-inspired framework, termed, Conceptual Text Region
Network (CTRNet). The framework utilizes Conceptual Text Regions (CTRs), which
is a class of cognition-based tools inheriting good mathematical properties,
allowing for sophisticated label design. Another component of CTRNet is an
inference pipeline that, with the help of CTRs, completely omits the need for
text kernel segmentation. Compared with previous segmentation-based methods,
our approach is not only more interpretable but also more accurate.
Experimental results show that CTRNet achieves state-of-the-art performance on
benchmark CTW1500, Total-Text, MSRA-TD500, and ICDAR 2015 datasets, yielding
performance gains of up to 2.0%. Notably, to the best of our knowledge, CTRNet
is among the first detection models to achieve F-measures higher than 85.0% on
all four of the benchmarks, with remarkable consistency and stability.
</p>
<a href="http://arxiv.org/abs/2103.09179" target="_blank">arXiv:2103.09179</a> [<a href="http://arxiv.org/pdf/2103.09179" target="_blank">pdf</a>]

<h2>Formation Control for UAVs Using a Flux Guided Approach. (arXiv:2103.09184v1 [cs.RO])</h2>
<h3>John Hartley, Hubert P. H. Shum, Edmond S. L. Ho, He Wang, Subramanian Ramamoorthy</h3>
<p>While multiple studies have proposed methods for the formation control of
unmanned aerial vehicles (UAV), the trajectories generated are generally
unsuitable for tracking targets where the optimum coverage of the target by the
formation is required at all times. We propose a path planning approach called
the Flux Guided (FG) method, which generates collision-free trajectories while
maximising the coverage of one or more targets. We show that by reformulating
an existing least-squares flux minimisation problem as a constrained
optimisation problem, the paths obtained are $1.5 \times$ shorter and track
directly toward the target. Also, we demonstrate that the scale of the
formation can be controlled during flight, and that this feature can be used to
track multiple scattered targets. The method is highly scalable since the
planning algorithm is only required for a sub-set of UAVs on the open boundary
of the formation's surface. Finally, through simulating a 3d dynamic particle
system that tracks the desired trajectories using a PID controller, we show
that the resulting trajectories after time-optimal parameterisation are
suitable for robotic controls.
</p>
<a href="http://arxiv.org/abs/2103.09184" target="_blank">arXiv:2103.09184</a> [<a href="http://arxiv.org/pdf/2103.09184" target="_blank">pdf</a>]

<h2>Sparse Curriculum Reinforcement Learning for End-to-End Driving. (arXiv:2103.09189v1 [cs.RO])</h2>
<h3>Pranav Agarwal, Pierre de Beaucorps, Raoul de Charette</h3>
<p>Deep reinforcement Learning for end-to-end driving is limited by the need of
complex reward engineering. Sparse rewards can circumvent this challenge but
suffers from long training time and leads to sub-optimal policy. In this work,
we explore driving using only goal conditioned sparse rewards and propose a
curriculum learning approach for end to end driving using only navigation view
maps that benefit from small virtual-to-real domain gap. To address the
complexity of multiple driving policies, we learn concurrent individual
policies which are selected at inference by a navigation system. We demonstrate
the ability of our proposal to generalize on unseen road layout, and to drive
longer than in the training.
</p>
<a href="http://arxiv.org/abs/2103.09189" target="_blank">arXiv:2103.09189</a> [<a href="http://arxiv.org/pdf/2103.09189" target="_blank">pdf</a>]

<h2>Back to the Feature: Learning Robust Camera Localization from Pixels to Pose. (arXiv:2103.09213v1 [cs.CV])</h2>
<h3>Paul-Edouard Sarlin, Ajaykumar Unagar, M&#xe5;ns Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, Torsten Sattler</h3>
<p>Camera pose estimation in known scenes is a 3D geometry task recently tackled
by multiple learning algorithms. Many regress precise geometric quantities,
like poses or 3D points, from an input image. This either fails to generalize
to new viewpoints or ties the model parameters to a specific scene. In this
paper, we go Back to the Feature: we argue that deep networks should focus on
learning robust and invariant visual features, while the geometric estimation
should be left to principled algorithms. We introduce PixLoc, a scene-agnostic
neural network that estimates an accurate 6-DoF pose from an image and a 3D
model. Our approach is based on the direct alignment of multiscale deep
features, casting camera localization as metric learning. PixLoc learns strong
data priors by end-to-end training from pixels to pose and exhibits exceptional
generalization to new scenes by separating model parameters and scene geometry.
The system can localize in large environments given coarse pose priors but also
improve the accuracy of sparse feature matching by jointly refining keypoints
and poses with little overhead. The code will be publicly available at
https://github.com/cvg/pixloc.
</p>
<a href="http://arxiv.org/abs/2103.09213" target="_blank">arXiv:2103.09213</a> [<a href="http://arxiv.org/pdf/2103.09213" target="_blank">pdf</a>]

<h2>Design and Development of Autonomous Delivery Robot. (arXiv:2103.09229v1 [cs.RO])</h2>
<h3>Aniket Gujarathi, Akshay Kulkarni, Unmesh Patil, Yogesh Phalak, Rajeshree Deotalu, Aman Jain, Navid Panchi, Ashwin Dhabale, Shital Chiddarwar</h3>
<p>The field of autonomous robotics is growing at a rapid rate. The trend to use
increasingly more sensors in vehicles is driven both by legislation and
consumer demands for higher safety and reliable service. Nowadays, robots are
found everywhere, ranging from homes, hospitals to industries, and military
operations. Autonomous robots are developed to be robust enough to work beside
humans and to carry out jobs efficiently. Humans have a natural sense of
understanding of the physical forces acting around them like gravity, sense of
motion, etc. which are not taught explicitly but are developed naturally.
However, this is not the case with robots. To make the robot fully autonomous
and competent to work with humans, the robot must be able to perceive the
situation and devise a plan for smooth operation, considering all the
adversities that may occur while carrying out the tasks. In this thesis, we
present an autonomous mobile robot platform that delivers the package within
the VNIT campus without any human intercommunication. From an initial
user-supplied geographic target location, the system plans an optimized path
and autonomously navigates through it. The entire pipeline of an autonomous
robot working in outdoor environments is explained in detail in this thesis.
</p>
<a href="http://arxiv.org/abs/2103.09229" target="_blank">arXiv:2103.09229</a> [<a href="http://arxiv.org/pdf/2103.09229" target="_blank">pdf</a>]

<h2>Restore from Restored: Video Restoration with Pseudo Clean Video. (arXiv:2003.04279v3 [cs.CV] UPDATED)</h2>
<h3>Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim</h3>
<p>In this study, we propose a self-supervised video denoising method called
"restore-from-restored." This method fine-tunes a pre-trained network by using
a pseudo clean video during the test phase. The pseudo clean video is obtained
by applying a noisy video to the baseline network. By adopting a fully
convolutional neural network (FCN) as the baseline, we can improve video
denoising performance without accurate optical flow estimation and registration
steps, in contrast to many conventional video restoration methods, due to the
translation equivariant property of the FCN. Specifically, the proposed method
can take advantage of plentiful similar patches existing across multiple
consecutive frames (i.e., patch-recurrence); these patches can boost the
performance of the baseline network by a large margin. We analyze the
restoration performance of the fine-tuned video denoising networks with the
proposed self-supervision-based learning algorithm, and demonstrate that the
FCN can utilize recurring patches without requiring accurate registration among
adjacent frames. In our experiments, we apply the proposed method to
state-of-the-art denoisers and show that our fine-tuned networks achieve a
considerable improvement in denoising performance.
</p>
<a href="http://arxiv.org/abs/2003.04279" target="_blank">arXiv:2003.04279</a> [<a href="http://arxiv.org/pdf/2003.04279" target="_blank">pdf</a>]

<h2>Guiding Robot Exploration in Reinforcement Learning via Automated Planning. (arXiv:2004.11456v2 [cs.RO] UPDATED)</h2>
<h3>Yohei Hayamizu, Saeid Amiri, Kishan Chandan, Keiki Takadama, Shiqi Zhang</h3>
<p>Reinforcement learning (RL) enables an agent to learn from trial-and-error
experiences toward achieving long-term goals; automated planning aims to
compute plans for accomplishing tasks using action knowledge. Despite their
shared goal of completing complex tasks, the development of RL and automated
planning has been largely isolated due to their different computational
modalities. Focusing on improving RL agents' learning efficiency, we develop
Guided Dyna-Q (GDQ) to enable RL agents to reason with action knowledge to
avoid exploring less-relevant states. The action knowledge is used for
generating artificial experiences from an optimistic simulation. GDQ has been
evaluated in simulation and using a mobile robot conducting navigation tasks in
a multi-room office environment. Compared with competitive baselines, GDQ
significantly reduces the effort in exploration while improving the quality of
learned policies.
</p>
<a href="http://arxiv.org/abs/2004.11456" target="_blank">arXiv:2004.11456</a> [<a href="http://arxiv.org/pdf/2004.11456" target="_blank">pdf</a>]

<h2>Decoupling Global and Local Representations via Invertible Generative Flows. (arXiv:2004.11820v2 [cs.CV] UPDATED)</h2>
<h3>Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard Hovy</h3>
<p>In this work, we propose a new generative model that is capable of
automatically decoupling global and local representations of images in an
entirely unsupervised setting, by embedding a generative flow in the VAE
framework to model the decoder. Specifically, the proposed model utilizes the
variational auto-encoding framework to learn a (low-dimensional) vector of
latent variables to capture the global information of an image, which is fed as
a conditional input to a flow-based invertible decoder with architecture
borrowed from style transfer literature. Experimental results on standard image
benchmarks demonstrate the effectiveness of our model in terms of density
estimation, image generation and unsupervised representation learning.
Importantly, this work demonstrates that with only architectural inductive
biases, a generative model with a likelihood-based objective is capable of
learning decoupled representations, requiring no explicit supervision. The code
for our model is available at https://github.com/XuezheMax/wolf.
</p>
<a href="http://arxiv.org/abs/2004.11820" target="_blank">arXiv:2004.11820</a> [<a href="http://arxiv.org/pdf/2004.11820" target="_blank">pdf</a>]

<h2>Dataset Bias in Few-shot Image Recognition. (arXiv:2008.07960v3 [cs.CV] UPDATED)</h2>
<h3>Shuqiang Jiang, Yaohui Zhu, Chenlong Liu, Xinhang Song, Xiangyang Li, Weiqing Min</h3>
<p>The goal of few-shot image recognition (FSIR) is to identify novel categories
with a small number of annotated samples by exploiting transferable knowledge
from training data (base categories). Most current studies assume that the
transferable knowledge can be well used to identify novel categories. However,
such transferable capability may be impacted by the dataset bias, and this
problem has rarely been investigated before. Besides, most of few-shot learning
methods are biased to different datasets, which is also an important issue that
needs to be investigated deeply. In this paper, we first investigate the impact
of transferable capabilities learned from base categories. Specifically, we use
the relevance to measure relationships between base categories and novel
categories. Distributions of base categories are depicted via the instance
density and category diversity. The FSIR model learns better transferable
knowledge from relevant training data. In the relevant data, dense instances or
diverse categories can further enrich the learned knowledge. Experimental
results on different sub-datasets of ImagNet demonstrate category relevance,
instance density and category diversity can depict transferable bias from base
categories. Second, we investigate performance differences on different
datasets from dataset structures and different few-shot learning methods.
Specifically, we introduce image complexity, intra-concept visual consistency,
and inter-concept visual similarity to quantify characteristics of dataset
structures. We use these quantitative characteristics and four few-shot
learning methods to analyze performance differences on five different datasets.
Based on the experimental analysis, some insightful observations are obtained
from the perspective of both dataset structures and few-shot learning methods.
We hope these observations are useful to guide future FSIR research.
</p>
<a href="http://arxiv.org/abs/2008.07960" target="_blank">arXiv:2008.07960</a> [<a href="http://arxiv.org/pdf/2008.07960" target="_blank">pdf</a>]

<h2>3D_DEN: Open-ended 3D Object Recognition using Dynamically Expandable Networks. (arXiv:2009.07213v2 [cs.CV] UPDATED)</h2>
<h3>Sudhakaran Jain, Hamidreza Kasaei</h3>
<p>Service robots, in general, have to work independently and adapt to the
dynamic changes happening in the environment in real-time. One important aspect
in such scenarios is to continually learn to recognize newer object categories
when they become available. This combines two main research problems namely
continual learning and 3D object recognition. Most of the existing research
approaches include the use of deep Convolutional Neural Networks (CNNs)
focusing on image datasets. A modified approach might be needed for continually
learning 3D object categories. A major concern in using CNNs is the problem of
catastrophic forgetting when a model tries to learn a new task. Despite various
proposed solutions to mitigate this problem, there still exist some downsides
of such solutions, e.g., computational complexity, especially when learning
substantial number of tasks. These downsides can pose major problems in robotic
scenarios where real-time response plays an essential role. Towards addressing
this challenge, we propose a new deep transfer learning approach based on a
dynamic architectural method to make robots capable of open-ended learning
about new 3D object categories. Furthermore, we make sure that the mentioned
downsides are minimized to a great extent. Experimental results showed that the
proposed model outperformed state-of-the-art approaches with regards to
accuracy and also substantially minimizes computational overhead.
</p>
<a href="http://arxiv.org/abs/2009.07213" target="_blank">arXiv:2009.07213</a> [<a href="http://arxiv.org/pdf/2009.07213" target="_blank">pdf</a>]

<h2>Uncertainty Sets for Image Classifiers using Conformal Prediction. (arXiv:2009.14193v3 [cs.CV] UPDATED)</h2>
<h3>Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, Michael I. Jordan</h3>
<p>Convolutional image classifiers can achieve high predictive accuracy, but
quantifying their uncertainty remains an unresolved challenge, hindering their
deployment in consequential settings. Existing uncertainty quantification
techniques, such as Platt scaling, attempt to calibrate the network's
probability estimates, but they do not have formal guarantees. We present an
algorithm that modifies any classifier to output a predictive set containing
the true label with a user-specified probability, such as 90%. The algorithm is
simple and fast like Platt scaling, but provides a formal finite-sample
coverage guarantee for every model and dataset. Our method modifies an existing
conformal prediction algorithm to give more stable predictive sets by
regularizing the small scores of unlikely classes after Platt scaling. In
experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other
classifiers, our scheme outperforms existing approaches, achieving coverage
with sets that are often factors of 5 to 10 smaller than a stand-alone Platt
scaling baseline.
</p>
<a href="http://arxiv.org/abs/2009.14193" target="_blank">arXiv:2009.14193</a> [<a href="http://arxiv.org/pdf/2009.14193" target="_blank">pdf</a>]

<h2>Contact Localization for Robot Arms in Motion without Torque Sensing. (arXiv:2011.03142v2 [cs.RO] UPDATED)</h2>
<h3>Jacky Liang, Oliver Kroemer</h3>
<p>Detecting and localizing contacts is essential for robot manipulators to
perform contact-rich tasks in unstructured environments. While robot skins can
localize contacts on the surface of robot arms, these sensors are not yet
robust or easily accessible. As such, prior works have explored using
proprioceptive observations, such as joint velocities and torques, to perform
contact localization. Many past approaches assume the robot is static during
contact incident, a single contact is made at a time, or having access to
accurate dynamics models and joint torque sensing. In this work, we relax these
assumptions and propose using Domain Randomization to train a neural network to
localize contacts of robot arms in motion without joint torque observations.
Our method uses a novel cylindrical projection encoding of the robot arm
surface, which allows the network to use convolution layers to process input
features and transposed convolution layers to predict contacts. The trained
network achieves a contact detection accuracy of 91.5% and a mean contact
localization error of 3.0cm. We further demonstrate an application of the
contact localization model in an obstacle mapping task, evaluated in both
simulation and the real world.
</p>
<a href="http://arxiv.org/abs/2011.03142" target="_blank">arXiv:2011.03142</a> [<a href="http://arxiv.org/pdf/2011.03142" target="_blank">pdf</a>]

<h2>Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v3 [cs.CV] UPDATED)</h2>
<h3>Shichao Li, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng</h3>
<p>We present a new learning-based approach to recover egocentric 3D vehicle
pose from a single RGB image. In contrast to previous works that directly map
from local appearance to 3D angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) for 3D
pose estimation. We design a deep model that transforms perceived intensities
to IGRs, which are mapped to a 3D representation encoding object orientation in
the camera coordinate system. To fulfill our goal, we need to specify what IGRs
to use and how to learn them more effectively. We answer the former question by
designing an interpolated cuboid representation that derives from primitive 3D
annotation readily. The latter question motivates us to incorporate geometry
knowledge by designing a new loss function based on a projective invariant.
This loss function allows unlabeled data to be used in the training stage which
is validated to improve representation learning. Our system outperforms
previous monocular RGB-based methods for joint vehicle detection and pose
estimation on the KITTI benchmark, achieving performance even comparable to
stereo methods. Code and pre-trained models will be available at the project
website.
</p>
<a href="http://arxiv.org/abs/2011.08464" target="_blank">arXiv:2011.08464</a> [<a href="http://arxiv.org/pdf/2011.08464" target="_blank">pdf</a>]

<h2>Detection of Binary Square Fiducial Markers Using an Event Camera. (arXiv:2012.06516v3 [cs.CV] UPDATED)</h2>
<h3>Hamid Sarmadi, Rafael Mu&#xf1;oz-Salinas, Miguel A. Olivares-Mendez, Rafael Medina-Carnicer</h3>
<p>Event cameras are a new type of image sensors that output changes in light
intensity (events) instead of absolute intensity values. They have a very high
temporal resolution and a high dynamic range. In this paper, we propose a
method to detect and decode binary square markers using an event camera. We
detect the edges of the markers by detecting line segments in an image created
from events in the current packet. The line segments are combined to form
marker candidates. The bit value of marker cells is decoded using the events on
their borders. To the best of our knowledge, no other approach exists for
detecting square binary markers directly from an event camera using only the
CPU unit in real-time. Experimental results show that the performance of our
proposal is much superior to the one from the RGB ArUco marker detector. The
proposed method can achieve the real-time performance on a single CPU thread.
</p>
<a href="http://arxiv.org/abs/2012.06516" target="_blank">arXiv:2012.06516</a> [<a href="http://arxiv.org/pdf/2012.06516" target="_blank">pdf</a>]

<h2>Progressive One-shot Human Parsing. (arXiv:2012.11810v2 [cs.CV] UPDATED)</h2>
<h3>Haoyu He, Jing Zhang, Bhavani Thuraisingham, Dacheng Tao</h3>
<p>Prior human parsing models are limited to parsing humans into classes
pre-defined in the training data, which is not flexible to generalize to unseen
classes, e.g., new clothing in fashion analysis. In this paper, we propose a
new problem named one-shot human parsing (OSHP) that requires to parse human
into an open set of reference classes defined by any single reference example.
During training, only base classes defined in the training set are exposed,
which can overlap with part of reference classes. In this paper, we devise a
novel Progressive One-shot Parsing network (POPNet) to address two critical
challenges , i.e., testing bias and small sizes. POPNet consists of two
collaborative metric learning modules named Attention Guidance Module and
Nearest Centroid Module, which can learn representative prototypes for base
classes and quickly transfer the ability to unseen classes during testing,
thereby reducing testing bias. Moreover, POPNet adopts a progressive human
parsing framework that can incorporate the learned knowledge of parent classes
at the coarse granularity to help recognize the descendant classes at the fine
granularity, thereby handling the small sizes issue. Experiments on the ATR-OS
benchmark tailored for OSHP demonstrate POPNet outperforms other representative
one-shot segmentation models by large margins and establishes a strong
baseline. Source code can be found at
https://github.com/Charleshhy/One-shot-Human-Parsing.
</p>
<a href="http://arxiv.org/abs/2012.11810" target="_blank">arXiv:2012.11810</a> [<a href="http://arxiv.org/pdf/2012.11810" target="_blank">pdf</a>]

<h2>SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v2 [cs.CV] UPDATED)</h2>
<h3>Prarthana Bhattacharyya, Chengjie Huang, Krzysztof Czarnecki</h3>
<p>Existing point-cloud based 3D object detectors use convolution-like operators
to process information in a local neighbourhood with fixed-weight kernels and
aggregate global context hierarchically. However, non-local neural networks and
self-attention for 2D vision have shown that explicitly modeling long-range
interactions can lead to more robust and competitive models. In this paper, we
propose two variants of self-attention for contextual modeling in 3D object
detection by augmenting convolutional features with self-attention features. We
first incorporate the pairwise self-attention mechanism into the current
state-of-the-art BEV, voxel and point-based detectors and show consistent
improvement over strong baseline models of up to 1.5 3D AP while simultaneously
reducing their parameter footprint and computational cost by 15-80\% and
30-50\%, respectively, on the KITTI validation set. We next propose a
self-attention variant that samples a subset of the most representative
features by learning deformations over randomly sampled locations. This not
only allows us to scale explicit global contextual modeling to larger
point-clouds, but also leads to more discriminative and informative feature
descriptors. Our method can be flexibly applied to most state-of-the-art
detectors with increased accuracy and parameter and compute efficiency. We show
our proposed method improves 3D object detection performance on KITTI, nuScenes
and Waymo Open datasets. Code is available at
\url{https://github.com/AutoVision-cloud/SA-Det3D}.
</p>
<a href="http://arxiv.org/abs/2101.02672" target="_blank">arXiv:2101.02672</a> [<a href="http://arxiv.org/pdf/2101.02672" target="_blank">pdf</a>]

<h2>Probability Trajectory: One New Movement Description for Trajectory Prediction. (arXiv:2101.10595v2 [cs.CV] UPDATED)</h2>
<h3>Pei Lv, Hui Wei, Tianxin Gu, Yuzhen Zhang, Xiaoheng Jiang, Bing Zhou, Mingliang Xu</h3>
<p>Trajectory prediction is a fundamental and challenging task for numerous
applications, such as autonomous driving and intelligent robots. Currently,
most of existing work treat the pedestrian trajectory as a series of fixed
two-dimensional coordinates. However, in real scenarios, the trajectory often
exhibits randomness, and has its own probability distribution. Inspired by this
observed fact, also considering other movement characteristics of pedestrians,
we propose one simple and intuitive movement description, probability
trajectory, which maps the coordinate points of pedestrian trajectory into
two-dimensional Gaussian distribution in images. Based on this unique
description, we develop one novel trajectory prediction method, called social
probability. The method combines the new probability trajectory and powerful
convolution recurrent neural networks together. Both the input and output of
our method are probability trajectories, which provide the recurrent neural
network with sufficient spatial and random information of moving pedestrians.
And the social probability extracts spatio-temporal features directly on the
new movement description to generate robust and accurate predicted results. The
experiments on public benchmark datasets show the effectiveness of the proposed
method.
</p>
<a href="http://arxiv.org/abs/2101.10595" target="_blank">arXiv:2101.10595</a> [<a href="http://arxiv.org/pdf/2101.10595" target="_blank">pdf</a>]

<h2>Multi-Stage Progressive Image Restoration. (arXiv:2102.02808v2 [cs.CV] UPDATED)</h2>
<h3>Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao</h3>
<p>Image restoration tasks demand a complex balance between spatial details and
high-level contextualized information while recovering images. In this paper,
we propose a novel synergistic design that can optimally balance these
competing goals. Our main proposal is a multi-stage architecture, that
progressively learns restoration functions for the degraded inputs, thereby
breaking down the overall recovery process into more manageable steps.
Specifically, our model first learns the contextualized features using
encoder-decoder architectures and later combines them with a high-resolution
branch that retains local information. At each stage, we introduce a novel
per-pixel adaptive design that leverages in-situ supervised attention to
reweight the local features. A key ingredient in such a multi-stage
architecture is the information exchange between different stages. To this end,
we propose a two-faceted approach where the information is not only exchanged
sequentially from early to late stages, but lateral connections between feature
processing blocks also exist to avoid any loss of information. The resulting
tightly interlinked multi-stage architecture, named as MPRNet, delivers strong
performance gains on ten datasets across a range of tasks including image
deraining, deblurring, and denoising. The source code and pre-trained models
are available at https://github.com/swz30/MPRNet.
</p>
<a href="http://arxiv.org/abs/2102.02808" target="_blank">arXiv:2102.02808</a> [<a href="http://arxiv.org/pdf/2102.02808" target="_blank">pdf</a>]

<h2>MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square. (arXiv:2102.03771v2 [cs.RO] UPDATED)</h2>
<h3>Yue Pan, Pengchuan Xiao, Yujie He, Zhenlei Shao, Zesong Li</h3>
<p>The rapid development of autonomous driving and mobile mapping calls for
off-the-shelf LiDAR SLAM solutions that are adaptive to LiDARs of different
specifications on various complex scenarios. To this end, we propose MULLS, an
efficient, low-drift, and versatile 3D LiDAR SLAM system. For the front-end,
roughly classified feature points (ground, facade, pillar, beam, etc.) are
extracted from each frame using dual-threshold ground filtering and principal
components analysis. Then the registration between the current frame and the
local submap is accomplished efficiently by the proposed multi-metric linear
least square iterative closest point algorithm. Point-to-point (plane, line)
error metrics within each point class are jointly optimized with a linear
approximation to estimate the ego-motion. Static feature points of the
registered frame are appended into the local map to keep it updated. For the
back-end, hierarchical pose graph optimization is conducted among regularly
stored history submaps to reduce the drift resulting from dead reckoning.
Extensive experiments are carried out on three datasets with more than 100,000
frames collected by seven types of LiDAR on various outdoor and indoor
scenarios. On the KITTI benchmark, MULLS ranks among the top LiDAR-only SLAM
systems with real-time performance.
</p>
<a href="http://arxiv.org/abs/2102.03771" target="_blank">arXiv:2102.03771</a> [<a href="http://arxiv.org/pdf/2102.03771" target="_blank">pdf</a>]

<h2>Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective. (arXiv:2102.11535v4 [cs.CV] UPDATED)</h2>
<h3>Wuyang Chen, Xinyu Gong, Zhangyang Wang</h3>
<p>Neural Architecture Search (NAS) has been explosively studied to automate the
discovery of top-performer neural networks. Current works require heavy
training of supernet or intensive architecture evaluations, thus suffering from
heavy resource consumption and often incurring search bias due to truncated
training or approximations. Can we select the best neural architectures without
involving any training and eliminate a drastic portion of the search cost? We
provide an affirmative answer, by proposing a novel framework called
training-free neural architecture search (TE-NAS). TE-NAS ranks architectures
by analyzing the spectrum of the neural tangent kernel (NTK) and the number of
linear regions in the input space. Both are motivated by recent theory advances
in deep networks and can be computed without any training and any label. We
show that: (1) these two measurements imply the trainability and expressivity
of a neural network; (2) they strongly correlate with the network's test
accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more
flexible and superior trade-off between the trainability and expressivity
during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes
high-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on
CIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in
bridging the theoretical findings of deep networks and practical impacts in
real NAS applications. Code is available at:
https://github.com/VITA-Group/TENAS.
</p>
<a href="http://arxiv.org/abs/2102.11535" target="_blank">arXiv:2102.11535</a> [<a href="http://arxiv.org/pdf/2102.11535" target="_blank">pdf</a>]

<h2>Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation. (arXiv:2103.04717v2 [cs.CV] UPDATED)</h2>
<h3>Jianzhong He, Xu Jia, Shuaijun Chen, Jianzhuang Liu</h3>
<p>Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models
trained on multiple labeled source domains to an unlabeled target domain. In
this paper, we propose a novel multi-source domain adaptation framework based
on collaborative learning for semantic segmentation. Firstly, a simple image
translation method is introduced to align the pixel value distribution to
reduce the gap between source domains and target domain to some extent. Then,
to fully exploit the essential semantic information across source domains, we
propose a collaborative learning method for domain adaptation without seeing
any data from target domain. In addition, similar to the setting of
unsupervised domain adaptation, unlabeled target domain data is leveraged to
further improve the performance of domain adaptation. This is achieved by
additionally constraining the outputs of multiple adaptation models with pseudo
labels online generated by an ensembled model. Extensive experiments and
ablation studies are conducted on the widely-used domain adaptation benchmark
datasets in semantic segmentation. Our proposed method achieves 59.0\% mIoU on
the validation set of Cityscapes by training on the labeled Synscapes and GTA5
datasets and unlabeled training set of Cityscapes. It significantly outperforms
all previous state-of-the-arts single-source and multi-source unsupervised
domain adaptation methods.
</p>
<a href="http://arxiv.org/abs/2103.04717" target="_blank">arXiv:2103.04717</a> [<a href="http://arxiv.org/pdf/2103.04717" target="_blank">pdf</a>]

<h2>Cybersecurity in Robotics: Challenges, Quantitative Modeling, and Practice. (arXiv:2103.05789v2 [cs.RO] UPDATED)</h2>
<h3>Quanyan Zhu, Stefan Rass, Bernhard Dieber, Victor Mayoral Vilches</h3>
<p>Robotics is becoming more and more ubiquitous, but the pressure to bring
systems to market occasionally goes at the cost of neglecting security
mechanisms during the development, deployment or while in production. As a
result, contemporary robotic systems are vulnerable to diverse attack patterns,
and an a posteriori hardening is at least challenging, if not impossible at
all. This book aims to stipulate the inclusion of security in robotics from the
earliest design phases onward and with a special focus on the cost-benefit
tradeoff that can otherwise be an inhibitor for the fast development of
affordable systems. We advocate quantitative methods of security management and
design, covering vulnerability scoring systems tailored to robotic systems, and
accounting for the highly distributed nature of robots as an interplay of
potentially very many components. A powerful quantitative approach to
model-based security is offered by game theory, providing a rich spectrum of
techniques to optimize security against various kinds of attacks. Such a
multi-perspective view on security is necessary to address the heterogeneity
and complexity of robotic systems. This book is intended as an accessible
starter for the theoretician and practitioner working in the field.
</p>
<a href="http://arxiv.org/abs/2103.05789" target="_blank">arXiv:2103.05789</a> [<a href="http://arxiv.org/pdf/2103.05789" target="_blank">pdf</a>]

<h2>Robust Collision-free Lightweight Aerial Autonomy for Unknown Area Exploration. (arXiv:2103.05798v2 [cs.RO] UPDATED)</h2>
<h3>Sunggoo Jung, Hanseob Lee, David Hyunchul Shim, Ali-akbar Agha-mohammadi</h3>
<p>Collision-free path planning is an essential requirement for autonomous
exploration in unknown environments, especially when operating in confined
spaces or near obstacles. This study presents an autonomous exploration
technique using a small drone. A local end-point selection method is designed
using LiDAR range measurement and then generates the path from the current
position to the selected end-point. The generated path shows the consistent
collision-free path in real-time by adopting the Euclidean signed distance
field-based grid-search method. The simulation results consistently showed the
safety, and reliability of the proposed path-planning method. Real-world
experiments are conducted in three different mines, demonstrating successful
autonomous exploration flight in environments with various structural
conditions. The results showed the high capability of the proposed flight
autonomy framework for lightweight aerial-robot systems. Besides, our drone
performs an autonomous mission during our entry at the Tunnel Circuit
competition (Phase 1) of the DARPA Subterranean Challenge.
</p>
<a href="http://arxiv.org/abs/2103.05798" target="_blank">arXiv:2103.05798</a> [<a href="http://arxiv.org/pdf/2103.05798" target="_blank">pdf</a>]

<h2>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v3 [cs.CV] UPDATED)</h2>
<h3>Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, Ji-Rong Wen</h3>
<p>Multi-modal pre-training models have been intensively explored to bridge
vision and language in recent years. However, most of them explicitly model the
cross-modal interaction between image-text pairs, by assuming that there exists
strong semantic correlation between the text and image modalities. Since this
strong assumption is often invalid in real-world scenarios, we choose to
implicitly model the cross-modal correlation for large-scale multi-modal
pre-training, which is the focus of the Chinese project `WenLan' led by our
team. Specifically, with the weak correlation assumption over image-text pairs,
we propose a two-tower pre-training model called BriVL within the cross-modal
contrastive learning framework. Unlike OpenAI CLIP that adopts a simple
contrastive learning method, we devise a more advanced algorithm by adapting
the latest method MoCo into the cross-modal scenario. By building a large
queue-based dictionary, our BriVL can incorporate more negative samples in
limited GPU resources. We further construct a large Chinese multi-source
image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.
Extensive experiments demonstrate that the pre-trained BriVL model outperforms
both UNITER and OpenAI CLIP on various downstream tasks.
</p>
<a href="http://arxiv.org/abs/2103.06561" target="_blank">arXiv:2103.06561</a> [<a href="http://arxiv.org/pdf/2103.06561" target="_blank">pdf</a>]

<h2>The Location of Optimal Object Colors with More Than Two Transitions. (arXiv:2103.06997v2 [cs.CV] UPDATED)</h2>
<h3>Scott A. Burns</h3>
<p>The chromaticity diagram associated with the CIE 1931 color matching
functions is shown to be slightly non-convex. While having no impact on
practical colorimetric computations, the non-convexity does have a significant
impact on the shape of some optimal object color reflectance distributions
associated with the outer surface of the object color solid. Instead of the
usual two-transition Schr\"odinger form, many optimal colors exhibit higher
transition counts. A linear programming formulation is developed and is used to
locate where these higher-transition optimal object colors reside on the object
color solid surface.
</p>
<a href="http://arxiv.org/abs/2103.06997" target="_blank">arXiv:2103.06997</a> [<a href="http://arxiv.org/pdf/2103.06997" target="_blank">pdf</a>]

<h2>Thousand to One: Semantic Prior Modeling for Conceptual Coding. (arXiv:2103.07131v2 [cs.CV] UPDATED)</h2>
<h3>Jianhui Chang, Zhenghui Zhao, Lingbo Yang, Chuanmin Jia, Jian Zhang, Siwei Ma</h3>
<p>Conceptual coding has been an emerging research topic recently, which encodes
natural images into disentangled conceptual representations for compression.
However, the compression performance of the existing methods is still
sub-optimal due to the lack of comprehensive consideration of rate constraint
and reconstruction quality. To this end, we propose a novel end-to-end semantic
prior modeling-based conceptual coding scheme towards extremely low bitrate
image compression, which leverages semantic-wise deep representations as a
unified prior for entropy estimation and texture synthesis. Specifically, we
employ semantic segmentation maps as structural guidance for extracting deep
semantic prior, which provides fine-grained texture distribution modeling for
better detail construction and higher flexibility in subsequent high-level
vision tasks. Moreover, a cross-channel entropy model is proposed to further
exploit the inter-channel correlation of the spatially independent semantic
prior, leading to more accurate entropy estimation for rate-constrained
training. The proposed scheme achieves an ultra-high 1000x compression ratio,
while still enjoying high visual reconstruction quality and versatility towards
visual processing and analysis tasks.
</p>
<a href="http://arxiv.org/abs/2103.07131" target="_blank">arXiv:2103.07131</a> [<a href="http://arxiv.org/pdf/2103.07131" target="_blank">pdf</a>]

<h2>Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images. (arXiv:2103.07894v2 [cs.CV] UPDATED)</h2>
<h3>Haolin Liu, Anran Lin, Xiaoguang Han, Lei Yang, Yizhou Yu, Shuguang Cui</h3>
<p>Grounding referring expressions in RGBD image has been an emerging field. We
present a novel task of 3D visual grounding in single-view RGBD image where the
referred objects are often only partially scanned due to occlusion. In contrast
to previous works that directly generate object proposals for grounding in the
3D scenes, we propose a bottom-up approach to gradually aggregate context-aware
information, effectively addressing the challenge posed by the partial
geometry. Our approach first fuses the language and the visual features at the
bottom level to generate a heatmap that coarsely localizes the relevant regions
in the RGBD image. Then our approach conducts an adaptive feature learning
based on the heatmap and performs the object-level matching with another
visio-linguistic fusion to finally ground the referred object. We evaluate the
proposed method by comparing to the state-of-the-art methods on both the RGBD
images extracted from the ScanRefer dataset and our newly collected SUNRefer
dataset. Experiments show that our method outperforms the previous methods by a
large margin (by 11.2% and 15.6% Acc@0.5) on both datasets.
</p>
<a href="http://arxiv.org/abs/2103.07894" target="_blank">arXiv:2103.07894</a> [<a href="http://arxiv.org/pdf/2103.07894" target="_blank">pdf</a>]

<h2>A Normal Distribution Transform-Based Radar Odometry Designed For Scanning and Automotive Radars. (arXiv:2103.07908v2 [cs.RO] UPDATED)</h2>
<h3>Pou-Chun Kung, Chieh-Chih Wang, Wen-Chieh Lin</h3>
<p>Existing radar sensors can be classified into automotive and scanning radars.
While most radar odometry (RO) methods are only designed for a specific type of
radar, our RO method adapts to both scanning and automotive radars. Our RO is
simple yet effective, where the pipeline consists of thresholding,
probabilistic submap building, and an NDT-based radar scan matching. The
proposed RO has been tested on two public radar datasets: the Oxford Radar
RobotCar dataset and the nuScenes dataset, which provide scanning and
automotive radar data respectively. The results show that our approach
surpasses state-of-the-art RO using either automotive or scanning radar by
reducing translational error by 51% and 30%, respectively, and rotational error
by 17% and 29%, respectively. Besides, we show that our RO achieves
centimeter-level accuracy as lidar odometry, and automotive and scanning RO
have similar accuracy.
</p>
<a href="http://arxiv.org/abs/2103.07908" target="_blank">arXiv:2103.07908</a> [<a href="http://arxiv.org/pdf/2103.07908" target="_blank">pdf</a>]

<h2>Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion. (arXiv:2103.07941v2 [cs.CV] UPDATED)</h2>
<h3>Ho Kei Cheng, Yu-Wing Tai, Chi-Keung Tang</h3>
<p>We present Modular interactive VOS (MiVOS) framework which decouples
interaction-to-mask and mask propagation, allowing for higher generalizability
and better performance. Trained separately, the interaction module converts
user interactions to an object mask, which is then temporally propagated by our
propagation module using a novel top-$k$ filtering strategy in reading the
space-time memory. To effectively take the user's intent into account, a novel
difference-aware module is proposed to learn how to properly fuse the masks
before and after each interaction, which are aligned with the target frames by
employing the space-time memory. We evaluate our method both qualitatively and
quantitatively with different forms of user interactions (e.g., scribbles,
clicks) on DAVIS to show that our method outperforms current state-of-the-art
algorithms while requiring fewer frame interactions, with the additional
advantage in generalizing to different types of user interactions. We
contribute a large-scale synthetic VOS dataset with pixel-accurate segmentation
of 4.8M frames to accompany our source codes to facilitate future research.
</p>
<a href="http://arxiv.org/abs/2103.07941" target="_blank">arXiv:2103.07941</a> [<a href="http://arxiv.org/pdf/2103.07941" target="_blank">pdf</a>]

<h2>TransFG: A Transformer Architecture for Fine-grained Recognition. (arXiv:2103.07976v2 [cs.CV] UPDATED)</h2>
<h3>Ju He, Jieneng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang, Alan Yuille</h3>
<p>Fine-grained visual classification (FGVC) which aims at recognizing objects
from subcategories is a very challenging task due to the inherently subtle
inter-class differences. Recent works mainly tackle this problem by focusing on
how to locate the most discriminative image regions and rely on them to improve
the capability of networks to capture subtle variances. Most of these works
achieve this by re-using the backbone network to extract features of selected
regions. However, this strategy inevitably complicates the pipeline and pushes
the proposed regions to contain most parts of the objects. Recently, vision
transformer (ViT) shows its strong performance in the traditional
classification task. The self-attention mechanism of the transformer links
every patch token to the classification token. The strength of the attention
link can be intuitively considered as an indicator of the importance of tokens.
In this work, we propose a novel transformer-based framework TransFG where we
integrate all raw attention weights of the transformer into an attention map
for guiding the network to effectively and accurately select discriminative
image patches and compute their relations. A contrastive loss is applied to
further enlarge the distance between feature representations of similar
sub-classes. We demonstrate the value of TransFG by conducting experiments on
five popular fine-grained benchmarks: CUB-200-2011, Stanford Cars, Stanford
Dogs, NABirds and iNat2017 where we achieve state-of-the-art performance.
Qualitative results are presented for better understanding of our model.
</p>
<a href="http://arxiv.org/abs/2103.07976" target="_blank">arXiv:2103.07976</a> [<a href="http://arxiv.org/pdf/2103.07976" target="_blank">pdf</a>]

<h2>Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging. (arXiv:2103.08292v2 [cs.CV] UPDATED)</h2>
<h3>&#xc1;lvaro Parra, Shin-Fang Chng, Tat-Jun Chin, Anders Eriksson, Ian Reid</h3>
<p>Under mild conditions on the noise level of the measurements, rotation
averaging satisfies strong duality, which enables global solutions to be
obtained via semidefinite programming (SDP) relaxation. However, generic
solvers for SDP are rather slow in practice, even on rotation averaging
instances of moderate size, thus developing specialised algorithms is vital. In
this paper, we present a fast algorithm that achieves global optimality called
rotation coordinate descent (RCD). Unlike block coordinate descent (BCD) which
solves SDP by updating the semidefinite matrix in a row-by-row fashion, RCD
directly maintains and updates all valid rotations throughout the iterations.
This obviates the need to store a large dense semidefinite matrix. We
mathematically prove the convergence of our algorithm and empirically show its
superior efficiency over state-of-the-art global methods on a variety of
problem configurations. Maintaining valid rotations also facilitates
incorporating local optimisation routines for further speed-ups. Moreover, our
algorithm is simple to implement; see supplementary material for a
demonstration program.
</p>
<a href="http://arxiv.org/abs/2103.08292" target="_blank">arXiv:2103.08292</a> [<a href="http://arxiv.org/pdf/2103.08292" target="_blank">pdf</a>]

<h2>Dynamic Emotion Modeling with Learnable Graphs and Graph Inception Network. (arXiv:2008.02661v2 [cs.CV] CROSS LISTED)</h2>
<h3>A. Shirian, S. Tripathi, T. Guha</h3>
<p>Human emotion is expressed, perceived and captured using a variety of dynamic
data modalities, such as speech (verbal), videos (facial expressions) and
motion sensors (body gestures). We propose a generalized approach to emotion
recognition that can adapt across modalities by modeling dynamic data as
structured graphs. The motivation behind the graph approach is to build compact
models without compromising on performance. To alleviate the problem of optimal
graph construction, we cast this as a joint graph learning and classification
task. To this end, we present the Learnable Graph Inception Network (L-GrIN)
that jointly learns to recognize emotion and to identify the underlying graph
structure in the dynamic data. Our architecture comprises multiple novel
components: a new graph convolution operation, a graph inception layer,
learnable adjacency, and a learnable pooling function that yields a graph-level
embedding. We evaluate the proposed architecture on five benchmark emotion
recognition databases spanning three different modalities (video, audio, motion
capture), where each database captures one of the following emotional cues:
facial expressions, speech and body gestures. We achieve state-of-the-art
performance on all five databases outperforming several competitive baselines
and relevant existing methods. Our graph architecture shows superior
performance with significantly fewer parameters (compared to convolutional or
recurrent neural networks) promising its applicability to resource-constrained
devices.
</p>
<a href="http://arxiv.org/abs/2008.02661" target="_blank">arXiv:2008.02661</a> [<a href="http://arxiv.org/pdf/2008.02661" target="_blank">pdf</a>]

