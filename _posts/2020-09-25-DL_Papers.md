---
title: Latest Deep Learning Papers
date: 2020-09-30 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>Finite-Time Analysis for Double Q-learning. (arXiv:2009.14257v1 [cs.LG])</h2>
<h3>Huaqing Xiong, Lin Zhao, Yingbin Liang, Wei Zhang</h3>
<p>Although Q-learning is one of the most successful algorithms for finding the
best action-value function (and thus the optimal policy) in reinforcement
learning, its implementation often suffers from large overestimation of
Q-function values incurred by random sampling. The double Q-learning algorithm
proposed in~\citet{hasselt2010double} overcomes such an overestimation issue by
randomly switching the update between two Q-estimators, and has thus gained
significant popularity in practice. However, the theoretical understanding of
double Q-learning is rather limited. So far only the asymptotic convergence has
been established, which does not characterize how fast the algorithm converges.
In this paper, we provide the first non-asymptotic (i.e., finite-time) analysis
for double Q-learning. We show that both synchronous and asynchronous double
Q-learning are guaranteed to converge to an $\epsilon$-accurate neighborhood of
the global optimum by taking $\tilde{\Omega}\left(\left(
\frac{1}{(1-\gamma)^6\epsilon^2}\right)^{\frac{1}{\omega}}
+\left(\frac{1}{1-\gamma}\right)^{\frac{1}{1-\omega}}\right)$ iterations, where
$\omega\in(0,1)$ is the decay parameter of the learning rate, and $\gamma$ is
the discount factor. Our analysis develops novel techniques to derive
finite-time bounds on the difference between two inter-connected stochastic
processes, which is new to the literature of stochastic approximation.
</p>
<a href="http://arxiv.org/abs/2009.14257">arXiv:2009.14257</a> [<a href="http://arxiv.org/pdf/2009.14257">pdf</a>]

<h2>Benign overfitting in ridge regression. (arXiv:2009.14286v1 [math.ST])</h2>
<h3>A. Tsigler (1), P. L. Bartlett (1) ((1) UC Berkeley)</h3>
<p>Classical learning theory suggests that strong regularization is needed to
learn a class with large complexity. This intuition is in contrast with the
modern practice of machine learning, in particular learning neural networks,
where the number of parameters often exceeds the number of data points. It has
been observed empirically that such overparametrized models can show good
generalization performance even if trained with vanishing or negative
regularization. The aim of this work is to understand theoretically how this
effect can occur, by studying the setting of ridge regression. We provide
non-asymptotic generalization bounds for overparametrized ridge regression that
depend on the arbitrary covariance structure of the data, and show that those
bounds are tight for a range of regularization parameter values. To our
knowledge this is the first work that studies overparametrized ridge regression
in such a general setting. We identify when small or negative regularization is
sufficient for obtaining small generalization error. On the technical side, our
bounds only require the data vectors to be i.i.d. sub-gaussian, while most
previous work assumes independence of the components of those vectors.
</p>
<a href="http://arxiv.org/abs/2009.14286">arXiv:2009.14286</a> [<a href="http://arxiv.org/pdf/2009.14286">pdf</a>]

<h2>Secure Aggregation with Heterogeneous Quantization in Federated Learning. (arXiv:2009.14388v1 [cs.IT])</h2>
<h3>Ahmed Roushdy Elkordy, A. Salman Avestimehr</h3>
<p>Secure model aggregation across many users is a key component of federated
learning systems. The state-of-the-art protocols for secure model aggregation,
which are based on additive masking, require all users to quantize their model
updates to the same level of quantization. This severely degrades their
performance due to lack of adaptation to available bandwidth at different
users. We propose three schemes that allow secure model aggregation while using
heterogeneous quantization. This enables the users to adjust their quantization
proportional to their available bandwidth, which can provide a substantially
better trade-off between the accuracy of training and the communication time.
The proposed schemes are based on a grouping strategy by partitioning the
network into groups, and partitioning the local model updates of users into
segments. Instead of applying aggregation protocol to the entire local model
update vector, it is applied on segments with specific coordination between
users. We theoretically evaluate the quantization error for our schemes, and
also demonstrate how our schemes can be utilized to overcome Byzantine users.
</p>
<a href="http://arxiv.org/abs/2009.14388">arXiv:2009.14388</a> [<a href="http://arxiv.org/pdf/2009.14388">pdf</a>]

<h2>Learning to Beamform for Intelligent Reflecting Surface with Implicit Channel Estimate. (arXiv:2009.14404v1 [eess.SP])</h2>
<h3>Tao Jiang, Hei Victor Cheng, Wei Yu</h3>
<p>Intelligent reflecting surface (IRS), consisting of massive number of tunable
reflective elements, is capable of boosting spectral efficiency between a base
station (BS) and a user by intelligently tuning the phase shifters at the IRS
according to the channel state information (CSI). However, due to the large
number of passive elements which cannot transmit and receive signals,
acquisition of CSI for IRS is a practically challenging task. Instead of using
the received pilots to estimate the channels explicitly, this paper shows that
it is possible to learn the effective IRS reflection pattern and beamforming at
the BS directly based on the received pilots. This is achieved by
parameterizing the mapping from the received pilots to the optimal
configuration of IRS and the beamforming matrix at the BS by properly tuning a
deep neural network using unsupervised training. Simulation results indicate
that the proposed neural network can efficiently learn to maximize the system
sum rate from much fewer received pilots as compared to the traditional channel
estimation based solutions.
</p>
<a href="http://arxiv.org/abs/2009.14404">arXiv:2009.14404</a> [<a href="http://arxiv.org/pdf/2009.14404">pdf</a>]

<h2>Accelerating Optimization and Reinforcement Learning with Quasi-Stochastic Approximation. (arXiv:2009.14431v1 [math.OC])</h2>
<h3>Shuhang Chen, Adithya Devraj, Andrey Bernstein, Sean Meyn</h3>
<p>The ODE method has been a workhorse for algorithm design and analysis since
the introduction of the stochastic approximation. It is now understood that
convergence theory amounts to establishing robustness of Euler approximations
for ODEs, while theory of rates of convergence requires finer probabilistic
analysis. This paper sets out to extend this theory to quasi-stochastic
approximation, based on algorithms in which the "noise" is based on
deterministic signals. The main results are obtained under minimal assumptions:
the usual Lipschitz conditions for ODE vector fields, and it is assumed that
there is a well defined linearization near the optimal parameter $\theta^*$,
with Hurwitz linearization matrix.

The main contributions are as follows:

(i) If the algorithm gain is $a_t=g/(1+t)^\rho$ with $g&gt;0$ and
$\rho\in(0,1)$, then the rate of convergence of the algorithm is $1/t^\rho$.
There is also a well defined "finite-$t$" approximation: \[
a_t^{-1}\{\ODEstate_t-\theta^*\}=\bar{Y}+\XiI_t+o(1) \] where $\bar{Y}\in\Re^d$
is a vector identified in the paper, and $\{\XiI_t\}$ is bounded with zero
temporal mean.

(ii) With gain $a_t = g/(1+t)$ the results are not as sharp: the rate of
convergence $1/t$ holds only if $I + g A^*$ is Hurwitz.

(iii) Based on the Ruppert-Polyak averaging technique of stochastic
approximation, one would expect that a convergence rate of $1/t$ can be
obtained by averaging: \[ \ODEstate^{\text{RP}}_T=\frac{1}{T}\int_{0}^T
\ODEstate_t\,dt \] where the estimates $\{\ODEstate_t\}$ are obtained using the
gain in (i). The preceding sharp bounds imply that averaging results in $1/t$
convergence rate if and only if $\bar{Y}=\Zero$. This condition holds if the
noise is additive, but appears to fail in general.

(iv) The theory is illustrated with applications to gradient-free
optimization and policy gradient algorithms for reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2009.14431">arXiv:2009.14431</a> [<a href="http://arxiv.org/pdf/2009.14431">pdf</a>]

<h2>Impact and Calibration of Nonlinear Reciprocity Mismatch in Massive MIMO. (arXiv:2009.14481v1 [cs.IT])</h2>
<h3>Rongjiang Nie, Li Chen, Nan Zhao, Yunfei Chen, Weidong Wang, Xianbin Wang</h3>
<p>Time-division-duplexing massive multiple-input multiple-output (MIMO) systems
estimate the channel state information (CSI) by leveraging the uplink-downlink
channel reciprocity, which is no longer valid when the mismatch arises from the
asymmetric uplink and downlink radio frequency (RF) chains. Existing works
treat the reciprocity mismatch as constant for simplicity. However, the
practical RF chain consists of nonlinear components, which leads to nonlinear
reciprocity mismatch. In this work, we examine the impact and the calibration
approach of the nonlinear reciprocity mismatch in massive MIMO systems. To
evaluate the impact of the nonlinear mismatch, we first derive the closed-form
expression of the ergodic achievable rate. Then, we analyze the performance
loss caused by the nonlinear mismatch to show that the impact of the mismatch
at the base station (BS) side is much larger than that at the user equipment
side. Therefore, we propose a calibration method for the BS. During the
calibration, polynomial function is applied to approximate the nonlinear
mismatch factor, and over-the-air training is employed to estimate the
polynomial coefficients. After that, the calibration coefficients are computed
by maximizing the downlink achievable rate. Simulation results are presented to
verify the analytical results and to show the performance of the proposed
calibration approach.
</p>
<a href="http://arxiv.org/abs/2009.14481">arXiv:2009.14481</a> [<a href="http://arxiv.org/pdf/2009.14481">pdf</a>]

<h2>Wasserstein Distributionally Robust Inverse Multiobjective Optimization. (arXiv:2009.14552v1 [math.OC])</h2>
<h3>Chaosheng Dong, Bo Zeng</h3>
<p>Inverse multiobjective optimization provides a general framework for the
unsupervised learning task of inferring parameters of a multiobjective decision
making problem (DMP), based on a set of observed decisions from the human
expert. However, the performance of this framework relies critically on the
availability of an accurate DMP, sufficient decisions of high quality, and a
parameter space that contains enough information about the DMP. To hedge
against the uncertainties in the hypothetical DMP, the data, and the parameter
space, we investigate in this paper the distributionally robust approach for
inverse multiobjective optimization. Specifically, we leverage the Wasserstein
metric to construct a ball centered at the empirical distribution of these
decisions. We then formulate a Wasserstein distributionally robust inverse
multiobjective optimization problem (WRO-IMOP) that minimizes a worst-case
expected loss function, where the worst case is taken over all distributions in
the Wasserstein ball. We show that the excess risk of the WRO-IMOP estimator
has a sub-linear convergence rate. Furthermore, we propose the semi-infinite
reformulations of the WRO-IMOP and develop a cutting-plane algorithm that
converges to an approximate solution in finite iterations. Finally, we
demonstrate the effectiveness of our method on both a synthetic multiobjective
quadratic program and a real world portfolio optimization problem.
</p>
<a href="http://arxiv.org/abs/2009.14552">arXiv:2009.14552</a> [<a href="http://arxiv.org/pdf/2009.14552">pdf</a>]

<h2>First-order Optimization for Superquantile-based Supervised Learning. (arXiv:2009.14575v1 [math.OC])</h2>
<h3>Yassine Laguel, J&#xe9;r&#xf4;me Malick, Zaid Harchaoui</h3>
<p>Classical supervised learning via empirical risk (or negative log-likelihood)
minimization hinges upon the assumption that the testing distribution coincides
with the training distribution. This assumption can be challenged in modern
applications of machine learning in which learning machines may operate at
prediction time with testing data whose distribution departs from the one of
the training data. We revisit the superquantile regression method by proposing
a first-order optimization algorithm to minimize a superquantile-based learning
objective. The proposed algorithm is based on smoothing the superquantile
function by infimal convolution. Promising numerical results illustrate the
interest of the approach towards safer supervised learning.
</p>
<a href="http://arxiv.org/abs/2009.14575">arXiv:2009.14575</a> [<a href="http://arxiv.org/pdf/2009.14575">pdf</a>]

<h2>Machine Learning and Computational Mathematics. (arXiv:2009.14596v1 [math.NA])</h2>
<h3>Weinan E</h3>
<p>Neural network-based machine learning is capable of approximating functions
in very high dimension with unprecedented efficiency and accuracy. This has
opened up many exciting new possibilities, not just in traditional areas of
artificial intelligence, but also in scientific computing and computational
science. At the same time, machine learning has also acquired the reputation of
being a set of "black box" type of tricks, without fundamental principles. This
has been a real obstacle for making further progress in machine learning. In
this article, we try to address the following two very important questions: (1)
How machine learning has already impacted and will further impact computational
mathematics, scientific computing and computational science? (2) How
computational mathematics, particularly numerical analysis, {can} impact
machine learning? We describe some of the most important progress that has been
made on these issues. Our hope is to put things into a perspective that will
help to integrate machine learning with computational mathematics.
</p>
<a href="http://arxiv.org/abs/2009.14596">arXiv:2009.14596</a> [<a href="http://arxiv.org/pdf/2009.14596">pdf</a>]

<h2>A DNN-based algorithm for multi-scale elliptic problems. (arXiv:2009.14597v1 [physics.comp-ph])</h2>
<h3>Xi-An Li, Zhi-Qin John Xu, Lei Zhang</h3>
<p>Algorithms based on deep neural networks (DNNs) have attracted increasing
attention from the scientific computing community. DNN based algorithms are
easy to implement, natural for nonlinear problems, and have shown great
potential to overcome the curse of dimensionality. In this work, we utilize the
multi-scale DNN-based algorithm (MscaleDNN) proposed by Liu, Cai and Xu (2020)
to solve multi-scale elliptic problems with possible nonlinearity, for example,
the p-Laplacian problem. We improve the MscaleDNN algorithm by a smooth and
localized activation function. Several numerical examples of multi-scale
elliptic problems with separable or non-separable scales in low-dimensional and
high-dimensional Euclidean spaces are used to demonstrate the effectiveness and
accuracy of the MscaleDNN numerical scheme.
</p>
<a href="http://arxiv.org/abs/2009.14597">arXiv:2009.14597</a> [<a href="http://arxiv.org/pdf/2009.14597">pdf</a>]

<h2>A group-theorist's perspective on symmetry groups in physics. (arXiv:2009.14613v1 [math.GR])</h2>
<h3>Robert Arnott Wilson</h3>
<p>There are many Lie groups used in physics, including the Lorentz group of
special relativity, the spin groups (relativistic and non-relativistic) and the
gauge groups of quantum electrodynamics and the weak and strong nuclear forces.
Various grand unified theories use larger Lie groups in different attempts to
unify some of these groups into something more fundamental. There are also a
number of finite symmetry groups that are related to the finite number of
distinct elementary particle types. I offer a group-theorist's perspective on
these groups, and suggest some ways in which a deeper use of group theory might
in principle be useful. These suggestions include a number of options that seem
not to be under active investigation at present. I leave open the question of
whether they can be implemented in physical theories.
</p>
<a href="http://arxiv.org/abs/2009.14613">arXiv:2009.14613</a> [<a href="http://arxiv.org/pdf/2009.14613">pdf</a>]

<h2>On the ubiquity of the ruler sequence. (arXiv:2009.14629v1 [math.HO])</h2>
<h3>Juan Carlos Nu&#xf1;o, Francisco J. Mu&#xf1;oz</h3>
<p>The ruler function or the Gros sequence is a classical infinite integer
sequence that is underlying some interesting mathematical problems. In this
paper, we provide four new problems containing this type of sequence: (i) a
demographic discrete dynamical automata, (ii) the middle interval Cantor set,
(iii) the construction by duplication of polygons and (iv) the horizontal
visibility sequence at the accumulation point of the Feigenbaum cascade. In all
of them, the infinte sequence is obtained by a recursive procedure of
duplication. The properties of the ruler sequence, in particular, those
relating to recursiveness and self-containing, are used to get a deeper
understanding of these four problems. These new representations of the ruler
sequence could inspire new studies in the field of discrete mathematics.
</p>
<a href="http://arxiv.org/abs/2009.14629">arXiv:2009.14629</a> [<a href="http://arxiv.org/pdf/2009.14629">pdf</a>]

<h2>Scalable Deep Reinforcement Learning for Ride-Hailing. (arXiv:2009.14679v1 [math.OC])</h2>
<h3>Jiekun Feng, Mark Gluzman, J. G. Dai</h3>
<p>Ride-hailing services, such as Didi Chuxing, Lyft, and Uber, arrange
thousands of cars to meet ride requests throughout the day. We consider a
Markov decision process (MDP) model of a ride-hailing service system, framing
it as a reinforcement learning (RL) problem. The simultaneous control of many
agents (cars) presents a challenge for the MDP optimization because the action
space grows exponentially with the number of cars. We propose a special
decomposition for the MDP actions by sequentially assigning tasks to the
drivers. The new actions structure resolves the scalability problem and enables
the use of deep RL algorithms for control policy optimization. We demonstrate
the benefit of our proposed decomposition with a numerical experiment based on
real data from Didi Chuxing.
</p>
<a href="http://arxiv.org/abs/2009.14679">arXiv:2009.14679</a> [<a href="http://arxiv.org/pdf/2009.14679">pdf</a>]

<h2>Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models. (arXiv:1806.04823v5 [math.ST] UPDATED)</h2>
<h3>Denis Nekipelov, Vira Semenova, Vasilis Syrgkanis</h3>
<p>This paper contributes to the literature on high-dimensional sparse
M-estimation by allowing the loss function to depend on a functional nuisance
parameter, which we estimate by modern machine learning tools. For a class of
single-index conditional moment restrictions (CMRs), we explicitly derive the
loss function. We first adjust the moment function so that the gradient of the
future M-estimator loss is insensitive (formally, Neyman-orthogonal) with
respect to the first-stage regularization bias. We then take the loss function
to be an indefinite integral of the adjusted moment function with respect to
the single-index. The proposed l1-regularized M-estimator achieves the oracle
convergence rate, where the oracle knows the nuisance parameter and solves only
the parametric problem. Our framework nests a novel approach to modeling
heterogeneous treatment effects with a binary dependent variable. In addition,
we apply our results to conditional moment models with missing data and static
games of incomplete information. Finally, we generalize our results to generic
extremum estimation with a nuisance component.
</p>
<a href="http://arxiv.org/abs/1806.04823">arXiv:1806.04823</a> [<a href="http://arxiv.org/pdf/1806.04823">pdf</a>]

<h2>The Right Complexity Measure in Locally Private Estimation: It is not the Fisher Information. (arXiv:1806.05756v3 [math.ST] UPDATED)</h2>
<h3>John C. Duchi, Feng Ruan</h3>
<p>We identify fundamental tradeoffs between statistical utility and privacy
under local models of privacy in which data is kept private even from the
statistician, providing instance-specific bounds for private estimation and
learning problems by developing the \emph{local minimax risk}. In contrast to
approaches based on worst-case (minimax) error, which are conservative, this
allows us to evaluate the difficulty of individual problem instances and
delineate the possibilities for adaptation in private estimation and inference.
Our main results show that the local modulus of continuity of the estimand with
respect to the variation distance---as opposed to the Hellinger distance
central to classical statistics---characterizes rates of convergence under
locally private estimation for many notions of privacy, including differential
privacy and its relaxations. As consequences of these results, we identify an
alternative to the Fisher information for private estimation, giving a more
nuanced understanding of the challenges of adaptivity and optimality.
</p>
<a href="http://arxiv.org/abs/1806.05756">arXiv:1806.05756</a> [<a href="http://arxiv.org/pdf/1806.05756">pdf</a>]

<h2>Tune smarter not harder: A principled approach to tuning learning rates for shallow nets. (arXiv:2003.09844v3 [cs.LG] UPDATED)</h2>
<h3>Thulasi Tholeti, Sheetal Kalyani</h3>
<p>Effective hyper-parameter tuning is essential to guarantee the performance
that neural networks have come to be known for. In this work, a principled
approach to choosing the learning rate is proposed for shallow feedforward
neural networks. We associate the learning rate with the gradient Lipschitz
constant of the objective to be minimized while training. An upper bound on the
mentioned constant is derived and a search algorithm, which always results in
non-divergent traces, is proposed to exploit the derived bound. It is shown
through simulations that the proposed search method significantly outperforms
the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed
method is applied to three different existing applications: a) channel
estimation in OFDM systems, b) prediction of the exchange currency rates and c)
offset estimation in OFDM receivers, and it is shown to pick better learning
rates than the existing methods using the same or lesser compute power.
</p>
<a href="http://arxiv.org/abs/2003.09844">arXiv:2003.09844</a> [<a href="http://arxiv.org/pdf/2003.09844">pdf</a>]

<h2>Rational neural networks. (arXiv:2004.01902v2 [cs.NE] UPDATED)</h2>
<h3>Nicolas Boull&#xe9;, Yuji Nakatsukasa, Alex Townsend</h3>
<p>We consider neural networks with rational activation functions. The choice of
the nonlinear activation function in deep learning architectures is crucial and
heavily impacts the performance of a neural network. We establish optimal
bounds in terms of network complexity and prove that rational neural networks
approximate smooth functions more efficiently than ReLU networks with
exponentially smaller depth. The flexibility and smoothness of rational
activation functions make them an attractive alternative to ReLU, as we
demonstrate with numerical experiments.
</p>
<a href="http://arxiv.org/abs/2004.01902">arXiv:2004.01902</a> [<a href="http://arxiv.org/pdf/2004.01902">pdf</a>]

<h2>Scenario optimization with relaxation: a new tool for design and application to machine learning problems. (arXiv:2004.05839v2 [cs.LG] UPDATED)</h2>
<h3>Marco C. Campi, Simone Garatti</h3>
<p>Scenario optimization is by now a well established technique to perform
designs in the presence of uncertainty. It relies on domain knowledge
integrated with first-hand information that comes from data and generates
solutions that are also accompanied by precise statements of reliability. In
this paper, following recent developments in (Garatti and Campi, 2019), we
venture beyond the traditional set-up of scenario optimization by analyzing the
concept of constraints relaxation. By a solid theoretical underpinning, this
new paradigm furnishes fundamental tools to perform designs that meet a proper
compromise between robustness and performance. After suitably expanding the
scope of constraints relaxation as proposed in (Garatti and Campi, 2019), we
focus on various classical Support Vector methods in machine learning -
including SVM (Support Vector Machine), SVR (Support Vector Regression) and
SVDD (Support Vector Data Description) - and derive new results for the ability
of these methods to generalize.
</p>
<a href="http://arxiv.org/abs/2004.05839">arXiv:2004.05839</a> [<a href="http://arxiv.org/pdf/2004.05839">pdf</a>]

<h2>Variance reduction for Random Coordinate Descent-Langevin Monte Carlo. (arXiv:2006.06068v3 [stat.ML] UPDATED)</h2>
<h3>Zhiyan Ding, Qin Li</h3>
<p>Sampling from a log-concave distribution function is one core problem that
has wide applications in Bayesian statistics and machine learning. While most
gradient free methods have slow convergence rate, the Langevin Monte Carlo
(LMC) that provides fast convergence requires the computation of gradients. In
practice one uses finite-differencing approximations as surrogates, and the
method is expensive in high-dimensions.

A natural strategy to reduce computational cost in each iteration is to
utilize random gradient approximations, such as random coordinate descent (RCD)
or simultaneous perturbation stochastic approximation (SPSA). We show by a
counter-example that blindly applying RCD does not achieve the goal in the most
general setting. The high variance induced by the randomness means a larger
number of iterations are needed, and this balances out the saving in each
iteration.

We then introduce a new variance reduction approach, termed Randomized
Coordinates Averaging Descent (RCAD), and incorporate it with both overdamped
and underdamped LMC. The methods are termed RCAD-O-LMC and RCAD-U-LMC
respectively. The methods still sit in the random gradient approximation
framework, and thus the computational cost in each iteration is low. However,
by employing RCAD, the variance is reduced, so the methods converge within the
same number of iterations as the classical overdamped and underdamped LMC. This
leads to a computational saving overall.
</p>
<a href="http://arxiv.org/abs/2006.06068">arXiv:2006.06068</a> [<a href="http://arxiv.org/pdf/2006.06068">pdf</a>]

<h2>PAC-Bayes unleashed: generalisation bounds with unbounded losses. (arXiv:2006.07279v2 [stat.ML] UPDATED)</h2>
<h3>Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, John Shawe-Taylor</h3>
<p>We present new PAC-Bayesian generalisation bounds for learning problems with
unbounded loss functions. This extends the relevance and applicability of the
PAC-Bayes learning framework, where most of the existing literature focuses on
supervised learning problems with a bounded loss function (typically assumed to
take values in the interval [0;1]). In order to relax this assumption, we
propose a new notion called HYPE (standing for \emph{HYPothesis-dependent
rangE}), which effectively allows the range of the loss to depend on each
predictor. Based on this new notion we derive a novel PAC-Bayesian
generalisation bound for unbounded loss functions, and we instantiate it on a
linear regression problem. To make our theory usable by the largest audience
possible, we include discussions on actual computation, practicality and
limitations of our assumptions.
</p>
<a href="http://arxiv.org/abs/2006.07279">arXiv:2006.07279</a> [<a href="http://arxiv.org/pdf/2006.07279">pdf</a>]

<h2>Deep Network with Approximation Error Being Reciprocal of Width to Power of Square Root of Depth. (arXiv:2006.12231v3 [cs.LG] UPDATED)</h2>
<h3>Zuowei Shen, Haizhao Yang, Shijun Zhang</h3>
<p>A new network with super approximation power is introduced. This network is
built with Floor ($\lfloor x\rfloor$) or ReLU ($\max\{0,x\}$) activation
function in each neuron and hence we call such networks Floor-ReLU networks.
{For any hyper-parameters $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$,} it is
shown that Floor-ReLU networks with a width $\max\{d,\, 5N+13\}$ and a depth
$64dL+3$ can {uniformly approximate a H{\"o}lder function $f$ on $[0,1]^d$ with
an approximation rate $3\lambda d^{\alpha/2}N^{-\alpha\sqrt{L}}$, where $\alpha
\in(0,1]$ and $\lambda$ are the H{\"o}lder order and constant, respectively.}
More generally for an arbitrary continuous function $f$ on $[0,1]^d$ with a
modulus of continuity $\omega_f(\cdot)$, the constructive approximation rate is
$\omega_f(\sqrt{d}\,N^{-\sqrt{L}})+2\omega_f(\sqrt{d}){N^{-\sqrt{L}}}$. As a
consequence, this new {class of networks} overcome{s} the curse of
dimensionality in approximation power when the variation of $\omega_f(r)$ as
$r\rightarrow 0$ is moderate (e.g., $\omega_f(r){\lesssim} r^\alpha$ for
H{\"o}lder continuous functions), since the major term to be concerned in our
approximation rate is {essentially $\sqrt{d}$ times a function of $N$ and $L$
independent of $d$ within the modulus of continuity. }
</p>
<a href="http://arxiv.org/abs/2006.12231">arXiv:2006.12231</a> [<a href="http://arxiv.org/pdf/2006.12231">pdf</a>]

<h2>Automatic Differentiation to Simultaneously Identify Nonlinear Dynamics and Extract Noise Probability Distributions from Data. (arXiv:2009.08810v2 [eess.SP] UPDATED)</h2>
<h3>Kadierdan Kaheman, Steven L. Brunton, J. Nathan Kutz</h3>
<p>The sparse identification of nonlinear dynamics (SINDy) is a regression
framework for the discovery of parsimonious dynamic models and governing
equations from time-series data. As with all system identification methods,
noisy measurements compromise the accuracy and robustness of the model
discovery procedure. In this work, we develop a variant of the SINDy algorithm
that integrates automatic differentiation and recent time-stepping constrained
motivated by Rudy et al. for simultaneously (i) denoising the data, (ii)
learning and parametrizing the noise probability distribution, and (iii)
identifying the underlying parsimonious dynamical system responsible for
generating the time-series data. Thus within an integrated optimization
framework, noise can be separated from signal, resulting in an architecture
that is approximately twice as robust to noise as state-of-the-art methods,
handling as much as 40% noise on a given time-series signal and explicitly
parametrizing the noise probability distribution. We demonstrate this approach
on several numerical examples, from Lotka-Volterra models to the
spatio-temporal Lorenz 96 model. Further, we show the method can identify a
diversity of probability distributions including Gaussian, uniform, Gamma, and
Rayleigh.
</p>
<a href="http://arxiv.org/abs/2009.08810">arXiv:2009.08810</a> [<a href="http://arxiv.org/pdf/2009.08810">pdf</a>]

<h2>Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS. (arXiv:2009.10683v3 [cs.LG] UPDATED)</h2>
<h3>Lin Chen, Sheng Xu</h3>
<p>We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural
tangent kernel and the Laplace kernel include the same set of functions, when
both kernels are restricted to the sphere $\mathbb{S}^{d-1}$. Additionally, we
prove that the exponential power kernel with a smaller power (making the kernel
more non-smooth) leads to a larger RKHS, when it is restricted to the sphere
$\mathbb{S}^{d-1}$ and when it is defined on the entire $\mathbb{R}^d$.
</p>
<a href="http://arxiv.org/abs/2009.10683">arXiv:2009.10683</a> [<a href="http://arxiv.org/pdf/2009.10683">pdf</a>]

<h2>ParaMonte: A high-performance serial/parallel Monte Carlo simulation library for C, C++, Fortran. (arXiv:2009.14229v1 [cs.MS])</h2>
<h3>Amir Shahmoradi, Fatemeh Bagheri</h3>
<p>ParaMonte (standing for Parallel Monte Carlo) is a serial and
MPI/Coarray-parallelized library of Monte Carlo routines for sampling
mathematical objective functions of arbitrary-dimensions, in particular, the
posterior distributions of Bayesian models in data science, Machine Learning,
and scientific inference. The ParaMonte library has been developed with the
design goal of unifying the **automation**, **accessibility**,
**high-performance**, **scalability**, and **reproducibility** of Monte Carlo
simulations. The current implementation of the library includes **ParaDRAM**, a
**Para**llel **D**elyaed-**R**ejection **A**daptive **M**etropolis Markov Chain
Monte Carlo sampler, accessible from a wide range of programming languages
including C, C++, Fortran, with a unified Application Programming Interface and
simulation environment across all supported programming languages. The
ParaMonte library is MIT-licensed and is permanently located and maintained at
[https://github.com/cdslaborg/paramonte](https://github.com/cdslaborg/paramonte).
</p>
<a href="http://arxiv.org/abs/2009.14229">arXiv:2009.14229</a> [<a href="http://arxiv.org/pdf/2009.14229">pdf</a>]

<h2>Lip-reading with Densely Connected Temporal Convolutional Networks. (arXiv:2009.14233v1 [cs.CV])</h2>
<h3>Pingchuan Ma, Yujiang Wang, Jie Shen, Stavros Petridis, Maja Pantic</h3>
<p>In this work, we present the Densely Connected Temporal Convolutional Network
(DC-TCN) for lip-reading of isolated words. Although Temporal Convolutional
Networks (TCN) have recently demonstrated great potential in many vision tasks,
its receptive fields are not dense enough to model the complex temporal
dynamics in lip-reading scenarios. To address this problem, we introduce dense
connections into the network to capture more robust temporal features.
Moreover, our approach utilises the Squeeze-and-Excitation block, a
light-weight attention mechanism, to further enhance the model's classification
power. Without bells and whistles, our DC-TCN method has achieved 88.36%
accuracy on the Lip Reading in the Wild (LRW) dataset and 43.65% on the
LRW-1000 dataset, which has surpassed all the baseline methods and is the new
state-of-the-art on both datasets.
</p>
<a href="http://arxiv.org/abs/2009.14233">arXiv:2009.14233</a> [<a href="http://arxiv.org/pdf/2009.14233">pdf</a>]

<h2>Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols. (arXiv:2009.14237v1 [cs.HC])</h2>
<h3>Andrew Head (UC Berkeley), Kyle Lo (Allen Institute for AI), Dongyeop Kang (UC Berkeley), Raymond Fok (University of Washington), Sam Skjonsberg (Allen Institute for AI), Daniel S. Weld (Allen Institute for AI, University of Washington), Marti A. Hearst (UC Berkeley)</h3>
<p>Despite the central importance of research papers to scientific progress,
they can be difficult to read. Comprehension is often stymied when the
information needed to understand a passage resides somewhere else: in another
section, or in another paper. In this work, we envision how interfaces can
bring definitions of technical terms and symbols to readers when and where they
need them most. We introduce ScholarPhi, an augmented reading interface with
four novel features: (1) tooltips that surface position-sensitive definitions
from elsewhere in a paper, (2) a filter over the paper that "declutters" it to
reveal how the term or symbol is used across the paper, (3) automatic equation
diagrams that expose multiple definitions in parallel, and (4) an automatically
generated glossary of important terms and symbols. A usability study showed
that the tool helps researchers of all experience levels read papers.
Furthermore, researchers were eager to have ScholarPhi's definitions available
to support their everyday reading.
</p>
<a href="http://arxiv.org/abs/2009.14237">arXiv:2009.14237</a> [<a href="http://arxiv.org/pdf/2009.14237">pdf</a>]

<h2>Temporal State Machines: Using temporal memory to stitch time-based graph computations. (arXiv:2009.14243v1 [cs.ET])</h2>
<h3>Advait Madhavan, Matthew Daniels, Mark Stiles</h3>
<p>Race logic, an arrival-time-coded logic family, has demonstrated energy and
performance improvements for applications ranging from dynamic programming to
machine learning. However, the ad hoc mappings of algorithms into hardware
result in custom architectures making them difficult to generalize. We
systematize the development of race logic by associating it with the
mathematical field called tropical algebra. This association between the
mathematical primitives of tropical algebra and generalized race logic
computations guides the design of temporally coded tropical circuits. It also
serves as a framework for expressing high level timing-based algorithms. This
abstraction, when combined with temporal memory, allows for the systematic
generalization of race logic by making it possible to partition feed-forward
computations into stages and organizing them into a state machine. We leverage
analog memristor-based temporal memories to design a such a state machine that
operates purely on time-coded wavefronts. We implement a version of Dijkstra's
algorithm to evaluate this temporal state machine. This demonstration shows the
promise of expanding the expressibility of temporal computing to enable it to
deliver significant energy and throughput advantages.
</p>
<a href="http://arxiv.org/abs/2009.14243">arXiv:2009.14243</a> [<a href="http://arxiv.org/pdf/2009.14243">pdf</a>]

<h2>Acceleration of Large Margin Metric Learning for Nearest Neighbor Classification Using Triplet Mining and Stratified Sampling. (arXiv:2009.14244v1 [cs.LG])</h2>
<h3>Parisa Abdolrahim Poorheravi, Benyamin Ghojogh, Vincent Gaudet, Fakhri Karray, Mark Crowley</h3>
<p>Metric learning is one of the techniques in manifold learning with the goal
of finding a projection subspace for increasing and decreasing the inter- and
intra-class variances, respectively. Some of the metric learning methods are
based on triplet learning with anchor-positive-negative triplets. Large margin
metric learning for nearest neighbor classification is one of the fundamental
methods to do this. Recently, Siamese networks have been introduced with the
triplet loss. Many triplet mining methods have been developed for Siamese
networks; however, these techniques have not been applied on the triplets of
large margin metric learning for nearest neighbor classification. In this work,
inspired by the mining methods for Siamese networks, we propose several triplet
mining techniques for large margin metric learning. Moreover, a hierarchical
approach is proposed, for acceleration and scalability of optimization, where
triplets are selected by stratified sampling in hierarchical hyper-spheres. We
analyze the proposed methods on three publicly available datasets, i.e., Fisher
Iris, ORL faces, and MNIST datasets.
</p>
<a href="http://arxiv.org/abs/2009.14244">arXiv:2009.14244</a> [<a href="http://arxiv.org/pdf/2009.14244">pdf</a>]

<h2>Ensemble Multi-Source Domain Adaptation with Pseudolabels. (arXiv:2009.14248v1 [cs.LG])</h2>
<h3>Seongmin Lee, Hyunsik Jeon, U Kang</h3>
<p>Given multiple source datasets with labels, how can we train a target model
with no labeled data? Multi-source domain adaptation (MSDA) aims to train a
model using multiple source datasets different from a target dataset in the
absence of target data labels. MSDA is a crucial problem applicable to many
practical cases where labels for the target data are unavailable due to privacy
issues. Existing MSDA frameworks are limited since they align data without
considering conditional distributions p(x|y) of each domain. They also miss a
lot of target label information by not considering the target label at all and
relying on only one feature extractor. In this paper, we propose Ensemble
Multi-source Domain Adaptation with Pseudolabels (EnMDAP), a novel method for
multi-source domain adaptation. EnMDAP exploits label-wise moment matching to
align conditional distributions p(x|y), using pseudolabels for the unavailable
target labels, and introduces ensemble learning theme by using multiple feature
extractors for accurate domain adaptation. Extensive experiments show that
EnMDAP provides the state-of-the-art performance for multi-source domain
adaptation tasks in both of image domains and text domains.
</p>
<a href="http://arxiv.org/abs/2009.14248">arXiv:2009.14248</a> [<a href="http://arxiv.org/pdf/2009.14248">pdf</a>]

<h2>A Framework of Learning Through Empirical Gain Maximization. (arXiv:2009.14250v1 [cs.LG])</h2>
<h3>Yunlong Feng, Qiang Wu</h3>
<p>We develop in this paper a framework of empirical gain maximization (EGM) to
address the robust regression problem where heavy-tailed noise or outliers may
present in the response variable. The idea of EGM is to approximate the density
function of the noise distribution instead of approximating the truth function
directly as usual. Unlike the classical maximum likelihood estimation that
encourages equal importance of all observations and could be problematic in the
presence of abnormal observations, EGM schemes can be interpreted from a
minimum distance estimation viewpoint and allow the ignorance of those
observations. Furthermore, it is shown that several well-known robust nonconvex
regression paradigms, such as Tukey regression and truncated least square
regression, can be reformulated into this new framework. We then develop a
learning theory for EGM, by means of which a unified analysis can be conducted
for these well-established but not fully-understood regression approaches.
Resulting from the new framework, a novel interpretation of existing bounded
nonconvex loss functions can be concluded. Within this new framework, the two
seemingly irrelevant terminologies, the well-known Tukey's biweight loss for
robust regression and the triweight kernel for nonparametric smoothing, are
closely related. More precisely, it is shown that the Tukey's biweight loss can
be derived from the triweight kernel. Similarly, other frequently employed
bounded nonconvex loss functions in machine learning such as the truncated
square loss, the Geman-McClure loss, and the exponential squared loss can also
be reformulated from certain smoothing kernels in statistics. In addition, the
new framework enables us to devise new bounded nonconvex loss functions for
robust learning.
</p>
<a href="http://arxiv.org/abs/2009.14250">arXiv:2009.14250</a> [<a href="http://arxiv.org/pdf/2009.14250">pdf</a>]

<h2>Towards decolonising computational sciences. (arXiv:2009.14258v1 [cs.CY])</h2>
<h3>Abeba Birhane, Olivia Guest</h3>
<p>This article sets out our perspective on how to begin the journey of
decolonising computational fields, such as data and cognitive sciences. We see
this struggle as requiring two basic steps: a) realisation that the present-day
system has inherited, and still enacts, hostile, conservative, and oppressive
behaviours and principles towards women of colour (WoC); and b) rejection of
the idea that centering individual people is a solution to system-level
problems. The longer we ignore these two steps, the more "our" academic system
maintains its toxic structure, excludes, and harms Black women and other
minoritised groups. This also keeps the door open to discredited pseudoscience,
like eugenics and physiognomy. We propose that grappling with our fields'
histories and heritage holds the key to avoiding mistakes of the past. For
example, initiatives such as "diversity boards" can still be harmful because
they superficially appear reformatory but nonetheless center whiteness and
maintain the status quo. Building on the shoulders of many WoC's work, who have
been paving the way, we hope to advance the dialogue required to build both a
grass-roots and a top-down re-imagining of computational sciences -- including
but not limited to psychology, neuroscience, cognitive science, computer
science, data science, statistics, machine learning, and artificial
intelligence. We aspire for these fields to progress away from their stagnant,
sexist, and racist shared past into carving and maintaining an ecosystem where
both a diverse demographics of researchers and scientific ideas that critically
challenge the status quo are welcomed.
</p>
<a href="http://arxiv.org/abs/2009.14258">arXiv:2009.14258</a> [<a href="http://arxiv.org/pdf/2009.14258">pdf</a>]

<h2>Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions. (arXiv:2009.14259v1 [cs.CL])</h2>
<h3>Peter A. Jansen</h3>
<p>The recently proposed ALFRED challenge task aims for a virtual robotic agent
to complete complex multi-step everyday tasks in a virtual home environment
from high-level natural language directives, such as "put a hot piece of bread
on a plate". Currently, the best-performing models are able to complete less
than 5% of these tasks successfully. In this work we focus on modeling the
translation problem of converting natural language directives into detailed
multi-step sequences of actions that accomplish those goals in the virtual
environment. We empirically demonstrate that it is possible to generate gold
multi-step plans from language directives alone without any visual input in 26%
of unseen cases. When a small amount of visual information is incorporated,
namely the starting location in the virtual environment, our best-performing
GPT-2 model successfully generates gold command sequences in 58% of cases. Our
results suggest that contextualized language models may provide strong visual
semantic planning modules for grounded virtual agents.
</p>
<a href="http://arxiv.org/abs/2009.14259">arXiv:2009.14259</a> [<a href="http://arxiv.org/pdf/2009.14259">pdf</a>]

<h2>Abusive Language Detection and Characterization of Twitter Behavior. (arXiv:2009.14261v1 [cs.CL])</h2>
<h3>Dincy Davis, Reena Murali, Remesh Babu</h3>
<p>In this work, abusive language detection in online content is performed using
Bidirectional Recurrent Neural Network (BiRNN) method. Here the main objective
is to focus on various forms of abusive behaviors on Twitter and to detect
whether a speech is abusive or not. The results are compared for various
abusive behaviors in social media, with Convolutional Neural Netwrok (CNN) and
Recurrent Neural Network (RNN) methods and proved that the proposed BiRNN is a
better deep learning model for automatic abusive speech detection.
</p>
<a href="http://arxiv.org/abs/2009.14261">arXiv:2009.14261</a> [<a href="http://arxiv.org/pdf/2009.14261">pdf</a>]

<h2>TEST_POSITIVE at W-NUT 2020 Shared Task-3: Joint Event Multi-task Learning for Slot Filling in Noisy Text. (arXiv:2009.14262v1 [cs.CL])</h2>
<h3>Chacha Chen, Chieh-Yang Huang, Yaqi Hou, Yang Shi, Enyan Dai, Jiaqi Wang</h3>
<p>The competition of extracting COVID-19 events from Twitter is to develop
systems that can automatically extract related events from tweets. The built
system should identify different pre-defined slots for each event, in order to
answer important questions (e.g., Who is tested positive? What is the age of
the person? Where is he/she?). To tackle these challenges, we propose the Joint
Event Multi-task Learning (JOELIN) model. Through a unified global learning
framework, we make use of all the training data across different events to
learn and fine-tune the language model. Moreover, we implement a type-aware
post-processing procedure using named entity recognition (NER) to further
filter the predictions. JOELIN outperforms the BERT baseline by 17.2% in micro
F1.
</p>
<a href="http://arxiv.org/abs/2009.14262">arXiv:2009.14262</a> [<a href="http://arxiv.org/pdf/2009.14262">pdf</a>]

<h2>Learning to swim in potential flow. (arXiv:2009.14280v1 [q-bio.QM])</h2>
<h3>Yusheng Jiao, Feng Ling, Sina Heydari, Nicolas Heess, Josh Merel, Eva Kanso</h3>
<p>Fish swim by undulating their bodies. These propulsive motions require
coordinated shape changes of a body that interacts with its fluid environment,
but the specific shape coordination that leads to robust turning and swimming
motions remains unclear. We propose a simple model of a three-link fish
swimming in a potential flow environment and we use model-free reinforcement
learning to arrive at optimal shape changes for two swimming tasks: swimming in
a desired direction and swimming towards a known target. This fish model
belongs to a class of problems in geometric mechanics, known as driftless
dynamical systems, which allow us to analyze the swimming behavior in terms of
geometric phases over the shape space of the fish. These geometric methods are
less intuitive in the presence of drift. Here, we use the shape space analysis
as a tool for assessing, visualizing, and interpreting the control policies
obtained via reinforcement learning in the absence of drift. We then examine
the robustness of these policies to drift-related perturbations. Although the
fish has no direct control over the drift itself, it learns to take advantage
of the presence of moderate drift to reach its target.
</p>
<a href="http://arxiv.org/abs/2009.14280">arXiv:2009.14280</a> [<a href="http://arxiv.org/pdf/2009.14280">pdf</a>]

<h2>Reannealing of Decaying Exploration Based On Heuristic Measure in Deep Q-Network. (arXiv:2009.14297v1 [cs.AI])</h2>
<h3>Xing Wang, Alexander Vinel</h3>
<p>Existing exploration strategies in reinforcement learning (RL) often either
ignore the history or feedback of search, or are complicated to implement.
There is also a very limited literature showing their effectiveness over
diverse domains. We propose an algorithm based on the idea of reannealing, that
aims at encouraging exploration only when it is needed, for example, when the
algorithm detects that the agent is stuck in a local optimum. The approach is
simple to implement. We perform an illustrative case study showing that it has
potential to both accelerate training and obtain a better policy.
</p>
<a href="http://arxiv.org/abs/2009.14297">arXiv:2009.14297</a> [<a href="http://arxiv.org/pdf/2009.14297">pdf</a>]

<h2>Learning an optimal PSF-pair for ultra-dense 3D localization microscopy. (arXiv:2009.14303v1 [eess.IV])</h2>
<h3>Elias Nehme, Boris Ferdman, Lucien E. Weiss, Tal Naor, Daniel Freedman, Tomer Michaeli, Yoav Shechtman</h3>
<p>A long-standing challenge in multiple-particle-tracking is the accurate and
precise 3D localization of individual particles at close proximity. One
established approach for snapshot 3D imaging is point-spread-function (PSF)
engineering, in which the PSF is modified to encode the axial information.
However, engineered PSFs are challenging to localize at high densities due to
lateral PSF overlaps. Here we suggest using multiple PSFs simultaneously to
help overcome this challenge, and investigate the problem of engineering
multiple PSFs for dense 3D localization. We implement our approach using a
bifurcated optical system that modifies two separate PSFs, and design the PSFs
using three different approaches including end-to-end learning. We demonstrate
our approach experimentally by volumetric imaging of fluorescently labelled
telomeres in cells.
</p>
<a href="http://arxiv.org/abs/2009.14303">arXiv:2009.14303</a> [<a href="http://arxiv.org/pdf/2009.14303">pdf</a>]

<h2>Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study. (arXiv:2009.14304v1 [cs.CL])</h2>
<h3>Saurabh Kulshreshtha, Jos&#xe9; Luis Redondo-Garc&#xed;a, Ching-Yun Chang</h3>
<p>Multilingual BERT (mBERT) has shown reasonable capability for zero-shot
cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not
pre-trained with explicit cross-lingual supervision, transfer performance can
further be improved by aligning mBERT with cross-lingual signal. Prior work
proposes several approaches to align contextualised embeddings. In this paper
we analyse how different forms of cross-lingual supervision and various
alignment methods influence the transfer capability of mBERT in zero-shot
setting. Specifically, we compare parallel corpora vs. dictionary-based
supervision and rotational vs. fine-tuning based alignment methods. We evaluate
the performance of different alignment methodologies across eight languages on
two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we
propose a novel normalisation method which consistently improves the
performance of rotation-based alignment including a notable 3% F1 improvement
for distant and typologically dissimilar languages. Importantly we identify the
biases of the alignment methods to the type of task and proximity to the
transfer language. We also find that supervision from parallel corpus is
generally superior to dictionary alignments.
</p>
<a href="http://arxiv.org/abs/2009.14304">arXiv:2009.14304</a> [<a href="http://arxiv.org/pdf/2009.14304">pdf</a>]

<h2>Attention-driven Body Pose Encoding for Human Activity Recognition. (arXiv:2009.14326v1 [cs.CV])</h2>
<h3>B Debnath, M O&#x27;brien, S Kumar, A Behera</h3>
<p>This article proposes a novel attention-based body pose encoding for human
activity recognition. The approach is combined with RGB video data and 3D human
pose information to give us a novel end-to-end trainable network. Most of the
existing human activity recognition approaches based on 3D pose data often
enrich the input data using additional handcrafted representations such as
velocity, super-normal vectors, pairwise relations, and so on. The enriched
data complements the 3D body joint position data and improves model
performance. In this paper, we propose a novel approach that learns enhanced
feature representations from a given sequence of 3D body joints. To achieve
this encoding, the approach exploits two body pose streams: 1) a spatial stream
which encodes the spatial relationship between various body joints at each time
point to learn spatial structure involving the spatial distribution of
different body joints 2) a temporal stream that learns the temporal variation
of individual body joints over the entire sequence duration to present a
temporally enhanced representation. Afterwards, these two pose streams are
fused with a multi-head attention mechanism. % adapted from neural machine
translation. We also capture the contextual information from the RGB video
stream using a deep Convolutional Neural Network (CNN) model combined with a
multi-head attention and a bidirectional Long Short-Term Memory (LSTM) network.
Moreover, we whose performance is enhanced through the multi-head attention
mechanism. Finally, the RGB video stream is combined with the fused body pose
stream to give a novel end-to-end deep model for effective human activity
recognition. The proposed model is evaluated on three datasets including the
challenging NTU-RGBD dataset and achieves state-of-the-art results.
</p>
<a href="http://arxiv.org/abs/2009.14326">arXiv:2009.14326</a> [<a href="http://arxiv.org/pdf/2009.14326">pdf</a>]

<h2>A machine learning approach for detecting CNAME cloaking-based tracking on the Web. (arXiv:2009.14330v1 [cs.CR])</h2>
<h3>Ha Dao, Kensuke Fukuda</h3>
<p>Various in-browser privacy protection techniques have been designed to
protect end-users from third-party tracking. In an arms race against these
counter-measures, the tracking providers developed a new technique called CNAME
cloaking based tracking to avoid issues with browsers that block third-party
cookies and requests. To detect this tracking technique, browser extensions
require on-demand DNS lookup APIs. This feature is however only supported by
the Firefox browser.

In this paper, we propose a supervised machine learning-based method to
detect CNAME cloaking-based tracking without the on-demand DNS lookup. Our goal
is to detect both sites and requests linked to CNAME cloaking-related tracking.
We crawl a list of target sites and store all HTTP/HTTPS requests with their
attributes. Then we label all instances automatically by looking up CNAME
record of subdomain, and applying wildcard matching based on well-known
tracking filter lists. After extracting features, we build a supervised
classification model to distinguish site and request related to CNAME
cloaking-based tracking. Our evaluation shows that the proposed approach
outperforms well-known tracking filter lists: F1 scores of 0.790 for sites and
0.885 for requests. By analyzing the feature permutation importance, we
demonstrate that the number of scripts and the proportion of XMLHttpRequests
are discriminative for detecting sites, and the length of URL request is
helpful in detecting requests. Finally, we analyze concept drift by using the
2018 dataset to train a model and obtain a reasonable performance on the 2020
dataset for detecting both sites and requests using CNAME cloaking-based
tracking.
</p>
<a href="http://arxiv.org/abs/2009.14330">arXiv:2009.14330</a> [<a href="http://arxiv.org/pdf/2009.14330">pdf</a>]

<h2>Direct Multi-hop Attention based Graph Neural Network. (arXiv:2009.14332v1 [cs.LG])</h2>
<h3>Guangtao Wang, Rex Ying, Jing Huang, Jure Leskovec</h3>
<p>Introducing self-attention mechanism in graph neural networks (GNNs) achieved
state-of-the-art performance for graph representation learning. However, at
every layer, attention is only computed between two connected nodes and depends
solely on the representation of both nodes. This attention computation cannot
account for the multi-hop neighbors which supply graph structure context
information and have influence on the node representation learning as well. In
this paper, we propose Direct Multi-hop Attention based Graph neural Network
(DAGN) for graph representation learning, a principled way to incorporate
multi-hop neighboring context into attention computation, enabling long-range
interactions at every layer. To compute attention between nodes that are
multiple hops away, DAGN diffuses the attention scores from neighboring nodes
to non-neighboring nodes, thus increasing the receptive field for every message
passing layer. Unlike previous methods, DAGN uses a diffusion prior on
attention values, to efficiently account for all paths between the pair of
nodes when computing multi-hop attention weights. This helps DAGN capture
large-scale structural information in a single layer, and learn more
informative attention distribution. Experimental results on standard
semi-supervised node classification as well as the knowledge graph completion
show that DAGN achieves state-of-the-art results: DAGN achieves up to 5.7%
relative error reduction over the previous state-of-the-art on Cora, Citeseer,
and Pubmed. DAGN also obtains the best performance on a large-scale Open Graph
Benchmark dataset. On knowledge graph completion DAGN advances state-of-the-art
on WN18RR and FB15k-237 across four different performance metrics.
</p>
<a href="http://arxiv.org/abs/2009.14332">arXiv:2009.14332</a> [<a href="http://arxiv.org/pdf/2009.14332">pdf</a>]

<h2>StratLearner: Learning a Strategy for Misinformation Prevention in Social Networks. (arXiv:2009.14337v1 [cs.LG])</h2>
<h3>Guangmo Tong</h3>
<p>Given a combinatorial optimization problem taking an input, can we learn a
strategy to solve it from the examples of input-solution pairs without knowing
its objective function? In this paper, we consider such a setting and study the
misinformation prevention problem. Given the examples of attacker-protector
pairs, our goal is to learn a strategy to compute protectors against future
attackers, without the need of knowing the underlying diffusion model. To this
end, we design a structured prediction framework, where the main idea is to
parameterize the scoring function using random features constructed through
distance functions on randomly sampled subgraphs, which leads to a kernelized
scoring function with weights learnable via the large margin method. Evidenced
by experiments, our method can produce near-optimal protectors without using
any information of the diffusion model, and it outperforms other possible
graph-based and learning-based methods by an evident margin.
</p>
<a href="http://arxiv.org/abs/2009.14337">arXiv:2009.14337</a> [<a href="http://arxiv.org/pdf/2009.14337">pdf</a>]

<h2>Computing Systems for Autonomous Driving: State-of-the-Art and Challenges. (arXiv:2009.14349v1 [cs.DC])</h2>
<h3>Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang Zhang, Weisong Shi</h3>
<p>The recent proliferation of computing technologies, e.g., sensors, computer
vision, machine learning, hardware acceleration, and the broad deployment of
communication mechanisms, e.g., DSRC, C-V2X, 5G, have pushed the horizon of
autonomous driving, which automates the decision and control of vehicles by
leveraging the perception results based on multiple sensors. The key to the
success of these autonomous systems is making a reliable decision in a
real-time fashion. However, accidents and fatalities caused by early deployed
autonomous vehicles arise from time to time. The real traffic environment is
too complicated for the current autonomous driving computing systems to
understand and handle. In this paper, we present the state-of-the-art computing
systems for autonomous driving, including seven performance metrics and nine
key technologies, followed by eleven challenges and opportunities to realize
autonomous driving. We hope this paper will gain attention from both the
computing and automotive communities and inspire more research in this
direction.
</p>
<a href="http://arxiv.org/abs/2009.14349">arXiv:2009.14349</a> [<a href="http://arxiv.org/pdf/2009.14349">pdf</a>]

<h2>Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning. (arXiv:2009.14352v1 [cs.CV])</h2>
<h3>Xiangxi Shi, Xu Yang, Jiuxiang Gu, Shafiq Joty, Jianfei Cai</h3>
<p>Change Captioning is a task that aims to describe the difference between
images with natural language. Most existing methods treat this problem as a
difference judgment without the existence of distractors, such as viewpoint
changes. However, in practice, viewpoint changes happen often and can overwhelm
the semantic difference to be described. In this paper, we propose a novel
visual encoder to explicitly distinguish viewpoint changes from semantic
changes in the change captioning task. Moreover, we further simulate the
attention preference of humans and propose a novel reinforcement learning
process to fine-tune the attention directly with language evaluation rewards.
Extensive experimental results show that our method outperforms the
state-of-the-art approaches by a large margin in both Spot-the-Diff and
CLEVR-Change datasets.
</p>
<a href="http://arxiv.org/abs/2009.14352">arXiv:2009.14352</a> [<a href="http://arxiv.org/pdf/2009.14352">pdf</a>]

<h2>Toolpath design for additive manufacturing using deep reinforcement learning. (arXiv:2009.14365v1 [cs.AI])</h2>
<h3>Mojtaba Mozaffar, Ablodghani Ebrahimi, Jian Cao</h3>
<p>Toolpath optimization of metal-based additive manufacturing processes is
currently hampered by the high-dimensionality of its design space. In this
work, a reinforcement learning platform is proposed that dynamically learns
toolpath strategies to build an arbitrary part. To this end, three prominent
model-free reinforcement learning formulations are investigated to design
additive manufacturing toolpaths and demonstrated for two cases of dense and
sparse reward structures. The results indicate that this learning-based
toolpath design approach achieves high scores, especially when a dense reward
structure is present.
</p>
<a href="http://arxiv.org/abs/2009.14365">arXiv:2009.14365</a> [<a href="http://arxiv.org/pdf/2009.14365">pdf</a>]

<h2>Toward Privacy and Utility Preserving Image Representation. (arXiv:2009.14376v1 [cs.CV])</h2>
<h3>Ahmadreza Mosallanezhad, Yasin Silva, Michelle V. Mancenido, Huan Liu</h3>
<p>Face images are rich data items that are useful and can easily be collected
in many applications, such as in 1-to-1 face verification tasks in the domain
of security and surveillance systems. Multiple methods have been proposed to
protect an individual's privacy by perturbing the images to remove traces of
identifiable information, such as gender or race. However, significantly less
attention has been given to the problem of protecting images while maintaining
optimal task utility. In this paper, we study the novel problem of creating
privacy-preserving image representations with respect to a given utility task
by proposing a principled framework called the Adversarial Image Anonymizer
(AIA). AIA first creates an image representation using a generative model, then
enhances the learned image representations using adversarial learning to
preserve privacy and utility for a given task. Experiments were conducted on a
publicly available data set to demonstrate the effectiveness of AIA as a
privacy-preserving mechanism for face images.
</p>
<a href="http://arxiv.org/abs/2009.14376">arXiv:2009.14376</a> [<a href="http://arxiv.org/pdf/2009.14376">pdf</a>]

<h2>Few-shot Learning for Time-series Forecasting. (arXiv:2009.14379v1 [stat.ML])</h2>
<h3>Tomoharu Iwata, Atsutoshi Kumagai</h3>
<p>Time-series forecasting is important for many applications. Forecasting
models are usually trained using time-series data in a specific target task.
However, sufficient data in the target task might be unavailable, which leads
to performance degradation. In this paper, we propose a few-shot learning
method that forecasts a future value of a time-series in a target task given a
few time-series in the target task. Our model is trained using time-series data
in multiple training tasks that are different from target tasks. Our model uses
a few time-series to build a forecasting function based on a recurrent neural
network with an attention mechanism. With the attention mechanism, we can
retrieve useful patterns in a small number of time-series for the current
situation. Our model is trained by minimizing an expected test error of
forecasting next timestep values. We demonstrate the effectiveness of the
proposed method using 90 time-series datasets.
</p>
<a href="http://arxiv.org/abs/2009.14379">arXiv:2009.14379</a> [<a href="http://arxiv.org/pdf/2009.14379">pdf</a>]

<h2>AutoDSE: Enabling Software Programmers Design Efficient FPGA Accelerators. (arXiv:2009.14381v1 [cs.AR])</h2>
<h3>Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, Jason Cong</h3>
<p>Adopting FPGA as an accelerator in datacenters is becoming mainstream for
customized computing, but the fact that FPGAs are hard to program creates a
steep learning curve for software programmers. Even with the help of high-level
synthesis (HLS), accelerator designers still have to manually perform code
reconstruction and cumbersome parameter tuning to achieve the optimal
performance. While many learning models have been leveraged by existing work to
automate the design of efficient accelerators, the unpredictability of modern
HLS tools becomes a major obstacle for them to maintain high accuracy. In this
paper, we address this problem by incorporating an automated DSE
framework-AutoDSE- that leverages bottleneck-guided gradient optimizer to
systematically find abetter design point. AutoDSE finds the bottleneck of the
design in each step and focuses on high-impact parameters to overcome that,
which is similar to the approach an expert would take. The experimental results
show that AutoDSE is able to find the design point that achieves, on the
geometric mean, 19.9x speedup over one CPU core for Machsuite and Rodinia
benchmarks and 1.04x over the manually designed HLS accelerated vision kernels
in Xilinx Vitis libraries yet with 26x reduction of their optimization pragmas.
With less than one optimization pragma per design on average, we are making
progress towards democratizing customizable computing by enabling software
programmers to design efficient FPGA accelerators.
</p>
<a href="http://arxiv.org/abs/2009.14381">arXiv:2009.14381</a> [<a href="http://arxiv.org/pdf/2009.14381">pdf</a>]

<h2>AttendNets: Tiny Deep Image Recognition Neural Networks for the Edge via Visual Attention Condensers. (arXiv:2009.14385v1 [cs.CV])</h2>
<h3>Alexander Wong, Mahmoud Famouri, Mohammad Javad Shafiee</h3>
<p>While significant advances in deep learning has resulted in state-of-the-art
performance across a large number of complex visual perception tasks, the
widespread deployment of deep neural networks for TinyML applications involving
on-device, low-power image recognition remains a big challenge given the
complexity of deep neural networks. In this study, we introduce AttendNets,
low-precision, highly compact deep neural networks tailored for on-device image
recognition. More specifically, AttendNets possess deep self-attention
architectures based on visual attention condensers, which extends on the
recently introduced stand-alone attention condensers to improve spatial-channel
selective attention. Furthermore, AttendNets have unique machine-designed
macroarchitecture and microarchitecture designs achieved via a machine-driven
design exploration strategy. Experimental results on ImageNet$_{50}$ benchmark
dataset for the task of on-device image recognition showed that AttendNets have
significantly lower architectural and computational complexity when compared to
several deep neural networks in research literature designed for efficiency
while achieving highest accuracies (with the smallest AttendNet achieving
$\sim$7.2% higher accuracy, while requiring $\sim$3$\times$ fewer multiply-add
operations, $\sim$4.17$\times$ fewer parameters, and $\sim$16.7$\times$ lower
weight memory requirements than MobileNet-V1). Based on these promising
results, AttendNets illustrate the effectiveness of visual attention condensers
as building blocks for enabling various on-device visual perception tasks for
TinyML applications.
</p>
<a href="http://arxiv.org/abs/2009.14385">arXiv:2009.14385</a> [<a href="http://arxiv.org/pdf/2009.14385">pdf</a>]

<h2>Can Automatic Post-Editing Improve NMT?. (arXiv:2009.14395v1 [cs.CL])</h2>
<h3>Shamil Chollampatt, Raymond Hendy Susanto, Liling Tan, Ewa Szymanska</h3>
<p>Automatic post-editing (APE) aims to improve machine translations, thereby
reducing human post-editing effort. APE has had notable success when used with
statistical machine translation (SMT) systems but has not been as successful
over neural machine translation (NMT) systems. This has raised questions on the
relevance of APE task in the current scenario. However, the training of APE
models has been heavily reliant on large-scale artificial corpora combined with
only limited human post-edited data. We hypothesize that APE models have been
underperforming in improving NMT translations due to the lack of adequate
supervision. To ascertain our hypothesis, we compile a larger corpus of human
post-edits of English to German NMT. We empirically show that a state-of-art
neural APE model trained on this corpus can significantly improve a strong
in-domain NMT system, challenging the current understanding in the field. We
further investigate the effects of varying training data sizes, using
artificial training data, and domain specificity for the APE task. We release
this new corpus under CC BY-NC-SA 4.0 license at
https://github.com/shamilcm/pedra.
</p>
<a href="http://arxiv.org/abs/2009.14395">arXiv:2009.14395</a> [<a href="http://arxiv.org/pdf/2009.14395">pdf</a>]

<h2>Deep Equals Shallow for ReLU Networks in Kernel Regimes. (arXiv:2009.14397v1 [stat.ML])</h2>
<h3>Alberto Bietti, Francis Bach</h3>
<p>Deep networks are often considered to be more expressive than shallow ones in
terms of approximation. Indeed, certain functions can be approximated by deep
networks provably more efficiently than by shallow ones, however, no tractable
algorithms are known for learning such deep models. Separately, a recent line
of work has shown that deep networks trained with gradient descent may behave
like (tractable) kernel methods in a certain over-parameterized regime, where
the kernel is determined by the architecture and initialization, and this paper
focuses on approximation for such kernels. We show that for ReLU activations,
the kernels derived from deep fully-connected networks have essentially the
same approximation properties as their shallow two-layer counterpart, namely
the same eigenvalue decay for the corresponding integral operator. This
highlights the limitations of the kernel framework for understanding the
benefits of such deep architectures. Our main theoretical result relies on
characterizing such eigenvalue decays through differentiability properties of
the kernel function, which also easily applies to the study of other kernels
defined on the sphere.
</p>
<a href="http://arxiv.org/abs/2009.14397">arXiv:2009.14397</a> [<a href="http://arxiv.org/pdf/2009.14397">pdf</a>]

<h2>Transfer Learning from Speech Synthesis to Voice Conversion with Non-Parallel Training Data. (arXiv:2009.14399v1 [eess.AS])</h2>
<h3>Mingyang Zhang, Yi Zhou, Li Zhao, Haizhou Li</h3>
<p>This paper presents a novel framework to build a voice conversion (VC) system
by learning from a text-to-speech (TTS) synthesis system, that is called TTS-VC
transfer learning. We first develop a multi-speaker speech synthesis system
with sequence-to-sequence encoder-decoder architecture, where the encoder
extracts robust linguistic representations of text, and the decoder,
conditioned on target speaker embedding, takes the context vectors and the
attention recurrent network cell output to generate target acoustic features.
We take advantage of the fact that TTS system maps input text to speaker
independent context vectors, and reuse such a mapping to supervise the training
of latent representations of an encoder-decoder voice conversion system. In the
voice conversion system, the encoder takes speech instead of text as input,
while the decoder is functionally similar to TTS decoder. As we condition the
decoder on speaker embedding, the system can be trained on non-parallel data
for any-to-any voice conversion. During voice conversion training, we present
both text and speech to speech synthesis and voice conversion networks
respectively. At run-time, the voice conversion network uses its own
encoder-decoder architecture. Experiments show that the proposed approach
outperforms two competitive voice conversion baselines consistently, namely
phonetic posteriorgram and variational autoencoder methods, in terms of speech
quality, naturalness, and speaker similarity.
</p>
<a href="http://arxiv.org/abs/2009.14399">arXiv:2009.14399</a> [<a href="http://arxiv.org/pdf/2009.14399">pdf</a>]

<h2>Teacher-Critical Training Strategies for Image Captioning. (arXiv:2009.14405v1 [cs.CV])</h2>
<h3>Yiqing Huang, Jiansheng Chen</h3>
<p>Existing image captioning models are usually trained by cross-entropy (XE)
loss and reinforcement learning (RL), which set ground-truth words as hard
targets and force the captioning model to learn from them. However, the widely
adopted training strategies suffer from misalignment in XE training and
inappropriate reward assignment in RL training. To tackle these problems, we
introduce a teacher model that serves as a bridge between the ground-truth
caption and the caption model by generating some easier-to-learn word proposals
as soft targets. The teacher model is constructed by incorporating the
ground-truth image attributes into the baseline caption model. To effectively
learn from the teacher model, we propose Teacher-Critical Training Strategies
(TCTS) for both XE and RL training to facilitate better learning processes for
the caption model. Experimental evaluations of several widely adopted caption
models on the benchmark MSCOCO dataset show the proposed TCTS comprehensively
enhances most evaluation metrics, especially the Bleu and Rouge-L scores, in
both training stages. TCTS is able to achieve to-date the best published single
model Bleu-4 and Rouge-L performances of 40.2% and 59.4% on the MSCOCO Karpathy
test split. Our codes and pre-trained models will be open-sourced.
</p>
<a href="http://arxiv.org/abs/2009.14405">arXiv:2009.14405</a> [<a href="http://arxiv.org/pdf/2009.14405">pdf</a>]

<h2>Bilateral Asymmetry Guided Counterfactual Generating Network for Mammogram Classification. (arXiv:2009.14406v1 [cs.CV])</h2>
<h3>Chu-ran Wang, Jing Li, Fandong Zhang, Xinwei Sun, Hao Dong, Yizhou Yu, Yizhou Wang</h3>
<p>Mammogram benign or malignant classification with only image-level labels is
challenging due to the absence of lesion annotations. Motivated by the
symmetric prior that the lesions on one side of breasts rarely appear in the
corresponding areas on the other side, given a diseased image, we can explore a
counterfactual problem that how would the features have behaved if there were
no lesions in the image, so as to identify the lesion areas. We derive a new
theoretical result for counterfactual generation based on the symmetric prior.
By building a causal model that entails such a prior for bilateral images, we
obtain two optimization goals for counterfactual generation, which can be
accomplished via our newly proposed counterfactual generative network. Our
proposed model is mainly composed of Generator Adversarial Network and a
\emph{prediction feedback mechanism}, they are optimized jointly and prompt
each other. Specifically, the former can further improve the classification
performance by generating counterfactual features to calculate lesion areas. On
the other hand, the latter helps counterfactual generation by the supervision
of classification loss. The utility of our method and the effectiveness of each
module in our model can be verified by state-of-the-art performance on INBreast
and an in-house dataset and ablation studies.
</p>
<a href="http://arxiv.org/abs/2009.14406">arXiv:2009.14406</a> [<a href="http://arxiv.org/pdf/2009.14406">pdf</a>]

<h2>Relay Pursuit of an Evader by a Heterogeneous Group of Pursuers Using Potential Games. (arXiv:2009.14407v1 [eess.SY])</h2>
<h3>Yoonjae Lee, Efstathios Bakolas</h3>
<p>We propose a decentralized solution for a pursuit-evasion game involving a
heterogeneous group of rational (selfish) pursuers and a single evader based on
the framework of potential games. In the proposed game, the evader aims to
delay (or, if possible, avoid) capture by any of the pursuers whereas each
pursuer tries to capture the latter only if this is to his best interest. Our
approach resembles in principle the so-called relay pursuit strategy introduced
in [1], in which only the pursuer that can capture the evader faster than the
others is active. In sharp contrast with the latter approach, the active
pursuer herein is not determined by a reactive ad-hoc rule but from the
solution of a corresponding potential game. We assume that each pursuer has
different capabilities and his decision whether to go after the evader or not
is based on the maximization of his individual utility (conditional on the
choices and actions of the other pursuers). The pursuers' utilities depend on
both the rewards that they will receive by capturing the evader and the time of
capture (cost of capturing the evader) so that a pursuer should only seek
capture when the incurred cost is relatively small. The determination of the
active pursuer-evader assignments (in other words, which pursuers should be
active) is done iteratively by having the pursuers exchange information and
updating their own actions by executing a learning algorithm for games known as
Spatial Adaptive Play (SAP). We illustrate the performance of our algorithm by
means of extensive numerical simulations.
</p>
<a href="http://arxiv.org/abs/2009.14407">arXiv:2009.14407</a> [<a href="http://arxiv.org/pdf/2009.14407">pdf</a>]

<h2>AUBER: Automated BERT Regularization. (arXiv:2009.14409v1 [cs.AI])</h2>
<h3>Hyun Dong Lee, Seongmin Lee, U Kang</h3>
<p>How can we effectively regularize BERT? Although BERT proves its
effectiveness in various downstream natural language processing tasks, it often
overfits when there are only a small number of training instances. A promising
direction to regularize BERT is based on pruning its attention heads based on a
proxy score for head importance. However, heuristic-based methods are usually
suboptimal since they predetermine the order by which attention heads are
pruned. In order to overcome such a limitation, we propose AUBER, an effective
regularization method that leverages reinforcement learning to automatically
prune attention heads from BERT. Instead of depending on heuristics or
rule-based policies, AUBER learns a pruning policy that determines which
attention heads should or should not be pruned for regularization. Experimental
results show that AUBER outperforms existing pruning methods by achieving up to
10% better accuracy. In addition, our ablation study empirically demonstrates
the effectiveness of our design choices for AUBER.
</p>
<a href="http://arxiv.org/abs/2009.14409">arXiv:2009.14409</a> [<a href="http://arxiv.org/pdf/2009.14409">pdf</a>]

<h2>Efficient Kernel Transfer in Knowledge Distillation. (arXiv:2009.14416v1 [cs.LG])</h2>
<h3>Qi Qian, Hao Li, Juhua Hu</h3>
<p>Knowledge distillation is an effective way for model compression in deep
learning. Given a large model (i.e., teacher model), it aims to improve the
performance of a compact model (i.e., student model) by transferring the
information from the teacher. An essential challenge in knowledge distillation
is to identify the appropriate information to transfer. In early works, only
the final output of the teacher model is used as the soft label to help the
training of student models. Recently, the information from intermediate layers
is also adopted for better distillation. In this work, we aim to optimize the
process of knowledge distillation from the perspective of kernel matrix. The
output of each layer in a neural network can be considered as a new feature
space generated by applying a kernel function on original images. Hence, we
propose to transfer the corresponding kernel matrix (i.e., Gram matrix) from
teacher models to student models for distillation. However, the size of the
whole kernel matrix is quadratic to the number of examples. To improve the
efficiency, we decompose the original kernel matrix with Nystr{\"{o}}m method
and then transfer the partial matrix obtained with landmark points, whose size
is linear in the number of examples. More importantly, our theoretical analysis
shows that the difference between the original kernel matrices of teacher and
student can be well bounded by that of their corresponding partial matrices.
Finally, a new strategy of generating appropriate landmark points is proposed
for better distillation. The empirical study on benchmark data sets
demonstrates the effectiveness of the proposed algorithm. Code will be
released.
</p>
<a href="http://arxiv.org/abs/2009.14416">arXiv:2009.14416</a> [<a href="http://arxiv.org/pdf/2009.14416">pdf</a>]

<h2>Towards Adaptive Semantic Segmentation by Progressive Feature Refinement. (arXiv:2009.14420v1 [cs.CV])</h2>
<h3>Bin Zhang, Shengjie Zhao, Rongqing Zhang</h3>
<p>As one of the fundamental tasks in computer vision, semantic segmentation
plays an important role in real world applications. Although numerous deep
learning models have made notable progress on several mainstream datasets with
the rapid development of convolutional networks, they still encounter various
challenges in practical scenarios. Unsupervised adaptive semantic segmentation
aims to obtain a robust classifier trained with source domain data, which is
able to maintain stable performance when deployed to a target domain with
different data distribution. In this paper, we propose an innovative
progressive feature refinement framework, along with domain adversarial
learning to boost the transferability of segmentation networks. Specifically,
we firstly align the multi-stage intermediate feature maps of source and target
domain images, and then a domain classifier is adopted to discriminate the
segmentation output. As a result, the segmentation models trained with source
domain images can be transferred to a target domain without significant
performance degradation. Experimental results verify the efficiency of our
proposed method compared with state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2009.14420">arXiv:2009.14420</a> [<a href="http://arxiv.org/pdf/2009.14420">pdf</a>]

<h2>Pairbot: A Novel Model for Autonomous Mobile Robot Systems Consisting of Paired Robots. (arXiv:2009.14426v1 [cs.DC])</h2>
<h3>Yonghwan Kim, Yoshiaki Katayama, Koichi Wada</h3>
<p>Programmable matter consists of many self-organizing computational entities
which are autonomous and cooperative with one another to achieve a goal and it
has been widely studied in various fields, e.g., robotics or mobile agents,
theoretically and practically. In the field of computer science, programmable
matter can be theoretically modeled as a distributed system consisting of
simple and small robots equipped with limited capabilities, e.g., no memory
and/or no geometrical coordination. A lot of theoretical research is studied
based on such theoretical models, to clarify the relation between the
solvability of various problems and the considered models.

We newly propose a computational model named Pairbot model where two
autonomous mobile robots operate as a pair on a grid plane. In Pairbot model,
every robot has the one robot as its unique partner, called buddy, each other.
We call the paired two robots pairbot. Two robots in one pairbot can recognize
each other, and repeatedly change their geometrical relationships, long and
short, to achieve the goal.

In this paper, as a first step to show the feasibility and effectiveness of
the proposed Pairbot model, we introduce two simple problems, the marching
problem and the object coating problem, and propose two algorithms to solve
these two problems, respectively. In both algorithms, it is assumed that the
visibility range is one (every robot can observe only its neighboring robots)
and the scheduler is asynchronous (ASYNC).
</p>
<a href="http://arxiv.org/abs/2009.14426">arXiv:2009.14426</a> [<a href="http://arxiv.org/pdf/2009.14426">pdf</a>]

<h2>A General Framework for Charger Scheduling Optimization Problems. (arXiv:2009.14428v1 [cs.NI])</h2>
<h3>Xuan Li, Miao Jin</h3>
<p>This paper presents a general framework to tackle a diverse range of NP-hard
charger scheduling problems, optimizing the trajectory of mobile chargers to
prolong the life of Wireless Rechargeable Sensor Network (WRSN), a system
consisting of sensors with rechargeable batteries and mobile chargers. Existing
solutions to charger scheduling problems require problem-specific design and a
trade-off between the solution quality and computing time. Instead, we observe
that instances of the same type of charger scheduling problem are solved
repeatedly with similar combinatorial structure but different data. We consider
searching an optimal charger scheduling as a trial and error process, and the
objective function of a charging optimization problem as reward, a scalar
feedback signal for each search. We propose a deep reinforcement learning-based
charger scheduling optimization framework. The biggest advantage of the
framework is that a diverse range of domain-specific charger scheduling
strategy can be learned automatically from previous experiences. A framework
also simplifies the complexity of algorithm design for individual charger
scheduling optimization problem. We pick three representative charger
scheduling optimization problems, design algorithms based on the proposed deep
reinforcement learning framework, implement them, and compare them with
existing ones. Extensive simulation results show that our algorithms based on
the proposed framework outperform all existing ones.
</p>
<a href="http://arxiv.org/abs/2009.14428">arXiv:2009.14428</a> [<a href="http://arxiv.org/pdf/2009.14428">pdf</a>]

<h2>Towards Improved Model Design for Authorship Identification: A Survey on Writing Style Understanding. (arXiv:2009.14445v1 [cs.CL])</h2>
<h3>Weicheng Ma, Ruibo Liu, Lili Wang, Soroush Vosoughi</h3>
<p>Authorship identification tasks, which rely heavily on linguistic styles,
have always been an important part of Natural Language Understanding (NLU)
research. While other tasks based on linguistic style understanding benefit
from deep learning methods, these methods have not behaved as well as
traditional machine learning methods in many authorship-based tasks. With these
tasks becoming more and more challenging, however, traditional machine learning
methods based on handcrafted feature sets are already approaching their
performance limits. Thus, in order to inspire future applications of deep
learning methods in authorship-based tasks in ways that benefit the extraction
of stylistic features, we survey authorship-based tasks and other tasks related
to writing style understanding. We first describe our survey results on the
current state of research in both sets of tasks and summarize existing
achievements and problems in authorship-related tasks. We then describe
outstanding methods in style-related tasks in general and analyze how they are
used in combination in the top-performing models. We are optimistic about the
applicability of these models to authorship-based tasks and hope our survey
will help advance research in this field.
</p>
<a href="http://arxiv.org/abs/2009.14445">arXiv:2009.14445</a> [<a href="http://arxiv.org/pdf/2009.14445">pdf</a>]

<h2>Ask-n-Learn: Active Learning via Reliable Gradient Representations for Image Classification. (arXiv:2009.14448v1 [stat.ML])</h2>
<h3>Bindya Venkatesh, Jayaraman J. Thiagarajan</h3>
<p>Deep predictive models rely on human supervision in the form of labeled
training data. Obtaining large amounts of annotated training data can be
expensive and time consuming, and this becomes a critical bottleneck while
building such models in practice. In such scenarios, active learning (AL)
strategies are used to achieve faster convergence in terms of labeling efforts.
Existing active learning employ a variety of heuristics based on uncertainty
and diversity to select query samples. Despite their wide-spread use, in
practice, their performance is limited by a number of factors including
non-calibrated uncertainties, insufficient trade-off between data exploration
and exploitation, presence of confirmation bias etc. In order to address these
challenges, we propose Ask-n-Learn, an active learning approach based on
gradient embeddings obtained using the pesudo-labels estimated in each
iteration of the algorithm. More importantly, we advocate the use of prediction
calibration to obtain reliable gradient embeddings, and propose a data
augmentation strategy to alleviate the effects of confirmation bias during
pseudo-labeling. Through empirical studies on benchmark image classification
tasks (CIFAR-10, SVHN, Fashion-MNIST, MNIST), we demonstrate significant
improvements over state-of-the-art baselines, including the recently proposed
BADGE algorithm.
</p>
<a href="http://arxiv.org/abs/2009.14448">arXiv:2009.14448</a> [<a href="http://arxiv.org/pdf/2009.14448">pdf</a>]

<h2>Accurate and Robust Feature Importance Estimation under Distribution Shifts. (arXiv:2009.14454v1 [stat.ML])</h2>
<h3>Jayaraman J. Thiagarajan, Vivek Narayanaswamy, Rushil Anirudh, Peer-Timo Bremer, Andreas Spanias</h3>
<p>With increasing reliance on the outcomes of black-box models in critical
applications, post-hoc explainability tools that do not require access to the
model internals are often used to enable humans understand and trust these
models. In particular, we focus on the class of methods that can reveal the
influence of input features on the predicted outputs. Despite their wide-spread
adoption, existing methods are known to suffer from one or more of the
following challenges: computational complexities, large uncertainties and most
importantly, inability to handle real-world domain shifts. In this paper, we
propose PRoFILE, a novel feature importance estimation method that addresses
all these challenges. Through the use of a loss estimator jointly trained with
the predictive model and a causal objective, PRoFILE can accurately estimate
the feature importance scores even under complex distribution shifts, without
any additional re-training. To this end, we also develop learning strategies
for training the loss estimator, namely contrastive and dropout calibration,
and find that it can effectively detect distribution shifts. Using empirical
studies on several benchmark image and non-image data, we show significant
improvements over state-of-the-art approaches, both in terms of fidelity and
robustness.
</p>
<a href="http://arxiv.org/abs/2009.14454">arXiv:2009.14454</a> [<a href="http://arxiv.org/pdf/2009.14454">pdf</a>]

<h2>Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks. (arXiv:2009.14456v1 [cs.NE])</h2>
<h3>Weihao Tan, Devdhar Patel, Robert Kozma</h3>
<p>Spiking neural networks (SNNs) have great potential for energy-efficient
implementation of Deep Neural Networks (DNNs) on dedicated neuromorphic
hardware. Recent studies demonstrated competitive performance of SNNs compared
with DNNs on image classification tasks, including CIFAR-10 and ImageNet data.
The present work focuses on using SNNs in combination with deep reinforcement
learning in ATARI games, which involves additional complexity as compared to
image classification. We review the theory of converting DNNs to SNNs and
extending the conversion to Deep Q-Networks (DQNs). We propose a robust
representation of the firing rate to reduce the error during the conversion
process. In addition, we introduce a new metric to evaluate the conversion
process by comparing the decisions made by the DQN and SNN, respectively. We
also analyze how the simulation time and parameter normalization influence the
performance of converted SNNs. We achieve competitive scores on 17
top-performing Atari games. To the best of our knowledge, our work is the first
to achieve state-of-the-art performance on multiple Atari games with SNNs. Our
work serves as a benchmark for the conversion of DQNs to SNNs and paves the way
for further research on solving reinforcement learning tasks with SNNs.
</p>
<a href="http://arxiv.org/abs/2009.14456">arXiv:2009.14456</a> [<a href="http://arxiv.org/pdf/2009.14456">pdf</a>]

<h2>Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning. (arXiv:2009.14457v1 [cs.CL])</h2>
<h3>Subhojeet Pramanik, Shashank Mujumdar, Hima Patel</h3>
<p>In this paper, we propose a multi-task learning-based framework that utilizes
a combination of self-supervised and supervised pre-training tasks to learn a
generic document representation. We design the network architecture and the
pre-training tasks to incorporate the multi-modal document information across
text, layout, and image dimensions and allow the network to work with
multi-page documents. We showcase the applicability of our pre-training
framework on a variety of different real-world document tasks such as document
classification, document information extraction, and document retrieval. We
conduct exhaustive experiments to compare performance against different
ablations of our framework and state-of-the-art baselines. We discuss the
current limitations and next steps for our work.
</p>
<a href="http://arxiv.org/abs/2009.14457">arXiv:2009.14457</a> [<a href="http://arxiv.org/pdf/2009.14457">pdf</a>]

<h2>DER Information Unaware Coordination via Day-ahead Dynamic Power Bounds. (arXiv:2009.14458v1 [eess.SY])</h2>
<h3>Thomas Navidi, Chloe Leblanc, Abbas El Gamal, Ram Rajagopal</h3>
<p>Reliability and voltage quality in distribution networks have been achieved
via a combination of transformer power rating satisfaction and voltage
management asset control. To maintain reliable operation under this paradigm,
however, future grids with deep DER penetrations would require costly equipment
upgrades. These upgrades can be mitigated via judicious coordination of DER
operation. Earlier work has assumed a hierarchical control architecture in
which a global controller (GC) uses detailed power injection and DER data and
knowledge of DER owners' objectives to determine setpoints that local
controllers should follow in order to achieve reliable and cost effective grid
operation. Having such data and assuming knowledge of DER owners' objectives,
however, are often not desirable or possible. In an earlier work, a 2-layer DER
coordination architecture was shown to achieve close to optimal performance
despite infrequent communication to a global controller. Motivated by this
work, this paper proposes a day-ahead coordination scheme that uses forecasted
power profile ranges to generate day-ahead dynamic power rating bounds at each
transformer. Novel features of this scheme include: (i) the GC knows only past
node power injection data and does not impose or know DER owner objectives,
(ii) we use bounds that ensure reliable operation to guide the local
controllers rather than setpoint tracking, and (iii) we consider electric
vehicle (EV) charging in addition to storage. Simulations using the IEEE
123-bus network show that with 50% solar, 50% EVs and 10% storage penetrations,
the uncoordinated approach incurs rating violations at nearly all 86
transformers and results in 10 times higher voltage deviation, while our
approach incurs only 12 rating violations and maintains almost the same voltage
deviations as before the addition of solar and EVs.
</p>
<a href="http://arxiv.org/abs/2009.14458">arXiv:2009.14458</a> [<a href="http://arxiv.org/pdf/2009.14458">pdf</a>]

<h2>LEBANONUPRISING: a thorough study of Lebanese tweets. (arXiv:2009.14459v1 [cs.CL])</h2>
<h3>Reda Khalaf, Mireille Makary</h3>
<p>Recent studies showed a huge interest in social networks sentiment analysis.
Twitter, which is a microblogging service, can be a great source of information
on how the users feel about a certain topic, or what their opinion is regarding
a social, economic and even political matter. On October 17, Lebanon witnessed
the start of a revolution; the LebanonUprising hashtag became viral on Twitter.
A dataset consisting of a 100,0000 tweets was collected between 18 and 21
October. In this paper, we conducted a sentiment analysis study for the tweets
in spoken Lebanese Arabic related to the LebanonUprising hashtag using
different machine learning algorithms. The dataset was manually annotated to
measure the precision and recall metrics and to compare between the different
algorithms. Furthermore, the work completed in this paper provides two more
contributions. The first is related to building a Lebanese to Modern Standard
Arabic mapping dictionary that was used for the preprocessing of the tweets and
the second is an attempt to move from sentiment analysis to emotion detection
using emojis, and the two emotions we tried to predict were the "sarcastic" and
"funny" emotions. We built a training set from the tweets collected in October
2019 and then we used this set to predict sentiments and emotions of the tweets
we collected between May and August 2020. The analysis we conducted shows the
variation in sentiments, emotions and users between the two datasets. The
results we obtained seem satisfactory especially considering that there was no
previous or similar work done involving Lebanese Arabic tweets, to our
knowledge.
</p>
<a href="http://arxiv.org/abs/2009.14459">arXiv:2009.14459</a> [<a href="http://arxiv.org/pdf/2009.14459">pdf</a>]

<h2>Task Matters When Scanning Data Visualizations. (arXiv:2009.14465v1 [cs.HC])</h2>
<h3>Laura Matzen, Kristin Divis, Deborah Cronin, Michael Haass</h3>
<p>One of the major challenges for evaluating the effectiveness of data
visualizations and visual analytics tools arises from the fact that different
users may be using these tools for different tasks. In this paper, we present a
simple example of how different tasks lead to different patterns of attention
to the same underlying data visualizations. We argue that the general approach
used in this experiment could be applied systematically to task and feature
taxonomies that have been developed by visualization researchers. Using eye
tracking to study the impact of common tasks on how humans attend to common
types of visualizations will support a deeper understanding of visualization
cognition and the development of more robust methods for evaluating the
effectiveness of visualizations.
</p>
<a href="http://arxiv.org/abs/2009.14465">arXiv:2009.14465</a> [<a href="http://arxiv.org/pdf/2009.14465">pdf</a>]

<h2>Learning Image-adaptive 3D Lookup Tables for High Performance Photo Enhancement in Real-time. (arXiv:2009.14468v1 [eess.IV])</h2>
<h3>Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, Lei Zhang</h3>
<p>Recent years have witnessed the increasing popularity of learning based
methods to enhance the color and tone of photos. However, many existing photo
enhancement methods either deliver unsatisfactory results or consume too much
computational and memory resources, hindering their application to
high-resolution images (usually with more than 12 megapixels) in practice. In
this paper, we learn image-adaptive 3-dimensional lookup tables (3D LUTs) to
achieve fast and robust photo enhancement. 3D LUTs are widely used for
manipulating color and tone of photos, but they are usually manually tuned and
fixed in camera imaging pipeline or photo editing tools. We, for the first time
to our best knowledge, propose to learn 3D LUTs from annotated data using
pairwise or unpaired learning. More importantly, our learned 3D LUT is
image-adaptive for flexible photo enhancement. We learn multiple basis 3D LUTs
and a small convolutional neural network (CNN) simultaneously in an end-to-end
manner. The small CNN works on the down-sampled version of the input image to
predict content-dependent weights to fuse the multiple basis 3D LUTs into an
image-adaptive one, which is employed to transform the color and tone of source
images efficiently. Our model contains less than 600K parameters and takes less
than 2 ms to process an image of 4K resolution using one Titan RTX GPU. While
being highly efficient, our model also outperforms the state-of-the-art photo
enhancement methods by a large margin in terms of PSNR, SSIM and a color
difference metric on two publically available benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2009.14468">arXiv:2009.14468</a> [<a href="http://arxiv.org/pdf/2009.14468">pdf</a>]

<h2>PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v1 [cs.LG])</h2>
<h3>Justin K. Terry, Benjamin Black, Ananth Hari, Luis Santos, Clemens Dieffendahl, Niall L. Williams, Yashas Lokesh, Caroline Horsch, Praveen Ravi</h3>
<p>OpenAI's Gym library contains a large, diverse set of environments that are
useful benchmarks in reinforcement learning, under a single elegant Python API
(with tools to develop new compliant environments) . The introduction of this
library has proven a watershed moment for the reinforcement learning community,
because it created an accessible set of benchmark environments that everyone
could use (including wrapper important existing libraries), and because a
standardized API let RL learning methods and environments from anywhere be
trivially exchanged. This paper similarly introduces PettingZoo, a library of
diverse set of multi-agent environments under a single elegant Python API, with
tools to easily make new compliant environments.
</p>
<a href="http://arxiv.org/abs/2009.14471">arXiv:2009.14471</a> [<a href="http://arxiv.org/pdf/2009.14471">pdf</a>]

<h2>The Utility of Decorrelating Colour Spaces in Vector Quantised Variational Autoencoders. (arXiv:2009.14487v1 [cs.CV])</h2>
<h3>Arash Akbarinia, Raquel Gil-Rodr&#xed;guez, Alban Flachot, Matteo Toscani</h3>
<p>Vector quantised variational autoencoders (VQ-VAE) are characterised by three
main components: 1) encoding visual data, 2) assigning $k$ different vectors in
the so-called embedding space, and 3) decoding the learnt features. While
images are often represented in RGB colour space, the specific organisation of
colours in other spaces also offer interesting features, e.g. CIE L*a*b*
decorrelates chromaticity into opponent axes. In this article, we propose
colour space conversion, a simple quasi-unsupervised task, to enforce a network
learning structured representations. To this end, we trained several instances
of VQ-VAE whose input is an image in one colour space, and its output in
another, e.g. from RGB to CIE L*a*b* (in total five colour spaces were
considered). We examined the finite embedding space of trained networks in
order to disentangle the colour representation in VQ-VAE models. Our analysis
suggests that certain vectors encode hue and others luminance information. We
further evaluated the quality of reconstructed images at low-level using
pixel-wise colour metrics, and at high-level by inputting them to image
classification and scene segmentation networks. We conducted experiments in
three benchmark datasets: ImageNet, COCO and CelebA. Our results show, with
respect to the baseline network (whose input and output are RGB), colour
conversion to decorrelated spaces obtains 1-2 Delta-E lower colour difference
and 5-10% higher classification accuracy. We also observed that the learnt
embedding space is easier to interpret in colour opponent models.
</p>
<a href="http://arxiv.org/abs/2009.14487">arXiv:2009.14487</a> [<a href="http://arxiv.org/pdf/2009.14487">pdf</a>]

<h2>Detecting Autism Spectrum Disorder using Machine Learning. (arXiv:2009.14499v1 [cs.LG])</h2>
<h3>Md Delowar Hossain, Muhammad Ashad Kabir, Adnan Anwar, Md Zahidul Islam</h3>
<p>Autism Spectrum Disorder (ASD), which is a neuro development disorder, is
often accompanied by sensory issues such an over sensitivity or under
sensitivity to sounds and smells or touch. Although its main cause is genetics
in nature, early detection and treatment can help to improve the conditions. In
recent years, machine learning based intelligent diagnosis has been evolved to
complement the traditional clinical methods which can be time consuming and
expensive. The focus of this paper is to find out the most significant traits
and automate the diagnosis process using available classification techniques
for improved diagnosis purpose. We have analyzed ASD datasets of Toddler,
Child, Adolescent and Adult. We determine the best performing classifier for
these binary datasets using the evaluation metrics recall, precision,
F-measures and classification errors. Our finding shows that Sequential minimal
optimization (SMO) based Support Vector Machines (SVM) classifier outperforms
all other benchmark machine learning algorithms in terms of accuracy during the
detection of ASD cases and produces less classification errors compared to
other algorithms. Also, we find that Relief Attributes algorithm is the best to
identify the most significant attributes in ASD datasets.
</p>
<a href="http://arxiv.org/abs/2009.14499">arXiv:2009.14499</a> [<a href="http://arxiv.org/pdf/2009.14499">pdf</a>]

<h2>Multi-Pen Robust Robotic 3D Drawing Using Closed-Loop Planning. (arXiv:2009.14501v1 [cs.RO])</h2>
<h3>Ruishuang Liu, Weiwei Wan, Keisuke Koyama, Kensuke Harada</h3>
<p>This paper develops a flexible and robust robotic system for autonomous
drawing on 3D surfaces. The system takes 2D drawing strokes and a 3D target
surface (mesh or point clouds) as input. It maps the 2D strokes onto the 3D
surface and generates a robot motion to draw the mapped strokes using visual
recognition, grasp pose reasoning, and motion planning. The system is flexible
compared to conventional robotic drawing systems as we do not fix drawing tools
to the end of a robot arm. Instead, a robot selects drawing tools using a
vision system and holds drawing tools for painting using its hand. Meanwhile,
with the flexibility, the system has high robustness thanks to the following
crafts: First, a high-quality mapping method is developed to minimize
deformation in the strokes. Second, visual detection is used to re-estimate the
drawing tool's pose before executing each drawing motion. Third, force control
is employed to avoid noisy visual detection and calibration, and ensure a firm
touch between the pen tip and a target surface. Fourth, error detection and
recovery are implemented to deal with unexpected problems. The planning and
executions are performed in a closed-loop manner until the strokes are
successfully drawn. We evaluate the system and analyze the necessity of the
various crafts using different real-word tasks. The results show that the
proposed system is flexible and robust to generate a robot motion from picking
and placing the pens to successfully drawing 3D strokes on given surfaces.
</p>
<a href="http://arxiv.org/abs/2009.14501">arXiv:2009.14501</a> [<a href="http://arxiv.org/pdf/2009.14501">pdf</a>]

<h2>Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks. (arXiv:2009.14502v1 [cs.LG])</h2>
<h3>Yoonho Boo, Sungho Shin, Jungwook Choi, Wonyong Sung</h3>
<p>The quantization of deep neural networks (QDNNs) has been actively studied
for deployment in edge devices. Recent studies employ the knowledge
distillation (KD) method to improve the performance of quantized networks. In
this study, we propose stochastic precision ensemble training for QDNNs (SPEQ).
SPEQ is a knowledge distillation training scheme; however, the teacher is
formed by sharing the model parameters of the student network. We obtain the
soft labels of the teacher by changing the bit precision of the activation
stochastically at each layer of the forward-pass computation. The student model
is trained with these soft labels to reduce the activation quantization noise.
The cosine similarity loss is employed, instead of the KL-divergence, for KD
training. As the teacher model changes continuously by random bit-precision
assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms
the existing quantization training methods in various tasks, such as image
classification, question-answering, and transfer learning without the need for
cumbersome teacher networks.
</p>
<a href="http://arxiv.org/abs/2009.14502">arXiv:2009.14502</a> [<a href="http://arxiv.org/pdf/2009.14502">pdf</a>]

<h2>Linear Matrix Factorization Embeddings for Single-objective Optimization Landscapes. (arXiv:2009.14506v1 [cs.NE])</h2>
<h3>Tome Eftimov, Gorjan Popovski, Quentin Renau, Peter Korosec, Carola Doerr</h3>
<p>Automated per-instance algorithm selection and configuration have shown
promising performances for a number of classic optimization problems, including
satisfiability, AI planning, and TSP. The techniques often rely on a set of
features that measure some characteristics of the problem instance at hand. In
the context of black-box optimization, these features have to be derived from a
set of $(x,f(x))$ samples. A number of different features have been proposed in
the literature, measuring, for example, the modality, the separability, or the
ruggedness of the instance at hand. Several of the commonly used features,
however, are highly correlated. While state-of-the-art machine learning
techniques can routinely filter such correlations, they hinder explainability
of the derived algorithm design techniques.

We therefore propose in this work to pre-process the measured (raw) landscape
features through representation learning. More precisely, we show that a linear
dimensionality reduction via matrix factorization significantly contributes
towards a better detection of correlation between different problem instances
-- a key prerequisite for successful automated algorithm design.
</p>
<a href="http://arxiv.org/abs/2009.14506">arXiv:2009.14506</a> [<a href="http://arxiv.org/pdf/2009.14506">pdf</a>]

<h2>Towards Target-Driven Visual Navigation in Indoor Scenes via Generative Imitation Learning. (arXiv:2009.14509v1 [cs.RO])</h2>
<h3>Qiaoyun Wu, Xiaoxi Gong, Kai Xu, Dinesh Manocha, Jingxuan Dong, Jun Wang</h3>
<p>We present a target-driven navigation system to improve mapless visual
navigation in indoor scenes. Our method takes a multi-view observation of a
robot and a target as inputs at each time step to provide a sequence of actions
that move the robot to the target without relying on odometry or GPS at
runtime. The system is learned by optimizing a combinational objective
encompassing three key designs. First, we propose that an agent conceives the
next observation before making an action decision. This is achieved by learning
a variational generative module from expert demonstrations. We then propose
predicting static collision in advance, as an auxiliary task to improve safety
during navigation. Moreover, to alleviate the training data imbalance problem
of termination action prediction, we also introduce a target checking module to
differentiate from augmenting navigation policy with a termination action. The
three proposed designs all contribute to the improved training data efficiency,
static collision avoidance, and navigation generalization performance,
resulting in a novel target-driven mapless navigation system. Through
experiments on a TurtleBot, we provide evidence that our model can be
integrated into a robotic system and navigate in the real world. Videos and
models can be found in the supplementary material.
</p>
<a href="http://arxiv.org/abs/2009.14509">arXiv:2009.14509</a> [<a href="http://arxiv.org/pdf/2009.14509">pdf</a>]

<h2>Uncertainty Estimation For Community Standards Violation In Online Social Networks. (arXiv:2009.14519v1 [cs.AI])</h2>
<h3>Narjes Torabi, Nimar S. Arora, Emma Yu, Kinjal Shah, Wenshun Liu, Michael Tingley</h3>
<p>Online Social Networks (OSNs) provide a platform for users to share their
thoughts and opinions with their community of friends or to the general public.
In order to keep the platform safe for all users, as well as to keep it
compliant with local laws, OSNs typically create a set of community standards
organized into policy groups, and use Machine Learning (ML) models to identify
and remove content that violates any of the policies. However, out of the
billions of content that is uploaded on a daily basis only a small fraction is
so unambiguously violating that it can be removed by the automated models.
Prevalence estimation is the task of estimating the fraction of violating
content in the residual items by sending a small sample of these items to human
labelers to get ground truth labels. This task is exceedingly hard because even
though we can easily get the ML scores or features for all of the billions of
items we can only get ground truth labels on a few thousands of these items due
to practical considerations. Indeed the prevalence can be so low that even
after a judicious choice of items to be labeled there can be many days in which
not even a single item is labeled violating. A pragmatic choice for such low
prevalence, $10^{-4}$ to $10^{-5}$, regimes is to report the upper bound, or
$97.5\%$ confidence interval, prevalence (UBP) that takes the uncertainties of
the sampling and labeling processes into account and gives a smoothed estimate.
In this work we present two novel techniques Bucketed-Beta-Binomial and a
Bucketed-Gaussian Process for this UBP task and demonstrate on real and
simulated data that it has much better coverage than the commonly used
bootstrapping technique.
</p>
<a href="http://arxiv.org/abs/2009.14519">arXiv:2009.14519</a> [<a href="http://arxiv.org/pdf/2009.14519">pdf</a>]

<h2>Embedded Emotions -- A Data Driven Approach to Learn Transferable Feature Representations from Raw Speech Input for Emotion Recognition. (arXiv:2009.14523v1 [eess.AS])</h2>
<h3>Dominik Schiller, Silvan Mertes, Elisabeth Andr&#xe9;</h3>
<p>Traditional approaches to automatic emotion recognition are relying on the
application of handcrafted features. More recently however the advent of deep
learning enabled algorithms to learn meaningful representations of input data
automatically. In this paper, we investigate the applicability of transferring
knowledge learned from large text and audio corpora to the task of automatic
emotion recognition. To evaluate the practicability of our approach, we are
taking part in this year's Interspeech ComParE Elderly Emotion Sub-Challenge,
where the goal is to classify spoken narratives of elderly people with respect
to the emotion of the speaker. Our results show that the learned feature
representations can be effectively applied for classifying emotions from spoken
language. We found the performance of the features extracted from the audio
signal to be not as consistent as those that have been extracted from the
transcripts. While the acoustic features achieved best in class results on the
development set, when compared to the baseline systems, their performance
dropped considerably on the test set of the challenge. The features extracted
from the text form, however, are showing promising results on both sets and are
outperforming the official baseline by 5.7 percentage points unweighted average
recall.
</p>
<a href="http://arxiv.org/abs/2009.14523">arXiv:2009.14523</a> [<a href="http://arxiv.org/pdf/2009.14523">pdf</a>]

<h2>Visual Semantic Multimedia Event Model for Complex Event Detection in Video Streams. (arXiv:2009.14525v1 [cs.AI])</h2>
<h3>Piyush Yadav, Edward Curry</h3>
<p>Multimedia data is highly expressive and has traditionally been very
difficult for a machine to interpret. Middleware systems such as complex event
processing (CEP) mine patterns from data streams and send notifications to
users in a timely fashion. Presently, CEP systems have inherent limitations to
process multimedia streams due to its data complexity and the lack of an
underlying structured data model. In this work, we present a visual event
specification method to enable complex multimedia event processing by creating
a semantic knowledge representation derived from low-level media streams. The
method enables the detection of high-level semantic concepts from the media
streams using an ensemble of pattern detection capabilities. The semantic model
is aligned with a multimedia CEP engine deep learning models to give
flexibility to end-users to build rules using spatiotemporal event calculus.
This enhances CEP capability to detect patterns from media streams and bridge
the semantic gap between highly expressive knowledge-centric user queries to
the low-level features of the multi-media data. We have built a small traffic
event ontology prototype to validate the approach and performance. The paper
contribution is threefold: i) we present a knowledge graph representation for
multimedia streams, ii) a hierarchical event network to detect visual patterns
from media streams and iii) define complex pattern rules for complex multimedia
event reasoning using event calculus
</p>
<a href="http://arxiv.org/abs/2009.14525">arXiv:2009.14525</a> [<a href="http://arxiv.org/pdf/2009.14525">pdf</a>]

<h2>FAN: Frequency Aggregation Network for Real Image Super-resolution. (arXiv:2009.14547v1 [eess.IV])</h2>
<h3>Yingxue Pang, Xin Li, Xin Jin, Yaojun Wu, Jianzhao Liu, Sen Liu, Zhibo Chen</h3>
<p>Single image super-resolution (SISR) aims to recover the high-resolution (HR)
image from its low-resolution (LR) input image. With the development of deep
learning, SISR has achieved great progress. However, It is still a challenge to
restore the real-world LR image with complicated authentic degradations.
Therefore, we propose FAN, a frequency aggregation network, to address the
real-world image super-resolu-tion problem. Specifically, we extract different
frequencies of the LR image and pass them to a channel attention-grouped
residual dense network (CA-GRDB) individually to output corresponding feature
maps. And then aggregating these residual dense feature maps adaptively to
recover the HR image with enhanced details and textures. We conduct extensive
experiments quantitatively and qualitatively to verify that our FAN performs
well on the real image super-resolution task of AIM 2020 challenge. According
to the released final results, our team SR-IM achieves the fourth place on the
X4 track with PSNR of 31.1735 and SSIM of 0.8728.
</p>
<a href="http://arxiv.org/abs/2009.14547">arXiv:2009.14547</a> [<a href="http://arxiv.org/pdf/2009.14547">pdf</a>]

<h2>Explainable Deep Reinforcement Learning for UAV Autonomous Navigation. (arXiv:2009.14551v1 [cs.RO])</h2>
<h3>Lei He, Aouf Nabil, Bifeng Song</h3>
<p>Modern deep reinforcement learning plays an important role to solve a wide
range of complex decision-making tasks. However, due to the use of deep neural
networks, the trained models are lacking transparency which causes distrust
from their user and hard to be used in the critical field such as self-driving
car and unmanned aerial vehicles. In this paper, an explainable deep
reinforcement learning method is proposed to deal with the multirotor obstacle
avoidance and navigation problem. Both visual and textual explanation is
provided to make the trained agent more transparency and comprehensible for
humans. Our model can provide real-time decision explanation for non-expert
users. Also, some global explanation results are provided for experts to
diagnose the learned policy. Our method is validated in the simulation
environment. The simulation result shows our proposed method can get useful
explanations to increase the user's trust to the network and also improve the
network performance.
</p>
<a href="http://arxiv.org/abs/2009.14551">arXiv:2009.14551</a> [<a href="http://arxiv.org/pdf/2009.14551">pdf</a>]

<h2>One Reflection Suffice. (arXiv:2009.14554v1 [cs.LG])</h2>
<h3>Alexander Mathiasen, Frederik Hvilsh&#xf8;j</h3>
<p>Orthogonal weight matrices are used in many areas of deep learning. Much
previous work attempt to alleviate the additional computational resources it
requires to constrain weight matrices to be orthogonal. One popular approach
utilizes *many* Householder reflections. The only practical drawback is that
many reflections cause low GPU utilization. We mitigate this final drawback by
proving that *one* reflection is sufficient, if the reflection is computed by
an auxiliary neural network.
</p>
<a href="http://arxiv.org/abs/2009.14554">arXiv:2009.14554</a> [<a href="http://arxiv.org/pdf/2009.14554">pdf</a>]

<h2>Learning Object Detection from Captions via Textual Scene Attributes. (arXiv:2009.14558v1 [cs.CV])</h2>
<h3>Achiya Jerbi, Roei Herzig, Jonathan Berant, Gal Chechik, Amir Globerson</h3>
<p>Object detection is a fundamental task in computer vision, requiring large
annotated datasets that are difficult to collect, as annotators need to label
objects and their bounding boxes. Thus, it is a significant challenge to use
cheaper forms of supervision effectively. Recent work has begun to explore
image captions as a source for weak supervision, but to date, in the context of
object detection, captions have only been used to infer the categories of the
objects in the image. In this work, we argue that captions contain much richer
information about the image, including attributes of objects and their
relations. Namely, the text represents a scene of the image, as described
recently in the literature. We present a method that uses the attributes in
this "textual scene graph" to train object detectors. We empirically
demonstrate that the resulting model achieves state-of-the-art results on
several challenging object detection datasets, outperforming recent approaches.
</p>
<a href="http://arxiv.org/abs/2009.14558">arXiv:2009.14558</a> [<a href="http://arxiv.org/pdf/2009.14558">pdf</a>]

<h2>Restoring Spatially-Heterogeneous Distortions using Mixture of Experts Network. (arXiv:2009.14563v1 [cs.CV])</h2>
<h3>Sijin Kim, Namhyuk Ahn, Kyung-Ah Sohn</h3>
<p>In recent years, deep learning-based methods have been successfully applied
to the image distortion restoration tasks. However, scenarios that assume a
single distortion only may not be suitable for many real-world applications. To
deal with such cases, some studies have proposed sequentially combined
distortions datasets. Viewing in a different point of combining, we introduce a
spatially-heterogeneous distortion dataset in which multiple corruptions are
applied to the different locations of each image. In addition, we also propose
a mixture of experts network to effectively restore a multi-distortion image.
Motivated by the multi-task learning, we design our network to have multiple
paths that learn both common and distortion-specific representations. Our model
is effective for restoring real-world distortions and we experimentally verify
that our method outperforms other models designed to manage both single
distortion and multiple distortions.
</p>
<a href="http://arxiv.org/abs/2009.14563">arXiv:2009.14563</a> [<a href="http://arxiv.org/pdf/2009.14563">pdf</a>]

<h2>Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text. (arXiv:2009.14578v1 [cs.CL])</h2>
<h3>Shaoxiong Ji, Erik Cambria, Pekka Marttinen</h3>
<p>Medical code assignment, which predicts medical codes from clinical texts, is
a fundamental task of intelligent medical information systems. The emergence of
deep models in natural language processing has boosted the development of
automatic assignment methods. However, recent advanced neural architectures
with flat convolutions or multi-channel feature concatenation ignore the
sequential causal constraint within a text sequence and may not learn
meaningful clinical text representations, especially for lengthy clinical notes
with long-term sequential dependency. This paper proposes a Dilated
Convolutional Attention Network (DCAN), integrating dilated convolutions,
residual connections, and label attention, for medical code assignment. It
adopts dilated convolutions to capture complex medical patterns with a
receptive field which increases exponentially with dilation size. Experiments
on a real-world clinical dataset empirically show that our model improves the
state of the art.
</p>
<a href="http://arxiv.org/abs/2009.14578">arXiv:2009.14578</a> [<a href="http://arxiv.org/pdf/2009.14578">pdf</a>]

<h2>EWS-GCN: Edge Weight-Shared Graph Convolutional Network for Transactional Banking Data. (arXiv:2009.14588v1 [stat.ML])</h2>
<h3>Ivan Sukharev, Valentina Shumovskaia, Kirill Fedyanin, Maxim Panov, Dmitry Berestnev</h3>
<p>In this paper, we discuss how modern deep learning approaches can be applied
to the credit scoring of bank clients. We show that information about
connections between clients based on money transfers between them allows us to
significantly improve the quality of credit scoring compared to the approaches
using information about the target client solely. As a final solution, we
develop a new graph neural network model EWS-GCN that combines ideas of graph
convolutional and recurrent neural networks via attention mechanism. The
resulting model allows for robust training and efficient processing of
large-scale data. We also demonstrate that our model outperforms the
state-of-the-art graph neural networks achieving excellent results
</p>
<a href="http://arxiv.org/abs/2009.14588">arXiv:2009.14588</a> [<a href="http://arxiv.org/pdf/2009.14588">pdf</a>]

<h2>The Role of Isomorphism Classes in Multi-Relational Datasets. (arXiv:2009.14593v1 [cs.LG])</h2>
<h3>Vijja Wichitwechkarn, Ben Day, Cristian Bodnar, Matthew Wales, Pietro Li&#xf2;</h3>
<p>Multi-interaction systems abound in nature, from colloidal suspensions to
gene regulatory circuits. These systems can produce complex dynamics and graph
neural networks have been proposed as a method to extract underlying
interactions and predict how systems will evolve. The current training and
evaluation procedures for these models through the use of synthetic
multi-relational datasets however are agnostic to interaction network
isomorphism classes, which produce identical dynamics up to initial conditions.
We extensively analyse how isomorphism class awareness affects these models,
focusing on neural relational inference (NRI) models, which are unique in
explicitly inferring interactions to predict dynamics in the unsupervised
setting. Specifically, we demonstrate that isomorphism leakage overestimates
performance in multi-relational inference and that sampling biases present in
the multi-interaction network generation process can impair generalisation. To
remedy this, we propose isomorphism-aware synthetic benchmarks for model
evaluation. We use these benchmarks to test generalisation abilities and
demonstrate the existence of a threshold sampling frequency of isomorphism
classes for successful learning. In addition, we demonstrate that isomorphism
classes can be utilised through a simple prioritisation scheme to improve model
performance, stability during training and reduce training time.
</p>
<a href="http://arxiv.org/abs/2009.14593">arXiv:2009.14593</a> [<a href="http://arxiv.org/pdf/2009.14593">pdf</a>]

<h2>Disruption in the Chinese E-Commerce During COVID-19:A Case Study of the Beidian Platform. (arXiv:2009.14605v1 [physics.soc-ph])</h2>
<h3>Yuan Yuan, Muzhi Guan, Zhilun Zhou, Sundong Kim, Meeyoung Cha, Yong Li</h3>
<p>The recent outbreak of the novel coronavirus (COVID-19) has infected millions
of citizens worldwide and claimed many lives. This pandemic has changed human
life in many ways, and the research community is putting efforts to understand
these impacts in various sectors. This paper focuses on its impact on the
Chinese market by analyzing the e-commerce behavioral changes seen at an online
shopping platform before and during the pandemic. We present how the epidemic
affected consumer behaviors via time series analysis. We find that disruptions
in the supply of essential goods needed during a disaster can be leveraged for
epidemic forecasting, and further design a hierarchical attention learning
model to predict the newly confirmed cases. Experiment results demonstrate that
our predictions outperform existing baselines, which also extends to the
long-term and province-level forecasts. We believe models like ours contribute
to better preparing for future epidemics by gaining extra time to launch
preventive steps.
</p>
<a href="http://arxiv.org/abs/2009.14605">arXiv:2009.14605</a> [<a href="http://arxiv.org/pdf/2009.14605">pdf</a>]

<h2>Improving Generalization of Deep Fault Detection Models in the Presence of Mislabeled Data. (arXiv:2009.14606v1 [cs.LG])</h2>
<h3>Katharina Rombach, Gabriel Michau, Olga Fink</h3>
<p>Mislabeled samples are ubiquitous in real-world datasets as rule-based or
expert labeling is usually based on incorrect assumptions or subject to biased
opinions. Neural networks can "memorize" these mislabeled samples and, as a
result, exhibit poor generalization. This poses a critical issue in fault
detection applications, where not only the training but also the validation
datasets are prone to contain mislabeled samples. In this work, we propose a
novel two-step framework for robust training with label noise. In the first
step, we identify outliers (including the mislabeled samples) based on the
update in the hypothesis space. In the second step, we propose different
approaches to modifying the training data based on the identified outliers and
a data augmentation technique. Contrary to previous approaches, we aim at
finding a robust solution that is suitable for real-world applications, such as
fault detection, where no clean, "noise-free" validation dataset is available.
Under an approximate assumption about the upper limit of the label noise, we
significantly improve the generalization ability of the model trained under
massive label noise.
</p>
<a href="http://arxiv.org/abs/2009.14606">arXiv:2009.14606</a> [<a href="http://arxiv.org/pdf/2009.14606">pdf</a>]

<h2>COVID-CT-MD: COVID-19 Computed Tomography (CT) Scan Dataset Applicable in Machine Learning and Deep Learning. (arXiv:2009.14623v1 [eess.IV])</h2>
<h3>Parnian Afshar, Shahin Heidarian, Nastaran Enshaei, Farnoosh Naderkhani, Moezedin Javad Rafiee, Anastasia Oikonomou, Faranak Babaki Fard, Kaveh Samimi, Konstantinos N. Plataniotis, Arash Mohammadi</h3>
<p>Novel Coronavirus (COVID-19) has drastically overwhelmed more than 200
countries affecting millions and claiming almost 1 million lives, since its
emergence in late 2019. This highly contagious disease can easily spread, and
if not controlled in a timely fashion, can rapidly incapacitate healthcare
systems. The current standard diagnosis method, the Reverse Transcription
Polymerase Chain Reaction (RT- PCR), is time consuming, and subject to low
sensitivity. Chest Radiograph (CXR), the first imaging modality to be used, is
readily available and gives immediate results. However, it has notoriously
lower sensitivity than Computed Tomography (CT), which can be used efficiently
to complement other diagnostic methods. This paper introduces a new COVID-19 CT
scan dataset, referred to as COVID-CT-MD, consisting of not only COVID-19
cases, but also healthy and subjects infected by Community Acquired Pneumonia
(CAP). COVID-CT-MD dataset, which is accompanied with lobe-level, slice-level
and patient-level labels, has the potential to facilitate the COVID-19
research, in particular COVID-CT-MD can assist in development of advanced
Machine Learning (ML) and Deep Neural Network (DNN) based solutions.
</p>
<a href="http://arxiv.org/abs/2009.14623">arXiv:2009.14623</a> [<a href="http://arxiv.org/pdf/2009.14623">pdf</a>]

<h2>A Traffic Light Dynamic Control Algorithm with Deep Reinforcement Learning Based on GNN Prediction. (arXiv:2009.14627v1 [cs.LG])</h2>
<h3>Xiaorong Hu, Chenguang Zhao, Gang Wang</h3>
<p>Today's intelligent traffic light control system is based on the current road
traffic conditions for traffic regulation. However, these approaches cannot
exploit the future traffic information in advance. In this paper, we propose
GPlight, a deep reinforcement learning (DRL) algorithm integrated with graph
neural network (GNN) , to relieve the traffic congestion for multi-intersection
intelligent traffic control system. In GPlight, the graph neural network (GNN)
is first used to predict the future short-term traffic flow at the
intersections. Then, the results of traffic flow prediction are used in traffic
light control, and the agent combines the predicted results with the observed
current traffic conditions to dynamically control the phase and duration of the
traffic lights at the intersection. Experiments on both synthetic and two
real-world data-sets of Hangzhou and New-York verify the effectiveness and
rationality of the GPlight algorithm.
</p>
<a href="http://arxiv.org/abs/2009.14627">arXiv:2009.14627</a> [<a href="http://arxiv.org/pdf/2009.14627">pdf</a>]

<h2>Adversarial Semi-Supervised Multi-Domain Tracking. (arXiv:2009.14635v1 [cs.CV])</h2>
<h3>Kourosh Meshgi, Maryam Sadat Mirzaei</h3>
<p>Neural networks for multi-domain learning empowers an effective combination
of information from different domains by sharing and co-learning the
parameters. In visual tracking, the emerging features in shared layers of a
multi-domain tracker, trained on various sequences, are crucial for tracking in
unseen videos. Yet, in a fully shared architecture, some of the emerging
features are useful only in a specific domain, reducing the generalization of
the learned feature representation. We propose a semi-supervised learning
scheme to separate domain-invariant and domain-specific features using
adversarial learning, to encourage mutual exclusion between them, and to
leverage self-supervised learning for enhancing the shared features using the
unlabeled reservoir. By employing these features and training dedicated layers
for each sequence, we build a tracker that performs exceptionally on different
types of videos.
</p>
<a href="http://arxiv.org/abs/2009.14635">arXiv:2009.14635</a> [<a href="http://arxiv.org/pdf/2009.14635">pdf</a>]

<h2>Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video Processing. (arXiv:2009.14639v1 [cs.CV])</h2>
<h3>Okan K&#xf6;p&#xfc;kl&#xfc;, Stefan H&#xf6;rmann, Fabian Herzog, Hakan Cevikalp, Gerhard Rigoll</h3>
<p>Convolutional Neural Networks with 3D kernels (3D CNNs) currently achieve
state-of-the-art results in video recognition tasks due to their supremacy in
extracting spatiotemporal features within video frames. There have been many
successful 3D CNN architectures surpassing the state-of-the-art results
successively. However, nearly all of them are designed to operate offline
creating several serious handicaps during online operation. Firstly,
conventional 3D CNNs are not dynamic since their output features represent the
complete input clip instead of the most recent frame in the clip. Secondly,
they are not temporal resolution-preserving due to their inherent temporal
downsampling. Lastly, 3D CNNs are constrained to be used with fixed temporal
input size limiting their flexibility. In order to address these drawbacks, we
propose dissected 3D CNNs, where the intermediate volumes of the network are
dissected and propagated over depth (time) dimension for future calculations,
substantially reducing the number of computations at online operation. For
action classification, the dissected version of ResNet models performs 74-90%
fewer computations at online operation while achieving $\sim$5% better
classification accuracy on the Kinetics-600 dataset than conventional 3D ResNet
models. Moreover, the advantages of dissected 3D CNNs are demonstrated by
deploying our approach onto several vision tasks, which consistently improved
the performance.
</p>
<a href="http://arxiv.org/abs/2009.14639">arXiv:2009.14639</a> [<a href="http://arxiv.org/pdf/2009.14639">pdf</a>]

<h2>Computational framework for real-time diagnostics and prognostics of aircraft actuation systems. (arXiv:2009.14645v1 [cs.CE])</h2>
<h3>Pier Carlo Berri, Matteo D.L. Dalla Vedova, Laura Mainini</h3>
<p>Prognostics and Health Management (PHM) are emerging approaches to product
life cycle that will maintain system safety and improve reliability, while
reducing operating and maintenance costs. This is particularly relevant for
aerospace systems, where high levels of integrity and high performances are
required at the same time. We propose a novel strategy for the nearly real-time
Fault Detection and Identification (FDI) of a dynamical assembly, and for the
estimation of Remaining Useful Life (RUL) of the system. The availability of a
timely estimate of the health status of the system will allow for an informed
adaptive planning of maintenance and a dynamical reconfiguration of the mission
profile, reducing operating costs and improving reliability. This work
addresses the three phases of the prognostic flow - namely (1) signal
acquisition, (2) Fault Detection and Identification, and (3) Remaining Useful
Life estimation - and introduces a computationally efficient procedure suitable
for real-time, on-board execution. To achieve this goal, we propose to combine
information from physical models of different fidelity with machine learning
techniques to obtain efficient representations (surrogate models) suitable for
nearly real-time applications. Additionally, we propose an importance sampling
strategy and a novel approach to model damage propagation for dynamical
systems. The methodology is assessed for the FDI and RUL estimation of an
aircraft electromechanical actuator (EMA) for secondary flight controls. The
results show that the proposed method allows for a high precision in the
evaluation of the system RUL, while outperforming common model-based techniques
in terms of computational time.
</p>
<a href="http://arxiv.org/abs/2009.14645">arXiv:2009.14645</a> [<a href="http://arxiv.org/pdf/2009.14645">pdf</a>]

<h2>Learning Hard Retrieval Cross Attention for Transformer. (arXiv:2009.14658v1 [cs.CL])</h2>
<h3>Hongfei Xu, Qiuhui Liu</h3>
<p>The Transformer translation model that based on the multi-head attention
mechanism can be parallelized easily and lead to competitive performance in
machine translation. The multi-head attention network performs the scaled
dot-product attention function in parallel, empowering the model by jointly
attending to information from different representation subspaces at different
positions. Though its advantages in parallelization, many previous works
suggest the computation of the attention mechanism is not sufficiently
efficient, especially when processing long sequences, and propose approaches to
improve its efficiency with long sentences. In this paper, we accelerate the
inference of the scaled dot-product attention in another perspective.
Specifically, instead of squeezing the sequence to attend, we simplify the
computation of the scaled dot-product attention by learning a hard retrieval
attention which only attends to one token in the sentence rather than all
tokens. Since the hard attention mechanism only attends to one position, the
matrix multiplication between attention probabilities and the value sequence in
the standard scaled dot-product attention can be replaced by a simple and
efficient retrieval operation. As a result, our hard retrieval attention
mechanism can empirically accelerate the scaled dot-product attention for both
long and short sequences by 66.5%, while performing competitively in a wide
range of machine translation tasks when using for cross attention networks.
</p>
<a href="http://arxiv.org/abs/2009.14658">arXiv:2009.14658</a> [<a href="http://arxiv.org/pdf/2009.14658">pdf</a>]

<h2>Driver Anomaly Detection: A Dataset and Contrastive Learning Approach. (arXiv:2009.14660v1 [cs.CV])</h2>
<h3>Okan K&#xf6;p&#xfc;kl&#xfc;, Jiapeng Zheng, Hang Xu, Gerhard Rigoll</h3>
<p>Distracted drivers are more likely to fail to anticipate hazards, which
result in car accidents. Therefore, detecting anomalies in drivers' actions
(i.e., any action deviating from normal driving) contains the utmost importance
to reduce driver-related accidents. However, there are unbounded many anomalous
actions that a driver can do while driving, which leads to an 'open set
recognition' problem. Accordingly, instead of recognizing a set of anomalous
actions that are commonly defined by previous dataset providers, in this work,
we propose a contrastive learning approach to learn a metric to differentiate
normal driving from anomalous driving. For this task, we introduce a new
video-based benchmark, the Driver Anomaly Detection (DAD) dataset, which
contains normal driving videos together with a set of anomalous actions in its
training set. In the test set of the DAD dataset, there are unseen anomalous
actions that still need to be winnowed out from normal driving. Our method
reaches 0.9673 AUC on the test set, demonstrating the effectiveness of the
contrastive learning approach on the anomaly detection task. Our dataset, codes
and pre-trained models are publicly available.
</p>
<a href="http://arxiv.org/abs/2009.14660">arXiv:2009.14660</a> [<a href="http://arxiv.org/pdf/2009.14660">pdf</a>]

<h2>Encode the Unseen: Predictive Video Hashing for Scalable Mid-Stream Retrieval. (arXiv:2009.14661v1 [cs.CV])</h2>
<h3>Tong Yu, Nicolas Padoy</h3>
<p>This paper tackles a new problem in computer vision: mid-stream
video-to-video retrieval. This task, which consists in searching a database for
content similar to a video right as it is playing, e.g. from a live stream,
exhibits challenging characteristics. Only the beginning part of the video is
available as query and new frames are constantly added as the video plays out.
To perform retrieval in this demanding situation, we propose an approach based
on a binary encoder that is both predictive and incremental in order to (1)
account for the missing video content at query time and (2) keep up with
repeated, continuously evolving queries throughout the streaming. In
particular, we present the first hashing framework that infers the unseen
future content of a currently playing video. Experiments on FCVID and
ActivityNet demonstrate the feasibility of this task. Our approach also yields
a significant mAP@20 performance increase compared to a baseline adapted from
the literature for this task, for instance 7.4% (2.6%) increase at 20% (50%) of
elapsed runtime on FCVID using bitcodes of size 192 bits.
</p>
<a href="http://arxiv.org/abs/2009.14661">arXiv:2009.14661</a> [<a href="http://arxiv.org/pdf/2009.14661">pdf</a>]

<h2>Facilitating Connected Autonomous Vehicle Operations Using Space-weighted Information Fusion and Deep Reinforcement Learning Based Control. (arXiv:2009.14665v1 [cs.AI])</h2>
<h3>Jiqian Dong, Sikai Chen, Yujie Li, Runjia Du, Aaron Steinfeld, Samuel Labi</h3>
<p>The connectivity aspect of connected autonomous vehicles (CAV) is beneficial
because it facilitates dissemination of traffic-related information to vehicles
through Vehicle-to-External (V2X) communication. Onboard sensing equipment
including LiDAR and camera can reasonably characterize the traffic environment
in the immediate locality of the CAV. However, their performance is limited by
their sensor range (SR). On the other hand, longer-range information is helpful
for characterizing imminent conditions downstream. By contemporaneously
coalescing the short- and long-range information, the CAV can construct
comprehensively its surrounding environment and thereby facilitate informed,
safe, and effective movement planning in the short-term (local decisions
including lane change) and long-term (route choice). In this paper, we describe
a Deep Reinforcement Learning based approach that integrates the data collected
through sensing and connectivity capabilities from other vehicles located in
the proximity of the CAV and from those located further downstream, and we use
the fused data to guide lane changing, a specific context of CAV operations. In
addition, recognizing the importance of the connectivity range (CR) to the
performance of not only the algorithm but also of the vehicle in the actual
driving environment, the paper carried out a case study. The case study
demonstrates the application of the proposed algorithm and duly identifies the
appropriate CR for each level of prevailing traffic density. It is expected
that implementation of the algorithm in CAVs can enhance the safety and
mobility associated with CAV driving operations. From a general perspective,
its implementation can provide guidance to connectivity equipment manufacturers
and CAV operators, regarding the default CR settings for CAVs or the
recommended CR setting in a given traffic environment.
</p>
<a href="http://arxiv.org/abs/2009.14665">arXiv:2009.14665</a> [<a href="http://arxiv.org/pdf/2009.14665">pdf</a>]

<h2>Transfer Learning from Monolingual ASR to Transcription-free Cross-lingual Voice Conversion. (arXiv:2009.14668v1 [eess.AS])</h2>
<h3>Che-Jui Chang</h3>
<p>Cross-lingual voice conversion (VC) is a task that aims to synthesize target
voices with the same content while source and target speakers speak in
different languages. Its challenge lies in the fact that the source and target
data are naturally non-parallel, and it is even difficult to bridge the gaps
between languages with no transcriptions provided. In this paper, we focus on
knowledge transfer from monolin-gual ASR to cross-lingual VC, in order to
address the con-tent mismatch problem. To achieve this, we first train a
monolingual acoustic model for the source language, use it to extract phonetic
features for all the speech in the VC dataset, and then train a Seq2Seq
conversion model to pre-dict the mel-spectrograms. We successfully address
cross-lingual VC without any transcription or language-specific knowledge for
foreign speech. We experiment this on Voice Conversion Challenge 2020 datasets
and show that our speaker-dependent conversion model outperforms the zero-shot
baseline, achieving MOS of 3.83 and 3.54 in speech quality and speaker
similarity for cross-lingual conversion. When compared to Cascade ASR-TTS
method, our proposed one significantly reduces the MOS drop be-tween intra- and
cross-lingual conversion.
</p>
<a href="http://arxiv.org/abs/2009.14668">arXiv:2009.14668</a> [<a href="http://arxiv.org/pdf/2009.14668">pdf</a>]

<h2>An Online Learning Algorithm for a Neuro-Fuzzy Classifier with Mixed-Attribute Data. (arXiv:2009.14670v1 [cs.LG])</h2>
<h3>Thanh Tung Khuat, Bogdan Gabrys</h3>
<p>General fuzzy min-max neural network (GFMMNN) is one of the efficient
neuro-fuzzy systems for data classification. However, one of the downsides of
its original learning algorithms is the inability to handle and learn from the
mixed-attribute data. While categorical features encoding methods can be used
with the GFMMNN learning algorithms, they exhibit a lot of shortcomings. Other
approaches proposed in the literature are not suitable for on-line learning as
they require entire training data available in the learning phase. With the
rapid change in the volume and velocity of streaming data in many application
areas, it is increasingly required that the constructed models can learn and
adapt to the continuous data changes in real-time without the need for their
full retraining or access to the historical data. This paper proposes an
extended online learning algorithm for the GFMMNN. The proposed method can
handle the datasets with both continuous and categorical features. The
extensive experiments confirmed superior and stable classification performance
of the proposed approach in comparison to other relevant learning algorithms
for the GFMM model.
</p>
<a href="http://arxiv.org/abs/2009.14670">arXiv:2009.14670</a> [<a href="http://arxiv.org/pdf/2009.14670">pdf</a>]

<h2>Global convergence of Negative Correlation Extreme Learning Machine. (arXiv:2009.14695v1 [cs.LG])</h2>
<h3>Carlos Perales-Gonz&#xe1;lez</h3>
<p>Ensemble approaches introduced in the Extreme Learning Machine (ELM)
literature mainly come from methods that relies on data sampling procedures,
under the assumption that the training data are heterogeneously enough to set
up diverse base learners. To overcome this assumption, it was proposed an ELM
ensemble method based on the Negative Correlation Learning (NCL) framework,
called Negative Correlation Extreme Learning Machine (NCELM). This model works
in two stages: i) different ELMs are generated as base learners with random
weights in the hidden layer, and ii) a NCL penalty term with the information of
the ensemble prediction is introduced in each ELM minimization problem,
updating the base learners, iii) second step is iterated until the ensemble
converges.

Although this NCL ensemble method was validated by an experimental study with
multiple benchmark datasets, no information was given on the conditions about
this convergence. This paper mathematically presents the sufficient conditions
to guarantee the global convergence of NCELM. The update of the ensemble in
each iteration is defined as a contraction mapping function, and through Banach
theorem, global convergence of the ensemble is proved.
</p>
<a href="http://arxiv.org/abs/2009.14695">arXiv:2009.14695</a> [<a href="http://arxiv.org/pdf/2009.14695">pdf</a>]

<h2>Where Does Trust Break Down? A Quantitative Trust Analysis of Deep Neural Networks via Trust Matrix and Conditional Trust Densities. (arXiv:2009.14701v1 [cs.LG])</h2>
<h3>Andrew Hryniowski, Xiao Yu Wang, Alexander Wong</h3>
<p>The advances and successes in deep learning in recent years have led to
considerable efforts and investments into its widespread ubiquitous adoption
for a wide variety of applications, ranging from personal assistants and
intelligent navigation to search and product recommendation in e-commerce. With
this tremendous rise in deep learning adoption comes questions about the
trustworthiness of the deep neural networks that power these applications.
Motivated to answer such questions, there has been a very recent interest in
trust quantification. In this work, we introduce the concept of trust matrix, a
novel trust quantification strategy that leverages the recently introduced
question-answer trust metric by Wong et al. to provide deeper, more detailed
insights into where trust breaks down for a given deep neural network given a
set of questions. More specifically, a trust matrix defines the expected
question-answer trust for a given actor-oracle answer scenario, allowing one to
quickly spot areas of low trust that needs to be addressed to improve the
trustworthiness of a deep neural network. The proposed trust matrix is simple
to calculate, humanly interpretable, and to the best of the authors' knowledge
is the first to study trust at the actor-oracle answer level. We further extend
the concept of trust densities with the notion of conditional trust densities.
We experimentally leverage trust matrices to study several well-known deep
neural network architectures for image recognition, and further study the trust
density and conditional trust densities for an interesting actor-oracle answer
scenario. The results illustrate that trust matrices, along with conditional
trust densities, can be useful tools in addition to the existing suite of trust
quantification metrics for guiding practitioners and regulators in creating and
certifying deep learning solutions for trusted operation.
</p>
<a href="http://arxiv.org/abs/2009.14701">arXiv:2009.14701</a> [<a href="http://arxiv.org/pdf/2009.14701">pdf</a>]

<h2>S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency. (arXiv:2009.14711v1 [cs.RO])</h2>
<h3>Mel Vecerik, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile Pevceviciute, Thomas Roth&#xf6;rl, Christopher Schuster, Raia Hadsell, Lourdes Agapito, Jonathan Scholz</h3>
<p>A robot's ability to act is fundamentally constrained by what it can
perceive. Many existing approaches to visual representation learning utilize
general-purpose training criteria, e.g. image reconstruction, smoothness in
latent space, or usefulness for control, or else make use of large datasets
annotated with specific features (bounding boxes, segmentations, etc.).
However, both approaches often struggle to capture the fine-detail required for
precision tasks on specific objects, e.g. grasping and mating a plug and
socket. We argue that these difficulties arise from a lack of geometric
structure in these models. In this work we advocate semantic 3D keypoints as a
visual representation, and present a semi-supervised training objective that
can allow instance or category-level keypoints to be trained to 1-5
millimeter-accuracy with minimal supervision. Furthermore, unlike local
texture-based approaches, our model integrates contextual information from a
large area and is therefore robust to occlusion, noise, and lack of discernible
texture. We demonstrate that this ability to locate semantic keypoints enables
high level scripting of human understandable behaviours. Finally we show that
these keypoints provide a good way to define reward functions for reinforcement
learning and are a good representation for training agents.
</p>
<a href="http://arxiv.org/abs/2009.14711">arXiv:2009.14711</a> [<a href="http://arxiv.org/pdf/2009.14711">pdf</a>]

<h2>Deep Learning-based Pipeline for Module Power Prediction from EL Measurements. (arXiv:2009.14712v1 [cs.CV])</h2>
<h3>Mathis Hoffmann, Claudia Buerhop-Lutz, Luca Reeb, Tobias Pickel, Thilo Winkler, Bernd Doll, Tobias W&#xfc;rfl, Ian Marius Peters, Christoph Brabec, Andreas Maier, Vincent Christlein</h3>
<p>Automated inspection plays an important role in monitoring large-scale
photovoltaic power plants. Commonly, electroluminescense measurements are used
to identify various types of defects on solar modules but have not been used to
determine the power of a module. However, knowledge of the power at maximum
power point is important as well, since drops in the power of a single module
can affect the performance of an entire string. By now, this is commonly
determined by measurements that require to discontact or even dismount the
module, rendering a regular inspection of individual modules infeasible. In
this work, we bridge the gap between electroluminescense measurements and the
power determination of a module. We compile a large dataset of 719
electroluminescense measurementsof modules at various stages of degradation,
especially cell cracks and fractures, and the corresponding power at maximum
power point. Here,we focus on inactive regions and cracks as the predominant
type of defect. We set up a baseline regression model to predict the power from
electroluminescense measurements with a mean absolute error of 9.0+/-3.7W
(4.0+/-8.4%). Then, we show that deep-learning can be used to train a model
that performs significantly better (7.3+/-2.7W or 3.2+/-6.5%). With this work,
we aim to open a new research topic. Therefore, we publicly release the
dataset, the code and trained models to empower other researchers to compare
against our results. Finally, we present a thorough evaluation of certain
boundary conditions like the dataset size and an automated preprocessing
pipeline for on-site measurements showing multiple modules at once.
</p>
<a href="http://arxiv.org/abs/2009.14712">arXiv:2009.14712</a> [<a href="http://arxiv.org/pdf/2009.14712">pdf</a>]

<h2>Learning Rewards from Linguistic Feedback. (arXiv:2009.14715v1 [cs.AI])</h2>
<h3>Theodore R. Sumers, Mark K. Ho, Robert D. Hawkins, Karthik Narasimhan, Thomas L. Griffiths</h3>
<p>We explore unconstrained natural language feedback as a learning signal for
artificial agents. Humans use rich and varied language to teach, yet most prior
work on interactive learning from language assumes a particular form of input
(e.g. commands). We propose a general framework which does not make this
assumption. We decompose linguistic feedback into two components: a grounding
to $\textit{features}$ of a Markov decision process and $\textit{sentiment}$
about those features. We then perform an analogue of inverse reinforcement
learning, regressing the teacher's sentiment on the features to infer their
latent reward function. To evaluate our approach, we first collect a corpus of
teaching behavior in a cooperative task where both teacher and learner are
human. We use our framework to implement two artificial learners: a simple
"literal" model and a "pragmatic" model with additional inductive biases. We
baseline these with a neural network trained end-to-end to predict latent
rewards. We then repeat our initial experiment pairing human teachers with our
models. We find our "literal" and "pragmatic" models successfully learn from
live human feedback and offer statistically-significant performance gains over
the end-to-end baseline, with the "pragmatic" model approaching human
performance on the task. Inspection reveals the end-to-end network learns
representations similar to our models, suggesting they reflect emergent
properties of the data. Our work thus provides insight into the information
structure of naturalistic linguistic feedback as well as methods to leverage it
for reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2009.14715">arXiv:2009.14715</a> [<a href="http://arxiv.org/pdf/2009.14715">pdf</a>]

<h2>RDSGAN: Rank-based Distant Supervision Relation Extraction with Generative Adversarial Framework. (arXiv:2009.14722v1 [cs.CL])</h2>
<h3>Guoqing Luo, Jiaxin Pan, Min Peng</h3>
<p>Distant supervision has been widely used for relation extraction but suffers
from noise labeling problem. Neural network models are proposed to denoise with
attention mechanism but cannot eliminate noisy data due to its non-zero
weights. Hard decision is proposed to remove wrongly-labeled instances from the
positive set though causes loss of useful information contained in removed
instances. In this paper, we propose a novel generative neural framework named
RDSGAN (Rank-based Distant Supervision GAN) which automatically generates valid
instances for distant supervision relation extraction. Our framework combines
soft attention and hard decision to learn the distribution of true positive
instances via adversarial training and selects valid instances conforming to
the distribution via rank-based distant supervision, which addresses the false
positive problem. Experimental results show the superiority of our framework
over strong baselines.
</p>
<a href="http://arxiv.org/abs/2009.14722">arXiv:2009.14722</a> [<a href="http://arxiv.org/pdf/2009.14722">pdf</a>]

<h2>Learning and Strongly Truthful Multi-Task Peer Prediction: A Variational Approach. (arXiv:2009.14730v1 [cs.GT])</h2>
<h3>Grant Schoenebeck, Fang-Yi Yu</h3>
<p>Peer prediction mechanisms incentivize agents to truthfully report their
signals even in the absence of verification by comparing agents' reports with
those of their peers. In the detail-free multi-task setting, agents respond to
multiple independent and identically distributed tasks, and the mechanism does
not know the prior distribution of agents' signals. The goal is to provide an
$\epsilon$-strongly truthful mechanism where truth-telling rewards agents
"strictly" more than any other strategy profile (with $\epsilon$ additive
error), and to do so while requiring as few tasks as possible. We design a
family of mechanisms with a scoring function that maps a pair of reports to a
score. The mechanism is strongly truthful if the scoring function is "prior
ideal," and $\epsilon$-strongly truthful as long as the scoring function is
sufficiently close to the ideal one. This reduces the above mechanism design
problem to a learning problem -- specifically learning an ideal scoring
function. We leverage this reduction to obtain the following three results. 1)
We show how to derive good bounds on the number of tasks required for different
types of priors. Our reduction applies to myriad continuous signal space
settings. This is the first peer-prediction mechanism on continuous signals
designed for the multi-task setting. 2) We show how to turn a soft-predictor of
an agent's signals (given the other agents' signals) into a mechanism. This
allows the practical use of machine learning algorithms that give good results
even when many agents provide noisy information. 3) For finite signal spaces,
we obtain $\epsilon$-strongly truthful mechanisms on any stochastically
relevant prior, which is the maximal possible prior. In contrast, prior work
only achieves a weaker notion of truthfulness (informed truthfulness) or
requires stronger assumptions on the prior.
</p>
<a href="http://arxiv.org/abs/2009.14730">arXiv:2009.14730</a> [<a href="http://arxiv.org/pdf/2009.14730">pdf</a>]

<h2>ResGCN: Attention-based Deep Residual Modeling for Anomaly Detection on Attributed Networks. (arXiv:2009.14738v1 [cs.LG])</h2>
<h3>Yulong Pei, Tianjin Huang, Werner van Ipenburg, Mykola Pechenizkiy</h3>
<p>Effectively detecting anomalous nodes in attributed networks is crucial for
the success of many real-world applications such as fraud and intrusion
detection. Existing approaches have difficulties with three major issues:
sparsity and nonlinearity capturing, residual modeling, and network smoothing.
We propose Residual Graph Convolutional Network (ResGCN), an attention-based
deep residual modeling approach that can tackle these issues: modeling the
attributed networks with GCN allows to capture the sparsity and nonlinearity;
utilizing a deep neural network allows to directly learn residual from the
input, and a residual-based attention mechanism reduces the adverse effect from
anomalous nodes and prevents over-smoothing. Extensive experiments on several
real-world attributed networks demonstrate the effectiveness of ResGCN in
detecting anomalies.
</p>
<a href="http://arxiv.org/abs/2009.14738">arXiv:2009.14738</a> [<a href="http://arxiv.org/pdf/2009.14738">pdf</a>]

<h2>Multi-channel Deep 3D Face Recognition. (arXiv:2009.14743v1 [cs.CV])</h2>
<h3>Zhiqian You, Tingting Yang, Miao Jin</h3>
<p>Face recognition has been of great importance in many applications as a
biometric for its throughput, convenience, and non-invasiveness. Recent
advancements in deep Convolutional Neural Network (CNN) architectures have
boosted significantly the performance of face recognition based on
two-dimensional (2D) facial texture images and outperformed the previous state
of the art using conventional methods. However, the accuracy of 2D face
recognition is still challenged by the change of pose, illumination, make-up,
and expression. On the other hand, the geometric information contained in
three-dimensional (3D) face data has the potential to overcome the fundamental
limitations of 2D face data.

We propose a multi-Channel deep 3D face network for face recognition based on
3D face data. We compute the geometric information of a 3D face based on its
piecewise-linear triangular mesh structure and then conformally flatten
geometric information along with the color from 3D to 2D plane to leverage the
state-of-the-art deep CNN architectures. We modify the input layer of the
network to take images with nine channels instead of three only such that more
geometric information can be explicitly fed to it. We pre-train the network
using images from the VGG-Face \cite{Parkhi2015} and then fine-tune it with the
generated multi-channel face images. The face recognition accuracy of the
multi-Channel deep 3D face network has achieved 98.6. The experimental results
also clearly show that the network performs much better when a 9-channel image
is flattened to plane based on the conformal map compared with the orthographic
projection.
</p>
<a href="http://arxiv.org/abs/2009.14743">arXiv:2009.14743</a> [<a href="http://arxiv.org/pdf/2009.14743">pdf</a>]

<h2>Enhanced Standard Compatible Image Compression Framework based on Auxiliary Codec Networks. (arXiv:2009.14754v1 [eess.IV])</h2>
<h3>Hanbin Son, Taeoh Kim, Hyeongmin Lee, Sangyoun Lee</h3>
<p>To enhance image compression performance, recent deep neural network-based
research can be divided into three categories: a learnable codec, a
postprocessing network, and a compact representation network. The learnable
codec has been designed for an end-to-end learning beyond the conventional
compression modules. The postprocessing network increases the quality of
decoded images using an example-based learning. The compact representation
network is learned to reduce the capacity of an input image to reduce the
bitrate while keeping the quality of the decoded image. However, these
approaches are not compatible with the existing codecs or not optimal to
increase the coding efficiency. Specifically, it is difficult to achieve
optimal learning in the previous studies using the compact representation
network, due to the inaccurate consideration of the codecs. In this paper, we
propose a novel standard compatible image compression framework based on
Auxiliary Codec Networks (ACNs). ACNs are designed to imitate image degradation
operations of the existing codec, which delivers more accurate gradients to the
compact representation network. Therefore, the compact representation and the
postprocessing networks can be learned effectively and optimally. We
demonstrate that our proposed framework based on JPEG and High Efficiency Video
Coding (HEVC) standard substantially outperforms existing image compression
algorithms in a standard compatible manner.
</p>
<a href="http://arxiv.org/abs/2009.14754">arXiv:2009.14754</a> [<a href="http://arxiv.org/pdf/2009.14754">pdf</a>]

<h2>Attention-Aware Noisy Label Learning for Image Classification. (arXiv:2009.14757v1 [cs.CV])</h2>
<h3>Zhenzhen Wang, Chunyan Xu, Yap-Peng Tan, Junsong Yuan</h3>
<p>Deep convolutional neural networks (CNNs) learned on large-scale labeled
samples have achieved remarkable progress in computer vision, such as
image/video classification. The cheapest way to obtain a large body of labeled
visual data is to crawl from websites with user-supplied labels, such as
Flickr. However, these samples often tend to contain incorrect labels (i.e.
noisy labels), which will significantly degrade the network performance. In
this paper, the attention-aware noisy label learning approach ($A^2NL$) is
proposed to improve the discriminative capability of the network trained on
datasets with potential label noise. Specifically, a Noise-Attention model,
which contains multiple noise-specific units, is designed to better capture
noisy information. Each unit is expected to learn a specific noisy distribution
for a subset of images so that different disturbances are more precisely
modeled. Furthermore, a recursive learning process is introduced to strengthen
the learning ability of the attention network by taking advantage of the
learned high-level knowledge. To fully evaluate the proposed method, we conduct
experiments from two aspects: manually flipped label noise on large-scale image
classification datasets, including CIFAR-10, SVHN; and real-world label noise
on an online crawled clothing dataset with multiple attributes. The superior
results over state-of-the-art methods validate the effectiveness of our
proposed approach.
</p>
<a href="http://arxiv.org/abs/2009.14757">arXiv:2009.14757</a> [<a href="http://arxiv.org/pdf/2009.14757">pdf</a>]

<h2>Graph-based Heuristic Search for Module Selection Procedure in Neural Module Network. (arXiv:2009.14759v1 [cs.AI])</h2>
<h3>Yuxuan Wu, Hideki Nakayama</h3>
<p>Neural Module Network (NMN) is a machine learning model for solving the
visual question answering tasks. NMN uses programs to encode modules'
structures, and its modularized architecture enables it to solve logical
problems more reasonably. However, because of the non-differentiable procedure
of module selection, NMN is hard to be trained end-to-end. To overcome this
problem, existing work either included ground-truth program into training data
or applied reinforcement learning to explore the program. However, both of
these methods still have weaknesses. In consideration of this, we proposed a
new learning framework for NMN. Graph-based Heuristic Search is the algorithm
we proposed to discover the optimal program through a heuristic search on the
data structure named Program Graph. Our experiments on FigureQA and CLEVR
dataset show that our methods can realize the training of NMN without
ground-truth programs and achieve superior efficiency over existing
reinforcement learning methods in program exploration.
</p>
<a href="http://arxiv.org/abs/2009.14759">arXiv:2009.14759</a> [<a href="http://arxiv.org/pdf/2009.14759">pdf</a>]

<h2>Joint Contrastive Learning with Infinite Possibilities. (arXiv:2009.14776v1 [cs.CV])</h2>
<h3>Qi Cai, Yu Wang, Yingwei Pan, Ting Yao, Tao Mei</h3>
<p>This paper explores useful modifications of the recent development in
contrastive learning via novel probabilistic modeling. We derive a particular
form of contrastive loss named Joint Contrastive Learning (JCL). JCL implicitly
involves the simultaneous learning of an infinite number of query-key pairs,
which poses tighter constraints when searching for invariant features. We
derive an upper bound on this formulation that allows analytical solutions in
an end-to-end training manner. While JCL is practically effective in numerous
computer vision applications, we also theoretically unveil the certain
mechanisms that govern the behavior of JCL. We demonstrate that the proposed
formulation harbors an innate agency that strongly favors similarity within
each instance-specific class, and therefore remains advantageous when searching
for discriminative features among distinct instances. We evaluate these
proposals on multiple benchmarks, demonstrating considerable improvements over
existing algorithms. Code is publicly available at:
https://github.com/caiqi/Joint-Contrastive-Learning.
</p>
<a href="http://arxiv.org/abs/2009.14776">arXiv:2009.14776</a> [<a href="http://arxiv.org/pdf/2009.14776">pdf</a>]

<h2>HetSeq: Distributed GPU Training on Heterogeneous Infrastructure. (arXiv:2009.14783v1 [cs.DC])</h2>
<h3>Yifan Ding, Nicholas Botzer, Tim Weninger</h3>
<p>Modern deep learning systems like PyTorch and Tensorflow are able to train
enormous models with billions (or trillions) of parameters on a distributed
infrastructure. These systems require that the internal nodes have the same
memory capacity and compute performance. Unfortunately, most organizations,
especially universities, have a piecemeal approach to purchasing computer
systems resulting in a heterogeneous infrastructure, which cannot be used to
compute large models. The present work describes HetSeq, a software package
adapted from the popular PyTorch package that provides the capability to train
large neural network models on heterogeneous infrastructure. Experiments with
transformer translation and BERT language model shows that HetSeq scales over
heterogeneous systems. HetSeq can be easily extended to other models like image
classification. Package with supported document is publicly available at
https://github.com/yifding/hetseq.
</p>
<a href="http://arxiv.org/abs/2009.14783">arXiv:2009.14783</a> [<a href="http://arxiv.org/pdf/2009.14783">pdf</a>]

<h2>Measuring Systematic Generalization in Neural Proof Generation with Transformers. (arXiv:2009.14786v1 [cs.LG])</h2>
<h3>Nicolas Gontier, Koustuv Sinha, Siva Reddy, Christopher Pal</h3>
<p>We are interested in understanding how well Transformer language models
(TLMs) can perform reasoning tasks when trained on knowledge encoded in the
form of natural language. We investigate systematic generalization abilities on
an inductive logical reasoning task in natural language, which involves
reasoning over relationships between entities grounded in first-order logical
proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to
generate logical proofs represented in natural language. We systematically test
proof generation capabilities, along with inference capabilities leveraging the
generated proofs. We observe length-generalization issues in proof generation
and inference when evaluated on longer-than-trained sequences. However, we
observe TLMs improve their generalization performance after being exposed to
longer, exhaustive proofs. In addition, we discover that TLMs are able to
generalize better using backward-chaining proofs compared to their
forward-chaining counterparts, while they find it easier to generate forward
chaining proofs. We observe that models that are not trained to generate proofs
are better at generalizing to problems based on longer proofs. This result
suggests that Transformers have efficient, yet not interpretable reasoning
strategies internally. These results also highlight the systematic
generalization issues in TLMs in the context of logical reasoning, and we
believe this work will motivate deeper inspection of their underlying reasoning
strategies.
</p>
<a href="http://arxiv.org/abs/2009.14786">arXiv:2009.14786</a> [<a href="http://arxiv.org/pdf/2009.14786">pdf</a>]

<h2>TorchRadon: Fast Differentiable Routines for Computed Tomography. (arXiv:2009.14788v1 [eess.IV])</h2>
<h3>Matteo Ronchetti</h3>
<p>This work presents TorchRadon -- an open source CUDA library which contains a
set of differentiable routines for solving computed tomography (CT)
reconstruction problems. The library is designed to help researchers working on
CT problems to combine deep learning and model-based approaches. The package is
developed as a PyTorch extension and can be seamlessly integrated into existing
deep learning training code. Compared to the existing Astra Toolbox, TorchRadon
is up to 125 faster. The operators implemented by TorchRadon allow the
computation of gradients using PyTorch backward(), and can therefore be easily
inserted inside existing neural networks architectures. Because of its speed
and GPU support, TorchRadon can also be effectively used as a fast backend for
the implementation of iterative algorithms. This paper presents the main
functionalities of the library, compares results with existing libraries and
provides examples of usage.
</p>
<a href="http://arxiv.org/abs/2009.14788">arXiv:2009.14788</a> [<a href="http://arxiv.org/pdf/2009.14788">pdf</a>]

<h2>Rethinking Attention with Performers. (arXiv:2009.14794v1 [cs.LG])</h2>
<h3>Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller</h3>
<p>We introduce Performers, Transformer architectures which can estimate regular
(softmax) full-rank-attention Transformers with provable accuracy, but using
only linear (as opposed to quadratic) space and time complexity, without
relying on any priors such as sparsity or low-rankness. To approximate softmax
attention-kernels, Performers use a novel Fast Attention Via positive
Orthogonal Random features approach (FAVOR+), which may be of independent
interest for scalable kernel methods. FAVOR+ can be also used to efficiently
model kernelizable attention mechanisms beyond softmax. This representational
power is crucial to accurately compare softmax with other kernels for the first
time on large-scale tasks, beyond the reach of regular Transformers, and
investigate optimal attention-kernels. Performers are linear architectures
fully compatible with regular Transformers and with strong theoretical
guarantees: unbiased or nearly-unbiased estimation of the attention matrix,
uniform convergence and low estimation variance. We tested Performers on a rich
set of tasks stretching from pixel-prediction through text models to protein
sequence modeling. We demonstrate competitive results with other examined
efficient sparse and dense attention methods, showcasing effectiveness of the
novel attention-learning paradigm leveraged by Performers.
</p>
<a href="http://arxiv.org/abs/2009.14794">arXiv:2009.14794</a> [<a href="http://arxiv.org/pdf/2009.14794">pdf</a>]

<h2>3D Dense Geometry-Guided Facial Expression Synthesis by Adversarial Learning. (arXiv:2009.14798v1 [cs.CV])</h2>
<h3>Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim</h3>
<p>Manipulating facial expressions is a challenging task due to fine-grained
shape changes produced by facial muscles and the lack of input-output pairs for
supervised learning. Unlike previous methods using Generative Adversarial
Networks (GAN), which rely on cycle-consistency loss or sparse geometry
(landmarks) loss for expression synthesis, we propose a novel GAN framework to
exploit 3D dense (depth and surface normals) information for expression
manipulation. However, a large-scale dataset containing RGB images with
expression annotations and their corresponding depth maps is not available. To
this end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction
model to estimate the depth and create a large-scale RGB-Depth dataset after a
manual data clean-up process. We utilise this dataset to minimise the novel
depth consistency loss via adversarial learning (note we do not have ground
truth depth maps for generated face images) and the depth categorical loss of
synthetic data on the discriminator. In addition, to improve the generalisation
and lower the bias of the depth parameters, we propose to use a novel
confidence regulariser on the discriminator side of the framework. We
extensively performed both quantitative and qualitative evaluations on two
publicly available challenging facial expression benchmarks: AffectNet and
RaFD. Our experiments demonstrate that the proposed method outperforms the
competitive baseline and existing arts by a large margin.
</p>
<a href="http://arxiv.org/abs/2009.14798">arXiv:2009.14798</a> [<a href="http://arxiv.org/pdf/2009.14798">pdf</a>]

<h2>Gradient Descent-Ascent Provably Converges to Strict Local Minmax Equilibria with a Finite Timescale Separation. (arXiv:2009.14820v1 [cs.LG])</h2>
<h3>Tanner Fiez, Lillian Ratliff</h3>
<p>We study the role that a finite timescale separation parameter $\tau$ has on
gradient descent-ascent in two-player non-convex, non-concave zero-sum games
where the learning rate of player 1 is denoted by $\gamma_1$ and the learning
rate of player 2 is defined to be $\gamma_2=\tau\gamma_1$. Existing work
analyzing the role of timescale separation in gradient descent-ascent has
primarily focused on the edge cases of players sharing a learning rate ($\tau
=1$) and the maximizing player approximately converging between each update of
the minimizing player ($\tau \rightarrow \infty$). For the parameter choice of
$\tau=1$, it is known that the learning dynamics are not guaranteed to converge
to a game-theoretically meaningful equilibria in general. In contrast, Jin et
al. (2020) showed that the stable critical points of gradient descent-ascent
coincide with the set of strict local minmax equilibria as
$\tau\rightarrow\infty$. In this work, we bridge the gap between past work by
showing there exists a finite timescale separation parameter $\tau^{\ast}$ such
that $x^{\ast}$ is a stable critical point of gradient descent-ascent for all
$\tau \in (\tau^{\ast}, \infty)$ if and only if it is a strict local minmax
equilibrium. Moreover, we provide an explicit construction for computing
$\tau^{\ast}$ along with corresponding convergence rates and results under
deterministic and stochastic gradient feedback. The convergence results we
present are complemented by a non-convergence result: given a critical point
$x^{\ast}$ that is not a strict local minmax equilibrium, then there exists a
finite timescale separation $\tau_0$ such that $x^{\ast}$ is unstable for all
$\tau\in (\tau_0, \infty)$. Finally, we empirically demonstrate on the CIFAR-10
and CelebA datasets the significant impact timescale separation has on training
performance.
</p>
<a href="http://arxiv.org/abs/2009.14820">arXiv:2009.14820</a> [<a href="http://arxiv.org/pdf/2009.14820">pdf</a>]

<h2>On Romanization for Model Transfer Between Scripts in Neural Machine Translation. (arXiv:2009.14824v1 [cs.CL])</h2>
<h3>Chantal Amrhein, Rico Sennrich</h3>
<p>Transfer learning is a popular strategy to improve the quality of
low-resource machine translation. For an optimal transfer of the embedding
layer, the child and parent model should share a substantial part of the
vocabulary. This is not the case when transferring to languages with a
different script. We explore the benefit of romanization in this scenario. Our
results show that romanization entails information loss and is thus not always
superior to simpler vocabulary transfer methods, but can improve the transfer
between related languages with different scripts. We compare two romanization
tools and find that they exhibit different degrees of information loss, which
affects translation quality. Finally, we extend romanization to the target
side, showing that this can be a successful strategy when coupled with a simple
deromanization model.
</p>
<a href="http://arxiv.org/abs/2009.14824">arXiv:2009.14824</a> [<a href="http://arxiv.org/pdf/2009.14824">pdf</a>]

<h2>Deep Reinforcement Learning for Efficient Measurement of Quantum Devices. (arXiv:2009.14825v1 [cond-mat.mes-hall])</h2>
<h3>V. Nguyen, S.B. Orbell, D.T. Lennon, H. Moon, F. Vigneau, L.C. Camenzind, L. Yu, D.M. Zumb&#xfc;hl, G.A.D. Briggs, M.A. Osborne, D. Sejdinovic, N. Ares</h3>
<p>Deep reinforcement learning is an emerging machine learning approach which
can teach a computer to learn from their actions and rewards similar to the way
humans learn from experience. It offers many advantages in automating decision
processes to navigate large parameter spaces. This paper proposes a novel
approach to the efficient measurement of quantum devices based on deep
reinforcement learning. We focus on double quantum dot devices, demonstrating
the fully automatic identification of specific transport features called bias
triangles. Measurements targeting these features are difficult to automate,
since bias triangles are found in otherwise featureless regions of the
parameter space. Our algorithm identifies bias triangles in a mean time of less
than 30 minutes, and sometimes as little as 1 minute. This approach, based on
dueling deep Q-networks, can be adapted to a broad range of devices and target
transport features. This is a crucial demonstration of the utility of deep
reinforcement learning for decision making in the measurement and operation of
quantum devices.
</p>
<a href="http://arxiv.org/abs/2009.14825">arXiv:2009.14825</a> [<a href="http://arxiv.org/pdf/2009.14825">pdf</a>]

<h2>Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. (arXiv:1710.08969v2 [cs.SD] UPDATED)</h2>
<h3>Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara</h3>
<p>This paper describes a novel text-to-speech (TTS) technique based on deep
convolutional neural networks (CNN), without use of any recurrent units.
Recurrent neural networks (RNN) have become a standard technique to model
sequential data recently, and this technique has been used in some cutting-edge
neural TTS techniques. However, training RNN components often requires a very
powerful computer, or a very long time, typically several days or weeks. Recent
other studies, on the other hand, have shown that CNN-based sequence synthesis
can be much faster than RNN-based techniques, because of high
parallelizability. The objective of this paper is to show that an alternative
neural TTS based only on CNN alleviate these economic costs of training. In our
experiment, the proposed Deep Convolutional TTS was sufficiently trained
overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while
the quality of the synthesized speech was almost acceptable.
</p>
<a href="http://arxiv.org/abs/1710.08969">arXiv:1710.08969</a> [<a href="http://arxiv.org/pdf/1710.08969">pdf</a>]

<h2>Learning from a tiny dataset of manual annotations: a teacher/student approach for surgical phase recognition. (arXiv:1812.00033v3 [cs.LG] UPDATED)</h2>
<h3>Tong Yu, Didier Mutter, Jacques Marescaux, Nicolas Padoy</h3>
<p>Vision algorithms capable of interpreting scenes from a real-time video
stream are necessary for computer-assisted surgery systems to achieve
context-aware behavior. In laparoscopic procedures one particular algorithm
needed for such systems is the identification of surgical phases, for which the
current state of the art is a model based on a CNN-LSTM. A number of previous
works using models of this kind have trained them in a fully supervised manner,
requiring a fully annotated dataset. Instead, our work confronts the problem of
learning surgical phase recognition in scenarios presenting scarce amounts of
annotated data (under 25% of all available video recordings). We propose a
teacher/student type of approach, where a strong predictor called the teacher,
trained beforehand on a small dataset of ground truth-annotated videos,
generates synthetic annotations for a larger dataset, which another model - the
student - learns from. In our case, the teacher features a novel CNN-biLSTM-CRF
architecture, designed for offline inference only. The student, on the other
hand, is a CNN-LSTM capable of making real-time predictions. Results for
various amounts of manually annotated videos demonstrate the superiority of the
new CNN-biLSTM-CRF predictor as well as improved performance from the CNN-LSTM
trained using synthetic labels generated for unannotated videos. For both
offline and online surgical phase recognition with very few annotated
recordings available, this new teacher/student strategy provides a valuable
performance improvement by efficiently leveraging the unannotated data.
</p>
<a href="http://arxiv.org/abs/1812.00033">arXiv:1812.00033</a> [<a href="http://arxiv.org/pdf/1812.00033">pdf</a>]

<h2>SeizureNet: Multi-Spectral Deep Feature Learning for Seizure Type Classification. (arXiv:1903.03232v6 [cs.LG] UPDATED)</h2>
<h3>Umar Asif, Subhrajit Roy, Jianbin Tang, Stefan Harrer</h3>
<p>Automatic classification of epileptic seizure types in electroencephalograms
(EEGs) data can enable more precise diagnosis and efficient management of the
disease. This task is challenging due to factors such as low signal-to-noise
ratios, signal artefacts, high variance in seizure semiology among epileptic
patients, and limited availability of clinical data. To overcome these
challenges, in this paper, we present SeizureNet, a deep learning framework
which learns multi-spectral feature embeddings using an ensemble architecture
for cross-patient seizure type classification. We used the recently released
TUH EEG Seizure Corpus (V1.4.0 and V1.5.2) to evaluate the performance of
SeizureNet. Experiments show that SeizureNet can reach a weighted F1 score of
up to 0.94 for seizure-wise cross validation and 0.59 for patient-wise cross
validation for scalp EEG based multi-class seizure type classification. We also
show that the high-level feature embeddings learnt by SeizureNet considerably
improve the accuracy of smaller networks through knowledge distillation for
applications with low-memory constraints.
</p>
<a href="http://arxiv.org/abs/1903.03232">arXiv:1903.03232</a> [<a href="http://arxiv.org/pdf/1903.03232">pdf</a>]

<h2>Scaling Distributed Machine Learning with In-Network Aggregation. (arXiv:1903.06701v2 [cs.DC] UPDATED)</h2>
<h3>Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, Peter Richt&#xe1;rik</h3>
<p>Training machine learning models in parallel is an increasingly important
workload. We accelerate distributed parallel training by designing a
communication primitive that uses a programmable switch dataplane to execute a
key step of the training process. Our approach, SwitchML, reduces the volume of
exchanged data by aggregating the model updates from multiple workers in the
network. We co-design the switch processing with the end-host protocols and ML
frameworks to provide an efficient solution that speeds up training by up to
5.5$\times$ for a number of real-world benchmark models.
</p>
<a href="http://arxiv.org/abs/1903.06701">arXiv:1903.06701</a> [<a href="http://arxiv.org/pdf/1903.06701">pdf</a>]

<h2>Single Image Reflection Removal with Physically-Based Training Images. (arXiv:1904.11934v2 [cs.CV] UPDATED)</h2>
<h3>Soomin Kim, Yuchi Huo, Sung-Eui Yoon</h3>
<p>Recently, deep learning-based single image reflection separation methods have
been exploited widely. To benefit the learning approach, a large number of
training image pairs (i.e., with and without reflections) were synthesized in
various ways, yet they are away from a physically-based direction. In this
paper, physically based rendering is used for faithfully synthesizing the
required training images, and a corresponding network structure and loss term
are proposed. We utilize existing RGBD/RGB images to estimate meshes, then
physically simulate the light transportation between meshes, glass, and lens
with path tracing to synthesize training data, which successfully reproduce the
spatially variant anisotropic visual effect of glass reflection. For guiding
the separation better, we additionally consider a module, backtrack network
(BT-net) for backtracking the reflections, which removes complicated ghosting,
attenuation, blurred and defocused effect of glass/lens. This enables obtaining
a priori information before having the distortion. The proposed method
considering additional a priori information with physically simulated training
data is validated with various real reflection images and shows visually
pleasant and numerical advantages compared with state-of-the-art techniques.
</p>
<a href="http://arxiv.org/abs/1904.11934">arXiv:1904.11934</a> [<a href="http://arxiv.org/pdf/1904.11934">pdf</a>]

<h2>Representation Learning on Visual-Symbolic Graphs for Video Understanding. (arXiv:1905.07385v2 [cs.CV] UPDATED)</h2>
<h3>Effrosyni Mavroudi, Benjam&#xed;n B&#xe9;jar Haro, Ren&#xe9; Vidal</h3>
<p>Events in natural videos typically arise from spatio-temporal interactions
between actors and objects and involve multiple co-occurring activities and
object classes. To capture this rich visual and semantic context, we propose
using two graphs: (1) an attributed spatio-temporal visual graph whose nodes
correspond to actors and objects and whose edges encode different types of
interactions, and (2) a symbolic graph that models semantic relationships. We
further propose a graph neural network for refining the representations of
actors, objects and their interactions on the resulting hybrid graph. Our model
goes beyond current approaches that assume nodes and edges are of the same
type, operate on graphs with fixed edge weights and do not use a symbolic
graph. In particular, our framework: a) has specialized attention-based message
functions for different node and edge types; b) uses visual edge features; c)
integrates visual evidence with label relationships; and d) performs global
reasoning in the semantic space. Experiments on challenging video understanding
tasks, such as temporal action localization on the Charades dataset, show that
the proposed method leads to state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/1905.07385">arXiv:1905.07385</a> [<a href="http://arxiv.org/pdf/1905.07385">pdf</a>]

<h2>ViterbiNet: A Deep Learning Based Viterbi Algorithm for Symbol Detection. (arXiv:1905.10750v2 [cs.LG] UPDATED)</h2>
<h3>Nir Shlezinger, Nariman Farsad, Yonina C. Eldar, Andrea J. Goldsmith</h3>
<p>Symbol detection plays an important role in the implementation of digital
receivers. In this work, we propose ViterbiNet, which is a data-driven symbol
detector that does not require channel state information (CSI). ViterbiNet is
obtained by integrating deep neural networks (DNNs) into the Viterbi algorithm.
We identify the specific parts of the Viterbi algorithm that are
channel-model-based, and design a DNN to implement only those computations,
leaving the rest of the algorithm structure intact. We then propose a
meta-learning based approach to train ViterbiNet online based on recent
decisions, allowing the receiver to track dynamic channel conditions without
requiring new training samples for every coherence block. Our numerical
evaluations demonstrate that the performance of ViterbiNet, which is ignorant
of the CSI, approaches that of the CSI-based Viterbi algorithm, and is capable
of tracking time-varying channels without needing instantaneous CSI or
additional training data. Moreover, unlike conventional Viterbi detection,
ViterbiNet is robust to CSI uncertainty, and it can be reliably implemented in
complex channel models with constrained computational burden. More broadly, our
results demonstrate the conceptual benefit of designing communication systems
to that integrate DNNs into established algorithms.
</p>
<a href="http://arxiv.org/abs/1905.10750">arXiv:1905.10750</a> [<a href="http://arxiv.org/pdf/1905.10750">pdf</a>]

<h2>Rethinking Formal Models of Partially Observable Multiagent Decision Making. (arXiv:1906.11110v2 [cs.AI] UPDATED)</h2>
<h3>Vojt&#x11b;ch Kova&#x159;&#xed;k, Martin Schmid, Neil Burch, Michael Bowling, Viliam Lis&#xfd;</h3>
<p>Multiagent decision-making in partially observable environments is usually
modelled as either an extensive-form game (EFG) in game theory or a partially
observable stochastic game (POSG) in multiagent reinforcement learning (MARL).
One issue with the current situation is that while most practical problems can
be modelled in both formalisms, the relationship of the two models is unclear,
which hinders the transfer of ideas between the two communities. A second issue
is that while EFGs have recently seen significant algorithmic progress, their
classical formalization is unsuitable for efficient presentation of the
underlying ideas, such as those around decomposition.

To solve the first issue, we introduce factored-observation stochastic games
(FOSGs), a minor modification of the POSG formalism which distinguishes between
private and public observation and thereby greatly simplifies decomposition. To
remedy the second issue, we show that FOSGs and POSGs are naturally connected
to EFGs: by "unrolling" a FOSG into its tree form, we obtain an EFG.
Conversely, any perfect-recall timeable EFG corresponds to some underlying FOSG
in this manner. Moreover, this relationship justifies several minor
modifications to the classical EFG formalization that recently appeared as an
implicit response to the model's issues with decomposition. Finally, we
illustrate the transfer of ideas between EFGs and MARL by presenting three key
EFG techniques -- counterfactual regret minimization, sequence form, and
decomposition -- in the FOSG framework.
</p>
<a href="http://arxiv.org/abs/1906.11110">arXiv:1906.11110</a> [<a href="http://arxiv.org/pdf/1906.11110">pdf</a>]

<h2>Large-Scale Traffic Signal Control Using a Novel Multi-Agent Reinforcement Learning. (arXiv:1908.03761v2 [cs.LG] UPDATED)</h2>
<h3>Xiaoqiang Wang, Liangjun Ke, Zhimin Qiao, Xinghua Chai</h3>
<p>Finding the optimal signal timing strategy is a difficult task for the
problem of large-scale traffic signal control (TSC). Multi-Agent Reinforcement
Learning (MARL) is a promising method to solve this problem. However, there is
still room for improvement in extending to large-scale problems and modeling
the behaviors of other agents for each individual agent. In this paper, a new
MARL, called Cooperative double Q-learning (Co-DQL), is proposed, which has
several prominent features. It uses a highly scalable independent double
Q-learning method based on double estimators and the UCB policy, which can
eliminate the over-estimation problem existing in traditional independent
Q-learning while ensuring exploration. It uses mean field approximation to
model the interaction among agents, thereby making agents learn a better
cooperative strategy. In order to improve the stability and robustness of the
learning process, we introduce a new reward allocation mechanism and a local
state sharing method. In addition, we analyze the convergence properties of the
proposed algorithm. Co-DQL is applied on TSC and tested on a multi-traffic
signal simulator. According to the results obtained on several traffic
scenarios, Co- DQL outperforms several state-of-the-art decentralized MARL
algorithms. It can effectively shorten the average waiting time of the vehicles
in the whole road system.
</p>
<a href="http://arxiv.org/abs/1908.03761">arXiv:1908.03761</a> [<a href="http://arxiv.org/pdf/1908.03761">pdf</a>]

<h2>In-bed Pressure-based Pose Estimation using Image Space Representation Learning. (arXiv:1908.08919v2 [cs.CV] UPDATED)</h2>
<h3>Vandad Davoodnia, Saeed Ghorbani, Ali Etemad</h3>
<p>Recent advances in deep pose estimation models have proven to be effective in
a wide range of applications such as health monitoring, sports, animations, and
robotics. However, pose estimation models fail to generalize when facing images
acquired from in-bed pressure sensing systems. In this paper, we address this
challenge by presenting a novel end-to-end framework capable of accurately
locating body parts from vague pressure data. Our method exploits the idea of
equipping an off-the-shelf pose estimator with a deep trainable neural network,
which pre-processes and prepares the pressure data for subsequent pose
estimation. Our model transforms the ambiguous pressure maps to images
containing shapes and structures similar to the common input domain of the
pre-existing pose estimation methods. As a result, we show that our model is
able to reconstruct unclear body parts, which in turn enables pose estimators
to accurately and robustly estimate the pose. We train and test our method on a
manually annotated public pressure map dataset using a combination of loss
functions. Results confirm the effectiveness of our method by the high visual
quality in the generated images and the high pose estimation rates achieved.
</p>
<a href="http://arxiv.org/abs/1908.08919">arXiv:1908.08919</a> [<a href="http://arxiv.org/pdf/1908.08919">pdf</a>]

<h2>Learning Multiparametric Biomarkers for Assessing MR-Guided Focused Ultrasound Treatment of Malignant Tumors. (arXiv:1910.10769v2 [eess.IV] UPDATED)</h2>
<h3>Blake E. Zimmerman (1,2), Sara Johnson (2), Henrik Od&#xe9;en (3), Jill Shea (4), Markus D. Foote (1,2), Nicole Winkler (4), Sarang C. Joshi (1,2), Allison Payne (3) ((1) Scientific Computing and Imaging Institute, University of Utah, (2) Department of Biomedical Engineering, University of Utah, (3) Department of Radiology and Imaging Sciences, University of Utah, (4) Department of Surgery, University of Utah)</h3>
<p>Noninvasive MR-guided focused ultrasound (MRgFUS) treatments are promising
alternatives to the surgical removal of malignant tumors. A significant
challenge is assessing the viability of treated tissue during and immediately
after MRgFUS procedures. Current clinical assessment uses the nonperfused
volume (NPV) biomarker immediately after treatment from contrast-enhanced MRI.
The NPV has variable accuracy, and the use of contrast agent prevents
continuing MRgFUS treatment if tumor coverage is inadequate. This work presents
a novel, noncontrast, learned multiparametric MR biomarker that can be used
during treatment for intratreatment assessment, validated in a VX2 rabbit tumor
model. A deep convolutional neural network was trained on noncontrast
multiparametric MR images using the NPV biomarker from follow-up MR imaging
(3-5 days after MRgFUS treatment) as the accurate label of nonviable tissue. A
novel volume-conserving registration algorithm yielded a voxel-wise correlation
between treatment and follow-up NPV, providing a rigorous validation of the
biomarker. The learned noncontrast multiparametric MR biomarker predicted the
follow-up NPV with an average DICE coefficient of 0.71, substantially
outperforming the current clinical standard (DICE coefficient = 0.53).
Noncontrast multiparametric MR imaging integrated with a deep convolutional
neural network provides a more accurate prediction of MRgFUS treatment outcome
than current contrast-based techniques.
</p>
<a href="http://arxiv.org/abs/1910.10769">arXiv:1910.10769</a> [<a href="http://arxiv.org/pdf/1910.10769">pdf</a>]

<h2>MAP-Net: Multi Attending Path Neural Network for Building Footprint Extraction from Remote Sensed Imagery. (arXiv:1910.12060v2 [cs.CV] UPDATED)</h2>
<h3>Qing Zhu, Cheng Liao, Han Hu, Xiaoming Mei, Haifeng Li</h3>
<p>Accurately and efficiently extracting building footprints from a wide range
of remote sensed imagery remains a challenge due to their complex structure,
variety of scales and diverse appearances. Existing convolutional neural
network (CNN)-based building extraction methods are complained that they cannot
detect the tiny buildings because the spatial information of CNN feature maps
are lost during repeated pooling operations of the CNN, and the large buildings
still have inaccurate segmentation edges. Moreover, features extracted by a CNN
are always partial which restricted by the size of the respective field, and
large-scale buildings with low texture are always discontinuous and holey when
extracted. This paper proposes a novel multi attending path neural network
(MAP-Net) for accurately extracting multiscale building footprints and precise
boundaries. MAP-Net learns spatial localization-preserved multiscale features
through a multi-parallel path in which each stage is gradually generated to
extract high-level semantic features with fixed resolution. Then, an attention
module adaptively squeezes channel-wise features from each path for
optimization, and a pyramid spatial pooling module captures global dependency
for refining discontinuous building footprints. Experimental results show that
MAP-Net outperforms state-of-the-art (SOTA) algorithms in boundary localization
accuracy as well as continuity of large buildings. Specifically, our method
achieved 0.68\%, 1.74\%, 1.46\% precision, and 1.50\%, 1.53\%, 0.82\% IoU score
improvement without increasing computational complexity compared with the
latest HRNetv2 on the Urban 3D, Deep Globe and WHU datasets, respectively. The
TensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.
</p>
<a href="http://arxiv.org/abs/1910.12060">arXiv:1910.12060</a> [<a href="http://arxiv.org/pdf/1910.12060">pdf</a>]

<h2>On the Shattering Coefficient of Supervised Learning Algorithms. (arXiv:1911.05461v2 [cs.LG] UPDATED)</h2>
<h3>Rodrigo Fernandes de Mello</h3>
<p>The Statistical Learning Theory (SLT) provides the theoretical background to
ensure that a supervised algorithm generalizes the mapping $f: \mathcal{X} \to
\mathcal{Y}$ given $f$ is selected from its search space bias $\mathcal{F}$.
This formal result depends on the Shattering coefficient function
$\mathcal{N}(\mathcal{F},2n)$ to upper bound the empirical risk minimization
principle, from which one can estimate the necessary training sample size to
ensure the probabilistic learning convergence and, most importantly, the
characterization of the capacity of $\mathcal{F}$, including its under and
overfitting abilities while addressing specific target problems. In this
context, we propose a new approach to estimate the maximal number of
hyperplanes required to shatter a given sample, i.e., to separate every pair of
points from one another, based on the recent contributions by Har-Peled and
Jones in the dataset partitioning scenario, and use such foundation to
analytically compute the Shattering coefficient function for both binary and
multi-class problems. As main contributions, one can use our approach to study
the complexity of the search space bias $\mathcal{F}$, estimate training sample
sizes, and parametrize the number of hyperplanes a learning algorithm needs to
address some supervised task, what is specially appealing to deep neural
networks. Experiments were performed to illustrate the advantages of our
approach while studying the search space $\mathcal{F}$ on synthetic and one toy
datasets and on two widely-used deep learning benchmarks (MNIST and CIFAR-10).
In order to permit reproducibility and the use of our approach, our source code
is made available at https://bitbucket.org/rodrigo_mello/shattering-rcode.
</p>
<a href="http://arxiv.org/abs/1911.05461">arXiv:1911.05461</a> [<a href="http://arxiv.org/pdf/1911.05461">pdf</a>]

<h2>Password-conditioned Anonymization and Deanonymization with Face Identity Transformers. (arXiv:1911.11759v4 [cs.CV] UPDATED)</h2>
<h3>Xiuye Gu, Weixin Luo, Michael S. Ryoo, Yong Jae Lee</h3>
<p>Cameras are prevalent in our daily lives, and enable many useful systems
built upon computer vision technologies such as smart cameras and home robots
for service applications. However, there is also an increasing societal concern
as the captured images/videos may contain privacy-sensitive information (e.g.,
face identity). We propose a novel face identity transformer which enables
automated photo-realistic password-based anonymization as well as
deanonymization of human faces appearing in visual data. Our face identity
transformer is trained to (1) remove face identity information after
anonymization, (2) make the recovery of the original face possible when given
the correct password, and (3) return a wrong--but photo-realistic--face given a
wrong password. Extensive experiments show that our approach enables multimodal
password-conditioned face anonymizations and deanonymizations, without
sacrificing privacy compared to existing anonymization approaches.
</p>
<a href="http://arxiv.org/abs/1911.11759">arXiv:1911.11759</a> [<a href="http://arxiv.org/pdf/1911.11759">pdf</a>]

<h2>Wide-Area Land Cover Mapping with Sentinel-1 Imagery using Deep Learning Semantic Segmentation Models. (arXiv:1912.05067v3 [eess.IV] UPDATED)</h2>
<h3>Sanja &#x160;&#x107;epanovi&#x107;, Oleg Antropov, Pekka Laurila, Vladimir Ignatenko, Jaan Praks</h3>
<p>Land cover mapping is essential for monitoring the environment and
understanding the effects of human activities on it. The automatic approaches
to land cover mapping (i.e., image segmentation) mostly used traditional
machine learning. On the natural images, deep learning has outperformed
traditional machine learning on a range of tasks, including the image
segmentation. On remote sensing images, recent studies are demonstrating
successful application of specific deep learning models or their adaptations to
particular small-scale mapping tasks (e.g., to classify wetland complexes).
However, it is not readily clear which of the existing models for natural
images are the best candidates to be taken for the particular remote sensing
task and data. In this study, we answer that question for mapping the
fundamental land cover classes using the satellite imaging radar data. We took
ESA Sentinel-1 C-band SAR images available at no cost to users as
representative data. CORINE land cover map was used as a reference, and the
models were trained to distinguish between the 5 Level-1 CORINE classes. We
selected seven among the state-of-the-art semantic segmentation models so that
they cover a diverse set of approaches. We used 14 ESA Sentinel-1 scenes
acquired during the summer season in Finland, which are representative of the
land cover in the country. Upon the benchmarking, all the models demonstrated
solid performance. The best model, FC-DenseNet (Fully Convolutional DenseNets),
achieved the overall accuracy of 90.7%. Overall, our results indicate that the
semantic segmentation models are suitable for efficient wide-area mapping using
satellite SAR imagery. Our results also provide baseline accuracy against which
the newly proposed models should be evaluated and suggest the DenseNet-based
models are the first candidate for this task.
</p>
<a href="http://arxiv.org/abs/1912.05067">arXiv:1912.05067</a> [<a href="http://arxiv.org/pdf/1912.05067">pdf</a>]

<h2>Structural plasticity on an accelerated analog neuromorphic hardware system. (arXiv:1912.12047v2 [q-bio.NC] UPDATED)</h2>
<h3>Sebastian Billaudelle, Benjamin Cramer, Mihai A. Petrovici, Korbinian Schreiber, David Kappel, Johannes Schemmel, Karlheinz Meier</h3>
<p>In computational neuroscience, as well as in machine learning, neuromorphic
devices promise an accelerated and scalable alternative to neural network
simulations. Their neural connectivity and synaptic capacity depends on their
specific design choices, but is always intrinsically limited. Here, we present
a strategy to achieve structural plasticity that optimizes resource allocation
under these constraints by constantly rewiring the pre- and gpostsynaptic
partners while keeping the neuronal fan-in constant and the connectome sparse.
In particular, we implemented this algorithm on the analog neuromorphic system
BrainScaleS-2. It was executed on a custom embedded digital processor located
on chip, accompanying the mixed-signal substrate of spiking neurons and synapse
circuits. We evaluated our implementation in a simple supervised learning
scenario, showing its ability to optimize the network topology with respect to
the nature of its training data, as well as its overall computational
efficiency.
</p>
<a href="http://arxiv.org/abs/1912.12047">arXiv:1912.12047</a> [<a href="http://arxiv.org/pdf/1912.12047">pdf</a>]

<h2>Implicit supervision for fault detection and segmentation of emerging fault types with Deep Variational Autoencoders. (arXiv:1912.12502v2 [cs.LG] UPDATED)</h2>
<h3>Manuel Arias Chao, Bryan T. Adey, Olga Fink</h3>
<p>Data-driven fault diagnostics of safety-critical systems often faces the
challenge of a complete lack of labeled data associated with faulty system
conditions (i.e., fault types) at training time. Since an unknown number and
nature of fault types can arise during deployment, data-driven fault
diagnostics in this scenario is an open-set learning problem. Most of the
algorithms for open-set diagnostics are one-class classification and
unsupervised algorithms that do not leverage all the available labeled and
unlabeled data in the learning algorithm. As a result, their fault detection
and segmentation performance (i.e., identifying and separating faults of
different types) are sub-optimal. With this work, we propose training a
variational autoencoder (VAE) with labeled and unlabeled samples while inducing
implicit supervision on the latent representation of the healthy conditions.
This, together with a modified sampling process of VAE, creates a compact and
informative latent representation that allows good detection and segmentation
of unseen fault types using existing one-class and clustering algorithms. We
refer to the proposed methodology as "knowledge induced variational autoencoder
with adaptive sampling" (KIL-AdaVAE). The fault detection and segmentation
capabilities of the proposed methodology are demonstrated in a new simulated
case study using the Advanced Geared Turbofan 30000 (AGTF30) dynamical model
under real flight conditions. In an extensive comparison, we demonstrate that
the proposed method outperforms other learning strategies (supervised learning,
supervised learning with embedding and semi-supervised learning) and deep
learning algorithms, yielding significant performance improvements on fault
detection and fault segmentation.
</p>
<a href="http://arxiv.org/abs/1912.12502">arXiv:1912.12502</a> [<a href="http://arxiv.org/pdf/1912.12502">pdf</a>]

<h2>Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience. (arXiv:2001.09219v4 [cs.HC] UPDATED)</h2>
<h3>Bhavya Ghai, Q. Vera Liao, Yunfeng Zhang, Rachel Bellamy, Klaus Mueller</h3>
<p>The wide adoption of Machine Learning technologies has created a rapidly
growing demand for people who can train ML models. Some advocated the term
"machine teacher" to refer to the role of people who inject domain knowledge
into ML models. One promising learning paradigm is Active Learning (AL), by
which the model intelligently selects instances to query the machine teacher
for labels. However, in current AL settings, the human-AI interface remains
minimal and opaque. We begin considering AI explanations as a core element of
the human-AI interface for teaching machines. When a human student learns, it
is a common pattern to present one's own reasoning and solicit feedback from
the teacher. When a ML model learns and still makes mistakes, the human teacher
should be able to understand the reasoning underlying the mistakes. When the
model matures, the machine teacher should be able to recognize its progress in
order to trust and feel confident about their teaching outcome. Toward this
vision, we propose a novel paradigm of explainable active learning (XAL), by
introducing techniques from the recently surging field of explainable AI (XAI)
into an AL setting. We conducted an empirical study comparing the model
learning outcomes, feedback content and experience with XAL, to that of
traditional AL and coactive learning (providing the model's prediction without
the explanation). Our study shows benefits of AI explanation as interfaces for
machine teaching--supporting trust calibration and enabling rich forms of
teaching feedback, and potential drawbacks--anchoring effect with the model
judgment and cognitive workload. Our study also reveals important individual
factors that mediate a machine teacher's reception to AI explanations,
including task knowledge, AI experience and need for cognition. By reflecting
on the results, we suggest future directions and design implications for XAL.
</p>
<a href="http://arxiv.org/abs/2001.09219">arXiv:2001.09219</a> [<a href="http://arxiv.org/pdf/2001.09219">pdf</a>]

<h2>Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences. (arXiv:2001.11122v3 [cs.CV] UPDATED)</h2>
<h3>Rosaura G. VidalMata, Walter J. Scheirer, Anna Kukleva, David Cox, Hilde Kuehne</h3>
<p>Understanding the structure of complex activities in untrimmed videos is a
challenging task in the area of action recognition. One problem here is that
this task usually requires a large amount of hand-annotated minute- or even
hour-long video data, but annotating such data is very time consuming and can
not easily be automated or scaled. To address this problem, this paper proposes
an approach for the unsupervised learning of actions in untrimmed video
sequences based on a joint visual-temporal embedding space. To this end, we
combine a visual embedding based on a predictive U-Net architecture with a
temporal continuous function. The resulting representation space allows
detecting relevant action clusters based on their visual as well as their
temporal appearance. The proposed method is evaluated on three standard
benchmark datasets, Breakfast Actions, INRIA YouTube Instructional Videos, and
50 Salads. We show that the proposed approach is able to provide a meaningful
visual and temporal embedding out of the visual cues present in contiguous
video frames and is suitable for the task of unsupervised temporal segmentation
of actions.
</p>
<a href="http://arxiv.org/abs/2001.11122">arXiv:2001.11122</a> [<a href="http://arxiv.org/pdf/2001.11122">pdf</a>]

<h2>Improving the Robustness of Graphs through Reinforcement Learning and Graph Neural Networks. (arXiv:2001.11279v3 [cs.LG] UPDATED)</h2>
<h3>Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi</h3>
<p>Graphs can be used to represent and reason about real world systems and a
variety of metrics have been devised to quantify their global characteristics.
An important property is robustness to failures and attacks, which is relevant
for the infrastructure and communication networks that power modern society.
Prior work on making topological modifications to a graph, e.g., adding edges,
in order to increase robustness is typically based on local and spectral
properties or a shallow search since robustness is expensive to compute
directly. However, such strategies are necessarily suboptimal.

In this work, we present RNet-DQN, an approach for constructing networks that
uses Reinforcement Learning to address improving the robustness of graphs to
random and targeted removals of nodes. In particular, the approach relies on
changes in the estimated robustness as a reward signal and Graph Neural
Networks for representing states. Experiments on synthetic and real-world
graphs show that this approach can deliver performance superior to existing
methods while being much cheaper to evaluate and generalizing to out-of-sample
graphs, as well as to larger out-of-distribution graphs in some cases. The
approach is readily applicable to optimizing other global structural properties
of graphs.
</p>
<a href="http://arxiv.org/abs/2001.11279">arXiv:2001.11279</a> [<a href="http://arxiv.org/pdf/2001.11279">pdf</a>]

<h2>Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic. (arXiv:2002.05502v2 [cs.LG] UPDATED)</h2>
<h3>Yangang Ren, Jingliang Duan, Shengbo Eben Li, Yang Guan, Qi Sun</h3>
<p>Reinforcement learning (RL) has achieved remarkable performance in numerous
sequential decision making and control tasks. However, a common problem is that
learned nearly optimal policy always overfits to the training environment and
may not be extended to situations never encountered during training. For
practical applications, the randomness of environment usually leads to some
devastating events, which should be the focus of safety-critical systems such
as autonomous driving. In this paper, we introduce the minimax formulation and
distributional framework to improve the generalization ability of RL algorithms
and develop the Minimax Distributional Soft Actor-Critic (Minimax DSAC)
algorithm. Minimax formulation aims to seek optimal policy considering the most
severe variations from environment, in which the protagonist policy maximizes
action-value function while the adversary policy tries to minimize it.
Distributional framework aims to learn a state-action return distribution, from
which we can model the risk of different returns explicitly, thereby
formulating a risk-averse protagonist policy and a risk-seeking adversarial
policy. We implement our method on the decision-making tasks of autonomous
vehicles at intersections and test the trained policy in distinct environments.
Results demonstrate that our method can greatly improve the generalization
ability of the protagonist agent to different environmental variations.
</p>
<a href="http://arxiv.org/abs/2002.05502">arXiv:2002.05502</a> [<a href="http://arxiv.org/pdf/2002.05502">pdf</a>]

<h2>Self-Adaptive Training: beyond Empirical Risk Minimization. (arXiv:2002.10319v2 [cs.LG] UPDATED)</h2>
<h3>Lang Huang, Chao Zhang, Hongyang Zhang</h3>
<p>We propose self-adaptive training---a new training algorithm that dynamically
corrects problematic training labels by model predictions without incurring
extra computational cost---to improve generalization of deep learning for
potentially corrupted training data. This problem is crucial towards robustly
learning from data that are corrupted by, e.g., label noises and
out-of-distribution samples. The standard empirical risk minimization (ERM) for
such data, however, may easily overfit noises and thus suffers from sub-optimal
performance. In this paper, we observe that model predictions can substantially
benefit the training process: self-adaptive training significantly improves
generalization over ERM under various levels of noises, and mitigates the
overfitting issue in both natural and adversarial training. We evaluate the
error-capacity curve of self-adaptive training: the test error is monotonously
decreasing w.r.t. model capacity. This is in sharp contrast to the
recently-discovered double-descent phenomenon in ERM which might be a result of
overfitting of noises. Experiments on CIFAR and ImageNet datasets verify the
effectiveness of our approach in two applications: classification with label
noise and selective classification. We release our code at
https://github.com/LayneH/self-adaptive-training.
</p>
<a href="http://arxiv.org/abs/2002.10319">arXiv:2002.10319</a> [<a href="http://arxiv.org/pdf/2002.10319">pdf</a>]

<h2>Bayesian Neural Networks With Maximum Mean Discrepancy Regularization. (arXiv:2003.00952v2 [cs.LG] UPDATED)</h2>
<h3>Jary Pomponi, Simone Scardapane, Aurelio Uncini</h3>
<p>Bayesian Neural Networks (BNNs) are trained to optimize an entire
distribution over their weights instead of a single set, having significant
advantages in terms of, e.g., interpretability, multi-task learning, and
calibration. Because of the intractability of the resulting optimization
problem, most BNNs are either sampled through Monte Carlo methods, or trained
by minimizing a suitable Evidence Lower BOund (ELBO) on a variational
approximation. In this paper, we propose a variant of the latter, wherein we
replace the Kullback-Leibler divergence in the ELBO term with a Maximum Mean
Discrepancy (MMD) estimator, inspired by recent work in variational inference.
After motivating our proposal based on the properties of the MMD term, we
proceed to show a number of empirical advantages of the proposed formulation
over the state-of-the-art. In particular, our BNNs achieve higher accuracy on
multiple benchmarks, including several image classification tasks. In addition,
they are more robust to the selection of a prior over the weights, and they are
better calibrated. As a second contribution, we provide a new formulation for
estimating the uncertainty on a given prediction, showing it performs in a more
robust fashion against adversarial attacks and the injection of noise over
their inputs, compared to more classical criteria such as the differential
entropy.
</p>
<a href="http://arxiv.org/abs/2003.00952">arXiv:2003.00952</a> [<a href="http://arxiv.org/pdf/2003.00952">pdf</a>]

<h2>Functionality-preserving Black-box Optimization of Adversarial Windows Malware. (arXiv:2003.13526v3 [cs.CR] UPDATED)</h2>
<h3>Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, Alessandro Armando</h3>
<p>Windows malware detectors based on machine learning are vulnerable to
adversarial examples, even if the attacker is only given black-box query access
to the model. The main drawback of these attacks is that: (i) they are
query-inefficient, as they rely on iteratively applying random transformations
to the input malware; and (ii) they may also require executing the adversarial
malware in a sandbox at each iteration of the optimization process, to ensure
that its intrusive functionality is preserved. In this paper, we overcome these
issues by presenting a novel family of black-box attacks that are both
query-efficient and functionality-preserving, as they rely on the injection of
benign content - which will never be executed - either at the end of the
malicious file, or within some newly-created sections. Our attacks are
formalized as a constrained minimization problem which also enables optimizing
the trade-off between the probability of evading detection and the size of the
injected payload. We empirically investigate this trade-off on two popular
static Windows malware detectors, and show that our black-box attacks can
bypass them with only few queries and small payloads, even when they only
return the predicted labels. We also evaluate whether our attacks transfer to
other commercial antivirus solutions, and surprisingly find that they can
evade, on average, more than 12 commercial antivirus engines. We conclude by
discussing the limitations of our approach, and its possible future extensions
to target malware classifiers based on dynamic analysis.
</p>
<a href="http://arxiv.org/abs/2003.13526">arXiv:2003.13526</a> [<a href="http://arxiv.org/pdf/2003.13526">pdf</a>]

<h2>Continuous Motion Planning with Temporal Logic Specifications using Deep Neural Networks. (arXiv:2004.02610v2 [cs.AI] UPDATED)</h2>
<h3>Chuanzheng Wang, Yinan Li, Stephen L. Smith, Jun Liu</h3>
<p>In this paper, we propose a model-free reinforcement learning method to
synthesize control policies for motion planning problems with continuous states
and actions. The robot is modelled as a labeled discrete-time Markov decision
process (MDP) with continuous state and action spaces. Linear temporal logics
(LTL) are used to specify high-level tasks. We then train deep neural networks
to approximate the value function and policy using an actor-critic
reinforcement learning method. The LTL specification is converted into an
annotated limit-deterministic B\"uchi automaton (LDBA) for continuously shaping
the reward so that dense rewards are available during training. A na\"ive way
of solving a motion planning problem with LTL specifications using
reinforcement learning is to sample a trajectory and then assign a high reward
for training if the trajectory satisfies the entire LTL formula. However, the
sampling complexity needed to find such a trajectory is too high when we have a
complex LTL formula for continuous state and action spaces. As a result, it is
very unlikely that we get enough reward for training if all sample trajectories
start from the initial state in the automata. In this paper, we propose a
method that samples not only an initial state from the state space, but also an
arbitrary state in the automata at the beginning of each training episode. We
test our algorithm in simulation using a car-like robot and find out that our
method can learn policies for different working configurations and LTL
specifications successfully.
</p>
<a href="http://arxiv.org/abs/2004.02610">arXiv:2004.02610</a> [<a href="http://arxiv.org/pdf/2004.02610">pdf</a>]

<h2>On the Language Neutrality of Pre-trained Multilingual Representations. (arXiv:2004.05160v4 [cs.CL] UPDATED)</h2>
<h3>Jind&#x159;ich Libovick&#xfd;, Rudolf Rosa, Alexander Fraser</h3>
<p>Multilingual contextual embeddings, such as multilingual BERT and
XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work
probed the cross-linguality of the representations indirectly using zero-shot
transfer learning on morphological and syntactic tasks. We instead investigate
the language-neutrality of multilingual contextual embeddings directly and with
respect to lexical semantics. Our results show that contextual embeddings are
more language-neutral and, in general, more informative than aligned static
word-type embeddings, which are explicitly trained for language neutrality.
Contextual embeddings are still only moderately language-neutral by default, so
we propose two simple methods for achieving stronger language neutrality:
first, by unsupervised centering of the representation for each language and
second, by fitting an explicit projection on small parallel data. Besides, we
show how to reach state-of-the-art accuracy on language identification and
match the performance of statistical methods for word alignment of parallel
sentences without using parallel data.
</p>
<a href="http://arxiv.org/abs/2004.05160">arXiv:2004.05160</a> [<a href="http://arxiv.org/pdf/2004.05160">pdf</a>]

<h2>New Protocols and Negative Results for Textual Entailment Data Collection. (arXiv:2004.11997v2 [cs.CL] UPDATED)</h2>
<h3>Samuel R. Bowman, Jennimaria Palomaki, Livio Baldini Soares, Emily Pitler</h3>
<p>Natural language inference (NLI) data has proven useful in benchmarking and,
especially, as pretraining data for tasks requiring language understanding.
However, the crowdsourcing protocol that was used to collect this data has
known issues and was not explicitly optimized for either of these purposes, so
it is likely far from ideal. We propose four alternative protocols, each aimed
at improving either the ease with which annotators can produce sound training
examples or the quality and diversity of those examples. Using these
alternatives and a fifth baseline protocol, we collect and compare five new
8.5k-example training sets. In evaluations focused on transfer learning
applications, our results are solidly negative, with models trained on our
baseline dataset yielding good transfer performance to downstream tasks, but
none of our four new methods (nor the recent ANLI) showing any improvements
over that baseline. In a small silver lining, we observe that all four new
protocols, especially those where annotators edit pre-filled text boxes, reduce
previously observed issues with annotation artifacts.
</p>
<a href="http://arxiv.org/abs/2004.11997">arXiv:2004.11997</a> [<a href="http://arxiv.org/pdf/2004.11997">pdf</a>]

<h2>HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training. (arXiv:2005.00200v2 [cs.CV] UPDATED)</h2>
<h3>Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu</h3>
<p>We present HERO, a novel framework for large-scale video+language
omni-representation learning. HERO encodes multimodal inputs in a hierarchical
structure, where local context of a video frame is captured by a Cross-modal
Transformer via multimodal fusion, and global video context is captured by a
Temporal Transformer. In addition to standard Masked Language Modeling (MLM)
and Masked Frame Modeling (MFM) objectives, we design two new pre-training
tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global
and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the
model predicts the right order of shuffled video frames. HERO is jointly
trained on HowTo100M and large-scale TV datasets to gain deep understanding of
complex social dynamics with multi-character interactions. Comprehensive
experiments demonstrate that HERO achieves new state of the art on multiple
benchmarks over Text-based Video/Video-moment Retrieval, Video Question
Answering (QA), Video-and-language Inference and Video Captioning tasks across
different domains. We also introduce two new challenging benchmarks How2QA and
How2R for Video QA and Retrieval, collected from diverse video content over
multimodalities.
</p>
<a href="http://arxiv.org/abs/2005.00200">arXiv:2005.00200</a> [<a href="http://arxiv.org/pdf/2005.00200">pdf</a>]

<h2>Differential Machine Learning. (arXiv:2005.02347v4 [q-fin.CP] UPDATED)</h2>
<h3>Brian Huge, Antoine Savine</h3>
<p>Differential machine learning combines automatic adjoint differentiation
(AAD) with modern machine learning (ML) in the context of risk management of
financial Derivatives. We introduce novel algorithms for training fast,
accurate pricing and risk approximations, online, in real-time, with
convergence guarantees. Our machinery is applicable to arbitrary Derivatives
instruments or trading books, under arbitrary stochastic models of the
underlying market variables. It effectively resolves computational bottlenecks
of Derivatives risk reports and capital calculations.

Differential ML is a general extension of supervised learning, where ML
models are trained on examples of not only inputs and labels but also
differentials of labels wrt inputs. It is also applicable in many situations
outside finance, where high quality first-order derivatives wrt training inputs
are available. Applications in Physics, for example, may leverage differentials
known from first principles to learn function approximations more effectively.

In finance, AAD computes pathwise differentials with remarkable efficacy so
differential ML algorithms provide extremely effective pricing and risk
approximations. We can produce fast analytics in models too complex for closed
form solutions, extract the risk factors of complex transactions and trading
books, and effectively compute risk management metrics like reports across a
large number of scenarios, backtesting and simulation of hedge strategies, or
regulations like XVA, CCR, FRTB or SIMM-MVA.

TensorFlow implementation is available on
https://github.com/differential-machine-learning
</p>
<a href="http://arxiv.org/abs/2005.02347">arXiv:2005.02347</a> [<a href="http://arxiv.org/pdf/2005.02347">pdf</a>]

<h2>Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet). (arXiv:2005.02706v3 [eess.IV] UPDATED)</h2>
<h3>Chen-Han Tsai, Nahum Kiryati, Eli Konen, Iris Eshed, Arnaldo Mayer</h3>
<p>Magnetic Resonance Imaging (MRI) is a widely-accepted imaging technique for
knee injury analysis. Its advantage of capturing knee structure in three
dimensions makes it the ideal tool for radiologists to locate potential tears
in the knee. In order to better confront the ever growing workload of
musculoskeletal (MSK) radiologists, automated tools for patients' triage are
becoming a real need, reducing delays in the reading of pathological cases. In
this work, we present the Efficiently-Layered Network (ELNet), a convolutional
neural network (CNN) architecture optimized for the task of initial knee MRI
diagnosis for triage. Unlike past approaches, we train ELNet from scratch
instead of using a transfer-learning approach. The proposed method is validated
quantitatively and qualitatively, and compares favorably against
state-of-the-art MRNet while using a single imaging stack (axial or coronal) as
input. Additionally, we demonstrate our model's capability to locate tears in
the knee despite the absence of localization information during training.
Lastly, the proposed model is extremely lightweight ($&lt;$ 1MB) and therefore
easy to train and deploy in real clinical settings. The code for our model is
provided at: https://github.com/mxtsai/ELNet.
</p>
<a href="http://arxiv.org/abs/2005.02706">arXiv:2005.02706</a> [<a href="http://arxiv.org/pdf/2005.02706">pdf</a>]

<h2>Detecting and Counting Pistachios based on Deep Learning. (arXiv:2005.03990v3 [cs.CV] UPDATED)</h2>
<h3>Mohammad Rahimzadeh, Abolfazl Attar</h3>
<p>Pistachios are nutritious nuts that are sorted based on the shape of their
shell into two categories: Open-mouth and Closed-mouth. The open-mouth
pistachios are higher in price, value, and demand than the closed-mouth
pistachios. Because of these differences, it is considerable for companies to
precisely count the number of each kind. This paper aims to propose a new
system for counting the different types of pistachios with computer vision. We
have introduced and shared a new dataset of pistachios, including six videos
with a total length of 167 seconds and 3927 labeled pistachios. At the first
stage, we have trained RetinaNet, the deep fully convolutional object detector
with three different backbones for detecting the pistachios in the video
frames. In the second stage, we introduce our novel method for counting the
open-mouth and closed-mouth pistachios in the videos. Pistachios that move and
roll on the transportation line may appear as closed-mouth in some frames and
open-mouth in other frames. Our work's main challenge is to count these two
kinds of pistachios correctly and fast with this circumstance. Our algorithm
performs very fast and achieves good counting results. The computed accuracy of
our algorithm on six videos (9486 frames) is 94.75%.
</p>
<a href="http://arxiv.org/abs/2005.03990">arXiv:2005.03990</a> [<a href="http://arxiv.org/pdf/2005.03990">pdf</a>]

<h2>Cyberbullying Detection with Fairness Constraints. (arXiv:2005.06625v2 [cs.CL] UPDATED)</h2>
<h3>Oguzhan Gencoglu</h3>
<p>Cyberbullying is a widespread adverse phenomenon among online social
interactions in today's digital society. While numerous computational studies
focus on enhancing the cyberbullying detection performance of machine learning
algorithms, proposed models tend to carry and reinforce unintended social
biases. In this study, we try to answer the research question of "Can we
mitigate the unintended bias of cyberbullying detection models by guiding the
model training with fairness constraints?". For this purpose, we propose a
model training scheme that can employ fairness constraints and validate our
approach with different datasets. We demonstrate that various types of
unintended biases can be successfully mitigated without impairing the model
quality. We believe our work contributes to the pursuit of unbiased,
transparent, and ethical machine learning solutions for cyber-social health.
</p>
<a href="http://arxiv.org/abs/2005.06625">arXiv:2005.06625</a> [<a href="http://arxiv.org/pdf/2005.06625">pdf</a>]

<h2>MOPO: Model-based Offline Policy Optimization. (arXiv:2005.13239v5 [cs.LG] UPDATED)</h2>
<h3>Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, Tengyu Ma</h3>
<p>Offline reinforcement learning (RL) refers to the problem of learning
policies entirely from a large batch of previously collected data. This problem
setting offers the promise of utilizing such datasets to acquire policies
without any costly or dangerous active exploration. However, it is also
challenging, due to the distributional shift between the offline training data
and those states visited by the learned policy. Despite significant recent
progress, the most successful prior methods are model-free and constrain the
policy to the support of data, precluding generalization to unseen states. In
this paper, we first observe that an existing model-based RL algorithm already
produces significant gains in the offline setting compared to model-free
approaches. However, standard model-based RL methods, designed for the online
setting, do not provide an explicit mechanism to avoid the offline setting's
distributional shift issue. Instead, we propose to modify the existing
model-based RL methods by applying them with rewards artificially penalized by
the uncertainty of the dynamics. We theoretically show that the algorithm
maximizes a lower bound of the policy's return under the true MDP. We also
characterize the trade-off between the gain and risk of leaving the support of
the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO),
outperforms standard model-based RL algorithms and prior state-of-the-art
model-free offline RL algorithms on existing offline RL benchmarks and two
challenging continuous control tasks that require generalizing from data
collected for a different task. The code is available at
https://github.com/tianheyu927/mopo.
</p>
<a href="http://arxiv.org/abs/2005.13239">arXiv:2005.13239</a> [<a href="http://arxiv.org/pdf/2005.13239">pdf</a>]

<h2>DocBank: A Benchmark Dataset for Document Layout Analysis. (arXiv:2006.01038v2 [cs.CL] UPDATED)</h2>
<h3>Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, Ming Zhou</h3>
<p>Document layout analysis usually relies on computer vision models to
understand documents while ignoring textual information that is vital to
capture. Meanwhile, high quality labeled datasets with both visual and textual
information are still insufficient. In this paper, we present \textbf{DocBank},
a benchmark dataset that contains 500K document pages with fine-grained
token-level annotations for document layout analysis. DocBank is constructed
using a simple yet effective way with weak supervision from the \LaTeX{}
documents available on the arXiv.com. With DocBank, models from different
modalities can be compared fairly and multi-modal approaches will be further
investigated and boost the performance of document layout analysis. We build
several strong baselines and manually split train/dev/test sets for evaluation.
Experiment results show that models trained on DocBank accurately recognize the
layout information for a variety of documents. The DocBank dataset is publicly
available at https://github.com/doc-analysis/DocBank.
</p>
<a href="http://arxiv.org/abs/2006.01038">arXiv:2006.01038</a> [<a href="http://arxiv.org/pdf/2006.01038">pdf</a>]

<h2>Evaluating the Disentanglement of Deep Generative Models through Manifold Topology. (arXiv:2006.03680v2 [stat.ML] UPDATED)</h2>
<h3>Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar Carlsson, Stefano Ermon</h3>
<p>Learning disentangled representations is regarded as a fundamental task for
improving the generalization, robustness, and interpretability of generative
models. However, measuring disentanglement has been challenging and
inconsistent, often dependent on an ad-hoc external model or specific to a
certain dataset. To address this, we present a method for quantifying
disentanglement that only uses the generative model, by measuring the
topological similarity of conditional submanifolds in the learned
representation. This method showcases both unsupervised and supervised
variants. To illustrate the effectiveness and applicability of our method, we
empirically evaluate several state-of-the-art models across multiple datasets.
We find that our method ranks models similarly to existing methods.
</p>
<a href="http://arxiv.org/abs/2006.03680">arXiv:2006.03680</a> [<a href="http://arxiv.org/pdf/2006.03680">pdf</a>]

<h2>MeshWalker: Deep Mesh Understanding by Random Walks. (arXiv:2006.05353v2 [cs.CV] UPDATED)</h2>
<h3>Alon Lahav, Ayellet Tal</h3>
<p>Most attempts to represent 3D shapes for deep learning have focused on
volumetric grids, multi-view images and point clouds. In this paper we look at
the most popular representation of 3D shapes in computer graphics - a
triangular mesh - and ask how it can be utilized within deep learning. The few
attempts to answer this question propose to adapt convolutions &amp; pooling to
suit Convolutional Neural Networks (CNNs). This paper proposes a very different
approach, termed MeshWalker, to learn the shape directly from a given mesh. The
key idea is to represent the mesh by random walks along the surface, which
"explore" the mesh's geometry and topology. Each walk is organized as a list of
vertices, which in some manner imposes regularity on the mesh. The walk is fed
into a Recurrent Neural Network (RNN) that "remembers" the history of the walk.
We show that our approach achieves state-of-the-art results for two fundamental
shape analysis tasks: shape classification and semantic segmentation.
Furthermore, even a very small number of examples suffices for learning. This
is highly important, since large datasets of meshes are difficult to acquire.
</p>
<a href="http://arxiv.org/abs/2006.05353">arXiv:2006.05353</a> [<a href="http://arxiv.org/pdf/2006.05353">pdf</a>]

<h2>Markov-Lipschitz Deep Learning. (arXiv:2006.08256v5 [cs.LG] UPDATED)</h2>
<h3>Stan Z. Li, Zelin Zang, Lirong Wu</h3>
<p>We propose a novel framework, called Markov-Lipschitz deep learning (MLDL),
to tackle geometric deterioration caused by collapse, twisting, or crossing in
vector-based neural network transformations for manifold-based representation
learning and manifold data generation. A prior constraint, called locally
isometric smoothness (LIS), is imposed across-layers and encoded into a Markov
random field (MRF)-Gibbs distribution. This leads to the best possible
solutions for local geometry preservation and robustness as measured by locally
geometric distortion and locally bi-Lipschitz continuity. Consequently, the
layer-wise vector transformations are enhanced into well-behaved,
LIS-constrained metric homeomorphisms. Extensive experiments, comparisons, and
ablation study demonstrate significant advantages of MLDL for manifold learning
and manifold data generation. MLDL is general enough to enhance any vector
transformation-based networks. The code is available at
https://github.com/westlake-cairi/Markov-Lipschitz-Deep-Learning.
</p>
<a href="http://arxiv.org/abs/2006.08256">arXiv:2006.08256</a> [<a href="http://arxiv.org/pdf/2006.08256">pdf</a>]

<h2>iSeeBetter: Spatio-temporal video super-resolution using recurrent generative back-projection networks. (arXiv:2006.11161v4 [cs.CV] UPDATED)</h2>
<h3>Aman Chadha, John Britto, M. Mani Roja</h3>
<p>Recently, learning-based models have enhanced the performance of single-image
super-resolution (SISR). However, applying SISR successively to each video
frame leads to a lack of temporal coherency. Convolutional neural networks
(CNNs) outperform traditional approaches in terms of image quality metrics such
as peak signal to noise ratio (PSNR) and structural similarity (SSIM). However,
generative adversarial networks (GANs) offer a competitive advantage by being
able to mitigate the issue of a lack of finer texture details, usually seen
with CNNs when super-resolving at large upscaling factors. We present
iSeeBetter, a novel GAN-based spatio-temporal approach to video
super-resolution (VSR) that renders temporally consistent super-resolution
videos. iSeeBetter extracts spatial and temporal information from the current
and neighboring frames using the concept of recurrent back-projection networks
as its generator. Furthermore, to improve the "naturality" of the
super-resolved image while eliminating artifacts seen with traditional
algorithms, we utilize the discriminator from super-resolution generative
adversarial network (SRGAN). Although mean squared error (MSE) as a primary
loss-minimization objective improves PSNR/SSIM, these metrics may not capture
fine details in the image resulting in misrepresentation of perceptual quality.
To address this, we use a four-fold (MSE, perceptual, adversarial, and
total-variation (TV)) loss function. Our results demonstrate that iSeeBetter
offers superior VSR fidelity and surpasses state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2006.11161">arXiv:2006.11161</a> [<a href="http://arxiv.org/pdf/2006.11161">pdf</a>]

<h2>Tracking-by-Trackers with a Distilled and Reinforced Model. (arXiv:2007.04108v2 [cs.CV] UPDATED)</h2>
<h3>Matteo Dunnhofer, Niki Martinel, Christian Micheloni</h3>
<p>Visual object tracking was generally tackled by reasoning independently on
fast processing algorithms, accurate online adaptation methods, and fusion of
trackers. In this paper, we unify such goals by proposing a novel tracking
methodology that takes advantage of other visual trackers, offline and online.
A compact student model is trained via the marriage of knowledge distillation
and reinforcement learning. The first allows to transfer and compress tracking
knowledge of other trackers. The second enables the learning of evaluation
measures which are then exploited online. After learning, the student can be
ultimately used to build (i) a very fast single-shot tracker, (ii) a tracker
with a simple and effective online adaptation mechanism, (iii) a tracker that
performs fusion of other trackers. Extensive validation shows that the proposed
algorithms compete with real-time state-of-the-art trackers.
</p>
<a href="http://arxiv.org/abs/2007.04108">arXiv:2007.04108</a> [<a href="http://arxiv.org/pdf/2007.04108">pdf</a>]

<h2>Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples. (arXiv:2007.05145v3 [cs.LG] UPDATED)</h2>
<h3>Shafi Goldwasser, Adam Tauman Kalai, Yael Tauman Kalai, Omar Montasser</h3>
<p>We present a transductive learning algorithm that takes as input training
examples from a distribution $P$ and arbitrary (unlabeled) test examples,
possibly chosen by an adversary. This is unlike prior work that assumes that
test examples are small perturbations of $P$. Our algorithm outputs a selective
classifier, which abstains from predicting on some examples. By considering
selective transductive learning, we give the first nontrivial guarantees for
learning classes of bounded VC dimension with arbitrary train and test
distributions---no prior guarantees were known even for simple classes of
functions such as intervals on the line. In particular, for any function in a
class $C$ of bounded VC dimension, we guarantee a low test error rate and a low
rejection rate with respect to $P$. Our algorithm is efficient given an
Empirical Risk Minimizer (ERM) for $C$. Our guarantees hold even for test
examples chosen by an unbounded white-box adversary. We also give guarantees
for generalization, agnostic, and unsupervised settings.
</p>
<a href="http://arxiv.org/abs/2007.05145">arXiv:2007.05145</a> [<a href="http://arxiv.org/pdf/2007.05145">pdf</a>]

<h2>Fair Division with Binary Valuations: One Rule to Rule Them All. (arXiv:2007.06073v2 [cs.GT] UPDATED)</h2>
<h3>Daniel Halpern, Ariel D. Procaccia, Alexandros Psomas, Nisarg Shah</h3>
<p>We study fair allocation of indivisible goods among agents. Prior research
focuses on additive agent preferences, which leads to an impossibility when
seeking truthfulness, fairness, and efficiency. We show that when agents have
binary additive preferences, a compelling rule -- maximum Nash welfare (MNW) --
provides all three guarantees.

Specifically, we show that deterministic MNW with lexicographic tie-breaking
is group strategyproof in addition to being envy-free up to one good and Pareto
optimal. We also prove that fractional MNW -- known to be group strategyproof,
envy-free, and Pareto optimal -- can be implemented as a distribution over
deterministic MNW allocations, which are envy-free up to one good. Our work
establishes maximum Nash welfare as the ultimate allocation rule in the realm
of binary additive preferences.
</p>
<a href="http://arxiv.org/abs/2007.06073">arXiv:2007.06073</a> [<a href="http://arxiv.org/pdf/2007.06073">pdf</a>]

<h2>Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance. (arXiv:2007.09267v2 [cs.CV] UPDATED)</h2>
<h3>Minghua Liu, Xiaoshuai Zhang, Hao Su</h3>
<p>We are interested in reconstructing the mesh representation of object
surfaces from point clouds. Surface reconstruction is a prerequisite for
downstream applications such as rendering, collision avoidance for planning,
animation, etc. However, the task is challenging if the input point cloud has a
low resolution, which is common in real-world scenarios (e.g., from LiDAR or
Kinect sensors). Existing learning-based mesh generative methods mostly predict
the surface by first building a shape embedding that is at the whole object
level, a design that causes issues in generating fine-grained details and
generalizing to unseen categories. Instead, we propose to leverage the input
point cloud as much as possible, by only adding connectivity information to
existing points. Particularly, we predict which triplets of points should form
faces. Our key innovation is a surrogate of local connectivity, calculated by
comparing the intrinsic/extrinsic metrics. We learn to predict this surrogate
using a deep point cloud network and then feed it to an efficient
post-processing module for high-quality mesh generation. We demonstrate that
our method can not only preserve details, handle ambiguous structures, but also
possess strong generalizability to unseen categories by experiments on
synthetic and real data. The code is available at
https://github.com/Colin97/Point2Mesh.
</p>
<a href="http://arxiv.org/abs/2007.09267">arXiv:2007.09267</a> [<a href="http://arxiv.org/pdf/2007.09267">pdf</a>]

<h2>Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary Supervised Deep Adversarial Learning. (arXiv:2007.11319v2 [cs.CV] UPDATED)</h2>
<h3>Mobarakol Islam, Daniel A. Atputharuban, Ravikiran Ramesh, Hongliang Ren</h3>
<p>Robot-assisted surgery is an emerging technology which has undergone rapid
growth with the development of robotics and imaging systems. Innovations in
vision, haptics and accurate movements of robot arms have enabled surgeons to
perform precise minimally invasive surgeries. Real-time semantic segmentation
of the robotic instruments and tissues is a crucial step in robot-assisted
surgery. Accurate and efficient segmentation of the surgical scene not only
aids in the identification and tracking of instruments but also provided
contextual information about the different tissues and instruments being
operated with. For this purpose, we have developed a light-weight cascaded
convolutional neural network (CNN) to segment the surgical instruments from
high-resolution videos obtained from a commercial robotic system. We propose a
multi-resolution feature fusion module (MFF) to fuse the feature maps of
different dimensions and channels from the auxiliary and main branch. We also
introduce a novel way of combining auxiliary loss and adversarial loss to
regularize the segmentation model. Auxiliary loss helps the model to learn
low-resolution features, and adversarial loss improves the segmentation
prediction by learning higher order structural information. The model also
consists of a light-weight spatial pyramid pooling (SPP) unit to aggregate rich
contextual information in the intermediate stage. We show that our model
surpasses existing algorithms for pixel-wise segmentation of surgical
instruments in both prediction accuracy and segmentation time of
high-resolution videos.
</p>
<a href="http://arxiv.org/abs/2007.11319">arXiv:2007.11319</a> [<a href="http://arxiv.org/pdf/2007.11319">pdf</a>]

<h2>Byzantine-Fault-Tolerant Consensus via Reinforcement Learning for Permissioned Blockchain Implemented in a V2X Network. (arXiv:2007.13957v2 [cs.NI] UPDATED)</h2>
<h3>Seungmo Kim, Ahmed S. Ibrahim</h3>
<p>Blockchain has been at the center of various trust-promoting applications for
vehicle-to-everything (V2X) networks. Recently, permissioned blockchains gain
practical popularity thanks to their improved scalability and diverse needs for
different organizations. One representative example of permissioned blockchain
is Hyperledger Fabric. Due to its unique execute-order procedure, there is a
critical need for a client to select an optimal number of peers. The
interesting problem that this paper targets to address is the tradeoff in the
number of peers: a too large number will lead to a lower scalability and a too
small number will leave a narrow margin in the number of peers sufficing the
Byzantine fault tolerance (BFT). This channel selection issue gets especially
challenging to deal with in V2X networks due to the mobility: a transaction
must be executed and the associated block must be committed before the vehicle
leaves a network. To this end, this paper proposes an optimal channel selection
mechanism based on reinforcement learning (RL) to keep a Hyperledger
Fabric-empowered V2X network impervious to dynamicity due to mobility. We model
the RL as a contextual multi-armed bandit (MAB) problem. The results prove the
outperformance of the proposed scheme.
</p>
<a href="http://arxiv.org/abs/2007.13957">arXiv:2007.13957</a> [<a href="http://arxiv.org/pdf/2007.13957">pdf</a>]

<h2>OREBA: A Dataset for Objectively Recognizing Eating Behaviour and Associated Intake. (arXiv:2007.15831v3 [cs.HC] UPDATED)</h2>
<h3>Philipp V. Rouast, Hamid Heydarian, Marc T. P. Adam, Megan E. Rollo</h3>
<p>Automatic detection of intake gestures is a key element of automatic dietary
monitoring. Several types of sensors, including inertial measurement units
(IMU) and video cameras, have been used for this purpose. The common machine
learning approaches make use of the labeled sensor data to automatically learn
how to make detections. One characteristic, especially for deep learning
models, is the need for large datasets. To meet this need, we collected the
Objectively Recognizing Eating Behavior and Associated Intake (OREBA) dataset.
The OREBA dataset aims to provide comprehensive multi-sensor data recorded
during the course of communal meals for researchers interested in intake
gesture detection. Two scenarios are included, with 100 participants for a
discrete dish and 102 participants for a shared dish, totalling 9069 intake
gestures. Available sensor data consists of synchronized frontal video and IMU
with accelerometer and gyroscope for both hands. We report the details of data
collection and annotation, as well as details of sensor processing. The results
of studies on IMU and video data involving deep learning models are reported to
provide a baseline for future research. Specifically, the best baseline models
achieve performances of $F_1$ = 0.853 for the discrete dish using video and
$F_1$ = 0.852 for the shared dish using inertial data.
</p>
<a href="http://arxiv.org/abs/2007.15831">arXiv:2007.15831</a> [<a href="http://arxiv.org/pdf/2007.15831">pdf</a>]

<h2>Temporal Context Aggregation for Video Retrieval with Contrastive Learning. (arXiv:2008.01334v2 [cs.CV] UPDATED)</h2>
<h3>Jie Shao, Xin Wen, Bingchen Zhao, Xiangyang Xue</h3>
<p>The current research focus on Content-Based Video Retrieval requires
higher-level video representation describing the long-range semantic
dependencies of relevant incidents, events, etc. However, existing methods
commonly process the frames of a video as individual images or short clips,
making the modeling of long-range semantic dependencies difficult. In this
paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a
video representation learning framework that incorporates long-range temporal
information between frame-level features using the self-attention mechanism. To
train it on video retrieval datasets, we propose a supervised contrastive
learning method that performs automatic hard negative mining and utilizes the
memory bank mechanism to increase the capacity of negative samples. Extensive
experiments are conducted on multiple video retrieval tasks, such as
CC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant
performance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods
with video-level features, and deliver competitive results with 22x faster
inference time comparing with frame-level features.
</p>
<a href="http://arxiv.org/abs/2008.01334">arXiv:2008.01334</a> [<a href="http://arxiv.org/pdf/2008.01334">pdf</a>]

<h2>Joint Policy Search for Multi-agent Collaboration with Imperfect Information. (arXiv:2008.06495v3 [cs.LG] UPDATED)</h2>
<h3>Yuandong Tian, Qucheng Gong, Tina Jiang</h3>
<p>To learn good joint policies for multi-agent collaboration with imperfect
information remains a fundamental challenge. While for two-player zero-sum
games, coordinate-ascent approaches (optimizing one agent's policy at a time,
e.g., self-play) work with guarantees, in multi-agent cooperative setting they
often converge to sub-optimal Nash equilibrium. On the other hand, directly
modeling joint policy changes in imperfect information game is nontrivial due
to complicated interplay of policies (e.g., upstream updates affect downstream
state reachability). In this paper, we show global changes of game values can
be decomposed to policy changes localized at each information set, with a novel
term named policy-change density. Based on this, we propose Joint Policy
Search(JPS) that iteratively improves joint policies of collaborative agents in
imperfect information games, without re-evaluating the entire game. On
multi-agent collaborative tabular games, JPS is proven to never worsen
performance and can improve solutions provided by unilateral approaches (e.g,
CFR), outperforming algorithms designed for collaborative policy learning (e.g.
BAD). Furthermore, for real-world games, JPS has an online form that naturally
links with gradient updates. We test it to Contract Bridge, a 4-player
imperfect-information game where a team of $2$ collaborates to compete against
the other. In its bidding phase, players bid in turn to find a good contract
through a limited information channel. Based on a strong baseline agent that
bids competitive bridge purely through domain-agnostic self-play, JPS improves
collaboration of team players and outperforms WBridge5, a championship-winning
software, by $+0.63$ IMPs (International Matching Points) per board over 1k
games, substantially better than previous SoTA ($+0.41$ IMPs/b) under
Double-Dummy evaluation.
</p>
<a href="http://arxiv.org/abs/2008.06495">arXiv:2008.06495</a> [<a href="http://arxiv.org/pdf/2008.06495">pdf</a>]

<h2>TextDecepter: Hard Label Black Box Attack on Text Classifiers. (arXiv:2008.06860v2 [cs.CL] UPDATED)</h2>
<h3>Sachin Saxena</h3>
<p>Machine learning has been proven to be susceptible to carefully crafted
samples, known as adversarial examples. The generation of these adversarial
examples helps to make the models more robust and give as an insight of the
underlying decision making of these models. Over the years, researchers have
successfully attacked image classifiers in, both, white and black-box setting.
Although, these methods are not directly applicable to texts as text data is
discrete in nature. In recent years, research on crafting adversarial examples
against textual applications has been on the rise. In this paper, we present a
novel approach for hard label black-box attacks against Natural Language
Processing (NLP) classifiers, where no model information is disclosed, and an
attacker can only query the model to get final decision of the classifier,
without confidence scores of the classes involved. Such attack scenario is
applicable to real world black-box models being used for security-sensitive
applications such as sentiment analysis and toxic content detection
</p>
<a href="http://arxiv.org/abs/2008.06860">arXiv:2008.06860</a> [<a href="http://arxiv.org/pdf/2008.06860">pdf</a>]

<h2>POSEIDON: Privacy-Preserving Federated Neural Network Learning. (arXiv:2009.00349v2 [cs.CR] UPDATED)</h2>
<h3>Sinem Sav, Apostolos Pyrgelis, Juan R. Troncoso-Pastoriza, David Froelicher, Jean-Philippe Bossuat, Joao Sa Sousa, Jean-Pierre Hubaux</h3>
<p>In this paper, we address the problem of privacy-preserving training and
evaluation of neural networks in an $N$-party, federated learning setting. We
propose a novel system, POSEIDON, the first of its kind in the regime of
privacy-preserving neural network training, employing multiparty lattice-based
cryptography and preserving the confidentiality of the training data, the
model, and the evaluation data, under a passive-adversary model and collusions
between up to $N-1$ parties. To efficiently execute the secure backpropagation
algorithm for training neural networks, we provide a generic packing approach
that enables Single Instruction, Multiple Data (SIMD) operations on encrypted
data. We also introduce arbitrary linear transformations within the
cryptographic bootstrapping operation, optimizing the costly cryptographic
computations over the parties, and we define a constrained optimization problem
for choosing the cryptographic parameters. Our experimental results show that
POSEIDON achieves accuracy similar to centralized or decentralized non-private
approaches and that its computation and communication overhead scales linearly
with the number of parties. POSEIDON trains a 3-layer neural network on the
MNIST dataset with 784 features and 60K samples distributed among 10 parties in
less than 2 hours.
</p>
<a href="http://arxiv.org/abs/2009.00349">arXiv:2009.00349</a> [<a href="http://arxiv.org/pdf/2009.00349">pdf</a>]

<h2>Data Readiness for Natural Language Processing. (arXiv:2009.02043v2 [cs.CY] UPDATED)</h2>
<h3>Fredrik Olsson, Magnus Sahlgren</h3>
<p>This document concerns data readiness in the context of machine learning and
Natural Language Processing. It describes how an organization may proceed to
identify, make available, validate, and prepare data to facilitate automated
analysis methods. The contents of the document is based on the practical
challenges and frequently asked questions we have encountered in our work as an
applied research institute with helping organizations and companies, both in
the public and private sectors, to use data in their business processes.
</p>
<a href="http://arxiv.org/abs/2009.02043">arXiv:2009.02043</a> [<a href="http://arxiv.org/pdf/2009.02043">pdf</a>]

<h2>Variational Deep Learning for the Identification and Reconstruction of Chaotic and Stochastic Dynamical Systems from Noisy and Partial Observations. (arXiv:2009.02296v4 [cs.LG] UPDATED)</h2>
<h3>Duong Nguyen, Said Ouala, Lucas Drumetz, Ronan Fablet</h3>
<p>The data-driven recovery of the unknown governing equations of dynamical
systems has recently received an increasing interest. However, the
identification of the governing equations remains challenging when dealing with
noisy and partial observations. Here, we address this challenge and investigate
variational deep learning schemes. Within the proposed framework, we jointly
learn an inference model to reconstruct the true states of the system from
series of noisy and partial data and the governing equations of these states.
In doing so, this framework bridges classical data assimilation and
state-of-the-art machine learning techniques and we show that it generalizes
state-of-the-art methods. Importantly, both the inference model and the
governing equations embed stochastic components to account for stochastic
variabilities, model errors and reconstruction uncertainties. Various
experiments on chaotic and stochastic dynamical systems support the relevance
of our scheme w.r.t. state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2009.02296">arXiv:2009.02296</a> [<a href="http://arxiv.org/pdf/2009.02296">pdf</a>]

<h2>CuratorNet: Visually-aware Recommendation of Art Images. (arXiv:2009.04426v2 [cs.IR] UPDATED)</h2>
<h3>Pablo Messina, Manuel Cartagena, Patricio Cerda-Mardini, Felipe del Rio, Denis Parra</h3>
<p>Although there are several visually-aware recommendation models in domains
like fashion or even movies, the art domain lacks thesame level of research
attention, despite the recent growth of the online artwork market. To reduce
this gap, in this article we introduceCuratorNet, a neural network architecture
for visually-aware recommendation of art images. CuratorNet is designed at the
core withthe goal of maximizing generalization: the network has a fixed set of
parameters that only need to be trained once, and thereafter themodel is able
to generalize to new users or items never seen before, without further
training. This is achieved by leveraging visualcontent: items are mapped to
item vectors through visual embeddings, and users are mapped to user vectors by
aggregating the visualcontent of items they have consumed. Besides the model
architecture, we also introduce novel triplet sampling strategies to build
atraining set for rank learning in the art domain, resulting in more effective
learning than naive random sampling. With an evaluationover a real-world
dataset of physical paintings, we show that CuratorNet achieves the best
performance among several baselines,including the state-of-the-art model VBPR.
CuratorNet is motivated and evaluated in the art domain, but its architecture
and trainingscheme could be adapted to recommend images in other areas
</p>
<a href="http://arxiv.org/abs/2009.04426">arXiv:2009.04426</a> [<a href="http://arxiv.org/pdf/2009.04426">pdf</a>]

<h2>Carousel Personalization in Music Streaming Apps with Contextual Bandits. (arXiv:2009.06546v2 [cs.LG] UPDATED)</h2>
<h3>Walid Bendada, Guillaume Salha, Th&#xe9;o Bontempelli</h3>
<p>Media services providers, such as music streaming platforms, frequently
leverage swipeable carousels to recommend personalized content to their users.
However, selecting the most relevant items (albums, artists, playlists...) to
display in these carousels is a challenging task, as items are numerous and as
users have different preferences. In this paper, we model carousel
personalization as a contextual multi-armed bandit problem with multiple plays,
cascade-based updates and delayed batch feedback. We empirically show the
effectiveness of our framework at capturing characteristics of real-world
carousels by addressing a large-scale playlist recommendation task on a global
music streaming mobile app. Along with this paper, we publicly release
industrial data from our experiments, as well as an open-source environment to
simulate comparable carousel personalization learning problems.
</p>
<a href="http://arxiv.org/abs/2009.06546">arXiv:2009.06546</a> [<a href="http://arxiv.org/pdf/2009.06546">pdf</a>]

<h2>Decoupling Representation Learning from Reinforcement Learning. (arXiv:2009.08319v2 [cs.LG] UPDATED)</h2>
<h3>Adam Stooke, Kimin Lee, Pieter Abbeel, Michael Laskin</h3>
<p>In an effort to overcome limitations of reward-driven feature learning in
deep reinforcement learning (RL) from images, we propose decoupling
representation learning from policy learning. To this end, we introduce a new
unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC),
which trains a convolutional encoder to associate pairs of observations
separated by a short time difference, under image augmentations and using a
contrastive loss. In online RL experiments, we show that training the encoder
exclusively using ATC matches or outperforms end-to-end RL in most
environments. Additionally, we benchmark several leading UL algorithms by
pre-training encoders on expert demonstrations and using them, with weights
frozen, in RL agents; we find that agents using ATC-trained encoders outperform
all others. We also train multi-task encoders on data from multiple
environments and show generalization to different downstream RL tasks. Finally,
we ablate components of ATC, and introduce a new data augmentation to enable
replay of (compressed) latent images from pre-trained encoders when RL requires
augmentation. Our experiments span visually diverse RL benchmarks in DeepMind
Control, DeepMind Lab, and Atari, and our complete code is available at
https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.
</p>
<a href="http://arxiv.org/abs/2009.08319">arXiv:2009.08319</a> [<a href="http://arxiv.org/pdf/2009.08319">pdf</a>]

<h2>Recurrent Graph Tensor Networks. (arXiv:2009.08727v2 [cs.LG] UPDATED)</h2>
<h3>Yao Lei Xu, Danilo P. Mandic</h3>
<p>Recurrent Neural Networks (RNNs) are among the most successful machine
learning models for sequence modelling. In this paper, we show that the
modelling of hidden states in RNNs can be approximated through a multi-linear
graph filter, which describes the directional flow of temporal information. The
so derived multi-linear graph filter is then generalized to a tensor network
form to improve its modelling power, resulting in a novel Recurrent Graph
Tensor Network (RGTN). To validate the expressive power of the derived network,
several variants of RGTN models were proposed and employed for the task of
time-series forecasting, demonstrating superior properties in terms of
convergence, performance, and complexity. By leveraging the multi-modal nature
of tensor networks, RGTN models were shown to out-perform a standard RNN by 45%
in terms of mean-squared-error while using up to 90% less parameters.
Therefore, by combining the expressive power of tensor networks with a suitable
graph filter, we show that the proposed RGTN models can out-perform a classical
RNN at a drastically lower parameter complexity, especially in the multi-modal
setting.
</p>
<a href="http://arxiv.org/abs/2009.08727">arXiv:2009.08727</a> [<a href="http://arxiv.org/pdf/2009.08727">pdf</a>]

<h2>PESAO: Psychophysical Experimental Setup for Active Observers. (arXiv:2009.09933v2 [cs.CV] UPDATED)</h2>
<h3>Markus D. Solbach, John K. Tsotsos</h3>
<p>Most past and present research in computer vision involves passively observed
data. Humans, however, are active observers outside the lab; they explore,
search, select what and how to look. Nonetheless, how exactly active
observation occurs in humans so that it can inform the design of active
computer vision systems is an open problem. PESAO is designed for investigating
active, visual observation in a 3D world. The goal was to build an experimental
setup for various active perception tasks with human subjects (active
observers) in mind that is capable of tracking the head and gaze. While many
studies explore human performances, usually, they use line drawings portrayed
in 2D, and no active observer is involved. PESAO allows us to bring many
studies to the three-dimensional world, even involving active observers. In our
instantiation, it spans an area of 400cm x 300cm and can track active observers
at a frequency of 120Hz. Furthermore, PESAO provides tracking and recording of
6D head motion, gaze, eye movement-type, first-person video, head-mounted IMU
sensor, birds-eye video, and experimenter notes. All are synchronized at
microsecond resolution.
</p>
<a href="http://arxiv.org/abs/2009.09933">arXiv:2009.09933</a> [<a href="http://arxiv.org/pdf/2009.09933">pdf</a>]

<h2>Measuring justice in machine learning. (arXiv:2009.10050v2 [cs.CY] UPDATED)</h2>
<h3>Alan Lundgard</h3>
<p>How can we build more just machine learning systems? To answer this question,
we need to know both what justice is and how to tell whether one system is more
or less just than another. That is, we need both a definition and a measure of
justice. Theories of distributive justice hold that justice can be measured (in
part) in terms of the fair distribution of benefits and burdens across people
in society. Recently, the field known as fair machine learning has turned to
John Rawls's theory of distributive justice for inspiration and
operationalization. However, philosophers known as capability theorists have
long argued that Rawls's theory uses the wrong measure of justice, thereby
encoding biases against people with disabilities. If these theorists are right,
is it possible to operationalize Rawls's theory in machine learning systems
without also encoding its biases? In this paper, I draw on examples from fair
machine learning to suggest that the answer to this question is no: the
capability theorists' arguments against Rawls's theory carry over into machine
learning systems. But capability theorists don't only argue that Rawls's theory
uses the wrong measure, they also offer an alternative measure. Which measure
of justice is right? And has fair machine learning been using the wrong one?
</p>
<a href="http://arxiv.org/abs/2009.10050">arXiv:2009.10050</a> [<a href="http://arxiv.org/pdf/2009.10050">pdf</a>]

<h2>Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem Formulation. (arXiv:2009.11044v2 [cs.CV] UPDATED)</h2>
<h3>Dimche Kostadinov, Davide Scaramuzza</h3>
<p>Event-based cameras record an asynchronous stream of per-pixel brightness
changes. As such, they have numerous advantages over the standard frame-based
cameras, including high temporal resolution, high dynamic range, and no motion
blur. Due to the asynchronous nature, efficient learning of compact
representation for event data is challenging. While it remains not explored the
extent to which the spatial and temporal event "information" is useful for
pattern recognition tasks. In this paper, we focus on single-layer
architectures. We analyze the performance of two general problem formulations:
the direct and the inverse, for unsupervised feature learning from local event
data (local volumes of events described in space-time). We identify and show
the main advantages of each approach. Theoretically, we analyze guarantees for
an optimal solution, possibility for asynchronous, parallel parameter update,
and the computational complexity. We present numerical experiments for object
recognition. We evaluate the solution under the direct and the inverse problem
and give a comparison with the state-of-the-art methods. Our empirical results
highlight the advantages of both approaches for representation learning from
event data. We show improvements of up to 9 % in the recognition accuracy
compared to the state-of-the-art methods from the same class of methods.
</p>
<a href="http://arxiv.org/abs/2009.11044">arXiv:2009.11044</a> [<a href="http://arxiv.org/pdf/2009.11044">pdf</a>]

<h2>Online Learning of Non-Markovian Reward Models. (arXiv:2009.12600v2 [cs.AI] UPDATED)</h2>
<h3>Gavin Rens, Jean-Fran&#xe7;ois Raskin, Rapha&#xeb;l Reynouad, Giuseppe Marra</h3>
<p>There are situations in which an agent should receive rewards only after
having accomplished a series of previous tasks, that is, rewards are
non-Markovian. One natural and quite general way to represent history-dependent
rewards is via a Mealy machine, a finite state automaton that produces output
sequences from input sequences. In our formal setting, we consider a Markov
decision process (MDP) that models the dynamics of the environment in which the
agent evolves and a Mealy machine synchronized with this MDP to formalize the
non-Markovian reward function. While the MDP is known by the agent, the reward
function is unknown to the agent and must be learned.

Our approach to overcome this challenge is to use Angluin's $L^*$ active
learning algorithm to learn a Mealy machine representing the underlying
non-Markovian reward machine (MRM). Formal methods are used to determine the
optimal strategy for answering so-called membership queries posed by $L^*$.

Moreover, we prove that the expected reward achieved will eventually be at
least as much as a given, reasonable value provided by a domain expert. We
evaluate our framework on three problems. The results show that using $L^*$ to
learn an MRM in a non-Markovian reward decision process is effective.
</p>
<a href="http://arxiv.org/abs/2009.12600">arXiv:2009.12600</a> [<a href="http://arxiv.org/pdf/2009.12600">pdf</a>]

<h2>Domain Generalization via Semi-supervised Meta Learning. (arXiv:2009.12658v2 [cs.LG] UPDATED)</h2>
<h3>Hossein Sharifi-Noghabi, Hossein Asghari, Nazanin Mehrasa, Martin Ester</h3>
<p>The goal of domain generalization is to learn from multiple source domains to
generalize to unseen target domains under distribution discrepancy. Current
state-of-the-art methods in this area are fully supervised, but for many
real-world problems it is hardly possible to obtain enough labeled samples. In
this paper, we propose the first method of domain generalization to leverage
unlabeled samples, combining of meta learning's episodic training and
semi-supervised learning, called DGSML. DGSML employs an entropy-based
pseudo-labeling approach to assign labels to unlabeled samples and then
utilizes a novel discrepancy loss to ensure that class centroids before and
after labeling unlabeled samples are close to each other. To learn a
domain-invariant representation, it also utilizes a novel alignment loss to
ensure that the distance between pairs of class centroids, computed after
adding the unlabeled samples, is preserved across different domains. DGSML is
trained by a meta learning approach to mimic the distribution shift between the
input source domains and unseen target domains. Experimental results on
benchmark datasets indicate that DGSML outperforms state-of-the-art domain
generalization and semi-supervised learning methods.
</p>
<a href="http://arxiv.org/abs/2009.12658">arXiv:2009.12658</a> [<a href="http://arxiv.org/pdf/2009.12658">pdf</a>]

<h2>Improving Low Compute Language Modeling with In-Domain Embedding Initialisation. (arXiv:2009.14109v2 [cs.CL] UPDATED)</h2>
<h3>Charles Welch, Rada Mihalcea, Jonathan K. Kummerfeld</h3>
<p>Many NLP applications, such as biomedical data and technical support, have
10-100 million tokens of in-domain data and limited computational resources for
learning from it. How should we train a language model in this scenario? Most
language modeling research considers either a small dataset with a closed
vocabulary (like the standard 1 million token Penn Treebank), or the whole web
with byte-pair encoding. We show that for our target setting in English,
initialising and freezing input embeddings using in-domain data can improve
language model performance by providing a useful representation of rare words,
and this pattern holds across several different domains. In the process, we
show that the standard convention of tying input and output embeddings does not
improve perplexity when initializing with embeddings trained on in-domain data.
</p>
<a href="http://arxiv.org/abs/2009.14109">arXiv:2009.14109</a> [<a href="http://arxiv.org/pdf/2009.14109">pdf</a>]

<h2>CoKe: Localized Contrastive Learning for Robust Keypoint Detection. (arXiv:2009.14115v2 [cs.CV] UPDATED)</h2>
<h3>Yutong Bai, Angtian Wang, Adam Kortylewski, Alan Yuille</h3>
<p>Today's most popular approaches to keypoint detection learn a holistic
representation of all keypoints. This enables them to implicitly leverage the
relative spatial geometry between keypoints and thus to prevent false-positive
detections due to local ambiguities. However, our experiments show that such
holistic representations do not generalize well when the 3D pose of objects
varies strongly, or when objects are partially occluded. In this paper, we
propose CoKe, a framework for the supervised contrastive learning of distinct
local feature representations for robust keypoint detection. In particular, we
introduce a feature bank mechanism and update rules for keypoint and
non-keypoint features which make possible to learn local keypoint detectors
that are accurate and robust to local ambiguities. Our experiments show that
CoKe achieves state-of-the-art results compared to approaches that jointly
represent all keypoints holistically (Stacked Hourglass Networks, MSS-Net) as
well as to approaches that are supervised with the detailed 3D object geometry
(StarMap). Notably, CoKe performs exceptionally well when objects are partially
occluded and outperforms related work on a range of diverse datasets
(PASCAL3D+, MPII, ObjectNet3D).
</p>
<a href="http://arxiv.org/abs/2009.14115">arXiv:2009.14115</a> [<a href="http://arxiv.org/pdf/2009.14115">pdf</a>]

<h2>The Illusion of the Illusion of Sparsity: An exercise in prior sensitivity. (arXiv:2009.14296v1 [stat.ME])</h2>
<h3>Bruno Fava, Hedibert F. Lopes</h3>
<p>The emergence of Big Data raises the question of how to model economic
relations when there is a large number of possible explanatory variables. We
revisit the issue by comparing the possibility of using dense or sparse models
in a Bayesian approach, allowing for variable selection and shrinkage. More
specifically, we discuss the results reached by Giannone, Lenza, and Primiceri
(2020) through a "Spike-and-Slab" prior, which suggest an "illusion of
sparsity" in economic data, as no clear patterns of sparsity could be detected.
We make a further revision of the posterior distributions of the model, and
propose three experiments to evaluate the robustness of the adopted prior
distribution. We find that the pattern of sparsity is sensitive to the prior
distribution of the regression coefficients, and present evidence that the
model indirectly induces variable selection and shrinkage, which suggests that
the "illusion of sparsity" could be, itself, an illusion. Code is available on
github.com/bfava/IllusionOfIllusion.
</p>
<a href="http://arxiv.org/abs/2009.14296">arXiv:2009.14296</a> [<a href="http://arxiv.org/pdf/2009.14296">pdf</a>]

<h2>Spatial Statistical Models: an overview under the Bayesian Approach. (arXiv:2009.14371v1 [stat.ME])</h2>
<h3>Francisco Louzada, Diego C. Nascimento, Osafu Augustine Egbon</h3>
<p>Spatial documentation is exponentially increasing given the availability of
Big IoT Data, enabled by the devices miniaturization and data storage capacity.
Bayesian spatial statistics is a useful statistical tool to determine the
dependence structure and hidden patterns over space through prior knowledge and
data likelihood. Nevertheless, this modeling class is not well explored as the
classification and regression machine learning models given their simplicity
and often weak (data) independence supposition. In this manner, this systematic
review aimed to unravel the main models presented in the literature in the past
20 years, identify gaps, and research opportunities. Elements such as random
fields, spatial domains, prior specification, covariance function, and
numerical approximations were discussed. This work explored the two subclasses
of spatial smoothing global and local.
</p>
<a href="http://arxiv.org/abs/2009.14371">arXiv:2009.14371</a> [<a href="http://arxiv.org/pdf/2009.14371">pdf</a>]

<h2>Double/Debiased Machine Learning for Logistic Partially Linear Model. (arXiv:2009.14461v1 [stat.ME])</h2>
<h3>Molei Liu, Yi Zhang, Doudou Zhou</h3>
<p>We propose double/debiased machine learning approaches to infer (at the
parametric rate) the parametric component of a logistic partially linear model
with the binary response following a conditional logistic model of a low
dimensional linear parametric function of some key (exposure) covariates and a
nonparametric function adjusting for the confounding effect of other
covariates. We consider a Neyman orthogonal (doubly robust) score equation
consisting of two nuisance functions: nonparametric component in the logistic
model and conditional mean of the exposure on the other covariates and with the
response fixed. To estimate the nuisance models, we separately consider the use
of high dimensional (HD) sparse parametric models and more general (typically
nonparametric) machine learning (ML) methods. In the HD case, we derive certain
moment equations to calibrate the first-order bias of the nuisance models and
grant our method a model double robustness property in the sense that our
estimator achieves the desirable rate when at least one of the nuisance models
is correctly specified and both of them are ultra-sparse. In the ML case, the
non-linearity of the logit link makes it substantially harder than the
partially linear setting to use an arbitrary conditional mean learning
algorithm to estimate the nuisance component of the logistic model. We handle
this obstacle through a novel full model refitting procedure that is
easy-to-implement and facilitates the use of nonparametric ML algorithms in our
framework. Our ML estimator is rate doubly robust in the same sense as
Chernozhukov et al. (2018a). We evaluate our methods through simulation studies
and apply them in assessing the effect of emergency contraceptive (EC) pill on
early gestation foetal with a policy reform in Chile in 2008 (Bentancor and
Clarke, 2017).
</p>
<a href="http://arxiv.org/abs/2009.14461">arXiv:2009.14461</a> [<a href="http://arxiv.org/pdf/2009.14461">pdf</a>]

<h2>Maximum Entropy classification for record linkage. (arXiv:2009.14797v1 [stat.ME])</h2>
<h3>Danhyang Lee, Li-Chun Zhang, Jae-Kwang Kim</h3>
<p>By record linkage one joins records residing in separate files which are
believed to be related to the same entity. In this paper we approach record
linkage as a classification problem, and adapt the maximum entropy
classification method in text mining to record linkage, both in the supervised
and unsupervised settings of machine learning. The set of links will be chosen
according to the associated uncertainty. On the one hand, our framework
overcomes some persistent theoretical flaws of the classical approach pioneered
by Fellegi and Sunter (1969); on the other hand, the proposed algorithm is
scalable and fully automatic, unlike the classical approach that generally
requires clerical review to resolve the undecided cases.
</p>
<a href="http://arxiv.org/abs/2009.14797">arXiv:2009.14797</a> [<a href="http://arxiv.org/pdf/2009.14797">pdf</a>]

<h2>WISDoM: characterizing neurological timeseries with the Wishart distribution. (arXiv:2001.10342v2 [physics.data-an] UPDATED)</h2>
<h3>Carlo Mengucci, Daniel Remondini, Gastone Castellani, Enrico Giampieri</h3>
<p>WISDoM (Wishart Distributed Matrices) is a new framework for the
quantification of deviation of symmetric positive-definite matrices associated
to experimental samples, like covariance or correlation matrices, from expected
ones governed by the Wishart distribution WISDoM can be applied to tasks of
supervised learning, like classification, in particular when such matrices are
generated by data of different dimensionality (e.g. time series with same
number of variables but different time sampling). We show the application of
the method in two different scenarios. The first is the ranking of features
associated to electro encephalogram (EEG) data with a time series design,
providing a theoretically sound approach for this type of studies. The second
is the classification of autistic subjects of the ABIDE study, using brain
connectivity measurements.
</p>
<a href="http://arxiv.org/abs/2001.10342">arXiv:2001.10342</a> [<a href="http://arxiv.org/pdf/2001.10342">pdf</a>]

