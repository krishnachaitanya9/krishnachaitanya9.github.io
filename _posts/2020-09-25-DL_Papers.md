---
title: Latest Deep Learning Papers
date: 2021-03-08 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (170 Articles)</h1>
<h2>LOHO: Latent Optimization of Hairstyles via Orthogonalization. (arXiv:2103.03891v1 [cs.CV])</h2>
<h3>Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi</h3>
<p>Hairstyle transfer is challenging due to hair structure differences in the
source and target hair. Therefore, we propose Latent Optimization of Hairstyles
via Orthogonalization (LOHO), an optimization-based approach using GAN
inversion to infill missing hair structure details in latent space during
hairstyle transfer. Our approach decomposes hair into three attributes:
perceptual structure, appearance, and style, and includes tailored losses to
model each of these attributes independently. Furthermore, we propose two-stage
optimization and gradient orthogonalization to enable disentangled latent space
optimization of our hair attributes. Using LOHO for latent space manipulation,
users can synthesize novel photorealistic images by manipulating hair
attributes either individually or jointly, transferring the desired attributes
from reference hairstyles. LOHO achieves a superior FID compared with the
current state-of-the-art (SOTA) for hairstyle transfer. Additionally, LOHO
preserves the subject's identity comparably well according to PSNR and SSIM
when compared to SOTA image embedding pipelines.
</p>
<a href="http://arxiv.org/abs/2103.03891" target="_blank">arXiv:2103.03891</a> [<a href="http://arxiv.org/pdf/2103.03891" target="_blank">pdf</a>]

<h2>ES-Net: An Efficient Stereo Matching Network. (arXiv:2103.03922v1 [cs.CV])</h2>
<h3>Zhengyu Huang, Theodore B. Norris, Panqu Wang</h3>
<p>Dense stereo matching with deep neural networks is of great interest to the
research community. Existing stereo matching networks typically use slow and
computationally expensive 3D convolutions to improve the performance, which is
not friendly to real-world applications such as autonomous driving. In this
paper, we propose the Efficient Stereo Network (ESNet), which achieves high
performance and efficient inference at the same time. ESNet relies only on 2D
convolution and computes multi-scale cost volume efficiently using a
warping-based method to improve the performance in regions with fine-details.
In addition, we address the matching ambiguity issue in the occluded region by
proposing ESNet-M, a variant of ESNet that additionally estimates an occlusion
mask without supervision. We further improve the network performance by
proposing a new training scheme that includes dataset scheduling and
unsupervised pre-training. Compared with other low-cost dense stereo depth
estimation methods, our proposed approach achieves state-of-the-art performance
on the Scene Flow [1], DrivingStereo [2], and KITTI-2015 dataset [3]. Our code
will be made available.
</p>
<a href="http://arxiv.org/abs/2103.03922" target="_blank">arXiv:2103.03922</a> [<a href="http://arxiv.org/pdf/2103.03922" target="_blank">pdf</a>]

<h2>Attention-Enhanced Cross-Task Network for Analysing Multiple Attributes of Lung Nodules in CT. (arXiv:2103.03931v1 [cs.CV])</h2>
<h3>Xiaohang Fu, Lei Bi, Ashnil Kumar, Michael Fulham, Jinman Kim</h3>
<p>Accurate characterisation of visual attributes such as spiculation,
lobulation, and calcification of lung nodules is critical in cancer management.
The characterisation of these attributes is often subjective, which may lead to
high inter- and intra-observer variability. Furthermore, lung nodules are often
heterogeneous in the cross-sectional image slices of a 3D volume. Current
state-of-the-art methods that score multiple attributes rely on deep
learning-based multi-task learning (MTL) schemes. These methods, however,
extract shared visual features across attributes and then examine each
attribute without explicitly leveraging their inherent intercorrelations.
Furthermore, current methods either treat each slice with equal importance
without considering their relevance or heterogeneity, or restrict the number of
input slices, which limits performance. In this study, we address these
challenges with a new convolutional neural network (CNN)-based MTL model that
incorporates attention modules to simultaneously score 9 visual attributes of
lung nodules in computed tomography (CT) image volumes. Our model processes
entire nodule volumes of arbitrary depth and uses a slice attention module to
filter out irrelevant slices. We also introduce cross-attribute and attribute
specialisation attention modules that learn an optimal amalgamation of
meaningful representations to leverage relationships between attributes. We
demonstrate that our model outperforms previous state-of-the-art methods at
scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary
nodules from over 1,000 patients. Our attention modules also provide
easy-to-interpret weights that offer insights into the predictions of the
model.
</p>
<a href="http://arxiv.org/abs/2103.03931" target="_blank">arXiv:2103.03931</a> [<a href="http://arxiv.org/pdf/2103.03931" target="_blank">pdf</a>]

<h2>An Ensemble with Shared Representations Based on Convolutional Networks for Continually Learning Facial Expressions. (arXiv:2103.03934v1 [cs.CV])</h2>
<h3>Henrique Siqueira, Pablo Barros, Sven Magg, Stefan Wermter</h3>
<p>Social robots able to continually learn facial expressions could
progressively improve their emotion recognition capability towards people
interacting with them. Semi-supervised learning through ensemble predictions is
an efficient strategy to leverage the high exposure of unlabelled facial
expressions during human-robot interactions. Traditional ensemble-based
systems, however, are composed of several independent classifiers leading to a
high degree of redundancy, and unnecessary allocation of computational
resources. In this paper, we proposed an ensemble based on convolutional
networks where the early layers are strong low-level feature extractors, and
their representations shared with an ensemble of convolutional branches. This
results in a significant drop in redundancy of low-level features processing.
Training in a semi-supervised setting, we show that our approach is able to
continually learn facial expressions through ensemble predictions using
unlabelled samples from different data distributions.
</p>
<a href="http://arxiv.org/abs/2103.03934" target="_blank">arXiv:2103.03934</a> [<a href="http://arxiv.org/pdf/2103.03934" target="_blank">pdf</a>]

<h2>An automated approach to mitigate transcription errors in braille texts for the Portuguese language. (arXiv:2103.03935v1 [cs.CV])</h2>
<h3>Andr&#xe9; Roberto Ortoncelli, Marlon Marcon, Franciele Beal</h3>
<p>The quota system in Brazil made it possible to include blind students in
higher education. Teachers' lack of knowledge about the braille system can
represent a barrier between them and students who use it for writing and
reading. Computer-vision-based transcription solutions represent mechanisms for
reducing understanding restrictions on this system. However, such tools face
nuisances inherent to image processing systems, e.g., illumination, noise, and
scale, harming the result. This paper presents an automated approach to
mitigate transcription errors in braille texts for the Portuguese language. We
propose a selection function, combined with dictionaries, that provides the
best correspondence of words based on their braille representation. We
validated our proposal on a dataset of synthetic images by submitting them to
different noise levels and testing the proposal's robustness. Experimental
results confirm the effectiveness of the solution compared to a standard
approach. As a contribution of this paper, we expect to provide a method to
support robust and adaptable solutions to real use conditions.
</p>
<a href="http://arxiv.org/abs/2103.03935" target="_blank">arXiv:2103.03935</a> [<a href="http://arxiv.org/pdf/2103.03935" target="_blank">pdf</a>]

<h2>Disambiguating Affective Stimulus Associations for Robot Perception and Dialogue. (arXiv:2103.03940v1 [cs.RO])</h2>
<h3>Henrique Siqueira, Alexander Sutherland, Pablo Barros, Mattias Kerzel, Sven Magg, Stefan Wermter</h3>
<p>Effectively recognising and applying emotions to interactions is a highly
desirable trait for social robots. Implicitly understanding how subjects
experience different kinds of actions and objects in the world is crucial for
natural HRI interactions, with the possibility to perform positive actions and
avoid negative actions. In this paper, we utilize the NICO robot's appearance
and capabilities to give the NICO the ability to model a coherent affective
association between a perceived auditory stimulus and a temporally asynchronous
emotion expression. This is done by combining evaluations of emotional valence
from vision and language. NICO uses this information to make decisions about
when to extend conversations in order to accrue more affective information if
the representation of the association is not coherent. Our primary contribution
is providing a NICO robot with the ability to learn the affective associations
between a perceived auditory stimulus and an emotional expression. NICO is able
to do this for both individual subjects and specific stimuli, with the aid of
an emotion-driven dialogue system that rectifies emotional expression
incoherences. The robot is then able to use this information to determine a
subject's enjoyment of perceived auditory stimuli in a real HRI scenario.
</p>
<a href="http://arxiv.org/abs/2103.03940" target="_blank">arXiv:2103.03940</a> [<a href="http://arxiv.org/pdf/2103.03940" target="_blank">pdf</a>]

<h2>Simultaneous Scene Reconstruction and Whole-Body Motion Planning for Safe Operation in Dynamic Environments. (arXiv:2103.03958v1 [cs.RO])</h2>
<h3>Mark Nicholas Finean, Wolfgang Merkt, Ioannis Havoutis</h3>
<p>Recent work has demonstrated real-time mapping and reconstruction from dense
perception, while motion planning based on distance fields has been shown to
achieve fast, collision-free motion synthesis with good convergence properties.
However, demonstration of a fully integrated system that can safely re-plan in
unknown environments, in the presence of static and dynamic obstacles, has
remained an open challenge. In this work, we first study the impact that signed
and unsigned distance fields have on optimisation convergence, and the
resultant error cost in trajectory optimisation problems in 2D path planning,
arm manipulator motion planning, and whole-body loco-manipulation planning. We
further analyse the performance of three state-of-the-art approaches to
generating distance fields (Voxblox, Fiesta, and GPU-Voxels) for use in
real-time environment reconstruction. Finally, we use our findings to construct
a practical hybrid mapping and motion planning system which uses GPU-Voxels and
GPMP2 to perform receding-horizon whole-body motion planning that can smoothly
avoid moving obstacles in 3D space using live sensor data. Our results are
validated in simulation and on a real-world Toyota Human Support Robot (HSR).
</p>
<a href="http://arxiv.org/abs/2103.03958" target="_blank">arXiv:2103.03958</a> [<a href="http://arxiv.org/pdf/2103.03958" target="_blank">pdf</a>]

<h2>Interpolation of CT Projections by Exploiting Their Self-Similarity and Smoothness. (arXiv:2103.03968v1 [cs.CV])</h2>
<h3>Davood Karimi, Rabab K. Ward</h3>
<p>As the medical usage of computed tomography (CT) continues to grow, the
radiation dose should remain at a low level to reduce the health risks.
Therefore, there is an increasing need for algorithms that can reconstruct
high-quality images from low-dose scans. In this regard, most of the recent
studies have focused on iterative reconstruction algorithms, and little
attention has been paid to restoration of the projection measurements, i.e.,
the sinogram. In this paper, we propose a novel sinogram interpolation
algorithm. The proposed algorithm exploits the self-similarity and smoothness
of the sinogram. Sinogram self-similarity is modeled in terms of the similarity
of small blocks extracted from stacked projections. The smoothness is modeled
via second-order total variation. Experiments with simulated and real CT data
show that sinogram interpolation with the proposed algorithm leads to a
substantial improvement in the quality of the reconstructed image, especially
on low-dose scans. The proposed method can result in a significant reduction in
the number of projection measurements. This will reduce the radiation dose and
also the amount of data that need to be stored or transmitted, if the
reconstruction is to be performed in a remote site.
</p>
<a href="http://arxiv.org/abs/2103.03968" target="_blank">arXiv:2103.03968</a> [<a href="http://arxiv.org/pdf/2103.03968" target="_blank">pdf</a>]

<h2>Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection. (arXiv:2103.03977v1 [cs.CV])</h2>
<h3>Nguyen Anh Minh Mai, Pierre Duthon, Louahdi Khoudour, Alain Crouzil, Sergio A. Velastin</h3>
<p>The ability to accurately detect and localize objects is recognized as being
the most important for the perception of self-driving cars. From 2D to 3D
object detection, the most difficult is to determine the distance from the
ego-vehicle to objects. Expensive technology like LiDAR can provide a precise
and accurate depth information, so most studies have tended to focus on this
sensor showing a performance gap between LiDAR-based methods and camera-based
methods. Although many authors have investigated how to fuse LiDAR with RGB
cameras, as far as we know there are no studies to fuse LiDAR and stereo in a
deep neural network for the 3D object detection task. This paper presents
SLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera
via a neural network for depth estimation to achieve better dense depth maps
and thereby improves 3D object detection performance. Since 4-beam LiDAR is
cheaper than the well-known 64-beam LiDAR, this approach is also classified as
a low-cost sensors-based method. Through evaluation on the KITTI benchmark, it
is shown that the proposed method significantly improves depth estimation
performance compared to a baseline method. Also, when applying it to 3D object
detection, a new state of the art on low-cost sensor based method is achieved.
</p>
<a href="http://arxiv.org/abs/2103.03977" target="_blank">arXiv:2103.03977</a> [<a href="http://arxiv.org/pdf/2103.03977" target="_blank">pdf</a>]

<h2>Passing Through Narrow Gaps with Deep Reinforcement Learning. (arXiv:2103.03991v1 [cs.RO])</h2>
<h3>Brendan Tidd, Akansel Cosgun, Jurgen Leitner, Nicolas Hudson</h3>
<p>The DARPA subterranean challenge requires teams of robots to traverse
difficult and diverse underground environments. Traversing small gaps is one of
the challenging scenarios that robots encounter. Imperfect sensor information
makes it difficult for classical navigation methods, where behaviours require
significant manual fine tuning. In this paper we present a deep reinforcement
learning method for autonomously navigating through small gaps, where contact
between the robot and the gap may be required. We first learn a gap behaviour
policy to get through small gaps (only centimeters wider than the robot). We
then learn a goal-conditioned behaviour selection policy that determines when
to activate the gap behaviour policy. We train our policies in simulation and
demonstrate their effectiveness with a large tracked robot in simulation and on
the real platform. In simulation experiments, our approach achieves 93% success
rate when the gap behaviour is activated manually by an operator, and 67% with
autonomous activation using the behaviour selection policy. In real robot
experiments, our approach achieves a success rate of 73% with manual
activation, and 40% with autonomous behaviour selection. While we show the
feasibility of our approach in simulation, the difference in performance
between simulated and real world scenarios highlight the difficulty of direct
sim-to-real transfer for deep reinforcement learning policies. In both the
simulated and real world environments alternative methods were unable to
traverse the gap.
</p>
<a href="http://arxiv.org/abs/2103.03991" target="_blank">arXiv:2103.03991</a> [<a href="http://arxiv.org/pdf/2103.03991" target="_blank">pdf</a>]

<h2>Modeling the locomotion of articulated soft robots in granular medium. (arXiv:2103.03993v1 [cs.RO])</h2>
<h3>Yayun Du, Jacqueline Lam, Karunesh Sachanandani, Mohammad Khalid Jawed</h3>
<p>Soft robots, in contrast to their rigid counter parts, have infinite degrees
of freedom that are coupled with their interaction with the environment. We
consider the locomotion of an untethered robot, in the granular medium,
comprised of multiple flexible flagella that rotate about an axis by a motor.
Drag from the grains causes the flagella to deform and the deformed shape
generates a net forward propulsion. This external drag force depends on the
shape of the flagella, while the change in flagellar shape is the result of the
competition between the external loading and elastic forces. We introduce a
numerical tool that couples discrete differential geometry based simulation of
elastic rods - our model for flagella - and a resistive force theory based
model for the drag. In parallel with simulations, we conduct experiments to
quantify the propulsive speed of this class of robots. We find reasonable
quantitative agreement between experiments and simulations. Owing to a
rod-based kinematic representation of the robot, the simulation runs faster
than real-time, and, therefore, we can use it as a design tool for this class
of soft robots. We find that there is an optimal rotational speed at which
maximum efficiency is achieved. Moreover, both experiments and simulations show
that increasing the number of flagella decreases the speed of the robot. We
also gain insight into the mechanics of granular medium - while resistive force
theory can successfully describe the propulsion at low number of flagella, it
fails when more flagella are added to the robot.
</p>
<a href="http://arxiv.org/abs/2103.03993" target="_blank">arXiv:2103.03993</a> [<a href="http://arxiv.org/pdf/2103.03993" target="_blank">pdf</a>]

<h2>Bilateral Control-Based Imitation Learning for Velocity-Controlled Robot. (arXiv:2103.04004v1 [cs.RO])</h2>
<h3>Sho Sakaino</h3>
<p>Machine learning is now playing important role in robotic object
manipulation. In addition, force control is necessary for manipulating various
objects to achieve robustness against perturbations of configurations and
stiffness. The author's group revealed that fast and dynamic object
manipulation with force control can be obtained by bilateral control-based
imitation learning. However, the method is applicable only in robots that can
control torque, while it is not applicable in robots that can only follow
position and velocity commands like many commercially available robots. Then,
in this research, a way to implement bilateral control-based imitation learning
to velocity-controlled robots is proposed. The validity of the proposed method
is experimentally verified by a mopping task.
</p>
<a href="http://arxiv.org/abs/2103.04004" target="_blank">arXiv:2103.04004</a> [<a href="http://arxiv.org/pdf/2103.04004" target="_blank">pdf</a>]

<h2>Fibrosis-Net: A Tailored Deep Convolutional Neural Network Design for Prediction of Pulmonary Fibrosis Progression from Chest CT Images. (arXiv:2103.04008v1 [cs.CV])</h2>
<h3>Alexander Wong, Jack Lu, Adam Dorfman, Paul McInnis, Mahmoud Famouri, Daniel Manary, James Ren Hou Lee, Michael Lynch</h3>
<p>Pulmonary fibrosis is a devastating chronic lung disease that causes
irreparable lung tissue scarring and damage, resulting in progressive loss in
lung capacity and has no known cure. A critical step in the treatment and
management of pulmonary fibrosis is the assessment of lung function decline,
with computed tomography (CT) imaging being a particularly effective method for
determining the extent of lung damage caused by pulmonary fibrosis. Motivated
by this, we introduce Fibrosis-Net, a deep convolutional neural network design
tailored for the prediction of pulmonary fibrosis progression from chest CT
images. More specifically, machine-driven design exploration was leveraged to
determine a strong architectural design for CT lung analysis, upon which we
build a customized network design tailored for predicting forced vital capacity
(FVC) based on a patient's CT scan, initial spirometry measurement, and
clinical metadata. Finally, we leverage an explainability-driven performance
validation strategy to study the decision-making behaviour of Fibrosis-Net as
to verify that predictions are based on relevant visual indicators in CT
images. Experiments using the OSIC Pulmonary Fibrosis Progression Challenge
benchmark dataset showed that the proposed Fibrosis-Net is able to achieve a
significantly higher modified Laplace Log Likelihood score than the winning
solutions on the challenge leaderboard. Furthermore, explainability-driven
performance validation demonstrated that the proposed Fibrosis-Net exhibits
correct decision-making behaviour by leveraging clinically-relevant visual
indicators in CT images when making predictions on pulmonary fibrosis progress.
While Fibrosis-Net is not yet a production-ready clinical assessment solution,
we hope that releasing the model in open source manner will encourage
researchers, clinicians, and citizen data scientists alike to leverage and
build upon it.
</p>
<a href="http://arxiv.org/abs/2103.04008" target="_blank">arXiv:2103.04008</a> [<a href="http://arxiv.org/pdf/2103.04008" target="_blank">pdf</a>]

<h2>Learning from Counting: Leveraging Temporal Classification for Weakly Supervised Object Localization and Detection. (arXiv:2103.04009v1 [cs.CV])</h2>
<h3>Chia-Yu Hsu, Wenwen Li</h3>
<p>This paper reports a new solution of leveraging temporal classification to
support weakly supervised object detection (WSOD). Specifically, we introduce
raster scan-order techniques to serialize 2D images into 1D sequence data, and
then leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist
Temporal Classification) network to achieve object localization based on a
total count (of interested objects). We term our proposed network LSTM-CCTC
(Count-based CTC). This "learning from counting" strategy differs from existing
WSOD methods in that our approach automatically identifies critical points on
or near a target object. This strategy significantly reduces the need of
generating a large number of candidate proposals for object localiza- tion.
Experiments show that our method yields state-of-the-art performance based on
an evaluation on PASCAL VOC datasets.
</p>
<a href="http://arxiv.org/abs/2103.04009" target="_blank">arXiv:2103.04009</a> [<a href="http://arxiv.org/pdf/2103.04009" target="_blank">pdf</a>]

<h2>Simultaneously Localize, Segment and Rank the Camouflaged Objects. (arXiv:2103.04011v1 [cs.CV])</h2>
<h3>Yunqiu Lyu, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, Deng-Ping Fan</h3>
<p>Camouflage is a key defence mechanism across species that is critical to
survival. Common strategies for camouflage include background matching,
imitating the color and pattern of the environment, and disruptive coloration,
disguising body outlines [35]. Camouflaged object detection (COD) aims to
segment camouflaged objects hiding in their surroundings. Existing COD models
are built upon binary ground truth to segment the camouflaged objects without
illustrating the level of camouflage. In this paper, we revisit this task and
argue that explicitly modeling the conspicuousness of camouflaged objects
against their particular backgrounds can not only lead to a better
understanding about camouflage and evolution of animals, but also provide
guidance to design more sophisticated camouflage techniques. Furthermore, we
observe that it is some specific parts of the camouflaged objects that make
them detectable by predators. With the above understanding about camouflaged
objects, we present the first ranking based COD network (Rank-Net) to
simultaneously localize, segment and rank camouflaged objects. The localization
model is proposed to find the discriminative regions that make the camouflaged
object obvious. The segmentation model segments the full scope of the
camouflaged objects. And, the ranking model infers the detectability of
different camouflaged objects. Moreover, we contribute a large COD testing set
to evaluate the generalization ability of COD models. Experimental results show
that our model achieves new state-of-the-art, leading to a more interpretable
COD network.
</p>
<a href="http://arxiv.org/abs/2103.04011" target="_blank">arXiv:2103.04011</a> [<a href="http://arxiv.org/pdf/2103.04011" target="_blank">pdf</a>]

<h2>Dynamic Resource Management for Providing QoS in Drone Delivery Systems. (arXiv:2103.04015v1 [cs.RO])</h2>
<h3>Behzad Khamidehi, Majid Raeis, Elvino S. Sousa</h3>
<p>Drones have been considered as an alternative means of package delivery to
reduce the delivery cost and time. Due to the battery limitations, the drones
are best suited for last-mile delivery, i.e., the delivery from the package
distribution centers (PDCs) to the customers. Since a typical delivery system
consists of multiple PDCs, each having random and time-varying demands, the
dynamic drone-to-PDC allocation would be of great importance in meeting the
demand in an efficient manner. In this paper, we study the dynamic UAV
assignment problem for a drone delivery system with the goal of providing
measurable Quality of Service (QoS) guarantees. We adopt a queueing theoretic
approach to model the customer-service nature of the problem. Furthermore, we
take a deep reinforcement learning approach to obtain a dynamic policy for the
re-allocation of the UAVs. This policy guarantees a probabilistic upper-bound
on the queue length of the packages waiting in each PDC, which is beneficial
from both the service provider's and the customers' viewpoints. We evaluate the
performance of our proposed algorithm by considering three broad arrival
classes, including Bernoulli, Time-Varying Bernoulli, and Markov-Modulated
Bernoulli arrivals. Our results show that the proposed method outperforms the
baselines, particularly in scenarios with Time-Varying and Markov-Modulated
Bernoulli arrivals, which are more representative of real-world demand
patterns. Moreover, our algorithm satisfies the QoS constraints in all the
studied scenarios while minimizing the average number of UAVs in use.
</p>
<a href="http://arxiv.org/abs/2103.04015" target="_blank">arXiv:2103.04015</a> [<a href="http://arxiv.org/pdf/2103.04015" target="_blank">pdf</a>]

<h2>Indoor Future Person Localization from an Egocentric Wearable Camera. (arXiv:2103.04019v1 [cs.CV])</h2>
<h3>Jianing Qiu, Frank P.-W. Lo, Xiao Gu, Yingnan Sun, Shuo Jiang, Benny Lo</h3>
<p>Accurate prediction of future person location and movement trajectory from an
egocentric wearable camera can benefit a wide range of applications, such as
assisting visually impaired people in navigation, and the development of
mobility assistance for people with disability. In this work, a new egocentric
dataset was constructed using a wearable camera, with 8,250 short clips of a
targeted person either walking 1) toward, 2) away, or 3) across the camera
wearer in indoor environments, or 4) staying still in the scene, and 13,817
person bounding boxes were manually labelled. Apart from the bounding boxes,
the dataset also contains the estimated pose of the targeted person as well as
the IMU signal of the wearable camera at each time point. An LSTM-based
encoder-decoder framework was designed to predict the future location and
movement trajectory of the targeted person in this egocentric setting.
Extensive experiments have been conducted on the new dataset, and have shown
that the proposed method is able to reliably and better predict future person
location and trajectory in egocentric videos captured by the wearable camera
compared to three baselines.
</p>
<a href="http://arxiv.org/abs/2103.04019" target="_blank">arXiv:2103.04019</a> [<a href="http://arxiv.org/pdf/2103.04019" target="_blank">pdf</a>]

<h2>NeRD: Neural Representation of Distribution for Medical Image Segmentation. (arXiv:2103.04020v1 [cs.CV])</h2>
<h3>Hang Zhang, Rongguang Wang, Jinwei Zhang, Chao Li, Gufeng Yang, Pascal Spincemaille, Thanh Nguyen, Yi Wang</h3>
<p>We introduce Neural Representation of Distribution (NeRD) technique, a module
for convolutional neural networks (CNNs) that can estimate the feature
distribution by optimizing an underlying function mapping image coordinates to
the feature distribution. Using NeRD, we propose an end-to-end deep learning
model for medical image segmentation that can compensate the negative impact of
feature distribution shifting issue caused by commonly used network operations
such as padding and pooling. An implicit function is used to represent the
parameter space of the feature distribution by querying the image coordinate.
With NeRD, the impact of issues such as over-segmenting and missing have been
reduced, and experimental results on the challenging white matter lesion
segmentation and left atrial segmentation verify the effectiveness of the
proposed method. The code is available via https://github.com/tinymilky/NeRD.
</p>
<a href="http://arxiv.org/abs/2103.04020" target="_blank">arXiv:2103.04020</a> [<a href="http://arxiv.org/pdf/2103.04020" target="_blank">pdf</a>]

<h2>PISE: Person Image Synthesis and Editing with Decoupled GAN. (arXiv:2103.04023v1 [cs.CV])</h2>
<h3>Jinsong Zhang, Kun Li, Yu-Kun Lai, Jingyu Yang</h3>
<p>Person image synthesis, e.g., pose transfer, is a challenging problem due to
large variation and occlusion. Existing methods have difficulties predicting
reasonable invisible regions and fail to decouple the shape and style of
clothing, which limits their applications on person image editing. In this
paper, we propose PISE, a novel two-stage generative model for Person Image
Synthesis and Editing, which is able to generate realistic person images with
desired poses, textures, or semantic layouts. For human pose transfer, we first
synthesize a human parsing map aligned with the target pose to represent the
shape of clothing by a parsing generator, and then generate the final image by
an image generator. To decouple the shape and style of clothing, we propose
joint global and local per-region encoding and normalization to predict the
reasonable style of clothing for invisible regions. We also propose
spatial-aware normalization to retain the spatial context relationship in the
source image. The results of qualitative and quantitative experiments
demonstrate the superiority of our model on human pose transfer. Besides, the
results of texture transfer and region editing show that our model can be
applied to person image editing.
</p>
<a href="http://arxiv.org/abs/2103.04023" target="_blank">arXiv:2103.04023</a> [<a href="http://arxiv.org/pdf/2103.04023" target="_blank">pdf</a>]

<h2>Morphological Operation Residual Blocks: Enhancing 3D Morphological Feature Representation in Convolutional Neural Networks for Semantic Segmentation of Medical Images. (arXiv:2103.04026v1 [cs.CV])</h2>
<h3>Chentian Li, Chi Ma, William W. Lu</h3>
<p>The shapes and morphology of the organs and tissues are important prior
knowledge in medical imaging recognition and segmentation. The morphological
operation is a well-known method for morphological feature extraction. As the
morphological operation is performed well in hand-crafted image segmentation
techniques, it is also promising to design an approach to approximate
morphological operation in the convolutional networks. However, using the
traditional convolutional neural network as a black-box is usually hard to
specify the morphological operation action. Here, we introduced a 3D
morphological operation residual block to extract morphological features in
end-to-end deep learning models for semantic segmentation. This study proposed
a novel network block architecture that embedded the morphological operation as
an infinitely strong prior in the convolutional neural network. Several 3D deep
learning models with the proposed morphological operation block were built and
compared in different medical imaging segmentation tasks. Experimental results
showed the proposed network achieved a relatively higher performance in the
segmentation tasks comparing with the conventional approach. In conclusion, the
novel network block could be easily embedded in traditional networks and
efficiently reinforce the deep learning models for medical imaging
segmentation.
</p>
<a href="http://arxiv.org/abs/2103.04026" target="_blank">arXiv:2103.04026</a> [<a href="http://arxiv.org/pdf/2103.04026" target="_blank">pdf</a>]

<h2>Learning to Predict Vehicle Trajectories with Model-based Planning. (arXiv:2103.04027v1 [cs.CV])</h2>
<h3>Haoran Song, Di Luan, Wenchao Ding, Michael Yu Wang, Qifeng Chen</h3>
<p>Predicting the future trajectories of on-road vehicles is critical for
autonomous driving. In this paper, we introduce a novel prediction framework
called PRIME, which stands for Prediction with Model-based Planning. Unlike
recent prediction works that utilize neural networks to model scene context and
produce unconstrained trajectories, PRIME is designed to generate accurate and
feasibility-guaranteed future trajectory predictions, which guarantees the
trajectory feasibility by exploiting a model-based generator to produce future
trajectories under explicit constraints and enables accurate multimodal
prediction by using a learning-based evaluator to select future trajectories.
We conduct experiments on the large-scale Argoverse Motion Forecasting
Benchmark. Our PRIME outperforms state-of-the-art methods in prediction
accuracy, feasibility, and robustness under imperfect tracking. Furthermore, we
achieve the 1st place on the Argoervese Leaderboard.
</p>
<a href="http://arxiv.org/abs/2103.04027" target="_blank">arXiv:2103.04027</a> [<a href="http://arxiv.org/pdf/2103.04027" target="_blank">pdf</a>]

<h2>Estimation of Spatially Correlated Ocean Currents from Ensemble Forecasts and Online Measurements. (arXiv:2103.04036v1 [cs.RO])</h2>
<h3>K. Y. Cadmus To, Felix H. Kong, Ki Myung Brian Lee, Chanyeol Yoo, Stuart Anstee, Robert Fitch</h3>
<p>We present a method to estimate two-dimensional, time-invariant oceanic flow
fields based on data from both ensemble forecasts and online measurements. Our
method produces a spatially coherent estimate in a computationally efficient
manner suitable for use in marine robotics for path planning and related
applications. We use kernel methods and singular value decomposition to find a
compact model of the ensemble data that is represented as a linear combination
of basis flow fields and that preserves the spatial correlations present in the
data. Online measurements of ocean current, taken for example by marine robots,
can then be incorporated using recursive Bayesian estimation. We provide
computational analysis, performance comparisons with related methods, and
demonstration with real-world ensemble data to show the computational
efficiency and validity of our method. Possible applications in addition to
path planning include active perception methods for model improvement through
intentional choice of measurement locations.
</p>
<a href="http://arxiv.org/abs/2103.04036" target="_blank">arXiv:2103.04036</a> [<a href="http://arxiv.org/pdf/2103.04036" target="_blank">pdf</a>]

<h2>Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision. (arXiv:2103.04037v1 [cs.CV])</h2>
<h3>Andrew Shin, Masato Ishii, Takuya Narihira</h3>
<p>Transformer architectures have brought about fundamental changes to
computational linguistic field, which had been dominated by recurrent neural
networks for many years. Its success also implies drastic changes in
cross-modal tasks with language and vision, and many researchers have already
tackled the issue. In this paper, we review some of the most critical
milestones in the field, as well as overall trends on how transformer
architecture has been incorporated into visuolinguistic cross-modal tasks.
Furthermore, we discuss its current limitations and speculate upon some of the
prospects that we find imminent.
</p>
<a href="http://arxiv.org/abs/2103.04037" target="_blank">arXiv:2103.04037</a> [<a href="http://arxiv.org/pdf/2103.04037" target="_blank">pdf</a>]

<h2>ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic. (arXiv:2103.04039v1 [cs.CV])</h2>
<h3>Xiangtao Kong, Hengyuan Zhao, Yu Qiao, Chao Dong</h3>
<p>We aim at accelerating super-resolution (SR) networks on large images
(2K-8K). The large images are usually decomposed into small sub-images in
practical usages. Based on this processing, we found that different image
regions have different restoration difficulties and can be processed by
networks with different capacities. Intuitively, smooth areas are easier to
super-solve than complex textures. To utilize this property, we can adopt
appropriate SR networks to process different sub-images after the
decomposition. On this basis, we propose a new solution pipeline -- ClassSR
that combines classification and SR in a unified framework. In particular, it
first uses a Class-Module to classify the sub-images into different classes
according to restoration difficulties, then applies an SR-Module to perform SR
for different classes. The Class-Module is a conventional classification
network, while the SR-Module is a network container that consists of the
to-be-accelerated SR network and its simplified versions. We further introduce
a new classification method with two losses -- Class-Loss and Average-Loss to
produce the classification results. After joint training, a majority of
sub-images will pass through smaller networks, thus the computational cost can
be significantly reduced. Experiments show that our ClassSR can help most
existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on
DIV8K datasets. This general framework can also be applied in other low-level
vision tasks.
</p>
<a href="http://arxiv.org/abs/2103.04039" target="_blank">arXiv:2103.04039</a> [<a href="http://arxiv.org/pdf/2103.04039" target="_blank">pdf</a>]

<h2>Noisy Label Learning for Large-scale Medical Image Classification. (arXiv:2103.04053v1 [cs.CV])</h2>
<h3>Fengbei Liu, Yu Tian, Filipe R. Cordeiro, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro</h3>
<p>The classification accuracy of deep learning models depends not only on the
size of their training sets, but also on the quality of their labels. In
medical image classification, large-scale datasets are becoming abundant, but
their labels will be noisy when they are automatically extracted from radiology
reports using natural language processing tools. Given that deep learning
models can easily overfit these noisy-label samples, it is important to study
training approaches that can handle label noise. In this paper, we adapt a
state-of-the-art (SOTA) noisy-label multi-class training approach to learn a
multi-label classifier for the dataset Chest X-ray14, which is a large scale
dataset known to contain label noise in the training set. Given that this
dataset also has label noise in the testing set, we propose a new theoretically
sound method to estimate the performance of the model on a hidden clean testing
data, given the result on the noisy testing data. Using our clean data
performance estimation, we notice that the majority of label noise on Chest
X-ray14 is present in the class 'No Finding', which is intuitively correct
because this is the most likely class to contain one or more of the 14 diseases
due to labelling mistakes.
</p>
<a href="http://arxiv.org/abs/2103.04053" target="_blank">arXiv:2103.04053</a> [<a href="http://arxiv.org/pdf/2103.04053" target="_blank">pdf</a>]

<h2>Visualizing Robot Intent for Object Handovers with Augmented Reality. (arXiv:2103.04055v1 [cs.RO])</h2>
<h3>Rhys Newbury, Akansel Cosgun, Tysha Crowley-Davis, Wesley P. Chan, Tom Drummond, Elizabeth Croft</h3>
<p>Humans are very skillful in communicating their intent for when and where a
handover would occur. On the other hand, even the state-of-the-art robotic
implementations for handovers display a general lack of communication skills.
We propose visualizing the internal state and intent of robots for
Human-to-Robot Handovers using Augmented Reality. Specifically, we visualize 3D
models of the object and the robotic gripper to communicate the robot's
estimation of where the object is and the pose that the robot intends to grasp
the object. We conduct a user study with 16 participants, in which each
participant handed over a cube-shaped object to the robot 12 times. Results
show that visualizing robot intent using augmented reality substantially
improves the subjective experience of the users for handovers and decreases the
time to transfer the object. Results also indicate that the benefits of
augmented reality are still present even when the robot makes errors in
localizing the object.
</p>
<a href="http://arxiv.org/abs/2103.04055" target="_blank">arXiv:2103.04055</a> [<a href="http://arxiv.org/pdf/2103.04055" target="_blank">pdf</a>]

<h2>A Simple and Efficient Multi-task Network for 3D Object Detection and Road Understanding. (arXiv:2103.04056v1 [cs.CV])</h2>
<h3>Di Feng, Yiyang Zhou, Chenfeng Xu, Masayoshi Tomizuka, Wei Zhan</h3>
<p>Detecting dynamic objects and predicting static road information such as
drivable areas and ground heights are crucial for safe autonomous driving.
Previous works studied each perception task separately, and lacked a collective
quantitative analysis. In this work, we show that it is possible to perform all
perception tasks via a simple and efficient multi-task network. Our proposed
network, LidarMTL, takes raw LiDAR point cloud as inputs, and predicts six
perception outputs for 3D object detection and road understanding. The network
is based on an encoder-decoder architecture with 3D sparse convolution and
deconvolution operations. Extensive experiments verify the proposed method with
competitive accuracies compared to state-of-the-art object detectors and other
task-specific networks. LidarMTL is also leveraged for online localization.
Code and pre-trained model have been made available at
https://github.com/frankfengdi/LidarMTL.
</p>
<a href="http://arxiv.org/abs/2103.04056" target="_blank">arXiv:2103.04056</a> [<a href="http://arxiv.org/pdf/2103.04056" target="_blank">pdf</a>]

<h2>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning. (arXiv:2103.04059v1 [cs.CV])</h2>
<h3>Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, Mehrtash Harandi</h3>
<p>Few-shot class incremental learning (FSCIL) portrays the problem of learning
new concepts gradually, where only a few examples per concept are available to
the learner. Due to the limited number of examples for training, the techniques
developed for standard incremental learning cannot be applied verbatim to
FSCIL. In this work, we introduce a distillation algorithm to address the
problem of FSCIL and propose to make use of semantic information during
training. To this end, we make use of word embeddings as semantic information
which is cheap to obtain and which facilitate the distillation process.
Furthermore, we propose a method based on an attention mechanism on multiple
parallel embeddings of visual data to align visual and semantic vectors, which
reduces issues related to catastrophic forgetting. Via experiments on
MiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art
results by outperforming existing approaches.
</p>
<a href="http://arxiv.org/abs/2103.04059" target="_blank">arXiv:2103.04059</a> [<a href="http://arxiv.org/pdf/2103.04059" target="_blank">pdf</a>]

<h2>Adaptive Multi-Teacher Multi-level Knowledge Distillation. (arXiv:2103.04062v1 [cs.CV])</h2>
<h3>Yuang Liu, Wei Zhang, Jun Wang</h3>
<p>Knowledge distillation~(KD) is an effective learning paradigm for improving
the performance of lightweight student networks by utilizing additional
supervision knowledge distilled from teacher networks. Most pioneering studies
either learn from only a single teacher in their distillation learning methods,
neglecting the potential that a student can learn from multiple teachers
simultaneously, or simply treat each teacher to be equally important, unable to
reveal the different importance of teachers for specific examples. To bridge
this gap, we propose a novel adaptive multi-teacher multi-level knowledge
distillation learning framework~(AMTML-KD), which consists two novel insights:
(i) associating each teacher with a latent representation to adaptively learn
instance-level teacher importance weights which are leveraged for acquiring
integrated soft-targets~(high-level knowledge) and (ii) enabling the
intermediate-level hints~(intermediate-level knowledge) to be gathered from
multiple teachers by the proposed multi-group hint strategy. As such, a student
model can learn multi-level knowledge from multiple teachers through AMTML-KD.
Extensive results on publicly available datasets demonstrate the proposed
learning framework ensures student to achieve improved performance than strong
competitors.
</p>
<a href="http://arxiv.org/abs/2103.04062" target="_blank">arXiv:2103.04062</a> [<a href="http://arxiv.org/pdf/2103.04062" target="_blank">pdf</a>]

<h2>Improving Automated Sonar Video Analysis to Notify About Jellyfish Blooms. (arXiv:2103.04068v1 [cs.CV])</h2>
<h3>Artjoms Gorpincenko, Geoffrey French, Peter Knight, Mike Challiss, Michal Mackiewicz</h3>
<p>Human enterprise often suffers from direct negative effects caused by
jellyfish blooms. The investigation of a prior jellyfish monitoring system
showed that it was unable to reliably perform in a cross validation setting,
i.e. in new underwater environments. In this paper, a number of enhancements
are proposed to the part of the system that is responsible for object
classification. First, the training set is augmented by adding synthetic data,
making the deep learning classifier able to generalise better. Then, the
framework is enhanced by employing a new second stage model, which analyzes the
outputs of the first network to make the final prediction. Finally, weighted
loss and confidence threshold are added to balance out true and false
positives. With all the upgrades in place, the system can correctly classify
30.16% (comparing to the initial 11.52%) of all spotted jellyfish, keep the
amount of false positives as low as 0.91% (comparing to the initial 2.26%) and
operate in real-time within the computational constraints of an autonomous
embedded platform.
</p>
<a href="http://arxiv.org/abs/2103.04068" target="_blank">arXiv:2103.04068</a> [<a href="http://arxiv.org/pdf/2103.04068" target="_blank">pdf</a>]

<h2>Adaptive Lidar Scan Frame Integration: Tracking Known MAVs in 3D Point Clouds. (arXiv:2103.04069v1 [cs.RO])</h2>
<h3>Li Qingqing, Yu Xianjia, Jorge Pe&#xf1;a Queralta, Tomi Westerlund</h3>
<p>Micro-aerial vehicles (MAVs) are becoming ubiquitous across multiple
industries and application domains. Lightweight MAVs with only an onboard
flight controller and a minimal sensor suite (e.g., IMU, vision, and vertical
ranging sensors) have potential as mobile and easily deployable sensing
platforms. When deployed from a ground robot, a key parameter is a relative
localization between the ground robot and the MAV. This paper proposes a novel
method for tracking MAVs in lidar point clouds. In lidar point clouds, we
consider the speed and distance of the MAV to actively adapt the lidar's frame
integration time and, in essence, the density and size of the point cloud to be
processed. We show that this method enables more persistent and robust tracking
when the speed of the MAV or its distance to the tracking sensor changes. In
addition, we propose a multi-modal tracking method that relies on
high-frequency scans for accurate state estimation, lower-frequency scans for
robust and persistent tracking, and sub-Hz processing for trajectory and object
identification. These three integration and processing modalities allow for an
overall accurate and robust MAV tracking while ensuring the object being
tracked meets shape and size constraints.
</p>
<a href="http://arxiv.org/abs/2103.04069" target="_blank">arXiv:2103.04069</a> [<a href="http://arxiv.org/pdf/2103.04069" target="_blank">pdf</a>]

<h2>Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v1 [cs.CV])</h2>
<h3>Xueying Shi, Yueming Jin, Qi Dou, Jing Qin, Pheng-Ann Heng</h3>
<p>Automated surgical gesture recognition is of great importance in
robot-assisted minimally invasive surgery. However, existing methods assume
that training and testing data are from the same domain, which suffers from
severe performance degradation when a domain gap exists, such as the simulator
and real robot. In this paper, we propose a novel unsupervised domain
adaptation framework which can simultaneously transfer multi-modality
knowledge, i.e., both kinematic and visual data, from simulator to real robot.
It remedies the domain gap with enhanced transferable features by using
temporal cues in videos, and inherent correlations in multi-modal towards
recognizing gesture. Specifically, we first propose an MDO-K to align
kinematics, which exploits temporal continuity to transfer motion directions
with smaller gap rather than position values, relieving the adaptation burden.
Moreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of
kinematics and vision. Such features attended by correlation similarity are
more informative for enhancing domain-invariance of the model. Two feature
alignment strategies benefit the model mutually during the end-to-end learning
process. We extensively evaluate our method for gesture recognition using DESK
dataset with peg transfer procedure. Results show that our approach recovers
the performance with great improvement gains, up to 12.91% in ACC and 20.16% in
F1score without using any annotations in real robot.
</p>
<a href="http://arxiv.org/abs/2103.04075" target="_blank">arXiv:2103.04075</a> [<a href="http://arxiv.org/pdf/2103.04075" target="_blank">pdf</a>]

<h2>Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration. (arXiv:2103.04077v1 [cs.RO])</h2>
<h3>Xiaofeng Gao, Luyao Yuan, Tianmin Shu, Hongjing Lu, Song-Chun Zhu</h3>
<p>Aligning humans' assessment of what a robot can do with its true capability
is crucial for establishing a common ground between human and robot partners
when they collaborate on a joint task. In this work, we propose an approach to
calibrate humans' estimate of a robot's reachable workspace through a small
number of demonstrations before collaboration. We develop a novel motion
planning method, REMP (Reachability-Expressive Motion Planning), which jointly
optimizes the physical cost and the expressiveness of robot motion to reveal
the robot's motion capability to a human observer. Our experiments with human
participants demonstrate that a short calibration using REMP can effectively
bridge the gap between what a non-expert user thinks a robot can reach and the
ground-truth. We show that this calibration procedure not only results in
better user perception, but also promotes more efficient human-robot
collaborations in a subsequent joint task.
</p>
<a href="http://arxiv.org/abs/2103.04077" target="_blank">arXiv:2103.04077</a> [<a href="http://arxiv.org/pdf/2103.04077" target="_blank">pdf</a>]

<h2>WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition. (arXiv:2103.04098v1 [cs.CV])</h2>
<h3>Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, Jie Zhou</h3>
<p>In this paper, we contribute a new million-scale face benchmark containing
noisy 4M identities/260M faces (WebFace260M) and cleaned 2M identities/42M
faces (WebFace42M) training data, as well as an elaborately designed
time-constrained evaluation protocol. Firstly, we collect 4M name list and
download 260M faces from the Internet. Then, a Cleaning Automatically utilizing
Self-Training (CAST) pipeline is devised to purify the tremendous WebFace260M,
which is efficient and scalable. To the best of our knowledge, the cleaned
WebFace42M is the largest public face recognition training set and we expect to
close the data gap between academia and industry. Referring to practical
scenarios, Face Recognition Under Inference Time conStraint (FRUITS) protocol
and a test set are constructed to comprehensively evaluate face matchers.

Equipped with this benchmark, we delve into million-scale face recognition
problems. A distributed framework is developed to train face recognition models
efficiently without tampering with the performance. Empowered by WebFace42M, we
reduce relative 40% failure rate on the challenging IJB-C set, and ranks the
3rd among 430 entries on NIST-FRVT. Even 10% data (WebFace4M) shows superior
performance compared with public training set. Furthermore, comprehensive
baselines are established on our rich-attribute test set under
FRUITS-100ms/500ms/1000ms protocol, including MobileNet, EfficientNet,
AttentionNet, ResNet, SENet, ResNeXt and RegNet families. Benchmark website is
https://www.face-benchmark.org.
</p>
<a href="http://arxiv.org/abs/2103.04098" target="_blank">arXiv:2103.04098</a> [<a href="http://arxiv.org/pdf/2103.04098" target="_blank">pdf</a>]

<h2>Panoptic Lintention Network: Towards Efficient Navigational Perception for the Visually Impaired. (arXiv:2103.04128v1 [cs.CV])</h2>
<h3>Wei Mao, Jiaming Zhang, Kailun Yang, Rainer Stiefelhagen</h3>
<p>Classic computer vision algorithms, instance segmentation, and semantic
segmentation can not provide a holistic understanding of the surroundings for
the visually impaired. In this paper, we utilize panoptic segmentation to
assist the navigation of visually impaired people by offering both things and
stuff awareness in the proximity of the visually impaired efficiently. To this
end, we propose an efficient Attention module -- Lintention which can model
long-range interactions in linear time using linear space. Based on Lintention,
we then devise a novel panoptic segmentation model which we term Panoptic
Lintention Net. Experiments on the COCO dataset indicate that the Panoptic
Lintention Net raises the Panoptic Quality (PQ) from 39.39 to 41.42 with 4.6\%
performance gain while only requiring 10\% fewer GFLOPs and 25\% fewer
parameters in the semantic branch. Furthermore, a real-world test via our
designed compact wearable panoptic segmentation system, indicates that our
system based on the Panoptic Lintention Net accomplishes a relatively stable
and exceptionally remarkable panoptic segmentation in real-world scenes.
</p>
<a href="http://arxiv.org/abs/2103.04128" target="_blank">arXiv:2103.04128</a> [<a href="http://arxiv.org/pdf/2103.04128" target="_blank">pdf</a>]

<h2>Learning to Generate 3D Shapes with Generative Cellular Automata. (arXiv:2103.04130v1 [cs.CV])</h2>
<h3>Dongsu Zhang, Changwoon Choi, Jeonghwan Kim, Young Min Kim</h3>
<p>We present a probabilistic 3D generative model, named Generative Cellular
Automata, which is able to produce diverse and high quality shapes. We
formulate the shape generation process as sampling from the transition kernel
of a Markov chain, where the sampling chain eventually evolves to the full
shape of the learned distribution. The transition kernel employs the local
update rules of cellular automata, effectively reducing the search space in a
high-resolution 3D grid space by exploiting the connectivity and sparsity of 3D
shapes. Our progressive generation only focuses on the sparse set of occupied
voxels and their neighborhood, thus enabling the utilization of an expressive
sparse convolutional network. We propose an effective training scheme to obtain
the local homogeneous rule of generative cellular automata with sequences that
are slightly different from the sampling chain but converge to the full shapes
in the training data. Extensive experiments on probabilistic shape completion
and shape generation demonstrate that our method achieves competitive
performance against recent methods.
</p>
<a href="http://arxiv.org/abs/2103.04130" target="_blank">arXiv:2103.04130</a> [<a href="http://arxiv.org/pdf/2103.04130" target="_blank">pdf</a>]

<h2>Omni-swarm: An Aerial Swarm System with Decentralized Omni-directional Visual-Inertial-UWB State Estimation. (arXiv:2103.04131v1 [cs.RO])</h2>
<h3>Hao Xu, Yichen Zhang, Boyu Zhou, Luqi Wang, Shaojie Shen</h3>
<p>The collaboration of unmanned aerial vehicles (UAVs), also known as aerial
swarm, has become a popular research topic for its practicality and flexibility
in plenty of scenarios. However, one of the most fundamental components for
autonomous aerial swarm systems in GPS-denied areas, the robust decentralized
relative state estimation, remains to be an extremely challenging research
topic. In order to address this research niche, the Omni-swarm, an aerial swarm
system with decentralized Omni-directional visual-inertial-UWB state
estimation, which features robustness, accuracy, and global consistency, is
proposed in this paper. We introduce a map-based localization method using deep
learning tools to perform relative localization and re-localization within the
aerial swarm while achieving the fast initialization and maintaining the global
consistency of state estimation. Furthermore, to overcome the sensors'
visibility issues with the limited field of view (FoV), which severely affect
the performance of the state estimation, Omni-directional sensors, including
fisheye cameras and ultra-wideband (UWB) sensors, are adopted. The state
estimation module, together with the planning and the control modules, is
integrated on the aerial system with Omni-directional sensors to attain the
Omni-swarm, and extensive experiments are performed to verify the validity and
examine the performance of the proposed framework. According to the experiment
result, the proposed framework can achieve centimeter-level relative state
estimation accuracy while ensuring global consistency.
</p>
<a href="http://arxiv.org/abs/2103.04131" target="_blank">arXiv:2103.04131</a> [<a href="http://arxiv.org/pdf/2103.04131" target="_blank">pdf</a>]

<h2>A Real-time Low-cost Artificial Intelligence System for Autonomous Spraying in Palm Plantations. (arXiv:2103.04132v1 [cs.CV])</h2>
<h3>Zhenwang Qin, Wensheng Wang, Karl-Heinz Dammer, Leifeng Guo, Zhen Cao</h3>
<p>In precision crop protection, (target-orientated) object detection in image
processing can help navigate Unmanned Aerial Vehicles (UAV, crop protection
drones) to the right place to apply the pesticide. Unnecessary application of
non-target areas could be avoided. Deep learning algorithms dominantly use in
modern computer vision tasks which require high computing time, memory
footprint, and power consumption. Based on the Edge Artificial Intelligence, we
investigate the main three paths that lead to dealing with this problem,
including hardware accelerators, efficient algorithms, and model compression.
Finally, we integrate them and propose a solution based on a light deep neural
network (DNN), called Ag-YOLO, which can make the crop protection UAV have the
ability to target detection and autonomous operation. This solution is
restricted in size, cost, flexible, fast, and energy-effective. The hardware is
only 18 grams in weight and 1.5 watts in energy consumption, and the developed
DNN model needs only 838 kilobytes of disc space. We tested the developed
hardware and software in comparison to the tiny version of the state-of-art
YOLOv3 framework, known as YOLOv3-Tiny to detect individual palm in a
plantation. An average F1 score of 0.9205 at the speed of 36.5 frames per
second (in comparison to similar accuracy at 18 frames per second and 8.66
megabytes of the YOLOv3-Tiny algorithm) was reached. This developed detection
system is easily plugged into any machines already purchased as long as the
machines have USB ports and run Linux Operating System.
</p>
<a href="http://arxiv.org/abs/2103.04132" target="_blank">arXiv:2103.04132</a> [<a href="http://arxiv.org/pdf/2103.04132" target="_blank">pdf</a>]

<h2>Learning Statistical Texture for Semantic Segmentation. (arXiv:2103.04133v1 [cs.CV])</h2>
<h3>Lanyun Zhu, Deyi Ji, Shiping Zhu, Weihao Gan, Wei Wu, Junjie Yan</h3>
<p>Existing semantic segmentation works mainly focus on learning the contextual
information in high-level semantic features with CNNs. In order to maintain a
precise boundary, low-level texture features are directly skip-connected into
the deeper layers. Nevertheless, texture features are not only about local
structure, but also include global statistical knowledge of the input image. In
this paper, we fully take advantages of the low-level texture features and
propose a novel Statistical Texture Learning Network (STLNet) for semantic
segmentation. For the first time, STLNet analyzes the distribution of low level
information and efficiently utilizes them for the task. Specifically, a novel
Quantization and Counting Operator (QCO) is designed to describe the texture
information in a statistical manner. Based on QCO, two modules are introduced:
(1) Texture Enhance Module (TEM), to capture texture-related information and
enhance the texture details; (2) Pyramid Texture Feature Extraction Module
(PTFEM), to effectively extract the statistical texture features from multiple
scales. Through extensive experiments, we show that the proposed STLNet
achieves state-of-the-art performance on three semantic segmentation
benchmarks: Cityscapes, PASCAL Context and ADE20K.
</p>
<a href="http://arxiv.org/abs/2103.04133" target="_blank">arXiv:2103.04133</a> [<a href="http://arxiv.org/pdf/2103.04133" target="_blank">pdf</a>]

<h2>Perception Framework through Real-Time Semantic Segmentation and Scene Recognition on a Wearable System for the Visually Impaired. (arXiv:2103.04136v1 [cs.CV])</h2>
<h3>Yingzhi Zhang, Haoye Chen, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen</h3>
<p>As the scene information, including objectness and scene type, are important
for people with visual impairment, in this work we present a multi-task
efficient perception system for the scene parsing and recognition tasks.
Building on the compact ResNet backbone, our designed network architecture has
two paths with shared parameters. In the structure, the semantic segmentation
path integrates fast attention, with the aim of harvesting long-range
contextual information in an efficient manner. Simultaneously, the scene
recognition path attains the scene type inference by passing the semantic
features into semantic-driven attention networks and combining the semantic
extracted representations with the RGB extracted representations through a
gated attention module. In the experiments, we have verified the systems'
accuracy and efficiency on both public datasets and real-world scenes. This
system runs on a wearable belt with an Intel RealSense LiDAR camera and an
Nvidia Jetson AGX Xavier processor, which can accompany visually impaired
people and provide assistive scene information in their navigation tasks.
</p>
<a href="http://arxiv.org/abs/2103.04136" target="_blank">arXiv:2103.04136</a> [<a href="http://arxiv.org/pdf/2103.04136" target="_blank">pdf</a>]

<h2>Simple online and real-time tracking with occlusion handling. (arXiv:2103.04147v1 [cs.CV])</h2>
<h3>Mohammad Hossein Nasseri, Hadi Moradi, Reshad Hosseini, Mohammadreza Babaee</h3>
<p>Multiple object tracking is a challenging problem in computer vision due to
difficulty in dealing with motion prediction, occlusion handling, and object
re-identification. Many recent algorithms use motion and appearance cues to
overcome these challenges. But using appearance cues increases the computation
cost notably and therefore the speed of the algorithm decreases significantly
which makes them inappropriate for online applications. In contrast, there are
algorithms that only use motion cues to increase speed, especially for online
applications. But these algorithms cannot handle occlusions and re-identify
lost objects. In this paper, a novel online multiple object tracking algorithm
is presented that only uses geometric cues of objects to tackle the occlusion
and reidentification challenges simultaneously. As a result, it decreases the
identity switch and fragmentation metrics. Experimental results show that the
proposed algorithm could decrease identity switch by 40% and fragmentation by
28% compared to the state of the art online tracking algorithms. The code is
also publicly available.
</p>
<a href="http://arxiv.org/abs/2103.04147" target="_blank">arXiv:2103.04147</a> [<a href="http://arxiv.org/pdf/2103.04147" target="_blank">pdf</a>]

<h2>Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations. (arXiv:2103.04167v1 [cs.CV])</h2>
<h3>Hongwei Li, Fei-Fei Xue, Krishna Chaitanya, Shengda Liu, Ivan Ezhov, Benedikt Wiestler, Jianguo Zhang, Bjoern Menze</h3>
<p>Radiomic representations can quantify properties of regions of interest in
medical image data. Classically, they account for pre-defined statistics of
shape, texture, and other low-level image features. Alternatively, deep
learning-based representations are derived from supervised learning but require
expensive annotations from experts and often suffer from overfitting and data
imbalance issues. In this work, we address the challenge of learning
representations of 3D medical images for an effective quantification under data
imbalance. We propose a \emph{self-supervised} representation learning
framework to learn high-level features of 3D volumes as a complement to
existing radiomics features. Specifically, we demonstrate how to learn image
representations in a self-supervised fashion using a 3D Siamese network. More
importantly, we deal with data imbalance by exploiting two unsupervised
strategies: a) sample re-weighting, and b) balancing the composition of
training batches. When combining our learned self-supervised feature with
traditional radiomics, we show significant improvement in brain tumor
classification and lung cancer staging tasks covering MRI and CT imaging
modalities.
</p>
<a href="http://arxiv.org/abs/2103.04167" target="_blank">arXiv:2103.04167</a> [<a href="http://arxiv.org/pdf/2103.04167" target="_blank">pdf</a>]

<h2>LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment. (arXiv:2103.04173v1 [cs.CV])</h2>
<h3>Filipe R. Cordeiro, Ragav Sachdeva, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro</h3>
<p>Deep neural network models are robust to a limited amount of label noise, but
their ability to memorise noisy labels in high noise rate problems is still an
open issue. The most competitive noisy-label learning algorithms rely on a
2-stage process comprising an unsupervised learning to classify training
samples as clean or noisy, followed by a semi-supervised learning that
minimises the empirical vicinal risk (EVR) using a labelled set formed by
samples classified as clean, and an unlabelled set with samples classified as
noisy. In this paper, we hypothesise that the generalisation of such 2-stage
noisy-label learning methods depends on the precision of the unsupervised
classifier and the size of the training set to minimise the EVR. We empirically
validate these two hypotheses and propose the new 2-stage noisy-label training
algorithm LongReMix. We test LongReMix on the noisy-label benchmarks CIFAR-10,
CIFAR-100, WebVision, Clothing1M, and Food101-N. The results show that our
LongReMix generalises better than competing approaches, particularly in high
label noise problems. Furthermore, our approach achieves state-of-the-art
performance in most datasets. The code will be available upon paper acceptance.
</p>
<a href="http://arxiv.org/abs/2103.04173" target="_blank">arXiv:2103.04173</a> [<a href="http://arxiv.org/pdf/2103.04173" target="_blank">pdf</a>]

<h2>Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction. (arXiv:2103.04174v1 [cs.CV])</h2>
<h3>Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, Chelsea Finn</h3>
<p>A video prediction model that generalizes to diverse scenes would enable
intelligent agents such as robots to perform a variety of tasks via planning
with the model. However, while existing video prediction models have produced
promising results on small datasets, they suffer from severe underfitting when
trained on large and diverse datasets. To address this underfitting challenge,
we first observe that the ability to train larger video prediction models is
often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep
hierarchical latent variable models can produce higher quality predictions by
capturing the multi-level stochasticity of future observations, but end-to-end
optimization of such models is notably difficult. Our key insight is that
greedy and modular optimization of hierarchical autoencoders can simultaneously
address both the memory constraints and the optimization challenges of
large-scale video prediction. We introduce Greedy Hierarchical Variational
Autoencoders (GHVAEs), a method that learns high-fidelity video predictions by
greedily training each level of a hierarchical autoencoder. In comparison to
state-of-the-art models, GHVAEs provide 17-55% gains in prediction performance
on four video datasets, a 35-40% higher success rate on real robot tasks, and
can improve performance monotonically by simply adding more modules.
</p>
<a href="http://arxiv.org/abs/2103.04174" target="_blank">arXiv:2103.04174</a> [<a href="http://arxiv.org/pdf/2103.04174" target="_blank">pdf</a>]

<h2>End-to-end optimized image compression for multiple machine tasks. (arXiv:2103.04178v1 [cs.CV])</h2>
<h3>Lahiru D. Chamain, Fabien Racap&#xe9;, Jean B&#xe9;gaint, Akshay Pushparaja, Simon Feltman</h3>
<p>An increasing share of captured images and videos are transmitted for storage
and remote analysis by computer vision algorithms, rather than to be viewed by
humans. Contrary to traditional standard codecs with engineered tools, neural
network based codecs can be trained end-to-end to optimally compress images
with respect to a target rate and any given differentiable performance metric.
Although it is possible to train such compression tools to achieve better
rate-accuracy performance for a particular computer vision task, it could be
practical and relevant to re-use the compressed bit-stream for multiple machine
tasks. For this purpose, we introduce 'Connectors' that are inserted between
the decoder and the task algorithms to enable a direct transformation of the
compressed content, which was previously optimized for a specific task, to
multiple other machine tasks. We demonstrate the effectiveness of the proposed
method by achieving significant rate-accuracy performance improvement for both
image classification and object segmentation, using the same bit-stream,
originally optimized for object detection.
</p>
<a href="http://arxiv.org/abs/2103.04178" target="_blank">arXiv:2103.04178</a> [<a href="http://arxiv.org/pdf/2103.04178" target="_blank">pdf</a>]

<h2>High Perceptual Quality Image Denoising with a Posterior Sampling CGAN. (arXiv:2103.04192v1 [cs.CV])</h2>
<h3>Guy Ohayon, Theo Adrai, Gregory Vaksman, Michael Elad, Peyman Milanfar</h3>
<p>The vast work in Deep Learning (DL) has led to a leap in image denoising
research. Most DL solutions for this task have chosen to put their efforts on
the denoiser's architecture while maximizing distortion performance. However,
distortion driven solutions lead to blurry results with sub-optimal perceptual
quality, especially in immoderate noise levels. In this paper we propose a
different perspective, aiming to produce sharp and visually pleasing denoised
images that are still faithful to their clean sources. Formally, our goal is to
achieve high perceptual quality with acceptable distortion. This is attained by
a stochastic denoiser that samples from the posterior distribution, trained as
a generator in the framework of conditional generative adversarial networks
(CGANs). Contrary to distortion-based regularization terms that conflict with
perceptual quality, we introduce to the CGANs objective a theoretically founded
penalty term that does not force a distortion requirement on individual
samples, but rather on their mean. We showcase our proposed method with a novel
denoiser architecture that achieves the reformed denoising goal and produces
vivid and diverse outcomes in immoderate noise levels.
</p>
<a href="http://arxiv.org/abs/2103.04192" target="_blank">arXiv:2103.04192</a> [<a href="http://arxiv.org/pdf/2103.04192" target="_blank">pdf</a>]

<h2>Consensus Maximisation Using Influences of Monotone Boolean Functions. (arXiv:2103.04200v1 [cs.CV])</h2>
<h3>Ruwan Tennakoon, David Suter, Erchuan Zhang, Tat-Jun Chin, Alireza Bab-Hadiashar</h3>
<p>Consensus maximisation (MaxCon), which is widely used for robust fitting in
computer vision, aims to find the largest subset of data that fits the model
within some tolerance level. In this paper, we outline the connection between
MaxCon problem and the abstract problem of finding the maximum upper zero of a
Monotone Boolean Function (MBF) defined over the Boolean Cube. Then, we link
the concept of influences (in a MBF) to the concept of outlier (in MaxCon) and
show that influences of points belonging to the largest structure in data would
generally be smaller under certain conditions. Based on this observation, we
present an iterative algorithm to perform consensus maximisation. Results for
both synthetic and real visual data experiments show that the MBF based
algorithm is capable of generating a near optimal solution relatively quickly.
This is particularly important where there are large number of outliers (gross
or pseudo) in the observed data.
</p>
<a href="http://arxiv.org/abs/2103.04200" target="_blank">arXiv:2103.04200</a> [<a href="http://arxiv.org/pdf/2103.04200" target="_blank">pdf</a>]

<h2>Virtual Normal: Enforcing Geometric Constraintsfor Accurate and Robust Depth Prediction. (arXiv:2103.04216v1 [cs.CV])</h2>
<h3>Wei Yin, Yifan Liu, Chunhua Shen</h3>
<p>Monocular depth prediction plays a crucial role in understanding 3D scene
geometry. Although recent methods have achieved impressive progress in terms of
evaluation metrics such as the pixel-wise relative error, most methods neglect
the geometric constraints in the 3D space. In this work, we show the importance
of the high-order 3D geometric constraints for depth prediction. By designing a
loss term that enforces a simple geometric constraint, namely, virtual normal
directions determined by randomly sampled three points in the reconstructed 3D
space, we significantly improve the accuracy and robustness of monocular depth
estimation. Significantly, the virtual normal loss can not only improve the
performance of learning metric depth, but also disentangle the scale
information and enrich the model with better shape information. Therefore, when
not having access to absolute metric depth training data, we can use virtual
normal to learn a robust affine-invariant depth generated on diverse scenes. In
experiments, We show state-of-the-art results of learning metric depth on NYU
Depth-V2 and KITTI. From the high-quality predicted depth, we are now able to
recover good 3D structures of the scene such as the point cloud and surface
normal directly, eliminating the necessity of relying on additional models as
was previously done. To demonstrate the excellent generalizability of learning
affine-invariant depth on diverse data with the virtual normal loss, we
construct a large-scale and diverse dataset for training affine-invariant
depth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five
datasets with the zero-shot test setting.
</p>
<a href="http://arxiv.org/abs/2103.04216" target="_blank">arXiv:2103.04216</a> [<a href="http://arxiv.org/pdf/2103.04216" target="_blank">pdf</a>]

<h2>MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection. (arXiv:2103.04224v1 [cs.CV])</h2>
<h3>Vibashan VS, Poojan Oza, Vishwanath A. Sindagi, Vikram Gupta, Vishal M. Patel</h3>
<p>Existing approaches for unsupervised domain adaptive object detection perform
feature alignment via adversarial training. While these methods achieve
reasonable improvements in performance, they typically perform
category-agnostic domain alignment, thereby resulting in negative transfer of
features. To overcome this issue, in this work, we attempt to incorporate
category information into the domain adaptation process by proposing Memory
Guided Attention for Category-Aware Domain Adaptation (MeGA-CDA). The proposed
method consists of employing category-wise discriminators to ensure
category-aware feature alignment for learning domain-invariant discriminative
features. However, since the category information is not available for the
target samples, we propose to generate memory-guided category-specific
attention maps which are then used to route the features appropriately to the
corresponding category discriminator. The proposed method is evaluated on
several benchmark datasets and is shown to outperform existing approaches.
</p>
<a href="http://arxiv.org/abs/2103.04224" target="_blank">arXiv:2103.04224</a> [<a href="http://arxiv.org/pdf/2103.04224" target="_blank">pdf</a>]

<h2>GANav: Group-wise Attention Network for Classifying Navigable Regions in Unstructured Outdoor Environments. (arXiv:2103.04233v1 [cs.RO])</h2>
<h3>Tianrui Guan, Divya Kothandaraman, Rohan Chandra, Dinesh Manocha</h3>
<p>We present a new learning-based method for identifying safe and navigable
regions in off-road terrains and unstructured environments from RGB images. Our
approach consists of classifying groups of terrain classes based on their
navigability levels using coarse-grained semantic segmentation. We propose a
bottleneck transformer-based deep neural network architecture that uses a novel
group-wise attention mechanism to distinguish between navigability levels of
different terrains.Our group-wise attention heads enable the network to
explicitly focus on the different groups and improve the accuracy. In addition,
we propose a dynamic weighted cross entropy loss function to handle the
long-tailed nature of the dataset. We show through extensive evaluations on the
RUGD and RELLIS-3D datasets that our learning algorithm improves the accuracy
of visual perception in off-road terrains for navigation. We compare our
approach with prior work on these datasets and achieve an improvement over the
state-of-the-art mIoU by 6.74-39.1% on RUGD and 3.82-10.64% on RELLIS-3D.
</p>
<a href="http://arxiv.org/abs/2103.04233" target="_blank">arXiv:2103.04233</a> [<a href="http://arxiv.org/pdf/2103.04233" target="_blank">pdf</a>]

<h2>MetaView: Few-shot Active Object Recognition. (arXiv:2103.04242v1 [cs.RO])</h2>
<h3>Wei Wei, Haonan Yu, Haichao Zhang, Wei Xu, Ying Wu</h3>
<p>In robot sensing scenarios, instead of passively utilizing human captured
views, an agent should be able to actively choose informative viewpoints of a
3D object as discriminative evidence to boost the recognition accuracy. This
task is referred to as active object recognition. Recent works on this task
rely on a massive amount of training examples to train an optimal view
selection policy. But in realistic robot sensing scenarios, the large-scale
training data may not exist and whether the intelligent view selection policy
can be still learned from few object samples remains unclear. In this paper, we
study this new problem which is extremely challenging but very meaningful in
robot sensing -- Few-shot Active Object Recognition, i.e., to learn view
selection policies from few object samples, which has not been considered and
addressed before. We solve the proposed problem by adopting the framework of
meta learning and name our method "MetaView". Extensive experiments on both
category-level and instance-level classification tasks demonstrate that the
proposed method can efficiently resolve issues that are hard for
state-of-the-art active object recognition methods to handle, and outperform
several baselines by large margins.
</p>
<a href="http://arxiv.org/abs/2103.04242" target="_blank">arXiv:2103.04242</a> [<a href="http://arxiv.org/pdf/2103.04242" target="_blank">pdf</a>]

<h2>Estimating and Improving Fairness with Adversarial Learning. (arXiv:2103.04243v1 [cs.CV])</h2>
<h3>Xiaoxiao Li, Ziteng Cui, Yifan Wu, Li Gu, Tatsuya Harada</h3>
<p>Fairness and accountability are two essential pillars for trustworthy
Artificial Intelligence (AI) in healthcare. However, the existing AI model may
be biased in its decision marking. To tackle this issue, we propose an
adversarial multi-task training strategy to simultaneously mitigate and detect
bias in the deep learning-based medical image analysis system. Specifically, we
propose to add a discrimination module against bias and a critical module that
predicts unfairness within the base classification model. We further impose an
orthogonality regularization to force the two modules to be independent during
training. Hence, we can keep these deep learning tasks distinct from one
another, and avoid collapsing them into a singular point on the manifold.
Through this adversarial training method, the data from the underprivileged
group, which is vulnerable to bias because of attributes such as sex and skin
tone, are transferred into a domain that is neutral relative to these
attributes. Furthermore, the critical module can predict fairness scores for
the data with unknown sensitive attributes. We evaluate our framework on a
large-scale public-available skin lesion dataset under various fairness
evaluation metrics. The experiments demonstrate the effectiveness of our
proposed method for estimating and improving fairness in the deep
learning-based medical image analysis system.
</p>
<a href="http://arxiv.org/abs/2103.04243" target="_blank">arXiv:2103.04243</a> [<a href="http://arxiv.org/pdf/2103.04243" target="_blank">pdf</a>]

<h2>Cascaded Filtering Using the Sigma Point Transformation (Extended Version). (arXiv:2103.04249v1 [cs.RO])</h2>
<h3>Mohammed Shalaby, Charles Champagne Cossette, Jerome Le Ny, James Richard Forbes</h3>
<p>It is often convenient to separate a state estimation task into smaller
"local" tasks, where each local estimator estimates a subset of the overall
system state. However, neglecting cross-covariance terms between state
estimates can result in overconfident estimates, which can ultimately degrade
the accuracy of the estimator. Common cascaded filtering techniques focus on
the problem of modelling cross-covariances when the local estimators share a
common state vector. This letter introduces a novel cascaded and decentralized
filtering approach that approximates the cross-covariances when the local
estimators consider distinct state vectors. The proposed estimator is validated
in simulations and in experiments on a three-dimensional attitude and position
estimation problem. The proposed approach is compared to a naive cascaded
filtering approach that neglects cross-covariance terms, a sigma point-based
Covariance Intersection filter, and a full-state filter. In both simulations
and experiments, the proposed filter outperforms the naive and the Covariance
Intersection filters, while performing comparatively to the full-state filter.
</p>
<a href="http://arxiv.org/abs/2103.04249" target="_blank">arXiv:2103.04249</a> [<a href="http://arxiv.org/pdf/2103.04249" target="_blank">pdf</a>]

<h2>Robust Point Cloud Registration Framework Based on Deep Graph Matching. (arXiv:2103.04256v1 [cs.CV])</h2>
<h3>Kexue Fu, Shaolei Liu, Xiaoyuan Luo, Manning Wang</h3>
<p>3D point cloud registration is a fundamental problem in computer vision and
robotics. There has been extensive research in this area, but existing methods
meet great challenges in situations with a large proportion of outliers and
time constraints, but without good transformation initialization. Recently, a
series of learning-based algorithms have been introduced and show advantages in
speed. Many of them are based on correspondences between the two point clouds,
so they do not rely on transformation initialization. However, these
learning-based methods are sensitive to outliers, which lead to more incorrect
correspondences. In this paper, we propose a novel deep graph matchingbased
framework for point cloud registration. Specifically, we first transform point
clouds into graphs and extract deep features for each point. Then, we develop a
module based on deep graph matching to calculate a soft correspondence matrix.
By using graph matching, not only the local geometry of each point but also its
structure and topology in a larger range are considered in establishing
correspondences, so that more correct correspondences are found. We train the
network with a loss directly defined on the correspondences, and in the test
stage the soft correspondences are transformed into hard one-to-one
correspondences so that registration can be performed by singular value
decomposition. Furthermore, we introduce a transformer-based method to generate
edges for graph construction, which further improves the quality of the
correspondences. Extensive experiments on registering clean, noisy,
partial-to-partial and unseen category point clouds show that the proposed
method achieves state-of-the-art performance. The code will be made publicly
available at https://github.com/fukexue/RGM.
</p>
<a href="http://arxiv.org/abs/2103.04256" target="_blank">arXiv:2103.04256</a> [<a href="http://arxiv.org/pdf/2103.04256" target="_blank">pdf</a>]

<h2>Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection. (arXiv:2103.04257v1 [cs.CV])</h2>
<h3>Guodong Wang, Shumin Han, Errui Ding, Di Huang</h3>
<p>Anomaly detection is a challenging task and usually formulated as an
unsupervised learning problem for the unexpectedness of anomalies. This paper
proposes a simple yet powerful approach to this issue, which is implemented in
the student-teacher framework for its advantages but substantially extends it
in terms of both accuracy and efficiency. Given a strong model pre-trained on
image classification as the teacher, we distill the knowledge into a single
student network with the identical architecture to learn the distribution of
anomaly-free images and this one-step transfer preserves the crucial clues as
much as possible. Moreover, we integrate the multi-scale feature matching
strategy into the framework, and this hierarchical feature alignment enables
the student network to receive a mixture of multi-level knowledge from the
feature pyramid under better supervision, thus allowing to detect anomalies of
various sizes. The difference between feature pyramids generated by the two
networks serves as a scoring function indicating the probability of anomaly
occurring. Due to such operations, our approach achieves accurate and fast
pixel-level anomaly detection. Very competitive results are delivered on three
major benchmarks, significantly superior to the state of the art ones. In
addition, it makes inferences at a very high speed (with 100 FPS for images of
the size at 256x256), at least dozens of times faster than the latest
counterparts.
</p>
<a href="http://arxiv.org/abs/2103.04257" target="_blank">arXiv:2103.04257</a> [<a href="http://arxiv.org/pdf/2103.04257" target="_blank">pdf</a>]

<h2>High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection. (arXiv:2103.04258v1 [cs.CV])</h2>
<h3>Yunxiang Li, Yifan Zhang, Yaqi Wang, Shuai Wang, Ruizi Peng, Kai Tang, Qianni Zhang, Jun Wang, Qun Jin, Lingling Sun</h3>
<p>As the most economical and routine auxiliary examination in the diagnosis of
root canal treatment, oral X-ray has been widely used by stomatologists. It is
still challenging to segment the tooth root with a blurry boundary for the
traditional image segmentation method. To this end, we propose a model for
high-resolution segmentation based on polynomial curve fitting with landmark
detection (HS-PCL). It is based on detecting multiple landmarks evenly
distributed on the edge of the tooth root to fit a smooth polynomial curve as
the segmentation of the tooth root, thereby solving the problem of fuzzy edge.
In our model, a maximum number of the shortest distances algorithm (MNSDA) is
proposed to automatically reduce the negative influence of the wrong landmarks
which are detected incorrectly and deviate from the tooth root on the fitting
result. Our numerical experiments demonstrate that the proposed approach not
only reduces Hausdorff95 (HD95) by 33.9% and Average Surface Distance (ASD) by
42.1% compared with the state-of-the-art method, but it also achieves excellent
results on the minute quantity of datasets, which greatly improves the
feasibility of automatic root canal therapy evaluation by medical image
computing.
</p>
<a href="http://arxiv.org/abs/2103.04258" target="_blank">arXiv:2103.04258</a> [<a href="http://arxiv.org/pdf/2103.04258" target="_blank">pdf</a>]

<h2>ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring. (arXiv:2103.04260v1 [cs.CV])</h2>
<h3>Dongxu Li, Chenchen Xu, Kaihao Zhang, Xin Yu, Yiran Zhong, Wenqi Ren, Hanna Suominen, Hongdong Li</h3>
<p>Video deblurring models exploit consecutive frames to remove blurs from
camera shakes and object motions. In order to utilize neighboring sharp
patches, typical methods rely mainly on homography or optical flows to
spatially align neighboring blurry frames. However, such explicit approaches
are less effective in the presence of fast motions with large pixel
displacements. In this work, we propose a novel implicit method to learn
spatial correspondence among blurry frames in the feature space. To construct
distant pixel correspondences, our model builds a correlation volume pyramid
among all the pixel-pairs between neighboring frames. To enhance the features
of the reference frame, we design a correlative aggregation module that
maximizes the pixel-pair correlations with its neighbors based on the volume
pyramid. Finally, we feed the aggregated features into a reconstruction module
to obtain the restored frame. We design a generative adversarial paradigm to
optimize the model progressively. Our proposed method is evaluated on the
widely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000
fps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative
experiments show that our model performs favorably on both datasets against
previous state-of-the-art methods, confirming the benefit of modeling all-range
spatial correspondence for video deblurring.
</p>
<a href="http://arxiv.org/abs/2103.04260" target="_blank">arXiv:2103.04260</a> [<a href="http://arxiv.org/pdf/2103.04260" target="_blank">pdf</a>]

<h2>Developing a Data-Driven Categorical Taxonomy of Emotional Expressions in Real World Human Robot Interactions. (arXiv:2103.04262v1 [cs.RO])</h2>
<h3>Ghazal Saheb Jam, Jimin Rhim, Angelica Lim</h3>
<p>Emotions are reactions that can be expressed through a variety of social
signals. For example, anger can be expressed through a scowl, narrowed eyes, a
long stare, or many other expressions. This complexity is problematic when
attempting to recognize a human's expression in a human-robot interaction:
categorical emotion models used in HRI typically use only a few prototypical
classes, and do not cover the wide array of expressions in the wild. We propose
a data-driven method towards increasing the number of known emotion classes
present in human-robot interactions, to 28 classes or more. The method includes
the use of automatic segmentation of video streams into short (&lt;10s) videos,
and annotation using the large set of widely-understood emojis as categories.
In this work, we showcase our initial results using a large in-the-wild HRI
dataset (UE-HRI), with 61 clips randomly sampled from the dataset, labeled with
28 different emojis. In particular, our results showed that the "skeptical"
emoji was a common expression in our dataset, which is not often considered in
typical emotion taxonomies. This is the first step in developing a rich
taxonomy of emotional expressions that can be used in the future as labels for
training machine learning models, towards more accurate perception of humans by
robots.
</p>
<a href="http://arxiv.org/abs/2103.04262" target="_blank">arXiv:2103.04262</a> [<a href="http://arxiv.org/pdf/2103.04262" target="_blank">pdf</a>]

<h2>Tendon-Driven Soft Robotic Gripper for Berry Harvesting. (arXiv:2103.04270v1 [cs.RO])</h2>
<h3>Anthony L. Gunderman, Jeremy Collins, Andrea Myer, Renee Threlfall, Yue Chen</h3>
<p>Global berry production and consumption have significantly increased in
recent years, coinciding with increased consumer awareness of the
health-promoting benefits of berries. Among them, fresh market blackberries and
raspberries are primarily harvested by hand to maintain post-harvest quality.
However, fresh market berry harvesting is an arduous, costly endeavor that
accounts for up to 50% of the worker hours. Additionally, the inconsistent
forces applied during hand-harvesting can result in an 85% loss of marketable
berries due to red drupelet reversion (RDR). Herein, we present a novel,
tendon- driven soft robotic gripper with active contact force feedback control,
which leverages the passive compliance of the gripper for the gentle harvesting
of blackberries. The versatile gripper was able to apply a desired force as low
as 0.5 N with a mean error of 0.046 N, while also holding payloads that produce
forces as high as 18 N. Field test results indicate that the gripper is capable
of harvesting berries with minimal berry damage, while maintaining a harvesting
reliability of 95% and a harvesting rate of approximately 4.8 seconds per
berry.
</p>
<a href="http://arxiv.org/abs/2103.04270" target="_blank">arXiv:2103.04270</a> [<a href="http://arxiv.org/pdf/2103.04270" target="_blank">pdf</a>]

<h2>Robust Reflection Removal with Reflection-free Flash-only Cues. (arXiv:2103.04273v1 [cs.CV])</h2>
<h3>Chenyang Lei, Qifeng Chen</h3>
<p>We propose a simple yet effective reflection-free cue for robust reflection
removal from a pair of flash and ambient (no-flash) images. The reflection-free
cue exploits a flash-only image obtained by subtracting the ambient image from
the corresponding flash image in raw data space. The flash-only image is
equivalent to an image taken in a dark environment with only a flash on. We
observe that this flash-only image is visually reflection-free, and thus it can
provide robust cues to infer the reflection in the ambient image. Since the
flash-only image usually has artifacts, we further propose a dedicated model
that not only utilizes the reflection-free cue but also avoids introducing
artifacts, which helps accurately estimate reflection and transmission. Our
experiments on real-world images with various types of reflection demonstrate
the effectiveness of our model with reflection-free flash-only cues: our model
outperforms state-of-the-art reflection removal approaches by more than 5.23dB
in PSNR, 0.04 in SSIM, and 0.068 in LPIPS. Our source code and dataset will be
publicly available at
\href{https://github.com/ChenyangLEI/flash-reflection-removal}{this website}.
</p>
<a href="http://arxiv.org/abs/2103.04273" target="_blank">arXiv:2103.04273</a> [<a href="http://arxiv.org/pdf/2103.04273" target="_blank">pdf</a>]

<h2>Hierarchical Self Attention Based Autoencoder for Open-Set Human Activity Recognition. (arXiv:2103.04279v1 [cs.CV])</h2>
<h3>M Tanjid Hasan Tonmoy, Saif Mahmud, A K M Mahbubur Rahman, M Ashraful Amin, Amin Ahsan Ali</h3>
<p>Wearable sensor based human activity recognition is a challenging problem due
to difficulty in modeling spatial and temporal dependencies of sensor signals.
Recognition models in closed-set assumption are forced to yield members of
known activity classes as prediction. However, activity recognition models can
encounter an unseen activity due to body-worn sensor malfunction or disability
of the subject performing the activities. This problem can be addressed through
modeling solution according to the assumption of open-set recognition. Hence,
the proposed self attention based approach combines data hierarchically from
different sensor placements across time to classify closed-set activities and
it obtains notable performance improvement over state-of-the-art models on five
publicly available datasets. The decoder in this autoencoder architecture
incorporates self-attention based feature representations from encoder to
detect unseen activity classes in open-set recognition setting. Furthermore,
attention maps generated by the hierarchical model demonstrate explainable
selection of features in activity recognition. We conduct extensive leave one
subject out validation experiments that indicate significantly improved
robustness to noise and subject specific variability in body-worn sensor
signals. The source code is available at:
github.com/saif-mahmud/hierarchical-attention-HAR
</p>
<a href="http://arxiv.org/abs/2103.04279" target="_blank">arXiv:2103.04279</a> [<a href="http://arxiv.org/pdf/2103.04279" target="_blank">pdf</a>]

<h2>Learning Cycle-Consistent Cooperative Networks via Alternating MCMC Teaching for Unsupervised Cross-Domain Translation. (arXiv:2103.04285v1 [cs.CV])</h2>
<h3>Jianwen Xie, Zilong Zheng, Xiaolin Fang, Song-Chun Zhu, Ying Nian Wu</h3>
<p>This paper studies the unsupervised cross-domain translation problem by
proposing a generative framework, in which the probability distribution of each
domain is represented by a generative cooperative network that consists of an
energy-based model and a latent variable model. The use of generative
cooperative network enables maximum likelihood learning of the domain model by
MCMC teaching, where the energy-based model seeks to fit the data distribution
of domain and distills its knowledge to the latent variable model via MCMC.
Specifically, in the MCMC teaching process, the latent variable model
parameterized by an encoder-decoder maps examples from the source domain to the
target domain, while the energy-based model further refines the mapped results
by Langevin revision such that the revised results match to the examples in the
target domain in terms of the statistical properties, which are defined by the
learned energy function. For the purpose of building up a correspondence
between two unpaired domains, the proposed framework simultaneously learns a
pair of cooperative networks with cycle consistency, accounting for a two-way
translation between two domains, by alternating MCMC teaching. Experiments show
that the proposed framework is useful for unsupervised image-to-image
translation and unpaired image sequence translation.
</p>
<a href="http://arxiv.org/abs/2103.04285" target="_blank">arXiv:2103.04285</a> [<a href="http://arxiv.org/pdf/2103.04285" target="_blank">pdf</a>]

<h2>RFN-Nest: An end-to-end residual fusion network for infrared and visible images. (arXiv:2103.04286v1 [cs.CV])</h2>
<h3>Hui Li, Xiao-Jun Wu, Josef Kittler</h3>
<p>In the image fusion field, the design of deep learning-based fusion methods
is far from routine. It is invariably fusion-task specific and requires a
careful consideration. The most difficult part of the design is to choose an
appropriate strategy to generate the fused image for a specific task in hand.
Thus, devising learnable fusion strategy is a very challenging problem in the
community of image fusion. To address this problem, a novel end-to-end fusion
network architecture (RFN-Nest) is developed for infrared and visible image
fusion. We propose a residual fusion network (RFN) which is based on a residual
architecture to replace the traditional fusion approach. A novel
detail-preserving loss function, and a feature enhancing loss function are
proposed to train RFN. The fusion model learning is accomplished by a novel
two-stage training strategy. In the first stage, we train an auto-encoder based
on an innovative nest connection (Nest) concept. Next, the RFN is trained using
the proposed loss functions. The experimental results on public domain data
sets show that, compared with the existing methods, our end-to-end fusion
network delivers a better performance than the state-of-the-art methods in both
subjective and objective evaluation. The code of our fusion method is available
at https://github.com/hli1221/imagefusion-rfn-nest
</p>
<a href="http://arxiv.org/abs/2103.04286" target="_blank">arXiv:2103.04286</a> [<a href="http://arxiv.org/pdf/2103.04286" target="_blank">pdf</a>]

<h2>Learn to Differ: Sim2Real Small Defection Segmentation Network. (arXiv:2103.04297v1 [cs.CV])</h2>
<h3>Zexi Chen, Zheyuan Huang, Yunkai Wang, Xuecheng Xu, Yue Wang, Rong Xiong</h3>
<p>Recent studies on deep-learning-based small defection segmentation approaches
are trained in specific settings and tend to be limited by fixed context.
Throughout the training, the network inevitably learns the representation of
the background of the training data before figuring out the defection. They
underperform in the inference stage once the context changed and can only be
solved by training in every new setting. This eventually leads to the
limitation in practical robotic applications where contexts keep varying. To
cope with this, instead of training a network context by context and hoping it
to generalize, why not stop misleading it with any limited context and start
training it with pure simulation? In this paper, we propose the network SSDS
that learns a way of distinguishing small defections between two images
regardless of the context, so that the network can be trained once for all. A
small defection detection layer utilizing the pose sensitivity of phase
correlation between images is introduced and is followed by an outlier masking
layer. The network is trained on randomly generated simulated data with simple
shapes and is generalized across the real world. Finally, SSDS is validated on
real-world collected data and demonstrates the ability that even when trained
in cheap simulation, SSDS can still find small defections in the real world
showing the effectiveness and its potential for practical applications.
</p>
<a href="http://arxiv.org/abs/2103.04297" target="_blank">arXiv:2103.04297</a> [<a href="http://arxiv.org/pdf/2103.04297" target="_blank">pdf</a>]

<h2>Robotic Visuomotor Control with Unsupervised Forward Model Learned from Videos. (arXiv:2103.04301v1 [cs.RO])</h2>
<h3>Haoqi Yuan, Ruihai Wu, Andrew Zhao, Haipeng Zhang, Zihan Ding, Hao Dong</h3>
<p>Learning an accurate model of the environment is essential for model-based
control tasks. Existing methods in robotic visuomotor control usually learn
from data with heavily labelled actions, object entities or locations, which
can be demanding in many cases. To cope with this limitation, we propose a
method that trains a forward model from video data only, via disentangling the
motion of controllable agent to model the transition dynamics. An object
extractor and an interaction learner are trained in an end-to-end manner
without supervision. The agent's motions are explicitly represented using
spatial transformation matrices containing physical meanings. In the
experiments, our method achieves superior performance on learning an accurate
forward model in a Grid World environment, as well as a more realistic robot
control environment in simulation. With the accurate learned forward models, we
further demonstrate their usage in model predictive control as an effective
approach for robotic manipulations.
</p>
<a href="http://arxiv.org/abs/2103.04301" target="_blank">arXiv:2103.04301</a> [<a href="http://arxiv.org/pdf/2103.04301" target="_blank">pdf</a>]

<h2>ERASOR: Egocentric Ratio of Pseudo Occupancy-based Dynamic Object Removal for Static 3D Point Cloud Map Building. (arXiv:2103.04316v1 [cs.CV])</h2>
<h3>Hyungtae Lim, Sungwon Hwang, Hyun Myung</h3>
<p>Scan data of urban environments often include representations of dynamic
objects, such as vehicles, pedestrians, and so forth. However, when it comes to
constructing a 3D point cloud map with sequential accumulations of the scan
data, the dynamic objects often leave unwanted traces in the map. These traces
of dynamic objects act as obstacles and thus impede mobile vehicles from
achieving good localization and navigation performances. To tackle the problem,
this paper presents a novel static map building method called ERASOR,
Egocentric RAtio of pSeudo Occupancy-based dynamic object Removal, which is
fast and robust to motion ambiguity. Our approach directs its attention to the
nature of most dynamic objects in urban environments being inevitably in
contact with the ground. Accordingly, we propose the novel concept called
pseudo occupancy to express the occupancy of unit space and then discriminate
spaces of varying occupancy. Finally, Region-wise Ground Plane Fitting (R-GPF)
is adopted to distinguish static points from dynamic points within the
candidate bins that potentially contain dynamic points. As experimentally
verified on SemanticKITTI, our proposed method yields promising performance
against state-of-the-art methods overcoming the limitations of existing ray
tracing-based and visibility-based methods.
</p>
<a href="http://arxiv.org/abs/2103.04316" target="_blank">arXiv:2103.04316</a> [<a href="http://arxiv.org/pdf/2103.04316" target="_blank">pdf</a>]

<h2>Pose Discrepancy Spatial Transformer Based Feature Disentangling for Partial Aspect Angles SAR Target Recognition. (arXiv:2103.04329v1 [cs.CV])</h2>
<h3>Zaidao Wen, Jiaxiang Liu, Zhunga Liu, Quan Pan</h3>
<p>This letter presents a novel framework termed DistSTN for the task of
synthetic aperture radar (SAR) automatic target recognition (ATR). In contrast
to the conventional SAR ATR algorithms, DistSTN considers a more challenging
practical scenario for non-cooperative targets whose aspect angles for training
are incomplete and limited in a partial range while those of testing samples
are unlimited. To address this issue, instead of learning the pose invariant
features, DistSTN newly involves an elaborated feature disentangling model to
separate the learned pose factors of a SAR target from the identity ones so
that they can independently control the representation process of the target
image. To disentangle the explainable pose factors, we develop a pose
discrepancy spatial transformer module in DistSTN to characterize the intrinsic
transformation between the factors of two different targets with an explicit
geometric model. Furthermore, DistSTN develops an amortized inference scheme
that enables efficient feature extraction and recognition using an
encoder-decoder mechanism. Experimental results with the moving and stationary
target acquisition and recognition (MSTAR) benchmark demonstrate the
effectiveness of our proposed approach. Compared with the other ATR algorithms,
DistSTN can achieve higher recognition accuracy.
</p>
<a href="http://arxiv.org/abs/2103.04329" target="_blank">arXiv:2103.04329</a> [<a href="http://arxiv.org/pdf/2103.04329" target="_blank">pdf</a>]

<h2>Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification. (arXiv:2103.04337v1 [cs.CV])</h2>
<h3>Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu, Xiaoyun Yang</h3>
<p>Video-based person re-identification (Re-ID) aims to automatically retrieve
video sequences of the same person under non-overlapping cameras. To achieve
this goal, it is the key to fully utilize abundant spatial and temporal cues in
videos. Existing methods usually focus on the most conspicuous image regions,
thus they may easily miss out fine-grained clues due to the person varieties in
image sequences. To address above issues, in this paper, we propose a novel
Global-guided Reciprocal Learning (GRL) framework for video-based person Re-ID.
Specifically, we first propose a Global-guided Correlation Estimation (GCE) to
generate feature correlation maps of local features and global features, which
help to localize the high-and low-correlation regions for identifying the same
person. After that, the discriminative features are disentangled into
high-correlation features and low-correlation features under the guidance of
the global representations. Moreover, a novel Temporal Reciprocal Learning
(TRL) mechanism is designed to sequentially enhance the high-correlation
semantic information and accumulate the low-correlation sub-critical clues.
Extensive experiments on three public benchmarks indicate that our approach can
achieve better performance than other state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2103.04337" target="_blank">arXiv:2103.04337</a> [<a href="http://arxiv.org/pdf/2103.04337" target="_blank">pdf</a>]

<h2>Learning a State Representation and Navigation in Cluttered and Dynamic Environments. (arXiv:2103.04351v1 [cs.RO])</h2>
<h3>David Hoeller, Lorenz Wellhausen, Farbod Farshidian, Marco Hutter</h3>
<p>In this work, we present a learning-based pipeline to realise local
navigation with a quadrupedal robot in cluttered environments with static and
dynamic obstacles. Given high-level navigation commands, the robot is able to
safely locomote to a target location based on frames from a depth camera
without any explicit mapping of the environment. First, the sequence of images
and the current trajectory of the camera are fused to form a model of the world
using state representation learning. The output of this lightweight module is
then directly fed into a target-reaching and obstacle-avoiding policy trained
with reinforcement learning. We show that decoupling the pipeline into these
components results in a sample efficient policy learning stage that can be
fully trained in simulation in just a dozen minutes. The key part is the state
representation, which is trained to not only estimate the hidden state of the
world in an unsupervised fashion, but also helps bridging the reality gap,
enabling successful sim-to-real transfer. In our experiments with the
quadrupedal robot ANYmal in simulation and in reality, we show that our system
can handle noisy depth images, avoid dynamic obstacles unseen during training,
and is endowed with local spatial awareness.
</p>
<a href="http://arxiv.org/abs/2103.04351" target="_blank">arXiv:2103.04351</a> [<a href="http://arxiv.org/pdf/2103.04351" target="_blank">pdf</a>]

<h2>Spatial-Spectral Feedback Network for Super-Resolution of Hyperspectral Imagery. (arXiv:2103.04354v1 [cs.CV])</h2>
<h3>Enhai Liu, Zhenjie Tang, Bin Pan, Zhenwei Shi</h3>
<p>Recently, single gray/RGB image super-resolution (SR) methods based on deep
learning have achieved great success. However, there are two obstacles to limit
technical development in the single hyperspectral image super-resolution. One
is the high-dimensional and complex spectral patterns in hyperspectral image,
which make it difficult to explore spatial information and spectral information
among bands simultaneously. The other is that the number of available
hyperspectral training samples is extremely small, which can easily lead to
overfitting when training a deep neural network. To address these issues, in
this paper, we propose a novel Spatial-Spectral Feedback Network (SSFN) to
refine low-level representations among local spectral bands with high-level
information from global spectral bands. It will not only alleviate the
difficulty in feature extraction due to high dimensional of hyperspectral data,
but also make the training process more stable. Specifically, we use hidden
states in an RNN with finite unfoldings to achieve such feedback manner. To
exploit the spatial and spectral prior, a Spatial-Spectral Feedback Block
(SSFB) is designed to handle the feedback connections and generate powerful
high-level representations. The proposed SSFN comes with a early predictions
and can reconstruct the final high-resolution hyperspectral image step by step.
Extensive experimental results on three benchmark datasets demonstrate that the
proposed SSFN achieves superior performance in comparison with the
state-of-the-art methods. The source code is available at
https://github.com/tangzhenjie/SSFN.
</p>
<a href="http://arxiv.org/abs/2103.04354" target="_blank">arXiv:2103.04354</a> [<a href="http://arxiv.org/pdf/2103.04354" target="_blank">pdf</a>]

<h2>IRON: Invariant-based Highly Robust Point Cloud Registration. (arXiv:2103.04357v1 [cs.CV])</h2>
<h3>Lei Sun</h3>
<p>In this paper, we present IRON (Invariant-based global Robust estimation and
OptimizatioN), a non-minimal and highly robust solution for point cloud
registration with a great number of outliers among the correspondences. To
realize this, we decouple the registration problem into the estimation of
scale, rotation and translation, respectively. Our first contribution is to
propose RANSIC (RANdom Samples with Invariant Compatibility), which employs the
invariant compatibility to seek inliers among random samples and robustly
estimates the scale between two sets of point clouds in the meantime. Once the
scale is estimated, our second contribution is to relax the non-convex global
registration problem into a convex Semi-Definite Program (SDP) in a certifiable
way using Sum-of-Squares (SOS) Relaxation and show that the relaxation is
tight. For robust estimation, we further propose RT-GNC (Rough Trimming and
Graduated Non-Convexity), a global outlier rejection heuristic having better
robustness and time-efficiency than traditional GNC, as our third contribution.
With these contributions, we can render our registration algorithm, IRON.
Through experiments over real datasets, we show that IRON is efficient, highly
accurate and robust against as many as 99% outliers whether the scale is known
or unknown, outperforming the existing state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/2103.04357" target="_blank">arXiv:2103.04357</a> [<a href="http://arxiv.org/pdf/2103.04357" target="_blank">pdf</a>]

<h2>Learning When to Quit: Meta-Reasoning for Motion Planning. (arXiv:2103.04374v1 [cs.RO])</h2>
<h3>Yoonchang Sung, Leslie Pack Kaelbling, Tom&#xe1;s Lozano-P&#xe9;rez</h3>
<p>Anytime motion planners are widely used in robotics. However, the
relationship between their solution quality and computation time is not well
understood, and thus, determining when to quit planning and start execution is
unclear. In this paper, we address the problem of deciding when to stop
deliberation under bounded computational capacity, so called meta-reasoning,
for anytime motion planning. We propose data-driven learning methods,
model-based and model-free meta-reasoning, that are applicable to different
environment distributions and agnostic to the choice of anytime motion
planners. As a part of the framework, we design a convolutional neural
network-based optimal solution predictor that predicts the optimal path length
from a given 2D workspace image. We empirically evaluate the performance of the
proposed methods in simulation in comparison with baselines.
</p>
<a href="http://arxiv.org/abs/2103.04374" target="_blank">arXiv:2103.04374</a> [<a href="http://arxiv.org/pdf/2103.04374" target="_blank">pdf</a>]

<h2>Repurposing GANs for One-shot Semantic Part Segmentation. (arXiv:2103.04379v1 [cs.CV])</h2>
<h3>Nontawat Tritrong, Pitchaporn Rewatbowornwong, Supasorn Suwajanakorn</h3>
<p>While GANs have shown success in realistic image generation, the idea of
using GANs for other tasks unrelated to synthesis is underexplored. Do GANs
learn meaningful structural parts of objects during their attempt to reproduce
those objects? In this work, we test this hypothesis and propose a simple and
effective approach based on GANs for semantic part segmentation that requires
as few as one label example along with an unlabeled dataset. Our key idea is to
leverage a trained GAN to extract pixel-wise representation from the input
image and use it as feature vectors for a segmentation network. Our experiments
demonstrate that GANs representation is "readily discriminative" and produces
surprisingly good results that are comparable to those from supervised
baselines trained with significantly more labels. We believe this novel
repurposing of GANs underlies a new class of unsupervised representation
learning that is applicable to many other tasks. More results are available at
https://repurposegans.github.io/.
</p>
<a href="http://arxiv.org/abs/2103.04379" target="_blank">arXiv:2103.04379</a> [<a href="http://arxiv.org/pdf/2103.04379" target="_blank">pdf</a>]

<h2>Automatic Flare Spot Artifact Detection and Removal in Photographs. (arXiv:2103.04384v1 [cs.CV])</h2>
<h3>Patricia Vitoria, Coloma Ballester</h3>
<p>Flare spot is one type of flare artifact caused by a number of conditions,
frequently provoked by one or more high-luminance sources within or close to
the camera field of view. When light rays coming from a high-luminance source
reach the front element of a camera, it can produce intra-reflections within
camera elements that emerge at the film plane forming non-image information or
flare on the captured image. Even though preventive mechanisms are used,
artifacts can appear. In this paper, we propose a robust computational method
to automatically detect and remove flare spot artifacts. Our contribution is
threefold: firstly, we propose a characterization which is based on intrinsic
properties that a flare spot is likely to satisfy; secondly, we define a new
confidence measure able to select flare spots among the candidates; and,
finally, a method to accurately determine the flare region is given. Then, the
detected artifacts are removed by using exemplar-based inpainting. We show that
our algorithm achieve top-tier quantitative and qualitative performance.
</p>
<a href="http://arxiv.org/abs/2103.04384" target="_blank">arXiv:2103.04384</a> [<a href="http://arxiv.org/pdf/2103.04384" target="_blank">pdf</a>]

<h2>Robopheus: A Virtual-Physical Interactive Mobile Robotic Testbed. (arXiv:2103.04391v1 [cs.RO])</h2>
<h3>Xuda Ding, Han Wang, Hongbo Li, Hao Jiang, Jianping He</h3>
<p>The mobile robotic testbed is an essential and critical support to verify the
effectiveness of mobile robotics research. This paper introduces a novel
multi-robot testbed, named Robopheus, which exploits the ideas of
virtual-physical modeling in digital-twin. Unlike most existing testbeds, the
developed Robopheus constructs a bridge that connects the traditional physical
hardware and virtual simulation testbeds, providing scalable, interactive, and
high-fidelity simulations-tests on both sides. Another salient feature of the
Robopheus is that it enables a new form to learn the actual models from the
physical environment dynamically and is compatible with heterogeneous robot
chassis and controllers. In turn, the virtual world's learned models are
further leveraged to approximate the robot dynamics online on the physical
side. Extensive experiments demonstrate the extraordinary performance of the
Robopheus. Significantly, the physical-virtual interaction design increases the
trajectory accuracy of a real robot by 300%, compared with that of not using
the interaction.
</p>
<a href="http://arxiv.org/abs/2103.04391" target="_blank">arXiv:2103.04391</a> [<a href="http://arxiv.org/pdf/2103.04391" target="_blank">pdf</a>]

<h2>What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels. (arXiv:2103.04400v1 [cs.CV])</h2>
<h3>Jeonghun Baek, Yusuke Matsui, Kiyoharu Aizawa</h3>
<p>Scene text recognition (STR) task has a common practice: All state-of-the-art
STR models are trained on large synthetic data. In contrast to this practice,
training STR models only on fewer real labels (STR with fewer labels) is
important when we have to train STR models without synthetic data: for
handwritten or artistic texts that are difficult to generate synthetically and
for languages other than English for which we do not always have synthetic
data. However, there has been implicit common knowledge that training STR
models on real data is nearly impossible because real data is insufficient. We
consider that this common knowledge has obstructed the study of STR with fewer
labels. In this work, we would like to reactivate STR with fewer labels by
disproving the common knowledge. We consolidate recently accumulated public
real data and show that we can train STR models satisfactorily only with real
labeled data. Subsequently, we find simple data augmentation to fully exploit
real data. Furthermore, we improve the models by collecting unlabeled data and
introducing semi- and self-supervised methods. As a result, we obtain a
competitive model to state-of-the-art methods. To the best of our knowledge,
this is the first study that 1) shows sufficient performance by only using real
labels and 2) introduces semi- and self-supervised methods into STR with fewer
labels. Our code and data are available:
https://github.com/ku21fan/STR-Fewer-Labels
</p>
<a href="http://arxiv.org/abs/2103.04400" target="_blank">arXiv:2103.04400</a> [<a href="http://arxiv.org/pdf/2103.04400" target="_blank">pdf</a>]

<h2>Multimodal VAE Active Inference Controller. (arXiv:2103.04412v1 [cs.RO])</h2>
<h3>Cristian Meo, Pablo Lanillos</h3>
<p>Active inference, a theoretical construct inspired by brain processing, is a
promising alternative to control artificial agents. However, current methods do
not yet scale to high-dimensional inputs in continuous control. Here we present
a novel active inference torque controller for industrial arms that maintains
the adaptive characteristics of previous proprioceptive approaches but also
enables large-scale multimodal integration (e.g., raw images). We extended our
previous mathematical formulation by including multimodal state representation
learning using a linearly coupled multimodal variational autoencoder. We
evaluated our model on a simulated 7DOF Franka Emika Panda robot arm and
compared its behavior with a previous active inference baseline and the Panda
built-in optimized controller. Results showed improved tracking and control in
goal-directed reaching due to the increased representation power, high
robustness to noise and adaptability in changes on the environmental conditions
and robot parameters without the need to relearn the generative models nor
parameters retuning.
</p>
<a href="http://arxiv.org/abs/2103.04412" target="_blank">arXiv:2103.04412</a> [<a href="http://arxiv.org/pdf/2103.04412" target="_blank">pdf</a>]

<h2>Snapshot Compressive Imaging: Principle, Implementation, Theory, Algorithms and Applications. (arXiv:2103.04421v1 [cs.CV])</h2>
<h3>Xin Yuan, David J. Brady, Aggelos K. Katsaggelos</h3>
<p>Capturing high-dimensional (HD) data is a long-term challenge in signal
processing and related fields. Snapshot compressive imaging (SCI) uses a
two-dimensional (2D) detector to capture HD ($\ge3$D) data in a {\em snapshot}
measurement. Via novel optical designs, the 2D detector samples the HD data in
a {\em compressive} manner; following this, algorithms are employed to
reconstruct the desired HD data-cube. SCI has been used in hyperspectral
imaging, video, holography, tomography, focal depth imaging, polarization
imaging, microscopy, \etc.~Though the hardware has been investigated for more
than a decade, the theoretical guarantees have only recently been derived.
Inspired by deep learning, various deep neural networks have also been
developed to reconstruct the HD data-cube in spectral SCI and video SCI. This
article reviews recent advances in SCI hardware, theory and algorithms,
including both optimization-based and deep-learning-based algorithms. Diverse
applications and the outlook of SCI are also discussed.
</p>
<a href="http://arxiv.org/abs/2103.04421" target="_blank">arXiv:2103.04421</a> [<a href="http://arxiv.org/pdf/2103.04421" target="_blank">pdf</a>]

<h2>When Being Soft Makes You Tough: A Collision Resilient Quadcopter Inspired by Arthropod Exoskeletons. (arXiv:2103.04423v1 [cs.RO])</h2>
<h3>Ricardo de Azambuja, Hassan Fouad, Giovanni Beltrame</h3>
<p>Flying robots are usually rather delicate, and require protective enclosures
when facing the risk of collision. High complexity and reduced payload are
recurrent problems with collision-tolerant flying robots. Inspired by
arthropods' exoskeletons, we design a simple, easily manufactured, semi-rigid
structure with flexible joints that can withstand high-velocity impacts. With
an exoskeleton, the protective shell becomes part of the main robot structure,
thereby minimizing its loss in payload capacity. Our design is simple to build
and customize using cheap components and consumer-grade 3D printers. Our
results show we can build a sub-250g, autonomous quadcopter with visual
navigation that can survive multiple collisions at speeds up to 7m/s that is
also suitable for automated battery swapping, and with enough computing power
to run deep neural network models. This structure makes for an ideal platform
for high-risk activities (such as flying in a cluttered environment or
reinforcement learning training) without damage to the hardware or the
environment.
</p>
<a href="http://arxiv.org/abs/2103.04423" target="_blank">arXiv:2103.04423</a> [<a href="http://arxiv.org/pdf/2103.04423" target="_blank">pdf</a>]

<h2>TransBTS: Multimodal Brain Tumor Segmentation Using Transformer. (arXiv:2103.04430v1 [cs.CV])</h2>
<h3>Wenxuan Wang, Chen Chen, Meng Ding, Jiangyun Li, Hong Yu, Sen Zha</h3>
<p>Transformer, which can benefit from global (long-range) information modeling
using self-attention mechanisms, has been successful in natural language
processing and 2D image classification recently. However, both local and global
features are crucial for dense prediction tasks, especially for 3D medical
image segmentation. In this paper, we for the first time exploit Transformer in
3D CNN for MRI Brain Tumor Segmentation and propose a novel network named
TransBTS based on the encoder-decoder structure. To capture the local 3D
context information, the encoder first utilizes 3D CNN to extract the
volumetric spatial feature maps. Meanwhile, the feature maps are reformed
elaborately for tokens that are fed into Transformer for global feature
modeling. The decoder leverages the features embedded by Transformer and
performs progressive upsampling to predict the detailed segmentation map.
Experimental results on the BraTS 2019 dataset show that TransBTS outperforms
state-of-the-art methods for brain tumor segmentation on 3D MRI scans. Code is
available at https://github.com/Wenxuan-1119/TransBTS
</p>
<a href="http://arxiv.org/abs/2103.04430" target="_blank">arXiv:2103.04430</a> [<a href="http://arxiv.org/pdf/2103.04430" target="_blank">pdf</a>]

<h2>Adaptive Agent Architecture for Real-time Human-Agent Teaming. (arXiv:2103.04439v1 [cs.RO])</h2>
<h3>Tianwei Ni, Huao Li, Siddharth Agrawal, Suhas Raja, Fan Jia, Yikang Gui, Dana Hughes, Michael Lewis, Katia Sycara</h3>
<p>Teamwork is a set of interrelated reasoning, actions and behaviors of team
members that facilitate common objectives. Teamwork theory and experiments have
resulted in a set of states and processes for team effectiveness in both
human-human and agent-agent teams. However, human-agent teaming is less well
studied because it is so new and involves asymmetry in policy and intent not
present in human teams. To optimize team performance in human-agent teaming, it
is critical that agents infer human intent and adapt their polices for smooth
coordination. Most literature in human-agent teaming builds agents referencing
a learned human model. Though these agents are guaranteed to perform well with
the learned model, they lay heavy assumptions on human policy such as
optimality and consistency, which is unlikely in many real-world scenarios. In
this paper, we propose a novel adaptive agent architecture in human-model-free
setting on a two-player cooperative game, namely Team Space Fortress (TSF).
Previous human-human team research have shown complementary policies in TSF
game and diversity in human players' skill, which encourages us to relax the
assumptions on human policy. Therefore, we discard learning human models from
human data, and instead use an adaptation strategy on a pre-trained library of
exemplar policies composed of RL algorithms or rule-based methods with minimal
assumptions of human behavior. The adaptation strategy relies on a novel
similarity metric to infer human policy and then selects the most complementary
policy in our library to maximize the team performance. The adaptive agent
architecture can be deployed in real-time and generalize to any off-the-shelf
static agents. We conducted human-agent experiments to evaluate the proposed
adaptive agent framework, and demonstrated the suboptimality, diversity, and
adaptability of human policies in human-agent teams.
</p>
<a href="http://arxiv.org/abs/2103.04439" target="_blank">arXiv:2103.04439</a> [<a href="http://arxiv.org/pdf/2103.04439" target="_blank">pdf</a>]

<h2>Correct-by-Construction Navigation Functions with Application to Sensor Based Robot Navigation. (arXiv:2103.04445v1 [cs.RO])</h2>
<h3>Savvas G. Loizou, Elon D. Rimon</h3>
<p>This paper brings together the concepts of navigation transformation and
harmonic functions to form navigation functions that are
correct-by-construction in the sense that no tuning is required. The form of
the navigation function is explicitly related to the number of obstacles in the
environment. This enables application of navigation functions for autonomous
robot navigation in partially or fully unknown environments, with the
capability of on-the-fly adjustment of the navigation function when new
obstacles are discovered by the robot. Appropriate navigation controllers,
applicable to robots with local, sector bounded sensing, are presented and
analyzed for a~kinematic point-mass robot and then for the dynamic point-mass
robot system. The closed form nature of the proposed navigation scheme provides
for online, fast-feedback based navigation. In addition to the analytic
guarantees, simulation studies are presented to verify the effectiveness of the
methodology.
</p>
<a href="http://arxiv.org/abs/2103.04445" target="_blank">arXiv:2103.04445</a> [<a href="http://arxiv.org/pdf/2103.04445" target="_blank">pdf</a>]

<h2>Decentralized 2-Robot Transportation with Local and Indirect Sensing. (arXiv:2103.04460v1 [cs.RO])</h2>
<h3>Monimoy Bujarbaruah, Yvonne R. St&#xfc;rz, Conrad Holda, Karl H. Johansson, Francesco Borrelli</h3>
<p>In this paper, we propose a leader-follower hierarchical strategy for two
robots collaboratively transporting an object in a partially known environment
with obstacles. Both robots sense the local surrounding environment and react
to obstacles in their proximity. We consider no explicit communication, so the
local environment information and the control actions are not shared between
the robots. At any given time step, the leader solves a model predictive
control (MPC) problem with its known set of obstacles and plans a feasible
trajectory to complete the task. The follower estimates the inputs of the
leader and uses a policy to assist the leader while reacting to obstacles in
its proximity. The leader infers obstacles in the follower's vicinity by using
the difference between the predicted and the real-time estimated follower
control action. A method to switch the leader-follower roles is used to improve
the control performance in tight environments. The efficacy of our approach is
demonstrated with detailed comparisons to two alternative strategies, where it
achieves the highest success rate, while completing the task fastest.
</p>
<a href="http://arxiv.org/abs/2103.04460" target="_blank">arXiv:2103.04460</a> [<a href="http://arxiv.org/pdf/2103.04460" target="_blank">pdf</a>]

<h2>Rapidly-exploring Random Forest: Adaptively Exploits Local Structure with Generalised Multi-Trees Motion Planning. (arXiv:2103.04487v1 [cs.RO])</h2>
<h3>Tin Lai</h3>
<p>Sampling-based motion planners perform exceptionally well in robotic
applications that operate in high-dimensional space. However, most works often
constrain the planning workspace rooted at some fixed locations, do not
adaptively reason on strategy in narrow passages, and ignore valuable local
structure information. In this paper, we propose Rapidly-exploring Random
Forest (RRF*) -- a generalised multi-trees motion planner that combines the
rapid exploring property of tree-based methods and adaptively learns to deploys
a Bayesian local sampling strategy in regions that are deemed to be
bottlenecks. Local sampling exploits the local-connectivity of spaces via
Markov Chain random sampling, which is updated sequentially with a Bayesian
proposal distribution to learns the local structure from past observations. The
trees selection problem is formulated as a multi-armed bandit problem, which
efficiently allocates resources on the most promising tree to accelerate
planning runtime. RRF* learns the region that is difficult to perform tree
extensions and adaptively deploys local sampling in those regions to maximise
the benefit of exploiting local structure. We provide rigorous proofs of
completeness and optimal convergence guarantees, and we experimentally
demonstrate that the effectiveness of RRF*'s adaptive multi-trees approach
allows it to performs well in a wide range of problems.
</p>
<a href="http://arxiv.org/abs/2103.04487" target="_blank">arXiv:2103.04487</a> [<a href="http://arxiv.org/pdf/2103.04487" target="_blank">pdf</a>]

<h2>Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems. (arXiv:2103.04490v1 [cs.RO])</h2>
<h3>Spencer M. Richards, Navid Azizan, Jean-Jacques E. Slotine, Marco Pavone</h3>
<p>Real-time adaptation is imperative to the control of robots operating in
complex, dynamic environments. Adaptive control laws can endow even nonlinear
systems with good trajectory tracking performance, provided that any uncertain
dynamics terms are linearly parameterizable with known nonlinear features.
However, it is often difficult to specify such features a priori, such as for
aerodynamic disturbances on rotorcraft or interaction forces between a
manipulator arm and various objects. In this paper, we turn to data-driven
modeling with neural networks to learn, offline from past data, an adaptive
controller with an internal parametric model of these nonlinear features. Our
key insight is that we can better prepare the controller for deployment with
control-oriented meta-learning of features in closed-loop simulation, rather
than regression-oriented meta-learning of features to fit input-output data.
Specifically, we meta-learn the adaptive controller with closed-loop tracking
simulation as the base-learner and the average tracking error as the
meta-objective. With a nonlinear planar rotorcraft subject to wind, we
demonstrate that our adaptive controller outperforms other controllers trained
with regression-oriented meta-learning when deployed in closed-loop for
trajectory tracking control.
</p>
<a href="http://arxiv.org/abs/2103.04490" target="_blank">arXiv:2103.04490</a> [<a href="http://arxiv.org/pdf/2103.04490" target="_blank">pdf</a>]

<h2>Localization and Mapping using Instance-specific Mesh Models. (arXiv:2103.04493v1 [cs.CV])</h2>
<h3>Qiaojun Feng, Yue Meng, Mo Shan, Nikolay Atanasov</h3>
<p>This paper focuses on building semantic maps, containing object poses and
shapes, using a monocular camera. This is an important problem because robots
need rich understanding of geometry and context if they are to shape the future
of transportation, construction, and agriculture. Our contribution is an
instance-specific mesh model of object shape that can be optimized online based
on semantic information extracted from camera images. Multi-view constraints on
the object shape are obtained by detecting objects and extracting
category-specific keypoints and segmentation masks. We show that the errors
between projections of the mesh model and the observed keypoints and masks can
be differentiated in order to obtain accurate instance-specific object shapes.
We evaluate the performance of the proposed approach in simulation and on the
KITTI dataset by building maps of car poses and shapes.
</p>
<a href="http://arxiv.org/abs/2103.04493" target="_blank">arXiv:2103.04493</a> [<a href="http://arxiv.org/pdf/2103.04493" target="_blank">pdf</a>]

<h2>Fully Convolutional Geometric Features for Category-level Object Alignment. (arXiv:2103.04494v1 [cs.CV])</h2>
<h3>Qiaojun Feng, Nikolay Atanasov</h3>
<p>This paper focuses on pose registration of different object instances from
the same category. This is required in online object mapping because object
instances detected at test time usually differ from the training instances. Our
approach transforms instances of the same category to a normalized canonical
coordinate frame and uses metric learning to train fully convolutional
geometric features. The resulting model is able to generate pairs of matching
points between the instances, allowing category-level registration. Evaluation
on both synthetic and real-world data shows that our method provides robust
features, leading to accurate alignment of instances with different shapes.
</p>
<a href="http://arxiv.org/abs/2103.04494" target="_blank">arXiv:2103.04494</a> [<a href="http://arxiv.org/pdf/2103.04494" target="_blank">pdf</a>]

<h2>Sparsification for Fast Optimal Multi-Robot Path Planning in Lazy Compilation Schemes. (arXiv:2103.04496v1 [cs.RO])</h2>
<h3>Pavel Surynek</h3>
<p>Path planning for multiple robots (MRPP) represents a task of finding
non-colliding paths for robots through which they can navigate from their
initial positions to specified goal positions. The problem is usually modeled
using undirected graphs where robots move between vertices across edges.
Contemporary optimal solving algorithms include dedicated search-based methods,
that solve the problem directly, and compilation-based algorithms that reduce
MRPP to a different formalism for which an efficient solver exists, such as
constraint programming (CP), mixed integer programming (MIP), or Boolean
satisfiability (SAT). In this paper, we enhance existing SAT-based algorithm
for MRPP via spartification of the set of candidate paths for each robot from
which target Boolean encoding is derived. Suggested sparsification of the set
of paths led to smaller target Boolean formulae that can be constructed and
solved faster while optimality guarantees of the approach have been kept.
</p>
<a href="http://arxiv.org/abs/2103.04496" target="_blank">arXiv:2103.04496</a> [<a href="http://arxiv.org/pdf/2103.04496" target="_blank">pdf</a>]

<h2>Let's be friends! A rapport-building 3D embodied conversational agent for the Human Support Robot. (arXiv:2103.04498v1 [cs.RO])</h2>
<h3>Katarzyna Pasternak, Zishi Wu, Ubbo Visser, Christine Lisetti</h3>
<p>Partial subtle mirroring of nonverbal behaviors during conversations (also
known as mimicking or parallel empathy), is essential for rapport building,
which in turn is essential for optimal human-human communication outcomes.
Mirroring has been studied in interactions between robots and humans, and in
interactions between Embodied Conversational Agents (ECAs) and humans. However,
very few studies examine interactions between humans and ECAs that are
integrated with robots, and none of them examine the effect of mirroring
nonverbal behaviors in such interactions. Our research question is whether
integrating an ECA able to mirror its interlocutor's facial expressions and
head movements (continuously or intermittently) with a human-service robot will
improve the user's experience with the support robot that is able to perform
useful mobile manipulative tasks (e.g. at home). Our contribution is the
complex integration of an expressive ECA, able to track its interlocutor's
face, and to mirror his/her facial expressions and head movements in real time,
integrated with a human support robot such that the robot and the agent are
fully aware of each others', and of the users', nonverbals cues. We also
describe a pilot study we conducted towards answering our research question,
which shows promising results for our forthcoming larger user study.
</p>
<a href="http://arxiv.org/abs/2103.04498" target="_blank">arXiv:2103.04498</a> [<a href="http://arxiv.org/pdf/2103.04498" target="_blank">pdf</a>]

<h2>End-to-End Human Object Interaction Detection with HOI Transformer. (arXiv:2103.04503v1 [cs.CV])</h2>
<h3>Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei, Jian Sun</h3>
<p>We propose HOI Transformer to tackle human object interaction (HOI) detection
in an end-to-end manner. Current approaches either decouple HOI task into
separated stages of object detection and interaction classification or
introduce surrogate interaction problem. In contrast, our method, named HOI
Transformer, streamlines the HOI pipeline by eliminating the need for many
hand-designed components. HOI Transformer reasons about the relations of
objects and humans from global image context and directly predicts HOI
instances in parallel. A quintuple matching loss is introduced to force HOI
predictions in a unified way. Our method is conceptually much simpler and
demonstrates improved accuracy. Without bells and whistles, HOI Transformer
achieves $26.61\% $ $ AP $ on HICO-DET and $52.9\%$ $AP_{role}$ on V-COCO,
surpassing previous methods with the advantage of being much simpler. We hope
our approach will serve as a simple and effective alternative for HOI tasks.
Code is available at https://github.com/bbepoch/HoiTransformer .
</p>
<a href="http://arxiv.org/abs/2103.04503" target="_blank">arXiv:2103.04503</a> [<a href="http://arxiv.org/pdf/2103.04503" target="_blank">pdf</a>]

<h2>OPANAS: One-Shot Path Aggregation Network Architecture Search for Object. (arXiv:2103.04507v1 [cs.CV])</h2>
<h3>Tingting Liang, Yongtao Wang, Guosheng Hu, Zhi Tang, Haibin Ling</h3>
<p>Recently, neural architecture search (NAS) has been exploited to design
feature pyramid networks (FPNs) and achieved promising results for visual
object detection. Encouraged by the success, we propose a novel One-Shot Path
Aggregation Network Architecture Search (OPANAS) algorithm, which significantly
improves both searching efficiency and detection accuracy. Specifically, we
first introduce six heterogeneous information paths to build our search space,
namely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect
and none. Second, we propose a novel search space of FPNs, in which each FPN
candidate is represented by a densely-connected directed acyclic graph (each
node is a feature pyramid and each edge is one of the six heterogeneous
information paths). Third, we propose an efficient one-shot search method to
find the optimal path aggregation architecture, that is, we first train a
super-net and then find the optimal candidate with an evolutionary algorithm.
Experimental results demonstrate the efficacy of the proposed OPANAS for object
detection: (1) OPANAS is more efficient than state-of-the-art methods (e.g.,
NAS-FPN and Auto-FPN), at significantly smaller searching cost (e.g., only 4
GPU days on MS-COCO); (2) the optimal architecture found by OPANAS
significantly improves main-stream detectors including RetinaNet, Faster R-CNN
and Cascade R-CNN, by 2.3-3.2 % mAP comparing to their FPN counterparts; and
(3) a new state-of-the-art accuracy-speed trade-off (52.2 % mAP at 7.6 FPS) at
smaller training costs than comparable state-of-the-arts. Code will be released
at https://github.com/VDIGPKU/OPANAS.
</p>
<a href="http://arxiv.org/abs/2103.04507" target="_blank">arXiv:2103.04507</a> [<a href="http://arxiv.org/pdf/2103.04507" target="_blank">pdf</a>]

<h2>Predictive Visual Tracking: A New Benchmark and Baseline Approach. (arXiv:2103.04508v1 [cs.CV])</h2>
<h3>Bowen Li, Yiming Li, Junjie Ye, Changhong Fu, Hang Zhao</h3>
<p>As a crucial robotic perception capability, visual tracking has been
intensively studied recently. In the real-world scenarios, the onboard
processing time of the image streams inevitably leads to a discrepancy between
the tracking results and the real-world states. However, existing visual
tracking benchmarks commonly run the trackers offline and ignore such latency
in the evaluation. In this work, we aim to deal with a more realistic problem
of latency-aware tracking. The state-of-the-art trackers are evaluated in the
aerial scenarios with new metrics jointly assessing the tracking accuracy and
efficiency. Moreover, a new predictive visual tracking baseline is developed to
compensate for the latency stemming from the onboard computation. Our
latency-aware benchmark can provide a more realistic evaluation of the trackers
for the robotic applications. Besides, exhaustive experiments have proven the
effectiveness of the proposed predictive visual tracking baseline approach.
</p>
<a href="http://arxiv.org/abs/2103.04508" target="_blank">arXiv:2103.04508</a> [<a href="http://arxiv.org/pdf/2103.04508" target="_blank">pdf</a>]

<h2>ColoRadar: The Direct 3D Millimeter Wave Radar Dataset. (arXiv:2103.04510v1 [cs.RO])</h2>
<h3>Andrew Kramer, Kyle Harlow, Christopher Williams, Christoffer Heckman</h3>
<p>Millimeter wave radar is becoming increasingly popular as a sensing modality
for robotic mapping and state estimation. However, there are very few publicly
available datasets that include dense, high-resolution millimeter wave radar
scans and there are none focused on 3D odometry and mapping. In this paper we
present a solution to that problem. The ColoRadar dataset includes 3 different
forms of dense, high-resolution radar data from 2 FMCW radar sensors as well as
3D lidar, IMU, and highly accurate groundtruth for the sensor rig's pose over
approximately 2 hours of data collection in highly diverse 3D environments.
</p>
<a href="http://arxiv.org/abs/2103.04510" target="_blank">arXiv:2103.04510</a> [<a href="http://arxiv.org/pdf/2103.04510" target="_blank">pdf</a>]

<h2>Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN. (arXiv:2103.04513v1 [cs.CV])</h2>
<h3>Desheng Wang (1), Weidong Jin (1), Yunpu Wu (1), Aamir Khan (1) ((1) School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P. R. China)</h3>
<p>Convolutional neural networks (CNNs) have achieved beyond human-level
accuracy in the image classification task and are widely deployed in real-world
environments. However, CNNs show vulnerability to adversarial perturbations
that are well-designed noises aiming to mislead the classification models. In
order to defend against the adversarial perturbations, adversarially trained
GAN (ATGAN) is proposed to improve the adversarial robustness generalization of
the state-of-the-art CNNs trained by adversarial training. ATGAN incorporates
adversarial training into standard GAN training procedure to remove obfuscated
gradients which can lead to a false sense in defending against the adversarial
perturbations and are commonly observed in existing GANs-based adversarial
defense methods. Moreover, ATGAN adopts the image-to-image generator as data
augmentation to increase the sample complexity needed for adversarial
robustness generalization in adversarial training. Experimental results in
MNIST SVHN and CIFAR-10 datasets show that the proposed method doesn't rely on
obfuscated gradients and achieves better global adversarial robustness
generalization performance than the adversarially trained state-of-the-art
CNNs.
</p>
<a href="http://arxiv.org/abs/2103.04513" target="_blank">arXiv:2103.04513</a> [<a href="http://arxiv.org/pdf/2103.04513" target="_blank">pdf</a>]

<h2>Loosely Synchronized Search for Multi-agent Path Finding with Asynchronous Actions. (arXiv:2103.04516v1 [cs.RO])</h2>
<h3>Zhongqiang Ren, Sivakumar Rathinam, Howie Choset</h3>
<p>Multi-agent path finding (MAPF) determines an ensemble of collision-free
paths for multiple agents between their respective start and goal locations.
Among the available MAPF planners for workspaces modeled as a graph, A*-based
approaches have been widely investigated and have demonstrated their efficiency
in numerous scenarios. However, almost all of these A*-based approaches assume
that each agent executes an action concurrently in that all agents start and
stop together. This article presents a natural generalization of MAPF with
asynchronous actions where agents do not necessarily start and stop
concurrently. The main contribution of the work is a proposed approach called
Loosely Synchronized Search (LSS) that extends A*-based MAPF planners to handle
asynchronous actions. We show LSS is complete and finds an optimal solution if
one exists. We also combine LSS with other existing MAPF methods that aims to
trade-off optimality for computational efficiency. Extensive numerical results
are presented to corroborate the performance of the proposed approaches.
Finally, we also verify the applicability of our method in the Robotarium, a
remotely accessible swarm robotics research platform.
</p>
<a href="http://arxiv.org/abs/2103.04516" target="_blank">arXiv:2103.04516</a> [<a href="http://arxiv.org/pdf/2103.04516" target="_blank">pdf</a>]

<h2>Unveiling the Potential of Structure-Preserving for Weakly Supervised Object Localization. (arXiv:2103.04523v1 [cs.CV])</h2>
<h3>Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, Changsheng Xu</h3>
<p>Weakly supervised object localization remains an open problem due to the
deficiency of finding object extent information using a classification network.
While prior works struggle to localize objects by various spatial
regularization strategies, we argue that how to extract object structural
information from the trained classification network is neglected. In this
paper, we propose a two-stage approach, termed structure-preserving activation
(SPA), towards fully leveraging the structure information incorporated in
convolutional features for WSOL. In the first stage, a restricted activation
module (RAM) is designed to alleviate the structure-missing issue caused by the
classification network, based on the observation that the unbounded
classification map and global average pooling layer drive the network to focus
only on object parts. In the second stage, we propose a post-process approach,
termed self-correlation map generating (SCG) module to obtain
structure-preserving localization maps on the basis of the activation maps
acquired from the first stage. Specifically, we utilize the high-order
self-correlation (HSC) to extract the inherent structural information retained
in the learned model and then aggregate HSC of multiple points for precise
object localization. Extensive experiments on two publicly available benchmarks
including CUB-200-2011 and ILSVRC show that the proposed SPA achieves
substantial and consistent performance gains compared with baseline approaches.
</p>
<a href="http://arxiv.org/abs/2103.04523" target="_blank">arXiv:2103.04523</a> [<a href="http://arxiv.org/pdf/2103.04523" target="_blank">pdf</a>]

<h2>FastFlowNet: A Lightweight Network for Fast Optical Flow Estimation. (arXiv:2103.04524v1 [cs.CV])</h2>
<h3>Lingtong Kong, Chunhua Shen, Jie Yang</h3>
<p>Dense optical flow estimation plays a key role in many robotic vision tasks.
It has been predicted with satisfying accuracy than traditional methods with
advent of deep learning. However, current networks often occupy large number of
parameters and require heavy computation costs. These drawbacks have hindered
applications on power- or memory-constrained mobile devices. To deal with these
challenges, in this paper, we dive into designing efficient structure for fast
and accurate optical flow prediction. Our proposed FastFlowNet works in the
well-known coarse-to-fine manner with following innovations. First, a new head
enhanced pooling pyramid (HEPP) feature extractor is employed to intensify
high-resolution pyramid feature while reducing parameters. Second, we introduce
a novel center dense dilated correlation (CDDC) layer for constructing compact
cost volume that can keep large search radius with reduced computation burden.
Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid
level to acclerate flow estimation with marginal drops in accuracy. Experiments
on both synthetic Sintel and real-world KITTI datasets demonstrate the
effectiveness of proposed approaches, which consumes only 1/10 computation of
comparable networks to get 90% of their performance. In particular, FastFlowNet
only contains 1.37 M parameters and runs at 90 or 5.7 fps with one desktop
NVIDIA GTX 1080 Ti or embedded Jetson TX2 GPU on Sintel resolution images.
</p>
<a href="http://arxiv.org/abs/2103.04524" target="_blank">arXiv:2103.04524</a> [<a href="http://arxiv.org/pdf/2103.04524" target="_blank">pdf</a>]

<h2>Incremental Learning for Multi-organ Segmentation with Partially Labeled Datasets. (arXiv:2103.04526v1 [cs.CV])</h2>
<h3>Pengbo Liu, Li Xiao, S. Kevin Zhou</h3>
<p>There exists a large number of datasets for organ segmentation, which are
partially annotated, and sequentially constructed. A typical dataset is
constructed at a certain time by curating medical images and annotating the
organs of interest. In other words, new datasets with annotations of new organ
categories are built over time. To unleash the potential behind these partially
labeled, sequentially-constructed datasets, we propose to learn a multi-organ
segmentation model through incremental learning (IL). In each IL stage, we lose
access to the previous annotations, whose knowledge is assumingly captured by
the current model, and gain the access to a new dataset with annotations of new
organ categories, from which we learn to update the organ segmentation model to
include the new organs. We give the first attempt to conjecture that the
different distribution is the key reason for 'catastrophic forgetting' that
commonly exists in IL methods, and verify that IL has the natural adaptability
to medical image scenarios. Extensive experiments on five open-sourced datasets
are conducted to prove the effectiveness of our method and the conjecture
mentioned above.
</p>
<a href="http://arxiv.org/abs/2103.04526" target="_blank">arXiv:2103.04526</a> [<a href="http://arxiv.org/pdf/2103.04526" target="_blank">pdf</a>]

<h2>One-Shot Medical Landmark Detection. (arXiv:2103.04527v1 [cs.CV])</h2>
<h3>Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou</h3>
<p>The success of deep learning methods relies on the availability of a large
number of datasets with annotations; however, curating such datasets is
burdensome, especially for medical images. To relieve such a burden for a
landmark detection task, we explore the feasibility of using only a single
annotated image and propose a novel framework named Cascade Comparing to Detect
(CC2D) for one-shot landmark detection. CC2D consists of two stages: 1)
Self-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels
(CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a
coarse-to-fine fashion by comparing the cascade feature representations and
generates predictions on the training set. CC2D-TPL further improves the
performance by training a new landmark detector with those predictions. The
effectiveness of CC2D is evaluated on a widely-used public dataset of
cephalometric landmark detection, which achieves a competitive detection
accuracy of 81.01\% within 4.0mm, comparable to the state-of-the-art
fully-supervised methods using a lot more than one training image.
</p>
<a href="http://arxiv.org/abs/2103.04527" target="_blank">arXiv:2103.04527</a> [<a href="http://arxiv.org/pdf/2103.04527" target="_blank">pdf</a>]

<h2>Decision-Making under On-Ramp merge Scenarios by Distributional Soft Actor-Critic Algorithm. (arXiv:2103.04535v1 [cs.RO])</h2>
<h3>Yiting Kong, Yang Guan, Jingliang Duan, Shengbo Eben Li, Qi Sun, Bingbing Nie</h3>
<p>Merging into the highway from the on-ramp is an essential scenario for
automated driving. The decision-making under the scenario needs to balance the
safety and efficiency performance to optimize a long-term objective, which is
challenging due to the dynamic, stochastic, and adversarial characteristics.
The Rule-based methods often lead to conservative driving on this task while
the learning-based methods have difficulties meeting the safety requirements.
In this paper, we propose an RL-based end-to-end decision-making method under a
framework of offline training and online correction, called the Shielded
Distributional Soft Actor-critic (SDSAC). The SDSAC adopts the policy
evaluation with safety consideration and a safety shield parameterized with the
barrier function in its offline training and online correction, respectively.
These two measures support each other for better safety while not damaging the
efficiency performance severely. We verify the SDSAC on an on-ramp merge
scenario in simulation. The results show that the SDSAC has the best safety
performance compared to baseline algorithms and achieves efficient driving
simultaneously.
</p>
<a href="http://arxiv.org/abs/2103.04535" target="_blank">arXiv:2103.04535</a> [<a href="http://arxiv.org/pdf/2103.04535" target="_blank">pdf</a>]

<h2>Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v1 [cs.CV])</h2>
<h3>Ruizhi Liao, Daniel Moyer, Miriam Cha, Keegan Quigley, Seth Berkowitz, Steven Horng, Polina Golland, William M. Wells</h3>
<p>We propose and demonstrate a representation learning approach by maximizing
the mutual information between local features of images and text. The goal of
this approach is to learn useful image representations by taking advantage of
the rich information contained in the free text that describes the findings in
the image. Our method learns image and text encoders by encouraging the
resulting representations to exhibit high local mutual information. We make use
of recent advances in mutual information estimation with neural network
discriminators. We argue that, typically, the sum of local mutual information
is a lower bound on the global mutual information. Our experimental results in
the downstream image classification tasks demonstrate the advantages of using
local features for image-text representation learning.
</p>
<a href="http://arxiv.org/abs/2103.04537" target="_blank">arXiv:2103.04537</a> [<a href="http://arxiv.org/pdf/2103.04537" target="_blank">pdf</a>]

<h2>Learning Unstable Dynamics with One Minute of Data: A Differentiation-based Gaussian Process Approach. (arXiv:2103.04548v1 [cs.RO])</h2>
<h3>Ivan D. Jimenez Rodriguez, Ugo Rosolia, Aaron D. Ames, Yisong Yue</h3>
<p>We present a straightforward and efficient way to estimate dynamics models
for unstable robotic systems. Specifically, we show how to exploit the
differentiability of Gaussian processes to create a state-dependent linearized
approximation of the true continuous dynamics. Our approach is compatible with
most Gaussian process approaches for system identification, and can learn an
accurate model using modest amounts of training data. We validate our approach
by iteratively learning the system dynamics of an unstable system such as a 9-D
segway (using only one minute of data) and we show that the resulting
controller is robust to unmodelled dynamics and disturbances, while
state-of-the-art control methods based on nominal models can fail under small
perturbations.
</p>
<a href="http://arxiv.org/abs/2103.04548" target="_blank">arXiv:2103.04548</a> [<a href="http://arxiv.org/pdf/2103.04548" target="_blank">pdf</a>]

<h2>U-DuDoNet: Unpaired dual-domain network for CT metal artifact reduction. (arXiv:2103.04552v1 [cs.CV])</h2>
<h3>Yuanyuan Lyu, Jiajun Fu, Cheng Peng, S. Kevin Zhou</h3>
<p>Recently, both supervised and unsupervised deep learning methods have been
widely applied on the CT metal artifact reduction (MAR) task. Supervised
methods such as Dual Domain Network (Du-DoNet) work well on simulation data;
however, their performance on clinical data is limited due to domain gap.
Unsupervised methods are more generalized, but do not eliminate artifacts
completely through the sole processing on the image domain. To combine the
advantages of both MAR methods, we propose an unpaired dual-domain network
(U-DuDoNet) trained using unpaired data. Unlike the artifact disentanglement
network (ADN) that utilizes multiple encoders and decoders for disentangling
content from artifact, our U-DuDoNet directly models the artifact generation
process through additions in both sinogram and image domains, which is
theoretically justified by an additive property associated with metal artifact.
Our design includes a self-learned sinogram prior net, which provides guidance
for restoring the information in the sinogram domain, and cyclic constraints
for artifact reduction and addition on unpaired data. Extensive experiments on
simulation data and clinical images demonstrate that our novel framework
outperforms the state-of-the-art unpaired approaches.
</p>
<a href="http://arxiv.org/abs/2103.04552" target="_blank">arXiv:2103.04552</a> [<a href="http://arxiv.org/pdf/2103.04552" target="_blank">pdf</a>]

<h2>The Stroke Correspondence Problem, Revisited. (arXiv:1909.11995v3 [cs.CV] UPDATED)</h2>
<h3>Dominik Klein</h3>
<p>We revisit the stroke correspondence problem [13,14]. We optimize this
algorithm by 1) evaluating suitable preprocessing (normalization) methods 2)
extending the algorithm with an additional distance measure to handle Hiragana,
Katakana and Kanji characters with a low number of strokes and c) simplify the
stroke linking algorithms. Our contributions are implemented in the free,
open-source library ctegaki and in the demo-tools jTegaki and Kanjicanvas.
</p>
<a href="http://arxiv.org/abs/1909.11995" target="_blank">arXiv:1909.11995</a> [<a href="http://arxiv.org/pdf/1909.11995" target="_blank">pdf</a>]

<h2>An End-to-End Foreground-Aware Network for Person Re-Identification. (arXiv:1910.11547v2 [cs.CV] UPDATED)</h2>
<h3>Yiheng Liu, Wengang Zhou, Jianzhuang Liu, Guojun Qi, Qi Tian, Houqiang Li</h3>
<p>Person re-identification is a crucial task of identifying pedestrians of
interest across multiple surveillance camera views. In person
re-identification, a pedestrian is usually represented with features extracted
from a rectangular image region that inevitably contains the scene background,
which incurs ambiguity to distinguish different pedestrians and degrades the
accuracy. To this end, we propose an end-to-end foreground-aware network to
discriminate foreground from background by learning a soft mask for person
re-identification. In our method, in addition to the pedestrian ID as
supervision for foreground, we introduce the camera ID of each pedestrian image
for background modeling. The foreground branch and the background branch are
optimized collaboratively. By presenting a target attention loss, the
pedestrian features extracted from the foreground branch become more
insensitive to the backgrounds, which greatly reduces the negative impacts of
changing backgrounds on matching an identical across different camera views.
Notably, in contrast to existing methods, our approach does not require any
additional dataset to train a human landmark detector or a segmentation model
for locating the background regions. The experimental results conducted on
three challenging datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17,
demonstrate the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/1910.11547" target="_blank">arXiv:1910.11547</a> [<a href="http://arxiv.org/pdf/1910.11547" target="_blank">pdf</a>]

<h2>Gating Revisited: Deep Multi-layer RNNs That Can Be Trained. (arXiv:1911.11033v4 [cs.CV] UPDATED)</h2>
<h3>Mehmet Ozgur Turkoglu, Stefano D&#x27;Aronco, Jan Dirk Wegner, Konrad Schindler</h3>
<p>We propose a new STAckable Recurrent cell (STAR) for recurrent neural
networks (RNNs), which has fewer parameters than widely used LSTM and GRU while
being more robust against vanishing or exploding gradients. Stacking recurrent
units into deep architectures suffers from two major limitations: (i) many
recurrent cells (e.g., LSTMs) are costly in terms of parameters and computation
resources; and (ii) deep RNNs are prone to vanishing or exploding gradients
during training. We investigate the training of multi-layer RNNs and examine
the magnitude of the gradients as they propagate through the network in the
"vertical" direction. We show that, depending on the structure of the basic
recurrent unit, the gradients are systematically attenuated or amplified. Based
on our analysis we design a new type of gated cell that better preserves
gradient magnitude. We validate our design on a large number of sequence
modelling tasks and demonstrate that the proposed STAR cell allows to build and
train deeper recurrent architectures, ultimately leading to improved
performance while being computationally more efficient.
</p>
<a href="http://arxiv.org/abs/1911.11033" target="_blank">arXiv:1911.11033</a> [<a href="http://arxiv.org/pdf/1911.11033" target="_blank">pdf</a>]

<h2>Rethinking Class Relations: Absolute-relative Supervised and Unsupervised Few-shot Learning. (arXiv:2001.03919v2 [cs.CV] UPDATED)</h2>
<h3>Hongguang Zhang, Philip H. S. Torr, Hongdong Li, Songlei Jian, Piotr Koniusz</h3>
<p>The majority of existing few-shot learning methods describe image relations
with binary labels. However, such binary relations are insufficient to teach
the network complicated real-world relations, due to the lack of decision
smoothness. Furthermore, current few-shot learning models capture only the
similarity via relation labels, but they are not exposed to class concepts
associated with objects, which is likely detrimental to the classification
performance due to underutilization of the available class labels. To
paraphrase, children learn the concept of tiger from a few of actual examples
as well as from comparisons of tiger to other animals. Thus, we hypothesize
that in fact both similarity and class concept learning must be occurring
simultaneously. With these observations at hand, we study the fundamental
problem of simplistic class modeling in current few-shot learning methods. We
rethink the relations between class concepts, and propose a novel
Absolute-relative Learning paradigm to fully take advantage of label
information to refine the image representations and correct the relation
understanding in both supervised and unsupervised scenarios. Our proposed
paradigm improves the performance of several the state-of-the-art models on
publicly available datasets.
</p>
<a href="http://arxiv.org/abs/2001.03919" target="_blank">arXiv:2001.03919</a> [<a href="http://arxiv.org/pdf/2001.03919" target="_blank">pdf</a>]

<h2>Domain Adaptive Ensemble Learning. (arXiv:2003.07325v2 [cs.CV] UPDATED)</h2>
<h3>Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang</h3>
<p>The problem of generalizing deep neural networks from multiple source domains
to a target one is studied under two settings: When unlabeled target data is
available, it is a multi-source unsupervised domain adaptation (UDA) problem,
otherwise a domain generalization (DG) problem. We propose a unified framework
termed domain adaptive ensemble learning (DAEL) to address both problems. A
DAEL model is composed of a CNN feature extractor shared across domains and
multiple classifier heads each trained to specialize in a particular source
domain. Each such classifier is an expert to its own domain and a non-expert to
others. DAEL aims to learn these experts collaboratively so that when forming
an ensemble, they can leverage complementary information from each other to be
more effective for an unseen target domain. To this end, each source domain is
used in turn as a pseudo-target-domain with its own expert providing
supervisory signal to the ensemble of non-experts learned from the other
sources. For unlabeled target data under the UDA setting where real expert does
not exist, DAEL uses pseudo-label to supervise the ensemble learning. Extensive
experiments on three multi-source UDA datasets and two DG datasets show that
DAEL improves the state of the art on both problems, often by significant
margins. The code is released at
\url{https://github.com/KaiyangZhou/Dassl.pytorch}.
</p>
<a href="http://arxiv.org/abs/2003.07325" target="_blank">arXiv:2003.07325</a> [<a href="http://arxiv.org/pdf/2003.07325" target="_blank">pdf</a>]

<h2>Self-Supervised Learning for Domain Adaptation on Point-Clouds. (arXiv:2003.12641v4 [cs.CV] UPDATED)</h2>
<h3>Idan Achituve, Haggai Maron, Gal Chechik</h3>
<p>Self-supervised learning (SSL) is a technique for learning useful
representations from unlabeled data. It has been applied effectively to domain
adaptation (DA) on images and videos. It is still unknown if and how it can be
leveraged for domain adaptation in 3D perception problems. Here we describe the
first study of SSL for DA on point clouds. We introduce a new family of pretext
tasks, Deformation Reconstruction, inspired by the deformations encountered in
sim-to-real transformations. In addition, we propose a novel training procedure
for labeled point cloud data motivated by the MixUp method called Point cloud
Mixup (PCM). Evaluations on domain adaptations datasets for classification and
segmentation, demonstrate a large improvement over existing and baseline
methods.
</p>
<a href="http://arxiv.org/abs/2003.12641" target="_blank">arXiv:2003.12641</a> [<a href="http://arxiv.org/pdf/2003.12641" target="_blank">pdf</a>]

<h2>Landmark Detection and 3D Face Reconstruction for Caricature using a Nonlinear Parametric Model. (arXiv:2004.09190v2 [cs.CV] UPDATED)</h2>
<h3>Hongrui Cai, Yudong Guo, Zhuang Peng, Juyong Zhang</h3>
<p>Caricature is an artistic abstraction of the human face by distorting or
exaggerating certain facial features, while still retains a likeness with the
given face. Due to the large diversity of geometric and texture variations,
automatic landmark detection and 3D face reconstruction for caricature is a
challenging problem and has rarely been studied before. In this paper, we
propose the first automatic method for this task by a novel 3D approach. To
this end, we first build a dataset with various styles of 2D caricatures and
their corresponding 3D shapes, and then build a parametric model on vertex
based deformation space for 3D caricature face. Based on the constructed
dataset and the nonlinear parametric model, we propose a neural network based
method to regress the 3D face shape and orientation from the input 2D
caricature image. Ablation studies and comparison with state-of-the-art methods
demonstrate the effectiveness of our algorithm design. Extensive experimental
results demonstrate that our method works well for various caricatures. Our
constructed dataset, source code and trained model are available at
https://github.com/Juyong/CaricatureFace.
</p>
<a href="http://arxiv.org/abs/2004.09190" target="_blank">arXiv:2004.09190</a> [<a href="http://arxiv.org/pdf/2004.09190" target="_blank">pdf</a>]

<h2>Project RISE: Recognizing Industrial Smoke Emissions. (arXiv:2005.06111v8 [cs.CV] UPDATED)</h2>
<h3>Yen-Chia Hsu, Ting-Hao &#x27;Kenneth&#x27; Huang, Ting-Yao Hu, Paul Dille, Sean Prendi, Ryan Hoffman, Anastasia Tsuhlares, Jessica Pachuta, Randy Sargent, Illah Nourbakhsh</h3>
<p>Industrial smoke emissions pose a significant concern to human health. Prior
works have shown that using Computer Vision (CV) techniques to identify smoke
as visual evidence can influence the attitude of regulators and empower
citizens to pursue environmental justice. However, existing datasets are not of
sufficient quality nor quantity to train the robust CV models needed to support
air quality advocacy. We introduce RISE, the first large-scale video dataset
for Recognizing Industrial Smoke Emissions. We adopted a citizen science
approach to collaborate with local community members to annotate whether a
video clip has smoke emissions. Our dataset contains 12,567 clips from 19
distinct views from cameras that monitored three industrial facilities. These
daytime clips span 30 days over two years, including all four seasons. We ran
experiments using deep neural networks to establish a strong performance
baseline and reveal smoke recognition challenges. Our survey study discussed
community feedback, and our data analysis displayed opportunities for
integrating citizen scientists and crowd workers into the application of
Artificial Intelligence for Social Impact.
</p>
<a href="http://arxiv.org/abs/2005.06111" target="_blank">arXiv:2005.06111</a> [<a href="http://arxiv.org/pdf/2005.06111" target="_blank">pdf</a>]

<h2>Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification. (arXiv:2006.04150v4 [cs.CV] UPDATED)</h2>
<h3>Guile Wu, Shaogang Gong</h3>
<p>Deep learning has been successful for many computer vision tasks due to the
availability of shared and centralised large-scale training data. However,
increasing awareness of privacy concerns poses new challenges to deep learning,
especially for human subject related recognition such as person
re-identification (Re-ID). In this work, we solve the Re-ID problem by
decentralised learning from non-shared private training data distributed at
multiple user sites of independent multi-domain label spaces. We propose a
novel paradigm called Federated Person Re-Identification (FedReID) to construct
a generalisable global model (a central server) by simultaneously learning with
multiple privacy-preserved local models (local clients). Specifically, each
local client receives global model updates from the server and trains a local
model using its local data independent from all the other clients. Then, the
central server aggregates transferrable local model updates to construct a
generalisable global feature embedding model without accessing local data so to
preserve local privacy. This client-server collaborative learning process is
iteratively performed under privacy control, enabling FedReID to realise
decentralised learning without sharing distributed data nor collecting any
centralised data. Extensive experiments on ten Re-ID benchmarks show that
FedReID achieves compelling generalisation performance beyond any locally
trained models without using shared training data, whilst inherently protects
the privacy of each local client. This is uniquely advantageous over
contemporary Re-ID methods.
</p>
<a href="http://arxiv.org/abs/2006.04150" target="_blank">arXiv:2006.04150</a> [<a href="http://arxiv.org/pdf/2006.04150" target="_blank">pdf</a>]

<h2>Dataset Condensation with Gradient Matching. (arXiv:2006.05929v3 [cs.CV] UPDATED)</h2>
<h3>Bo Zhao, Konda Reddy Mopuri, Hakan Bilen</h3>
<p>As the state-of-the-art machine learning methods in many fields rely on
larger datasets, storing datasets and training models on them become
significantly more expensive. This paper proposes a training set synthesis
technique for data-efficient learning, called Dataset Condensation, that learns
to condense large dataset into a small set of informative synthetic samples for
training deep neural networks from scratch. We formulate this goal as a
gradient matching problem between the gradients of deep neural network weights
that are trained on the original and our synthetic data. We rigorously evaluate
its performance in several computer vision benchmarks and demonstrate that it
significantly outperforms the state-of-the-art methods. Finally we explore the
use of our method in continual learning and neural architecture search and
report promising gains when limited memory and computations are available.
</p>
<a href="http://arxiv.org/abs/2006.05929" target="_blank">arXiv:2006.05929</a> [<a href="http://arxiv.org/pdf/2006.05929" target="_blank">pdf</a>]

<h2>Trust Aware Emergency Response for A Resilient Human-Swarm Cooperative System. (arXiv:2006.15466v2 [cs.RO] UPDATED)</h2>
<h3>Yijiang Pang, Rui Liu</h3>
<p>A human-swarm cooperative system, which mixes multiple robots and a human
supervisor to form a heterogeneous team, is widely used for emergent scenarios
such as criminal tracking in social security and victim assistance in a natural
disaster. These emergent scenarios require a cooperative team to quickly
terminate the current task and transit the system to a new task, bringing
difficulty in motion planning. Moreover, due to the immediate task transitions,
uncertainty from both physical systems and prior tasks is accumulated to
decrease swarm performance, causing robot failures and influencing the
cooperation effectiveness between the human and the robot swarm. Therefore,
given the quick-transition requirements and the introduced uncertainty, it is
challenging for a human-swarm system to respond to emergent tasks, compared
with executing normal tasks where a gradual transition between tasks is
allowed. Human trust reveals the behavior expectations of others and is used to
adjust unsatisfactory behaviors for better cooperation. Inspired by human
trust, in this paper, a trust-aware reflective control (Trust-R) is developed
to dynamically calibrate human-swarm cooperation. Trust-R, based on a weighted
mean subsequence reduced algorithm (WMSR) and human trust modeling, helps a
swarm to self-reflect its performance from the perspective of human trust; then
proactively correct its faulty behaviors in an early stage before a human
intervenes. One typical task scenario {emergency response} was designed in the
real-gravity simulation environment, and a human user study with 145 volunteers
was conducted. Trust-R's effectiveness in correcting faulty behaviors in
emergency response was validated by the improved swarm performance and
increased trust scores.
</p>
<a href="http://arxiv.org/abs/2006.15466" target="_blank">arXiv:2006.15466</a> [<a href="http://arxiv.org/pdf/2006.15466" target="_blank">pdf</a>]

<h2>Multi-level Cross-modal Interaction Network for RGB-D Salient Object Detection. (arXiv:2007.14352v2 [cs.CV] UPDATED)</h2>
<h3>Zhou Huang, Huai-Xin Chen, Tao Zhou, Yun-Zhi Yang, Bi-Yuan Liu</h3>
<p>Depth cues with affluent spatial information have been proven beneficial in
boosting salient object detection (SOD), while the depth quality directly
affects the subsequent SOD performance. However, it is inevitable to obtain
some low-quality depth cues due to limitations of its acquisition devices,
which can inhibit the SOD performance. Besides, existing methods tend to
combine RGB images and depth cues in a direct fusion or a simple fusion module,
which makes they can not effectively exploit the complex correlations between
the two sources. Moreover, few methods design an appropriate module to fully
fuse multi-level features, resulting in cross-level feature interaction
insufficient. To address these issues, we propose a novel Multi-level
Cross-modal Interaction Network (MCINet) for RGB-D based SOD. Our MCI-Net
includes two key components: 1) a cross-modal feature learning network, which
is used to learn the high-level features for the RGB images and depth cues,
effectively enabling the correlations between the two sources to be exploited;
and 2) a multi-level interactive integration network, which integrates
multi-level cross-modal features to boost the SOD performance. Extensive
experiments on six benchmark datasets demonstrate the superiority of our
MCI-Net over 14 state-of-the-art methods, and validate the effectiveness of
different components in our MCI-Net. More important, our MCI-Net significantly
improves the SOD performance as well as has a higher FPS.
</p>
<a href="http://arxiv.org/abs/2007.14352" target="_blank">arXiv:2007.14352</a> [<a href="http://arxiv.org/pdf/2007.14352" target="_blank">pdf</a>]

<h2>Experimental Verification of Stability Theory for a Planar Rigid Body with Two Unilateral Frictional Contacts. (arXiv:2008.10323v3 [cs.RO] UPDATED)</h2>
<h3>Yizhar Or, Peter L. Varkonyi</h3>
<p>Stability of equilibrium states in mechanical systems with multiple
unilateral frictional contacts is an important practical requirement, with high
relevance for robotic applications. In our previous work, we theoretically
analyzed finite-time Lyapunov stability for a minimal model of planar rigid
body with two frictional point contacts. Assuming inelastic impacts and Coulomb
friction, conditions for stability and instability of an equilibrium
configuration have been derived. In this work, we present for the first time an
experimental demonstration of this stability theory, using a variable-structure
rigid ''biped'' with frictional footpads on an inclined plane. By changing the
biped's center-of-mass location, we attain different equilibrium states, which
respond to small perturbations by divergence or convergence, showing remarkable
agreement with the predictions of the stability theory. Using high-speed
recording of video movies, good quantitative agreement between experiments and
numerical simulations is obtained, and limitations of the rigid-body model and
inelastic impact assumptions are also studied. The results prove the utility
and practical value of our stability theory.
</p>
<a href="http://arxiv.org/abs/2008.10323" target="_blank">arXiv:2008.10323</a> [<a href="http://arxiv.org/pdf/2008.10323" target="_blank">pdf</a>]

<h2>Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v3 [cs.RO] UPDATED)</h2>
<h3>Brandon Wagstaff, Jonathan Kelly</h3>
<p>The self-supervised loss formulation for jointly training depth and egomotion
neural networks with monocular images is well studied and has demonstrated
state-of-the-art accuracy. One of the main limitations of this approach,
however, is that the depth and egomotion estimates are only determined up to an
unknown scale. In this paper, we present a novel scale recovery loss that
enforces consistency between a known camera height and the estimated camera
height, generating metric (scaled) depth and egomotion predictions. We show
that our proposed method is competitive with other scale recovery techniques
that have more information available. Further, we demonstrate how our method
facilitates network retraining within new environments, whereas other
scale-resolving approaches are incapable of doing so. Notably, our egomotion
network is able to produce more accurate estimates than a similar method that
only recovers scale at test time.
</p>
<a href="http://arxiv.org/abs/2009.03787" target="_blank">arXiv:2009.03787</a> [<a href="http://arxiv.org/pdf/2009.03787" target="_blank">pdf</a>]

<h2>Semi-supervised Medical Image Segmentation through Dual-task Consistency. (arXiv:2009.04448v2 [cs.CV] UPDATED)</h2>
<h3>Xiangde Luo, Jieneng Chen, Tao Song, Guotai Wang</h3>
<p>Deep learning-based semi-supervised learning (SSL) algorithms have led to
promising results in medical images segmentation and can alleviate doctors'
expensive annotations by leveraging unlabeled data. However, most of the
existing SSL algorithms in literature tend to regularize the model training by
perturbing networks and/or data. Observing that multi/dual-task learning
attends to various levels of information which have inherent prediction
perturbation, we ask the question in this work: can we explicitly build
task-level regularization rather than implicitly constructing networks- and/or
data-level perturbation-and-transformation for SSL? To answer this question, we
propose a novel dual-task-consistency semi-supervised framework for the first
time. Concretely, we use a dual-task deep network that jointly predicts a
pixel-wise segmentation map and a geometry-aware level set representation of
the target. The level set representation is converted to an approximated
segmentation map through a differentiable task transform layer. Simultaneously,
we introduce a dual-task consistency regularization between the level
set-derived segmentation maps and directly predicted segmentation maps for both
labeled and unlabeled data. Extensive experiments on two public datasets show
that our method can largely improve the performance by incorporating the
unlabeled data. Meanwhile, our framework outperforms the state-of-the-art
semi-supervised medical image segmentation methods. Code is available at:
https://github.com/Luoxd1996/DTC
</p>
<a href="http://arxiv.org/abs/2009.04448" target="_blank">arXiv:2009.04448</a> [<a href="http://arxiv.org/pdf/2009.04448" target="_blank">pdf</a>]

<h2>RaLL: End-to-end Radar Localization on Lidar Map Using Differentiable Measurement Model. (arXiv:2009.07061v3 [cs.RO] UPDATED)</h2>
<h3>Huan Yin, Runjian Chen, Yue Wang, Rong Xiong</h3>
<p>Compared to the onboard camera and laser scanner, radar sensor provides
lighting and weather invariant sensing, which is naturally suitable for
long-term localization under adverse conditions. However, radar data is sparse
and noisy, resulting in challenges for radar mapping. On the other hand, the
most popular available map currently is built by lidar. In this paper, we
propose an end-to-end deep learning framework for Radar Localization on Lidar
Map (RaLL) to bridge the gap, which not only achieves the robust radar
localization but also exploits the mature lidar mapping technique, thus
reducing the cost of radar mapping. We first embed both sensor modals into a
common feature space by a neural network. Then multiple offsets are added to
the map modal for exhaustive similarity evaluation against the current radar
modal, yielding the regression of the current pose. Finally, we apply this
differentiable measurement model to a Kalman Filter (KF) to learn the whole
sequential localization process in an end-to-end manner. \textit{The whole
learning system is differentiable with the network based measurement model at
the front-end and KF at the back-end.} To validate the feasibility and
effectiveness, we employ multi-session multi-scene datasets collected from the
real world, and the results demonstrate that our proposed system achieves
superior performance over $90km$ driving, even in generalization scenarios
where the model training is in UK, while testing in South Korea. We also
release the source code publicly.
</p>
<a href="http://arxiv.org/abs/2009.07061" target="_blank">arXiv:2009.07061</a> [<a href="http://arxiv.org/pdf/2009.07061" target="_blank">pdf</a>]

<h2>MetaPhys: Few-Shot Adaptation for Non-Contact Physiological Measurement. (arXiv:2010.01773v3 [cs.CV] UPDATED)</h2>
<h3>Xin Liu, Ziheng Jiang, Josh Fromm, Xuhai Xu, Shwetak Patel, Daniel McDuff</h3>
<p>There are large individual differences in physiological processes, making
designing personalized health sensing algorithms challenging. Existing machine
learning systems struggle to generalize well to unseen subjects or contexts and
can often contain problematic biases. Video-based physiological measurement is
not an exception. Therefore, learning personalized or customized models from a
small number of unlabeled samples is very attractive as it would allow fast
calibrations to improve generalization and help correct biases. In this paper,
we present a novel meta-learning approach called MetaPhys for personalized
video-based cardiac measurement for contactless pulse and heart rate
monitoring. Our method uses only 18-seconds of video for customization and
works effectively in both supervised and unsupervised manners. We evaluate our
proposed approach on two benchmark datasets and demonstrate superior
performance in cross-dataset evaluation with substantial reductions (42% to
44%) in errors compared with state-of-the-art approaches. We have also
demonstrated our proposed method significantly helps reduce the bias in skin
type.
</p>
<a href="http://arxiv.org/abs/2010.01773" target="_blank">arXiv:2010.01773</a> [<a href="http://arxiv.org/pdf/2010.01773" target="_blank">pdf</a>]

<h2>Agile Robot Navigation through Hallucinated Learning and Sober Deployment. (arXiv:2010.08098v2 [cs.RO] UPDATED)</h2>
<h3>Xuesu Xiao, Bo Liu, Peter Stone</h3>
<p>Learning from Hallucination (LfH) is a recent machine learning paradigm for
autonomous navigation, which uses training data collected in completely safe
environments and adds numerous imaginary obstacles to make the environment
densely constrained, to learn navigation planners that produce feasible
navigation even in highly constrained (more dangerous) spaces. However, LfH
requires hallucinating the robot perception during deployment to match with the
hallucinated training data, which creates a need for sometimes-infeasible prior
knowledge and tends to generate very conservative planning. In this work, we
propose a new LfH paradigm that does not require runtime hallucination -- a
feature we call "sober deployment" -- and can therefore adapt to more realistic
navigation scenarios. This novel Hallucinated Learning and Sober Deployment
(HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation
environments with a wide range of difficulty levels, and in the real-world. In
most cases, HLSD outperforms both the original LfH method and a classical
navigation planner.
</p>
<a href="http://arxiv.org/abs/2010.08098" target="_blank">arXiv:2010.08098</a> [<a href="http://arxiv.org/pdf/2010.08098" target="_blank">pdf</a>]

<h2>What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions. (arXiv:2010.08539v2 [cs.CV] UPDATED)</h2>
<h3>Kiana Ehsani, Daniel Gordon, Thomas Nguyen, Roozbeh Mottaghi, Ali Farhadi</h3>
<p>Learning effective representations of visual data that generalize to a
variety of downstream tasks has been a long quest for computer vision. Most
representation learning approaches rely solely on visual data such as images or
videos. In this paper, we explore a novel approach, where we use human
interaction and attention cues to investigate whether we can learn better
representations compared to visual-only representations. For this study, we
collect a dataset of human interactions capturing body part movements and gaze
in their daily lives. Our experiments show that our "muscly-supervised"
representation that encodes interaction and attention cues outperforms a
visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of
target tasks: scene classification (semantic), action recognition (temporal),
depth estimation (geometric), dynamics prediction (physics) and walkable
surface estimation (affordance). Our code and dataset are available at:
https://github.com/ehsanik/muscleTorch.
</p>
<a href="http://arxiv.org/abs/2010.08539" target="_blank">arXiv:2010.08539</a> [<a href="http://arxiv.org/pdf/2010.08539" target="_blank">pdf</a>]

<h2>Efficient Generalized Spherical CNNs. (arXiv:2010.11661v3 [cs.CV] UPDATED)</h2>
<h3>Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A. Price, Mayeul d&#x27;Avezac, Jason D. McEwen</h3>
<p>Many problems across computer vision and the natural sciences require the
analysis of spherical data, for which representations may be learned
efficiently by encoding equivariance to rotational symmetries. We present a
generalized spherical CNN framework that encompasses various existing
approaches and allows them to be leveraged alongside each other. The only
existing non-linear spherical CNN layer that is strictly equivariant has
complexity $\mathcal{O}(C^2L^5)$, where $C$ is a measure of representational
capacity and $L$ the spherical harmonic bandlimit. Such a high computational
cost often prohibits the use of strictly equivariant spherical CNNs. We develop
two new strictly equivariant layers with reduced complexity $\mathcal{O}(CL^4)$
and $\mathcal{O}(CL^3 \log L)$, making larger, more expressive models
computationally feasible. Moreover, we adopt efficient sampling theory to
achieve further computational savings. We show that these developments allow
the construction of more expressive hybrid models that achieve state-of-the-art
accuracy and parameter efficiency on spherical benchmark problems.
</p>
<a href="http://arxiv.org/abs/2010.11661" target="_blank">arXiv:2010.11661</a> [<a href="http://arxiv.org/pdf/2010.11661" target="_blank">pdf</a>]

<h2>PILOT: Efficient Planning by Imitation Learning and Optimisation for Safe Autonomous Driving. (arXiv:2011.00509v2 [cs.RO] UPDATED)</h2>
<h3>Henry Pulver, Francisco Eiras, Ludovico Carozza, Majd Hawasly, Stefano Albrecht, Subramanian Ramamoorthy</h3>
<p>Achieving the right balance between planning quality, safety and efficiency
is a major challenge for autonomous driving. Optimisation-based motion planners
are capable of producing safe, smooth and comfortable plans, but often at the
cost of runtime efficiency. On the other hand, naively deploying trajectories
produced by efficient-to-run deep imitation learning approaches might risk
compromising safety. In this paper, we present PILOT -- a planning framework
that comprises an imitation neural network followed by an efficient optimiser
that actively rectifies the network's plan, guaranteeing fulfilment of safety
and comfort requirements. The objective of the efficient optimiser is the same
as the objective of an expensive-to-run optimisation-based planning system that
the neural network is trained offline to imitate. This efficient optimiser
provides a key layer of online protection from learning failures or deficiency
on out-of-distribution situations that might compromise safety or comfort.
Using a state-of-the-art, runtime-intensive optimisation-based method as the
expert, we demonstrate in simulated autonomous driving experiments in CARLA
that PILOT achieves a significant reduction in runtime when compared to the
expert it imitates without sacrificing planning quality.
</p>
<a href="http://arxiv.org/abs/2011.00509" target="_blank">arXiv:2011.00509</a> [<a href="http://arxiv.org/pdf/2011.00509" target="_blank">pdf</a>]

<h2>Towards Personalized Explanation of Robot Path Planning via User Feedback. (arXiv:2011.00524v2 [cs.RO] UPDATED)</h2>
<h3>Kayla Boggess, Shenghui Chen, Lu Feng</h3>
<p>Prior studies have found that explaining robot decisions and actions helps to
increase system transparency, improve user understanding, and enable effective
human-robot collaboration. In this paper, we present a system for generating
personalized explanations of robot path planning via user feedback. We consider
a robot navigating in an environment modeled as a Markov decision process
(MDP), and develop an algorithm to automatically generate a personalized
explanation of an optimal MDP policy, based on the user preference regarding
four elements (i.e., objective, locality, specificity, and corpus). In
addition, we design the system to interact with users via answering users'
further questions about the generated explanations. Users have the option to
update their preferences to view different explanations. The system is capable
of detecting and resolving any preference conflict via user interaction. The
results of an online user study show that the generated personalized
explanations improve user satisfaction, while the majority of users liked the
system's capabilities of question-answering and conflict detection/resolution.
</p>
<a href="http://arxiv.org/abs/2011.00524" target="_blank">arXiv:2011.00524</a> [<a href="http://arxiv.org/pdf/2011.00524" target="_blank">pdf</a>]

<h2>Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs. (arXiv:2011.00844v3 [cs.CV] UPDATED)</h2>
<h3>Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo</h3>
<p>Natural images are projections of 3D objects on a 2D image plane. While
state-of-the-art 2D generative models like GANs show unprecedented quality in
modeling the natural image manifold, it is unclear whether they implicitly
capture the underlying 3D object structures. And if so, how could we exploit
such knowledge to recover the 3D shapes of objects in the images? To answer
these questions, in this work, we present the first attempt to directly mine 3D
geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only.
Through our investigation, we found that such a pre-trained GAN indeed contains
rich 3D knowledge and thus can be used to recover 3D shape from a single 2D
image in an unsupervised manner. The core of our framework is an iterative
strategy that explores and exploits diverse viewpoint and lighting variations
in the GAN image manifold. The framework does not require 2D keypoint or 3D
annotations, or strong assumptions on object shapes (e.g. shapes are
symmetric), yet it successfully recovers 3D shapes with high precision for
human faces, cats, cars, and buildings. The recovered 3D shapes immediately
allow high-quality image editing like relighting and object rotation. We
quantitatively demonstrate the effectiveness of our approach compared to
previous methods in both 3D shape reconstruction and face rotation. Our code is
available at https://github.com/XingangPan/GAN2Shape.
</p>
<a href="http://arxiv.org/abs/2011.00844" target="_blank">arXiv:2011.00844</a> [<a href="http://arxiv.org/pdf/2011.00844" target="_blank">pdf</a>]

<h2>Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints. (arXiv:2011.01354v2 [cs.CV] UPDATED)</h2>
<h3>Kenny Chen, Alexandra Pogue, Brett T. Lopez, Ali-akbar Agha-mohammadi, Ankur Mehta</h3>
<p>Monocular depth inference has gained tremendous attention from researchers in
recent years and remains as a promising replacement for expensive
time-of-flight sensors, but issues with scale acquisition and implementation
overhead still plague these systems. To this end, this work presents an
unsupervised learning framework that is able to predict at-scale depth maps and
egomotion, in addition to camera intrinsics, from a sequence of monocular
images via a single network. Our method incorporates both spatial and temporal
geometric constraints to resolve depth and pose scale factors, which are
enforced within the supervisory reconstruction loss functions at training time.
Only unlabeled stereo sequences are required for training the weights of our
single-network architecture, which reduces overall implementation overhead as
compared to previous methods. Our results demonstrate strong performance when
compared to the current state-of-the-art on multiple sequences of the KITTI
driving dataset and can provide faster training times with its reduced network
complexity.
</p>
<a href="http://arxiv.org/abs/2011.01354" target="_blank">arXiv:2011.01354</a> [<a href="http://arxiv.org/pdf/2011.01354" target="_blank">pdf</a>]

<h2>Risk-Averse Planning via CVaR Barrier Functions: Application to Bipedal Robot Locomotion. (arXiv:2011.01578v2 [cs.RO] UPDATED)</h2>
<h3>Mohamadreza Ahmadi, Xiaobin Xiong, Aaron D. Ames</h3>
<p>Enforcing safety in the presence of stochastic uncertainty is a challenging
problem. Traditionally, researchers have proposed safety in the statistical
mean as a safety measure in this case. However, ensuring safety in the
statistical mean is only reasonable if system's safe behavior in the large
number of runs is of interest, which precludes the use of mean safety in
practical scenarios. In this paper, we propose a risk sensitive notion of
safety called conditional-value-at-risk (CVaR) safety, which is concerned with
safe performance in the worst case realizations. We introduce CVaR barrier
functions as a tool to enforce CVaR-safety and propose conditions for their
Boolean compositions. Given a legacy controller, we show that we can design a
minimally interfering CVaR-safe controller via solving difference convex
programs. We elucidate the proposed method by applying it to a bipedal robot
locomotion case study.
</p>
<a href="http://arxiv.org/abs/2011.01578" target="_blank">arXiv:2011.01578</a> [<a href="http://arxiv.org/pdf/2011.01578" target="_blank">pdf</a>]

<h2>FDRN: A Fast Deformable Registration Network for Medical Images. (arXiv:2011.02307v2 [cs.CV] UPDATED)</h2>
<h3>Kaicong Sun, Sven Simon</h3>
<p>Deformable image registration is a fundamental task in medical imaging. Due
to the large computational complexity of deformable registration of volumetric
images, conventional iterative methods usually face the tradeoff between the
registration accuracy and the computation time in practice. In order to boost
the registration performance in both accuracy and runtime, we propose a fast
convolutional neural network. Specially, to efficiently utilize the memory
resources and enlarge the model capacity, we adopt additive forwarding instead
of channel concatenation and deepen the network in each encoder and decoder
stage. To facilitate the learning efficiency, we leverage skip connection
within the encoder and decoder stages to enable residual learning and employ an
auxiliary loss at the bottom layer with lowest resolution to involve deep
supervision. Particularly, the low-resolution auxiliary loss is weighted by an
exponentially decayed parameter during the training phase. In conjunction with
the main loss in high-resolution grid, we achieve a coarse-to-fine learning
strategy. Last but not least, we involve a proposed multi-label segmentation
loss to improve the network performance in Dice score. Comparing to average
Dice score, the proposed segmentation loss does not require additional memory
in the training phase and improves the registration accuracy efficiently. In
the experiments, we show FDRN performs better than the existing
state-of-the-art registration methods for brain MR images by resorting to the
compact autoencoder structure and efficient learning. Besides, FDRN is a
generalized framework for image registration which is not confined to a
particular type of medical images or anatomy.
</p>
<a href="http://arxiv.org/abs/2011.02307" target="_blank">arXiv:2011.02307</a> [<a href="http://arxiv.org/pdf/2011.02307" target="_blank">pdf</a>]

<h2>TTVOS: Lightweight Video Object Segmentation with Adaptive Template Attention Module and Temporal Consistency Loss. (arXiv:2011.04445v2 [cs.CV] UPDATED)</h2>
<h3>Hyojin Park, Ganesh Venkatesh, Nojun Kwak</h3>
<p>Semi-supervised video object segmentation (semi-VOS) is widely used in many
applications. This task is tracking class-agnostic objects from a given target
mask. For doing this, various approaches have been developed based on
online-learning, memory networks, and optical flow. These methods show high
accuracy but are hard to be utilized in real-world applications due to slow
inference time and tremendous complexity. To resolve this problem, template
matching methods are devised for fast processing speed but sacrificing lots of
performance in previous models. We introduce a novel semi-VOS model based on a
template matching method and a temporal consistency loss to reduce the
performance gap from heavy models while expediting inference time a lot. Our
template matching method consists of short-term and long-term matching. The
short-term matching enhances target object localization, while long-term
matching improves fine details and handles object shape-changing through the
newly proposed adaptive template attention module. However, the long-term
matching causes error-propagation due to the inflow of the past estimated
results when updating the template. To mitigate this problem, we also propose a
temporal consistency loss for better temporal coherence between neighboring
frames by adopting the concept of a transition matrix. Our model obtains 79.5%
J&amp;F score at the speed of 73.8 FPS on the DAVIS16 benchmark.
</p>
<a href="http://arxiv.org/abs/2011.04445" target="_blank">arXiv:2011.04445</a> [<a href="http://arxiv.org/pdf/2011.04445" target="_blank">pdf</a>]

<h2>A Fast and Reliable Pick-and-Place Application with a Spherical Soft Robotic Arm. (arXiv:2011.04624v2 [cs.RO] UPDATED)</h2>
<h3>Jasan Zughaibi, Matthias Hofer, Raffaello D&#x27;Andrea</h3>
<p>This paper presents the application of a learning control approach for the
realization of a fast and reliable pick-and-place application with a spherical
soft robotic arm. The arm is characterized by a lightweight design and exhibits
compliant behavior due to the soft materials deployed. A soft, continuum joint
is employed, which allows for simultaneous control of one translational and two
rotational degrees of freedom in a single joint. This allows us to axially
approach and pick an object with the attached suction cup during the
pick-and-place application. A control allocation based on pressure differences
and the antagonistic actuator configuration is introduced, allowing decoupling
of the system dynamics and simplifying the modeling and control. A linear
parameter-varying model is identified, which is parametrized by the attached
load mass and a parameter related to the joint stiffness. A gain-scheduled
feedback controller is proposed, which asymptotically stabilizes the robotic
system for aggressive tuning and over large variations of the parameters
considered. The control architecture is augmented with an iterative learning
control scheme enabling accurate tracking of aggressive trajectories involving
set point transitions of 60 degrees within 0.3 seconds (no mass attached) to
0.6 seconds (load mass attached). The modeling and control approach proposed
results in a reliable realization of a pick-and-place application and is
experimentally demonstrated.
</p>
<a href="http://arxiv.org/abs/2011.04624" target="_blank">arXiv:2011.04624</a> [<a href="http://arxiv.org/pdf/2011.04624" target="_blank">pdf</a>]

<h2>RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v3 [cs.CV] UPDATED)</h2>
<h3>Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou</h3>
<p>We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video
Frame Interpolation (VFI). Most existing flow-based methods first estimate the
bi-directional optical flows, then scale and reverse them to approximate
intermediate flows, leading to artifacts on motion boundaries. RIFE uses a
neural network named IFNet that can directly estimate the intermediate flows
from images with much better speed. Based on our proposed leakage distillation
loss, RIFE can be trained in an end-to-end fashion. Experiments demonstrate
that our method is flexible and can achieve impressive performance on several
public benchmarks. The code is available at
https://github.com/hzwer/arXiv2020-RIFE.
</p>
<a href="http://arxiv.org/abs/2011.06294" target="_blank">arXiv:2011.06294</a> [<a href="http://arxiv.org/pdf/2011.06294" target="_blank">pdf</a>]

<h2>DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic Scenarios with Graph Neural Networks. (arXiv:2011.06775v2 [cs.RO] UPDATED)</h2>
<h3>Peide Cai, Hengli Wang, Yuxiang Sun, Ming Liu</h3>
<p>Traditional modular self-driving frameworks scale poorly in new scenarios,
which usually require tedious hand-tuning of rules and parameters to maintain
acceptable performance in all foreseeable occasions. Therefore, robust and safe
self-driving using traditional frameworks is still challenging, especially in
complex and dynamic environments. Recently, deep-learning based self-driving
methods have shown promising results with better generalization capability but
less hand engineering effort. However, most of the previous learning-based
methods are trained and evaluated in limited driving scenarios with scattered
tasks, such as lane-following, autonomous braking, and conditional driving. In
this paper, we propose a graph-based deep network to achieve scalable
self-driving that can handle massive traffic scenarios. Specifically, more than
7,000 km of evaluation is conducted in a high-fidelity driving simulator, in
which our method can obey the traffic rules and safely navigate the vehicle in
a large variety of urban, rural, and highway environments, including
unprotected left turns, narrow roads, roundabouts, and pedestrian-rich
intersections. The results also show that our method achieves better
performance over the baselines in terms of success rate. This work is
accompanied with some demonstration videos which are available at
https://sites.google.com/view/dignet-self-driving/video-clips/
</p>
<a href="http://arxiv.org/abs/2011.06775" target="_blank">arXiv:2011.06775</a> [<a href="http://arxiv.org/pdf/2011.06775" target="_blank">pdf</a>]

<h2>SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation. (arXiv:2011.07215v2 [cs.RO] UPDATED)</h2>
<h3>Xingyu Lin, Yufei Wang, Jake Olkin, David Held</h3>
<p>Manipulating deformable objects has long been a challenge in robotics due to
its high dimensional state representation and complex dynamics. Recent success
in deep reinforcement learning provides a promising direction for learning to
manipulate deformable objects with data driven methods. However, existing
reinforcement learning benchmarks only cover tasks with direct state
observability and simple low-dimensional dynamics or with relatively simple
image-based environments, such as those with rigid objects. In this paper, we
present SoftGym, a set of open-source simulated benchmarks for manipulating
deformable objects, with a standard OpenAI Gym API and a Python interface for
creating new environments. Our benchmark will enable reproducible research in
this important area. Further, we evaluate a variety of algorithms on these
tasks and highlight challenges for reinforcement learning algorithms, including
dealing with a state representation that has a high intrinsic dimensionality
and is partially observable. The experiments and analysis indicate the
strengths and limitations of existing methods in the context of deformable
object manipulation that can help point the way forward for future methods
development. Code and videos of the learned policies can be found on our
project website.
</p>
<a href="http://arxiv.org/abs/2011.07215" target="_blank">arXiv:2011.07215</a> [<a href="http://arxiv.org/pdf/2011.07215" target="_blank">pdf</a>]

<h2>Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v2 [cs.CV] UPDATED)</h2>
<h3>Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, Mubarak Shah</h3>
<p>Anomaly detection in video is a challenging computer vision problem. Due to
the lack of anomalous events at training time, anomaly detection requires the
design of learning methods without full supervision. In this paper, we approach
anomalous event detection in video through self-supervised and multi-task
learning at the object level. We first utilize a pre-trained detector to detect
objects. Then, we train a 3D convolutional neural network to produce
discriminative anomaly-specific information by jointly learning multiple proxy
tasks: three self-supervised and one based on knowledge distillation. The
self-supervised tasks are: (i) discrimination of forward/backward moving
objects (arrow of time), (ii) discrimination of objects in
consecutive/intermittent frames (motion irregularity) and (iii) reconstruction
of object-specific appearance information. The knowledge distillation task
takes into account both classification and detection information, generating
large prediction discrepancies between teacher and student models when
anomalies occur. To the best of our knowledge, we are the first to approach
anomalous event detection in video as a multi-task learning problem,
integrating multiple self-supervised and knowledge distillation proxy tasks in
a single architecture. Our lightweight architecture outperforms the
state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD
Ped2. Additionally, we perform an ablation study demonstrating the importance
of integrating self-supervised learning and normality-specific distillation in
a multi-task learning setting.
</p>
<a href="http://arxiv.org/abs/2011.07491" target="_blank">arXiv:2011.07491</a> [<a href="http://arxiv.org/pdf/2011.07491" target="_blank">pdf</a>]

<h2>Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph Learning for Collaborative Object Localization. (arXiv:2011.07704v2 [cs.CV] UPDATED)</h2>
<h3>Peng Gao, Rui Guo, Hongsheng Lu, Hao Zhang</h3>
<p>Collaborative object localization aims to collaboratively estimate locations
of objects observed from multiple views or perspectives, which is a critical
ability for multi-agent systems such as connected vehicles. To enable
collaborative localization, several model-based state estimation and
learning-based localization methods have been developed. Given their
encouraging performance, model-based state estimation often lacks the ability
to model the complex relationships among multiple objects, while learning-based
methods are typically not able to fuse the observations from an arbitrary
number of views and cannot well model uncertainty. In this paper, we introduce
a novel spatiotemporal graph filter approach that integrates graph learning and
model-based estimation to perform multi-view sensor fusion for collaborative
object localization. Our approach models complex object relationships using a
new spatiotemporal graph representation and fuses multi-view observations in a
Bayesian fashion to improve location estimation under uncertainty. We evaluate
our approach in the applications of connected autonomous driving and multiple
pedestrian localization. Experimental results show that our approach
outperforms previous techniques and achieves the state-of-the-art performance
on collaboration localization.
</p>
<a href="http://arxiv.org/abs/2011.07704" target="_blank">arXiv:2011.07704</a> [<a href="http://arxiv.org/pdf/2011.07704" target="_blank">pdf</a>]

<h2>Co-Design of Embodied Intelligence: A Structured Approach. (arXiv:2011.10756v3 [cs.RO] UPDATED)</h2>
<h3>Gioele Zardini, Dejan Milojevic, Andrea Censi, Emilio Frazzoli</h3>
<p>We consider the problem of co-designing embodied intelligence as a whole in a
structured way, from hardware components such as propulsion systems and sensors
to software modules such as control and perception pipelines. We propose a
principled approach to formulate and solve complex embodied intelligence
co-design problems, leveraging a monotone co-design theory. The methods we
propose are intuitive and integrate heterogeneous engineering disciplines,
allowing analytical and simulation-based modeling techniques and enabling
interdisciplinarity. We illustrate through a case study how, given a set of
desired behaviors, our framework is able to compute Pareto efficient solutions
for the entire hardware and software stack of a self-driving vehicle.
</p>
<a href="http://arxiv.org/abs/2011.10756" target="_blank">arXiv:2011.10756</a> [<a href="http://arxiv.org/pdf/2011.10756" target="_blank">pdf</a>]

<h2>Unsupervised Object Keypoint Learning using Local Spatial Predictability. (arXiv:2011.12930v2 [cs.CV] UPDATED)</h2>
<h3>Anand Gopalakrishnan, Sjoerd van Steenkiste, J&#xfc;rgen Schmidhuber</h3>
<p>We propose PermaKey, a novel approach to representation learning based on
object keypoints. It leverages the predictability of local image regions from
spatial neighborhoods to identify salient regions that correspond to object
parts, which are then converted to keypoints. Unlike prior approaches, it
utilizes predictability to discover object keypoints, an intrinsic property of
objects. This ensures that it does not overly bias keypoints to focus on
characteristics that are not unique to objects, such as movement, shape, colour
etc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints
corresponding to the most salient object parts and is robust to certain visual
distractors. Further, on downstream RL tasks in the Atari domain we demonstrate
how agents equipped with our keypoints outperform those using competing
alternatives, even on challenging environments with moving backgrounds or
distractor objects.
</p>
<a href="http://arxiv.org/abs/2011.12930" target="_blank">arXiv:2011.12930</a> [<a href="http://arxiv.org/pdf/2011.12930" target="_blank">pdf</a>]

<h2>DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution. (arXiv:2011.13328v2 [cs.CV] UPDATED)</h2>
<h3>Tong He, Chunhua Shen, Anton van den Hengel</h3>
<p>Previous top-performing approaches for point cloud instance segmentation
involve a bottom-up strategy, which often includes inefficient operations or
complex pipelines, such as grouping over-segmented components, introducing
additional steps for refining, or designing complicated loss functions. The
inevitable variation in the instance scales can lead bottom-up methods to
become particularly sensitive to hyper-parameter values. To this end, we
propose instead a dynamic, proposal-free, data-driven approach that generates
the appropriate convolution kernels to apply in response to the nature of the
instances. To make the kernels discriminative, we explore a large context by
gathering homogeneous points that share identical semantic categories and have
close votes for the geometric centroids. Instances are then decoded by several
simple convolutional layers. Due to the limited receptive field introduced by
the sparse convolution, a small light-weight transformer is also devised to
capture the long-range dependencies and high-level interactions among point
samples. The proposed method achieves promising results on both ScanetNetV2 and
S3DIS, and this performance is robust to the particular hyper-parameter values
chosen. It also improves inference speed by more than 25% over the current
state-of-the-art. Code is available at: https://git.io/DyCo3D
</p>
<a href="http://arxiv.org/abs/2011.13328" target="_blank">arXiv:2011.13328</a> [<a href="http://arxiv.org/pdf/2011.13328" target="_blank">pdf</a>]

<h2>Continuous Transition: Improving Sample Efficiency for Continuous Control Problems via MixUp. (arXiv:2011.14487v2 [cs.RO] UPDATED)</h2>
<h3>Junfan Lin, Zhongzhan Huang, Keze Wang, Xiaodan Liang, Weiwei Chen, Liang Lin</h3>
<p>Although deep reinforcement learning (RL) has been successfully applied to a
variety of robotic control tasks, it's still challenging to apply it to
real-world tasks, due to the poor sample efficiency. Attempting to overcome
this shortcoming, several works focus on reusing the collected trajectory data
during the training by decomposing them into a set of policy-irrelevant
discrete transitions. However, their improvements are somewhat marginal since
i) the amount of the transitions is usually small, and ii) the value assignment
only happens in the joint states. To address these issues, this paper
introduces a concise yet powerful method to construct Continuous Transition,
which exploits the trajectory information by exploiting the potential
transitions along the trajectory. Specifically, we propose to synthesize new
transitions for training by linearly interpolating the consecutive transitions.
To keep the constructed transitions authentic, we also develop a discriminator
to guide the construction process automatically. Extensive experiments
demonstrate that our proposed method achieves a significant improvement in
sample efficiency on various complex continuous robotic control problems in
MuJoCo and outperforms the advanced model-based / model-free RL methods. The
source code is available.
</p>
<a href="http://arxiv.org/abs/2011.14487" target="_blank">arXiv:2011.14487</a> [<a href="http://arxiv.org/pdf/2011.14487" target="_blank">pdf</a>]

<h2>HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation. (arXiv:2011.14672v2 [cs.CV] UPDATED)</h2>
<h3>Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, Cewu Lu</h3>
<p>Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh
for the human body by estimating several parameters. However, learning the
abstract parameters is a highly non-linear process and suffers from image-model
misalignment, leading to mediocre model performance. In contrast, 3D keypoint
estimation methods combine deep CNN network with the volumetric representation
to achieve pixel-level localization accuracy but may predict unrealistic body
structure. In this paper, we address the above issues by bridging the gap
between body mesh estimation and 3D keypoint estimation. We propose a novel
hybrid inverse kinematics solution (HybrIK). HybrIK directly transforms
accurate 3D joints to relative body-part rotations for 3D body mesh
reconstruction, via the twist-and-swing decomposition. The swing rotation is
analytically solved with 3D joints, and the twist rotation is derived from the
visual cues through the neural network. We show that HybrIK preserves both the
accuracy of 3D pose and the realistic body structure of the parametric human
model, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than
the pure 3D keypoint estimation methods. Without bells and whistles, the
proposed method surpasses the state-of-the-art methods by a large margin on
various 3D human pose and shape benchmarks. As an illustrative example, HybrIK
outperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW
dataset. Our code is available at https://github.com/Jeff-sjtu/HybrIK.
</p>
<a href="http://arxiv.org/abs/2011.14672" target="_blank">arXiv:2011.14672</a> [<a href="http://arxiv.org/pdf/2011.14672" target="_blank">pdf</a>]

<h2>Unpaired Image-to-Image Translation via Latent Energy Transport. (arXiv:2012.00649v2 [cs.CV] UPDATED)</h2>
<h3>Yang Zhao, Changyou Chen</h3>
<p>Image-to-image translation aims to preserve source contents while translating
to discriminative target styles between two visual domains. Most works apply
adversarial learning in the ambient image space, which could be computationally
expensive and challenging to train. In this paper, we propose to deploy an
energy-based model (EBM) in the latent space of a pretrained autoencoder for
this task. The pretrained autoencoder serves as both a latent code extractor
and an image reconstruction worker. Our model is based on the assumption that
two domains share the same latent space, where latent representation is
implicitly decomposed as a content code and a domain-specific style code.
Instead of explicitly extracting the two codes and applying adaptive instance
normalization to combine them, our latent EBM can implicitly learn to transport
the source style code to the target style code while preserving the content
code, which is an advantage over existing image translation methods. This
simplified solution also brings us far more efficiency in the one-sided
unpaired image translation setting. Qualitative and quantitative comparisons
demonstrate superior translation quality and faithfulness for content
preservation. To the best of our knowledge, our model is the first to be
applicable to 1024$\times$1024-resolution unpaired image translation.
</p>
<a href="http://arxiv.org/abs/2012.00649" target="_blank">arXiv:2012.00649</a> [<a href="http://arxiv.org/pdf/2012.00649" target="_blank">pdf</a>]

<h2>Interpretable Graph Capsule Networks for Object Recognition. (arXiv:2012.01674v3 [cs.CV] UPDATED)</h2>
<h3>Jindong Gu, Volker Tresp</h3>
<p>Capsule Networks, as alternatives to Convolutional Neural Networks, have been
proposed to recognize objects from images. The current literature demonstrates
many advantages of CapsNets over CNNs. However, how to create explanations for
individual classifications of CapsNets has not been well explored. The widely
used saliency methods are mainly proposed for explaining CNN-based
classifications; they create saliency map explanations by combining activation
values and the corresponding gradients, e.g., Grad-CAM. These saliency methods
require a specific architecture of the underlying classifiers and cannot be
trivially applied to CapsNets due to the iterative routing mechanism therein.
To overcome the lack of interpretability, we can either propose new post-hoc
interpretation methods for CapsNets or modifying the model to have build-in
explanations. In this work, we explore the latter. Specifically, we propose
interpretable Graph Capsule Networks (GraCapsNets), where we replace the
routing part with a multi-head attention-based Graph Pooling approach. In the
proposed model, individual classification explanations can be created
effectively and efficiently. Our model also demonstrates some unexpected
benefits, even though it replaces the fundamental part of CapsNets. Our
GraCapsNets achieve better classification performance with fewer parameters and
better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets
also keep other advantages of CapsNets, namely, disentangled representations
and affine transformation robustness.
</p>
<a href="http://arxiv.org/abs/2012.01674" target="_blank">arXiv:2012.01674</a> [<a href="http://arxiv.org/pdf/2012.01674" target="_blank">pdf</a>]

<h2>Object-Driven Active Mapping for More Accurate Object Pose Estimation and Robotic Grasping. (arXiv:2012.01788v2 [cs.RO] UPDATED)</h2>
<h3>Yanmin Wu, Yunzhou Zhang, Delong Zhu, Xin Chen, Sonya Coleman, Wenkai Sun, Xinggang Hu, Zhiqiang Deng</h3>
<p>This paper presents the first active object mapping framework for complex
robotic grasping tasks. The framework is built on an object SLAM system
integrated with a simultaneous multi-object pose estimation process. Aiming to
reduce the observation uncertainty on target objects and increase their pose
estimation accuracy, we also design an object-driven exploration strategy to
guide the object mapping process. By combining the mapping module and the
exploration strategy, an accurate object map that is compatible with robotic
grasping can be generated. Quantitative evaluations also show that the proposed
framework has a very high mapping accuracy. Manipulation experiments, including
object grasping, object placement, and the augmented reality, significantly
demonstrate the effectiveness and advantages of our proposed framework.
</p>
<a href="http://arxiv.org/abs/2012.01788" target="_blank">arXiv:2012.01788</a> [<a href="http://arxiv.org/pdf/2012.01788" target="_blank">pdf</a>]

<h2>Human Tactile Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v2 [cs.RO] UPDATED)</h2>
<h3>Elizabeth Bibit Bianchini, Prateek Verma, Kenneth Salisbury</h3>
<p>Physical human-robot interactions (pHRI) are less efficient and communicative
than human-human interactions, and a key reason is a lack of informative sense
of touch in robotic systems. Interpreting human touch gestures is a nuanced,
challenging task with extreme gaps between human and robot capability. Among
prior works that demonstrate human touch recognition capability, differences in
sensors, gesture classes, feature sets, and classification algorithms yield a
conglomerate of non-transferable results and a glaring lack of a standard. To
address this gap, this work presents 1) four proposed touch gesture classes
that cover the majority of the gesture characteristics identified in the
literature, 2) the collection of an extensive force dataset on a common pHRI
robotic arm with only its internal wrist force-torque sensor, and 3) an
exhaustive performance comparison of combinations of feature sets and
classification algorithms on this dataset. We demonstrate high classification
accuracies among our proposed gesture definitions on a test set, emphasizing
that neural network classifiers on the raw data outperform other combinations
of feature sets and algorithms.
</p>
<a href="http://arxiv.org/abs/2012.01959" target="_blank">arXiv:2012.01959</a> [<a href="http://arxiv.org/pdf/2012.01959" target="_blank">pdf</a>]

<h2>PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths. (arXiv:2012.03408v2 [cs.CV] UPDATED)</h2>
<h3>Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, Yu-Shen Liu</h3>
<p>The task of point cloud completion aims to predict the missing part for an
incomplete 3D shape. A widely used strategy is to generate a complete point
cloud from the incomplete one. However, the unordered nature of point clouds
will degrade the generation of high-quality 3D shapes, as the detailed topology
and structure of discrete points are hard to be captured by the generative
process only using a latent code. In this paper, we address the above problem
by reconsidering the completion task from a new perspective, where we formulate
the prediction as a point cloud deformation process. Specifically, we design a
novel neural network, named PMP-Net, to mimic the behavior of an earth mover.
It moves move each point of the incomplete input to complete the point cloud,
where the total distance of point moving paths (PMP) should be shortest.
Therefore, PMP-Net predicts a unique point moving path for each point according
to the constraint of total point moving distances. As a result, the network
learns a strict and unique correspondence on point-level, which can capture the
detailed topology and structure relationships between the incomplete shape and
the complete target, and thus improves the quality of the predicted complete
shape. We conduct comprehensive experiments on Completion3D and PCN datasets,
which demonstrate our advantages over the state-of-the-art point cloud
completion methods.
</p>
<a href="http://arxiv.org/abs/2012.03408" target="_blank">arXiv:2012.03408</a> [<a href="http://arxiv.org/pdf/2012.03408" target="_blank">pdf</a>]

<h2>Video Deblurring by Fitting to Test Data. (arXiv:2012.05228v2 [cs.CV] UPDATED)</h2>
<h3>Xuanchi Ren, Zian Qian, Qifeng Chen</h3>
<p>Motion blur in videos captured by autonomous vehicles and robots can degrade
their perception capability. In this work, we present a novel approach to video
deblurring by fitting a deep network to the test video. Our key observation is
that some frames in a video with motion blur are much sharper than others, and
thus we can transfer the texture information in those sharp frames to blurry
frames. Our approach heuristically selects sharp frames from a video and then
trains a convolutional neural network on these sharp frames. The trained
network often absorbs enough details in the scene to perform deblurring on all
the video frames. As an internal learning method, our approach has no domain
gap between training and test data, which is a problematic issue for existing
video deblurring approaches. The conducted experiments on real-world video data
show that our model can reconstruct clearer and sharper videos than
state-of-the-art video deblurring approaches. Code and data are available at
https://github.com/xrenaa/Deblur-by-Fitting.
</p>
<a href="http://arxiv.org/abs/2012.05228" target="_blank">arXiv:2012.05228</a> [<a href="http://arxiv.org/pdf/2012.05228" target="_blank">pdf</a>]

<h2>Improved Image Matting via Real-time User Clicks and Uncertainty Estimation. (arXiv:2012.08323v2 [cs.CV] UPDATED)</h2>
<h3>Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Hanqing Zhao, Weiming Zhang, Nenghai Yu</h3>
<p>Image matting is a fundamental and challenging problem in computer vision and
graphics. Most existing matting methods leverage a user-supplied trimap as an
auxiliary input to produce good alpha matte. However, obtaining high-quality
trimap itself is arduous, thus restricting the application of these methods.
Recently, some trimap-free methods have emerged, however, the matting quality
is still far behind the trimap-based methods. The main reason is that, without
the trimap guidance in some cases, the target network is ambiguous about which
is the foreground target. In fact, choosing the foreground is a subjective
procedure and depends on the user's intention. To this end, this paper proposes
an improved deep image matting framework which is trimap-free and only needs
several user click interactions to eliminate the ambiguity. Moreover, we
introduce a new uncertainty estimation module that can predict which parts need
polishing and a following local refinement module. Based on the computation
budget, users can choose how many local parts to improve with the uncertainty
guidance. Quantitative and qualitative results show that our method performs
better than existing trimap-free methods and comparably to state-of-the-art
trimap-based methods with minimal user effort.
</p>
<a href="http://arxiv.org/abs/2012.08323" target="_blank">arXiv:2012.08323</a> [<a href="http://arxiv.org/pdf/2012.08323" target="_blank">pdf</a>]

<h2>Simultaneous View and Feature Selection for Collaborative Multi-Robot Perception. (arXiv:2012.09328v2 [cs.RO] UPDATED)</h2>
<h3>Brian Reily, Hao Zhang</h3>
<p>Collaborative multi-robot perception provides multiple views of an
environment, offering varying perspectives to collaboratively understand the
environment even when individual robots have poor points of view or when
occlusions are caused by obstacles. These multiple observations must be
intelligently fused for accurate recognition, and relevant observations need to
be selected in order to allow unnecessary robots to continue on to observe
other targets. This research problem has not been well studied in the
literature yet. In this paper, we propose a novel approach to collaborative
multi-robot perception that simultaneously integrates view selection, feature
selection, and object recognition into a unified regularized optimization
formulation, which uses sparsity-inducing norms to identify the robots with the
most representative views and the modalities with the most discriminative
features. As our optimization formulation is hard to solve due to the
introduced non-smooth norms, we implement a new iterative optimization
algorithm, which is guaranteed to converge to the optimal solution. We evaluate
our approach through a case-study in simulation and on a physical multi-robot
system. Experimental results demonstrate that our approach enables effective
collaborative perception through accurate object recognition and effective view
and feature selection.
</p>
<a href="http://arxiv.org/abs/2012.09328" target="_blank">arXiv:2012.09328</a> [<a href="http://arxiv.org/pdf/2012.09328" target="_blank">pdf</a>]

<h2>Game Theoretic Decentralized and Communication-Free Multi-Robot Navigation. (arXiv:2012.09335v2 [cs.RO] UPDATED)</h2>
<h3>Brian Reily, Terran Mott, Hao Zhang</h3>
<p>Effective multi-robot teams require the ability to move to goals in complex
environments in order to address real-world applications such as search and
rescue. Multi-robot teams should be able to operate in a completely
decentralized manner, with individual robot team members being capable of
acting without explicit communication between neighbors. In this paper, we
propose a novel game theoretic model that enables decentralized and
communication-free navigation to a goal position. Robots estimate the behavior
of their local teammates in order to identify behaviors that move them in the
direction of the goal, while also avoiding obstacles and maintaining team
cohesion without collisions. We prove theoretically that generated actions
approach a Nash equilibrium, which also corresponds to an optimal strategy
identified for each robot. We show through extensive simulations that our
approach enables decentralized and communication-free navigation by a
multi-robot system to a goal position, and is able to avoid obstacles and
collisions, maintain connectivity, and respond robustly to sensor noise.
</p>
<a href="http://arxiv.org/abs/2012.09335" target="_blank">arXiv:2012.09335</a> [<a href="http://arxiv.org/pdf/2012.09335" target="_blank">pdf</a>]

<h2>Achieving Real-Time LiDAR 3D Object Detection on a Mobile Device. (arXiv:2012.13801v2 [cs.CV] UPDATED)</h2>
<h3>Pu Zhao, Wei Niu, Geng Yuan, Yuxuan Cai, Hsin-Hsuan Sung, Sijia Liu, Xipeng Shen, Bin Ren, Yanzhi Wang, Xue Lin</h3>
<p>3D object detection is an important task, especially in the autonomous
driving application domain. However, it is challenging to support the real-time
performance with the limited computation and memory resources on edge-computing
devices in self-driving cars. To achieve this, we propose a compiler-aware
unified framework incorporating network enhancement and pruning search with the
reinforcement learning techniques, to enable real-time inference of 3D object
detection on the resource-limited edge-computing devices. Specifically, a
generator Recurrent Neural Network (RNN) is employed to provide the unified
scheme for both network enhancement and pruning search automatically, without
human expertise and assistance. And the evaluated performance of the unified
schemes can be fed back to train the generator RNN. The experimental results
demonstrate that the proposed framework firstly achieves real-time 3D object
detection on mobile devices (Samsung Galaxy S20 phone) with competitive
detection performance.
</p>
<a href="http://arxiv.org/abs/2012.13801" target="_blank">arXiv:2012.13801</a> [<a href="http://arxiv.org/pdf/2012.13801" target="_blank">pdf</a>]

<h2>Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action Recognition. (arXiv:2012.13823v2 [cs.CV] UPDATED)</h2>
<h3>Raphael Memmesheimer, Simon H&#xe4;ring, Nick Theisen, Dietrich Paulus</h3>
<p>One-shot action recognition allows the recognition of human-performed actions
with only a single training example. This can influence human-robot-interaction
positively by enabling the robot to react to previously unseen behaviour. We
formulate the one-shot action recognition problem as a deep metric learning
problem and propose a novel image-based skeleton representation that performs
well in a metric learning setting. Therefore, we train a model that projects
the image representations into an embedding space. In embedding space the
similar actions have a low euclidean distance while dissimilar actions have a
higher distance. The one-shot action recognition problem becomes a
nearest-neighbor search in a set of activity reference samples. We evaluate the
performance of our proposed representation against a variety of other
skeleton-based image representations. In addition, we present an ablation study
that shows the influence of different embedding vector sizes, losses and
augmentation. Our approach lifts the state-of-the-art by 3.3% for the one-shot
action recognition protocol on the NTU RGB+D 120 dataset under a comparable
training setup. With additional augmentation our result improved over 7.7%.
</p>
<a href="http://arxiv.org/abs/2012.13823" target="_blank">arXiv:2012.13823</a> [<a href="http://arxiv.org/pdf/2012.13823" target="_blank">pdf</a>]

<h2>GAN Inversion: A Survey. (arXiv:2101.05278v2 [cs.CV] UPDATED)</h2>
<h3>Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang</h3>
<p>GAN inversion aims to invert a given image back into the latent space of a
pretrained GAN model, for the image to be faithfully reconstructed from the
inverted code by the generator. As an emerging technique to bridge the real and
fake image domains, GAN inversion plays an essential role in enabling the
pretrained GAN models such as StyleGAN and BigGAN to be used for real image
editing applications. Meanwhile, GAN inversion also provides insights on the
interpretation of GAN's latent space and how the realistic images can be
generated. In this paper, we provide an overview of GAN inversion with a focus
on its recent algorithms and applications. We cover important techniques of GAN
inversion and their applications to image restoration and image manipulation.
We further elaborate on some trends and challenges for future directions.
</p>
<a href="http://arxiv.org/abs/2101.05278" target="_blank">arXiv:2101.05278</a> [<a href="http://arxiv.org/pdf/2101.05278" target="_blank">pdf</a>]

<h2>The Pitfall of More Powerful Autoencoders in Lidar-Based Navigation. (arXiv:2102.02127v2 [cs.RO] UPDATED)</h2>
<h3>Christopher Gebauer, Maren Bennewitz</h3>
<p>The benefit of pretrained autoencoders for reinforcement learning in
comparison to training on raw observations is already known [1]. In this paper,
we address the generation of a compact and information-rich state
representation. In particular, we train a variational autoencoder for 2D-lidar
scans to use its latent state for reinforcement learning of navigation tasks.
To achieve high reconstruction power of our autoencoding pipeline, we propose
an - in the context of autoencoding 2D-lidar scans - novel preprocessing into a
local binary occupancy image. This has no additional requirements, neither
self-localization nor robust mapping, and therefore can be applied in any
setting and easily transferred from simulation in real-world. In a second
stage, we show the usage of the compact state representation generated by our
autoencoding pipeline in a simplistic navigation task and expose the pitfall
that increased reconstruction power will always lead to an improved
performance. We implemented our approach in python using tensorflow. Our
datasets are simulated with pybullet as well as recorded using a slamtec
rplidar A3. The experiments show the significantly improved reconstruction
capabilities of our approach for 2D-lidar scans w.r.t. the state of the art.
However, as we demonstrate in the experiments the impact on reinforcement
learning in lidar-based navigation tasks is non-predictable when improving the
latent state representation generated by an autoencoding pipeline. This is
surprising and needs to be taken into account during the process of optimizing
a pretrained autoencoder for reinforcement learning tasks.
</p>
<a href="http://arxiv.org/abs/2102.02127" target="_blank">arXiv:2102.02127</a> [<a href="http://arxiv.org/pdf/2102.02127" target="_blank">pdf</a>]

<h2>Colorization Transformer. (arXiv:2102.04432v2 [cs.CV] UPDATED)</h2>
<h3>Manoj Kumar, Dirk Weissenborn, Nal Kalchbrenner</h3>
<p>We present the Colorization Transformer, a novel approach for diverse high
fidelity image colorization based on self-attention. Given a grayscale image,
the colorization proceeds in three steps. We first use a conditional
autoregressive transformer to produce a low resolution coarse coloring of the
grayscale image. Our architecture adopts conditional transformer layers to
effectively condition grayscale input. Two subsequent fully parallel networks
upsample the coarse colored low resolution image into a finely colored high
resolution image. Sampling from the Colorization Transformer produces diverse
colorings whose fidelity outperforms the previous state-of-the-art on
colorising ImageNet based on FID results and based on a human evaluation in a
Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators
prefer the highest rated among three generated colorings over the ground truth.
The code and pre-trained checkpoints for Colorization Transformer are publicly
available at
https://github.com/google-research/google-research/tree/master/coltran
</p>
<a href="http://arxiv.org/abs/2102.04432" target="_blank">arXiv:2102.04432</a> [<a href="http://arxiv.org/pdf/2102.04432" target="_blank">pdf</a>]

<h2>Fair Robust Assignment using Redundancy. (arXiv:2102.06265v2 [cs.RO] UPDATED)</h2>
<h3>Matthew Malencia, Vijay Kumar, George Pappas, Amanda Prorok</h3>
<p>We study the consideration of fairness in redundant assignment for
multi-agent task allocation. It has recently been shown that redundant
assignment of agents to tasks provides robustness to uncertainty in task
performance. However, the question of how to fairly assign these redundant
resources across tasks remains unaddressed. In this paper, we present a novel
problem formulation for fair redundant task allocation, which we cast as the
optimization of worst-case task costs under a cardinality constraint. Solving
this problem optimally is NP-hard. We exploit properties of supermodularity to
propose a polynomial-time, near-optimal solution. In supermodular redundant
assignment, the use of additional agents always improves task costs. Therefore,
we provide a solution set that is $\alpha$ times larger than the cardinality
constraint. This constraint relaxation enables our approach to achieve a
super-optimal cost by using a sub-optimal assignment size. We derive the
sub-optimality bound on this cardinality relaxation, $\alpha$. Additionally, we
demonstrate that our algorithm performs near-optimally without the cardinality
relaxation. We show simulations of redundant assignments of robots to goal
nodes on transport networks with uncertain travel times. Empirically, our
algorithm outperforms benchmarks, scales to large problems, and provides
improvements in both fairness and average utility.
</p>
<a href="http://arxiv.org/abs/2102.06265" target="_blank">arXiv:2102.06265</a> [<a href="http://arxiv.org/pdf/2102.06265" target="_blank">pdf</a>]

<h2>A Novel Bio-Inspired Texture Descriptor based on Biodiversity and Taxonomic Measures. (arXiv:2102.06997v2 [cs.CV] UPDATED)</h2>
<h3>Steve Tsham Mpinda Ataky, Alessandro Lameiras Koerich</h3>
<p>Texture can be defined as the change of image intensity that forms repetitive
patterns, resulting from physical properties of the object's roughness or
differences in a reflection on the surface. Considering that texture forms a
complex system of patterns in a non-deterministic way, biodiversity concepts
can help texture characterization in images. This paper proposes a novel
approach capable of quantifying such a complex system of diverse patterns
through species diversity and richness and taxonomic distinctiveness. The
proposed approach considers each image channel as a species ecosystem and
computes species diversity and richness measures as well as taxonomic measures
to describe the texture. The proposed approach takes advantage of ecological
patterns' invariance characteristics to build a permutation, rotation, and
translation invariant descriptor. Experimental results on three datasets of
natural texture images and two datasets of histopathological images have shown
that the proposed texture descriptor has advantages over several texture
descriptors and deep methods.
</p>
<a href="http://arxiv.org/abs/2102.06997" target="_blank">arXiv:2102.06997</a> [<a href="http://arxiv.org/pdf/2102.06997" target="_blank">pdf</a>]

<h2>GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation. (arXiv:2102.12145v2 [cs.CV] UPDATED)</h2>
<h3>Gu Wang, Fabian Manhardt, Federico Tombari, Xiangyang Ji</h3>
<p>6D pose estimation from a single RGB image is a fundamental task in computer
vision. The current top-performing deep learning-based methods rely on an
indirect strategy, i.e., first establishing 2D-3D correspondences between the
coordinates in the image plane and object coordinate system, and then applying
a variant of the P$n$P/RANSAC algorithm. However, this two-stage pipeline is
not end-to-end trainable, thus is hard to be employed for many tasks requiring
differentiable poses. On the other hand, methods based on direct regression are
currently inferior to geometry-based methods. In this work, we perform an
in-depth investigation on both direct and indirect methods, and propose a
simple yet effective Geometry-guided Direct Regression Network (GDR-Net) to
learn the 6D pose in an end-to-end manner from dense correspondence-based
intermediate geometric representations. Extensive experiments show that our
approach remarkably outperforms state-of-the-art methods on LM, LM-O and YCB-V
datasets. Code is available at https://git.io/GDR-Net.
</p>
<a href="http://arxiv.org/abs/2102.12145" target="_blank">arXiv:2102.12145</a> [<a href="http://arxiv.org/pdf/2102.12145" target="_blank">pdf</a>]

<h2>$SE_2(3)$ based Extended Kalman Filter and Smoothing for Inertial-Integrated Navigation. (arXiv:2102.12897v4 [cs.RO] UPDATED)</h2>
<h3>Yarong Luo, Chi Guo, Shenyong You, Jianlang Hu, Jingnan Liu</h3>
<p>The error representation using the straight difference of two vectors in the
inertial navigation system may not be reasonable as it does not take the
direction difference into consideration. Therefore, we proposed to use the
$SE_2(3)$ matrix Lie group to represent the state of the inertial-integrated
navigation system which consequently leads to the common frame error
representation.

With the new velocity and position error definition, we leverage the group
affine dynamics with the autonomous error properties and derive the error state
differential equation for the inertial-integrated navigation on the
north-east-down local-level navigation frame and the earth-centered earth-fixed
frame, respectively, the corresponding extending Kalman filter (EKF), terms as
$SE_2(3)$-EKF has also been derived. It provides a new perspective on the
geometric EKF with a more sophisticated formula for the inertial-integrated
navigation system. Furthermore, we propose a $SE_2(3)$-based smoothing
algorithm based on the $SE_2(3)$-based EKF.
</p>
<a href="http://arxiv.org/abs/2102.12897" target="_blank">arXiv:2102.12897</a> [<a href="http://arxiv.org/pdf/2102.12897" target="_blank">pdf</a>]

<h2>Trajectory Servoing: Image-Based Trajectory Tracking Using SLAM. (arXiv:2103.00055v2 [cs.RO] UPDATED)</h2>
<h3>Shiyu Feng, Zixuan Wu, Yipu Zhao, Patricio A. Vela</h3>
<p>This paper describes an image based visual servoing (IBVS) system for a
nonholonomic robot to achieve good trajectory following without real-time robot
pose information and without a known visual map of the environment. We call it
trajectory servoing. The critical component is a feature-based, indirect SLAM
method to provide a pool of available features with estimated depth, so that
they may be propagated forward in time to generate image feature trajectories
for visual servoing. Short and long distance experiments show the benefits of
trajectory servoing for navigating unknown areas without absolute positioning.
Trajectory servoing is shown to be more accurate than pose-based feedback when
both rely on the same underlying SLAM system.
</p>
<a href="http://arxiv.org/abs/2103.00055" target="_blank">arXiv:2103.00055</a> [<a href="http://arxiv.org/pdf/2103.00055" target="_blank">pdf</a>]

<h2>A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion from Convolutional Neural Networks to Spiking Neural Networks. (arXiv:2103.00944v2 [cs.CV] UPDATED)</h2>
<h3>Dengyu Wu, Xinping Yi, Xiaowei Huang</h3>
<p>Spiking neural networks (SNNs) offer an inherent ability to process
spatial-temporal data, or in other words, realworld sensory data, but suffer
from the difficulty of training high accuracy models. A major thread of
research on SNNs is on converting a pre-trained convolutional neural network
(CNN) to an SNN of the same structure. State-of-the-art conversion methods are
approaching the accuracy limit, i.e., the near-zero accuracy loss of SNN
against the original CNN. However, we note that this is made possible only when
significantly more energy is consumed to process an input. In this paper, we
argue that this trend of "energy for accuracy" is not necessary -- a little
energy can go a long way to achieve the near-zero accuracy loss. Specifically,
we propose a novel CNN-to-SNN conversion method that is able to use a
reasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to
achieve the near-zero accuracy loss. The new conversion method, named as
explicit current control (ECC), contains three techniques (current
normalisation, thresholding for residual elimination, and consistency
maintenance for batch-normalisation), in order to explicitly control the
currents flowing through the SNN when processing inputs. We implement ECC into
a tool nicknamed SpKeras, which can conveniently import Keras CNN models and
convert them into SNNs. We conduct an extensive set of experiments with the
tool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 --
and compare with state-of-the-art conversion methods. Results show that ECC is
a promising method that can optimise over energy consumption and accuracy loss
simultaneously.
</p>
<a href="http://arxiv.org/abs/2103.00944" target="_blank">arXiv:2103.00944</a> [<a href="http://arxiv.org/pdf/2103.00944" target="_blank">pdf</a>]

<h2>A Deep Emulator for Secondary Motion of 3D Characters. (arXiv:2103.01261v3 [cs.CV] UPDATED)</h2>
<h3>Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbi&#x10d;</h3>
<p>Fast and light-weight methods for animating 3D characters are desirable in
various applications such as computer games. We present a learning-based
approach to enhance skinning-based animations of 3D characters with vivid
secondary motion effects. We design a neural network that encodes each local
patch of a character simulation mesh where the edges implicitly encode the
internal forces between the neighboring vertices. The network emulates the
ordinary differential equations of the character dynamics, predicting new
vertex positions from the current accelerations, velocities and positions.
Being a local method, our network is independent of the mesh topology and
generalizes to arbitrarily shaped 3D character meshes at test time. We further
represent per-vertex constraints and material properties such as stiffness,
enabling us to easily adjust the dynamics in different parts of the mesh. We
evaluate our method on various character meshes and complex motion sequences.
Our method can be over 30 times more efficient than ground-truth physically
based simulation, and outperforms alternative solutions that provide fast
approximations.
</p>
<a href="http://arxiv.org/abs/2103.01261" target="_blank">arXiv:2103.01261</a> [<a href="http://arxiv.org/pdf/2103.01261" target="_blank">pdf</a>]

<h2>Exploiting latent representation of sparse semantic layers for improved short-term motion prediction with Capsule Networks. (arXiv:2103.01644v2 [cs.CV] UPDATED)</h2>
<h3>Albert Dulian, John C. Murray</h3>
<p>As urban environments manifest high levels of complexity it is of vital
importance that safety systems embedded within autonomous vehicles (AVs) are
able to accurately anticipate short-term future motion of nearby agents. This
problem can be further understood as generating a sequence of coordinates
describing the future motion of the tracked agent. Various proposed approaches
demonstrate significant benefits of using a rasterised top-down image of the
road, with a combination of Convolutional Neural Networks (CNNs), for
extraction of relevant features that define the road structure (eg. driveable
areas, lanes, walkways). In contrast, this paper explores use of Capsule
Networks (CapsNets) in the context of learning a hierarchical representation of
sparse semantic layers corresponding to small regions of the High-Definition
(HD) map. Each region of the map is dismantled into separate geometrical layers
that are extracted with respect to the agent's current position. By using an
architecture based on CapsNets the model is able to retain hierarchical
relationships between detected features within images whilst also preventing
loss of spatial data often caused by the pooling operation. We train and
evaluate our model on publicly available dataset nuTonomy scenes and compare it
to recently published methods. We show that our model achieves significant
improvement over recently published works on deterministic prediction, whilst
drastically reducing the overall size of the network.
</p>
<a href="http://arxiv.org/abs/2103.01644" target="_blank">arXiv:2103.01644</a> [<a href="http://arxiv.org/pdf/2103.01644" target="_blank">pdf</a>]

<h2>ID-Unet: Iterative Soft and Hard Deformation for View Synthesis. (arXiv:2103.02264v3 [cs.CV] UPDATED)</h2>
<h3>Mingyu Yin, Li Sun, Qingli Li</h3>
<p>View synthesis is usually done by an autoencoder, in which the encoder maps a
source view image into a latent content code, and the decoder transforms it
into a target view image according to the condition. However, the source
contents are often not well kept in this setting, which leads to unnecessary
changes during the view translation. Although adding skipped connections, like
Unet, alleviates the problem, but it often causes the failure on the view
conformity. This paper proposes a new architecture by performing the
source-to-target deformation in an iterative way. Instead of simply
incorporating the features from multiple layers of the encoder, we design soft
and hard deformation modules, which warp the encoder features to the target
view at different resolutions, and give results to the decoder to complement
the details. Particularly, the current warping flow is not only used to align
the feature of the same resolution, but also as an approximation to coarsely
deform the high resolution feature. Then the residual flow is estimated and
applied in the high resolution, so that the deformation is built up in the
coarse-to-fine fashion. To better constrain the model, we synthesize a rough
target view image based on the intermediate flows and their warped features.
The extensive ablation studies and the final results on two different data sets
show the effectiveness of the proposed model.
</p>
<a href="http://arxiv.org/abs/2103.02264" target="_blank">arXiv:2103.02264</a> [<a href="http://arxiv.org/pdf/2103.02264" target="_blank">pdf</a>]

<h2>Multi-attentional Deepfake Detection. (arXiv:2103.02406v3 [cs.CV] UPDATED)</h2>
<h3>Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang, Nenghai Yu</h3>
<p>Face forgery by deepfake is widely spread over the internet and has raised
severe societal concerns. Recently, how to detect such forgery contents has
become a hot research topic and many deepfake detection methods have been
proposed. Most of them model deepfake detection as a vanilla binary
classification problem, i.e, first use a backbone network to extract a global
feature and then feed it into a binary classifier (real/fake). But since the
difference between the real and fake images in this task is often subtle and
local, we argue this vanilla solution is not optimal. In this paper, we instead
formulate deepfake detection as a fine-grained classification problem and
propose a new multi-attentional deepfake detection network. Specifically, it
consists of three key components: 1) multiple spatial attention heads to make
the network attend to different local parts; 2) textural feature enhancement
block to zoom in the subtle artifacts in shallow features; 3) aggregate the
low-level textural feature and high-level semantic features guided by the
attention maps. Moreover, to address the learning difficulty of this network,
we further introduce a new regional independence loss and an attention guided
data augmentation strategy. Through extensive experiments on different
datasets, we demonstrate the superiority of our method over the vanilla binary
classifier counterparts, and achieve state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2103.02406" target="_blank">arXiv:2103.02406</a> [<a href="http://arxiv.org/pdf/2103.02406" target="_blank">pdf</a>]

<h2>Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions. (arXiv:2103.02760v2 [cs.RO] UPDATED)</h2>
<h3>Ivan Fursa, Elias Fandi, Valentina Musat, Jacob Culley, Enric Gil, Louise Bilous, Isaac Vander Sluis, Alexander Rast, Andrew Bradley</h3>
<p>Autonomous vehicles rely heavily upon their perception subsystems to see the
environment in which they operate. Unfortunately, the effect of varying weather
conditions presents a significant challenge to object detection algorithms, and
thus it is imperative to test the vehicle extensively in all conditions which
it may experience. However, unpredictable weather can make real-world testing
in adverse conditions an expensive and time consuming task requiring access to
specialist facilities, and weatherproofing of sensitive electronics. Simulation
provides an alternative to real world testing, with some studies developing
increasingly visually realistic representations of the real world on powerful
compute hardware. Given that subsequent subsystems in the autonomous vehicle
pipeline are unaware of the visual realism of the simulation, when developing
modules downstream of perception the appearance is of little consequence -
rather it is how the perception system performs in the prevailing weather
condition that is important. This study explores the potential of using a
simple, lightweight image augmentation system in an autonomous racing vehicle -
focusing not on visual accuracy, but rather the effect upon perception system
performance. With minimal adjustment, the prototype system developed in this
study can replicate the effects of both water droplets on the camera lens, and
fading light conditions. The system introduces a latency of less than 8 ms
using compute hardware that is well suited to being carried in the vehicle -
rendering it ideally suited to real-time implementation that can be run during
experiments in simulation, and augmented reality testing in the real world.
</p>
<a href="http://arxiv.org/abs/2103.02760" target="_blank">arXiv:2103.02760</a> [<a href="http://arxiv.org/pdf/2103.02760" target="_blank">pdf</a>]

<h2>Sub-pixel face landmarks using heatmaps and a bag of tricks. (arXiv:2103.03059v2 [cs.CV] UPDATED)</h2>
<h3>Samuel W. F. Earp, Aubin Samacoits, Sanjana Jain, Pavit Noinongyao, Siwa Boonpunmongkol</h3>
<p>Accurate face landmark localization is an essential part of face recognition,
reconstruction and morphing. To accurately localize face landmarks, we present
our heatmap regression approach. Each model consists of a MobileNetV2 backbone
followed by several upscaling layers, with different tricks to optimize both
performance and inference cost. We use five na\"ive face landmarks from a
publicly available face detector to position and align the face instead of
using the bounding box like traditional methods. Moreover, we show by adding
random rotation, displacement and scaling -- after alignment -- that the model
is more sensitive to the face position than orientation. We also show that it
is possible to reduce the upscaling complexity by using a mixture of
deconvolution and pixel-shuffle layers without impeding localization
performance. We present our state-of-the-art face landmark localization model
(ranking second on The 2nd Grand Challenge of 106-Point Facial Landmark
Localization validation set). Finally, we test the effect on face recognition
using these landmarks, using a publicly available model and benchmarks.
</p>
<a href="http://arxiv.org/abs/2103.03059" target="_blank">arXiv:2103.03059</a> [<a href="http://arxiv.org/pdf/2103.03059" target="_blank">pdf</a>]

<h2>Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis. (arXiv:2103.03328v2 [cs.CV] UPDATED)</h2>
<h3>Aleksandar Vakanski, Min Xian</h3>
<p>The generalization performance of deep learning models for medical image
analysis often decreases on images collected with different devices for data
acquisition, device settings, or patient population. A better understanding of
the generalization capacity on new images is crucial for clinicians'
trustworthiness in deep learning. Although significant research efforts have
been recently directed toward establishing generalization bounds and complexity
measures, still, there is often a significant discrepancy between the predicted
and actual generalization performance. As well, related large empirical studies
have been primarily based on validation with general-purpose image datasets.
This paper presents an empirical study that investigates the correlation
between 25 complexity measures and the generalization abilities of supervised
deep learning classifiers for breast ultrasound images. The results indicate
that PAC-Bayes flatness-based and path norm-based measures produce the most
consistent explanation for the combination of models and data. We also
investigate the use of multi-task classification and segmentation approach for
breast images, and report that such learning approach acts as an implicit
regularizer and is conducive toward improved generalization.
</p>
<a href="http://arxiv.org/abs/2103.03328" target="_blank">arXiv:2103.03328</a> [<a href="http://arxiv.org/pdf/2103.03328" target="_blank">pdf</a>]

<h2>Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v2 [cs.CV] UPDATED)</h2>
<h3>Tariq Bdair, Nassir Navab, Shadi Albarqouni</h3>
<p>Skin cancer is one of the most deadly cancers worldwide. Yet, it can be
reduced by early detection. Recent deep-learning methods have shown a
dermatologist-level performance in skin cancer classification. Yet, this
success demands a large amount of centralized data, which is oftentimes not
available. Federated learning has been recently introduced to train machine
learning models in a privacy-preserved distributed fashion demanding annotated
data at the clients, which is usually expensive and not available, especially
in the medical field. To this end, we propose FedPerl, a semi-supervised
federated learning method that utilizes peer learning from social sciences and
ensemble averaging from committee machines to build communities and encourage
its members to learn from each other such that they produce more accurate
pseudo labels. We also propose the peer anonymization (PA) technique as a core
component of FedPerl. PA preserves privacy and reduces the communication cost
while maintaining the performance without additional complexity. We validated
our method on 38,000 skin lesion images collected from 4 publicly available
datasets. FedPerl achieves superior performance over the baselines and
state-of-the-art SSFL by 15.8%, and 1.8% respectively. Further, FedPerl shows
less sensitivity to noisy clients.
</p>
<a href="http://arxiv.org/abs/2103.03703" target="_blank">arXiv:2103.03703</a> [<a href="http://arxiv.org/pdf/2103.03703" target="_blank">pdf</a>]

<h2>Visual diagnosis of the Varroa destructor parasitic mite in honeybees using object detector techniques. (arXiv:2103.03133v1 [cs.CV] CROSS LISTED)</h2>
<h3>Simon Bilik, Lukas Kratochvila, Adam Ligocki, Ondrej Bostik, Tomas Zemcik, Matous Hybl, Karel Horak, Ludek Zalud</h3>
<p>The Varroa destructor mite is one of the most dangerous Honey Bee (Apis
mellifera) parasites worldwide and the bee colonies have to be regularly
monitored in order to control its spread. Here we present an object detector
based method for health state monitoring of bee colonies. This method has the
potential for online measurement and processing. In our experiment, we compare
the YOLO and SSD object detectors along with the Deep SVDD anomaly detector.
Based on the custom dataset with 600 ground-truth images of healthy and
infected bees in various scenes, the detectors reached a high F1 score up to
0.874 in the infected bee detection and up to 0.727 in the detection of the
Varroa Destructor mite itself. The results demonstrate the potential of this
approach, which will be later used in the real-time computer vision based honey
bee inspection system. To the best of our knowledge, this study is the first
one using object detectors for this purpose. We expect that performance of
those object detectors will enable us to inspect the health status of the honey
bee colonies.
</p>
<a href="http://arxiv.org/abs/2103.03133" target="_blank">arXiv:2103.03133</a> [<a href="http://arxiv.org/pdf/2103.03133" target="_blank">pdf</a>]

