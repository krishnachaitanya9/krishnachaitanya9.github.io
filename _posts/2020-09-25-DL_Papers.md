---
title: Latest Deep Learning Papers
date: 2020-11-23 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (298 Articles)</h1>
<h2>Learn to Bind and Grow Neural Structures. (arXiv:2011.10568v1 [cs.LG])</h2>
<h3>Azhar Shaikh, Nishant Sinha</h3>
<p>Task-incremental learning involves the challenging problem of learning new
tasks continually, without forgetting past knowledge. Many approaches address
the problem by expanding the structure of a shared neural network as tasks
arrive, but struggle to grow optimally, without losing past knowledge. We
present a new framework, Learn to Bind and Grow, which learns a neural
architecture for a new task incrementally, either by binding with layers of a
similar task or by expanding layers which are more likely to conflict between
tasks. Central to our approach is a novel, interpretable, parameterization of
the shared, multi-task architecture space, which then enables computing
globally optimal architectures using Bayesian optimization. Experiments on
continual learning benchmarks show that our framework performs comparably with
earlier expansion based approaches and is able to flexibly compute multiple
optimal solutions with performance-size trade-offs.
</p>
<a href="http://arxiv.org/abs/2011.10568" target="_blank">arXiv:2011.10568</a> [<a href="http://arxiv.org/pdf/2011.10568" target="_blank">pdf</a>]

<h2>ATSal: An Attention Based Architecture for Saliency Prediction in 360 Videos. (arXiv:2011.10600v1 [cs.CV])</h2>
<h3>Yasser Dahou, Marouane Tliba, Kevin McGuinness, Noel O&#x27;Connor</h3>
<p>The spherical domain representation of 360 video/image presents many
challenges related to the storage, processing, transmission and rendering of
omnidirectional videos (ODV). Models of human visual attention can be used so
that only a single viewport is rendered at a time, which is important when
developing systems that allow users to explore ODV with head mounted displays
(HMD). Accordingly, researchers have proposed various saliency models for 360
video/images. This paper proposes ATSal, a novel attention based (head-eye)
saliency model for 360\degree videos. The attention mechanism explicitly
encodes global static visual attention allowing expert models to focus on
learning the saliency on local patches throughout consecutive frames. We
compare the proposed approach to other state-of-the-art saliency models on two
datasets: Salient360! and VR-EyeTracking. Experimental results on over 80 ODV
videos (75K+ frames) show that the proposed method outperforms the existing
state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2011.10600" target="_blank">arXiv:2011.10600</a> [<a href="http://arxiv.org/pdf/2011.10600" target="_blank">pdf</a>]

<h2>Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System. (arXiv:2011.10605v1 [cs.LG])</h2>
<h3>Junhyeok Ahn, Luis Sentis</h3>
<p>Model-based reinforcement learning (MBRL) algorithms can attain significant
sample efficiency but require an appropriate network structure to represent
system dynamics. Current approaches include white-box modeling using analytic
parameterizations and black-box modeling using deep neural networks. However,
both can suffer from a bias-variance trade-off in the learning process, and
neither provides a structured method for injecting domain knowledge into the
network. As an alternative, gray-box modeling leverages prior knowledge in
neural network training but only for simple systems. In this paper, we devise a
nested mixture of experts (NMOE) for representing and learning hybrid dynamical
systems. An NMOE combines both white-box and black-box models while optimizing
bias-variance trade-off. Moreover, an NMOE provides a structured method for
incorporating various types of prior knowledge by training the associative
experts cooperatively or competitively. The prior knowledge includes
information on robots' physical contacts with the environments as well as their
kinematic and dynamic properties. In this paper, we demonstrate how to
incorporate prior knowledge into our NMOE in various continuous control
domains, including hybrid dynamical systems. We also show the effectiveness of
our method in terms of data-efficiency, generalization to unseen data, and
bias-variance trade-off. Finally, we evaluate our NMOE using an MBRL setup,
where the model is integrated with a model-based controller and trained online.
</p>
<a href="http://arxiv.org/abs/2011.10605" target="_blank">arXiv:2011.10605</a> [<a href="http://arxiv.org/pdf/2011.10605" target="_blank">pdf</a>]

<h2>Lightweight Data Fusion with Conjugate Mappings. (arXiv:2011.10607v1 [stat.ML])</h2>
<h3>Christopher L. Dean, Stephen J. Lee, Jason Pacheco, John W. Fisher III</h3>
<p>We present an approach to data fusion that combines the interpretability of
structured probabilistic graphical models with the flexibility of neural
networks. The proposed method, lightweight data fusion (LDF), emphasizes
posterior analysis over latent variables using two types of information:
primary data, which are well-characterized but with limited availability, and
auxiliary data, readily available but lacking a well-characterized statistical
relationship to the latent quantity of interest. The lack of a forward model
for the auxiliary data precludes the use of standard data fusion approaches,
while the inability to acquire latent variable observations severely limits
direct application of most supervised learning methods. LDF addresses these
issues by utilizing neural networks as conjugate mappings of the auxiliary
data: nonlinear transformations into sufficient statistics with respect to the
latent variables. This facilitates efficient inference by preserving the
conjugacy properties of the primary data and leads to compact representations
of the latent variable posterior distributions. We demonstrate the LDF
methodology on two challenging inference problems: (1) learning electrification
rates in Rwanda from satellite imagery, high-level grid infrastructure, and
other sources; and (2) inferring county-level homicide rates in the USA by
integrating socio-economic data using a mixture model of multiple conjugate
mappings.
</p>
<a href="http://arxiv.org/abs/2011.10607" target="_blank">arXiv:2011.10607</a> [<a href="http://arxiv.org/pdf/2011.10607" target="_blank">pdf</a>]

<h2>Large Scale Neural Architecture Search with Polyharmonic Splines. (arXiv:2011.10608v1 [cs.CV])</h2>
<h3>Ulrich Finkler, Michele Merler, Rameswar Panda, Mayoore S. Jaiswal, Hui Wu, Kandan Ramakrishnan, Chun-Fu Chen, Minsik Cho, David Kung, Rogerio Feris, Bishwaranjan Bhattacharjee</h3>
<p>Neural Architecture Search (NAS) is a powerful tool to automatically design
deep neural networks for many tasks, including image classification. Due to the
significant computational burden of the search phase, most NAS methods have
focused so far on small, balanced datasets. All attempts at conducting NAS at
large scale have employed small proxy sets, and then transferred the learned
architectures to larger datasets by replicating or stacking the searched cells.
We propose a NAS method based on polyharmonic splines that can perform search
directly on large scale, imbalanced target datasets. We demonstrate the
effectiveness of our method on the ImageNet22K benchmark[16], which contains 14
million images distributed in a highly imbalanced manner over 21,841
categories. By exploring the search space of the ResNet [23] and Big-Little Net
ResNext [11] architectures directly on ImageNet22K, our polyharmonic splines
NAS method designed a model which achieved a top-1 accuracy of 40.03% on
ImageNet22K, an absolute improvement of 3.13% over the state of the art with
similar global batch size [15].
</p>
<a href="http://arxiv.org/abs/2011.10608" target="_blank">arXiv:2011.10608</a> [<a href="http://arxiv.org/pdf/2011.10608" target="_blank">pdf</a>]

<h2>Adversarial Training for EM Classification Networks. (arXiv:2011.10615v1 [cs.LG])</h2>
<h3>Tom Grimes, Eric Church, William Pitts, Lynn Wood, Eva Brayfindley, Luke Erikson, Mark Greaves</h3>
<p>We present a novel variant of Domain Adversarial Networks with impactful
improvements to the loss functions, training paradigm, and hyperparameter
optimization. New loss functions are defined for both forks of the DANN
network, the label predictor and domain classifier, in order to facilitate more
rapid gradient descent, provide more seamless integration into modern neural
networking frameworks, and allow previously unavailable inferences into network
behavior. Using these loss functions, it is possible to extend the concept of
'domain' to include arbitrary user defined labels applicable to subsets of the
training data, the test data, or both. As such, the network can be operated in
either 'On the Fly' mode where features provided by the feature extractor
indicative of differences between 'domain' labels in the training data are
removed or in 'Test Collection Informed' mode where features indicative of
difference between 'domain' labels in the combined training and test data are
removed (without needing to know or provide test activity labels to the
network). This work also draws heavily from previous works on Robust Training
which draws training examples from a L_inf ball around the training data in
order to remove fragile features induced by random fluctuations in the data. On
these networks we explore the process of hyperparameter optimization for both
the domain adversarial and robust hyperparameters. Finally, this network is
applied to the construction of a binary classifier used to identify the
presence of EM signal emitted by a turbopump. For this example, the effect of
the robust and domain adversarial training is to remove features indicative of
the difference in background between instances of operation of the device -
providing highly discriminative features on which to construct the classifier.
</p>
<a href="http://arxiv.org/abs/2011.10615" target="_blank">arXiv:2011.10615</a> [<a href="http://arxiv.org/pdf/2011.10615" target="_blank">pdf</a>]

<h2>Bridging Physics-based and Data-driven modeling for Learning Dynamical Systems. (arXiv:2011.10616v1 [cs.LG])</h2>
<h3>Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang, Rose Yu</h3>
<p>How can we learn a dynamical system to make forecasts, when some variables
are unobserved? For instance, in COVID-19, we want to forecast the number of
infected and death cases but we do not know the count of susceptible and
exposed people. While mechanics compartment models are widely-used in epidemic
modeling, data-driven models are emerging for disease forecasting. As a case
study, we compare these two types of models for COVID-19 forecasting and notice
that physics-based models significantly outperform deep learning models. We
present a hybrid approach, AutoODE-COVID, which combines a novel compartmental
model with automatic differentiation. Our method obtains a 57.4% reduction in
mean absolute errors for 7-day ahead COVID-19 forecasting compared with the
best deep learning competitor. To understand the inferior performance of deep
learning, we investigate the generalization problem in forecasting. Through
systematic experiments, we found that deep learning models fail to forecast
under shifted distributions either in the data domain or the parameter domain.
This calls attention to rethink generalization especially for learning
dynamical systems.
</p>
<a href="http://arxiv.org/abs/2011.10616" target="_blank">arXiv:2011.10616</a> [<a href="http://arxiv.org/pdf/2011.10616" target="_blank">pdf</a>]

<h2>Semantic SLAM with Autonomous Object-Level Data Association. (arXiv:2011.10625v1 [cs.RO])</h2>
<h3>Zhentian Qian, Kartik Patath, Jie Fu, Jing Xiao</h3>
<p>It is often desirable to capture and map semantic information of an
environment during simultaneous localization and mapping (SLAM). Such semantic
information can enable a robot to better distinguish places with similar
low-level geometric and visual features and perform high-level tasks that use
semantic information about objects to be manipulated and environments to be
navigated. While semantic SLAM has gained increasing attention, there is little
research on semanticlevel data association based on semantic objects, i.e.,
object-level data association. In this paper, we propose a novel object-level
data association algorithm based on bag of words algorithm, formulated as a
maximum weighted bipartite matching problem. With object-level data association
solved, we develop a quadratic-programming-based semantic object initialization
scheme using dual quadric and introduce additional constraints to improve the
success rate of object initialization. The integrated semantic-level SLAM
system can achieve high-accuracy object-level data association and real-time
semantic mapping as demonstrated in the experiments. The online semantic map
building and semantic-level localization capabilities facilitate semantic-level
mapping and task planning in a priori unknown environment.
</p>
<a href="http://arxiv.org/abs/2011.10625" target="_blank">arXiv:2011.10625</a> [<a href="http://arxiv.org/pdf/2011.10625" target="_blank">pdf</a>]

<h2>Assessment and Linear Programming under Fuzzy Conditions. (arXiv:2011.10640v1 [cs.AI])</h2>
<h3>Michael Voskoglou</h3>
<p>A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers
for evaluating a group's mean performance, when qualitative grades instead of
numerical scores are used for assessing its members' individual performance.
Also, a new technique is developed for solving Linear Programming problems with
fuzzy coefficients and everyday life applications are presented to illustrate
our results.
</p>
<a href="http://arxiv.org/abs/2011.10640" target="_blank">arXiv:2011.10640</a> [<a href="http://arxiv.org/pdf/2011.10640" target="_blank">pdf</a>]

<h2>On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Optimization. (arXiv:2011.10643v1 [cs.LG])</h2>
<h3>Abolfazl Hashemi, Anish Acharya, Rudrajit Das, Haris Vikalo, Sujay Sanghavi, Inderjit Dhillon</h3>
<p>In decentralized optimization, it is common algorithmic practice to have
nodes interleave (local) gradient descent iterations with gossip (i.e.
averaging over the network) steps. Motivated by the training of large-scale
machine learning models, it is also increasingly common to require that
messages be {\em lossy compressed} versions of the local parameters. In this
paper, we show that, in such compressed decentralized optimization settings,
there are benefits to having {\em multiple} gossip steps between subsequent
gradient iterations, even when the cost of doing so is appropriately accounted
for e.g. by means of reducing the precision of compressed information. In
particular, we show that having $O(\log\frac{1}{\epsilon})$ gradient iterations
{with constant step size} - and $O(\log\frac{1}{\epsilon})$ gossip steps
between every pair of these iterations - enables convergence to within
$\epsilon$ of the optimal value for smooth non-convex objectives satisfying
Polyak-\L{}ojasiewicz condition. This result also holds for smooth strongly
convex objectives. To our knowledge, this is the first work that derives
convergence results for nonconvex optimization under arbitrary communication
compression.
</p>
<a href="http://arxiv.org/abs/2011.10643" target="_blank">arXiv:2011.10643</a> [<a href="http://arxiv.org/pdf/2011.10643" target="_blank">pdf</a>]

<h2>Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. (arXiv:2011.10650v1 [cs.LG])</h2>
<h3>Rewon Child</h3>
<p>We present a hierarchical VAE that, for the first time, outperforms the
PixelCNN in log-likelihood on all natural image benchmarks. We begin by
observing that VAEs can actually implement autoregressive models, and other,
more efficient generative models, if made sufficiently deep. Despite this,
autoregressive models have traditionally outperformed VAEs. We test if
insufficient depth explains the performance gap by by scaling a VAE to greater
stochastic depth than previously explored and evaluating it on CIFAR-10,
ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very
deep VAEs achieve higher likelihoods, use fewer parameters, generate samples
thousands of times faster, and are more easily applied to high-resolution
images. We visualize the generative process and show the VAEs learn efficient
hierarchical visual representations. We release our source code and models at
https://github.com/openai/vdvae.
</p>
<a href="http://arxiv.org/abs/2011.10650" target="_blank">arXiv:2011.10650</a> [<a href="http://arxiv.org/pdf/2011.10650" target="_blank">pdf</a>]

<h2>Viability of Optical Coherence Tomography for Iris Presentation Attack Detection. (arXiv:2011.10655v1 [cs.CV])</h2>
<h3>Renu Sharma, Arun Ross</h3>
<p>In this paper, we propose the use of Optical Coherence Tomography (OCT)
imaging for the problem of iris presentation attack (PA) detection. We assess
its viability by comparing its performance with respect to traditional iris
imaging modalities, viz., near-infrared (NIR) and visible spectrum. OCT imaging
provides a cross-sectional view of an eye, whereas traditional imaging provides
2D iris textural information. PA detection is performed using three
state-of-the-art deep architectures (VGG19, ResNet50 and DenseNet121) to
differentiate between bonafide and PA samples for each of the three imaging
modalities. Experiments are performed on a dataset of 2,169 bonafide, 177 Van
Dyke eyes and 360 cosmetic contact images acquired using all three imaging
modalities under intra-attack (known PAs) and cross-attack (unknown PAs)
scenarios. We observe promising results demonstrating OCT as a viable solution
for iris presentation attack detection.
</p>
<a href="http://arxiv.org/abs/2011.10655" target="_blank">arXiv:2011.10655</a> [<a href="http://arxiv.org/pdf/2011.10655" target="_blank">pdf</a>]

<h2>Enhancing Poaching Predictions for Under-Resourced Wildlife Conservation Parks Using Remote Sensing Imagery. (arXiv:2011.10666v1 [cs.LG])</h2>
<h3>Rachel Guo, Lily Xu, Drew Cronin, Francis Okeke, Andrew Plumptre, Milind Tambe</h3>
<p>Illegal wildlife poaching is driving the loss of biodiversity. To combat
poaching, rangers patrol expansive protected areas for illegal poaching
activity. However, rangers often cannot comprehensively search such large
parks. Thus, the Protection Assistant for Wildlife Security (PAWS) was
introduced as a machine learning approach to help identify the areas with
highest poaching risk. As PAWS is deployed to parks around the world, we
recognized that many parks have limited resources for data collection and
therefore have scarce feature sets. To ensure under-resourced parks have access
to meaningful poaching predictions, we introduce the use of publicly available
remote sensing data to extract features for parks. By employing this data from
Google Earth Engine, we also incorporate previously unavailable dynamic data to
enrich predictions with seasonal trends. We automate the entire
data-to-deployment pipeline and find that, with only using publicly available
data, we recuperate prediction performance comparable to predictions made using
features manually computed by park specialists. We conclude that the inclusion
of satellite imagery creates a robust system through which parks of any
resource level can benefit from poaching risks for years to come.
</p>
<a href="http://arxiv.org/abs/2011.10666" target="_blank">arXiv:2011.10666</a> [<a href="http://arxiv.org/pdf/2011.10666" target="_blank">pdf</a>]

<h2>A General Framework for Distributed Inference with Uncertain Models. (arXiv:2011.10669v1 [cs.AI])</h2>
<h3>James Z. Hare, Cesar A. Uribe, Lance Kaplan, Ali Jadbabaie</h3>
<p>This paper studies the problem of distributed classification with a network
of heterogeneous agents. The agents seek to jointly identify the underlying
target class that best describes a sequence of observations. The problem is
first abstracted to a hypothesis-testing framework, where we assume that the
agents seek to agree on the hypothesis (target class) that best matches the
distribution of observations. Non-Bayesian social learning theory provides a
framework that solves this problem in an efficient manner by allowing the
agents to sequentially communicate and update their beliefs for each hypothesis
over the network. Most existing approaches assume that agents have access to
exact statistical models for each hypothesis. However, in many practical
applications, agents learn the likelihood models based on limited data, which
induces uncertainty in the likelihood function parameters. In this work, we
build upon the concept of uncertain models to incorporate the agents'
uncertainty in the likelihoods by identifying a broad set of parametric
distribution that allows the agents' beliefs to converge to the same result as
a centralized approach. Furthermore, we empirically explore extensions to
non-parametric models to provide a generalized framework of uncertain models in
non-Bayesian social learning.
</p>
<a href="http://arxiv.org/abs/2011.10669" target="_blank">arXiv:2011.10669</a> [<a href="http://arxiv.org/pdf/2011.10669" target="_blank">pdf</a>]

<h2>Joint Analysis and Prediction of Human Actions and Paths in Video. (arXiv:2011.10670v1 [cs.CV])</h2>
<h3>Junwei Liang</h3>
<p>With the advancement in computer vision deep learning, systems now are able
to analyze an unprecedented amount of rich visual information from videos to
enable applications such as autonomous driving, socially-aware robot assistant
and public safety monitoring. Deciphering human behaviors to predict their
future paths/trajectories and what they would do from videos is important in
these applications. However, modern vision systems in self-driving applications
usually perform the detection (perception) and prediction task in separate
components, which leads to error propagation and sub-optimal performance. More
importantly, these systems do not provide high-level semantic attributes to
reason about pedestrian future. This design hinders prediction performance in
video data from diverse domains and unseen scenarios. To enable optimal future
human behavioral forecasting, it is crucial for the system to be able to detect
and analyze human activities leading up to the prediction period, passing
informative features to the subsequent prediction module for context
understanding. In this thesis, with the goal of improving the performance and
generalization ability of future trajectory and action prediction models, we
conduct human action analysis and jointly optimize models for action detection,
prediction and trajectory prediction.
</p>
<a href="http://arxiv.org/abs/2011.10670" target="_blank">arXiv:2011.10670</a> [<a href="http://arxiv.org/pdf/2011.10670" target="_blank">pdf</a>]

<h2>A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving. (arXiv:2011.10671v1 [cs.CV])</h2>
<h3>Di Feng, Ali Harakeh, Steven Waslander, Klaus Dietmayer</h3>
<p>Capturing uncertainty in object detection is indispensable for safe
autonomous driving. In recent years, deep learning has become the de-facto
approach for object detection, and many probabilistic object detectors have
been proposed. However, there is no summary on uncertainty estimation in deep
object detection, and existing methods are not only built with different
network architectures and uncertainty estimation methods, but also evaluated on
different datasets with a wide range of evaluation metrics. As a result, a
comparison among methods remains challenging, as does the selection of a model
that best suits a particular application. This paper aims to alleviate this
problem by providing a review and comparative study on existing probabilistic
object detection methods for autonomous driving applications. First, we provide
an overview of generic uncertainty estimation in deep learning, and then
systematically survey existing methods and evaluation metrics for probabilistic
object detection. Next, we present a strict comparative study for probabilistic
object detection based on an image detector and three public autonomous driving
datasets. Finally, we present a discussion of the remaining challenges and
future works. Code has been made available at
https://github.com/asharakeh/pod_compare.git
</p>
<a href="http://arxiv.org/abs/2011.10671" target="_blank">arXiv:2011.10671</a> [<a href="http://arxiv.org/pdf/2011.10671" target="_blank">pdf</a>]

<h2>AI Governance for Businesses. (arXiv:2011.10672v1 [cs.AI])</h2>
<h3>Johannes Schneider, Rene Abraham, Christian Meske</h3>
<p>Artificial Intelligence (AI) governance regulates the exercise of authority
and control over the management of AI. It aims at leveraging AI through
effective use of data and minimization of AI-related cost and risk. While
topics such as AI governance and AI ethics are thoroughly discussed on a
theoretical, philosophical, societal and regulatory level, there is limited
work on AI governance targeted to companies and corporations. This work views
AI products as systems, where key functionality is delivered by machine
learning (ML) models leveraging (training) data. We derive a conceptual
framework by synthesizing literature on AI and related fields such as ML. Our
framework decomposes AI governance into governance of data, (ML) models and
(AI) systems along four dimensions. It relates to existing IT and data
governance frameworks and practices. It can be adopted by practitioners and
academics alike. For practitioners the synthesis of mainly research papers, but
also practitioner publications and publications of regulatory bodies provides a
valuable starting point to implement AI governance, while for academics the
paper highlights a number of areas of AI governance that deserve more
attention.
</p>
<a href="http://arxiv.org/abs/2011.10672" target="_blank">arXiv:2011.10672</a> [<a href="http://arxiv.org/pdf/2011.10672" target="_blank">pdf</a>]

<h2>An Effective Anti-Aliasing Approach for Residual Networks. (arXiv:2011.10675v1 [cs.CV])</h2>
<h3>Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin, Nicolas Le Roux, Ross Goroshin</h3>
<p>Image pre-processing in the frequency domain has traditionally played a vital
role in computer vision and was even part of the standard pipeline in the early
days of deep learning. However, with the advent of large datasets, many
practitioners concluded that this was unnecessary due to the belief that these
priors can be learned from the data itself. Frequency aliasing is a phenomenon
that may occur when sub-sampling any signal, such as an image or feature map,
causing distortion in the sub-sampled output. We show that we can mitigate this
effect by placing non-trainable blur filters and using smooth activation
functions at key locations, particularly where networks lack the capacity to
learn them. These simple architectural changes lead to substantial improvements
in out-of-distribution generalization on both image classification under
natural corruptions on ImageNet-C [10] and few-shot learning on Meta-Dataset
[17], without introducing additional trainable parameters and using the default
hyper-parameters of open source codebases.
</p>
<a href="http://arxiv.org/abs/2011.10675" target="_blank">arXiv:2011.10675</a> [<a href="http://arxiv.org/pdf/2011.10675" target="_blank">pdf</a>]

<h2>Open-Vocabulary Object Detection Using Captions. (arXiv:2011.10678v1 [cs.CV])</h2>
<h3>Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang</h3>
<p>Despite the remarkable accuracy of deep neural networks in object detection,
they are costly to train and scale due to supervision requirements.
Particularly, learning more object categories typically requires proportionally
more bounding box annotations. Weakly supervised and zero-shot learning
techniques have been explored to scale object detectors to more categories with
less supervision, but they have not been as successful and widely adopted as
supervised models. In this paper, we put forth a novel formulation of the
object detection problem, namely open-vocabulary object detection, which is
more general, more practical, and more effective than weakly supervised and
zero-shot approaches. We propose a new method to train object detectors using
bounding box annotations for a limited set of object categories, as well as
image-caption pairs that cover a larger variety of objects at a significantly
lower cost. We show that the proposed method can detect and localize objects
for which no bounding box annotation is provided during training, at a
significantly higher accuracy than zero-shot approaches. Meanwhile, objects
with bounding box annotation can be detected almost as accurately as supervised
methods, which is significantly better than weakly supervised baselines.
Accordingly, we establish a new state of the art for scalable object detection.
</p>
<a href="http://arxiv.org/abs/2011.10678" target="_blank">arXiv:2011.10678</a> [<a href="http://arxiv.org/pdf/2011.10678" target="_blank">pdf</a>]

<h2>HAWQV3: Dyadic Neural Network Quantization. (arXiv:2011.10680v1 [cs.CV])</h2>
<h3>Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, Kurt Keutzer</h3>
<p>Quantization is one of the key techniques used to make Neural Networks (NNs)
faster and more energy efficient. However, current low precision quantization
algorithms often have the hidden cost of conversion back and forth from
floating point to quantized integer values. This hidden cost limits the latency
improvement realized by quantizing NNs. To address this, we present HAWQV3, a
novel dyadic quantization framework. The contributions of HAWQV3 are the
following. (i) The entire inference process consists of only integer
multiplication, addition, and bit shifting in INT4/8 mixed precision, without
any floating point operations/casting or even integer division. (ii) We pose
the mixed-precision quantization as an integer linear programming problem,
where the bit precision setting is computed to minimize model perturbation,
while observing application specific constraints on memory footprint, latency,
and BOPS. (iii) To verify our approach, we develop the first open source 4-bit
mixed-precision quantization in TVM, and we directly deploy the quantized
models to T4 GPUs using only the Turing Tensor Cores. We observe an average
speed up of $1.45\times$ for uniform 4-bit, as compared to uniform 8-bit,
precision for ResNet50. (iv) We extensively test the proposed dyadic
quantization approach on multiple different NNs, including ResNet18/50 and
InceptionV3, for various model compression levels with/without mixed precision.
For instance, we achieve an accuracy of $78.50\%$ with dyadic INT8
quantization, which is more than $4\%$ higher than prior integer-only work for
InceptionV3. Furthermore, we show that mixed-precision INT4/8 quantization can
be used to achieve higher speed ups, as compared to INT8 inference, with
minimal impact on accuracy. For example, for ResNet50 we can reduce INT8
latency by $23\%$ with mixed precision and still achieve $76.73\%$ accuracy.
</p>
<a href="http://arxiv.org/abs/2011.10680" target="_blank">arXiv:2011.10680</a> [<a href="http://arxiv.org/pdf/2011.10680" target="_blank">pdf</a>]

<h2>SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations. (arXiv:2011.10684v1 [cs.LG])</h2>
<h3>Hao-Zhe Feng, Kezhi Kong, Minghao Chen, Tianye Zhang, Minfeng Zhu, Wei Chen</h3>
<p>Semi-supervised variational autoencoders (VAEs) have obtained strong results,
but have also encountered the challenge that good ELBO values do not always
imply accurate inference results. In this paper, we investigate and propose two
causes of this problem: (1) The ELBO objective cannot utilize the label
information directly. (2) A bottleneck value exists and continuing to optimize
ELBO after this value will not improve inference accuracy. On the basis of the
experiment results, we propose SHOT-VAE to address these problems without
introducing additional prior knowledge. The SHOT-VAE offers two contributions:
(1) A new ELBO approximation named smooth-ELBO that integrates the label
predictive loss into ELBO. (2) An approximation based on optimal interpolation
that breaks the ELBO value bottleneck by reducing the margin between ELBO and
the data likelihood. The SHOT-VAE achieves good performance with a 25.30% error
rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on
CIFAR-10 with 4k labels.
</p>
<a href="http://arxiv.org/abs/2011.10684" target="_blank">arXiv:2011.10684</a> [<a href="http://arxiv.org/pdf/2011.10684" target="_blank">pdf</a>]

<h2>HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v1 [cs.CV])</h2>
<h3>Gowri Somanath, Daniel Kurz</h3>
<p>We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.
</p>
<a href="http://arxiv.org/abs/2011.10687" target="_blank">arXiv:2011.10687</a> [<a href="http://arxiv.org/pdf/2011.10687" target="_blank">pdf</a>]

<h2>Iterative Text-based Editing of Talking-heads Using Neural Retargeting. (arXiv:2011.10688v1 [cs.CV])</h2>
<h3>Xinwei Yao, Ohad Fried, Kayvon Fatahalian, Maneesh Agrawala</h3>
<p>We present a text-based tool for editing talking-head video that enables an
iterative editing workflow. On each iteration users can edit the wording of the
speech, further refine mouth motions if necessary to reduce artifacts and
manipulate non-verbal aspects of the performance by inserting mouth gestures
(e.g. a smile) or changing the overall performance style (e.g. energetic,
mumble). Our tool requires only 2-3 minutes of the target actor video and it
synthesizes the video for each iteration in about 40 seconds, allowing users to
quickly explore many editing possibilities as they iterate. Our approach is
based on two key ideas. (1) We develop a fast phoneme search algorithm that can
quickly identify phoneme-level subsequences of the source repository video that
best match a desired edit. This enables our fast iteration loop. (2) We
leverage a large repository of video of a source actor and develop a new
self-supervised neural retargeting technique for transferring the mouth motions
of the source actor to the target actor. This allows us to work with relatively
short target actor videos, making our approach applicable in many real-world
editing scenarios. Finally, our refinement and performance controls give users
the ability to further fine-tune the synthesized results.
</p>
<a href="http://arxiv.org/abs/2011.10688" target="_blank">arXiv:2011.10688</a> [<a href="http://arxiv.org/pdf/2011.10688" target="_blank">pdf</a>]

<h2>Self-adapting Robustness in Demand Learning. (arXiv:2011.10690v1 [cs.LG])</h2>
<h3>Boxiao Chen, Selvaprabu Nadarajah, Parshan Pakiman, Stefanus Jasin</h3>
<p>We study dynamic pricing over a finite number of periods in the presence of
demand model ambiguity. Departing from the typical no-regret learning
environment, where price changes are allowed at any time, pricing decisions are
made at pre-specified points in time and each price can be applied to a large
number of arrivals. In this environment, which arises in retailing, a pricing
decision based on an incorrect demand model can significantly impact cumulative
revenue. We develop an adaptively-robust-learning (ARL) pricing policy that
learns the true model parameters from the data while actively managing demand
model ambiguity. It optimizes an objective that is robust with respect to a
self-adapting set of demand models, where a given model is included in this set
only if the sales data revealed from prior pricing decisions makes it
"probable". As a result, it gracefully transitions from being robust when
demand model ambiguity is high to minimizing regret when this ambiguity
diminishes upon receiving more data. We characterize the stochastic behavior of
ARL's self-adapting ambiguity sets and derive a regret bound that highlights
the link between the scale of revenue loss and the customer arrival pattern. We
also show that ARL, by being conscious of both model ambiguity and revenue,
bridges the gap between a distributionally robust policy and a
follow-the-leader policy, which focus on model ambiguity and revenue,
respectively. We numerically find that the ARL policy, or its extension
thereof, exhibits superior performance compared to distributionally robust,
follow-the-leader, and upper-confidence-bound policies in terms of expected
revenue and/or value at risk.
</p>
<a href="http://arxiv.org/abs/2011.10690" target="_blank">arXiv:2011.10690</a> [<a href="http://arxiv.org/pdf/2011.10690" target="_blank">pdf</a>]

<h2>Height Prediction and Refinement from Aerial Images with Semantic and Geometric Guidance. (arXiv:2011.10697v1 [cs.CV])</h2>
<h3>Elhousni Mahdi, Huang Xinming, Zhang Ziming</h3>
<p>Deep learning provides a powerful new approach to many computer vision tasks.
Height prediction from aerial images is one of those tasks that benefited
greatly from the deployment of deep learning which replaced old multi-view
geometry techniques. This letter proposes a two-stage approach, where first a
multi-task neural network is used to predict the height map resulting from a
single RGB aerial input image. We also include a second refinement step, where
a denoising autoencoder is used to produce higher quality height maps.
Experiments on two publicly available datasets show that our method is capable
of producing state-of-the-art results
</p>
<a href="http://arxiv.org/abs/2011.10697" target="_blank">arXiv:2011.10697</a> [<a href="http://arxiv.org/pdf/2011.10697" target="_blank">pdf</a>]

<h2>CancerNet-SCa: Tailored Deep Neural Network Designs for Detection of Skin Cancer from Dermoscopy Images. (arXiv:2011.10702v1 [cs.CV])</h2>
<h3>James Ren Hou Lee, Maya Pavlova, Mahmoud Famouri, Alexander Wong</h3>
<p>Skin cancer continues to be the most frequently diagnosed form of cancer in
the U.S., with not only significant effects on health and well-being but also
significant economic costs associated with treatment. A crucial step to the
treatment and management of skin cancer is effective skin cancer detection due
to strong prognosis when treated at an early stage, with one of the key
screening approaches being dermoscopy examination. Motivated by the advances of
deep learning and inspired by the open source initiatives in the research
community, in this study we introduce CancerNet-SCa, a suite of deep neural
network designs tailored for the detection of skin cancer from dermoscopy
images that is open source and available to the general public as part of the
Cancer-Net initiative. To the best of the authors' knowledge, CancerNet-SCa
comprises of the first machine-designed deep neural network architecture
designs tailored specifically for skin cancer detection, one of which
possessing a self-attention architecture design with attention condensers.
Furthermore, we investigate and audit the behaviour of CancerNet-SCa in a
responsible and transparent manner via explainability-driven model auditing.
While CancerNet-SCa is not a production-ready screening solution, the hope is
that the release of CancerNet-SCa in open source, open access form will
encourage researchers, clinicians, and citizen data scientists alike to
leverage and build upon them.
</p>
<a href="http://arxiv.org/abs/2011.10702" target="_blank">arXiv:2011.10702</a> [<a href="http://arxiv.org/pdf/2011.10702" target="_blank">pdf</a>]

<h2>Neural Group Testing to Accelerate Deep Learning. (arXiv:2011.10704v1 [cs.LG])</h2>
<h3>Weixin Liang, James Zou</h3>
<p>Recent advances in deep learning have made the use of large, deep neural
networks with tens of millions of parameters. The sheer size of these networks
imposes a challenging computational burden during inference. Existing work
focuses primarily on accelerating each forward pass of a neural network.
Inspired by the group testing strategy for efficient disease testing, we
propose neural group testing, which accelerates by testing a group of samples
in one forward pass. Groups of samples that test negative are ruled out. If a
group tests positive, samples in that group are then retested adaptively. A key
challenge of neural group testing is to modify a deep neural network so that it
could test multiple samples in one forward pass. We propose three designs to
achieve this without introducing any new parameters and evaluate their
performances. We applied neural group testing in an image moderation task to
detect rare but inappropriate images. We found that neural group testing can
group up to 16 images in one forward pass and reduce the overall computation
cost by over 73% while improving detection performance.
</p>
<a href="http://arxiv.org/abs/2011.10704" target="_blank">arXiv:2011.10704</a> [<a href="http://arxiv.org/pdf/2011.10704" target="_blank">pdf</a>]

<h2>Explainable Composition of Aggregated Assistants. (arXiv:2011.10707v1 [cs.AI])</h2>
<h3>Sarath Sreedharan, Tathagata Chakraborti, Yara Rizk, Yasaman Khazaeni</h3>
<p>A new design of an AI assistant that has become increasingly popular is that
of an "aggregated assistant" -- realized as an orchestrated composition of
several individual skills or agents that can each perform atomic tasks. In this
paper, we will talk about the role of planning in the automated composition of
such assistants and explore how concepts in automated planning can help to
establish transparency of the inner workings of the assistant to the end-user.
</p>
<a href="http://arxiv.org/abs/2011.10707" target="_blank">arXiv:2011.10707</a> [<a href="http://arxiv.org/pdf/2011.10707" target="_blank">pdf</a>]

<h2>Near-Optimal Data Source Selection for Bayesian Learning. (arXiv:2011.10712v1 [cs.LG])</h2>
<h3>Lintao Ye, Aritra Mitra, Shreyas Sundaram</h3>
<p>We study a fundamental problem in Bayesian learning, where the goal is to
select a set of data sources with minimum cost while achieving a certain
learning performance based on the data streams provided by the selected data
sources. First, we show that the data source selection problem for Bayesian
learning is NP-hard. We then show that the data source selection problem can be
transformed into an instance of the submodular set covering problem studied in
the literature, and provide a standard greedy algorithm to solve the data
source selection problem with provable performance guarantees. Next, we propose
a fast greedy algorithm that improves the running times of the standard greedy
algorithm, while achieving performance guarantees that are comparable to those
of the standard greedy algorithm. We provide insights into the performance
guarantees of the greedy algorithms by analyzing special classes of the
problem. Finally, we validate the theoretical results using numerical examples,
and show that the greedy algorithms work well in practice.
</p>
<a href="http://arxiv.org/abs/2011.10712" target="_blank">arXiv:2011.10712</a> [<a href="http://arxiv.org/pdf/2011.10712" target="_blank">pdf</a>]

<h2>Double Meta-Learning for Data Efficient Policy Optimization in Non-Stationary Environments. (arXiv:2011.10714v1 [cs.LG])</h2>
<h3>Elahe Aghapour, Nora Ayanian</h3>
<p>We are interested in learning models of non-stationary environments, which
can be framed as a multi-task learning problem. Model-free reinforcement
learning algorithms can achieve good asymptotic performance in multi-task
learning at a cost of extensive sampling, due to their approach, which requires
learning from scratch. While model-based approaches are among the most data
efficient learning algorithms, they still struggle with complex tasks and model
uncertainties. Meta-reinforcement learning addresses the efficiency and
generalization challenges on multi task learning by quickly leveraging the
meta-prior policy for a new task. In this paper, we propose a
meta-reinforcement learning approach to learn the dynamic model of a
non-stationary environment to be used for meta-policy optimization later. Due
to the sample efficiency of model-based learning methods, we are able to
simultaneously train both the meta-model of the non-stationary environment and
the meta-policy until dynamic model convergence. Then, the meta-learned dynamic
model of the environment will generate simulated data for meta-policy
optimization. Our experiment demonstrates that our proposed method can
meta-learn the policy in a non-stationary environment with the data efficiency
of model-based learning approaches while achieving the high asymptotic
performance of model-free meta-reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2011.10714" target="_blank">arXiv:2011.10714</a> [<a href="http://arxiv.org/pdf/2011.10714" target="_blank">pdf</a>]

<h2>A Worrying Analysis of Probabilistic Time-series Models for Sales Forecasting. (arXiv:2011.10715v1 [cs.LG])</h2>
<h3>Seungjae Jung, Kyung-Min Kim, Hanock Kwak, Young-Jin Park</h3>
<p>Probabilistic time-series models become popular in the forecasting field as
they help to make optimal decisions under uncertainty. Despite the growing
interest, a lack of thorough analysis hinders choosing what is worth applying
for the desired task. In this paper, we analyze the performance of three
prominent probabilistic time-series models for sales forecasting. To remove the
role of random chance in architecture's performance, we make two experimental
principles; 1) Large-scale dataset with various cross-validation sets. 2) A
standardized training and hyperparameter selection. The experimental results
show that a simple Multi-layer Perceptron and Linear Regression outperform the
probabilistic models on RMSE without any feature engineering. Overall, the
probabilistic models fail to achieve better performance on point estimation,
such as RMSE and MAPE, than comparably simple baselines. We analyze and discuss
the performances of probabilistic time-series models.
</p>
<a href="http://arxiv.org/abs/2011.10715" target="_blank">arXiv:2011.10715</a> [<a href="http://arxiv.org/pdf/2011.10715" target="_blank">pdf</a>]

<h2>Object Rearrangement Using Learned Implicit Collision Functions. (arXiv:2011.10726v1 [cs.RO])</h2>
<h3>Michael Danielczuk, Arsalan Mousavian, Clemens Eppner, Dieter Fox</h3>
<p>Robotic object rearrangement combines the skills of picking and placing
objects. When object models are unavailable, typical collision-checking models
may be unable to predict collisions in partial point clouds with occlusions,
making generation of collision-free grasping or placement trajectories
challenging. We propose a learned collision model that accepts scene and query
object point clouds and predicts collisions for 6DOF object poses within the
scene. We train the model on a synthetic set of 1 million scene/object point
cloud pairs and 2 billion collision queries. We leverage the learned collision
model as part of a model predictive path integral (MPPI) policy in a tabletop
rearrangement task and show that the policy can plan collision-free grasps and
placements for objects unseen in training in both simulated and physical
cluttered scenes with a Franka Panda robot. The learned model outperforms both
traditional pipelines and learned ablations by 9.8% in accuracy on a dataset of
simulated collision queries and is 75x faster than the best-performing
baseline. Videos and supplementary material are available at
https://sites.google.com/nvidia.com/scenecollisionnet.
</p>
<a href="http://arxiv.org/abs/2011.10726" target="_blank">arXiv:2011.10726</a> [<a href="http://arxiv.org/pdf/2011.10726" target="_blank">pdf</a>]

<h2>Stochastic Talking Face Generation Using Latent Distribution Matching. (arXiv:2011.10727v1 [cs.CV])</h2>
<h3>Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde</h3>
<p>The ability to envisage the visual of a talking face based just on hearing a
voice is a unique human capability. There have been a number of works that have
solved for this ability recently. We differ from these approaches by enabling a
variety of talking face generations based on single audio input. Indeed, just
having the ability to generate a single talking face would make a system almost
robotic in nature. In contrast, our unsupervised stochastic audio-to-video
generation model allows for diverse generations from a single audio input.
Particularly, we present an unsupervised stochastic audio-to-video generation
model that can capture multiple modes of the video distribution. We ensure that
all the diverse generations are plausible. We do so through a principled
multi-modal variational autoencoder framework. We demonstrate its efficacy on
the challenging LRW and GRID datasets and demonstrate performance better than
the baseline, while having the ability to generate multiple diverse lip
synchronized videos.
</p>
<a href="http://arxiv.org/abs/2011.10727" target="_blank">arXiv:2011.10727</a> [<a href="http://arxiv.org/pdf/2011.10727" target="_blank">pdf</a>]

<h2>Neural Network iLQR: A New Reinforcement Learning Architecture. (arXiv:2011.10737v1 [cs.LG])</h2>
<h3>Zilong Cheng, Jun Ma, Xiaoxue Zhang, Frank L. Lewis, Tong Heng Lee</h3>
<p>As a notable machine learning paradigm, the research efforts in the context
of reinforcement learning have certainly progressed leaps and bounds. When
compared with reinforcement learning methods with the given system model, the
methodology of the reinforcement learning architecture based on the unknown
model generally exhibits significantly broader universality and applicability.
In this work, a new reinforcement learning architecture is developed and
presented without the requirement of any prior knowledge of the system model,
which is termed as an approach of a "neural network iterative linear quadratic
regulator (NNiLQR)". Depending solely on measurement data, this method yields a
completely new non-parametric routine for the establishment of the optimal
policy (without the necessity of system modeling) through iterative refinements
of the neural network system. Rather importantly, this approach significantly
outperforms the classical iterative linear quadratic regulator (iLQR) method in
terms of the given objective function because of the innovative utilization of
further exploration in the methodology. As clearly indicated from the results
attained in two illustrative examples, these significant merits of the NNiLQR
method are demonstrated rather evidently.
</p>
<a href="http://arxiv.org/abs/2011.10737" target="_blank">arXiv:2011.10737</a> [<a href="http://arxiv.org/pdf/2011.10737" target="_blank">pdf</a>]

<h2>A Trace-restricted Kronecker-Factored Approximation to Natural Gradient. (arXiv:2011.10741v1 [cs.LG])</h2>
<h3>Kai-Xin Gao, Xiao-Lei Liu, Zheng-Hai Huang, Min Wang, Zidong Wang, Dachuan Xu, Fan Yu</h3>
<p>Second-order optimization methods have the ability to accelerate convergence
by modifying the gradient through the curvature matrix. There have been many
attempts to use second-order optimization methods for training deep neural
networks. Inspired by diagonal approximations and factored approximations such
as Kronecker-Factored Approximate Curvature (KFAC), we propose a new
approximation to the Fisher information matrix (FIM) called Trace-restricted
Kronecker-factored Approximate Curvature (TKFAC) in this work, which can hold
the certain trace relationship between the exact and the approximate FIM. In
TKFAC, we decompose each block of the approximate FIM as a Kronecker product of
two smaller matrices and scaled by a coefficient related to trace. We
theoretically analyze TKFAC's approximation error and give an upper bound of
it. We also propose a new damping technique for TKFAC on convolutional neural
networks to maintain the superiority of second-order optimization methods
during training. Experiments show that our method has better performance
compared with several state-of-the-art algorithms on some deep network
architectures.
</p>
<a href="http://arxiv.org/abs/2011.10741" target="_blank">arXiv:2011.10741</a> [<a href="http://arxiv.org/pdf/2011.10741" target="_blank">pdf</a>]

<h2>Semantic-Based VPS for Smartphone Localization in Challenging Urban Environments. (arXiv:2011.10743v1 [cs.RO])</h2>
<h3>Max Jwo Lem Lee, Li-Ta Hsu, Hoi-Fung Ng, Shang Lee</h3>
<p>Accurate smartphone-based outdoor localization system in deep urban canyons
are increasingly needed for various IoT applications such as augmented reality,
intelligent transportation, etc. The recently developed feature-based visual
positioning system (VPS) by Google detects edges from smartphone images to
match with pre-surveyed edges in their map database. As smart cities develop,
the building information modeling (BIM) becomes widely available, which
provides an opportunity for a new semantic-based VPS. This article proposes a
novel 3D city model and semantic-based VPS for accurate and robust pose
estimation in urban canyons where global navigation satellite system (GNSS)
tends to fail. In the offline stage, a material segmented city model is used to
generate segmented images. In the online stage, an image is taken with a
smartphone camera that provides textual information about the surrounding
environment. The approach utilizes computer vision algorithms to rectify and
hand segment between the different types of material identified in the
smartphone image. A semantic-based VPS method is then proposed to match the
segmented generated images with the segmented smartphone image. Each generated
image holds a pose that contains the latitude, longitude, altitude, yaw, pitch,
and roll. The candidate with the maximum likelihood is regarded as the precise
pose of the user. The positioning results achieves 2.0m level accuracy in
common high rise along street, 5.5m in foliage dense environment and 15.7m in
alleyway. A 45% positioning improvement to current state-of-the-art method. The
estimation of yaw achieves 2.3{\deg} level accuracy, 8 times the improvement to
smartphone IMU.
</p>
<a href="http://arxiv.org/abs/2011.10743" target="_blank">arXiv:2011.10743</a> [<a href="http://arxiv.org/pdf/2011.10743" target="_blank">pdf</a>]

<h2>Computation harvesting in road traffic dynamics. (arXiv:2011.10744v1 [cs.LG])</h2>
<h3>Hiroyasu Ando, T. Okamoto, H. Chang, T. Noguchi, Shinji Nakaoka</h3>
<p>Owing to recent advances in artificial intelligence and internet of things
(IoT) technologies, collected big data facilitates high computational
performance, while its computational resources and energy cost are large.
Moreover, data are often collected but not used. To solve these problems, we
propose a framework for a computational model that follows a natural
computational system, such as the human brain, and does not rely heavily on
electronic computers. In particular, we propose a methodology based on the
concept of `computation harvesting', which uses IoT data collected from rich
sensors and leaves most of the computational processes to real-world phenomena
as collected data. This aspect assumes that large-scale computations can be
fast and resilient. Herein, we perform prediction tasks using real-world road
traffic data to show the feasibility of computation harvesting. First, we show
that the substantial computation in traffic flow is resilient against sensor
failure and real-time traffic changes due to several combinations of harvesting
from spatiotemporal dynamics to synthesize specific patterns. Next, we show the
practicality of this method as a real-time prediction because of its low
computational cost. Finally, we show that, compared to conventional methods,
our method requires lower resources while providing a comparable performance.
</p>
<a href="http://arxiv.org/abs/2011.10744" target="_blank">arXiv:2011.10744</a> [<a href="http://arxiv.org/pdf/2011.10744" target="_blank">pdf</a>]

<h2>Emergent Road Rules In Multi-Agent Driving Environments. (arXiv:2011.10753v1 [cs.LG])</h2>
<h3>Avik Pal, Jonah Philion, Yuan-Hong Liao, Sanja Fidler</h3>
<p>For autonomous vehicles to safely share the road with human drivers,
autonomous vehicles must abide by specific "road rules" that human drivers have
agreed to follow. "Road rules" include rules that drivers are required to
follow by law -- such as the requirement that vehicles stop at red lights -- as
well as more subtle social rules -- such as the implicit designation of fast
lanes on the highway. In this paper, we provide empirical evidence that
suggests that -- instead of hard-coding road rules into self-driving algorithms
-- a scalable alternative may be to design multi-agent environments in which
road rules emerge as optimal solutions to the problem of maximizing traffic
flow. We analyze what ingredients in driving environments cause the emergence
of these road rules and find that two crucial factors are noisy perception and
agents' spatial density. We provide qualitative and quantitative evidence of
the emergence of seven social driving behaviors, ranging from obeying traffic
signals to following lanes, all of which emerge from training agents to drive
quickly to destinations without colliding. Our results add empirical support
for the social road rules that countries worldwide have agreed on for safe,
efficient driving.
</p>
<a href="http://arxiv.org/abs/2011.10753" target="_blank">arXiv:2011.10753</a> [<a href="http://arxiv.org/pdf/2011.10753" target="_blank">pdf</a>]

<h2>A Formal Approach to the Co-Design of Embodied Intelligence. (arXiv:2011.10756v1 [cs.RO])</h2>
<h3>Gioele Zardini, Dejan Milojevic, Andrea Censi, Emilio Frazzoli</h3>
<p>We consider the problem of formally co-designing embodied intelligence as a
whole, from hardware components such as chassis and sensors to software modules
such as control and perception pipelines. We propose a principled approach to
formulate and solve complex embodied intelligence co-design problems,
leveraging a monotone co-design theory. The methods we propose are intuitive
and integrate heterogeneous engineering disciplines, allowing analytical and
simulation-based modeling techniques and enabling interdisciplinarity. We
illustrate through a case study how, given a set of desired behaviors, our
framework is able to compute Pareto efficient solutions for the entire hardware
and software stack of a self-driving vehicle.
</p>
<a href="http://arxiv.org/abs/2011.10756" target="_blank">arXiv:2011.10756</a> [<a href="http://arxiv.org/pdf/2011.10756" target="_blank">pdf</a>]

<h2>Visual Recognition of Great Ape Behaviours in the Wild. (arXiv:2011.10759v1 [cs.CV])</h2>
<h3>Faizaan Sakib, Tilo Burghardt</h3>
<p>We propose a first great ape-specific visual behaviour recognition system
utilising deep learning that is capable of detecting nine core ape behaviours.
</p>
<a href="http://arxiv.org/abs/2011.10759" target="_blank">arXiv:2011.10759</a> [<a href="http://arxiv.org/pdf/2011.10759" target="_blank">pdf</a>]

<h2>One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks. (arXiv:2011.10772v1 [cs.CV])</h2>
<h3>Kemal Oksuz, Baris Can Cam, Sinan Kalkan, Emre Akbas</h3>
<p>Despite being widely used as a performance measure for visual detection
tasks, Average Precision (AP) is limited in (i) including localisation quality,
(ii) interpretability and (iii) applicability to outputs without confidence
scores. Panoptic Quality (PQ), a measure proposed for evaluating panoptic
segmentation (Kirillov et al., 2019), does not suffer from these limitations
but is limited to panoptic segmentation. In this paper, we propose Localisation
Recall Precision (LRP) Error as the performance measure for all visual
detection tasks. LRP Error, initially proposed only for object detection by
Oksuz et al. (2018), does not suffer from the aforementioned limitations and is
applicable to all visual detection tasks. We also introduce Optimal LRP (oLRP)
Error as the minimum LRP error obtained over confidence scores to evaluate
visual detectors and obtain optimal thresholds for deployment. We provide a
detailed comparative analysis of LRP with AP and PQ, and use 35
state-of-the-art visual detectors from four common visual detection tasks (i.e.
object detection, keypoint detection, instance segmentation and panoptic
segmentation) to empirically show that LRP provides richer and more
discriminative information than its counterparts. Code available at:
https://github.com/kemaloksuz/LRP-Error
</p>
<a href="http://arxiv.org/abs/2011.10772" target="_blank">arXiv:2011.10772</a> [<a href="http://arxiv.org/pdf/2011.10772" target="_blank">pdf</a>]

<h2>DmifNet:3D Shape Reconstruction Based on Dynamic Multi-Branch Information Fusion. (arXiv:2011.10776v1 [cs.CV])</h2>
<h3>Lei Li, Suping Wu</h3>
<p>3D object reconstruction from a single-view image is a long-standing
challenging problem. Previous work was difficult to accurately reconstruct 3D
shapes with a complex topology which has rich details at the edges and corners.
Moreover, previous works used synthetic data to train their network, but domain
adaptation problems occurred when tested on real data. In this paper, we
propose a Dynamic Multi-branch Information Fusion Network (DmifNet) which can
recover a high-fidelity 3D shape of arbitrary topology from a 2D image.
Specifically, we design several side branches from the intermediate layers to
make the network produce more diverse representations to improve the
generalization ability of network. In addition, we utilize DoG (Difference of
Gaussians) to extract edge geometry and corners information from input images.
Then, we use a separate side branch network to process the extracted data to
better capture edge geometry and corners feature information. Finally, we
dynamically fuse the information of all branches to gain final predicted
probability. Extensive qualitative and quantitative experiments on a
large-scale publicly available dataset demonstrate the validity and efficiency
of our method. Code and models are publicly available at
https://github.com/leilimaster/DmifNet.
</p>
<a href="http://arxiv.org/abs/2011.10776" target="_blank">arXiv:2011.10776</a> [<a href="http://arxiv.org/pdf/2011.10776" target="_blank">pdf</a>]

<h2>Chitrakar: Robotic System for Drawing Jordan Curve of Facial Portrait. (arXiv:2011.10781v1 [cs.RO])</h2>
<h3>Aniruddha Singhal, Ayush Kumar, Shivam Thukral, Deepak Raina, Swagat Kumar</h3>
<p>This paper presents a robotic system (\textit{Chitrakar}) which autonomously
converts any image of a human face to a recognizable non-self-intersecting loop
(Jordan Curve) and draws it on any planar surface. The image is processed using
Mask R-CNN for instance segmentation, Laplacian of Gaussian (LoG) for feature
enhancement and intensity-based probabilistic stippling for the image to points
conversion. These points are treated as a destination for a travelling salesman
and are connected with an optimal path which is calculated heuristically by
minimizing the total distance to be travelled. This path is converted to a
Jordan Curve in feasible time by removing intersections using a combination of
image processing, 2-opt, and Bresenham's Algorithm. The robotic system
generates $n$ instances of each image for human aesthetic judgement, out of
which the most appealing instance is selected for the final drawing. The
drawing is executed carefully by the robot's arm using trapezoidal velocity
profiles for jerk-free and fast motion. The drawing, with a decent resolution,
can be completed in less than 30 minutes which is impossible to do by hand.
This work demonstrates the use of robotics to augment humans in executing
difficult craft-work instead of replacing them altogether.
</p>
<a href="http://arxiv.org/abs/2011.10781" target="_blank">arXiv:2011.10781</a> [<a href="http://arxiv.org/pdf/2011.10781" target="_blank">pdf</a>]

<h2>Spatially Correlated Patterns in Adversarial Images. (arXiv:2011.10794v1 [cs.AI])</h2>
<h3>Nandish Chattopadhyay, Lionell Yip En Zhi, Bryan Tan Bing Xing, Anupam Chattopadhyay</h3>
<p>Adversarial attacks have proved to be the major impediment in the progress on
research towards reliable machine learning solutions. Carefully crafted
perturbations, imperceptible to human vision, can be added to images to force
misclassification by an otherwise high performing neural network. To have a
better understanding of the key contributors of such structured attacks, we
searched for and studied spatially co-located patterns in the distribution of
pixels in the input space. In this paper, we propose a framework for
segregating and isolating regions within an input image which are particularly
critical towards either classification (during inference), or adversarial
vulnerability or both. We assert that during inference, the trained model looks
at a specific region in the image, which we call Region of Importance (RoI);
and the attacker looks at a region to alter/modify, which we call Region of
Attack (RoA). The success of this approach could also be used to design a
post-hoc adversarial defence method, as illustrated by our observations. This
uses the notion of blocking out (we call neutralizing) that region of the image
which is highly vulnerable to adversarial attacks but is not important for the
task of classification. We establish the theoretical setup for formalising the
process of segregation, isolation and neutralization and substantiate it
through empirical analysis on standard benchmarking datasets. The findings
strongly indicate that mapping features into the input space preserves the
significant patterns typically observed in the feature-space while adding major
interpretability and therefore simplifies potential defensive mechanisms.
</p>
<a href="http://arxiv.org/abs/2011.10794" target="_blank">arXiv:2011.10794</a> [<a href="http://arxiv.org/pdf/2011.10794" target="_blank">pdf</a>]

<h2>Adversarial Classification: Necessary conditions and geometric flows. (arXiv:2011.10797v1 [cs.LG])</h2>
<h3>Nicolas Garcia Trillos, Ryan Murray</h3>
<p>We study a version of adversarial classification where an adversary is
empowered to corrupt data inputs up to some distance $\varepsilon$, using tools
from variational analysis. In particular, we describe necessary conditions
associated with the optimal classifier subject to such an adversary. Using the
necessary conditions, we derive a geometric evolution equation which can be
used to track the change in classification boundaries as $\varepsilon$ varies.
This evolution equation may be described as an uncoupled system of differential
equations in one dimension, or as a mean curvature type equation in higher
dimension. In one dimension we rigorously prove that one can use the initial
value problem starting from $\varepsilon=0$, which is simply the Bayes
classifier, in order to solve for the global minimizer of the adversarial
problem. Numerical examples illustrating these ideas are also presented.
</p>
<a href="http://arxiv.org/abs/2011.10797" target="_blank">arXiv:2011.10797</a> [<a href="http://arxiv.org/pdf/2011.10797" target="_blank">pdf</a>]

<h2>Deep Smartphone Sensors-WiFi Fusion for Indoor Positioning and Tracking. (arXiv:2011.10799v1 [cs.LG])</h2>
<h3>Leonid Antsfeld, Boris Chidlovskii, Emilio Sansano-Sansano</h3>
<p>We address the indoor localization problem, where the goal is to predict
user's trajectory from the data collected by their smartphone, using inertial
sensors such as accelerometer, gyroscope and magnetometer, as well as other
environment and network sensors such as barometer and WiFi. Our system
implements a deep learning based pedestrian dead reckoning (deep PDR) model
that provides a high-rate estimation of the relative position of the user.
Using Kalman Filter, we correct the PDR's drift using WiFi that provides a
prediction of the user's absolute position each time a WiFi scan is received.
Finally, we adjust Kalman Filter results with a map-free projection method that
takes into account the physical constraints of the environment (corridors,
doors, etc.) and projects the prediction on the possible walkable paths. We
test our pipeline on IPIN'19 Indoor Localization challenge dataset and
demonstrate that it improves the winner's results by 20\% using the challenge
evaluation protocol.
</p>
<a href="http://arxiv.org/abs/2011.10799" target="_blank">arXiv:2011.10799</a> [<a href="http://arxiv.org/pdf/2011.10799" target="_blank">pdf</a>]

<h2>Central and Non-central Limit Theorems arising from the Scattering Transform and its Neural Activation Generalization. (arXiv:2011.10801v1 [stat.ML])</h2>
<h3>Gi-Ren Liu, Yuan-Chung Sheu, Hau-Tieng Wu</h3>
<p>Motivated by analyzing complicated and non-stationary time series, we study a
generalization of the scattering transform (ST) that includes broad neural
activation functions, which is called neural activation ST (NAST). On the
whole, NAST is a transform that comprises a sequence of ``neural processing
units'', each of which applies a high pass filter to the input from the
previous layer followed by a composition with a nonlinear function as the
output to the next neuron. Here, the nonlinear function models how a neuron
gets excited by the input signal. In addition to showing properties like
non-expansion, horizontal translational invariability and insensitivity to
local deformation, the statistical properties of the second order NAST of a
Gaussian process with various dependence and (non-)stationarity structure and
its interaction with the chosen high pass filters and activation functions are
explored and central limit theorem (CLT) and non-CLT results are provided.
Numerical simulations are also provided. The results explain how NAST processes
complicated and non-stationary time series, and pave a way towards statistical
inference based on NAST under the non-null case.
</p>
<a href="http://arxiv.org/abs/2011.10801" target="_blank">arXiv:2011.10801</a> [<a href="http://arxiv.org/pdf/2011.10801" target="_blank">pdf</a>]

<h2>BARS: Joint Search of Cell Topology and Layout for Accurate and Efficient Binary ARchitectures. (arXiv:2011.10804v1 [cs.AI])</h2>
<h3>Tianchen Zhao, Xuefei Ning, Songyi Yang, Shuang Liang, Peng Lei, Jianfei Chen, Huazhong Yang, Yu Wang</h3>
<p>Binary Neural Networks (BNNs) have received significant attention due to
their promising efficiency. Currently, most BNN studies directly adopt
widely-used CNN architectures, which can be suboptimal for BNNs. This paper
proposes a novel Binary ARchitecture Search (BARS) flow to discover superior
binary architecture in a large design space. Specifically, we design a
two-level (Macro \&amp; Micro) search space tailored for BNNs and apply a
differentiable neural architecture search (NAS) to explore this search space
efficiently. The macro-level search space includes depth and width decisions,
which is required for better balancing the model performance and capacity. And
we also make modifications to the micro-level search space to strengthen the
information flow for BNN. A notable challenge of BNN architecture search lies
in that binary operations exacerbate the "collapse" problem of differentiable
NAS, and we incorporate various search and derive strategies to stabilize the
search process. On CIFAR-10, \method achieves $1.5\%$ higher accuracy with
$2/3$ binary Ops and $1/10$ floating-point Ops. On ImageNet, with similar
resource consumption, \method-discovered architecture achieves $3\%$ accuracy
gain than hand-crafted architectures, while removing the full-precision
downsample layer.
</p>
<a href="http://arxiv.org/abs/2011.10804" target="_blank">arXiv:2011.10804</a> [<a href="http://arxiv.org/pdf/2011.10804" target="_blank">pdf</a>]

<h2>MoNet: Motion-based Point Cloud Prediction Network. (arXiv:2011.10812v1 [cs.CV])</h2>
<h3>Fan Lu, Guang Chen, Yinlong Liu, Zhijun Li, Sanqing Qu, Tianpei Zou</h3>
<p>Predicting the future can significantly improve the safety of intelligent
vehicles, which is a key component in autonomous driving. 3D point clouds
accurately model 3D information of surrounding environment and are crucial for
intelligent vehicles to perceive the scene. Therefore, prediction of 3D point
clouds has great significance for intelligent vehicles, which can be utilized
for numerous further applications. However, due to point clouds are unordered
and unstructured, point cloud prediction is challenging and has not been deeply
explored in current literature. In this paper, we propose a novel motion-based
neural network named MoNet. The key idea of the proposed MoNet is to integrate
motion features between two consecutive point clouds into the prediction
pipeline. The introduction of motion features enables the model to more
accurately capture the variations of motion information across frames and thus
make better predictions for future motion. In addition, content features are
introduced to model the spatial content of individual point clouds. A recurrent
neural network named MotionRNN is proposed to capture the temporal correlations
of both features. Besides, we propose an attention-based motion align module to
address the problem of missing motion features in the inference pipeline.
Extensive experiments on two large scale outdoor LiDAR datasets demonstrate the
performance of the proposed MoNet. Moreover, we perform experiments on
applications using the predicted point clouds and the results indicate the
great application potential of the proposed method.
</p>
<a href="http://arxiv.org/abs/2011.10812" target="_blank">arXiv:2011.10812</a> [<a href="http://arxiv.org/pdf/2011.10812" target="_blank">pdf</a>]

<h2>Control and implementation of fluid-driven soft gripper with dynamic uncertainty of object. (arXiv:2011.10822v1 [cs.RO])</h2>
<h3>Amirhosein Alian, Mohammad Zareinejad, Heidar Ali Talebi</h3>
<p>Soft grippers, for stable grasping of objects, with high compliance could be
considered a suitable candidate for replacement of conventional rigid grippers,
and in recent years, they have been emerging exponentially in industries. Not
only are these highly adaptable grippers capable of static grasping of an
object, but also they can be utilized for performing object manipulation tasks.
Plenty of contemporary studies have been emphasizing on static grasping ability
of soft grippers. However, in this thesis, planar in-hand object manipulation
in a soft gripper which comprises a pair of soft finger is investigated.
</p>
<a href="http://arxiv.org/abs/2011.10822" target="_blank">arXiv:2011.10822</a> [<a href="http://arxiv.org/pdf/2011.10822" target="_blank">pdf</a>]

<h2>Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks. (arXiv:2011.10824v1 [cs.LG])</h2>
<h3>Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, Adish Singla</h3>
<p>We study a security threat to reinforcement learning where an attacker
poisons the learning environment to force the agent into executing a target
policy chosen by the attacker. As a victim, we consider RL agents whose
objective is to find a policy that maximizes reward in infinite-horizon problem
settings. The attacker can manipulate the rewards and the transition dynamics
in the learning environment at training-time, and is interested in doing so in
a stealthy manner. We propose an optimization framework for finding an optimal
stealthy attack for different measures of attack cost. We provide lower/upper
bounds on the attack cost, and instantiate our attacks in two settings: (i) an
offline setting where the agent is doing planning in the poisoned environment,
and (ii) an online setting where the agent is learning a policy with poisoned
feedback. Our results show that the attacker can easily succeed in teaching any
target policy to the victim under mild conditions and highlight a significant
security threat to reinforcement learning agents in practice.
</p>
<a href="http://arxiv.org/abs/2011.10824" target="_blank">arXiv:2011.10824</a> [<a href="http://arxiv.org/pdf/2011.10824" target="_blank">pdf</a>]

<h2>On the Convergence of Reinforcement Learning. (arXiv:2011.10829v1 [cs.LG])</h2>
<h3>Suman Chakravorty, Ran Wang, Mohamed Naveed Gul Mohamed</h3>
<p>We consider the problem of Reinforcement Learning for nonlinear stochastic
dynamical systems. We show that in the RL setting, there is an inherent "Curse
of Variance" in addition to Bellman's infamous "Curse of Dimensionality", in
particular, we show that the variance in the solution grows
factorial-exponentially in the order of the approximation. A fundamental
consequence is that this precludes the search for anything other than "local"
feedback solutions in RL, in order to control the explosive variance growth,
and thus, ensure accuracy. We further show that the deterministic optimal
control has a perturbation structure, in that the higher order terms do not
affect the calculation of lower order terms, which can be utilized in RL to get
accurate local solutions.
</p>
<a href="http://arxiv.org/abs/2011.10829" target="_blank">arXiv:2011.10829</a> [<a href="http://arxiv.org/pdf/2011.10829" target="_blank">pdf</a>]

<h2>Boundary-sensitive Pre-training for Temporal Localization in Videos. (arXiv:2011.10830v1 [cs.CV])</h2>
<h3>Mengmeng Xu, Juan-Manuel Perez-Rua, Victor Escorcia, Brais Martinez, Xiatian Zhu, Bernard Ghanem, Tao Xiang</h3>
<p>Many video analysis tasks require temporal localization thus detection of
content changes. However, most existing models developed for these tasks are
pre-trained on general video action classification tasks. This is because large
scale annotation of temporal boundaries in untrimmed videos is expensive.
Therefore no suitable datasets exist for temporal boundary-sensitive
pre-training. In this paper for the first time, we investigate model
pre-training for temporal localization by introducing a novel
boundary-sensitive pretext (BSP) task. Instead of relying on costly manual
annotations of temporal boundaries, we propose to synthesize temporal
boundaries in existing video action classification datasets. With the
synthesized boundaries, BSP can be simply conducted via classifying the
boundary types. This enables the learning of video representations that are
much more transferable to downstream temporal localization tasks. Extensive
experiments show that the proposed BSP is superior and complementary to the
existing action classification based pre-training counterpart, and achieves new
state-of-the-art performance on several temporal localization tasks.
</p>
<a href="http://arxiv.org/abs/2011.10830" target="_blank">arXiv:2011.10830</a> [<a href="http://arxiv.org/pdf/2011.10830" target="_blank">pdf</a>]

<h2>Deep Learning-Based Computer Vision for Real Time Intravenous Drip Infusion Monitoring. (arXiv:2011.10839v1 [cs.CV])</h2>
<h3>Nicola Giaquinto, Marco Scarpetta, Maurizio Spadavecchia, Gregorio Andria</h3>
<p>This paper explores the use of deep learning-based computer vision for
real-time monitoring of the flow in intravenous (IV) infusions. IV infusions
are among the most common therapies in hospitalized patients and, given that
both over-infusion and under-infusion can cause severe damages, monitoring the
flow rate of the fluid being administered to patients is very important for
their safety. The proposed system uses a camera to film the IV drip infusion
kit and a deep learning-based algorithm to classify acquired frames into two
different states: frames with a drop that has just begun to take shape and
frames with a well-formed drop. The alternation of these two states is used to
count drops and derive a measurement of the flow rate of the drip. The usage of
a camera as sensing element makes the proposed system safe in medical
environments and easier to be integrated into current health facilities.
Experimental results are reported in the paper that confirm the accuracy of the
system and its capability to produce real-time estimates. The proposed method
can be therefore effectively adopted to implement IV infusion monitoring and
control systems.
</p>
<a href="http://arxiv.org/abs/2011.10839" target="_blank">arXiv:2011.10839</a> [<a href="http://arxiv.org/pdf/2011.10839" target="_blank">pdf</a>]

<h2>Robust Watermarking Using Inverse Gradient Attention. (arXiv:2011.10850v1 [cs.CV])</h2>
<h3>Honglei Zhang, Hu Wang, Yidong Li, Yuanzhouhan Cao, Chunhua Shen</h3>
<p>Watermarking is the procedure of encoding desired information into an image
to resist potential noises while ensuring the embedded image has little
perceptual perturbations from the original image. Recently, with the tremendous
successes gained by deep neural networks in various fields, digital
watermarking has attracted increasing number of attentions. The neglect of
considering the pixel importance within the cover image of deep neural models
will inevitably affect the model robustness for information hiding. Targeting
at the problem, in this paper, we propose a novel deep watermarking scheme with
Inverse Gradient Attention (IGA), combing the ideas of adversarial learning and
attention mechanism to endow different importance to different pixels. With the
proposed method, the model is able to spotlight pixels with more robustness for
embedding data. Besides, from an orthogonal point of view, in order to increase
the model embedding capacity, we propose a complementary message coding module.
Empirically, extensive experiments show that the proposed model outperforms the
state-of-the-art methods on two prevalent datasets under multiple settings.
</p>
<a href="http://arxiv.org/abs/2011.10850" target="_blank">arXiv:2011.10850</a> [<a href="http://arxiv.org/pdf/2011.10850" target="_blank">pdf</a>]

<h2>Contextual Interference Reduction by Selective Fine-Tuning of Neural Networks. (arXiv:2011.10857v1 [cs.CV])</h2>
<h3>Mahdi Biparva, John Tsotsos</h3>
<p>Feature disentanglement of the foreground target objects and the background
surrounding context has not been yet fully accomplished. The lack of network
interpretability prevents advancing for feature disentanglement and better
generalization robustness. We study the role of the context on interfering with
a disentangled foreground target object representation in this work. We
hypothesize that the representation of the surrounding context is heavily tied
with the foreground object due to the dense hierarchical parametrization of
convolutional networks with under-constrained learning algorithms. Working on a
framework that benefits from the bottom-up and top-down processing paradigms,
we investigate a systematic approach to shift learned representations in
feedforward networks from the emphasis on the irrelevant context to the
foreground objects. The top-down processing provides importance maps as the
means of the network internal self-interpretation that will guide the learning
algorithm to focus on the relevant foreground regions towards achieving a more
robust representations. We define an experimental evaluation setup with the
role of context emphasized using the MNIST dataset. The experimental results
reveal not only that the label prediction accuracy is improved but also a
higher degree of robustness to the background perturbation using various noise
generation methods is obtained.
</p>
<a href="http://arxiv.org/abs/2011.10857" target="_blank">arXiv:2011.10857</a> [<a href="http://arxiv.org/pdf/2011.10857" target="_blank">pdf</a>]

<h2>Neural Network Gaussian Process Considering Input Uncertainty for Composite Structures Assembly. (arXiv:2011.10861v1 [stat.ML])</h2>
<h3>Cheolhei Lee, Jianguo Wu, Wenjia Wang, Xiaowei Yue</h3>
<p>Developing machine learning enabled smart manufacturing is promising for
composite structures assembly process. To improve production quality and
efficiency of the assembly process, accurate predictive analysis on dimensional
deviations and residual stress of the composite structures is required. The
novel composite structures assembly involves two challenges: (i) the highly
nonlinear and anisotropic properties of composite materials; and (ii)
inevitable uncertainty in the assembly process. To overcome those problems, we
propose a neural network Gaussian process model considering input uncertainty
for composite structures assembly. Deep architecture of our model allows us to
approximate a complex process better, and consideration of input uncertainty
enables robust modeling with complete incorporation of the process uncertainty.
Based on simulation and case study, the NNGPIU can outperform other benchmark
methods when the response function is nonsmooth and nonlinear. Although we use
composite structure assembly as an example, the proposed methodology can be
applicable to other engineering systems with intrinsic uncertainties.
</p>
<a href="http://arxiv.org/abs/2011.10861" target="_blank">arXiv:2011.10861</a> [<a href="http://arxiv.org/pdf/2011.10861" target="_blank">pdf</a>]

<h2>A Neuro-Inspired Autoencoding Defense Against Adversarial Perturbations. (arXiv:2011.10867v1 [cs.LG])</h2>
<h3>Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow</h3>
<p>Deep Neural Networks (DNNs) are vulnerable to adversarial attacks: carefully
constructed perturbations to an image can seriously impair classification
accuracy, while being imperceptible to humans. While there has been a
significant amount of research on defending against such attacks, most defenses
based on systematic design principles have been defeated by appropriately
modified attacks. For a fixed set of data, the most effective current defense
is to train the network using adversarially perturbed examples. In this paper,
we investigate a radically different, neuro-inspired defense mechanism,
starting from the observation that human vision is virtually unaffected by
adversarial examples designed for machines. We aim to reject L^inf bounded
adversarial perturbations before they reach a classifier DNN, using an encoder
with characteristics commonly observed in biological vision: sparse
overcomplete representations, randomness due to synaptic noise, and drastic
nonlinearities. Encoder training is unsupervised, using standard dictionary
learning. A CNN-based decoder restores the size of the encoder output to that
of the original image, enabling the use of a standard CNN for classification.
Our nominal design is to train the decoder and classifier together in standard
supervised fashion, but we also consider unsupervised decoder training based on
a regression objective (as in a conventional autoencoder) with separate
supervised training of the classifier. Unlike adversarial training, all
training is based on clean images.

Our experiments on the CIFAR-10 show performance competitive with
state-of-the-art defenses based on adversarial training, and point to the
promise of neuro-inspired techniques for the design of robust neural networks.
In addition, we provide results for a subset of the Imagenet dataset to verify
that our approach scales to larger images.
</p>
<a href="http://arxiv.org/abs/2011.10867" target="_blank">arXiv:2011.10867</a> [<a href="http://arxiv.org/pdf/2011.10867" target="_blank">pdf</a>]

<h2>Transparent Object Tracking Benchmark. (arXiv:2011.10875v1 [cs.CV])</h2>
<h3>Heng Fan, Halady Akhilesha Miththanthaya, Harshit, Siranjiv Ramana Rajan, Xiaoqiong Liu, Zhilin Zou, Yuewei Lin, Haibin Ling</h3>
<p>Visual tracking has achieved considerable progress in recent years. However,
current research in the field mainly focuses on tracking of opaque objects,
while little attention is paid to transparent object tracking. In this paper,
we make the first attempt in exploring this problem by proposing a Transparent
Object Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos
(86K frames) from 15 diverse transparent object categories. Each sequence is
manually labeled with axis-aligned bounding boxes. To the best of our
knowledge, TOTB is the first benchmark dedicated to transparent object
tracking. In order to understand how existing trackers perform and to provide
comparison for future research on TOTB, we extensively evaluate 25
state-of-the-art tracking algorithms. The evaluation results exhibit that more
efforts are needed to improve transparent object tracking. Besides, we observe
some nontrivial findings from the evaluation that are discrepant with some
common beliefs in opaque object tracking. For example, we find that deep(er)
features are not always good for improvements. Moreover, to encourage future
research, we introduce a novel tracker, named TransATOM, which leverages
transparency features for tracking and surpasses all 25 evaluated approaches by
a large margin. By releasing TOTB, we expect to facilitate future research and
application of transparent object tracking in both the academia and industry.
The TOTB and evaluation results as well as TransATOM will be made available at
https://hengfan2010.github.io/projects/TOTB.
</p>
<a href="http://arxiv.org/abs/2011.10875" target="_blank">arXiv:2011.10875</a> [<a href="http://arxiv.org/pdf/2011.10875" target="_blank">pdf</a>]

<h2>Use of Student's t-Distribution for the Latent Layer in a Coupled Variational Autoencoder. (arXiv:2011.10879v1 [cs.LG])</h2>
<h3>Kevin R. Chen, Daniel Svoboda, Kenric P. Nelson</h3>
<p>A Coupled Variational Autoencoder, which incorporates both a generalized loss
function and latent layer distribution, shows improvement in the accuracy and
robustness of generated replicas of MNIST numerals. The latent layer uses a
Student's t-distribution to incorporate heavy-tail decay. The loss function
uses a coupled logarithm, which increases the penalty on images with outlier
likelihood. The generalized mean of the generated image's likelihood is used to
measure the performance of the algorithm's decisiveness, accuracy, and
robustness.
</p>
<a href="http://arxiv.org/abs/2011.10879" target="_blank">arXiv:2011.10879</a> [<a href="http://arxiv.org/pdf/2011.10879" target="_blank">pdf</a>]

<h2>Rethinking Transformer-based Set Prediction for Object Detection. (arXiv:2011.10881v1 [cs.CV])</h2>
<h3>Zhiqing Sun, Shengcao Cao, Yiming Yang, Kris Kitani</h3>
<p>DETR is a recently proposed Transformer-based method which views object
detection as a set prediction problem and achieves state-of-the-art performance
but demands extra-long training time to converge. In this paper, we investigate
the causes of the optimization difficulty in the training of DETR. Our
examinations reveal several factors contributing to the slow convergence of
DETR, primarily the issues with the Hungarian loss and the Transformer cross
attention mechanism. To overcome these issues we propose two solutions, namely,
TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN
(Transformer-based Set Prediction with RCNN). Experimental results show that
the proposed methods not only converge much faster than the original DETR, but
also significantly outperform DETR and other baselines in terms of detection
accuracy.
</p>
<a href="http://arxiv.org/abs/2011.10881" target="_blank">arXiv:2011.10881</a> [<a href="http://arxiv.org/pdf/2011.10881" target="_blank">pdf</a>]

<h2>Zero-Shot Learning with Knowledge Enhanced Visual Semantic Embeddings. (arXiv:2011.10889v1 [cs.CV])</h2>
<h3>Karan Sikka, Jihua Huang, Andrew Silberfarb, Prateeth Nayak, Luke Rohrer, Pritish Sahu, John Byrnes, Ajay Divakaran, Richard Rohwer</h3>
<p>We improve zero-shot learning (ZSL) by incorporating common-sense knowledge
in DNNs. We propose Common-Sense based Neuro-Symbolic Loss (CSNL) that
formulates prior knowledge as novel neuro-symbolic loss functions that
regularize visual-semantic embedding. CSNL forces visual features in the VSE to
obey common-sense rules relating to hypernyms and attributes. We introduce two
key novelties for improved learning: (1) enforcement of rules for a group
instead of a single concept to take into account class-wise relationships, and
(2) confidence margins inside logical operators that enable implicit curriculum
learning and prevent premature overfitting. We evaluate the advantages of
incorporating each knowledge source and show consistent gains over prior
state-of-art methods in both conventional and generalized ZSL e.g. 11.5%, 5.5%,
and 11.6% improvements on AWA2, CUB, and Kinetics respectively.
</p>
<a href="http://arxiv.org/abs/2011.10889" target="_blank">arXiv:2011.10889</a> [<a href="http://arxiv.org/pdf/2011.10889" target="_blank">pdf</a>]

<h2>Multi-agent Deep FBSDE Representation For Large Scale Stochastic Differential Games. (arXiv:2011.10890v1 [cs.AI])</h2>
<h3>Tianrong Chen, Ziyi Wang, Ioannis Exarchos, Evangelos A. Theodorou</h3>
<p>In this paper, we present a deep learning framework for solving large-scale
multi-agent non-cooperative stochastic games using fictitious play. The
Hamilton-Jacobi-Bellman (HJB) PDE associated with each agent is reformulated
into a set of Forward-Backward Stochastic Differential Equations (FBSDEs) and
solved via forward sampling on a suitably defined neural network architecture.
Decision-making in multi-agent systems suffers from the curse of dimensionality
and strategy degeneration as the number of agents and time horizon increase. We
propose a novel Deep FBSDE controller framework which is shown to outperform
the current state-of-the-art deep fictitious play algorithm on a high
dimensional inter-bank lending/borrowing problem. More importantly, our
approach mitigates the curse of many agents and reduces computational and
memory complexity, allowing us to scale up to 1,000 agents in simulation, a
scale which, to the best of our knowledge, represents a new state of the art.
Finally, we showcase the framework's applicability in robotics on a
belief-space autonomous racing problem.
</p>
<a href="http://arxiv.org/abs/2011.10890" target="_blank">arXiv:2011.10890</a> [<a href="http://arxiv.org/pdf/2011.10890" target="_blank">pdf</a>]

<h2>Rank-smoothed Pairwise Learning In Perceptual Quality Assessment. (arXiv:2011.10893v1 [cs.CV])</h2>
<h3>Hossein Talebi, Ehsan Amid, Peyman Milanfar, Manfred K. Warmuth</h3>
<p>Conducting pairwise comparisons is a widely used approach in curating human
perceptual preference data. Typically raters are instructed to make their
choices according to a specific set of rules that address certain dimensions of
image quality and aesthetics. The outcome of this process is a dataset of
sampled image pairs with their associated empirical preference probabilities.
Training a model on these pairwise preferences is a common deep learning
approach. However, optimizing by gradient descent through mini-batch learning
means that the "global" ranking of the images is not explicitly taken into
account. In other words, each step of the gradient descent relies only on a
limited number of pairwise comparisons. In this work, we demonstrate that
regularizing the pairwise empirical probabilities with aggregated rankwise
probabilities leads to a more reliable training loss. We show that training a
deep image quality assessment model with our rank-smoothed loss consistently
improves the accuracy of predicting human preferences.
</p>
<a href="http://arxiv.org/abs/2011.10893" target="_blank">arXiv:2011.10893</a> [<a href="http://arxiv.org/pdf/2011.10893" target="_blank">pdf</a>]

<h2>Reinforcement learning with distance-based incentive/penalty (DIP) updates for highly constrained industrial control systems. (arXiv:2011.10897v1 [cs.AI])</h2>
<h3>Hyungjun Park, Daiki Min, Jong-hyun Ryu, Dong Gu Choi</h3>
<p>Typical reinforcement learning (RL) methods show limited applicability for
real-world industrial control problems because industrial systems involve
various constraints and simultaneously require continuous and discrete control.
To overcome these challenges, we devise a novel RL algorithm that enables an
agent to handle a highly constrained action space. This algorithm has two main
features. First, we devise two distance-based Q-value update schemes, incentive
update and penalty update, in a distance-based incentive/penalty update
technique to enable the agent to decide discrete and continuous actions in the
feasible region and to update the value of these types of actions. Second, we
propose a method for defining the penalty cost as a shadow price-weighted
penalty. This approach affords two advantages compared to previous methods to
efficiently induce the agent to not select an infeasible action. We apply our
algorithm to an industrial control problem, microgrid system operation, and the
experimental results demonstrate its superiority.
</p>
<a href="http://arxiv.org/abs/2011.10897" target="_blank">arXiv:2011.10897</a> [<a href="http://arxiv.org/pdf/2011.10897" target="_blank">pdf</a>]

<h2>Experimental Assessment of Human-Robot Teaming for Multi-Step Remote Manipulation with Expert Operators. (arXiv:2011.10898v1 [cs.RO])</h2>
<h3>Claudia P&#xe9;rez-D&#x27;Arpino, Rebecca P. Khurshid, Julie A. Shah</h3>
<p>Remote robot manipulation with human control enables applications where
safety and environmental constraints are adverse to humans (e.g. underwater,
space robotics and disaster response) or the complexity of the task demands
human-level cognition and dexterity (e.g. robotic surgery and manufacturing).
These systems typically use direct teleoperation at the motion level, and are
usually limited to low-DOF arms and 2D perception. Improving dexterity and
situational awareness demands new interaction and planning workflows. We
explore the use of human-robot teaming through teleautonomy with assisted
planning for remote control of a dual-arm dexterous robot for multi-step
manipulation tasks, and conduct a within-subjects experimental assessment (n=12
expert users) to compare it with other methods, resulting in the following four
conditions: (A) Direct teleoperation with imitation controller + 2D perception,
(B) Condition A + 3D perception, (C) Teleautonomy interface teleoperation + 2D
&amp; 3D perception, (D) Condition C + assisted planning. The results indicate that
this approach (D) achieves task times comparable with direct teleoperation
(A,B) while improving a number of other objective and subjective metrics,
including re-grasps, collisions, and TLX workload metrics. When compared to a
similar interface but removing the assisted planning (C), D reduces the task
time and removes a significant interaction with the level of expertise of the
operator, resulting in a performance equalizer across users.
</p>
<a href="http://arxiv.org/abs/2011.10898" target="_blank">arXiv:2011.10898</a> [<a href="http://arxiv.org/pdf/2011.10898" target="_blank">pdf</a>]

<h2>Evolving Search Space for Neural Architecture Search. (arXiv:2011.10904v1 [cs.CV])</h2>
<h3>Yuanzheng Ci, Chen Lin, Ming Sun, Boyu Chen, Hongwen Zhang, Wanli Ouyang</h3>
<p>The automation of neural architecture design has been a coveted alternative
to human experts. Recent works have small search space, which is easier to
optimize but has a limited upper bound of the optimal solution. Extra human
design is needed for those methods to propose a more suitable space with
respect to the specific task and algorithm capacity. To further enhance the
degree of automation for neural architecture search, we present a Neural
Search-space Evolution (NSE) scheme that iteratively amplifies the results from
the previous effort by maintaining an optimized search space subset. This
design minimizes the necessity of a well-designed search space. We further
extend the flexibility of obtainable architectures by introducing a learnable
multi-branch setting. By employing the proposed method, a consistent
performance gain is achieved during a progressive search over upcoming search
spaces. We achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs,
which yielded a state-of-the-art performance among previous auto-generated
architectures that do not involve knowledge distillation or weight pruning.
When the latency constraint is adopted, our result also performs better than
the previous best-performing mobile models with a 77.9% Top-1 retrain accuracy.
</p>
<a href="http://arxiv.org/abs/2011.10904" target="_blank">arXiv:2011.10904</a> [<a href="http://arxiv.org/pdf/2011.10904" target="_blank">pdf</a>]

<h2>Video SemNet: Memory-Augmented Video Semantic Network. (arXiv:2011.10909v1 [cs.CV])</h2>
<h3>Prashanth Vijayaraghavan, Deb Roy</h3>
<p>Stories are a very compelling medium to convey ideas, experiences, social and
cultural values. Narrative is a specific manifestation of the story that turns
it into knowledge for the audience. In this paper, we propose a machine
learning approach to capture the narrative elements in movies by bridging the
gap between the low-level data representations and semantic aspects of the
visual medium. We present a Memory-Augmented Video Semantic Network, called
Video SemNet, to encode the semantic descriptors and learn an embedding for the
video. The model employs two main components: (i) a neural semantic learner
that learns latent embeddings of semantic descriptors and (ii) a memory module
that retains and memorizes specific semantic patterns from the video. We
evaluate the video representations obtained from variants of our model on two
tasks: (a) genre prediction and (b) IMDB Rating prediction. We demonstrate that
our model is able to predict genres and IMDB ratings with a weighted F-1 score
of 0.72 and 0.63 respectively. The results are indicative of the
representational power of our model and the ability of such representations to
measure audience engagement.
</p>
<a href="http://arxiv.org/abs/2011.10909" target="_blank">arXiv:2011.10909</a> [<a href="http://arxiv.org/pdf/2011.10909" target="_blank">pdf</a>]

<h2>Multi-Agent Reinforcement Learning for Dynamic Routing Games: A Unified Paradigm. (arXiv:2011.10915v1 [cs.LG])</h2>
<h3>Zhenyu Shou, Xuan Di</h3>
<p>This paper aims to develop a unified paradigm that models one's learning
behavior and the system's equilibrating processes in a routing game among
atomic selfish agents. Such a paradigm can assist policymakers in devising
optimal operational and planning countermeasures under both normal and abnormal
circumstances. To this end, a multi-agent reinforcement learning (MARL)
paradigm is proposed in which each agent learns and updates her own en-route
path choice policy while interacting with others on transportation networks.
This paradigm is shown to generalize the classical notion of dynamic user
equilibrium (DUE) to model-free and data-driven scenarios. We also illustrate
that the equilibrium outcomes computed from our developed MARL paradigm
coincide with DUE and dynamic system optimal (DSO), respectively, when rewards
are set differently. In addition, with the goal to optimize some systematic
objective (e.g., overall traffic condition) of city planners, we formulate a
bilevel optimization problem with the upper level as city planners and the
lower level as a multi-agent system where each rational and selfish traveler
aims to minimize her travel cost. We demonstrate the effect of two
administrative measures, namely tolling and signal control, on the behavior of
travelers and show that the systematic objective of city planners can be
optimized by a proper control. The results show that on the Braess network, the
optimal toll charge on the central link is greater or equal to 25, with which
the average travel time of selfish agents is minimized and the emergence of
Braess paradox could be avoided. In a large-sized real-world road network with
69 nodes and 166 links, the optimal offset for signal control on Broadway is
derived as 4 seconds, with which the average travel time of all controllable
agents is minimized.
</p>
<a href="http://arxiv.org/abs/2011.10915" target="_blank">arXiv:2011.10915</a> [<a href="http://arxiv.org/pdf/2011.10915" target="_blank">pdf</a>]

<h2>Hierachical Delta-Attention Method for Multimodal Fusion. (arXiv:2011.10916v1 [cs.CV])</h2>
<h3>Kunjal Panchal</h3>
<p>In vision and linguistics; the main input modalities are facial expressions,
speech patterns, and the words uttered. The issue with analysis of any one mode
of expression (Visual, Verbal or Vocal) is that lot of contextual information
can get lost. This asks researchers to inspect multiple modalities to get a
thorough understanding of the cross-modal dependencies and temporal context of
the situation to analyze the expression. This work attempts at preserving the
long-range dependencies within and across different modalities, which would be
bottle-necked by the use of recurrent networks and adds the concept of
delta-attention to focus on local differences per modality to capture the
idiosyncrasy of different people. We explore a cross-attention fusion technique
to get the global view of the emotion expressed through these
delta-self-attended modalities, in order to fuse all the local nuances and
global context together. The addition of attention is new to the multi-modal
fusion field and currently being scrutinized for on what stage the attention
mechanism should be used, this work achieves competitive accuracy for overall
and per-class classification which is close to the current state-of-the-art
with almost half number of parameters.
</p>
<a href="http://arxiv.org/abs/2011.10916" target="_blank">arXiv:2011.10916</a> [<a href="http://arxiv.org/pdf/2011.10916" target="_blank">pdf</a>]

<h2>A Bayesian Account of Measures of Interpretability in Human-AI Interaction. (arXiv:2011.10920v1 [cs.AI])</h2>
<h3>Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, David E. Smith, Subbarao Kambhampati</h3>
<p>Existing approaches for the design of interpretable agent behavior consider
different measures of interpretability in isolation. In this paper we posit
that, in the design and deployment of human-aware agents in the real world,
notions of interpretability are just some among many considerations; and the
techniques developed in isolation lack two key properties to be useful when
considered together: they need to be able to 1) deal with their mutually
competing properties; and 2) an open world where the human is not just there to
interpret behavior in one specific form. To this end, we consider three
well-known instances of interpretable behavior studied in existing literature
-- namely, explicability, legibility, and predictability -- and propose a
revised model where all these behaviors can be meaningfully modeled together.
We will highlight interesting consequences of this unified model and motivate,
through results of a user study, why this revision is necessary.
</p>
<a href="http://arxiv.org/abs/2011.10920" target="_blank">arXiv:2011.10920</a> [<a href="http://arxiv.org/pdf/2011.10920" target="_blank">pdf</a>]

<h2>Locally Linear Embedding and its Variants: Tutorial and Survey. (arXiv:2011.10925v1 [stat.ML])</h2>
<h3>Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley</h3>
<p>This is a tutorial and survey paper for Locally Linear Embedding (LLE) and
its variants. The idea of LLE is fitting the local structure of manifold in the
embedding space. In this paper, we first cover LLE, kernel LLE, inverse LLE,
and feature fusion with LLE. Then, we cover out-of-sample embedding using
linear reconstruction, eigenfunctions, and kernel mapping. Incremental LLE is
explained for embedding streaming data. Landmark LLE methods using the Nystrom
approximation and locally linear landmarks are explained for big data
embedding. We introduce the methods for parameter selection of number of
neighbors using residual variance, Procrustes statistics, preservation
neighborhood error, and local neighborhood selection. Afterwards, Supervised
LLE (SLLE), enhanced SLLE, SLLE projection, probabilistic SLLE, supervised
guided LLE (using Hilbert-Schmidt independence criterion), and semi-supervised
LLE are explained for supervised and semi-supervised embedding. Robust LLE
methods using least squares problem and penalty functions are also introduced
for embedding in the presence of outliers and noise. Then, we introduce fusion
of LLE with other manifold learning methods including Isomap (i.e., ISOLLE),
principal component analysis, Fisher discriminant analysis, discriminant LLE,
and Isotop. Finally, we explain weighted LLE in which the distances,
reconstruction weights, or the embeddings are adjusted for better embedding; we
cover weighted LLE for deformed distributed data, weighted LLE using
probability of occurrence, SLLE by adjusting weights, modified LLE, and
iterative LLE.
</p>
<a href="http://arxiv.org/abs/2011.10925" target="_blank">arXiv:2011.10925</a> [<a href="http://arxiv.org/pdf/2011.10925" target="_blank">pdf</a>]

<h2>We don't Need Thousand Proposals$\colon$ Single Shot Actor-Action Detection in Videos. (arXiv:2011.10927v1 [cs.CV])</h2>
<h3>Aayush J Rana, Yogesh S Rawat</h3>
<p>We propose SSA2D, a simple yet effective end-to-end deep network for
actor-action detection in videos. The existing methods take a top-down approach
based on region-proposals (RPN), where the action is estimated based on the
detected proposals followed by post-processing such as non-maximal suppression.
While effective in terms of performance, these methods pose limitations in
scalability for dense video scenes with a high memory requirement for thousands
of proposals. We propose to solve this problem from a different perspective
where we don't need any proposals. SSA2D is a unified network, which performs
pixel level joint actor-action detection in a single-shot, where every pixel of
the detected actor is assigned an action label. SSA2D has two main advantages:
1) It is a fully convolutional network which does not require any proposals and
post-processing making it memory as well as time efficient, 2) It is easily
scalable to dense video scenes as its memory requirement is independent of the
number of actors present in the scene. We evaluate the proposed method on the
Actor-Action dataset (A2D) and Video Object Relation (VidOR) dataset,
demonstrating its effectiveness in multiple actors and action detection in a
video. SSA2D is 11x faster during inference with comparable (sometimes better)
performance and fewer network parameters when compared with the prior works.
</p>
<a href="http://arxiv.org/abs/2011.10927" target="_blank">arXiv:2011.10927</a> [<a href="http://arxiv.org/pdf/2011.10927" target="_blank">pdf</a>]

<h2>CORAL: Colored structural representation for bi-modal place recognition. (arXiv:2011.10934v1 [cs.CV])</h2>
<h3>Yiyuan Pan, Xuecheng Xu, Weijie Li, Yue Wang, Rong Xiong</h3>
<p>Place recognition is indispensable for drift-free localization system. Due to
the variations of the environment, place recognition using single modality has
limitations. In this paper, we propose a bi-modal place recognition method,
which can extract compound global descriptor from the two modalities, vision
and LiDAR. Specifically, we build elevation image generated from point cloud
modality as a discriminative structural representation. Based on the 3D
information, we derive the correspondences between 3D points and image pixels,
by which the pixel-wise visual features can be inserted into the elevation map
grids. In this way, we fuse the structural features and visual features in the
consistent bird-eye view frame, yielding a semantic feature representation with
sensible geometry, namely CORAL. Comparisons on the Oxford RobotCar show that
CORAL has superior performance against other state-of-the-art methods. We also
demonstrate that our network can be generalized to other scenes and sensor
configurations using cross-city datasets.
</p>
<a href="http://arxiv.org/abs/2011.10934" target="_blank">arXiv:2011.10934</a> [<a href="http://arxiv.org/pdf/2011.10934" target="_blank">pdf</a>]

<h2>Run Away From your Teacher: Understanding BYOL by a Novel Self-Supervised Approach. (arXiv:2011.10944v1 [cs.LG])</h2>
<h3>Haizhou Shi, Dongliang Luo, Siliang Tang, Jian Wang, Yueting Zhuang</h3>
<p>Recently, a newly proposed self-supervised framework Bootstrap Your Own
Latent (BYOL) seriously challenges the necessity of negative samples in
contrastive learning frameworks. BYOL works like a charm despite the fact that
it discards the negative samples completely and there is no measure to prevent
collapse in its training objective. In this paper, we suggest understanding
BYOL from the view of our proposed interpretable self-supervised learning
framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at
the same time: (i) aligning two views of the same data to similar
representations and (ii) running away from the model's Mean Teacher (MT, the
exponential moving average of the history models) instead of BYOL's running
towards it. The second term of RAFT explicitly prevents the representation
collapse and thus makes RAFT a more conceptually reliable framework. We provide
basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our
method. Furthermore, we prove that BYOL is equivalent to RAFT under certain
conditions, providing solid reasoning for BYOL's counter-intuitive success.
</p>
<a href="http://arxiv.org/abs/2011.10944" target="_blank">arXiv:2011.10944</a> [<a href="http://arxiv.org/pdf/2011.10944" target="_blank">pdf</a>]

<h2>FP-NAS: Fast Probabilistic Neural Architecture Search. (arXiv:2011.10949v1 [cs.CV])</h2>
<h3>Zhicheng Yan, Xiaoliang Dai, Peizhao Zhang, Yuandong Tian, Bichen Wu, Matt Feiszli</h3>
<p>Differential Neural Architecture Search (NAS) requires all layer choices to
be held in memory simultaneously; this limits the size of both search space and
final architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a
distribution over high-performing architectures, and uses only as much memory
as needed to train a single model. Nevertheless, it needs to sample many
architectures, making it computationally expensive for searching in an
extensive space. To solve these problems, we propose a sampling method adaptive
to the distribution entropy, drawing more samples to encourage explorations at
the beginning, and reducing samples as learning proceeds. Furthermore, to
search fast in the multi-variate space, we propose a coarse-to-fine strategy by
using a factorized distribution at the beginning which can reduce the number of
architecture parameters by over an order of magnitude.We call this method Fast
Probabilistic NAS (FP-NAS). Compared with PARSEC, it can sample 64% fewer
architectures and search 2.1x faster. Compared with FBNetV2, FP-NAS is 1.9x -
3.6x faster, and the searched models outperform FBNetV2 models on ImageNet.
FP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger
channel choices) and deeper (i.e. more blocks), while adding Split-Attention
block and enabling the search over the number of splits. When searching a model
of size 0.4G FLOPS, FP-NAS is 132x faster than EfficientNet, and the searched
FP-NAS-L0 model outperforms EfficientNet-B0 by 0.6% accuracy. Without using any
architecture surrogate or scaling tricks, we directly search large models up to
1.0G FLOPS. Our FP-NAS-L2 model with simple distillation outperforms BigNAS-XL
with advanced inplace distillation by 0.7% accuracy with less FLOPS.
</p>
<a href="http://arxiv.org/abs/2011.10949" target="_blank">arXiv:2011.10949</a> [<a href="http://arxiv.org/pdf/2011.10949" target="_blank">pdf</a>]

<h2>Towards Class-Specific Unit. (arXiv:2011.10951v1 [cs.LG])</h2>
<h3>Runkai Zheng, Zhijia Yu, Yinqi Zhang, Chris Ding, Hei Victor Cheng, Li Liu</h3>
<p>Class selectivity is an attribute of a unit in deep neural networks, which
characterizes the discriminative ability of units to a specific class.
Intuitively, decisions made by several highly selective units are more
interpretable since it is easier to be traced back to the origin while that
made by complex combinations of lowly selective units are more difficult to
interpret. In this work, we develop a novel way to directly train highly
selective units, through which we are able to examine the performance of a
network that only rely on highly selective units. Specifically, we train the
network such that all the units in the penultimate layer only response to one
specific class, which we named as class-specific unit. By innovatively
formulating the problem using mutual information, we find that in such a case,
the output of the model has a special form that all the probabilities over
non-target classes are uniformly distributed. We then propose a minimax loss
based on a game theoretic framework to achieve the goal. Nash equilibria are
proved to exist and the outcome is consistent with our regularization
objective. Experimental results show that the model trained with the proposed
objective outperforms models trained with baseline objective among all the
tasks we test. Our results shed light on the role of class-specific units by
indicating that they can be directly used for decisions without relying on low
selective units.
</p>
<a href="http://arxiv.org/abs/2011.10951" target="_blank">arXiv:2011.10951</a> [<a href="http://arxiv.org/pdf/2011.10951" target="_blank">pdf</a>]

<h2>DiaLex: A Benchmark for Evaluating Multidialectal Arabic Word Embeddings. (arXiv:2011.10970v1 [cs.AI])</h2>
<h3>Muhammad Abdul-Mageed, Shady Elbassuoni, Jad Doughman, AbdelRahim Elmadany, El Moatez Billah Nagoudi, Yorgo Zoughby, Ahmad Shaher Iskander Gaba, Ahmed Helal, Mohammed El-Razzaz</h3>
<p>Word embeddings are a core component of modern natural language processing
systems, making the ability to thoroughly evaluate them a vital task. We
describe DiaLex, a benchmark for intrinsic evaluation of dialectal Arabic word
embedding. DiaLex covers five important Arabic dialects: Algerian, Egyptian,
Lebanese, Syrian, and Tunisian. Across these dialects, DiaLex provides a
testbank for six syntactic and semantic relations, namely male to female,
singular to dual, singular to plural, antonym, comparative, and genitive to
past tense. DiaLex thus consists of a collection of word pairs representing
each of the six relations in each of the five dialects. To demonstrate the
utility of DiaLex, we use it to evaluate a set of existing and new Arabic word
embeddings that we developed. Our benchmark, evaluation code, and new word
embedding models will be publicly available.
</p>
<a href="http://arxiv.org/abs/2011.10970" target="_blank">arXiv:2011.10970</a> [<a href="http://arxiv.org/pdf/2011.10970" target="_blank">pdf</a>]

<h2>Language-guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning. (arXiv:2011.10972v1 [cs.CV])</h2>
<h3>Weixia Zhang, Chao Ma, Qi Wu, Xiaokang Yang</h3>
<p>The emerging vision-and-language navigation (VLN) problem aims at learning to
navigate an agent to the target location in unseen photo-realistic environments
according to the given language instruction. The main challenges of VLN arise
mainly from two aspects: first, the agent needs to attend to the meaningful
paragraphs of the language instruction corresponding to the dynamically-varying
visual environments; second, during the training process, the agent usually
imitate the shortest-path to the target location. Due to the discrepancy of
action selection between training and inference, the agent solely on the basis
of imitation learning does not perform well. Sampling the next action from its
predicted probability distribution during the training process allows the agent
to explore diverse routes from the environments, yielding higher success rates.
Nevertheless, without being presented with the shortest navigation paths during
the training process, the agent may arrive at the target location through an
unexpected longer route. To overcome these challenges, we design a cross-modal
grounding module, which is composed of two complementary attention mechanisms,
to equip the agent with a better ability to track the correspondence between
the textual and visual modalities. We then propose to recursively alternate the
learning schemes of imitation and exploration to narrow the discrepancy between
training and inference. We further exploit the advantages of both these two
learning schemes via adversarial learning. Extensive experimental results on
the Room-to-Room (R2R) benchmark dataset demonstrate that the proposed learning
scheme is generalized and complementary to prior arts. Our method performs well
against state-of-the-art approaches in terms of effectiveness and efficiency.
</p>
<a href="http://arxiv.org/abs/2011.10972" target="_blank">arXiv:2011.10972</a> [<a href="http://arxiv.org/pdf/2011.10972" target="_blank">pdf</a>]

<h2>Learnable Sampling 3D Convolution for Video Enhancement and Action Recognition. (arXiv:2011.10974v1 [cs.CV])</h2>
<h3>Shuyang Gu, Jianmin Bao, Dong Chen</h3>
<p>A key challenge in video enhancement and action recognition is to fuse useful
information from neighboring frames. Recent works suggest establishing accurate
correspondences between neighboring frames before fusing temporal information.
However, the generated results heavily depend on the quality of correspondence
estimation. In this paper, we propose a more robust solution: \emph{sampling
and fusing multi-level features} across neighborhood frames to generate the
results. Based on this idea, we introduce a new module to improve the
capability of 3D convolution, namely, learnable sampling 3D convolution
(\emph{LS3D-Conv}). We add learnable 2D offsets to 3D convolution which aims to
sample locations on spatial feature maps across frames. The offsets can be
learned for specific tasks. The \emph{LS3D-Conv} can flexibly replace 3D
convolution layers in existing 3D networks and get new architectures, which
learns the sampling at multiple feature levels. The experiments on video
interpolation, video super-resolution, video denoising, and action recognition
demonstrate the effectiveness of our approach.
</p>
<a href="http://arxiv.org/abs/2011.10974" target="_blank">arXiv:2011.10974</a> [<a href="http://arxiv.org/pdf/2011.10974" target="_blank">pdf</a>]

<h2>A decentralized aggregation mechanism for training deep learning models using smart contract system for bank loan prediction. (arXiv:2011.10981v1 [cs.LG])</h2>
<h3>Pratik Ratadiya, Khushi Asawa, Omkar Nikhal</h3>
<p>Data privacy and sharing has always been a critical issue when trying to
build complex deep learning-based systems to model data. Facilitation of a
decentralized approach that could take benefit from data across multiple nodes
while not needing to merge their data contents physically has been an area of
active research. In this paper, we present a solution to benefit from a
distributed data setup in the case of training deep learning architectures by
making use of a smart contract system. Specifically, we propose a mechanism
that aggregates together the intermediate representations obtained from local
ANN models over a blockchain. Training of local models takes place on their
respective data. The intermediate representations derived from them, when
combined and trained together on the host node, helps to get a more accurate
system. While federated learning primarily deals with the same features of data
where the number of samples being distributed on multiple nodes, here we are
dealing with the same number of samples but with their features being
distributed on multiple nodes. We consider the task of bank loan prediction
wherein the personal details of an individual and their bank-specific details
may not be available at the same place. Our aggregation mechanism helps to
train a model on such existing distributed data without having to share and
concatenate together the actual data values. The obtained performance, which is
better than that of individual nodes, and is at par with that of a centralized
data setup makes a strong case for extending our technique across other
architectures and tasks. The solution finds its application in organizations
that want to train deep learning models on vertically partitioned data.
</p>
<a href="http://arxiv.org/abs/2011.10981" target="_blank">arXiv:2011.10981</a> [<a href="http://arxiv.org/pdf/2011.10981" target="_blank">pdf</a>]

<h2>Stacked Graph Filter. (arXiv:2011.10988v1 [cs.LG])</h2>
<h3>Hoang NT, Takanori Maehara, Tsuyoshi Murata</h3>
<p>We study Graph Convolutional Networks (GCN) from the graph signal processing
viewpoint by addressing a difference between learning graph filters with fully
connected weights versus trainable polynomial coefficients. We find that by
stacking graph filters with learnable polynomial parameters, we can build a
highly adaptive and robust vertex classification model. Our treatment here
relaxes the low-frequency (or equivalently, high homophily) assumptions in
existing vertex classification models, resulting a more ubiquitous solution in
terms of spectral properties. Empirically, by using only one hyper-parameter
setting, our model achieves strong results on most benchmark datasets across
the frequency spectrum.
</p>
<a href="http://arxiv.org/abs/2011.10988" target="_blank">arXiv:2011.10988</a> [<a href="http://arxiv.org/pdf/2011.10988" target="_blank">pdf</a>]

<h2>Time series classification for predictive maintenance on event logs. (arXiv:2011.10996v1 [cs.LG])</h2>
<h3>Antoine Guillaume, Christel Vrain, Elloumi Wael</h3>
<p>Time series classification (TSC) gained a lot of attention in the pastdecade
and number of methods for representing and classifying time series havebeen
proposed. Nowadays, methods based on convolutional networks and
ensembletechniques represent the state of the art for time series
classification. Techniquestransforming time series to image or text also
provide reliable ways to extractmeaningful features or representations of time
series. We compare the state-of-the-art representation and classification
methods on a specific application, thatis predictive maintenance from sequences
of event logs. The contributions of thispaper are twofold: introducing a new
data set for predictive maintenance on auto-mated teller machines (ATMs) log
data and comparing the performance of differentrepresentation methods for
predicting the occurrence of a breakdown. The prob-lem is difficult since
unlike the classic case of predictive maintenance via signalsfrom sensors, we
have sequences of discrete event logs occurring at any time andthe lengths of
the sequences, corresponding to life cycles, vary a lot.
</p>
<a href="http://arxiv.org/abs/2011.10996" target="_blank">arXiv:2011.10996</a> [<a href="http://arxiv.org/pdf/2011.10996" target="_blank">pdf</a>]

<h2>Fairness-guided SMT-based Rectification of Decision Trees and Random Forests. (arXiv:2011.11001v1 [cs.LG])</h2>
<h3>Jiang Zhang, Ivan Beschastnikh, Sergey Mechtaev, Abhik Roychoudhury</h3>
<p>Data-driven decision making is gaining prominence with the popularity of
various machine learning models. Unfortunately, real-life data used in machine
learning training may capture human biases, and as a result the learned models
may lead to unfair decision making. In this paper, we provide a solution to
this problem for decision trees and random forests. Our approach converts any
decision tree or random forest into a fair one with respect to a specific data
set, fairness criteria, and sensitive attributes. The \emph{FairRepair} tool,
built based on our approach, is inspired by automated program repair techniques
for traditional programs. It uses an SMT solver to decide which paths in the
decision tree could have their outcomes flipped to improve the fairness of the
model. Our experiments on the well-known adult dataset from UC Irvine
demonstrate that FairRepair scales to realistic decision trees and random
forests. Furthermore, FairRepair provides formal guarantees about soundness and
completeness of finding a repair. Since our fairness-guided repair technique
repairs decision trees and random forests obtained from a given (unfair)
data-set, it can help to identify and rectify biases in decision-making in an
organisation.
</p>
<a href="http://arxiv.org/abs/2011.11001" target="_blank">arXiv:2011.11001</a> [<a href="http://arxiv.org/pdf/2011.11001" target="_blank">pdf</a>]

<h2>AST-GCN: Attribute-Augmented Spatiotemporal Graph Convolutional Network for Traffic Forecasting. (arXiv:2011.11004v1 [cs.LG])</h2>
<h3>Jiawei Zhu, Chao Tao, Hanhan Deng, Ling Zhao, Pu Wang, Tao Lin, Haifeng Li</h3>
<p>Traffic forecasting is a fundamental and challenging task in the field of
intelligent transportation. Accurate forecasting not only depends on the
historical traffic flow information but also needs to consider the influence of
a variety of external factors, such as weather conditions and surrounding POI
distribution. Recently, spatiotemporal models integrating graph convolutional
networks and recurrent neural networks have become traffic forecasting research
hotspots and have made significant progress. However, few works integrate
external factors. Therefore, based on the assumption that introducing external
factors can enhance the spatiotemporal accuracy in predicting traffic and
improving interpretability, we propose an attribute-augmented spatiotemporal
graph convolutional network (AST-GCN). We model the external factors as dynamic
attributes and static attributes and design an attribute-augmented unit to
encode and integrate those factors into the spatiotemporal graph convolution
model. Experiments on real datasets show the effectiveness of considering
external information on traffic forecasting tasks when compared to traditional
traffic prediction methods. Moreover, under different attribute-augmented
schemes and prediction horizon settings, the forecasting accuracy of the
AST-GCN is higher than that of the baselines.
</p>
<a href="http://arxiv.org/abs/2011.11004" target="_blank">arXiv:2011.11004</a> [<a href="http://arxiv.org/pdf/2011.11004" target="_blank">pdf</a>]

<h2>Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning. (arXiv:2011.11005v1 [cs.CV])</h2>
<h3>Xinzheng Zhang, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, Peter M. Atkinson</h3>
<p>Small area change detection from synthetic aperture radar (SAR) is a highly
challenging task. In this paper, a robust unsupervised approach is proposed for
small area change detection from multi-temporal SAR images using deep learning.
First, a multi-scale superpixel reconstruction method is developed to generate
a difference image (DI), which can suppress the speckle noise effectively and
enhance edges by exploiting local, spatially homogeneous information. Second, a
two-stage centre-constrained fuzzy c-means clustering algorithm is proposed to
divide the pixels of the DI into changed, unchanged and intermediate classes
with a parallel clustering strategy. Image patches belonging to the first two
classes are then constructed as pseudo-label training samples, and image
patches of the intermediate class are treated as testing samples. Finally, a
convolutional wavelet neural network (CWNN) is designed and trained to classify
testing samples into changed or unchanged classes, coupled with a deep
convolutional generative adversarial network (DCGAN) to increase the number of
changed class within the pseudo-label training samples. Numerical experiments
on four real SAR datasets demonstrate the validity and robustness of the
proposed approach, achieving up to 99.61% accuracy for small area change
detection.
</p>
<a href="http://arxiv.org/abs/2011.11005" target="_blank">arXiv:2011.11005</a> [<a href="http://arxiv.org/pdf/2011.11005" target="_blank">pdf</a>]

<h2>SAMA-VTOL: A new unmanned aircraft system for remotely sensed data collection. (arXiv:2011.11007v1 [cs.CV])</h2>
<h3>Mohammad R. Bayanlou, Mehdi Khoshboresh-Masouleh</h3>
<p>In recent years, unmanned aircraft systems (UASs) are frequently used in many
different applications of photogrammetry such as building damage monitoring,
archaeological mapping and vegetation monitoring. In this paper, a new
state-of-the-art vertical take-off and landing fixed-wing UAS is proposed to
robust photogrammetry missions, called SAMA-VTOL. In this study, the capability
of SAMA-VTOL is investigated for generating orthophoto. The major stages are
including designing, building and experimental scenario. First, a brief
description of design and build is introduced. Next, an experiment was done to
generate accurate orthophoto with minimum ground control points requirements.
The processing step, which includes automatic aerial triangulation with camera
calibration and model generation. In this regard, the Pix4Dmapper software was
used to orientate the images, produce point clouds, creating digital surface
model and generating orthophoto mosaic. Experimental results based on the test
area covering 26.3 hectares indicate that our SAMA-VTOL performs well in the
orthophoto mosaic task.
</p>
<a href="http://arxiv.org/abs/2011.11007" target="_blank">arXiv:2011.11007</a> [<a href="http://arxiv.org/pdf/2011.11007" target="_blank">pdf</a>]

<h2>Distributed Deep Reinforcement Learning: An Overview. (arXiv:2011.11012v1 [cs.LG])</h2>
<h3>Mohammad Reza Samsami, Hossein Alimadad</h3>
<p>Deep reinforcement learning (DRL) is a very active research area. However,
several technical and scientific issues require to be addressed, amongst which
we can mention data inefficiency, exploration-exploitation trade-off, and
multi-task learning. Therefore, distributed modifications of DRL were
introduced; agents that could be run on many machines simultaneously. In this
article, we provide a survey of the role of the distributed approaches in DRL.
We overview the state of the field, by studying the key research works that
have a significant impact on how we can use distributed methods in DRL. We
choose to overview these papers, from the perspective of distributed learning,
and not the aspect of innovations in reinforcement learning algorithms. Also,
we evaluate these methods on different tasks and compare their performance with
each other and with single actor and learner agents.
</p>
<a href="http://arxiv.org/abs/2011.11012" target="_blank">arXiv:2011.11012</a> [<a href="http://arxiv.org/pdf/2011.11012" target="_blank">pdf</a>]

<h2>Angular Embedding: A New Angular Robust Principal Component Analysis. (arXiv:2011.11013v1 [cs.LG])</h2>
<h3>Shenglan Liu, Yang Yu</h3>
<p>As a widely used method in machine learning, principal component analysis
(PCA) shows excellent properties for dimensionality reduction. It is a serious
problem that PCA is sensitive to outliers, which has been improved by numerous
Robust PCA (RPCA) versions. However, the existing state-of-the-art RPCA
approaches cannot easily remove or tolerate outliers by a non-iterative manner.
To tackle this issue, this paper proposes Angular Embedding (AE) to formulate a
straightforward RPCA approach based on angular density, which is improved for
large scale or high-dimensional data. Furthermore, a trimmed AE (TAE) is
introduced to deal with data with large scale outliers. Extensive experiments
on both synthetic and real-world datasets with vector-level or pixel-level
outliers demonstrate that the proposed AE/TAE outperforms the state-of-the-art
RPCA based methods.
</p>
<a href="http://arxiv.org/abs/2011.11013" target="_blank">arXiv:2011.11013</a> [<a href="http://arxiv.org/pdf/2011.11013" target="_blank">pdf</a>]

<h2>Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings. (arXiv:2011.11015v1 [cs.CV])</h2>
<h3>Brett D. Roads, Bradley C. Love</h3>
<p>Advances in object recognition flourished in part because of the availability
of high-quality datasets and associated benchmarks. However, these
benchmarks---such as ILSVRC---are relatively task-specific, focusing
predominately on predicting class labels. We introduce a publicly-available
dataset that embodies the task-general capabilities of human perception and
reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ)
is composed of human similarity judgments that supplement the ILSVRC validation
set. The new dataset supports a range of task and performance metrics,
including the evaluation of unsupervised learning algorithms. We demonstrate
two methods of assessment: using the similarity judgments directly and using a
psychological embedding trained on the similarity judgments. This embedding
space contains an order of magnitude more points (i.e., images) than previous
efforts based on human judgments. Scaling to the full 50,000 image set was made
possible through a selective sampling process that used variational Bayesian
inference and model ensembles to sample aspects of the embedding space that
were most uncertain. This methodological innovation not only enables scaling,
but should also improve the quality of solutions by focusing sampling where it
is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity
ratings and the embedding space to evaluate how well several popular models
conform to human similarity judgments. One finding is that more complex models
that perform better on task-specific benchmarks do not better conform to human
semantic judgments. In addition to the human similarity judgments, pre-trained
psychological embeddings and code for inferring variational embeddings are made
publicly available. Collectively, ImageNet-HSJ assets support the appraisal of
internal representations and the development of more human-like models.
</p>
<a href="http://arxiv.org/abs/2011.11015" target="_blank">arXiv:2011.11015</a> [<a href="http://arxiv.org/pdf/2011.11015" target="_blank">pdf</a>]

<h2>Interpreting Super-Resolution Networks with Local Attribution Maps. (arXiv:2011.11036v1 [cs.CV])</h2>
<h3>Jinjin Gu, Chao Dong</h3>
<p>Image super-resolution (SR) techniques have been developing rapidly,
benefiting from the invention of deep networks and its successive
breakthroughs. However, it is acknowledged that deep learning and deep neural
networks are difficult to interpret. SR networks inherit this mysterious nature
and little works make attempt to understand them. In this paper, we perform
attribution analysis of SR networks, which aims at finding the input pixels
that strongly influence the SR results. We propose a novel attribution approach
called local attribution map (LAM), which inherits the integral gradient method
yet with two unique features. One is to use the blurred image as the baseline
input, and the other is to adopt the progressive blurring function as the path
function. Based on LAM, we show that: (1) SR networks with a wider range of
involved input pixels could achieve better performance. (2) Attention networks
and non-local networks extract features from a wider range of input pixels. (3)
Comparing with the range that actually contributes, the receptive field is
large enough for most deep networks. (4) For SR networks, textures with regular
stripes or grids are more likely to be noticed, while complex semantics are
difficult to utilize. Our work opens new directions for designing SR networks
and interpreting low-level vision deep models.
</p>
<a href="http://arxiv.org/abs/2011.11036" target="_blank">arXiv:2011.11036</a> [<a href="http://arxiv.org/pdf/2011.11036" target="_blank">pdf</a>]

<h2>Efficient embedding network for 3D brain tumor segmentation. (arXiv:2011.11052v1 [cs.CV])</h2>
<h3>Hicham Messaoudi, Ahror Belaid, Mohamed Lamine Allaoui, Ahcene Zetout, Mohand Said Allili, Souhil Tliba, Douraied Ben Salem, Pierre-Henri Conze</h3>
<p>3D medical image processing with deep learning greatly suffers from a lack of
data. Thus, studies carried out in this field are limited compared to works
related to 2D natural image analysis, where very large datasets exist. As a
result, powerful and efficient 2D convolutional neural networks have been
developed and trained. In this paper, we investigate a way to transfer the
performance of a two-dimensional classiffication network for the purpose of
three-dimensional semantic segmentation of brain tumors. We propose an
asymmetric U-Net network by incorporating the EfficientNet model as part of the
encoding branch. As the input data is in 3D, the first layers of the encoder
are devoted to the reduction of the third dimension in order to fit the input
of the EfficientNet network. Experimental results on validation and test data
from the BraTS 2020 challenge demonstrate that the proposed method achieve
promising performance.
</p>
<a href="http://arxiv.org/abs/2011.11052" target="_blank">arXiv:2011.11052</a> [<a href="http://arxiv.org/pdf/2011.11052" target="_blank">pdf</a>]

<h2>Robust Gaussian Process Regression Based on Iterative Trimming. (arXiv:2011.11057v1 [cs.LG])</h2>
<h3>Zhao-Zhou Li, Lu Li, Zhengyi Shao</h3>
<p>The model prediction of the Gaussian process (GP) regression can be
significantly biased when the data are contaminated by outliers. We propose a
new robust GP regression algorithm that iteratively trims a portion of the data
points with the largest deviation from the predicted mean. While the new
algorithm retains the attractive properties of the standard GP as a
nonparametric and flexible regression method, it can significantly reduce the
influence of outliers even in some extreme cases. It is also easier to
implement than previous robust GP variants that rely on approximate inference.
Applied to various synthetic datasets with contaminations, the proposed method
outperforms the standard GP and the popular robust GP variant with the
Student's t likelihood, especially when the outlier fraction is high. Lastly,
as a practical example in the astrophysical study, we show that this method can
determine the main-sequence ridge line precisely in the color-magnitude diagram
of star clusters.
</p>
<a href="http://arxiv.org/abs/2011.11057" target="_blank">arXiv:2011.11057</a> [<a href="http://arxiv.org/pdf/2011.11057" target="_blank">pdf</a>]

<h2>Registration of serial sections: An evaluation method based on distortions of the ground truths. (arXiv:2011.11060v1 [cs.CV])</h2>
<h3>Oleg Lobachev, Takuya Funatomi, Alexander Pfaffenroth, Reinhold F&#xf6;rster, Lars Knudsen, Christoph Wrede, Michael Guthe, David Haberth&#xfc;r, Ruslan Hlushchuk, Thomas Salaets, Jaan Toelen, Simone Gaffling, Christian M&#xfc;hlfeld, Roman Grothausmann</h3>
<p>Registration of histological serial sections is a challenging task. Serial
sections exhibit distortions from sectioning. Missing information on how the
tissue looked before cutting makes a realistic validation of 2D registrations
impossible.

This work proposes methods for more realistic evaluation of registrations.
Firstly, we survey existing registration and validation efforts. Secondly, we
present a methodology to generate test data for registrations. We distort an
innately registered image stack in the manner similar to the cutting distortion
of serial sections. Test cases are generated from existing 3D data sets, thus
the ground truth is known. Thirdly, our test case generation premises
evaluation of the registrations with known ground truths. Our methodology for
such an evaluation technique distinguishes this work from other approaches.

We present a full-series evaluation across six different registration methods
applied to our distorted 3D data sets of animal lungs. Our distorted and ground
truth data sets are made publicly available.
</p>
<a href="http://arxiv.org/abs/2011.11060" target="_blank">arXiv:2011.11060</a> [<a href="http://arxiv.org/pdf/2011.11060" target="_blank">pdf</a>]

<h2>A Population-based Hybrid Approach to Hyperparameter Optimization for Neural Networks. (arXiv:2011.11062v1 [cs.LG])</h2>
<h3>Marcello Serqueira, Pedro Gonz&#xe1;lez, Eduardo Bezerra</h3>
<p>In recent years, large amounts of data have been generated, and computer
power has kept growing. This scenario has led to a resurgence in the interest
in artificial neural networks. One of the main challenges in training effective
neural network models is finding the right combination of hyperparameters to be
used. Indeed, the choice of an adequate approach to search the hyperparameter
space directly influences the accuracy of the resulting neural network model.
Common approaches for hyperparameter optimization are Grid Search, Random
Search, and Bayesian Optimization. There are also population-based methods such
as CMA-ES. In this paper, we present HBRKGA, a new population-based approach
for hyperparameter optimization. HBRKGA is a hybrid approach that combines the
Biased Random Key Genetic Algorithm with a Random Walk technique to search the
hyperparameter space efficiently. Several computational experiments on eight
different datasets were performed to assess the effectiveness of the proposed
approach. Results showed that HBRKGA could find hyperparameter configurations
that outperformed (in terms of predictive quality) the baseline methods in six
out of eight datasets while showing a reasonable execution time.
</p>
<a href="http://arxiv.org/abs/2011.11062" target="_blank">arXiv:2011.11062</a> [<a href="http://arxiv.org/pdf/2011.11062" target="_blank">pdf</a>]

<h2>Learning a Deep Generative Model like a Program: the Free Category Prior. (arXiv:2011.11063v1 [cs.LG])</h2>
<h3>Eli Sennesh</h3>
<p>Humans surpass the cognitive abilities of most other animals in our ability
to "chunk" concepts into words, and then combine the words to combine the
concepts. In this process, we make "infinite use of finite means", enabling us
to learn new concepts quickly and nest concepts within each-other. While
program induction and synthesis remain at the heart of foundational theories of
artificial intelligence, only recently has the community moved forward in
attempting to use program learning as a benchmark task itself. The cognitive
science community has thus often assumed that if the brain has simulation and
reasoning capabilities equivalent to a universal computer, then it must employ
a serialized, symbolic representation. Here we confront that assumption, and
provide a counterexample in which compositionality is expressed via network
structure: the free category prior over programs. We show how our formalism
allows neural networks to serve as primitives in probabilistic programs. We
learn both program structure and model parameters end-to-end.
</p>
<a href="http://arxiv.org/abs/2011.11063" target="_blank">arXiv:2011.11063</a> [<a href="http://arxiv.org/pdf/2011.11063" target="_blank">pdf</a>]

<h2>A Homotopy-based Algorithm for Sparse Multiple Right-hand Sides Nonnegative Least Squares. (arXiv:2011.11066v1 [cs.LG])</h2>
<h3>Nicolas Nadisic, Arnaud Vandaele, Nicolas Gillis</h3>
<p>Nonnegative least squares (NNLS) problems arise in models that rely on
additive linear combinations. In particular, they are at the core of
nonnegative matrix factorization (NMF) algorithms. The nonnegativity constraint
is known to naturally favor sparsity, that is, solutions with few non-zero
entries. However, it is often useful to further enhance this sparsity, as it
improves the interpretability of the results and helps reducing noise. While
the $\ell_0$-"norm", equal to the number of non-zeros entries in a vector, is a
natural sparsity measure, its combinatorial nature makes it difficult to use in
practical optimization schemes. Most existing approaches thus rely either on
its convex surrogate, the $\ell_1$-norm, or on heuristics such as greedy
algorithms. In the case of multiple right-hand sides NNLS (MNNLS), which are
used within NMF algorithms, sparsity is often enforced column- or row-wise, and
the fact that the solution is a matrix is not exploited. In this paper, we
first introduce a novel formulation for sparse MNNLS, with a matrix-wise
$\ell_0$ sparsity constraint. Then, we present a two-step algorithm to tackle
this problem. The first step uses a homotopy algorithm to produce the whole
regularization path for all the $\ell_1$-penalized NNLS problems arising in
MNNLS, that is, to produce a set of solutions representing different tradeoffs
between reconstruction error and sparsity. The second step selects solutions
among these paths in order to build a sparsity-constrained matrix that
minimizes the reconstruction error. We illustrate the advantages of our
proposed algorithm for the unmixing of facial and hyperspectral images.
</p>
<a href="http://arxiv.org/abs/2011.11066" target="_blank">arXiv:2011.11066</a> [<a href="http://arxiv.org/pdf/2011.11066" target="_blank">pdf</a>]

<h2>RNNP: A Robust Few-Shot Learning Approach. (arXiv:2011.11067v1 [cs.CV])</h2>
<h3>Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri</h3>
<p>Learning from a few examples is an important practical aspect of training
classifiers. Various works have examined this aspect quite well. However, all
existing approaches assume that the few examples provided are always correctly
labeled. This is a strong assumption, especially if one considers the current
techniques for labeling using crowd-based labeling services. We address this
issue by proposing a novel robust few-shot learning approach. Our method relies
on generating robust prototypes from a set of few examples. Specifically, our
method refines the class prototypes by producing hybrid features from the
support examples of each class. The refined prototypes help to classify the
query images better. Our method can replace the evaluation phase of any
few-shot learning method that uses a nearest neighbor prototype-based
evaluation procedure to make them robust. We evaluate our method on standard
mini-ImageNet and tiered-ImageNet datasets. We perform experiments with various
label corruption rates in the support examples of the few-shot classes. We
obtain significant improvement over widely used few-shot learning methods that
suffer significant performance degeneration in the presence of label noise. We
finally provide extensive ablation experiments to validate our method.
</p>
<a href="http://arxiv.org/abs/2011.11067" target="_blank">arXiv:2011.11067</a> [<a href="http://arxiv.org/pdf/2011.11067" target="_blank">pdf</a>]

<h2>QuerYD: A video dataset with high-quality textual and audio narrations. (arXiv:2011.11071v1 [cs.CV])</h2>
<h3>Andreea-Maria Oncescu, J&#xf5;ao F. Henriques, Yang Liu, Andrew Zisserman, Samuel Albanie</h3>
<p>We introduce QuerYD, a new large-scale dataset for retrieval and event
localisation in video. A unique feature of our dataset is the availability of
two audio tracks for each video: the original audio, and a high-quality spoken
description of the visual content. The dataset is based on YouDescribe, a
volunteer project that assists visually-impaired people by attaching voiced
narrations to existing YouTube videos. This ever-growing collection of videos
contains highly detailed, temporally aligned audio and text annotations. The
content descriptions are more relevant than dialogue, and more detailed than
previous description attempts, which can be observed to contain many
superficial or uninformative descriptions. To demonstrate the utility of the
QuerYD dataset, we show that it can be used to train and benchmark strong
models for retrieval and event localisation. All data, code and models will be
made available, and we hope that QuerYD inspires further research on video
understanding with written and spoken natural language.
</p>
<a href="http://arxiv.org/abs/2011.11071" target="_blank">arXiv:2011.11071</a> [<a href="http://arxiv.org/pdf/2011.11071" target="_blank">pdf</a>]

<h2>End-to-End Differentiable 6DoF Object Pose Estimation with Local and Global Constraints. (arXiv:2011.11078v1 [cs.CV])</h2>
<h3>Anshul Gupta, Joydeep Medhi, Aratrik Chattopadhyay, Vikram Gupta</h3>
<p>Inferring the 6DoF pose of an object from a single RGB image is an important
but challenging task, especially under heavy occlusion. While recent approaches
improve upon the two stage approaches by training an end-to-end pipeline, they
do not leverage local and global constraints. In this paper, we propose
pairwise feature extraction to integrate local constraints, and triplet
regularization to integrate global constraints for improved 6DoF object pose
estimation. Coupled with better augmentation, our approach achieves state of
the art results on the challenging Occlusion Linemod dataset, with a 9%
improvement over the previous state of the art, and achieves competitive
results on the Linemod dataset.
</p>
<a href="http://arxiv.org/abs/2011.11078" target="_blank">arXiv:2011.11078</a> [<a href="http://arxiv.org/pdf/2011.11078" target="_blank">pdf</a>]

<h2>Deep learning model trained on mobile phone-acquired frozen section images effectively detects basal cell carcinoma. (arXiv:2011.11081v1 [cs.CV])</h2>
<h3>Junli Cao, B.S., Junyan Wu, M.S., Jing W. Zhang, M.D., Ph.D., Jay J. Ye, M.D., Ph.D., Limin Yu, M.D., M.S</h3>
<p>Background: Margin assessment of basal cell carcinoma using the frozen
section is a common task of pathology intraoperative consultation. Although
frequently straight-forward, the determination of the presence or absence of
basal cell carcinoma on the tissue sections can sometimes be challenging. We
explore if a deep learning model trained on mobile phone-acquired frozen
section images can have adequate performance for future deployment. Materials
and Methods: One thousand two hundred and forty-one (1241) images of frozen
sections performed for basal cell carcinoma margin status were acquired using
mobile phones. The photos were taken at 100x magnification (10x objective). The
images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel
resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone
was used for model training. Results: The model uses an image as input and
produces a 2-dimensional black and white output of prediction of the same
dimension; the areas determined to be basal cell carcinoma were displayed with
white color, in a black background. Any output with the number of white pixels
exceeding 0.5% of the total number of pixels is deemed positive for basal cell
carcinoma. On the test set, the model achieves area under curve of 0.99 for
receiver operator curve and 0.97 for precision-recall curve at the pixel level.
The accuracy of classification at the slide level is 96%. Conclusions: The deep
learning model trained with mobile phone images shows satisfactory performance
characteristics, and thus demonstrates the potential for deploying as a mobile
phone app to assist in frozen section interpretation in real time.
</p>
<a href="http://arxiv.org/abs/2011.11081" target="_blank">arXiv:2011.11081</a> [<a href="http://arxiv.org/pdf/2011.11081" target="_blank">pdf</a>]

<h2>Dense open-set recognition with synthetic outliers generated by Real NVP. (arXiv:2011.11094v1 [cs.CV])</h2>
<h3>Matej Grci&#x107;, Petra Bevandi&#x107;, Sini&#x161;a &#x160;egvi&#x107;</h3>
<p>Today's deep models are often unable to detect inputs which do not belong to
the training distribution. This gives rise to confident incorrect predictions
which could lead to devastating consequences in many important application
fields such as healthcare and autonomous driving. Interestingly, both
discriminative and generative models appear to be equally affected.
Consequently, this vulnerability represents an important research challenge. We
consider an outlier detection approach based on discriminative training with
jointly learned synthetic outliers. We obtain the synthetic outliers by
sampling an RNVP model which is jointly trained to generate datapoints at the
border of the training distribution. We show that this approach can be adapted
for simultaneous semantic segmentation and dense outlier detection. We present
image classification experiments on CIFAR-10, as well as semantic segmentation
experiments on three existing datasets (StreetHazards, WD-Pascal, Fishyscapes
Lost &amp; Found), and one contributed dataset. Our models perform competitively
with respect to the state of the art despite producing predictions with only
one forward pass.
</p>
<a href="http://arxiv.org/abs/2011.11094" target="_blank">arXiv:2011.11094</a> [<a href="http://arxiv.org/pdf/2011.11094" target="_blank">pdf</a>]

<h2>A non-autonomous equation discovery method for time signal classification. (arXiv:2011.11096v1 [stat.ML])</h2>
<h3>Ryeongkyung Yoon, Harish S. Bhat, Braxton Osting</h3>
<p>Certain neural network architectures, in the infinite-layer limit, lead to
systems of nonlinear differential equations. Motivated by this idea, we develop
a framework for analyzing time signals based on non-autonomous dynamical
equations. We view the time signal as a forcing function for a dynamical system
that governs a time-evolving hidden variable. As in equation discovery, the
dynamical system is represented using a dictionary of functions and the
coefficients are learned from data. This framework is applied to the time
signal classification problem. We show how gradients can be efficiently
computed using the adjoint method, and we apply methods from dynamical systems
to establish stability of the classifier. Through a variety of experiments, on
both synthetic and real datasets, we show that the proposed method uses orders
of magnitude fewer parameters than competing methods, while achieving
comparable accuracy. We created the synthetic datasets using dynamical systems
of increasing complexity; though the ground truth vector fields are often
polynomials, we find consistently that a Fourier dictionary yields the best
results. We also demonstrate how the proposed method yields graphical
interpretability in the form of phase portraits.
</p>
<a href="http://arxiv.org/abs/2011.11096" target="_blank">arXiv:2011.11096</a> [<a href="http://arxiv.org/pdf/2011.11096" target="_blank">pdf</a>]

<h2>Model Predictive Control for Micro Aerial Vehicles: A Survey. (arXiv:2011.11104v1 [cs.RO])</h2>
<h3>Huan Nguyen, Mina Kamel, Kostas Alexis, Roland Siegwart</h3>
<p>This paper presents a review of the design and application of model
predictive control strategies for Micro Aerial Vehicles and specifically
multirotor configurations such as quadrotors. The diverse set of works in the
domain is organized based on the control law being optimized over linear or
nonlinear dynamics, the integration of state and input constraints, possible
fault-tolerant design, if reinforcement learning methods have been utilized and
if the controller refers to free-flight or other tasks such as physical
interaction or load transportation. A selected set of comparison results are
also presented and serve to provide insight for the selection between linear
and nonlinear schemes, the tuning of the prediction horizon, the importance of
disturbance observer-based offset-free tracking and the intrinsic robustness of
such methods to parameter uncertainty. Furthermore, an overview of recent
research trends on the combined application of modern deep reinforcement
learning techniques and model predictive control for multirotor vehicles is
presented. Finally, this review concludes with explicit discussion regarding
selected open-source software packages that deliver off-the-shelf model
predictive control functionality applicable to a wide variety of Micro Aerial
Vehicle configurations.
</p>
<a href="http://arxiv.org/abs/2011.11104" target="_blank">arXiv:2011.11104</a> [<a href="http://arxiv.org/pdf/2011.11104" target="_blank">pdf</a>]

<h2>Multiresolution Knowledge Distillation for Anomaly Detection. (arXiv:2011.11108v1 [cs.CV])</h2>
<h3>Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad Hossein Rohban, Hamid R. Rabiee</h3>
<p>Unsupervised representation learning has proved to be a critical component of
anomaly detection/localization in images. The challenges to learn such a
representation are two-fold. Firstly, the sample size is not often large enough
to learn a rich generalizable representation through conventional techniques.
Secondly, while only normal samples are available at training, the learned
features should be discriminative of normal and anomalous samples. Here, we
propose to use the "distillation" of features at various layers of an expert
network, pre-trained on ImageNet, into a simpler cloner network to tackle both
issues. We detect and localize anomalies using the discrepancy between the
expert and cloner networks' intermediate activation values given the input
data. We show that considering multiple intermediate hints in distillation
leads to better exploiting the expert's knowledge and more distinctive
discrepancy compared to solely utilizing the last layer activation values.
Notably, previous methods either fail in precise anomaly localization or need
expensive region-based training. In contrast, with no need for any special or
intensive training procedure, we incorporate interpretability algorithms in our
novel framework for the localization of anomalous regions. Despite the striking
contrast between some test datasets and ImageNet, we achieve competitive or
significantly superior results compared to the SOTA methods on MNIST, F-MNIST,
CIFAR-10, MVTecAD, Retinal-OCT, and two Medical datasets on both anomaly
detection and localization.
</p>
<a href="http://arxiv.org/abs/2011.11108" target="_blank">arXiv:2011.11108</a> [<a href="http://arxiv.org/pdf/2011.11108" target="_blank">pdf</a>]

<h2>Online Orthogonal Matching Pursuit. (arXiv:2011.11117v1 [stat.ML])</h2>
<h3>El Mehdi Saad, Gilles Blanchard, Sylvain Arlot</h3>
<p>Greedy algorithms for feature selection are widely used for recovering sparse
high-dimensional vectors in linear models. In classical procedures, the main
emphasis was put on the sample complexity, with little or no consideration of
the computation resources required. We present a novel online algorithm: Online
Orthogonal Matching Pursuit (OOMP) for online support recovery in the random
design setting of sparse linear regression. Our procedure selects features
sequentially, alternating between allocation of samples only as needed to
candidate features, and optimization over the selected set of variables to
estimate the regression coefficients. Theoretical guarantees about the output
of this algorithm are proven and its computational complexity is analysed.
</p>
<a href="http://arxiv.org/abs/2011.11117" target="_blank">arXiv:2011.11117</a> [<a href="http://arxiv.org/pdf/2011.11117" target="_blank">pdf</a>]

<h2>Uncorrelated Semi-paired Subspace Learning. (arXiv:2011.11124v1 [cs.LG])</h2>
<h3>Li Wang, Lei-Hong Zhang, Chungen Shen, Ren-Cang Li</h3>
<p>Multi-view datasets are increasingly collected in many real-world
applications, and we have seen better learning performance by existing
multi-view learning methods than by conventional single-view learning methods
applied to each view individually. But, most of these multi-view learning
methods are built on the assumption that at each instance no view is missing
and all data points from all views must be perfectly paired. Hence they cannot
handle unpaired data but ignore them completely from their learning process.
However, unpaired data can be more abundant in reality than paired ones and
simply ignoring all unpaired data incur tremendous waste in resources. In this
paper, we focus on learning uncorrelated features by semi-paired subspace
learning, motivated by many existing works that show great successes of
learning uncorrelated features. Specifically, we propose a generalized
uncorrelated multi-view subspace learning framework, which can naturally
integrate many proven learning criteria on the semi-paired data. To showcase
the flexibility of the framework, we instantiate five new semi-paired models
for both unsupervised and semi-supervised learning. We also design a successive
alternating approximation (SAA) method to solve the resulting optimization
problem and the method can be combined with the powerful Krylov subspace
projection technique if needed. Extensive experimental results on multi-view
feature extraction and multi-modality classification show that our proposed
models perform competitively to or better than the baselines.
</p>
<a href="http://arxiv.org/abs/2011.11124" target="_blank">arXiv:2011.11124</a> [<a href="http://arxiv.org/pdf/2011.11124" target="_blank">pdf</a>]

<h2>Predictive process mining by network of classifiers and clusterers: the PEDF model. (arXiv:2011.11136v1 [cs.LG])</h2>
<h3>Amir Mohammad Esmaieeli Sikaroudi, Md Habibor Rahman</h3>
<p>In this research, a model is proposed to learn from event log and predict
future events of a system. The proposed PEDF model learns based on events'
sequences, durations, and extra features. The PEDF model is built by a network
made of standard clusterers and classifiers, and it has high flexibility to
update the model iteratively. The model requires to extract two sets of data
from log files i.e., transition differences, and cumulative features. The model
has one layer of memory which means that each transition is dependent on both
the current event and the previous event. To evaluate the performance of the
proposed model, it is compared to the Recurrent Neural Network and Sequential
Prediction models, and it outperforms them. Since there is missing performance
measure for event log prediction models, three measures are proposed.
</p>
<a href="http://arxiv.org/abs/2011.11136" target="_blank">arXiv:2011.11136</a> [<a href="http://arxiv.org/pdf/2011.11136" target="_blank">pdf</a>]

<h2>On the Convergence of Continuous Constrained Optimization for Structure Learning. (arXiv:2011.11150v1 [cs.LG])</h2>
<h3>Ignavier Ng, S&#xe9;bastien Lachapelle, Nan Rosemary Ke, Simon Lacoste-Julien</h3>
<p>Structure learning of directed acyclic graphs (DAGs) is a fundamental problem
in many scientific endeavors. A new line of work, based on NOTEARS (Zheng et
al., 2018), reformulates the structure learning problem as a continuous
optimization one by leveraging an algebraic characterization of DAG constraint.
The constrained problem is typically solved using the augmented Lagrangian
method (ALM) which is often preferred to the quadratic penalty method (QPM) by
virtue of its convergence result that does not require the penalty coefficient
to go to infinity, hence avoiding ill-conditioning. In this work, we review the
standard convergence result of the ALM and show that the required conditions
are not satisfied in the recent continuous constrained formulation for learning
DAGs. We demonstrate empirically that its behavior is akin to that of the QPM
which is prone to ill-conditioning, thus motivating the use of second-order
method in this setting. We also establish the convergence guarantee of QPM to a
DAG solution, under mild conditions, based on a property of the DAG constraint
term.
</p>
<a href="http://arxiv.org/abs/2011.11150" target="_blank">arXiv:2011.11150</a> [<a href="http://arxiv.org/pdf/2011.11150" target="_blank">pdf</a>]

<h2>LaHAR: Latent Human Activity Recognition using LDA. (arXiv:2011.11151v1 [cs.LG])</h2>
<h3>Zeyd Boukhers, Danniene Wete, Steffen Staab</h3>
<p>Processing sequential multi-sensor data becomes important in many tasks due
to the dramatic increase in the availability of sensors that can acquire
sequential data over time. Human Activity Recognition (HAR) is one of the
fields which are actively benefiting from this availability. Unlike most of the
approaches addressing HAR by considering predefined activity classes, this
paper proposes a novel approach to discover the latent HAR patterns in
sequential data. To this end, we employed Latent Dirichlet Allocation (LDA),
which is initially a topic modelling approach used in text analysis. To make
the data suitable for LDA, we extract the so-called "sensory words" from the
sequential data. We carried out experiments on a challenging HAR dataset,
demonstrating that LDA is capable of uncovering underlying structures in
sequential data, which provide a human-understandable representation of the
data. The extrinsic evaluations reveal that LDA is capable of accurately
clustering HAR data sequences compared to the labelled activities.
</p>
<a href="http://arxiv.org/abs/2011.11151" target="_blank">arXiv:2011.11151</a> [<a href="http://arxiv.org/pdf/2011.11151" target="_blank">pdf</a>]

<h2>Stable Weight Decay Regularization. (arXiv:2011.11152v1 [cs.LG])</h2>
<h3>Zeke Xie, Issei Sato, Masashi Sugiyama</h3>
<p>Weight decay is a popular regularization technique for training of deep
neural networks. Modern deep learning libraries mainly use $L_{2}$
regularization as the default implementation of weight decay.
\citet{loshchilov2018decoupled} demonstrated that $L_{2}$ regularization is not
identical to weight decay for adaptive gradient methods, such as Adaptive
Momentum Estimation (Adam), and proposed Adam with Decoupled Weight Decay
(AdamW). However, we found that the popular implementations of weight decay,
including $L_{2}$ regularization and decoupled weight decay, in modern deep
learning libraries usually damage performance. First, the $L_{2}$
regularization is unstable weight decay for all optimizers that use Momentum,
such as stochastic gradient descent (SGD). Second, decoupled weight decay is
highly unstable for all adaptive gradient methods. We further propose the
Stable Weight Decay (SWD) method to fix the unstable weight decay problem from
a dynamical perspective. The proposed SWD method makes significant improvements
over $L_{2}$ regularization and decoupled weight decay in our experiments.
Simply fixing weight decay in Adam by SWD, with no extra hyperparameter, can
usually outperform complex Adam variants, which have more hyperparameters.
</p>
<a href="http://arxiv.org/abs/2011.11152" target="_blank">arXiv:2011.11152</a> [<a href="http://arxiv.org/pdf/2011.11152" target="_blank">pdf</a>]

<h2>Imbalance Robust Softmax for Deep Embeeding Learning. (arXiv:2011.11155v1 [cs.CV])</h2>
<h3>Hao Zhu, Yang Yuan, Guosheng Hu, Xiang Wu, Neil Robertson</h3>
<p>Deep embedding learning is expected to learn a metric space in which features
have smaller maximal intra-class distance than minimal inter-class distance. In
recent years, one research focus is to solve the open-set problem by
discriminative deep embedding learning in the field of face recognition (FR)
and person re-identification (re-ID). Apart from open-set problem, we find that
imbalanced training data is another main factor causing the performance
degradation of FR and re-ID, and data imbalance widely exists in the real
applications. However, very little research explores why and how data imbalance
influences the performance of FR and re-ID with softmax or its variants. In
this work, we deeply investigate data imbalance in the perspective of neural
network optimisation and feature distribution about softmax. We find one main
reason of performance degradation caused by data imbalance is that the weights
(from the penultimate fully-connected layer) are far from their class centers
in feature space. Based on this investigation, we propose a unified framework,
Imbalance-Robust Softmax (IR-Softmax), which can simultaneously solve the
open-set problem and reduce the influence of data imbalance. IR-Softmax can
generalise to any softmax and its variants (which are discriminative for
open-set problem) by directly setting the weights as their class centers,
naturally solving the data imbalance problem. In this work, we explicitly
re-formulate two discriminative softmax (A-Softmax and AM-Softmax) under the
framework of IR-Softmax. We conduct extensive experiments on FR databases (LFW,
MegaFace) and re-ID database (Market-1501, Duke), and IR-Softmax outperforms
many state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.11155" target="_blank">arXiv:2011.11155</a> [<a href="http://arxiv.org/pdf/2011.11155" target="_blank">pdf</a>]

<h2>When and Why Test-Time Augmentation Works. (arXiv:2011.11156v1 [cs.CV])</h2>
<h3>Divya Shanmugam, Davis Blalock, Guha Balakrishnan, John Guttag</h3>
<p>Test-time augmentation (TTA)---the aggregation of predictions across
transformed versions of a test input---is a common practice in image
classification. In this paper, we present theoretical and experimental analyses
that shed light on 1) when test time augmentation is likely to be helpful and
2) when to use various test-time augmentation policies. A key finding is that
even when TTA produces a net improvement in accuracy, it can change many
correct predictions into incorrect predictions. We delve into when and why
test-time augmentation changes a prediction from being correct to incorrect and
vice versa. Our analysis suggests that the nature and amount of training data,
the model architecture, and the augmentation policy all matter. Building on
these insights, we present a learning-based method for aggregating test-time
augmentations. Experiments across a diverse set of models, datasets, and
augmentations show that our method delivers consistent improvements over
existing approaches.
</p>
<a href="http://arxiv.org/abs/2011.11156" target="_blank">arXiv:2011.11156</a> [<a href="http://arxiv.org/pdf/2011.11156" target="_blank">pdf</a>]

<h2>LINDT: Tackling Negative Federated Learning with Local Adaptation. (arXiv:2011.11160v1 [cs.LG])</h2>
<h3>Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu</h3>
<p>Federated Learning (FL) is a promising distributed learning paradigm, which
allows a number of data owners (also called clients) to collaboratively learn a
shared model without disclosing each client's data. However, FL may fail to
proceed properly, amid a state that we call negative federated learning (NFL).
This paper addresses the problem of negative federated learning. We formulate a
rigorous definition of NFL and analyze its essential cause. We propose a novel
framework called LINDT for tackling NFL in run-time. The framework can
potentially work with any neural-network-based FL systems for NFL detection and
recovery. Specifically, we introduce a metric for detecting NFL from the
server. On occasion of NFL recovery, the framework makes adaptation to the
federated model on each client's local data by learning a Layer-wise
Intertwined Dual-model. Experiment results show that the proposed approach can
significantly improve the performance of FL on local data in various scenarios
of NFL.
</p>
<a href="http://arxiv.org/abs/2011.11160" target="_blank">arXiv:2011.11160</a> [<a href="http://arxiv.org/pdf/2011.11160" target="_blank">pdf</a>]

<h2>Learnable Boundary Guided Adversarial Training. (arXiv:2011.11164v1 [cs.CV])</h2>
<h3>Jiequan Cui, Shu Liu, Liwei Wang, Jiaya Jia</h3>
<p>Previous adversarial training raises model robustness under the compromise of
accuracy on natural data. In this paper, our target is to reduce natural
accuracy degradation. We use the model logits from one clean model
$\mathcal{M}^{natural}$ to guide learning of the robust model
$\mathcal{M}^{robust}$, taking into consideration that logits from the well
trained clean model $\mathcal{M}^{natural}$ embed the most discriminative
features of natural data, {\it e.g.}, generalizable classifier boundary. Our
solution is to constrain logits from the robust model $\mathcal{M}^{robust}$
that takes adversarial examples as input and make it similar to those from a
clean model $\mathcal{M}^{natural}$ fed with corresponding natural data. It
lets $\mathcal{M}^{robust}$ inherit the classifier boundary of
$\mathcal{M}^{natural}$. Thus, we name our method Boundary Guided Adversarial
Training (BGAT). Moreover, we generalize BGAT to Learnable Boundary Guided
Adversarial Training (LBGAT) by training $\mathcal{M}^{natural}$ and
$\mathcal{M}^{robust}$ simultaneously and collaboratively to learn one most
robustness-friendly classifier boundary for the strongest robustness. Extensive
experiments are conducted on CIFAR-10, CIFAR-100, and challenging Tiny ImageNet
datasets. Along with other state-of-the-art adversarial training approaches,
{\it e.g.}, Adversarial Logit Pairing (ALP) and TRADES, the performance is
further enhanced.
</p>
<a href="http://arxiv.org/abs/2011.11164" target="_blank">arXiv:2011.11164</a> [<a href="http://arxiv.org/pdf/2011.11164" target="_blank">pdf</a>]

<h2>The Selectivity and Competition of the Mind's Eye in Visual Perception. (arXiv:2011.11167v1 [cs.CV])</h2>
<h3>Edward Kim, Maryam Daniali, Jocelyn Rego, Garrett T. Kenyon</h3>
<p>Research has shown that neurons within the brain are selective to certain
stimuli. For example, the fusiform face area (FFA) region is known by
neuroscientists to selectively activate when people see faces over non-face
objects. However, the mechanisms by which the primary visual system directs
information to the correct higher levels of the brain are currently unknown. In
our work, we advance the understanding of the neural mechanisms of perception
by creating a novel computational model that incorporates lateral and top down
feedback in the form of hierarchical competition. We show that these elements
can help explain the information flow and selectivity of high level areas
within the brain. Additionally, we present both quantitative and qualitative
results that demonstrate consistency with general themes and specific responses
observed in the visual system. Finally, we show that our generative framework
enables a wide range of applications in computer vision, including overcoming
issues of bias that have been discovered in standard deep learning models.
</p>
<a href="http://arxiv.org/abs/2011.11167" target="_blank">arXiv:2011.11167</a> [<a href="http://arxiv.org/pdf/2011.11167" target="_blank">pdf</a>]

<h2>On InstaHide, Phase Retrieval, and Sparse Matrix Factorization. (arXiv:2011.11181v1 [cs.LG])</h2>
<h3>Sitan Chen, Zhao Song, Danyang Zhuo</h3>
<p>In this work, we examine the security of InstaHide, a scheme recently
proposed by [Huang, Song, Li and Arora, ICML'20] for preserving the security of
private datasets in the context of distributed learning. To generate a
synthetic training example to be shared among the distributed learners,
InstaHide takes a convex combination of private feature vectors and randomly
flips the sign of each entry of the resulting vector with probability 1/2. A
salient question is whether this scheme is secure in any provable sense,
perhaps under a plausible hardness assumption and assuming the distributions
generating the public and private data satisfy certain properties.

We show that the answer to this appears to be quite subtle and closely
related to the average-case complexity of a new multi-task, missing-data
version of the classic problem of phase retrieval. Motivated by this
connection, we design a provable algorithm that can recover private vectors
using only the public vectors and synthetic vectors generated by InstaHide,
under the assumption that the private and public vectors are isotropic
Gaussian.
</p>
<a href="http://arxiv.org/abs/2011.11181" target="_blank">arXiv:2011.11181</a> [<a href="http://arxiv.org/pdf/2011.11181" target="_blank">pdf</a>]

<h2>CoMatch: Semi-supervised Learning with Contrastive Graph Regularization. (arXiv:2011.11183v1 [cs.LG])</h2>
<h3>Junnan Li, Caiming Xiong, Steven Hoi</h3>
<p>Semi-supervised learning has been an effective paradigm for leveraging
unlabeled data to reduce the reliance on labeled data. We propose CoMatch, a
new semi-supervised learning method that unifies dominant approaches and
addresses their limitations. CoMatch jointly learns two representations of the
training data, their class probabilities and low-dimensional embeddings. The
two representations interact with each other to jointly evolve. The embeddings
impose a smoothness constraint on the class probabilities to improve the
pseudo-labels, whereas the pseudo-labels regularize the structure of the
embeddings through graph-based contrastive learning. CoMatch achieves
state-of-the-art performance on multiple datasets. It achieves ~20% accuracy
improvement on the label-scarce CIFAR-10 and STL-10. On ImageNet with 1%
labels, CoMatch achieves a top-1 accuracy of 66.0%, outperforming FixMatch by
12.6%. The accuracy further increases to 67.1% with self-supervised
pre-training. Furthermore, CoMatch achieves better representation learning
performance on downstream tasks, outperforming both supervised learning and
self-supervised learning.
</p>
<a href="http://arxiv.org/abs/2011.11183" target="_blank">arXiv:2011.11183</a> [<a href="http://arxiv.org/pdf/2011.11183" target="_blank">pdf</a>]

<h2>Cancer image classification based on DenseNet model. (arXiv:2011.11186v1 [cs.CV])</h2>
<h3>Ziliang Zhong, Muhang Zheng, Huafeng Mai, Jianan Zhao, Xinyi Liu</h3>
<p>Computer-aided diagnosis establishes methods for robust assessment of medical
image-based examination. Image processing introduced a promising strategy to
facilitate disease classification and detection while diminishing unnecessary
expenses. In this paper, we propose a novel metastatic cancer image
classification model based on DenseNet Block, which can effectively identify
metastatic cancer in small image patches taken from larger digital pathology
scans. We evaluate the proposed approach to the slightly modified version of
the PatchCamelyon (PCam) benchmark dataset. The dataset is the slightly
modified version of the PatchCamelyon (PCam) benchmark dataset provided by
Kaggle competition, which packs the clinically-relevant task of metastasis
detection into a straight-forward binary image classification task. The
experiments indicated that our model outperformed other classical methods like
Resnet34, Vgg19. Moreover, we also conducted data augmentation experiment and
study the relationship between Batches processed and loss value during the
training and validation process.
</p>
<a href="http://arxiv.org/abs/2011.11186" target="_blank">arXiv:2011.11186</a> [<a href="http://arxiv.org/pdf/2011.11186" target="_blank">pdf</a>]

<h2>Integrating Deep Learning in Domain Sciences at Exascale. (arXiv:2011.11188v1 [cs.LG])</h2>
<h3>Rick Archibald, Edmond Chow, Eduardo D&#x27;Azevedo, Jack Dongarra, Markus Eisenbach, Rocco Febbo, Florent Lopez, Daniel Nichols, Stanimire Tomov, Kwai Wong, Junqi Yin</h3>
<p>This paper presents some of the current challenges in designing deep learning
artificial intelligence (AI) and integrating it with traditional
high-performance computing (HPC) simulations. We evaluate existing packages for
their ability to run deep learning models and applications on large-scale HPC
systems efficiently, identify challenges, and propose new asynchronous
parallelization and optimization techniques for current large-scale
heterogeneous systems and upcoming exascale systems. These developments, along
with existing HPC AI software capabilities, have been integrated into MagmaDNN,
an open-source HPC deep learning framework. Many deep learning frameworks are
targeted at data scientists and fall short in providing quality integration
into existing HPC workflows. This paper discusses the necessities of an HPC
deep learning framework and how those needs can be provided (e.g., as in
MagmaDNN) through a deep integration with existing HPC libraries, such as MAGMA
and its modular memory management, MPI, CuBLAS, CuDNN, MKL, and HIP.
Advancements are also illustrated through the use of algorithmic enhancements
in reduced- and mixed-precision, as well as asynchronous optimization methods.
Finally, we present illustrations and potential solutions for enhancing
traditional compute- and data-intensive applications at ORNL and UTK with AI.
The approaches and future challenges are illustrated in materials science,
imaging, and climate applications.
</p>
<a href="http://arxiv.org/abs/2011.11188" target="_blank">arXiv:2011.11188</a> [<a href="http://arxiv.org/pdf/2011.11188" target="_blank">pdf</a>]

<h2>Attentional-GCNN: Adaptive Pedestrian Trajectory Prediction towards Generic Autonomous Vehicle Use Cases. (arXiv:2011.11190v1 [cs.CV])</h2>
<h3>Kunming Li, Stuart Eiffert, Mao Shan, Francisco Gomez-Donoso, Stewart Worrall, Eduardo Nebot</h3>
<p>Autonomous vehicle navigation in shared pedestrian environments requires the
ability to predict future crowd motion both accurately and with minimal delay.
Understanding the uncertainty of the prediction is also crucial. Most existing
approaches however can only estimate uncertainty through repeated sampling of
generative models. Additionally, most current predictive models are trained on
datasets that assume complete observability of the crowd using an aerial view.
These are generally not representative of real-world usage from a vehicle
perspective, and can lead to the underestimation of uncertainty bounds when the
on-board sensors are occluded. Inspired by prior work in motion prediction
using spatio-temporal graphs, we propose a novel Graph Convolutional Neural
Network (GCNN)-based approach, Attentional-GCNN, which aggregates information
of implicit interaction between pedestrians in a crowd by assigning attention
weight in edges of the graph. Our model can be trained to either output a
probabilistic distribution or faster deterministic prediction, demonstrating
applicability to autonomous vehicle use cases where either speed or accuracy
with uncertainty bounds are required. To further improve the training of
predictive models, we propose an automatically labelled pedestrian dataset
collected from an intelligent vehicle platform representative of real-world
use. Through experiments on a number of datasets, we show our proposed method
achieves an improvement over the state of art by 10% Average Displacement Error
(ADE) and 12% Final Displacement Error (FDE) with fast inference speeds.
</p>
<a href="http://arxiv.org/abs/2011.11190" target="_blank">arXiv:2011.11190</a> [<a href="http://arxiv.org/pdf/2011.11190" target="_blank">pdf</a>]

<h2>Socially Aware Crowd Navigation with Multimodal Pedestrian Trajectory Prediction for Autonomous Vehicles. (arXiv:2011.11191v1 [cs.RO])</h2>
<h3>Kunming Li, Mao Shan, Karan Narula, Stewart Worrall, Eduardo Nebot</h3>
<p>Seamlessly operating an autonomous vehicle in a crowded pedestrian
environment is a very challenging task. This is because human movement and
interactions are very hard to predict in such environments. Recent work has
demonstrated that reinforcement learning-based methods have the ability to
learn to drive in crowds. However, these methods can have very poor performance
due to inaccurate predictions of the pedestrians' future state as human motion
prediction has a large variance. To overcome this problem, we propose a new
method, SARL-SGAN-KCE, that combines a deep socially aware attentive value
network with a human multimodal trajectory prediction model to help identify
the optimal driving policy. We also introduce a novel technique to extend the
discrete action space with minimal additional computational requirements. The
kinematic constraints of the vehicle are also considered to ensure smooth and
safe trajectories. We evaluate our method against the state of art methods for
crowd navigation and provide an ablation study to show that our method is safer
and closer to human behaviour.
</p>
<a href="http://arxiv.org/abs/2011.11191" target="_blank">arXiv:2011.11191</a> [<a href="http://arxiv.org/pdf/2011.11191" target="_blank">pdf</a>]

<h2>An off-the-grid approach to multi-compartment magnetic resonance fingerprinting. (arXiv:2011.11193v1 [cs.CV])</h2>
<h3>Mohammad Golbabaee, Clarice Poon</h3>
<p>We propose a novel numerical approach to separate multiple tissue
compartments in image voxels and to estimate quantitatively their nuclear
magnetic resonance (NMR) properties and mixture fractions, given magnetic
resonance fingerprinting (MRF) measurements. The number of tissues, their types
or quantitative properties are not a-priori known, but the image is assumed to
be composed of sparse compartments with linearly mixed Bloch magnetisation
responses within voxels. Fine-grid discretisation of the multi-dimensional NMR
properties creates large and highly coherent MRF dictionaries that can
challenge scalability and precision of the numerical methods for (discrete)
sparse approximation. To overcome these issues, we propose an off-the-grid
approach equipped with an extended notion of the sparse group lasso
regularisation for sparse approximation using continuous (non-discretised)
Bloch response models. Further, the nonlinear and non-analytical Bloch
responses are approximated by a neural network, enabling efficient
back-propagation of the gradients through the proposed algorithm. Tested on
simulated and in-vivo healthy brain MRF data, we demonstrate effectiveness of
the proposed scheme compared to the baseline multicompartment MRF methods.
</p>
<a href="http://arxiv.org/abs/2011.11193" target="_blank">arXiv:2011.11193</a> [<a href="http://arxiv.org/pdf/2011.11193" target="_blank">pdf</a>]

<h2>V3H: Incomplete Multi-view Clustering via View Variation and View Heredity. (arXiv:2011.11194v1 [cs.LG])</h2>
<h3>Xiang Fang, Yuchong Hu, Pan Zhou, Dapeng Oliver Wu</h3>
<p>Real data often appear in the form of multiple incomplete views, and
incomplete multi-view clustering is an effective method to integrate these
incomplete views. Previous methods only learn the consistent information
between different views and ignore the unique information of each view, which
limits their clustering performance and generalizations. To overcome this
limitation, we propose a novel View Variation and View Heredity approach (V 3
H). Inspired by the variation and the heredity in genetics, V 3 H first
decomposes each subspace into a variation matrix for the corresponding view and
a heredity matrix for all the views to represent the unique information and the
consistent information respectively. Then, by aligning different views based on
their cluster indicator matrices, V3H integrates the unique information from
different views to improve the clustering performance. Finally, with the help
of the adjustable low-rank representation based on the heredity matrix, V3H
recovers the underlying true data structure to reduce the influence of the
large incompleteness. More importantly, V3H presents possibly the first work to
introduce genetics to clustering algorithms for learning simultaneously the
consistent information and the unique information from incomplete multi-view
data. Extensive experimental results on fifteen benchmark datasets validate its
superiority over other state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2011.11194" target="_blank">arXiv:2011.11194</a> [<a href="http://arxiv.org/pdf/2011.11194" target="_blank">pdf</a>]

<h2>The Emerging Trends of Multi-Label Learning. (arXiv:2011.11197v1 [cs.LG])</h2>
<h3>Weiwei Liu, Xiaobo Shen, Haobo Wang, Ivor W. Tsang</h3>
<p>Exabytes of data are generated daily by humans, leading to the growing need
for new efforts in dealing with the grand challenges for multi-label learning
brought by big data. For example, extreme multi-label classification is an
active and rapidly growing research area that deals with classification tasks
with an extremely large number of classes or labels; utilizing massive data
with limited supervision to build a multi-label classification model becomes
valuable for practical applications, etc. Besides these, there are tremendous
efforts on how to harvest the strong learning capability of deep learning to
better capture the label dependencies in multi-label learning, which is the key
for deep learning to address real-world classification tasks. However, it is
noted that there has been a lack of systemic studies that focus explicitly on
analyzing the emerging trends and new challenges of multi-label learning in the
era of big data. It is imperative to call for a comprehensive survey to fulfill
this mission and delineate future research directions and new applications.
</p>
<a href="http://arxiv.org/abs/2011.11197" target="_blank">arXiv:2011.11197</a> [<a href="http://arxiv.org/pdf/2011.11197" target="_blank">pdf</a>]

<h2>Complex-valued Iris Recognition Network. (arXiv:2011.11198v1 [cs.CV])</h2>
<h3>Kien Nguyen, Clinton Fookes, Sridha Sridharan, Arun Ross</h3>
<p>In this work, we design a complex-valued neural network for the task of iris
recognition. Unlike the problem of general object recognition, where
real-valued neural networks can be used to extract pertinent features, iris
recognition depends on the extraction of both phase and amplitude information
from the input iris texture in order to better represent its stochastic
content. This necessitates the extraction and processing of phase information
that cannot be effectively handled by a real-valued neural network. In this
regard, we design a complex-valued neural network that can better capture the
multi-scale, multi-resolution, and multi-orientation phase and amplitude
features of the iris texture. We show a strong correspondence of the proposed
complex-valued iris recognition network with Gabor wavelets that are used to
generate the classical IrisCode; however, the proposed method enables automatic
complex-valued feature learning that is tailored for iris recognition.
Experiments conducted on three benchmark datasets - ND-CrossSensor-2013,
CASIA-Iris-Thousand and UBIRIS.v2 - show the benefit of the proposed network
for the task of iris recognition. Further, the generalization capability of the
proposed network is demonstrated by training and testing it across different
datasets. Finally, visualization schemes are used to convey the type of
features being extracted by the complex-valued network in comparison to
classical real-valued networks. The results of this work are likely to be
applicable in other domains, where complex Gabor filters are used for texture
modeling.
</p>
<a href="http://arxiv.org/abs/2011.11198" target="_blank">arXiv:2011.11198</a> [<a href="http://arxiv.org/pdf/2011.11198" target="_blank">pdf</a>]

<h2>Balance Regularized Neural Network Models for Causal Effect Estimation. (arXiv:2011.11199v1 [cs.LG])</h2>
<h3>Mehrdad Farajtabar, Andrew Lee, Yuanjian Feng, Vishal Gupta, Peter Dolan, Harish Chandran, Martin Szummer</h3>
<p>Estimating individual and average treatment effects from observational data
is an important problem in many domains such as healthcare and e-commerce. In
this paper, we advocate balance regularization of multi-head neural network
architectures. Our work is motivated by representation learning techniques to
reduce differences between treated and untreated distributions that potentially
arise due to confounding factors. We further regularize the model by
encouraging it to predict control outcomes for individuals in the treatment
group that are similar to control outcomes in the control group. We empirically
study the bias-variance trade-off between different weightings of the
regularizers, as well as between inductive and transductive inference.
</p>
<a href="http://arxiv.org/abs/2011.11199" target="_blank">arXiv:2011.11199</a> [<a href="http://arxiv.org/pdf/2011.11199" target="_blank">pdf</a>]

<h2>Ranking Neural Checkpoints. (arXiv:2011.11200v1 [cs.LG])</h2>
<h3>Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang Wang, Boqing Gong</h3>
<p>This paper is concerned with ranking many pre-trained deep neural networks
(DNNs), called checkpoints, for the transfer learning to a downstream task.
Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints
from various sources. Which of them transfers the best to our downstream task
of interest? Striving to answer this question thoroughly, we establish a neural
checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking
measures. These measures are generic, applying to the checkpoints of different
output types without knowing how the checkpoints are pre-trained on which
dataset. They also incur low computation cost, making them practically
meaningful. Our results suggest that the linear separability of the features
extracted by the checkpoints is a strong indicator of transferability. We also
arrive at a new ranking measure, NLEEP, which gives rise to the best
performance in the experiments.
</p>
<a href="http://arxiv.org/abs/2011.11200" target="_blank">arXiv:2011.11200</a> [<a href="http://arxiv.org/pdf/2011.11200" target="_blank">pdf</a>]

<h2>Action Concept Grounding Network for Semantically-Consistent Video Generation. (arXiv:2011.11201v1 [cs.CV])</h2>
<h3>Wei Yu, Wenxin Chen, Steve Easterbrook, Animesh Garg</h3>
<p>Recent works in self-supervised video prediction have mainly focused on
passive forecasting and low-level action-conditional prediction, which
sidesteps the problem of semantic learning. We introduce the task of semantic
action-conditional video prediction, which can be regarded as an inverse
problem of action recognition. The challenge of this new task primarily lies in
how to effectively inform the model of semantic action information. To bridge
vision and language, we utilize the idea of capsule and propose a novel video
prediction model Action Concept Grounding Network (AGCN). Our method is
evaluated on two newly designed synthetic datasets, CLEVR-Building-Blocks and
Sapien-Kitchen, and experiments show that given different action labels, our
ACGN can correctly condition on instructions and generate corresponding future
frames without need of bounding boxes. We further demonstrate our trained model
can make out-of-distribution predictions for concurrent actions, be quickly
adapted to new object categories and exploit its learnt features for object
detection. Additional visualizations can be found at
https://iclr-acgn.github.io/ACGN/.
</p>
<a href="http://arxiv.org/abs/2011.11201" target="_blank">arXiv:2011.11201</a> [<a href="http://arxiv.org/pdf/2011.11201" target="_blank">pdf</a>]

<h2>Effectiveness of MPC-friendly Softmax Replacement. (arXiv:2011.11202v1 [cs.LG])</h2>
<h3>Marcel Keller, Ke Sun</h3>
<p>Softmax is widely used in deep learning to map some representation to a
probability distribution. As it is based on exp/log functions that is
relatively expensive in multi-party computation, Mohassel and Zhang (2017)
proposed a simpler replacement based on ReLU to be used in secure computation.
However, we could not reproduce the accuracy they reported for training on
MNIST with three fully connected layers. Later works (e.g., Wagh et al., 2019
and 2021) used the softmax replacement not for computing the output probability
distribution but for approximating the gradient in back-propagation. In this
work, we analyze the two uses of the replacement and compare them to softmax,
both in terms of accuracy and cost in multi-party computation. We found that
the replacement only provides a significant speed-up for a one-layer network
while it always reduces accuracy, sometimes significantly. Thus we conclude
that its usefulness is limited and one should use the original softmax function
instead.
</p>
<a href="http://arxiv.org/abs/2011.11202" target="_blank">arXiv:2011.11202</a> [<a href="http://arxiv.org/pdf/2011.11202" target="_blank">pdf</a>]

<h2>Geometry-Aware Universal Mirror-Prox. (arXiv:2011.11203v1 [cs.LG])</h2>
<h3>Reza Babanezhad, Simon Lacoste-Julien</h3>
<p>Mirror-prox (MP) is a well-known algorithm to solve variational inequality
(VI) problems. VI with a monotone operator covers a large group of settings
such as convex minimization, min-max or saddle point problems. To get a
convergent algorithm, the step-size of the classic MP algorithm relies heavily
on the problem dependent knowledge of the operator such as its smoothness
parameter which is hard to estimate. Recently, a universal variant of MP for
smooth/bounded operators has been introduced that depends only on the norm of
updates in MP. In this work, we relax the dependence to evaluating the norm of
updates to Bregman divergence between updates. This relaxation allows us to
extends the analysis of universal MP to the settings where the operator is not
smooth or bounded. Furthermore, we analyse the VI problem with a stochastic
monotone operator in different settings and obtain an optimal rate up to a
logarithmic factor.
</p>
<a href="http://arxiv.org/abs/2011.11203" target="_blank">arXiv:2011.11203</a> [<a href="http://arxiv.org/pdf/2011.11203" target="_blank">pdf</a>]

<h2>Graph Attention Tracking. (arXiv:2011.11204v1 [cs.CV])</h2>
<h3>Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua Wang, Liyan Zhang, Chunhua Shen</h3>
<p>Siamese network based trackers formulate the visual tracking task as a
similarity matching problem. Almost all popular Siamese trackers realize the
similarity learning via convolutional feature cross-correlation between a
target branch and a search branch. However, since the size of target feature
region needs to be pre-fixed, these cross-correlation base methods suffer from
either reserving much adverse background information or missing a great deal of
foreground information. Moreover, the global matching between the target and
search region also largely neglects the target structure and part-level
information.

In this paper, to solve the above issues, we propose a simple target-aware
Siamese graph attention network for general object tracking. We propose to
establish part-to-part correspondence between the target and the search region
with a complete bipartite graph, and apply the graph attention mechanism to
propagate target information from the template feature to the search feature.
Further, instead of using the pre-fixed region cropping for
template-feature-area selection, we investigate a target-aware area selection
mechanism to fit the size and aspect ratio variations of different objects.
Experiments on challenging benchmarks including GOT-10k, UAV123, OTB-100 and
LaSOT demonstrate that the proposed SiamGAT outperforms many state-of-the-art
trackers and achieves leading performance. Code is available at:
https://git.io/SiamGAT
</p>
<a href="http://arxiv.org/abs/2011.11204" target="_blank">arXiv:2011.11204</a> [<a href="http://arxiv.org/pdf/2011.11204" target="_blank">pdf</a>]

<h2>Structure-Aware Completion of Photogrammetric Meshes in Urban Road Environment. (arXiv:2011.11210v1 [cs.CV])</h2>
<h3>Qing Zhu, Qishen Shang, Han Hu, Haojia Yu</h3>
<p>Photogrammetric mesh models obtained from aerial oblique images have been
widely used for urban reconstruction. However, the photogrammetric meshes also
suffer from severe texture problems, especially on the road areas due to
occlusion. This paper proposes a structure-aware completion approach to improve
the quality of meshes by removing undesired vehicles on the road seamlessly.
Specifically, the discontinuous texture atlas is first integrated to a
continuous screen space through rendering by the graphics pipeline; the
rendering also records necessary mapping for deintegration to the original
texture atlas after editing. Vehicle regions are masked by a standard object
detection approach, e.g. Faster RCNN. Then, the masked regions are completed
guided by the linear structures and regularities in the road region, which is
implemented based on Patch Match. Finally, the completed rendered image is
deintegrated to the original texture atlas and the triangles for the vehicles
are also flattened for improved meshes. Experimental evaluations and analyses
are conducted against three datasets, which are captured with different sensors
and ground sample distances. The results reveal that the proposed method can
quite realistic meshes after removing the vehicles. The structure-aware
completion approach for road regions outperforms popular image completion
methods and ablation study further confirms the effectiveness of the linear
guidance. It should be noted that the proposed method is also capable to handle
tiled mesh models for large-scale scenes. Dataset and code are available at
vrlab.org.cn/~hanhu/projects/mesh.
</p>
<a href="http://arxiv.org/abs/2011.11210" target="_blank">arXiv:2011.11210</a> [<a href="http://arxiv.org/pdf/2011.11210" target="_blank">pdf</a>]

<h2>Adversarial Refinement Network for Human Motion Prediction. (arXiv:2011.11221v1 [cs.CV])</h2>
<h3>Xianjin Chao, Yanrui Bin, Wenqing Chu, Xuan Cao, Yanhao Ge, Chengjie Wang, Jilin Li, Feiyue Huang, Howard Leung</h3>
<p>Human motion prediction aims to predict future 3D skeletal sequences by
giving a limited human motion as inputs. Two popular methods, recurrent neural
networks and feed-forward deep networks, are able to predict rough motion
trend, but motion details such as limb movement may be lost. To predict more
accurate future human motion, we propose an Adversarial Refinement Network
(ARNet) following a simple yet effective coarse-to-fine mechanism with novel
adversarial error augmentation. Specifically, we take both the historical
motion sequences and coarse prediction as input of our cascaded refinement
network to predict refined human motion and strengthen the refinement network
with adversarial error augmentation. During training, we deliberately introduce
the error distribution by learning through the adversarial mechanism among
different subjects. In testing, our cascaded refinement network alleviates the
prediction error from the coarse predictor resulting in a finer prediction
robustly. This adversarial error augmentation provides rich error cases as
input to our refinement network, leading to better generalization performance
on the testing dataset. We conduct extensive experiments on three standard
benchmark datasets and show that our proposed ARNet outperforms other
state-of-the-art methods, especially on challenging aperiodic actions in both
short-term and long-term predictions.
</p>
<a href="http://arxiv.org/abs/2011.11221" target="_blank">arXiv:2011.11221</a> [<a href="http://arxiv.org/pdf/2011.11221" target="_blank">pdf</a>]

<h2>Improved Confidence Bounds for the Linear Logistic Model and Applications to Linear Bandits. (arXiv:2011.11222v1 [stat.ML])</h2>
<h3>Kwang-Sung Jun, Lalit Jain, Houssam Nassif</h3>
<p>We propose improved fixed-design confidence bounds for the linear logistic
model. Our bounds significantly improve upon the state-of-the-art bounds of Li
et al. (2017) by leveraging the self-concordance of the logistic loss inspired
by Faury et al. (2020). Specifically, our confidence width does not scale with
the problem dependent parameter $1/\kappa$, where $\kappa$ is the worst-case
variance of an arm reward. At worse, $\kappa$ scales exponentially with the
norm of the unknown linear parameter $\theta^*$. Instead, our bound scales
directly on the local variance induced by $\theta^*$. We present two
applications of our novel bounds on two logistic bandit problems: regret
minimization and pure exploration. Our analysis shows that the new confidence
bounds improve upon previous state-of-the-art performance guarantees.
</p>
<a href="http://arxiv.org/abs/2011.11222" target="_blank">arXiv:2011.11222</a> [<a href="http://arxiv.org/pdf/2011.11222" target="_blank">pdf</a>]

<h2>Detection and Classification of mental illnesses on social media using RoBERTa. (arXiv:2011.11226v1 [cs.LG])</h2>
<h3>Ankit Murarka, Balaji Radhakrishnan, Sushma Ravichandran</h3>
<p>Given the current social distancing regulations across the world, social
media has become the primary mode of communication for most people. This has
resulted in the isolation of many people suffering from mental illnesses who
are unable to receive assistance in person. They have increasingly turned to
social media to express themselves and to look for guidance in dealing with
their illnesses. Keeping this in mind, we propose a solution to detect and
classify mental illness posts on social media thereby enabling users to seek
appropriate help. In this work, we detect and classify five prominent kinds of
mental illnesses: depression, anxiety, bipolar disorder, ADHD and PTSD by
analyzing unstructured user data on social media platforms. In addition, we are
sharing a new high-quality dataset to drive research on this topic. We believe
that our work is the first multi-class model that uses a Transformer-based
architecture such as RoBERTa to analyze people's emotions and psychology. We
also demonstrate how we stress-test our model using behavioral testing. With
this research, we hope to be able to contribute to the public health system by
automating some of the detection and classification process.
</p>
<a href="http://arxiv.org/abs/2011.11226" target="_blank">arXiv:2011.11226</a> [<a href="http://arxiv.org/pdf/2011.11226" target="_blank">pdf</a>]

<h2>NeuralAnnot: Neural Annotator for in-the-wild Expressive 3D Human Pose and Mesh Training Sets. (arXiv:2011.11232v1 [cs.CV])</h2>
<h3>Gyeongsik Moon, Kyoung Mu Lee</h3>
<p>Recovering expressive 3D human pose and mesh from in-the-wild images is
greatly challenging due to the absence of the training data. Several
optimization-based methods have been used to obtain pseudo-groundtruth (GT) 3D
poses and meshes from GT 2D poses. However, they often produce bad ones with
long running time because their frameworks are optimized on each sample only
using 2D supervisions in a sequential way. To overcome the limitations, we
present NeuralAnnot, a neural annotator that learns to construct in-the-wild
expressive 3D human pose and mesh training sets. Our NeuralAnnot is trained on
a large number of samples by 2D supervisions from a target in-the-wild dataset
and 3D supervisions from auxiliary datasets with GT 3D poses in a parallel way.
We show that our NeuralAnnot produces far better 3D pseudo-GTs with much
shorter running time than the optimization-based methods, and the newly
obtained training set brings great performance gain. The newly obtained
training sets and codes will be publicly available.
</p>
<a href="http://arxiv.org/abs/2011.11232" target="_blank">arXiv:2011.11232</a> [<a href="http://arxiv.org/pdf/2011.11232" target="_blank">pdf</a>]

<h2>ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradients Accumulation. (arXiv:2011.11233v1 [cs.LG])</h2>
<h3>Xiaoxing Wang, Xiangxiang Chu, Yuda Fan, Zhexi Zhang, Xiaolin Wei, Junchi Yan, Xiaokang Yang</h3>
<p>Single-path based differentiable neural architecture search has great
strengths for its low computational cost and memory-friendly nature. However,
we surprisingly discover that it suffers from severe searching instability
which has been primarily ignored, posing a potential weakness for a wider
application. In this paper, we delve into its performance collapse issue and
propose a new algorithm called RObustifying Memory-Efficient NAS (ROME).
Specifically, 1) for consistent topology in the search and evaluation stage, we
involve separate parameters to disentangle the topology from the operations of
the architecture. In such a way, we can independently sample connections and
operations without interference; 2) to discount sampling unfairness and
variance, we enforce fair sampling for weight update and apply a gradient
accumulation mechanism for architecture parameters. Extensive experiments
demonstrate that our proposed method has strong performance and robustness,
where it mostly achieves state-of-the-art results on a large number of standard
benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.11233" target="_blank">arXiv:2011.11233</a> [<a href="http://arxiv.org/pdf/2011.11233" target="_blank">pdf</a>]

<h2>An Empirical Study of Representation Learning for Reinforcement Learning in Healthcare. (arXiv:2011.11235v1 [cs.LG])</h2>
<h3>Taylor W. Killian, Haoran Zhang, Jayakumar Subramanian, Mehdi Fatemi, Marzyeh Ghassemi</h3>
<p>Reinforcement Learning (RL) has recently been applied to sequential
estimation and prediction problems identifying and developing hypothetical
treatment strategies for septic patients, with a particular focus on offline
learning with observational data. In practice, successful RL relies on
informative latent states derived from sequential observations to develop
optimal treatment strategies. To date, how best to construct such states in a
healthcare setting is an open question. In this paper, we perform an empirical
study of several information encoding architectures using data from septic
patients in the MIMIC-III dataset to form representations of a patient state.
We evaluate the impact of representation dimension, correlations with
established acuity scores, and the treatment policies derived from them. We
find that sequentially formed state representations facilitate effective policy
learning in batch settings, validating a more thoughtful approach to
representation learning that remains faithful to the sequential and partial
nature of healthcare data.
</p>
<a href="http://arxiv.org/abs/2011.11235" target="_blank">arXiv:2011.11235</a> [<a href="http://arxiv.org/pdf/2011.11235" target="_blank">pdf</a>]

<h2>Learning Hidden Markov Models from Aggregate Observations. (arXiv:2011.11236v1 [cs.LG])</h2>
<h3>Rahul Singh, Qinsheng Zhang, Yongxin Chen</h3>
<p>In this paper, we propose an algorithm for estimating the parameters of a
time-homogeneous hidden Markov model from aggregate observations. This problem
arises when only the population level counts of the number of individuals at
each time step are available, from which one seeks to learn the individual
hidden Markov model. Our algorithm is built upon expectation-maximization and
the recently proposed aggregate inference algorithm, the Sinkhorn belief
propagation. As compared with existing methods such as expectation-maximization
with non-linear belief propagation, our algorithm exhibits convergence
guarantees. Moreover, our learning framework naturally reduces to the standard
Baum-Welch learning algorithm when observations corresponding to a single
individual are recorded. We further extend our learning algorithm to handle
HMMs with continuous observations. The efficacy of our algorithm is
demonstrated on a variety of datasets.
</p>
<a href="http://arxiv.org/abs/2011.11236" target="_blank">arXiv:2011.11236</a> [<a href="http://arxiv.org/pdf/2011.11236" target="_blank">pdf</a>]

<h2>Data-driven Holistic Framework for Automated Laparoscope Optimal View Control with Learning-based Depth Perception. (arXiv:2011.11241v1 [cs.RO])</h2>
<h3>Bin Li, Bo Lu, Yiang Lu, Qi Dou, Yun-Hui Liu</h3>
<p>Laparoscopic Field of View (FOV) control is one of the most fundamental and
important components in Minimally Invasive Surgery (MIS), nevertheless, the
traditional manual holding paradigm may easily bring fatigue to surgical
assistants, and misunderstanding between surgeons also hinders assistants to
provide a high-quality FOV. Targeting this problem, we here present a
data-driven framework to realize an automated laparoscopic optimal FOV control.
To achieve this goal, we offline learn a motion strategy of laparoscope
relative to the surgeon's hand-held surgical tool from our in-house surgical
videos, developing our control domain knowledge and an optimal view generator.
To adjust the laparoscope online, we first adopt a learning-based method to
segment the two-dimensional (2D) position of the surgical tool, and further
leverage this outcome to obtain its scale-aware depth from dense depth
estimation results calculated by our novel unsupervised RoboDepth model only
with the monocular camera feedback, hence in return fusing the above real-time
3D position into our control loop. To eliminate the misorientation of FOV
caused by Remote Center of Motion (RCM) constraints when moving the
laparoscope, we propose a novel distortion constraint using an affine map to
minimize the visual warping problem, and a null-space controller is also
embedded into the framework to optimize all types of errors in a unified and
decoupled manner. Experiments are conducted using Universal Robot (UR) and Karl
Storz Laparoscope/Instruments, which prove the feasibility of our domain
knowledge and learning enabled framework for automated camera control.
</p>
<a href="http://arxiv.org/abs/2011.11241" target="_blank">arXiv:2011.11241</a> [<a href="http://arxiv.org/pdf/2011.11241" target="_blank">pdf</a>]

<h2>BiOpt: Bi-Level Optimization for Few-Shot Segmentation. (arXiv:2011.11245v1 [cs.CV])</h2>
<h3>Jinlu Liu, Liang Song, Yongqiang Qin</h3>
<p>Few-shot segmentation is a challenging task that aims to segment objects of
new classes given scarce support images. In the inductive setting, existing
prototype-based methods focus on extracting prototypes from the support images;
however, they fail to utilize semantic information of the query images. In this
paper, we propose Bi-level Optimization (BiOpt), which succeeds to compute
class prototypes from the query images under inductive setting. The learning
procedure of BiOpt is decomposed into two nested loops: inner and outer loop.
On each task, the inner loop aims to learn optimized prototypes from the query
images. An init step is conducted to fully exploit knowledge from both support
and query features, so as to give reasonable initialized prototypes into the
inner loop. The outer loop aims to learn a discriminative embedding space
across different tasks. Extensive experiments on two benchmarks verify the
superiority of our proposed BiOpt algorithm. In particular, we consistently
achieve the state-of-the-art performance on 5-shot PASCAL-$5^i$ and 1-shot
COCO-$20^i$.
</p>
<a href="http://arxiv.org/abs/2011.11245" target="_blank">arXiv:2011.11245</a> [<a href="http://arxiv.org/pdf/2011.11245" target="_blank">pdf</a>]

<h2>Learning Quantized Neural Nets by Coarse Gradient Method for Non-linear Classification. (arXiv:2011.11256v1 [cs.LG])</h2>
<h3>Ziang Long, Penghang Yin, Jack Xin</h3>
<p>Quantized or low-bit neural networks are attractive due to their inference
efficiency. However, training deep neural networks with quantized activations
involves minimizing a discontinuous and piecewise constant loss function. Such
a loss function has zero gradients almost everywhere (a.e.), which makes the
conventional gradient-based algorithms inapplicable. To this end, we study a
novel class of \emph{biased} first-order oracle, termed coarse gradient, for
overcoming the vanished gradient issue. A coarse gradient is generated by
replacing the a.e. zero derivatives of quantized (i.e., stair-case) ReLU
activation composited in the chain rule with some heuristic proxy derivative
called straight-through estimator (STE). Although having been widely used in
training quantized networks empirically, fundamental questions like when and
why the ad-hoc STE trick works, still lacks theoretical understanding. In this
paper, we propose a class of STEs with certain monotonicity, and consider their
applications to the training of a two-linear-layer network with quantized
activation functions for non-linear multi-category classification. We establish
performance guarantees for the proposed STEs by showing that the corresponding
coarse gradient methods converge to the global minimum, which leads to a
perfect classification. Lastly, we present experimental results on synthetic
data as well as MNIST dataset to verify our theoretical findings and
demonstrate the effectiveness of our proposed STEs.
</p>
<a href="http://arxiv.org/abs/2011.11256" target="_blank">arXiv:2011.11256</a> [<a href="http://arxiv.org/pdf/2011.11256" target="_blank">pdf</a>]

<h2>Application of Facial Recognition using Convolutional Neural Networks for Entry Access Control. (arXiv:2011.11257v1 [cs.CV])</h2>
<h3>Lars Lien Ankile, Morgan Feet Heggland, Kjartan Krange</h3>
<p>The purpose of this paper is to design a solution to the problem of facial
recognition by use of convolutional neural networks, with the intention of
applying the solution in a camera-based home-entry access control system. More
specifically, the paper focuses on solving the supervised classification
problem of taking images of people as input and classifying the person in the
image as one of the authors or not. Two approaches are proposed: (1) building
and training a neural network called WoodNet from scratch and (2) leveraging
transfer learning by utilizing a network pre-trained on the ImageNet database
and adapting it to this project's data and classes. In order to train the
models to recognize the authors, a dataset containing more than 150 000 images
has been created, balanced over the authors and others. Image extraction from
videos and image augmentation techniques were instrumental for dataset
creation. The results are two models classifying the individuals in the dataset
with high accuracy, achieving over 99% accuracy on held-out test data. The
pre-trained model fitted significantly faster than WoodNet, and seems to
generalize better. However, these results come with a few caveats. Because of
the way the dataset was compiled, as well as the high accuracy, one has reason
to believe the models over-fitted to the data to some degree. An added
consequence of the data compilation method is that the test dataset may not be
sufficiently different from the training data, limiting its ability to validate
generalization of the models. However, utilizing the models in a web-cam based
system, classifying faces in real-time, shows promising results and indicates
that the models generalized fairly well for at least some of the classes (see
the accompanying video).
</p>
<a href="http://arxiv.org/abs/2011.11257" target="_blank">arXiv:2011.11257</a> [<a href="http://arxiv.org/pdf/2011.11257" target="_blank">pdf</a>]

<h2>3D Registration for Self-Occluded Objects in Context. (arXiv:2011.11260v1 [cs.CV])</h2>
<h3>Zheng Dang, Fei Wang, Mathieu Salzmann</h3>
<p>While much progress has been made on the task of 3D point cloud registration,
there still exists no learning-based method able to estimate the 6D pose of an
object observed by a 2.5D sensor in a scene. The challenges of this scenario
include the fact that most measurements are outliers depicting the object's
surrounding context, and the mismatch between the complete 3D object model and
its self-occluded observations.

We introduce the first deep learning framework capable of effectively
handling this scenario. Our method consists of an instance segmentation module
followed by a pose estimation one. It allows us to perform 3D registration in a
one-shot manner, without requiring an expensive iterative procedure. We further
develop an on-the-fly rendering-based training strategy that is both time- and
memory-efficient. Our experiments evidence the superiority of our approach over
the state-of-the-art traditional and learning-based 3D registration methods.
</p>
<a href="http://arxiv.org/abs/2011.11260" target="_blank">arXiv:2011.11260</a> [<a href="http://arxiv.org/pdf/2011.11260" target="_blank">pdf</a>]

<h2>Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning. (arXiv:2011.11261v1 [cs.CV])</h2>
<h3>Zehua Zhang, David Crandall</h3>
<p>We present a novel way for self-supervised video representation learning by:
(a) decoupling the learning objective into two contrastive subtasks
respectively emphasizing spatial and temporal features, and (b) performing it
hierarchically to encourage multi-scale understanding. Motivated by their
effectiveness in supervised learning, we first introduce spatial-temporal
feature learning decoupling and hierarchical learning to the context of
unsupervised video learning. In particular, our method directs the network to
separately capture spatial and temporal features on the basis of contrastive
learning via manipulating augmentations as regularization, and further solve
such proxy tasks hierarchically by optimizing towards a compound contrastive
loss. Experiments show that our proposed Hierarchically Decoupled
Spatial-Temporal Contrast (HDC) achieves substantial gains over directly
learning spatial-temporal features as a whole and significantly outperforms
other state-of-the-art unsupervised methods on downstream action recognition
benchmarks on UCF101 and HMDB51. We will release our code and pretrained
weights.
</p>
<a href="http://arxiv.org/abs/2011.11261" target="_blank">arXiv:2011.11261</a> [<a href="http://arxiv.org/pdf/2011.11261" target="_blank">pdf</a>]

<h2>Federated learning with class imbalance reduction. (arXiv:2011.11266v1 [cs.LG])</h2>
<h3>Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, Hua Qian</h3>
<p>Federated learning (FL) is a promising technique that enables a large amount
of edge computing devices to collaboratively train a global learning model. Due
to privacy concerns, the raw data on devices could not be available for
centralized server. Constrained by the spectrum limitation and computation
capacity, only a subset of devices can be engaged to train and transmit the
trained model to centralized server for aggregation. Since the local data
distribution varies among all devices, class imbalance problem arises along
with the unfavorable client selection, resulting in a slow converge rate of the
global model. In this paper, an estimation scheme is designed to reveal the
class distribution without the awareness of raw data. Based on the scheme, a
device selection algorithm towards minimal class imbalance is proposed, thus
can improve the convergence performance of the global model. Simulation results
demonstrate the effectiveness of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2011.11266" target="_blank">arXiv:2011.11266</a> [<a href="http://arxiv.org/pdf/2011.11266" target="_blank">pdf</a>]

<h2>COCOI: Contact-aware Online Context Inference for Generalizable Non-planar Pushing. (arXiv:2011.11270v1 [cs.RO])</h2>
<h3>Zhuo Xu, Wenhao Yu, Alexander Herzog, Wenlong Lu, Chuyuan Fu, Masayoshi Tomizuka, Yunfei Bai, C. Karen Liu, Daniel Ho</h3>
<p>General contact-rich manipulation problems are long-standing challenges in
robotics due to the difficulty of understanding complicated contact physics.
Deep reinforcement learning (RL) has shown great potential in solving robot
manipulation tasks. However, existing RL policies have limited adaptability to
environments with diverse dynamics properties, which is pivotal in solving many
contact-rich manipulation tasks. In this work, we propose Contact-aware Online
COntext Inference (COCOI), a deep RL method that encodes a context embedding of
dynamics properties online using contact-rich interactions. We study this
method based on a novel and challenging non-planar pushing task, where the
robot uses a monocular camera image and wrist force torque sensor reading to
push an object to a goal location while keeping it upright. We run extensive
experiments to demonstrate the capability of COCOI in a wide range of settings
and dynamics properties in simulation, and also in a sim-to-real transfer
scenario on a real robot (Video: https://youtu.be/nrmJYksh1Kc)
</p>
<a href="http://arxiv.org/abs/2011.11270" target="_blank">arXiv:2011.11270</a> [<a href="http://arxiv.org/pdf/2011.11270" target="_blank">pdf</a>]

<h2>IC Neuron: An Efficient Unit to Construct Neural Networks. (arXiv:2011.11271v1 [cs.LG])</h2>
<h3>Junyi An, Fengshan Liu, Jian Zhao, Furao Shen</h3>
<p>As a popular machine learning method, neural networks can be used to solve
many complex tasks. Their strong generalization ability comes from the
representation ability of the basic neuron model. The most popular neuron is
the MP neuron, which uses a linear transformation and a non-linear activation
function to process the input successively. Inspired by the elastic collision
model in physics, we propose a new neuron model that can represent more complex
distributions. We term it Inter-layer collision (IC) neuron. The IC neuron
divides the input space into multiple subspaces used to represent different
linear transformations. This operation enhanced non-linear representation
ability and emphasizes some useful input features for the given task. We build
the IC networks by integrating the IC neurons into the fully-connected (FC),
convolutional, and recurrent structures. The IC networks outperform the
traditional networks in a wide range of experiments. We believe that the IC
neuron can be a basic unit to build network structures.
</p>
<a href="http://arxiv.org/abs/2011.11271" target="_blank">arXiv:2011.11271</a> [<a href="http://arxiv.org/pdf/2011.11271" target="_blank">pdf</a>]

<h2>FakeSafe: Human Level Data Protection by Disinformation Mapping using Cycle-consistent Adversarial Network. (arXiv:2011.11278v1 [cs.AI])</h2>
<h3>Dianbo Liu, He Zhu</h3>
<p>The concept of disinformation is to use fake messages to confuse people in
order to protect the real information. This strategy can be adapted into data
science to protect valuable private and sensitive data. Huge amount of private
data are being generated from personal devices such as smart phone and wearable
in recent years. Being able to utilize these personal data will bring big
opportunities to design personalized products, conduct precision healthcare and
many other tasks that were impossible in the past. However, due to privacy,
safety and regulation reasons, it is often difficult to transfer or store data
in its original form while keeping them safe. Building a secure data transfer
and storage infrastructure to preserving privacy is costly in most cases and
there is always a concern of data security due to human errors. In this study,
we propose a method, named FakeSafe, to provide human level data protection
using generative adversarial network with cycle consistency and conducted
experiments using both benchmark and real world data sets to illustrate
potential applications of FakeSafe.
</p>
<a href="http://arxiv.org/abs/2011.11278" target="_blank">arXiv:2011.11278</a> [<a href="http://arxiv.org/pdf/2011.11278" target="_blank">pdf</a>]

<h2>AutoGraph: Automated Graph Neural Network. (arXiv:2011.11288v1 [cs.LG])</h2>
<h3>Yaoman Li, Irwin King</h3>
<p>Graphs play an important role in many applications. Recently, Graph Neural
Networks (GNNs) have achieved promising results in graph analysis tasks. Some
state-of-the-art GNN models have been proposed, e.g., Graph Convolutional
Networks (GCNs), Graph Attention Networks (GATs), etc. Despite these successes,
most of the GNNs only have shallow structure. This causes the low expressive
power of the GNNs. To fully utilize the power of the deep neural network, some
deep GNNs have been proposed recently. However, the design of deep GNNs
requires significant architecture engineering. In this work, we propose a
method to automate the deep GNNs design. In our proposed method, we add a new
type of skip connection to the GNNs search space to encourage feature reuse and
alleviate the vanishing gradient problem. We also allow our evolutionary
algorithm to increase the layers of GNNs during the evolution to generate
deeper networks. We evaluate our method in the graph node classification task.
The experiments show that the GNNs generated by our method can obtain
state-of-the-art results in Cora, Citeseer, Pubmed and PPI datasets.
</p>
<a href="http://arxiv.org/abs/2011.11288" target="_blank">arXiv:2011.11288</a> [<a href="http://arxiv.org/pdf/2011.11288" target="_blank">pdf</a>]

<h2>Evolutionary Planning in Latent Space. (arXiv:2011.11293v1 [cs.LG])</h2>
<h3>Thor V.A.N. Olesen, Dennis T.T. Nguyen, Rasmus Berg Palm, Sebastian Risi</h3>
<p>Planning is a powerful approach to reinforcement learning with several
desirable properties. However, it requires a model of the world, which is not
readily available in many real-life problems. In this paper, we propose to
learn a world model that enables Evolutionary Planning in Latent Space (EPLS).
We use a Variational Auto Encoder (VAE) to learn a compressed latent
representation of individual observations and extend a Mixture Density
Recurrent Neural Network (MDRNN) to learn a stochastic, multi-modal forward
model of the world that can be used for planning. We use the Random Mutation
Hill Climbing (RMHC) to find a sequence of actions that maximize expected
reward in this learned model of the world. We demonstrate how to build a model
of the world by bootstrapping it with rollouts from a random policy and
iteratively refining it with rollouts from an increasingly accurate planning
policy using the learned world model. After a few iterations of this
refinement, our planning agents are better than standard model-free
reinforcement learning approaches demonstrating the viability of our approach.
</p>
<a href="http://arxiv.org/abs/2011.11293" target="_blank">arXiv:2011.11293</a> [<a href="http://arxiv.org/pdf/2011.11293" target="_blank">pdf</a>]

<h2>Industrial object, machine part and defect recognition towards fully automated industrial monitoring employing deep learning. The case of multilevel VGG19. (arXiv:2011.11305v1 [cs.CV])</h2>
<h3>Ioannis D. Apostolopoulos, Mpesiana Tzani</h3>
<p>Modern industry requires modern solutions for monitoring the automatic
production of goods. Smart monitoring of the functionality of the mechanical
parts of technology systems or machines is mandatory for a fully automatic
production process. Although Deep Learning has been advancing, allowing for
real-time object detection and other tasks, little has been investigated about
the effectiveness of specially designed Convolutional Neural Networks for
defect detection and industrial object recognition. In the particular study, we
employed six publically available industrial-related datasets containing defect
materials and industrial tools or engine parts, aiming to develop a specialized
model for pattern recognition. Motivated by the recent success of the Virtual
Geometry Group (VGG) network, we propose a modified version of it, called
Multipath VGG19, which allows for more local and global feature extraction,
while the extra features are fused via concatenation. The experiments verified
the effectiveness of MVGG19 over the traditional VGG19. Specifically, top
classification performance was achieved in five of the six image datasets,
while the average classification improvement was 6.95%.
</p>
<a href="http://arxiv.org/abs/2011.11305" target="_blank">arXiv:2011.11305</a> [<a href="http://arxiv.org/pdf/2011.11305" target="_blank">pdf</a>]

<h2>Legacy Photo Editing with Learned Noise Prior. (arXiv:2011.11309v1 [cs.CV])</h2>
<h3>Zhao Yuzhi, Po Lai-Man, Wang Xuehui, Liu Kangcheng, Zhang Yujia, Yu Wing-Yin, Xian Pengfei, Xiong Jingjing</h3>
<p>There are quite a number of photographs captured under undesirable conditions
in the last century. Thus, they are often noisy, regionally incomplete, and
grayscale formatted. Conventional approaches mainly focus on one point so that
those restoration results are not perceptually sharp or clean enough. To solve
these problems, we propose a noise prior learner NEGAN to simulate the noise
distribution of real legacy photos using unpaired images. It mainly focuses on
matching high-frequency parts of noisy images through discrete wavelet
transform (DWT) since they include most of noise statistics. We also create a
large legacy photo dataset for learning noise prior. Using learned noise prior,
we can easily build valid training pairs by degrading clean images. Then, we
propose an IEGAN framework performing image editing including joint denoising,
inpainting and colorization based on the estimated noise prior. We evaluate the
proposed system and compare it with state-of-the-art image enhancement methods.
The experimental results demonstrate that it achieves the best perceptual
quality. Please see the webpage
\href{https://github.com/zhaoyuzhi/Legacy-Photo-Editing-with-Learned-Noise-Prior}{https://github.com/zhaoyuzhi/Legacy-Photo-Editing-with-Learned-Noise-Prior}
for the codes and the proposed LP dataset.
</p>
<a href="http://arxiv.org/abs/2011.11309" target="_blank">arXiv:2011.11309</a> [<a href="http://arxiv.org/pdf/2011.11309" target="_blank">pdf</a>]

<h2>Uncovering the Bias in Facial Expressions. (arXiv:2011.11311v1 [cs.CV])</h2>
<h3>Jessica Deuschel, Bettina Finzel, Ines Rieger</h3>
<p>Over the past decades the machine and deep learning community has celebrated
great achievements in challenging tasks such as image classification. The deep
architecture of artificial neural networks together with the plenitude of
available data makes it possible to describe highly complex relations. Yet, it
is still impossible to fully capture what the deep learning model has learned
and to verify that it operates fairly and without creating bias, especially in
critical tasks, for instance those arising in the medical field. One example
for such a task is the detection of distinct facial expressions, called Action
Units, in facial images. Considering this specific task, our research aims to
provide transparency regarding bias, specifically in relation to gender and
skin color. We train a neural network for Action Unit classification and
analyze its performance quantitatively based on its accuracy and qualitatively
based on heatmaps. A structured review of our results indicates that we are
able to detect bias. Even though we cannot conclude from our results that lower
classification performance emerged solely from gender and skin color bias,
these biases must be addressed, which is why we end by giving suggestions on
how the detected bias can be avoided.
</p>
<a href="http://arxiv.org/abs/2011.11311" target="_blank">arXiv:2011.11311</a> [<a href="http://arxiv.org/pdf/2011.11311" target="_blank">pdf</a>]

<h2>Building a Parallel Universe Image Synthesis from Land Cover Maps and Auxiliary Raster Data. (arXiv:2011.11314v1 [cs.CV])</h2>
<h3>Gerald Baier, Antonin Deschemps, Michael Schmitt, Naoto Yokoya</h3>
<p>We synthesize both optical RGB and SAR remote sensing images from land cover
maps and auxiliary raster data using GANs. In remote sensing many types of
data, such as digital elevation models or precipitation maps, are often not
reflected in land cover maps but still influence image content or structure.
Including such data in the synthesis process increases the quality of the
generated images and exerts more control on their characteristics. Our method
fuses both inputs by spatially adaptive normalization layers, previously
published as SPADE semantic image synthesis. In contrast to SPADE, these
normalization layers are applied to a full-blown generator architecture
consisting of encoder and decoder, to take full advantage of the information
content in the auxiliary raster data. Our method successfully synthesizes
medium (10m) and high (1m) resolution images, when trained with the
corresponding dataset. We show the advantage of data fusion of land cover maps
and auxiliary information using mean intersection over union, pixel accuracy
and FID using pre-trained U-Net segmentation models. Handpicked images
exemplify how fusing information avoids ambiguities in the synthesized images.
By slightly editing the input our method can be used to synthesize realistic
changes, i.e., raising the water levels. The source code is available at
https://github.com/gbaier/rs_img_synth and we published the newly created
high-resolution dataset at https://ieee-dataport.org/open-access/geonrw.
</p>
<a href="http://arxiv.org/abs/2011.11314" target="_blank">arXiv:2011.11314</a> [<a href="http://arxiv.org/pdf/2011.11314" target="_blank">pdf</a>]

<h2>Characterization of Industrial Smoke Plumes from Remote Sensing Data. (arXiv:2011.11344v1 [cs.CV])</h2>
<h3>Michael Mommert, Mario Sigel, Marcel Neuhausler, Linus Scheibenreif, Damian Borth</h3>
<p>The major driver of global warming has been identified as the anthropogenic
release of greenhouse gas (GHG) emissions from industrial activities. The
quantitative monitoring of these emissions is mandatory to fully understand
their effect on the Earth's climate and to enforce emission regulations on a
large scale. In this work, we investigate the possibility to detect and
quantify industrial smoke plumes from globally and freely available multi-band
image data from ESA's Sentinel-2 satellites. Using a modified ResNet-50, we can
detect smoke plumes of different sizes with an accuracy of 94.3%. The model
correctly ignores natural clouds and focuses on those imaging channels that are
related to the spectral absorption from aerosols and water vapor, enabling the
localization of smoke. We exploit this localization ability and train a U-Net
segmentation model on a labeled sub-sample of our data, resulting in an
Intersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the
detection of any smoke plume of 94.0%; on average, our model can reproduce the
area covered by smoke in an image to within 5.6%. The performance of our model
is mostly limited by occasional confusion with surface objects, the inability
to identify semi-transparent smoke, and human limitations to properly identify
smoke based on RGB-only images. Nevertheless, our results enable us to reliably
detect and qualitatively estimate the level of smoke activity in order to
monitor activity in industrial plants across the globe. Our data set and code
base are publicly available.
</p>
<a href="http://arxiv.org/abs/2011.11344" target="_blank">arXiv:2011.11344</a> [<a href="http://arxiv.org/pdf/2011.11344" target="_blank">pdf</a>]

<h2>Time Series Data Imputation: A Survey on Deep Learning Approaches. (arXiv:2011.11347v1 [cs.LG])</h2>
<h3>Chenguang Fang, Chen Wang</h3>
<p>Time series are all around in real-world applications. However, unexpected
accidents for example broken sensors or missing of the signals will cause
missing values in time series, making the data hard to be utilized. It then
does harm to the downstream applications such as traditional classification or
regression, sequential data integration and forecasting tasks, thus raising the
demand for data imputation. Currently, time series data imputation is a
well-studied problem with different categories of methods. However, these works
rarely take the temporal relations among the observations and treat the time
series as normal structured data, losing the information from the time data. In
recent, deep learning models have raised great attention. Time series methods
based on deep learning have made progress with the usage of models like RNN,
since it captures time information from data. In this paper, we mainly focus on
time series imputation technique with deep learning methods, which recently
made progress in this field. We will review and discuss their model
architectures, their pros and cons as well as their effects to show the
development of the time series imputation methods.
</p>
<a href="http://arxiv.org/abs/2011.11347" target="_blank">arXiv:2011.11347</a> [<a href="http://arxiv.org/pdf/2011.11347" target="_blank">pdf</a>]

<h2>CamVox: A Low-cost and Accurate Lidar-assisted Visual SLAM System. (arXiv:2011.11357v1 [cs.RO])</h2>
<h3>Yuewen Zhu, Chunran Zheng, Chongjian Yuan, Xu Huang, Xiaoping Hong</h3>
<p>Combining lidar in camera-based simultaneous localization and mapping (SLAM)
is an effective method in improving overall accuracy, especially at a large
scale outdoor scenario. Recent development of low-cost lidars (e.g. Livox
lidar) enable us to explore such SLAM systems with lower budget and higher
performance. In this paper we propose CamVox by adapting Livox lidars into
visual SLAM (ORB-SLAM2) by exploring the lidars' unique features. Based on the
non-repeating nature of Livox lidars, we propose an automatic lidar-camera
calibration method that will work in uncontrolled scenes. The long depth
detection range also benefit a more efficient mapping. Comparison of CamVox
with visual SLAM (VINS-mono) and lidar SLAM (LOAM) are evaluated on the same
dataset to demonstrate the performance. We open sourced our hardware, code and
dataset on GitHub.
</p>
<a href="http://arxiv.org/abs/2011.11357" target="_blank">arXiv:2011.11357</a> [<a href="http://arxiv.org/pdf/2011.11357" target="_blank">pdf</a>]

<h2>Synthesis and Pruning as a Dynamic Compression Strategy for Efficient Deep Neural Networks. (arXiv:2011.11358v1 [cs.AI])</h2>
<h3>Alastair Finlinson, Sotiris Moschoyiannis</h3>
<p>The brain is a highly reconfigurable machine capable of task-specific
adaptations. The brain continually rewires itself for a more optimal
configuration to solve problems. We propose a novel strategic synthesis
algorithm for feedforward networks that draws directly from the brain's
behaviours when learning. The proposed approach analyses the network and ranks
weights based on their magnitude. Unlike existing approaches that advocate
random selection, we select highly performing nodes as starting points for new
edges and exploit the Gaussian distribution over the weights to select
corresponding endpoints. The strategy aims only to produce useful connections
and result in a smaller residual network structure. The approach is
complemented with pruning to further the compression. We demonstrate the
techniques to deep feedforward networks. The residual sub-networks that are
formed from the synthesis approaches in this work form common sub-networks with
similarities up to ~90%. Using pruning as a complement to the strategic
synthesis approach, we observe improvements in compression.
</p>
<a href="http://arxiv.org/abs/2011.11358" target="_blank">arXiv:2011.11358</a> [<a href="http://arxiv.org/pdf/2011.11358" target="_blank">pdf</a>]

<h2>A Learning-based Optimization Algorithm:Image Registration Optimizer Network. (arXiv:2011.11365v1 [cs.CV])</h2>
<h3>Jia Wang, Ping Wang, Biao Li, Yinghui Gao, Siyi Zhao</h3>
<p>Remote sensing image registration is valuable for image-based navigation
system despite posing many challenges. As the search space of registration is
usually non-convex, the optimization algorithm, which aims to search the best
transformation parameters, is a challenging step. Conventional optimization
algorithms can hardly reconcile the contradiction of simultaneous rapid
convergence and the global optimization. In this paper, a novel learning-based
optimization algorithm named Image Registration Optimizer Network (IRON) is
proposed, which can predict the global optimum after single iteration. The IRON
is trained by a 3D tensor (9x9x9), which consists of similar metric values. The
elements of the 3D tensor correspond to the 9x9x9 neighbors of the initial
parameters in the search space. Then, the tensor's label is a vector that
points to the global optimal parameters from the initial parameters. Because of
the special architecture, the IRON could predict the global optimum directly
for any initialization. The experimental results demonstrate that the proposed
algorithm performs better than other classical optimization algorithms as it
has higher accuracy, lower root of mean square error (RMSE), and more
efficiency. Our IRON codes are available for further
study.https://www.github.com/jaxwangkd04/IRON
</p>
<a href="http://arxiv.org/abs/2011.11365" target="_blank">arXiv:2011.11365</a> [<a href="http://arxiv.org/pdf/2011.11365" target="_blank">pdf</a>]

<h2>Improving Federated Relational Data Modeling via Basis Alignment and Weight Penalty. (arXiv:2011.11369v1 [cs.LG])</h2>
<h3>Yilun Lin, Chaochao Chen, Cen Chen, Li Wang</h3>
<p>Federated learning (FL) has attracted increasing attention in recent years.
As a privacy-preserving collaborative learning paradigm, it enables a broader
range of applications, especially for computer vision and natural language
processing tasks. However, to date, there is limited research of federated
learning on relational data, namely Knowledge Graph (KG). In this work, we
present a modified version of the graph neural network algorithm that performs
federated modeling over KGs across different participants. Specifically, to
tackle the inherent data heterogeneity issue and inefficiency in algorithm
convergence, we propose a novel optimization algorithm, named FedAlign, with 1)
optimal transportation (OT) for on-client personalization and 2) weight
constraint to speed up the convergence. Extensive experiments have been
conducted on several widely used datasets. Empirical results show that our
proposed method outperforms the state-of-the-art FL methods, such as FedAVG and
FedProx, with better convergence.
</p>
<a href="http://arxiv.org/abs/2011.11369" target="_blank">arXiv:2011.11369</a> [<a href="http://arxiv.org/pdf/2011.11369" target="_blank">pdf</a>]

<h2>On the application of Physically-Guided Neural Networks with Internal Variables to Continuum Problems. (arXiv:2011.11376v1 [cs.LG])</h2>
<h3>Jacobo Ayensa-Jim&#xe9;nez, Mohamed H. Doweidar, Jose A. Sanz-Herrera, Manuel Doblar&#xe9;</h3>
<p>Predictive Physics has been historically based upon the development of
mathematical models that describe the evolution of a system under certain
external stimuli and constraints. The structure of such mathematical models
relies on a set of hysical hypotheses that are assumed to be fulfilled by the
system within a certain range of environmental conditions. A new perspective is
now raising that uses physical knowledge to inform the data prediction
capability of artificial neural networks. A particular extension of this
data-driven approach is Physically-Guided Neural Networks with Internal
Variables (PGNNIV): universal physical laws are used as constraints in the
neural network, in such a way that some neuron values can be interpreted as
internal state variables of the system. This endows the network with unraveling
capacity, as well as better predictive properties such as faster convergence,
fewer data needs and additional noise filtering. Besides, only observable data
are used to train the network, and the internal state equations may be
extracted as a result of the training processes, so there is no need to make
explicit the particular structure of the internal state model. We extend this
new methodology to continuum physical problems, showing again its predictive
and explanatory capacities when only using measurable values in the training
set. We show that the mathematical operators developed for image analysis in
deep learning approaches can be used and extended to consider standard
functional operators in continuum Physics, thus establishing a common framework
for both. The methodology presented demonstrates its ability to discover the
internal constitutive state equation for some problems, including heterogeneous
and nonlinear features, while maintaining its predictive ability for the whole
dataset coverage, with the cost of a single evaluation.
</p>
<a href="http://arxiv.org/abs/2011.11376" target="_blank">arXiv:2011.11376</a> [<a href="http://arxiv.org/pdf/2011.11376" target="_blank">pdf</a>]

<h2>SCGAN: Saliency Map-guided Colorization with Generative Adversarial Network. (arXiv:2011.11377v1 [cs.CV])</h2>
<h3>Yuzhi Zhao, Lai-Man Po, Kwok-Wai Cheung, Wing-Yin Yu, Yasar Abbas Ur Rehman</h3>
<p>Given a grayscale photograph, the colorization system estimates a visually
plausible colorful image. Conventional methods often use semantics to colorize
grayscale images. However, in these methods, only classification semantic
information is embedded, resulting in semantic confusion and color bleeding in
the final colorized image. To address these issues, we propose a fully
automatic Saliency Map-guided Colorization with Generative Adversarial Network
(SCGAN) framework. It jointly predicts the colorization and saliency map to
minimize semantic confusion and color bleeding in the colorized image. Since
the global features from pre-trained VGG-16-Gray network are embedded to the
colorization encoder, the proposed SCGAN can be trained with much less data
than state-of-the-art methods to achieve perceptually reasonable colorization.
In addition, we propose a novel saliency map-based guidance method. Branches of
the colorization decoder are used to predict the saliency map as a proxy
target. Moreover, two hierarchical discriminators are utilized for the
generated colorization and saliency map, respectively, in order to strengthen
visual perception performance. The proposed system is evaluated on ImageNet
validation set. Experimental results show that SCGAN can generate more
reasonable colorized images than state-of-the-art techniques.
</p>
<a href="http://arxiv.org/abs/2011.11377" target="_blank">arXiv:2011.11377</a> [<a href="http://arxiv.org/pdf/2011.11377" target="_blank">pdf</a>]

<h2>Deep Learning for Automatic Quality Grading of Mangoes: Methods and Insights. (arXiv:2011.11378v1 [cs.CV])</h2>
<h3>Shih-Lun Wu, Hsiao-Yen Tung, Yu-Lun Hsu</h3>
<p>The quality grading of mangoes is a crucial task for mango growers as it
vastly affects their profit. However, until today, this process still relies on
laborious efforts of humans, who are prone to fatigue and errors. To remedy
this, the paper approaches the grading task with various convolutional neural
networks (CNN), a tried-and-tested deep learning technology in computer vision.
The models involved include Mask R-CNN (for background removal), the numerous
past winners of the ImageNet challenge, namely AlexNet, VGGs, and ResNets; and,
a family of self-defined convolutional autoencoder-classifiers (ConvAE-Clfs)
inspired by the claimed benefit of multi-task learning in classification tasks.
Transfer learning is also adopted in this work via utilizing the ImageNet
pretrained weights. Besides elaborating on the preprocessing techniques,
training details, and the resulting performance, we go one step further to
provide explainable insights into the model's working with the help of saliency
maps and principal component analysis (PCA). These insights provide a succinct,
meaningful glimpse into the intricate deep learning black box, fostering trust,
and can also be presented to humans in real-world use cases for reviewing the
grading results.
</p>
<a href="http://arxiv.org/abs/2011.11378" target="_blank">arXiv:2011.11378</a> [<a href="http://arxiv.org/pdf/2011.11378" target="_blank">pdf</a>]

<h2>Transfer Adaptation Learning: A Decade Survey. (arXiv:1903.04687v2 [cs.CV] UPDATED)</h2>
<h3>Lei Zhang, Xinbo Gao</h3>
<p>The world we see is ever-changing and it always changes with people, things,
and the environment. Domain is referred to as the state of the world at a
certain moment. A research problem is characterized as transfer adaptation
learning (TAL) when it needs knowledge correspondence between different
moments/domains. Conventional machine learning aims to find a model with the
minimum expected risk on test data by minimizing the regularized empirical risk
on the training data, which, however, supposes that the training and test data
share similar joint probability distribution. TAL aims to build models that can
perform tasks of target domain by learning knowledge from a semantic related
but distribution different source domain. It is an energetic research filed of
increasing influence and importance, which is presenting a blowout publication
trend. This paper surveys the advances of TAL methodologies in the past decade,
and the technical challenges and essential problems of TAL have been observed
and discussed with deep insights and new perspectives. Broader solutions of
transfer adaptation learning being created by researchers are identified, i.e.,
instance re-weighting adaptation, feature adaptation, classifier adaptation,
deep network adaptation and adversarial adaptation, which are beyond the early
semi-supervised and unsupervised split. The survey helps researchers rapidly
but comprehensively understand and identify the research foundation, research
status, theoretical limitations, future challenges and under-studied issues
(universality, interpretability, and credibility) to be broken in the field
toward universal representation and safe applications in open-world scenarios.
</p>
<a href="http://arxiv.org/abs/1903.04687" target="_blank">arXiv:1903.04687</a> [<a href="http://arxiv.org/pdf/1903.04687" target="_blank">pdf</a>]

<h2>Learning to Optimize Computational Resources: Frugal Training with Generalization Guarantees. (arXiv:1905.10819v3 [cs.LG] UPDATED)</h2>
<h3>Maria-Florina Balcan, Tuomas Sandholm, Ellen Vitercik</h3>
<p>Algorithms typically come with tunable parameters that have a considerable
impact on the computational resources they consume. Too often, practitioners
must hand-tune the parameters, a tedious and error-prone task. A recent line of
research provides algorithms that return nearly-optimal parameters from within
a finite set. These algorithms can be used when the parameter space is infinite
by providing as input a random sample of parameters. This data-independent
discretization, however, might miss pockets of nearly-optimal parameters: prior
research has presented scenarios where the only viable parameters lie within an
arbitrarily small region. We provide an algorithm that learns a finite set of
promising parameters from within an infinite set. Our algorithm can help
compile a configuration portfolio, or it can be used to select the input to a
configuration algorithm for finite parameter spaces. Our approach applies to
any configuration problem that satisfies a simple yet ubiquitous structure: the
algorithm's performance is a piecewise constant function of its parameters.
Prior research has exhibited this structure in domains from integer programming
to clustering.
</p>
<a href="http://arxiv.org/abs/1905.10819" target="_blank">arXiv:1905.10819</a> [<a href="http://arxiv.org/pdf/1905.10819" target="_blank">pdf</a>]

<h2>Walsh-Hadamard Variational Inference for Bayesian Deep Learning. (arXiv:1905.11248v2 [stat.ML] UPDATED)</h2>
<h3>Simone Rossi, Sebastien Marmin, Maurizio Filippone</h3>
<p>Over-parameterized models, such as DeepNets and ConvNets, form a class of
models that are routinely adopted in a wide variety of applications, and for
which Bayesian inference is desirable but extremely challenging. Variational
inference offers the tools to tackle this challenge in a scalable way and with
some degree of flexibility on the approximation, but for over-parameterized
models this is challenging due to the over-regularization property of the
variational objective. Inspired by the literature on kernel methods, and in
particular on structured approximations of distributions of random matrices,
this paper proposes Walsh-Hadamard Variational Inference (WHVI), which uses
Walsh-Hadamard-based factorization strategies to reduce the parameterization
and accelerate computations, thus avoiding over-regularization issues with the
variational objective. Extensive theoretical and empirical analyses demonstrate
that WHVI yields considerable speedups and model reductions compared to other
techniques to carry out approximate inference for over-parameterized models,
and ultimately show how advances in kernel methods can be translated into
advances in approximate Bayesian inference.
</p>
<a href="http://arxiv.org/abs/1905.11248" target="_blank">arXiv:1905.11248</a> [<a href="http://arxiv.org/pdf/1905.11248" target="_blank">pdf</a>]

<h2>Interpretable PID Parameter Tuning for Control Engineering using General Dynamic Neural Networks: An Extensive Comparison. (arXiv:1905.13268v3 [cs.LG] UPDATED)</h2>
<h3>Johannes G&#xfc;nther, Elias Reichensd&#xf6;rfer, Patrick M. Pilarski, Klaus Diepold</h3>
<p>Modern automation systems rely on closed loop control, wherein a controller
interacts with a controlled process, based on observations. These systems are
increasingly complex, yet most controllers are linear
Proportional-Integral-Derivative (PID) controllers. PID controllers perform
well on linear and near-linear systems but their simplicity is at odds with the
robustness required to reliably control complex processes. Modern machine
learning offers a way to extend PID controllers beyond their linear
capabilities by using neural networks. However, such an extension comes at the
cost of losing stability guarantees and controller interpretability. In this
paper, we examine the utility of extending PID controllers with recurrent
neural networks-namely, General Dynamic Neural Networks (GDNN); we show that
GDNN (neural) PID controllers perform well on a range of control systems and
highlight how they can be a scalable and interpretable option for control
systems. To do so, we provide an extensive study using four benchmark systems
that represent the most common control engineering benchmarks. All control
benchmarks are evaluated with and without noise as well as with and without
disturbances. The neural PID controller performs better than standard PID
control in 15 of 16 tasks and better than model-based control in 13 of 16
tasks. As a second contribution, we address the lack of interpretability that
prevents neural networks from being used in real-world control processes. We
use bounded-input bounded-output stability analysis to evaluate the parameters
suggested by the neural network, thus making them understandable. This
combination of rigorous evaluation paired with better interpretability is an
important step towards the acceptance of neural-network-based control
approaches. It is furthermore an important step towards interpretable and
safely applied artificial intelligence.
</p>
<a href="http://arxiv.org/abs/1905.13268" target="_blank">arXiv:1905.13268</a> [<a href="http://arxiv.org/pdf/1905.13268" target="_blank">pdf</a>]

<h2>Three-Dimensional Fourier Scattering Transform and Classification of Hyperspectral Images. (arXiv:1906.06804v2 [cs.CV] UPDATED)</h2>
<h3>Ilya Kavalerov, Weilin Li, Wojciech Czaja, Rama Chellappa</h3>
<p>Recent developments in machine learning and signal processing have resulted
in many new techniques that are able to effectively capture the intrinsic yet
complex properties of hyperspectral imagery. Tasks ranging from anomaly
detection to classification can now be solved by taking advantage of very
efficient algorithms which have their roots in representation theory and in
computational approximation. Time-frequency methods are one example of such
techniques. They provide means to analyze and extract the spectral content from
data. On the other hand, hierarchical methods such as neural networks
incorporate spatial information across scales and model multiple levels of
dependencies between spectral features. Both of these approaches have recently
been proven to provide significant advances in the spectral-spatial
classification of hyperspectral imagery. The 3D Fourier scattering transform,
which is introduced in this paper, is an amalgamation of time-frequency
representations with neural network architectures. It leverages the benefits
provided by the Short-Time Fourier Transform with the numerical efficiency of
deep learning network structures. We test the proposed method on several
standard hyperspectral datasets, and we present results that indicate that the
3D Fourier scattering transform is highly effective at representing spectral
content when compared with other state-of-the-art spectral-spatial
classification methods.
</p>
<a href="http://arxiv.org/abs/1906.06804" target="_blank">arXiv:1906.06804</a> [<a href="http://arxiv.org/pdf/1906.06804" target="_blank">pdf</a>]

<h2>GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations. (arXiv:1907.13052v4 [cs.LG] UPDATED)</h2>
<h3>Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner</h3>
<p>Generative latent-variable models are emerging as promising tools in robotics
and reinforcement learning. Yet, even though tasks in these domains typically
involve distinct objects, most state-of-the-art generative models do not
explicitly capture the compositional nature of visual scenes. Two recent
exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised
fashion. Their underlying generative processes, however, do not account for
component interactions. Hence, neither of them allows for principled sampling
of novel scenes. Here we present GENESIS, the first object-centric generative
model of 3D visual scenes capable of both decomposing and generating scenes by
capturing relationships between scene components. GENESIS parameterises a
spatial GMM over images which is decoded from a set of object-centric latent
variables that are either inferred sequentially in an amortised fashion or
sampled from an autoregressive prior. We train GENESIS on several publicly
available datasets and evaluate its performance on scene generation,
decomposition, and semi-supervised learning.
</p>
<a href="http://arxiv.org/abs/1907.13052" target="_blank">arXiv:1907.13052</a> [<a href="http://arxiv.org/pdf/1907.13052" target="_blank">pdf</a>]

<h2>Integrating Multimodal Information in Large Pretrained Transformers. (arXiv:1908.05787v3 [cs.LG] UPDATED)</h2>
<h3>Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, Ehsan Hoque</h3>
<p>Recent Transformer-based contextual word representations, including BERT and
XLNet, have shown state-of-the-art performance in multiple disciplines within
NLP. Fine-tuning the trained contextual models on task-specific datasets has
been the key to achieving superior performance downstream. While fine-tuning
these pre-trained models is straightforward for lexical applications
(applications with only language modality), it is not trivial for multimodal
language (a growing area in NLP focused on modeling face-to-face
communication). Pre-trained models don't have the necessary components to
accept two extra modalities of vision and acoustic. In this paper, we proposed
an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG
allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.
It does so by generating a shift to internal representation of BERT and XLNet;
a shift that is conditioned on the visual and acoustic modalities. In our
experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for
multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly
boosts the sentiment analysis performance over previous baselines as well as
language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet
achieves human-level multimodal sentiment analysis performance for the first
time in the NLP community.
</p>
<a href="http://arxiv.org/abs/1908.05787" target="_blank">arXiv:1908.05787</a> [<a href="http://arxiv.org/pdf/1908.05787" target="_blank">pdf</a>]

<h2>On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons. (arXiv:1909.03194v2 [cs.LG] UPDATED)</h2>
<h3>Wenbo Ren, Jia Liu, Ness B. Shroff</h3>
<p>This paper studies the problem of finding the exact ranking from noisy
comparisons. A comparison over a set of $m$ items produces a noisy outcome
about the most preferred item, and reveals some information about the ranking.
By repeatedly and adaptively choosing items to compare, we want to fully rank
the items with a certain confidence, and use as few comparisons as possible.
Different from most previous works, in this paper, we have three main
novelties: (i) compared to prior works, our upper bounds (algorithms) and lower
bounds on the sample complexity (aka number of comparisons) require the minimal
assumptions on the instances, and are not restricted to specific models; (ii)
we give lower bounds and upper bounds on instances with unequal noise levels;
and (iii) this paper aims at the exact ranking without knowledge on the
instances, while most of the previous works either focus on approximate
rankings or study exact ranking but require prior knowledge. We first derive
lower bounds for pairwise ranking (i.e., compare two items each time), and then
propose (nearly) optimal pairwise ranking algorithms. We further make
extensions to listwise ranking (i.e., comparing multiple items each time).
Numerical results also show our improvements against the state of the art.
</p>
<a href="http://arxiv.org/abs/1909.03194" target="_blank">arXiv:1909.03194</a> [<a href="http://arxiv.org/pdf/1909.03194" target="_blank">pdf</a>]

<h2>GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models. (arXiv:1909.03935v3 [cs.LG] UPDATED)</h2>
<h3>Dingfan Chen, Ning Yu, Yang Zhang, Mario Fritz</h3>
<p>Deep learning has achieved overwhelming success, spanning from discriminative
models to generative models. In particular, deep generative models have
facilitated a new level of performance in a myriad of areas, ranging from media
manipulation to sanitized dataset generation. Despite the great success, the
potential risks of privacy breach caused by generative models have not been
analyzed systematically. In this paper, we focus on membership inference attack
against deep generative models that reveals information about the training data
used for victim models. Specifically, we present the first taxonomy of
membership inference attacks, encompassing not only existing attacks but also
our novel ones. In addition, we propose the first generic attack model that can
be instantiated in a large range of settings and is applicable to various kinds
of deep generative models. Moreover, we provide a theoretically grounded attack
calibration technique, which consistently boosts the attack performance in all
cases, across different attack settings, data modalities, and training
configurations. We complement the systematic analysis of attack performance by
a comprehensive experimental study, that investigates the effectiveness of
various attacks w.r.t. model type and training configurations, over three
diverse application scenarios (i.e., images, medical data, and location data).
</p>
<a href="http://arxiv.org/abs/1909.03935" target="_blank">arXiv:1909.03935</a> [<a href="http://arxiv.org/pdf/1909.03935" target="_blank">pdf</a>]

<h2>Clustering with the Average Silhouette Width. (arXiv:1910.11339v4 [stat.ML] UPDATED)</h2>
<h3>Fatima Batool, Christian Hennig</h3>
<p>The Average Silhouette Width (ASW; Rousseeuw (1987)) is a popular cluster
validation index to estimate the number of clusters. Here we address the
question whether it also is suitable as a general objective function to be
optimized for finding a clustering. We will propose two algorithms (the
standard version OSil and a fast version FOSil) and compare them with existing
clustering methods in an extensive simulation study covering the cases of a
known and unknown number of clusters. Real data sets are also analysed, partly
exploring the use of the new methods with non-Euclidean distances. We will also
show that the ASW satisfies some axioms that have been proposed for cluster
quality functions (Ackerman and Ben-David (2009)). The new methods prove useful
and sensible in many cases, but some weaknesses are also highlighted. These
also concern the use of the ASW for estimating the number of clusters together
with other methods, which is of general interest due to the popularity of the
ASW for this task.
</p>
<a href="http://arxiv.org/abs/1910.11339" target="_blank">arXiv:1910.11339</a> [<a href="http://arxiv.org/pdf/1910.11339" target="_blank">pdf</a>]

<h2>Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations. (arXiv:1911.01507v4 [cs.CV] UPDATED)</h2>
<h3>James Pritts, Zuzana Kukelova, Viktor Larsson, Yaroslava Lochman, Ond&#x159;ej Chum</h3>
<p>This paper introduces minimal solvers that jointly solve for radial lens
undistortion and affine-rectification using local features extracted from the
image of coplanar translated and reflected scene texture, which is common in
man-made environments. The proposed solvers accommodate different types of
local features and sampling strategies, and three of the proposed variants
require just one feature correspondence. State-of-the-art techniques from
algebraic geometry are used to simplify the formulation of the solvers. The
generated solvers are stable, small and fast. Synthetic and real-image
experiments show that the proposed solvers have superior robustness to noise
compared to the state of the art. The solvers are integrated with an automated
system for rectifying imaged scene planes from coplanar repeated texture.
Accurate rectifications on challenging imagery taken with narrow to wide
field-of-view lenses demonstrate the applicability of the proposed solvers.
</p>
<a href="http://arxiv.org/abs/1911.01507" target="_blank">arXiv:1911.01507</a> [<a href="http://arxiv.org/pdf/1911.01507" target="_blank">pdf</a>]

<h2>A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models. (arXiv:1911.01559v3 [cs.LG] UPDATED)</h2>
<h3>Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu, Ting Wang</h3>
<p>Despite their tremendous success in a range of domains, deep learning systems
are inherently susceptible to two types of manipulations: adversarial inputs --
maliciously crafted samples that deceive target deep neural network (DNN)
models, and poisoned models -- adversely forged DNNs that misbehave on
pre-defined inputs. While prior work has intensively studied the two attack
vectors in parallel, there is still a lack of understanding about their
fundamental connections: what are the dynamic interactions between the two
attack vectors? what are the implications of such interactions for optimizing
existing attacks? what are the potential countermeasures against the enhanced
attacks? Answering these key questions is crucial for assessing and mitigating
the holistic vulnerabilities of DNNs deployed in realistic settings.

Here we take a solid step towards this goal by conducting the first
systematic study of the two attack vectors within a unified framework.
Specifically, (i) we develop a new attack model that jointly optimizes
adversarial inputs and poisoned models; (ii) with both analytical and empirical
evidence, we reveal that there exist intriguing "mutual reinforcement" effects
between the two attack vectors -- leveraging one vector significantly amplifies
the effectiveness of the other; (iii) we demonstrate that such effects enable a
large design spectrum for the adversary to enhance the existing attacks that
exploit both vectors (e.g., backdoor attacks), such as maximizing the attack
evasiveness with respect to various detection methods; (iv) finally, we discuss
potential countermeasures against such optimized attacks and their technical
challenges, pointing to several promising research directions.
</p>
<a href="http://arxiv.org/abs/1911.01559" target="_blank">arXiv:1911.01559</a> [<a href="http://arxiv.org/pdf/1911.01559" target="_blank">pdf</a>]

<h2>Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning. (arXiv:1911.07158v3 [cs.CV] UPDATED)</h2>
<h3>Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos, Xiang Chen</h3>
<p>Current state-of-the-art object detectors can have significant performance
drop when deployed in the wild due to domain gaps with training data.
Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models
for new domains/environments without any expensive label cost. However, without
ground truth labels, most prior works on UDA for object detection tasks can
only perform coarse image-level and/or feature-level adaptation by using
adversarial learning methods. In this work, we show that such adversarial-based
methods can only reduce the domain style gap, but cannot address the domain
content distribution gap that is shown to be important for object detectors. To
overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning
(CDSSL) framework by leveraging high-quality pseudo labels to learn better
representations from the target domain directly. To enable SSL for cross-domain
object detection, we propose fine-grained domain transfer,
progressive-confidence-based label sharpening and imbalanced sampling strategy
to address two challenges: (i) non-identical distribution between source and
target domain data, (ii) error amplification/accumulation due to noisy pseudo
labeling on the target domain. Experiment results show that our proposed
approach consistently achieves new state-of-the-art performance (2.2% - 9.5%
better than prior best work on mAP) under various domain gap scenarios. The
code will be released.
</p>
<a href="http://arxiv.org/abs/1911.07158" target="_blank">arXiv:1911.07158</a> [<a href="http://arxiv.org/pdf/1911.07158" target="_blank">pdf</a>]

<h2>Correlative Channel-Aware Fusion for Multi-View Time Series Classification. (arXiv:1911.11561v2 [cs.LG] UPDATED)</h2>
<h3>Yue Bai, Lichen Wang, Zhiqiang Tao, Sheng Li, Yun Fu</h3>
<p>Multi-view time series classification (MVTSC) aims to improve the performance
by fusing the distinctive temporal information from multiple views. Existing
methods mainly focus on fusing multi-view information at an early stage, e.g.,
by learning a common feature subspace among multiple views. However, these
early fusion methods may not fully exploit the unique temporal patterns of each
view in complicated time series. Moreover, the label correlations of multiple
views, which are critical to boost-ing, are usually under-explored for the
MVTSC problem. To address the aforementioned issues, we propose a Correlative
Channel-Aware Fusion (C2AF) network. First, C2AF extracts comprehensive and
robust temporal patterns by a two-stream structured encoder for each view, and
captures the intra-view and inter-view label correlations with a graph-based
correlation matrix. Second, a channel-aware learnable fusion mechanism is
implemented through convolutional neural networks to further explore the global
correlative patterns. These two steps are trained end-to-end in the proposed
C2AF network. Extensive experimental results on three real-world datasets
demonstrate the superiority of our approach over the state-of-the-art methods.
A detailed ablation study is also provided to show the effectiveness of each
model component.
</p>
<a href="http://arxiv.org/abs/1911.11561" target="_blank">arXiv:1911.11561</a> [<a href="http://arxiv.org/pdf/1911.11561" target="_blank">pdf</a>]

<h2>cGANs with Multi-Hinge Loss. (arXiv:1912.04216v2 [cs.LG] UPDATED)</h2>
<h3>Ilya Kavalerov, Wojciech Czaja, Rama Chellappa</h3>
<p>We propose a new algorithm to incorporate class conditional information into
the critic of GANs via a multi-class generalization of the commonly used Hinge
loss that is compatible with both supervised and semi-supervised settings. We
study the compromise between training a state of the art generator and an
accurate classifier simultaneously, and propose a way to use our algorithm to
measure the degree to which a generator and critic are class conditional. We
show the trade-off between a generator-critic pair respecting class
conditioning inputs and generating the highest quality images. With our
multi-hinge loss modification we are able to improve Inception Scores and
Frechet Inception Distance on the Imagenet dataset. We make our tensorflow code
available at https://github.com/ilyakava/gan.
</p>
<a href="http://arxiv.org/abs/1912.04216" target="_blank">arXiv:1912.04216</a> [<a href="http://arxiv.org/pdf/1912.04216" target="_blank">pdf</a>]

<h2>Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation. (arXiv:2001.00503v3 [cs.LG] UPDATED)</h2>
<h3>Letian Chen, Rohan Paleja, Muyleng Ghuy, Matthew Gombolay</h3>
<p>Reinforcement learning (RL) has achieved tremendous success as a general
framework for learning how to make decisions. However, this success relies on
the interactive hand-tuning of a reward function by RL experts. On the other
hand, inverse reinforcement learning (IRL) seeks to learn a reward function
from readily-obtained human demonstrations. Yet, IRL suffers from two major
limitations: 1) reward ambiguity - there are an infinite number of possible
reward functions that could explain an expert's demonstration and 2)
heterogeneity - human experts adopt varying strategies and preferences, which
makes learning from multiple demonstrators difficult due to the common
assumption that demonstrators seeks to maximize the same reward. In this work,
we propose a method to jointly infer a task goal and humans' strategic
preferences via network distillation. This approach enables us to distill a
robust task reward (addressing reward ambiguity) and to model each strategy's
objective (handling heterogeneity). We demonstrate our algorithm can better
recover task reward and strategy rewards and imitate the strategies in two
simulated tasks and a real-world table tennis task.
</p>
<a href="http://arxiv.org/abs/2001.00503" target="_blank">arXiv:2001.00503</a> [<a href="http://arxiv.org/pdf/2001.00503" target="_blank">pdf</a>]

<h2>Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data. (arXiv:2001.03093v4 [cs.RO] UPDATED)</h2>
<h3>Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone</h3>
<p>Reasoning about human motion is an important prerequisite to safe and
socially-aware robotic navigation. As a result, multi-agent behavior prediction
has become a core component of modern human-robot interactive systems, such as
self-driving cars. While there exist many methods for trajectory forecasting,
most do not enforce dynamic constraints and do not account for environmental
information (e.g., maps). Towards this end, we present Trajectron++, a modular,
graph-structured recurrent model that forecasts the trajectories of a general
number of diverse agents while incorporating agent dynamics and heterogeneous
data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated
with robotic planning and control frameworks; for example, it can produce
predictions that are optionally conditioned on ego-agent motion plans. We
demonstrate its performance on several challenging real-world trajectory
forecasting datasets, outperforming a wide array of state-of-the-art
deterministic and generative methods.
</p>
<a href="http://arxiv.org/abs/2001.03093" target="_blank">arXiv:2001.03093</a> [<a href="http://arxiv.org/pdf/2001.03093" target="_blank">pdf</a>]

<h2>On Feature Interactions Identified by Shapley Values of Binary Classification Games. (arXiv:2001.03956v2 [stat.ML] UPDATED)</h2>
<h3>Sandhya Tripathi, N. Hemachandra, Prashant Trivedi</h3>
<p>For feature selection and related problems, we introduce the notion of
classification game, a cooperative game, with features as players and hinge
loss based characteristic function and relate a feature's contribution to
Shapley value based error apportioning (SVEA) of total training error. Our
major contribution is ($\star$) to show that for any dataset the threshold 0 on
SVEA value identifies feature subset whose joint interactions for label
prediction is significant or those features that span a subspace where the data
is predominantly lying. In addition, our scheme ($\star$) identifies the
features on which Bayes classifier doesn't depend but any surrogate loss
function based finite sample classifier does; this contributes to the excess
$0$-$1$ risk of such a classifier, ($\star$) estimates unknown true hinge risk
of a feature, and ($\star$) relate the stability property of an allocation and
negative valued SVEA by designing the analogue of core of classification game.
Due to Shapley value's computationally expensive nature, we build on a known
Monte Carlo based approximation algorithm that computes characteristic function
(Linear Programs) only when needed. We address the potential sample bias
problem in feature selection by providing interval estimates for SVEA values
obtained from multiple sub-samples. We illustrate all the above aspects on
various synthetic and real datasets and show that our scheme achieves better
results than existing recursive feature elimination technique and ReliefF in
most cases. Our theoretically grounded classification game in terms of well
defined characteristic function offers interpretability (which we formalize in
terms of final task) and explainability of our framework, including
identification of important features.
</p>
<a href="http://arxiv.org/abs/2001.03956" target="_blank">arXiv:2001.03956</a> [<a href="http://arxiv.org/pdf/2001.03956" target="_blank">pdf</a>]

<h2>Deep Image Clustering with Tensor Kernels and Unsupervised Companion Objectives. (arXiv:2001.07026v2 [stat.ML] UPDATED)</h2>
<h3>Daniel J. Trosten, Michael C. Kampffmeyer, Robert Jenssen</h3>
<p>In this paper we develop a new model for deep image clustering, using
convolutional neural networks and tensor kernels. The proposed Deep Tensor
Kernel Clustering (DTKC) consists of a convolutional neural network (CNN),
which is trained to reflect a common cluster structure at the output of its
intermediate layers. Encouraging a consistent cluster structure throughout the
network has the potential to guide it towards meaningful clusters, even though
these clusters might appear to be nonlinear in the input space. The cluster
structure is enforced through the idea of unsupervised companion objectives,
where separate loss functions are attached to layers in the network. These
unsupervised companion objectives are constructed based on a proposed
generalization of the Cauchy-Schwarz (CS) divergence, from vectors to tensors
of arbitrary rank. Generalizing the CS divergence to tensor-valued data is a
crucial step, due to the tensorial nature of the intermediate representations
in the CNN. Several experiments are conducted to thoroughly assess the
performance of the proposed DTKC model. The results indicate that the model
outperforms, or performs comparable to, a wide range of baseline algorithms. We
also empirically demonstrate that our model does not suffer from objective
function mismatch, which can be a problematic artifact in autoencoder-based
clustering models.
</p>
<a href="http://arxiv.org/abs/2001.07026" target="_blank">arXiv:2001.07026</a> [<a href="http://arxiv.org/pdf/2001.07026" target="_blank">pdf</a>]

<h2>Lesion Harvester: Iteratively Mining Unlabeled Lesions and Hard-Negative Examples at Scale. (arXiv:2001.07776v3 [cs.CV] UPDATED)</h2>
<h3>Jinzheng Cai, Adam P. Harrison, Youjing Zheng, Ke Yan, Yuankai Huo, Jing Xiao, Lin Yang, Le Lu</h3>
<p>Acquiring large-scale medical image data, necessary for training machine
learning algorithms, is frequently intractable, due to prohibitive
expert-driven annotation costs. Recent datasets extracted from hospital
archives, e.g., DeepLesion, have begun to address this problem. However, these
are often incompletely or noisily labeled, e.g., DeepLesion leaves over 50% of
its lesions unlabeled. Thus, effective methods to harvest missing annotations
are critical for continued progress in medical image analysis. This is the goal
of our work, where we develop a powerful system to harvest missing lesions from
the DeepLesion dataset at high precision. Accepting the need for some degree of
expert labor to achieve high fidelity, we exploit a small fully-labeled subset
of medical image volumes and use it to intelligently mine annotations from the
remainder. To do this, we chain together a highly sensitive lesion proposal
generator and a very selective lesion proposal classifier. While our framework
is generic, we optimize our performance by proposing a 3D contextual lesion
proposal generator and by using a multi-view multi-scale lesion proposal
classifier. These produce harvested and hard-negative proposals, which we then
re-use to finetune our proposal generator by using a novel hard negative
suppression loss, continuing this process until no extra lesions are found.
Extensive experimental analysis demonstrates that our method can harvest an
additional 9,805 lesions while keeping precision above 90%. To demonstrate the
benefits of our approach, we show that lesion detectors trained on our
harvested lesions can significantly outperform the same variants only trained
on the original annotations, with boost of average precision of 7% to 10%. We
open source our annotations at
https://github.com/JimmyCai91/DeepLesionAnnotation.
</p>
<a href="http://arxiv.org/abs/2001.07776" target="_blank">arXiv:2001.07776</a> [<a href="http://arxiv.org/pdf/2001.07776" target="_blank">pdf</a>]

<h2>Meta-learning framework with applications to zero-shot time-series forecasting. (arXiv:2002.02887v2 [cs.LG] UPDATED)</h2>
<h3>Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio</h3>
<p>Can meta-learning discover generic ways of processing time-series (TS) from a
diverse dataset so as to greatly improve generalization on new TS coming from
different datasets? This work provides positive evidence to demonstrate this
using a broad meta-learning framework which we show subsumes many existing
meta-learning algorithms as specific cases. We further identify via theoretical
analysis the meta-learning adaptation mechanisms within N-BEATS, a recent
neural TS forecasting model. Our meta-learning theory predicts that N-BEATS
iteratively generates a subset of its task-specific parameters based on a given
TS input, thus gradually expanding the expressive power of the architecture
on-the-fly. Our empirical results emphasize the importance of meta-learning for
successful zero-shot forecasting to new sources of TS, supporting the claim
that it is viable to train a neural network on a source TS dataset and deploy
it on a different target TS dataset without retraining, resulting in
performance that is at least as good as that of state-of-practice univariate
forecasting models.
</p>
<a href="http://arxiv.org/abs/2002.02887" target="_blank">arXiv:2002.02887</a> [<a href="http://arxiv.org/pdf/2002.02887" target="_blank">pdf</a>]

<h2>Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice. (arXiv:2002.08681v2 [cs.LG] UPDATED)</h2>
<h3>Yabin Zhang, Bin Deng, Hui Tang, Lei Zhang, Kui Jia</h3>
<p>In this paper, we study the formalism of unsupervised multi-class domain
adaptation (multi-class UDA), which underlies a few recent algorithms whose
learning objectives are only motivated empirically. Multi-Class Scoring
Disagreement (MCSD) divergence is presented by aggregating the absolute margin
violations in multi-class classification, and this proposed MCSD is able to
fully characterize the relations between any pair of multi-class scoring
hypotheses. By using MCSD as a measure of domain distance, we develop a new
domain adaptation bound for multi-class UDA; its data-dependent, probably
approximately correct bound is also developed that naturally suggests
adversarial learning objectives to align conditional feature distributions
across source and target domains. Consequently, an algorithmic framework of
Multi-class Domain-adversarial learning Networks (McDalNets) is developed, and
its different instantiations via surrogate learning objectives either coincide
with or resemble a few recently popular methods, thus (partially) underscoring
their practical effectiveness. Based on our identical theory for multi-class
UDA, we also introduce a new algorithm of Domain-Symmetric Networks (SymmNets),
which is featured by a novel adversarial strategy of domain confusion and
discrimination. SymmNets affords simple extensions that work equally well under
the problem settings of either closed set, partial, or open set UDA. We conduct
careful empirical studies to compare different algorithms of McDalNets and our
newly introduced SymmNets. Experiments verify our theoretical analysis and show
the efficacy of our proposed SymmNets. In addition, we have made our
implementation code publicly available.
</p>
<a href="http://arxiv.org/abs/2002.08681" target="_blank">arXiv:2002.08681</a> [<a href="http://arxiv.org/pdf/2002.08681" target="_blank">pdf</a>]

<h2>Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nystr\"om method. (arXiv:2002.09073v2 [cs.LG] UPDATED)</h2>
<h3>Micha&#x142; Derezi&#x144;ski, Rajiv Khanna, Michael W. Mahoney</h3>
<p>The Column Subset Selection Problem (CSSP) and the Nystr\"om method are among
the leading tools for constructing small low-rank approximations of large
datasets in machine learning and scientific computing. A fundamental question
in this area is: how well can a data subset of size k compete with the best
rank k approximation? We develop techniques which exploit spectral properties
of the data matrix to obtain improved approximation guarantees which go beyond
the standard worst-case analysis. Our approach leads to significantly better
bounds for datasets with known rates of singular value decay, e.g., polynomial
or exponential decay. Our analysis also reveals an intriguing phenomenon: the
approximation factor as a function of k may exhibit multiple peaks and valleys,
which we call a multiple-descent curve. A lower bound we establish shows that
this behavior is not an artifact of our analysis, but rather it is an inherent
property of the CSSP and Nystr\"om tasks. Finally, using the example of a
radial basis function (RBF) kernel, we show that both our improved bounds and
the multiple-descent curve can be observed on real datasets simply by varying
the RBF parameter.
</p>
<a href="http://arxiv.org/abs/2002.09073" target="_blank">arXiv:2002.09073</a> [<a href="http://arxiv.org/pdf/2002.09073" target="_blank">pdf</a>]

<h2>Unshuffling Data for Improved Generalization. (arXiv:2002.11894v3 [cs.CV] UPDATED)</h2>
<h3>Damien Teney, Ehsan Abbasnejad, Anton van den Hengel</h3>
<p>Generalization beyond the training distribution is a core challenge in
machine learning. The common practice of mixing and shuffling examples when
training neural networks may not be optimal in this regard. We show that
partitioning the data into well-chosen, non-i.i.d. subsets treated as multiple
training environments can guide the learning of models with better
out-of-distribution generalization. We describe a training procedure to capture
the patterns that are stable across environments while discarding spurious
ones. The method makes a step beyond correlation-based learning: the choice of
the partitioning allows injecting information about the task that cannot be
otherwise recovered from the joint distribution of the training data. We
demonstrate multiple use cases with the task of visual question answering,
which is notorious for dataset biases. We obtain significant improvements on
VQA-CP, using environments built from prior knowledge, existing meta data, or
unsupervised clustering. We also get improvements on GQA using annotations of
"equivalent questions", and on multi-dataset training (VQA v2 / Visual Genome)
by treating them as distinct environments.
</p>
<a href="http://arxiv.org/abs/2002.11894" target="_blank">arXiv:2002.11894</a> [<a href="http://arxiv.org/pdf/2002.11894" target="_blank">pdf</a>]

<h2>Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks. (arXiv:2002.12326v2 [cs.LG] UPDATED)</h2>
<h3>Ioana Bica, James Jordon, Mihaela van der Schaar</h3>
<p>While much attention has been given to the problem of estimating the effect
of discrete interventions from observational data, relatively little work has
been done in the setting of continuous-valued interventions, such as treatments
associated with a dosage parameter. In this paper, we tackle this problem by
building on a modification of the generative adversarial networks (GANs)
framework. Our model, SCIGAN, is flexible and capable of simultaneously
estimating counterfactual outcomes for several different continuous
interventions. The key idea is to use a significantly modified GAN model to
learn to generate counterfactual outcomes, which can then be used to learn an
inference model, using standard supervised methods, capable of estimating these
counterfactuals for a new sample. To address the challenges presented by
shifting to continuous interventions, we propose a novel architecture for our
discriminator - we build a hierarchical discriminator that leverages the
structure of the continuous intervention setting. Moreover, we provide
theoretical results to support our use of the GAN framework and of the
hierarchical discriminator. In the experiments section, we introduce a new
semi-synthetic data simulation for use in the continuous intervention setting
and demonstrate improvements over the existing benchmark models.
</p>
<a href="http://arxiv.org/abs/2002.12326" target="_blank">arXiv:2002.12326</a> [<a href="http://arxiv.org/pdf/2002.12326" target="_blank">pdf</a>]

<h2>Federated Continual Learning with Weighted Inter-client Transfer. (arXiv:2003.03196v3 [cs.LG] UPDATED)</h2>
<h3>Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, Sung Ju Hwang</h3>
<p>There has been a surge of interest in continual learning and federated
learning, both of which are important in deep neural networks in real-world
scenarios. Yet little research has been done regarding the scenario where each
client learns on a sequence of tasks from a private local data stream. This
problem of federated continual learning poses new challenges to continual
learning, such as utilizing knowledge from other clients, while preventing
interference from irrelevant knowledge. To resolve these issues, we propose a
novel federated continual learning framework, Federated Weighted Inter-client
Transfer (FedWeIT), which decomposes the network weights into global federated
parameters and sparse task-specific parameters, and each client receives
selective knowledge from other clients by taking a weighted combination of
their task-specific parameters. FedWeIT minimizes interference between
incompatible tasks, and also allows positive knowledge transfer across clients
during learning. We validate our \emph{FedWeIT}~against existing federated
learning and continual learning methods under varying degrees of task
similarity across clients, and our model significantly outperforms them with a
large reduction in the communication cost.
</p>
<a href="http://arxiv.org/abs/2003.03196" target="_blank">arXiv:2003.03196</a> [<a href="http://arxiv.org/pdf/2003.03196" target="_blank">pdf</a>]

<h2>Belief Base Revision for Further Improvement of Unified Answer Set Programming. (arXiv:2003.04369v2 [cs.AI] UPDATED)</h2>
<h3>Kumar Sankar Ray, Sandip Paul, Diganta Saha</h3>
<p>A belief base revision is developed. The belief base is represented using
Unified Answer Set Programs which is capable of representing imprecise and
uncertain information and perform nonomonotonic reasoning with them. The base
revision operator is developed using Removed Set Revision strategy. The
operator is characterized with respect to the postulates for base revisions
operator satisfies.
</p>
<a href="http://arxiv.org/abs/2003.04369" target="_blank">arXiv:2003.04369</a> [<a href="http://arxiv.org/pdf/2003.04369" target="_blank">pdf</a>]

<h2>A Fractional-Order Normalized Bouc-Wen Model for Piezoelectric Hysteresis Nonlinearity. (arXiv:2003.04917v3 [cs.RO] UPDATED)</h2>
<h3>Shengzheng Kang, Hongtao Wu, Yao Li, Xiaolong Yang, Jiafeng Yao</h3>
<p>This paper presents a new fractional-order normalized Bouc-Wen (BW) (FONBW)
model to describe the asymmetric and rate-dependent hysteresis nonlinearity of
piezoelectric actuators (PEAs). In view of the fact that the classical BW (CBW)
model is only efficient for the symmetric and rate-independent hysteresis
description, the FONBW model is devoted to characterizing the asymmetric and
rate-dependent behaviors of the hysteresis in PEAs by adopting an Nth-order
polynomial input function and two fractional operators, respectively. Different
from the traditional modified BW models, the proposed FONBW model also
eliminates the redundancy of parameters in the CBW model via the normalization
processing. By this way, the developed FONBW model has a relatively simple
mathematic expression with fewer parameters to simultaneously characterize the
asymmetric and rate-dependent hysteresis behaviors of PEAs. Model parameters
are identified by the self-adaptive differential evolution algorithm. To
validate the effectiveness of the proposed model, a series of model
verification and inverse-multiplicative-structure-based feedforward control
experiments are carried out on a PEA system. Results show that the proposed
model is superior to the CBW model and traditional modified BW model in
modeling accuracy and hysteresis compensation.
</p>
<a href="http://arxiv.org/abs/2003.04917" target="_blank">arXiv:2003.04917</a> [<a href="http://arxiv.org/pdf/2003.04917" target="_blank">pdf</a>]

<h2>Visual Task Progress Estimation with Appearance Invariant Embeddings for Robot Control and Planning. (arXiv:2003.06977v4 [cs.RO] UPDATED)</h2>
<h3>Guilherme Maeda, Joni V&#xe4;&#xe4;t&#xe4;inen, Hironori Yoshida</h3>
<p>One of the challenges of full autonomy is to have a robot capable of
manipulating its current environment to achieve another environment
configuration. This paper is a step towards this challenge, focusing on the
visual understanding of the task. Our approach trains a deep neural network to
represent images as measurable features that are useful to estimate the
progress (or phase) of a task. The training uses numerous variations of images
of identical tasks when taken under the same phase index. The goal is to make
the network sensitive to differences in task progress but insensitive to the
appearance of the images. To this end, our method builds upon Time-Contrastive
Networks (TCNs) to train a network using only discrete snapshots taken at
different stages of a task. A robot can then solve long-horizon tasks by using
the trained network to identify the progress of the current task and by
iteratively calling a motion planner until the task is solved. We quantify the
granularity achieved by the network in two simulated environments. In the
first, to detect the number of objects in a scene and in the second to measure
the volume of particles in a cup. Our experiments leverage this granularity to
make a mobile robot move a desired number of objects into a storage area and to
control the amount of pouring in a cup.
</p>
<a href="http://arxiv.org/abs/2003.06977" target="_blank">arXiv:2003.06977</a> [<a href="http://arxiv.org/pdf/2003.06977" target="_blank">pdf</a>]

<h2>Multiscale Sparsifying Transform Learning for Image Denoising. (arXiv:2003.11265v3 [cs.CV] UPDATED)</h2>
<h3>Ashkan Abbasi, Amirhassan Monadjemi, Leyuan Fang, Hossein Rabbani, Neda Noormohammadi, Yi Zhang</h3>
<p>The data-driven sparse methods such as synthesis dictionary learning and
sparsifying transform learning have been proven to be effective in image
denoising. However, these methods are intrinsically single-scale, which ignores
the multiscale nature of images. This often leads to suboptimal results. In
this paper, we propose several strategies to exploit multiscale information in
image denoising through the sparsifying transform learning denoising (TLD)
method. To this end, we first employ a simple method of denoising each wavelet
subband independently via TLD. Then, we show that this method can be greatly
enhanced using wavelet subbands mixing, which is a cheap fusion technique, to
combine the results of single-scale and multiscale methods. Finally, we remove
the need for denoising detail subbands. This simplification leads to an
efficient multiscale denoising method with competitive performance to its
baseline. The effectiveness of the proposed methods are experimentally shown
over two datasets: 1) classic test images corrupted with Gaussian noise, and 2)
fluorescence microscopy images corrupted with real Poisson-Gaussian noise. The
proposed multiscale methods improve over the single-scale baseline method by an
average of about 0.2 dB (in terms of PSNR) for removing synthetic Gaussian
noise form classic test images and real Poisson-Gaussian noise from microscopy
images, respectively. Interestingly, the proposed multiscale methods keep their
superiority over the baseline even when noise is relatively weak. More
importantly, we show that the proposed methods lead to visually pleasing
results, in which edges and textures are better recovered. Extensive
experiments over these two different datasets show that the proposed methods
offer a good trade-off between performance and complexity.
</p>
<a href="http://arxiv.org/abs/2003.11265" target="_blank">arXiv:2003.11265</a> [<a href="http://arxiv.org/pdf/2003.11265" target="_blank">pdf</a>]

<h2>Radon cumulative distribution transform subspace modeling for image classification. (arXiv:2004.03669v2 [cs.CV] UPDATED)</h2>
<h3>Mohammad Shifat-E-Rabbi, Xuwang Yin, Abu Hasnat Mohammad Rubaiyat, Shiying Li, Soheil Kolouri, Akram Aldroubi, Jonathan M. Nichols, Gustavo K. Rohde</h3>
<p>We present a new supervised image classification method applicable to a broad
class of image deformation models. The method makes use of the previously
described Radon Cumulative Distribution Transform (R-CDT) for image data, whose
mathematical properties are exploited to express the image data in a form that
is more suitable for machine learning. While certain operations such as
translation, scaling, and higher-order transformations are challenging to model
in native image space, we show the R-CDT can capture some of these variations
and thus render the associated image classification problems easier to solve.
The method -- utilizing a nearest-subspace algorithm in R-CDT space -- is
simple to implement, non-iterative, has no hyper-parameters to tune, is
computationally efficient, label efficient, and provides competitive accuracies
to state-of-the-art neural networks for many types of classification problems.
In addition to the test accuracy performances, we show improvements (with
respect to neural network-based methods) in terms of computational efficiency
(it can be implemented without the use of GPUs), number of training samples
needed for training, as well as out-of-distribution generalization. The Python
code for reproducing our results is available at
https://github.com/rohdelab/rcdt_ns_classifier.
</p>
<a href="http://arxiv.org/abs/2004.03669" target="_blank">arXiv:2004.03669</a> [<a href="http://arxiv.org/pdf/2004.03669" target="_blank">pdf</a>]

<h2>Towards Transferable Adversarial Attack against Deep Face Recognition. (arXiv:2004.05790v2 [cs.CV] UPDATED)</h2>
<h3>Yaoyao Zhong, Weihong Deng</h3>
<p>Face recognition has achieved great success in the last five years due to the
development of deep learning methods. However, deep convolutional neural
networks (DCNNs) have been found to be vulnerable to adversarial examples. In
particular, the existence of transferable adversarial examples can severely
hinder the robustness of DCNNs since this type of attacks can be applied in a
fully black-box manner without queries on the target system. In this work, we
first investigate the characteristics of transferable adversarial attacks in
face recognition by showing the superiority of feature-level methods over
label-level methods. Then, to further improve transferability of feature-level
adversarial examples, we propose DFANet, a dropout-based method used in
convolutional layers, which can increase the diversity of surrogate models and
obtain ensemble-like effects. Extensive experiments on state-of-the-art face
models with various training databases, loss functions and network
architectures show that the proposed method can significantly enhance the
transferability of existing attack methods. Finally, by applying DFANet to the
LFW database, we generate a new set of adversarial face pairs that can
successfully attack four commercial APIs without any queries. This TALFW
database is available to facilitate research on the robustness and defense of
deep face recognition.
</p>
<a href="http://arxiv.org/abs/2004.05790" target="_blank">arXiv:2004.05790</a> [<a href="http://arxiv.org/pdf/2004.05790" target="_blank">pdf</a>]

<h2>Knowledge Refactoring for Inductive Program Induction. (arXiv:2004.09931v2 [cs.AI] UPDATED)</h2>
<h3>Sebastijan Dumancic, Tias Guns, Andrew Cropper</h3>
<p>Humans constantly restructure knowledge to use it more efficiently. Our goal
is to give a machine learning system similar abilities so that it can learn
more efficiently. We introduce the \textit{knowledge refactoring} problem,
where the goal is to restructure a learner's knowledge base to reduce its size
and to minimise redundancy in it. We focus on inductive logic programming,
where the knowledge base is a logic program. We introduce Knorf, a system which
solves the refactoring problem using constraint optimisation. We evaluate our
approach on two program induction domains: real-world string transformations
and building Lego structures. Our experiments show that learning from
refactored knowledge can improve predictive accuracies fourfold and reduce
learning times by half.
</p>
<a href="http://arxiv.org/abs/2004.09931" target="_blank">arXiv:2004.09931</a> [<a href="http://arxiv.org/pdf/2004.09931" target="_blank">pdf</a>]

<h2>OF-VO: Reliable Navigation among Pedestrians Using Commodity Sensors. (arXiv:2004.10976v3 [cs.RO] UPDATED)</h2>
<h3>Jing Liang, Yi-Ling Qiao, Tianrui Guan, Dinesh Manocha</h3>
<p>We present a novel algorithm for safe navigation of a mobile robot in
uncertain environment among pedestrians. Our approach uses commodity visual
sensors, including mono-camera and a 2D lidar, for explicitly predicting the
velocities and positions of surrounding obstacles through optical flow
estimation, object detection and sensor fusion. Given these probabilistic
partial observations of the environment, we present a modified
velocity-obstacle (VO) algorithm to compute velocities to navigate robot as it
approaches to target. A key aspect of our work is coupling the perception (OF:
optical flow) and planning (VO) components for reliable navigation. Overall,
our OF-VO algorithm is a hybrid combination of learning-based and model-based
methods and offers better performance than prior algorithms in terms of
navigation time and success rate of collision avoidance. We highlight the
realtime performance of OF-VO in simulated and real-world dynamic scenes on a
Turtlebot robot navigating among pedestrians with commodity sensors. A demo
video is available at https://www.youtube.com/watch?v=5sYhZrGwsxM
</p>
<a href="http://arxiv.org/abs/2004.10976" target="_blank">arXiv:2004.10976</a> [<a href="http://arxiv.org/pdf/2004.10976" target="_blank">pdf</a>]

<h2>Bayesian Online Meta-Learning. (arXiv:2005.00146v2 [cs.LG] UPDATED)</h2>
<h3>Pauching Yap, Hippolyt Ritter, David Barber</h3>
<p>Neural networks are known to suffer from catastrophic forgetting when trained
on sequential datasets. While there have been numerous attempts to solve this
problem for large-scale supervised classification, little has been done to
overcome catastrophic forgetting for few-shot classification problems. We
demonstrate that the popular gradient-based few-shot meta-learning algorithm
Model-Agnostic Meta-Learning (MAML) indeed suffers from catastrophic forgetting
and introduce a Bayesian online meta-learning framework that tackles this
problem. Our framework incorporates MAML into a Bayesian online learning
algorithm with Laplace approximation. This framework enables few-shot
classification on a range of sequentially arriving datasets with a single
meta-learned model. The experimental evaluations demonstrate that our framework
can effectively prevent forgetting in various few-shot classification settings
compared to applying MAML sequentially.
</p>
<a href="http://arxiv.org/abs/2005.00146" target="_blank">arXiv:2005.00146</a> [<a href="http://arxiv.org/pdf/2005.00146" target="_blank">pdf</a>]

<h2>Information-Theoretic Generalization Bounds for Meta-Learning and Applications. (arXiv:2005.04372v3 [cs.LG] UPDATED)</h2>
<h3>Sharu Theresa Jose, Osvaldo Simeone</h3>
<p>Meta-learning, or "learning to learn", refers to techniques that infer an
inductive bias from data corresponding to multiple related tasks with the goal
of improving the sample efficiency for new, previously unobserved, tasks. A key
performance measure for meta-learning is the meta-generalization gap, that is,
the difference between the average loss measured on the meta-training data and
on a new, randomly selected task. This paper presents novel
information-theoretic upper bounds on the meta-generalization gap. Two broad
classes of meta-learning algorithms are considered that uses either separate
within-task training and test sets, like MAML, or joint within-task training
and test sets, like Reptile. Extending the existing work for conventional
learning, an upper bound on the meta-generalization gap is derived for the
former class that depends on the mutual information (MI) between the output of
the meta-learning algorithm and its input meta-training data. For the latter,
the derived bound includes an additional MI between the output of the per-task
learning procedure and corresponding data set to capture within-task
uncertainty. Tighter bounds are then developed, under given technical
conditions, for the two classes via novel Individual Task MI (ITMI) bounds.
Applications of the derived bounds are finally discussed, including a broad
class of noisy iterative algorithms for meta-learning.
</p>
<a href="http://arxiv.org/abs/2005.04372" target="_blank">arXiv:2005.04372</a> [<a href="http://arxiv.org/pdf/2005.04372" target="_blank">pdf</a>]

<h2>Transforming variables to central normality. (arXiv:2005.07946v2 [stat.ML] UPDATED)</h2>
<h3>Jakob Raymaekers, Peter J. Rousseeuw</h3>
<p>Many real data sets contain numerical features (variables) whose distribution
is far from normal (gaussian). Instead, their distribution is often skewed. In
order to handle such data it is customary to preprocess the variables to make
them more normal. The Box-Cox and Yeo-Johnson transformations are well-known
tools for this. However, the standard maximum likelihood estimator of their
transformation parameter is highly sensitive to outliers, and will often try to
move outliers inward at the expense of the normality of the central part of the
data. We propose a modification of these transformations as well as an
estimator of the transformation parameter that is robust to outliers, so the
transformed data can be approximately normal in the center and a few outliers
may deviate from it. It compares favorably to existing techniques in an
extensive simulation study and on real data.
</p>
<a href="http://arxiv.org/abs/2005.07946" target="_blank">arXiv:2005.07946</a> [<a href="http://arxiv.org/pdf/2005.07946" target="_blank">pdf</a>]

<h2>3D-OGSE: Online Safe and Smooth Trajectory Generation using Generalized Shape Expansion in Unknown 3-D Environments. (arXiv:2005.13229v5 [cs.RO] UPDATED)</h2>
<h3>Vrushabh Zinage, Senthil Hariharan Arul, Dinesh Manocha, Satadal Ghosh</h3>
<p>In this paper, we present an online motion planning algorithm (3D-OGSE) for
generating smooth, collision-free trajectories over multiple planning
iterations for 3-D agents operating in an unknown obstacle-cluttered 3-D
environment. Our approach constructs a safe-region, termed 'generalized shape',
at each planning iteration, which represents the obstacle-free region based on
locally-sensed environment information. A collision-free path is computed by
sampling points in the generalized shape and is used to generate a smooth,
time-parametrized trajectory by minimizing snap. The generated trajectories are
constrained to lie within the generalized shape, which ensures the agent
maneuvers in the locally obstacle-free space. As the agent reaches boundary of
'sensing shape' in a planning iteration, a re-plan is triggered by receding
horizon planning mechanism that also enables initialization of the next
planning iteration. Theoretical guarantee of probabilistic completeness over
the entire environment and of completely collision-free trajectory generation
is provided. We evaluate the proposed method in simulation on complex 3-D
environments with varied obstacle-densities. We observe that each re-planing
computation takes $\sim$1.4 milliseconds on a single thread of an Intel Core
i5-8500 3.0 GHz CPU. In addition, our method is found to perform 4-10 times
faster than several existing algorithms. In simulation over complex scenarios
such as narrow passages also we observe less conservative behavior.
</p>
<a href="http://arxiv.org/abs/2005.13229" target="_blank">arXiv:2005.13229</a> [<a href="http://arxiv.org/pdf/2005.13229" target="_blank">pdf</a>]

<h2>MOPO: Model-based Offline Policy Optimization. (arXiv:2005.13239v6 [cs.LG] UPDATED)</h2>
<h3>Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, Tengyu Ma</h3>
<p>Offline reinforcement learning (RL) refers to the problem of learning
policies entirely from a large batch of previously collected data. This problem
setting offers the promise of utilizing such datasets to acquire policies
without any costly or dangerous active exploration. However, it is also
challenging, due to the distributional shift between the offline training data
and those states visited by the learned policy. Despite significant recent
progress, the most successful prior methods are model-free and constrain the
policy to the support of data, precluding generalization to unseen states. In
this paper, we first observe that an existing model-based RL algorithm already
produces significant gains in the offline setting compared to model-free
approaches. However, standard model-based RL methods, designed for the online
setting, do not provide an explicit mechanism to avoid the offline setting's
distributional shift issue. Instead, we propose to modify the existing
model-based RL methods by applying them with rewards artificially penalized by
the uncertainty of the dynamics. We theoretically show that the algorithm
maximizes a lower bound of the policy's return under the true MDP. We also
characterize the trade-off between the gain and risk of leaving the support of
the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO),
outperforms standard model-based RL algorithms and prior state-of-the-art
model-free offline RL algorithms on existing offline RL benchmarks and two
challenging continuous control tasks that require generalizing from data
collected for a different task. The code is available at
https://github.com/tianheyu927/mopo.
</p>
<a href="http://arxiv.org/abs/2005.13239" target="_blank">arXiv:2005.13239</a> [<a href="http://arxiv.org/pdf/2005.13239" target="_blank">pdf</a>]

<h2>AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing Label Features from Multi-Modal Embeddings. (arXiv:2005.13402v3 [cs.CV] UPDATED)</h2>
<h3>Pratik Mazumder, Pravendra Singh, Kranti Kumar Parida, Vinay P. Namboodiri</h3>
<p>In this paper, we propose a novel approach for generalized zero-shot learning
in a multi-modal setting, where we have novel classes of audio/video during
testing that are not seen during training. We use the semantic relatedness of
text embeddings as a means for zero-shot learning by aligning audio and video
embeddings with the corresponding class label text feature space. Our approach
uses a cross-modal decoder and a composite triplet loss. The cross-modal
decoder enforces a constraint that the class label text features can be
reconstructed from the audio and video embeddings of data points. This helps
the audio and video embeddings to move closer to the class label text
embedding. The composite triplet loss makes use of the audio, video, and text
embeddings. It helps bring the embeddings from the same class closer and push
away the embeddings from different classes in a multi-modal setting. This helps
the network to perform better on the multi-modal zero-shot learning task.
Importantly, our multi-modal zero-shot learning approach works even if a
modality is missing at test time. We test our approach on the generalized
zero-shot classification and retrieval tasks and show that our approach
outperforms other models in the presence of a single modality as well as in the
presence of multiple modalities. We validate our approach by comparing it with
previous approaches and using various ablations.
</p>
<a href="http://arxiv.org/abs/2005.13402" target="_blank">arXiv:2005.13402</a> [<a href="http://arxiv.org/pdf/2005.13402" target="_blank">pdf</a>]

<h2>A Survey on 3D LiDAR Localization for Autonomous Vehicles. (arXiv:2006.00648v2 [cs.CV] UPDATED)</h2>
<h3>Mahdi Elhousni, Xinming Huang</h3>
<p>LiDAR sensors are becoming one of the most essential sensors in achieving
full autonomy for self driving cars. LiDARs are able to produce rich, dense and
precise spatial data, which can tremendously help in localizing and tracking a
moving vehicle. In this paper, we review the latest finding in 3D LiDAR
localization for autonomous driving cars, and analyse the results obtained by
each method, in an effort to guide the research community towards the path that
seems to be the most promising.
</p>
<a href="http://arxiv.org/abs/2006.00648" target="_blank">arXiv:2006.00648</a> [<a href="http://arxiv.org/pdf/2006.00648" target="_blank">pdf</a>]

<h2>Sampling Techniques in Bayesian Target Encoding. (arXiv:2006.01317v2 [cs.LG] UPDATED)</h2>
<h3>Michael Larionov</h3>
<p>Target encoding is an effective encoding technique of categorical variables
and is often used in machine learning systems for processing tabular data sets
with mixed numeric and categorical variables. Recently en enhanced version of
this encoding technique was proposed by using conjugate Bayesian modeling. This
paper presents a further development of Bayesian encoding method by using
sampling techniques, which helps in extracting information from intra-category
distribution of the target variable, improves generalization and reduces target
leakage.
</p>
<a href="http://arxiv.org/abs/2006.01317" target="_blank">arXiv:2006.01317</a> [<a href="http://arxiv.org/pdf/2006.01317" target="_blank">pdf</a>]

<h2>Automatic Code Summarization via Multi-dimensional Semantic Fusing in GNN. (arXiv:2006.05405v3 [cs.LG] UPDATED)</h2>
<h3>Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, Yang Liu</h3>
<p>Source code summarization aims to generate natural language summaries from
structured code snippets for better understanding code functionalities. Recent
works attempt to encode programs into graphs for learning program semantics and
yield promising results. However, these methods only use simple code
representations (e.g., AST), which limits the capability of learning the rich
semantics for complex programs. Furthermore, these models primarily rely on
graph-based message passing, which only captures local neighborhood relations.
To this end, in this paper, we combine diverse representations of the source
code (i.e., AST, CFG and PDG) into a joint code property graph. To better learn
semantics from the joint graph, we propose a retrieval-augmented mechanism to
augment source code semantics with external knowledge. Furthermore, we propose
a novel attention-based dynamic graph to capture global interactions among
nodes in the static graph and followed a hybrid message passing GNN to
incorporate both static and dynamic graph. To evaluate our proposed approach,
we release a new challenging benchmark, crawled from 200+ diversified
large-scale open-source C projects. Our method achieves the state-of-the-art
performance, improving existing methods by 1.66, 2.38 and 2.22 in terms of
BLEU-4, ROUGE-L and METEOR metrics.
</p>
<a href="http://arxiv.org/abs/2006.05405" target="_blank">arXiv:2006.05405</a> [<a href="http://arxiv.org/pdf/2006.05405" target="_blank">pdf</a>]

<h2>A Practical Online Method for Distributionally Deep Robust Optimization. (arXiv:2006.10138v3 [cs.LG] UPDATED)</h2>
<h3>Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, Tianbao Yang</h3>
<p>In this paper, we propose a practical online method for solving a
distributionally robust optimization (DRO) for deep learning, which has
important applications in machine learning for improving the robustness of
neural networks. In the literature, most methods for solving DRO are based on
stochastic primal-dual methods. However, primal-dual methods for deep DRO
suffer from several drawbacks: (1) manipulating a high-dimensional dual
variable corresponding to the size of data is time expensive; (2) they are not
friendly to online learning where data is coming sequentially. To address these
issues, we transform the min-max formulation into a minimization formulation
and propose a practical duality-free online stochastic method for solving deep
DRO with KL divergence regularization. The proposed online stochastic method
resembles the practical stochastic Nesterov's method in several perspectives
that are widely used for learning deep neural networks. Under a
Polyak-Lojasiewicz (PL) condition, we prove that the proposed method can enjoy
an optimal sample complexity and a better round complexity (the number of
gradient evaluations divided by a fixed mini-batch size) with a moderate
mini-batch size than existing algorithms for solving the min-max or min
formulation of DRO. Of independent interest, the proposed method can be also
used for solving a family of stochastic compositional problems.
</p>
<a href="http://arxiv.org/abs/2006.10138" target="_blank">arXiv:2006.10138</a> [<a href="http://arxiv.org/pdf/2006.10138" target="_blank">pdf</a>]

<h2>Adversarial Transfer of Pose Estimation Regression. (arXiv:2006.11658v2 [cs.CV] UPDATED)</h2>
<h3>Boris Chidlovskii, Assem Sadek</h3>
<p>We address the problem of camera pose estimation in visual localization.
Current regression-based methods for pose estimation are trained and evaluated
scene-wise. They depend on the coordinate frame of the training dataset and
show a low generalization across scenes and datasets. We identify the dataset
shift an important barrier to generalization and consider transfer learning as
an alternative way towards a better reuse of pose estimation models. We revise
domain adaptation techniques for classification and extend them to camera pose
estimation, which is a multi-regression task. We develop a deep adaptation
network for learning scene-invariant image representations and use adversarial
learning to generate such representations for model transfer. We enrich the
network with self-supervised learning and use the adaptability theory to
validate the existence of scene-invariant representation of images in two given
scenes. We evaluate our network on two public datasets, Cambridge Landmarks and
7Scene, demonstrate its superiority over several baselines and compare to the
state of the art methods.
</p>
<a href="http://arxiv.org/abs/2006.11658" target="_blank">arXiv:2006.11658</a> [<a href="http://arxiv.org/pdf/2006.11658" target="_blank">pdf</a>]

<h2>Geometry-Inspired Top-k Adversarial Perturbations. (arXiv:2006.15669v3 [cs.CV] UPDATED)</h2>
<h3>Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets</h3>
<p>Deep learning models are vulnerable to adversarial examples, which endangers
their usage in real-world applications. The main target of existing adversarial
perturbations is primarily limited to change the correct Top-1 predicted class
by the incorrect one, which does not intend changing the Top-$k$ prediction.
However, in many real-world scenarios, especially dealing with digital images,
Top-$k$ predictions are more important. In this work, we propose a simple yet
effective geometry-inspired method of computing Top-$k$ adversarial examples
for any $k$. We evaluate its effectiveness and efficiency by comparing it with
other adversarial example crafting techniques. Moreover, based on this method,
we propose Top-$k$ Universal Adversarial Perturbations, image-agnostic tiny
perturbations that cause true class to be absent among the Top-$k$ prediction
for most inputs in the dataset. We experimentally show that our approach
outperforms baseline methods and even improves existing techniques of
generating Universal Adversarial Perturbations.
</p>
<a href="http://arxiv.org/abs/2006.15669" target="_blank">arXiv:2006.15669</a> [<a href="http://arxiv.org/pdf/2006.15669" target="_blank">pdf</a>]

<h2>Improving Few-Shot Learning using Composite Rotation based Auxiliary Task. (arXiv:2006.15919v2 [cs.CV] UPDATED)</h2>
<h3>Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri</h3>
<p>In this paper, we propose an approach to improve few-shot classification
performance using a composite rotation based auxiliary task. Few-shot
classification methods aim to produce neural networks that perform well for
classes with a large number of training samples and classes with less number of
training samples. They employ techniques to enable the network to produce
highly discriminative features that are also very generic. Generally, the
better the quality and generic-nature of the features produced by the network,
the better is the performance of the network on few-shot learning. Our approach
aims to train networks to produce such features by using a self-supervised
auxiliary task. Our proposed composite rotation based auxiliary task performs
rotation at two levels, i.e., rotation of patches inside the image (inner
rotation) and rotation of the whole image (outer rotation) and assigns one out
of 16 rotation classes to the modified image. We then simultaneously train for
the composite rotation prediction task along with the original classification
task, which forces the network to learn high-quality generic features that help
improve the few-shot classification performance. We experimentally show that
our approach performs better than existing few-shot learning methods on
multiple benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2006.15919" target="_blank">arXiv:2006.15919</a> [<a href="http://arxiv.org/pdf/2006.15919" target="_blank">pdf</a>]

<h2>Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution. (arXiv:2007.00051v2 [cs.LG] UPDATED)</h2>
<h3>Hadi Pouransari, Mojan Javaheripi, Vinay Sharma, Oncel Tuzel</h3>
<p>Knowledge distillation has been used to transfer knowledge learned by a
sophisticated model (teacher) to a simpler model (student). This technique is
widely used to compress model complexity. However, in most applications the
compressed student model suffers from an accuracy gap with its teacher. We
propose extracurricular learning, a novel knowledge distillation method, that
bridges this gap by (1) modeling student and teacher output distributions; (2)
sampling examples from an approximation to the underlying data distribution;
and (3) matching student and teacher output distributions over this extended
set including uncertain samples. We conduct rigorous evaluations on regression
and classification tasks and show that compared to the standard knowledge
distillation, extracurricular learning reduces the gap by 46% to 68%. This
leads to major accuracy improvements compared to the empirical risk
minimization-based training for various recent neural network architectures:
16% regression error reduction on the MPIIGaze dataset, +3.4% to +9.1%
improvement in top-1 classification accuracy on the CIFAR100 dataset, and +2.9%
top-1 improvement on the ImageNet dataset.
</p>
<a href="http://arxiv.org/abs/2007.00051" target="_blank">arXiv:2007.00051</a> [<a href="http://arxiv.org/pdf/2007.00051" target="_blank">pdf</a>]

<h2>Federated Learning with Compression: Unified Analysis and Sharp Guarantees. (arXiv:2007.01154v2 [cs.LG] UPDATED)</h2>
<h3>Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, Mehrdad Mahdavi</h3>
<p>In federated learning, communication cost is often a critical bottleneck to
scale up distributed optimization algorithms to collaboratively learn a model
from millions of devices with potentially unreliable or limited communication
and heterogeneous data distributions. Two notable trends to deal with the
communication overhead of federated algorithms are gradient compression and
local computation with periodic communication. Despite many attempts,
characterizing the relationship between these two approaches has proven
elusive. We address this by proposing a set of algorithms with periodical
compressed (quantized or sparsified) communication and analyze their
convergence properties in both homogeneous and heterogeneous local data
distribution settings. For the homogeneous setting, our analysis improves
existing bounds by providing tighter convergence rates for both strongly convex
and non-convex objective functions. To mitigate data heterogeneity, we
introduce a local gradient tracking scheme and obtain sharp convergence rates
that match the best-known communication complexities without compression for
convex, strongly convex, and nonconvex settings. We complement our theoretical
results and demonstrate the effectiveness of our proposed methods by several
experiments on real-world datasets.
</p>
<a href="http://arxiv.org/abs/2007.01154" target="_blank">arXiv:2007.01154</a> [<a href="http://arxiv.org/pdf/2007.01154" target="_blank">pdf</a>]

<h2>Learning from Failure: Training Debiased Classifier from Biased Classifier. (arXiv:2007.02561v2 [cs.LG] UPDATED)</h2>
<h3>Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, Jinwoo Shin</h3>
<p>Neural networks often learn to make predictions that overly rely on spurious
correlation existing in the dataset, which causes the model to be biased. While
previous work tackles this issue by using explicit labeling on the spuriously
correlated attributes or presuming a particular bias type, we instead utilize a
cheaper, yet generic form of human knowledge, which can be widely applicable to
various types of bias. We first observe that neural networks learn to rely on
the spurious correlation only when it is "easier" to learn than the desired
knowledge, and such reliance is most prominent during the early phase of
training. Based on the observations, we propose a failure-based debiasing
scheme by training a pair of neural networks simultaneously. Our main idea is
twofold; (a) we intentionally train the first network to be biased by
repeatedly amplifying its "prejudice", and (b) we debias the training of the
second network by focusing on samples that go against the prejudice of the
biased network in (a). Extensive experiments demonstrate that our method
significantly improves the training of the network against various types of
biases in both synthetic and real-world datasets. Surprisingly, our framework
even occasionally outperforms the debiasing methods requiring explicit
supervision of the spuriously correlated attributes.
</p>
<a href="http://arxiv.org/abs/2007.02561" target="_blank">arXiv:2007.02561</a> [<a href="http://arxiv.org/pdf/2007.02561" target="_blank">pdf</a>]

<h2>Are Labels Necessary for Classifier Accuracy Evaluation?. (arXiv:2007.02915v2 [cs.CV] UPDATED)</h2>
<h3>Weijian Deng, Liang Zheng</h3>
<p>To calculate the model accuracy on a computer vision task, e.g., object
recognition, we usually require a test set composed of test samples and their
ground truth labels. Whilst standard usage cases satisfy this requirement, many
real-world scenarios involve unlabeled test data, rendering common model
evaluation methods infeasible. We investigate this important and under-explored
problem, Automatic model Evaluation (AutoEval). Specifically, given a labeled
training set and a model, we aim to estimate the model accuracy on unlabeled
test datasets. We construct a meta-dataset: a dataset comprised of datasets
generated from the original training set via various image transformations such
as rotation, background substitution, foreground scaling, etc. As the
classification accuracy of the model on each sample (dataset) is known from the
original dataset labels, our task can be solved via regression. Using the
feature statistics to represent the distribution of a sample dataset, we can
train regression techniques (e.g., a regression neural network) to predict
model performance. Using synthetic meta-dataset and real-world datasets in
training and testing, respectively, we report reasonable and promising
estimates of the model accuracy. We also provide insights into the application
scope, limitation, and future directions of AutoEval.
</p>
<a href="http://arxiv.org/abs/2007.02915" target="_blank">arXiv:2007.02915</a> [<a href="http://arxiv.org/pdf/2007.02915" target="_blank">pdf</a>]

<h2>Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting. (arXiv:2007.03260v3 [cs.LG] UPDATED)</h2>
<h3>Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, Guiguang Ding</h3>
<p>We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter
pruning), which aims to slim down a convolutional neural network (CNN) by
reducing the width (number of output channels) of convolutional layers.
Inspired by the neurobiology research about the independence of remembering and
forgetting, we propose to re-parameterize a CNN into the remembering parts and
forgetting parts, where the former learn to maintain the performance and the
latter learn for efficiency. By training the re-parameterized model using
regular SGD on the former but a novel update rule with penalty gradients on the
latter, we realize structured sparsity, enabling us to equivalently convert the
re-parameterized model into the original architecture with narrower layers.
Such a methodology distinguishes ResRep from the traditional learning-based
pruning paradigm that applies a penalty on parameters to produce structured
sparsity, which may suppress the parameters essential for the remembering. Our
method slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a
narrower one with only 45% FLOPs and no accuracy drop, which is the first to
achieve lossless pruning with such a high compression ratio, to the best of our
knowledge.
</p>
<a href="http://arxiv.org/abs/2007.03260" target="_blank">arXiv:2007.03260</a> [<a href="http://arxiv.org/pdf/2007.03260" target="_blank">pdf</a>]

<h2>InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning. (arXiv:2007.04589v6 [cs.LG] UPDATED)</h2>
<h3>Kwot Sin Lee, Ngoc-Trung Tran, Ngai-Man Cheung</h3>
<p>While Generative Adversarial Networks (GANs) are fundamental to many
generative modelling applications, they suffer from numerous issues. In this
work, we propose a principled framework to simultaneously mitigate two
fundamental issues in GANs: catastrophic forgetting of the discriminator and
mode collapse of the generator. We achieve this by employing for GANs a
contrastive learning and mutual information maximization approach, and perform
extensive analyses to understand sources of improvements. Our approach
significantly stabilizes GAN training and improves GAN performance for image
synthesis across five datasets under the same training and evaluation
conditions against state-of-the-art works. In particular, compared to the
state-of-the-art SSGAN, our approach does not suffer from poorer performance on
image domains such as faces, and instead improves performance significantly.
Our approach is simple to implement and practical: it involves only one
auxiliary objective, has a low computational cost, and performs robustly across
a wide range of training settings and datasets without any hyperparameter
tuning. For reproducibility, our code is available in Mimicry:
https://github.com/kwotsin/mimicry.
</p>
<a href="http://arxiv.org/abs/2007.04589" target="_blank">arXiv:2007.04589</a> [<a href="http://arxiv.org/pdf/2007.04589" target="_blank">pdf</a>]

<h2>Extendable and invertible manifold learning with geometry regularized autoencoders. (arXiv:2007.07142v2 [stat.ML] UPDATED)</h2>
<h3>Andr&#xe9;s F. Duque, Sacha Morin, Guy Wolf, Kevin R. Moon</h3>
<p>A fundamental task in data exploration is to extract simplified low
dimensional representations that capture intrinsic geometry in data, especially
for faithfully visualizing data in two or three dimensions. Common approaches
to this task use kernel methods for manifold learning. However, these methods
typically only provide an embedding of fixed input data and cannot extend to
new data points. Autoencoders have also recently become popular for
representation learning. But while they naturally compute feature extractors
that are both extendable to new data and invertible (i.e., reconstructing
original features from latent representation), they have limited capabilities
to follow global intrinsic geometry compared to kernel-based manifold learning.
We present a new method for integrating both approaches by incorporating a
geometric regularization term in the bottleneck of the autoencoder. Our
regularization, based on the diffusion potential distances from the
recently-proposed PHATE visualization method, encourages the learned latent
representation to follow intrinsic data geometry, similar to manifold learning
algorithms, while still enabling faithful extension to new data and
reconstruction of data in the original feature space from latent coordinates.
We compare our approach with leading kernel methods and autoencoder models for
manifold learning to provide qualitative and quantitative evidence of our
advantages in preserving intrinsic structure, out of sample extension, and
reconstruction. Our method is easily implemented for big-data applications,
whereas other methods are limited in this regard.
</p>
<a href="http://arxiv.org/abs/2007.07142" target="_blank">arXiv:2007.07142</a> [<a href="http://arxiv.org/pdf/2007.07142" target="_blank">pdf</a>]

<h2>Natural Graph Networks. (arXiv:2007.08349v2 [cs.LG] UPDATED)</h2>
<h3>Pim de Haan, Taco Cohen, Max Welling</h3>
<p>A key requirement for graph neural networks is that they must process a graph
in a way that does not depend on how the graph is described. Traditionally this
has been taken to mean that a graph network must be equivariant to node
permutations. Here we show that instead of equivariance, the more general
concept of naturality is sufficient for a graph network to be well-defined,
opening up a larger class of graph networks. We define global and local natural
graph networks, the latter of which are as scalable as conventional message
passing graph neural networks while being more flexible. We give one practical
instantiation of a natural network on graphs which uses an equivariant message
network parameterization, yielding good performance on several benchmarks.
</p>
<a href="http://arxiv.org/abs/2007.08349" target="_blank">arXiv:2007.08349</a> [<a href="http://arxiv.org/pdf/2007.08349" target="_blank">pdf</a>]

<h2>An ensemble classifier for vibration-based quality monitoring. (arXiv:2007.08789v2 [cs.CV] UPDATED)</h2>
<h3>Vahid Yaghoubi, Liangliang Cheng, Wim Van Paepegem, Mathias Kersemans</h3>
<p>Vibration-based quality monitoring of manufactured components often employs
pattern recognition methods. Albeit developing several classification methods,
they usually provide high accuracy for specific types of datasets, but not for
general cases. In this paper, this issue has been addressed by developing a
novel ensemble classifier based on the Dempster-Shafer theory of evidence. To
deal with conflicting evidences, three remedies are proposed prior to
combination: (i) selection of proper classifiers by evaluating the relevancy
between the predicted and target outputs, (ii) devising an optimization method
to minimize the distance between the predicted and target outputs, (iii)
utilizing five different weighting factors, including a new one, to enhance the
fusion performance. The effectiveness of the proposed framework is validated by
its application to 15 UCI and KEEL machine learning datasets. It is then
applied to two vibration-based datasets to detect defected samples: one
synthetic dataset generated from the finite element model of a dogbone
cylinder, and one real experimental dataset generated by collecting broadband
vibrational response of polycrystalline Nickel alloy first-stage turbine
blades. The investigation is made through statistical analysis in presence of
different levels of noise-to-signal ratio. Comparing the results with those of
four state-of-the-art fusion techniques reveals the good performance of the
proposed ensemble method.
</p>
<a href="http://arxiv.org/abs/2007.08789" target="_blank">arXiv:2007.08789</a> [<a href="http://arxiv.org/pdf/2007.08789" target="_blank">pdf</a>]

<h2>Balanced Meta-Softmax for Long-Tailed Visual Recognition. (arXiv:2007.10740v3 [cs.LG] UPDATED)</h2>
<h3>Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, Hongsheng Li</h3>
<p>Deep classifiers have achieved great success in visual recognition. However,
real-world data is long-tailed by nature, leading to the mismatch between
training and testing distributions. In this paper, we show that the Softmax
function, though used in most classification tasks, gives a biased gradient
estimation under the long-tailed setup. This paper presents Balanced Softmax,
an elegant unbiased extension of Softmax, to accommodate the label distribution
shift between training and testing. Theoretically, we derive the generalization
bound for multiclass Softmax regression and show our loss minimizes the bound.
In addition, we introduce Balanced Meta-Softmax, applying a complementary Meta
Sampler to estimate the optimal class sample rate and further improve
long-tailed learning. In our experiments, we demonstrate that Balanced
Meta-Softmax outperforms state-of-the-art long-tailed classification solutions
on both visual recognition and instance segmentation tasks.
</p>
<a href="http://arxiv.org/abs/2007.10740" target="_blank">arXiv:2007.10740</a> [<a href="http://arxiv.org/pdf/2007.10740" target="_blank">pdf</a>]

<h2>PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. (arXiv:2007.10985v3 [cs.CV] UPDATED)</h2>
<h3>Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas J. Guibas, Or Litany</h3>
<p>Arguably one of the top success stories of deep learning is transfer
learning. The finding that pre-training a network on a rich source set (eg.,
ImageNet) can help boost performance once fine-tuned on a usually much smaller
target set, has been instrumental to many applications in language and vision.
Yet, very little is known about its usefulness in 3D point cloud understanding.
We see this as an opportunity considering the effort required for annotating
data in 3D. In this work, we aim at facilitating research on 3D representation
learning. Different from previous works, we focus on high-level scene
understanding tasks. To this end, we select a suite of diverse datasets and
tasks to measure the effect of unsupervised pre-training on a large source set
of 3D scenes. Our findings are extremely encouraging: using a unified triplet
of architecture, source dataset, and contrastive loss for pre-training, we
achieve improvement over recent best results in segmentation and detection
across 6 different benchmarks for indoor and outdoor, real and synthetic
datasets -- demonstrating that the learned representation can generalize across
domains. Furthermore, the improvement was similar to supervised pre-training,
suggesting that future efforts should favor scaling data collection over more
detailed annotation. We hope these findings will encourage more research on
unsupervised pretext task design for 3D deep learning.
</p>
<a href="http://arxiv.org/abs/2007.10985" target="_blank">arXiv:2007.10985</a> [<a href="http://arxiv.org/pdf/2007.10985" target="_blank">pdf</a>]

<h2>Maximum Mutation Reinforcement Learning for Scalable Control. (arXiv:2007.13690v6 [cs.LG] UPDATED)</h2>
<h3>Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, Yuri A. Lawryshyn</h3>
<p>Advances in Reinforcement Learning (RL) have successfully tackled sample
efficiency and overestimation bias. However, these methods often fall short of
scalable performance. On the other hand, genetic methods provide scalability
but depict hyperparameter sensitivity to evolutionary operations. We present
the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. Our
contributions are threefold; ESAC (1) abstracts exploration from exploitation
by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC), (2)
provides dominant skill transfer between offsprings by making use of soft
winner selections and genetic crossovers in hindsight and (3) improves
hyperparameter sensitivity in evolutions using Automatic Mutation Tuning (AMT).
AMT gradually replaces the entropy framework of SAC allowing the population to
succeed at the task while acting as randomly as possible, without making use of
backpropagation updates. On a range of challenging robot control tasks
consisting of high-dimensional action spaces and sparse rewards, ESAC
demonstrates improved performance and sample efficiency in comparison to the
Maximum Entropy framework. ESAC demonstrates scalability comparable to ES on
the basis of hardware resources and algorithm overhead. A complete
implementation of ESAC with notes on reproducibility and videos can be found at
the project website https://karush17.github.io/esac-web/.
</p>
<a href="http://arxiv.org/abs/2007.13690" target="_blank">arXiv:2007.13690</a> [<a href="http://arxiv.org/pdf/2007.13690" target="_blank">pdf</a>]

<h2>Multiple Descent: Design Your Own Generalization Curve. (arXiv:2008.01036v4 [cs.LG] UPDATED)</h2>
<h3>Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi</h3>
<p>This paper explores the generalization loss of linear regression in variably
parameterized families of models, both under-parameterized and
over-parameterized. We show that the generalization curve can have an arbitrary
number of peaks, and moreover, locations of those peaks can be explicitly
controlled. Our results highlight the fact that both classical U-shaped
generalization curve and the recently observed double descent curve are not
intrinsic properties of the model family. Instead, their emergence is due to
the interaction between the properties of the data and the inductive biases of
learning algorithms.
</p>
<a href="http://arxiv.org/abs/2008.01036" target="_blank">arXiv:2008.01036</a> [<a href="http://arxiv.org/pdf/2008.01036" target="_blank">pdf</a>]

<h2>Few-shot Classification via Adaptive Attention. (arXiv:2008.02465v2 [cs.CV] UPDATED)</h2>
<h3>Zihang Jiang, Bingyi Kang, Kuangqi Zhou, Jiashi Feng</h3>
<p>Training a neural network model that can quickly adapt to a new task is
highly desirable yet challenging for few-shot learning problems. Recent
few-shot learning methods mostly concentrate on developing various
meta-learning strategies from two aspects, namely optimizing an initial model
or learning a distance metric. In this work, we propose a novel few-shot
learning method via optimizing and fast adapting the query sample
representation based on very few reference samples. To be specific, we devise a
simple and efficient meta-reweighting strategy to adapt the sample
representations and generate soft attention to refine the representation such
that the relevant features from the query and support samples can be extracted
for a better few-shot classification. Such an adaptive attention model is also
able to explain what the classification model is looking for as the evidence
for classification to some extent. As demonstrated experimentally, the proposed
model achieves state-of-the-art classification results on various benchmark
few-shot classification and fine-grained recognition datasets.
</p>
<a href="http://arxiv.org/abs/2008.02465" target="_blank">arXiv:2008.02465</a> [<a href="http://arxiv.org/pdf/2008.02465" target="_blank">pdf</a>]

<h2>Single-stage intake gesture detection using CTC loss and extended prefix beam search. (arXiv:2008.02999v2 [cs.CV] UPDATED)</h2>
<h3>Philipp V. Rouast, Marc T. P. Adam</h3>
<p>Accurate detection of individual intake gestures is a key step towards
automatic dietary monitoring. Both inertial sensor data of wrist movements and
video data depicting the upper body have been used for this purpose. The most
advanced approaches to date use a two-stage approach, in which (i) frame-level
intake probabilities are learned from the sensor data using a deep neural
network, and then (ii) sparse intake events are detected by finding the maxima
of the frame-level probabilities. In this study, we propose a single-stage
approach which directly decodes the probabilities learned from sensor data into
sparse intake detections. This is achieved by weakly supervised training using
Connectionist Temporal Classification (CTC) loss, and decoding using a novel
extended prefix beam search decoding algorithm. Benefits of this approach
include (i) end-to-end training for detections, (ii) simplified timing
requirements for intake gesture labels, and (iii) improved detection
performance compared to existing approaches. Across two separate datasets, we
achieve relative $F_1$ score improvements between 1.9% and 6.2% over the
two-stage approach for intake detection and eating/drinking detection tasks,
for both video and inertial sensors.
</p>
<a href="http://arxiv.org/abs/2008.02999" target="_blank">arXiv:2008.02999</a> [<a href="http://arxiv.org/pdf/2008.02999" target="_blank">pdf</a>]

<h2>Multimodal Deep Generative Models for Trajectory Prediction: A Conditional Variational Autoencoder Approach. (arXiv:2008.03880v2 [cs.RO] UPDATED)</h2>
<h3>Boris Ivanovic, Karen Leung, Edward Schmerling, Marco Pavone</h3>
<p>Human behavior prediction models enable robots to anticipate how humans may
react to their actions, and hence are instrumental to devising safe and
proactive robot planning algorithms. However, modeling complex interaction
dynamics and capturing the possibility of many possible outcomes in such
interactive settings is very challenging, which has recently prompted the study
of several different approaches. In this work, we provide a self-contained
tutorial on a conditional variational autoencoder (CVAE) approach to human
behavior prediction which, at its core, can produce a multimodal probability
distribution over future human trajectories conditioned on past interactions
and candidate robot future actions. Specifically, the goals of this tutorial
paper are to review and build a taxonomy of state-of-the-art methods in human
behavior prediction, from physics-based to purely data-driven methods, provide
a rigorous yet easily accessible description of a data-driven, CVAE-based
approach, highlight important design characteristics that make this an
attractive model to use in the context of model-based planning for human-robot
interactions, and provide important design considerations when using this class
of models.
</p>
<a href="http://arxiv.org/abs/2008.03880" target="_blank">arXiv:2008.03880</a> [<a href="http://arxiv.org/pdf/2008.03880" target="_blank">pdf</a>]

<h2>ReLU activated Multi-Layer Neural Networks trained with Mixed Integer Linear Programs. (arXiv:2008.08386v2 [cs.LG] UPDATED)</h2>
<h3>Steffen Goebbels</h3>
<p>This paper is a case study to demonstrate that, in principle, multi-layer
feedforward Neural Networks activated by ReLU functions can be iteratively
trained with Mixed Integer Linear Programs. To this end, two simple networks
were trained with a backpropagation-like algorithm on the MNIST dataset that
contains handwritten digits.
</p>
<a href="http://arxiv.org/abs/2008.08386" target="_blank">arXiv:2008.08386</a> [<a href="http://arxiv.org/pdf/2008.08386" target="_blank">pdf</a>]

<h2>Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach. (arXiv:2008.10224v3 [cs.RO] UPDATED)</h2>
<h3>Cristian C. Beltran-Hernandez, Damien Petit, Ixchel G. Ramirez-Alpizar, Kensuke Harada</h3>
<p>Industrial robot manipulators are playing a more significant role in modern
manufacturing industries. Though peg-in-hole assembly is a common industrial
task which has been extensively researched, safely solving complex high
precision assembly in an unstructured environment remains an open problem.
Reinforcement Learning (RL) methods have been proven successful in solving
manipulation tasks autonomously. However, RL is still not widely adopted on
real robotic systems because working with real hardware entails additional
challenges, especially when using position-controlled manipulators. The main
contribution of this work is a learning-based method to solve peg-in-hole tasks
with position uncertainty of the hole. We proposed the use of an off-policy
model-free reinforcement learning method and bootstrap the training speed by
using several transfer learning techniques (sim2real) and domain randomization.
Our proposed learning framework for position-controlled robots was extensively
evaluated on contact-rich insertion tasks on a variety of environments.
</p>
<a href="http://arxiv.org/abs/2008.10224" target="_blank">arXiv:2008.10224</a> [<a href="http://arxiv.org/pdf/2008.10224" target="_blank">pdf</a>]

<h2>Puzzle-AE: Novelty Detection in Images through Solving Puzzles. (arXiv:2008.12959v4 [cs.CV] UPDATED)</h2>
<h3>Mohammadreza Salehi, Ainaz Eftekhar, Niousha Sadjadi, Mohammad Hossein Rohban, Hamid R. Rabiee</h3>
<p>Autoencoder, as an essential part of many anomaly detection methods, is
lacking flexibility on normal data in complex datasets. U-Net is proved to be
effective for this purpose but overfits on the training data if trained by just
using reconstruction error similar to other AE-based frameworks.
Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods,
has earlier proved its ability in learning semantically meaningful features. We
show that training U-Nets based on this task is an effective remedy that
prevents overfitting and facilitates learning beyond pixel-level features.
Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw
puzzles. We propose adversarial robust training as an effective automatic
shortcut removal. We achieve competitive or superior results compared to the
State of the Art (SOTA) anomaly detection methods on various toy and real-world
datasets. Unlike many competitors, the proposed framework is stable, fast,
data-efficient, and does not require unprincipled early stopping.
</p>
<a href="http://arxiv.org/abs/2008.12959" target="_blank">arXiv:2008.12959</a> [<a href="http://arxiv.org/pdf/2008.12959" target="_blank">pdf</a>]

<h2>Sharp finite-sample concentration of independent variables. (arXiv:2008.13293v4 [cs.LG] UPDATED)</h2>
<h3>Akshay Balsubramani</h3>
<p>We show an extension of Sanov's theorem on large deviations, controlling the
tail probabilities of i.i.d. random variables with matching concentration and
anti-concentration bounds. This result has a general scope, applies to samples
of any size, and has a short information-theoretic proof using elementary
techniques.
</p>
<a href="http://arxiv.org/abs/2008.13293" target="_blank">arXiv:2008.13293</a> [<a href="http://arxiv.org/pdf/2008.13293" target="_blank">pdf</a>]

<h2>Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics. (arXiv:2009.00774v2 [cs.LG] UPDATED)</h2>
<h3>Yanchao Sun, Da Huo, Furong Huang</h3>
<p>Poisoning attacks on Reinforcement Learning (RL) systems could take advantage
of RL algorithm's vulnerabilities and cause failure of the learning. However,
prior works on poisoning RL usually either unrealistically assume the attacker
knows the underlying Markov Decision Process (MDP), or directly apply the
poisoning methods in supervised learning to RL. In this work, we build a
generic poisoning framework for online RL via a comprehensive investigation of
heterogeneous poisoning models in RL. Without any prior knowledge of the MDP,
we propose a strategic poisoning algorithm called Vulnerability-Aware
Adversarial Critic Poison (VA2C-P), which works for most policy-based deep RL
agents, closing the gap that no poisoning method exists for policy-based RL
agents. VA2C-P uses a novel metric, stability radius in RL, that measures the
vulnerability of RL algorithms. Experiments on multiple deep RL agents and
multiple environments show that our poisoning algorithm successfully prevents
agents from learning a good policy or teaches the agents to converge to a
target policy, with a limited attacking budget.
</p>
<a href="http://arxiv.org/abs/2009.00774" target="_blank">arXiv:2009.00774</a> [<a href="http://arxiv.org/pdf/2009.00774" target="_blank">pdf</a>]

<h2>Max-value Entropy Search for Multi-Objective Bayesian Optimization with Constraints. (arXiv:2009.01721v2 [cs.LG] UPDATED)</h2>
<h3>Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa</h3>
<p>We consider the problem of constrained multi-objective blackbox optimization
using expensive function evaluations, where the goal is to approximate the true
Pareto set of solutions satisfying a set of constraints while minimizing the
number of function evaluations. For example, in aviation power system design
applications, we need to find the designs that trade-off total energy and the
mass while satisfying specific thresholds for motor temperature and voltage of
cells. This optimization requires performing expensive computational
simulations to evaluate designs. In this paper, we propose a new approach
referred as {\em Max-value Entropy Search for Multi-objective Optimization with
Constraints (MESMOC)} to solve this problem. MESMOC employs an output-space
entropy based acquisition function to efficiently select the sequence of inputs
for evaluation to uncover high-quality pareto-set solutions while satisfying
constraints.

We apply MESMOC to two real-world engineering design applications to
demonstrate its effectiveness over state-of-the-art algorithms.
</p>
<a href="http://arxiv.org/abs/2009.01721" target="_blank">arXiv:2009.01721</a> [<a href="http://arxiv.org/pdf/2009.01721" target="_blank">pdf</a>]

<h2>Information-Theoretic Multi-Objective Bayesian Optimization with Continuous Approximations. (arXiv:2009.05700v3 [stat.ML] UPDATED)</h2>
<h3>Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa</h3>
<p>Many real-world applications involve black-box optimization of multiple
objectives using continuous function approximations that trade-off accuracy and
resource cost of evaluation. For example, in rocket launching research, we need
to find designs that trade-off return-time and angular distance using
continuous-fidelity simulators (e.g., varying tolerance parameter to trade-off
simulation time and accuracy) for design evaluations. The goal is to
approximate the optimal Pareto set by minimizing the cost for evaluations. In
this paper, we propose a novel approach referred to as information-Theoretic
Multi-Objective Bayesian Optimization with Continuous Approximations (iMOCA)}
to solve this problem. The key idea is to select the sequence of input and
function approximations for multiple objectives which maximize the information
gain per unit cost for the optimal Pareto front. Our experiments on diverse
synthetic and real-world benchmarks show that iMOCA significantly improves over
existing single-fidelity methods.
</p>
<a href="http://arxiv.org/abs/2009.05700" target="_blank">arXiv:2009.05700</a> [<a href="http://arxiv.org/pdf/2009.05700" target="_blank">pdf</a>]

<h2>Accurate and Lightweight Image Super-Resolution with Model-Guided Deep Unfolding Network. (arXiv:2009.06254v2 [cs.CV] UPDATED)</h2>
<h3>Qian Ning, Weisheng Dong, Guangming Shi, Leida Li, Xin Li</h3>
<p>Deep neural networks (DNNs) based methods have achieved great success in
single image super-resolution (SISR). However, existing state-of-the-art SISR
techniques are designed like black boxes lacking transparency and
interpretability. Moreover, the improvement in visual quality is often at the
price of increased model complexity due to black-box design. In this paper, we
present and advocate an explainable approach toward SISR named model-guided
deep unfolding network (MoG-DUN). Targeting at breaking the coherence barrier,
we opt to work with a well-established image prior named nonlocal
auto-regressive model and use it to guide our DNN design. By integrating deep
denoising and nonlocal regularization as trainable modules within a deep
learning framework, we can unfold the iterative process of model-based SISR
into a multi-stage concatenation of building blocks with three interconnected
modules (denoising, nonlocal-AR, and reconstruction). The design of all three
modules leverages the latest advances including dense/skip connections as well
as fast nonlocal implementation. In addition to explainability, MoG-DUN is
accurate (producing fewer aliasing artifacts), computationally efficient (with
reduced model parameters), and versatile (capable of handling multiple
degradations). The superiority of the proposed MoG-DUN method to existing
state-of-the-art image SR methods including RCAN, SRMDNF, and SRFBN is
substantiated by extensive experiments on several popular datasets and various
degradation scenarios.
</p>
<a href="http://arxiv.org/abs/2009.06254" target="_blank">arXiv:2009.06254</a> [<a href="http://arxiv.org/pdf/2009.06254" target="_blank">pdf</a>]

<h2>Sub-graph Contrast for Scalable Self-Supervised Graph Representation Learning. (arXiv:2009.10273v3 [cs.LG] UPDATED)</h2>
<h3>Yizhu Jiao, Yun Xiong, Jiawei Zhang, Yao Zhang, Tianqi Zhang, Yangyong Zhu</h3>
<p>Graph representation learning has attracted lots of attention recently.
Existing graph neural networks fed with the complete graph data are not
scalable due to limited computation and memory costs. Thus, it remains a great
challenge to capture rich information in large-scale graph data. Besides, these
methods mainly focus on supervised learning and highly depend on node label
information, which is expensive to obtain in the real world. As to unsupervised
network embedding approaches, they overemphasize node proximity instead, whose
learned representations can hardly be used in downstream application tasks
directly. In recent years, emerging self-supervised learning provides a
potential solution to address the aforementioned problems. However, existing
self-supervised works also operate on the complete graph data and are biased to
fit either global or very local (1-hop neighborhood) graph structures in
defining the mutual information based loss terms.

In this paper, a novel self-supervised representation learning method via
Subgraph Contrast, namely \textsc{Subg-Con}, is proposed by utilizing the
strong correlation between central nodes and their sampled subgraphs to capture
regional structure information. Instead of learning on the complete input graph
data, with a novel data augmentation strategy, \textsc{Subg-Con} learns node
representations through a contrastive loss defined based on subgraphs sampled
from the original graph instead. Compared with existing graph representation
learning approaches, \textsc{Subg-Con} has prominent performance advantages in
weaker supervision requirements, model learning scalability, and
parallelization. Extensive experiments verify both the effectiveness and the
efficiency of our work compared with both classic and state-of-the-art graph
representation learning approaches on multiple real-world large-scale benchmark
datasets from different domains.
</p>
<a href="http://arxiv.org/abs/2009.10273" target="_blank">arXiv:2009.10273</a> [<a href="http://arxiv.org/pdf/2009.10273" target="_blank">pdf</a>]

<h2>MonoClothCap: Towards Temporally Coherent Clothing Capture from Monocular RGB Video. (arXiv:2009.10711v2 [cs.CV] UPDATED)</h2>
<h3>Donglai Xiang, Fabian Prada, Chenglei Wu, Jessica Hodgins</h3>
<p>We present a method to capture temporally coherent dynamic clothing
deformation from a monocular RGB video input. In contrast to the existing
literature, our method does not require a pre-scanned personalized mesh
template, and thus can be applied to in-the-wild videos. To constrain the
output to a valid deformation space, we build statistical deformation models
for three types of clothing: T-shirt, short pants and long pants. A
differentiable renderer is utilized to align our captured shapes to the input
frames by minimizing the difference in both silhouette, segmentation, and
texture. We develop a UV texture growing method which expands the visible
texture region of the clothing sequentially in order to minimize drift in
deformation tracking. We also extract fine-grained wrinkle detail from the
input videos by fitting the clothed surface to the normal maps estimated by a
convolutional neural network. Our method produces temporally coherent
reconstruction of body and clothing from monocular video. We demonstrate
successful clothing capture results from a variety of challenging videos.
Extensive quantitative experiments demonstrate the effectiveness of our method
on metrics including body pose error and surface reconstruction error of the
clothing.
</p>
<a href="http://arxiv.org/abs/2009.10711" target="_blank">arXiv:2009.10711</a> [<a href="http://arxiv.org/pdf/2009.10711" target="_blank">pdf</a>]

<h2>Group Whitening: Balancing Learning Efficiency and Representational Capacity. (arXiv:2009.13333v2 [cs.LG] UPDATED)</h2>
<h3>Lei Huang, Li Liu, Fan Zhu, Ling Shao</h3>
<p>Batch normalization (BN) is an important technique commonly incorporated into
deep learning models to perform standardization within mini-batches. The merits
of BN in improving a model's learning efficiency can be further amplified by
applying whitening, while its drawbacks in estimating population statistics for
inference can be avoided through group normalization (GN). This paper proposes
group whitening (GW), which exploits the advantages of the whitening operation
and avoids the disadvantages of normalization within mini-batches. In addition,
we analyze the constraints imposed on features by normalization, and show how
the batch size (group number) affects the performance of batch (group)
normalized networks, from the perspective of model's representational capacity.
This analysis provides theoretical guidance for applying GW in practice.
Finally, we apply the proposed GW to ResNet and ResNeXt architectures and
conduct experiments on the ImageNet and COCO benchmarks. Results show that GW
consistently improves the performance of different architectures, with absolute
gains of $1.02\%$ $\sim$ $1.49\%$ in top-1 accuracy on ImageNet and $1.82\%$
$\sim$ $3.21\%$ in bounding box AP on COCO.
</p>
<a href="http://arxiv.org/abs/2009.13333" target="_blank">arXiv:2009.13333</a> [<a href="http://arxiv.org/pdf/2009.13333" target="_blank">pdf</a>]

<h2>Fully Automated Left Atrium Segmentation from Anatomical Cine Long-axis MRI Sequences using Deep Convolutional Neural Network with Unscented Kalman Filter. (arXiv:2009.13627v2 [cs.CV] UPDATED)</h2>
<h3>Xiaoran Zhang, Michelle Noga, David Glynn Martin, Kumaradevan Punithakumar</h3>
<p>This study proposes a fully automated approach for the left atrial
segmentation from routine cine long-axis cardiac magnetic resonance image
sequences using deep convolutional neural networks and Bayesian filtering. The
proposed approach consists of a classification network that automatically
detects the type of long-axis sequence and three different convolutional neural
network models followed by unscented Kalman filtering (UKF) that delineates the
left atrium. Instead of training and predicting all long-axis sequence types
together, the proposed approach first identifies the image sequence type as to
2, 3 and 4 chamber views, and then performs prediction based on neural nets
trained for that particular sequence type. The datasets were acquired
retrospectively and ground truth manual segmentation was provided by an expert
radiologist. In addition to neural net based classification and segmentation,
another neural net is trained and utilized to select image sequences for
further processing using UKF to impose temporal consistency over cardiac cycle.
A cyclic dynamic model with time-varying angular frequency is introduced in UKF
to characterize the variations in cardiac motion during image scanning. The
proposed approach was trained and evaluated separately with varying amount of
training data with images acquired from 20, 40, 60 and 80 patients. Evaluations
over 1515 images with equal number of images from each chamber group acquired
from an additional 20 patients demonstrated that the proposed model
outperformed state-of-the-art and yielded a mean Dice coefficient value of
94.1%, 93.7% and 90.1% for 2, 3 and 4-chamber sequences, respectively, when
trained with datasets from 80 patients.
</p>
<a href="http://arxiv.org/abs/2009.13627" target="_blank">arXiv:2009.13627</a> [<a href="http://arxiv.org/pdf/2009.13627" target="_blank">pdf</a>]

<h2>CoKe: Localized Contrastive Learning for Robust Keypoint Detection. (arXiv:2009.14115v3 [cs.CV] UPDATED)</h2>
<h3>Yutong Bai, Angtian Wang, Adam Kortylewski, Alan Yuille</h3>
<p>Today's most popular approaches to keypoint detection involve very complex
network architectures that aim to learn holistic representations of all
keypoints. In this work, we take a step back and ask: Can we simply learn a
local keypoint representation from the output of a standard backbone
architecture? This will help make the network simpler and more robust,
particularly if large parts of the object are occluded. We demonstrate that
this is possible by looking at the problem from the perspective of
representation learning. Specifically, the keypoint kernels need to be chosen
to optimize three types of distances in the feature space: Features of the same
keypoint should be similar to each other, while differing from those of other
keypoints, and also being distinct from features from the background clutter.
We formulate this optimization process within a framework, which we call CoKe,
which includes supervised contrastive learning. CoKe needs to make several
approximations to enable representation learning process on large datasets. In
particular, we introduce a clutter bank to approximate non-keypoint features,
and a momentum update to compute the keypoint representation while training the
feature extractor. Our experiments show that CoKe achieves state-of-the-art
results compared to approaches that jointly represent all keypoints
holistically (Stacked Hourglass Networks, MSS-Net) as well as to approaches
that are supervised by detailed 3D object geometry (StarMap). Moreover, CoKe is
robust and performs exceptionally well when objects are partially occluded and
significantly outperforms related work on a range of diverse datasets
(PASCAL3D+, MPII, ObjectNet3D).
</p>
<a href="http://arxiv.org/abs/2009.14115" target="_blank">arXiv:2009.14115</a> [<a href="http://arxiv.org/pdf/2009.14115" target="_blank">pdf</a>]

<h2>Rain-Code Fusion : Code-to-code ConvLSTM Forecasting Spatiotemporal Precipitation. (arXiv:2009.14573v5 [cs.LG] UPDATED)</h2>
<h3>Yasuno Takato, Ishii Akira, Amakata Masazumi</h3>
<p>Recently, flood damage has become a social problem owing to unexperienced
weather conditions arising from climate change. An immediate response to heavy
rain is important for the mitigation of economic losses and also for rapid
recovery. Spatiotemporal precipitation forecasts may enhance the accuracy of
dam inflow prediction, more than 6 hours forward for flood damage mitigation.
However, the ordinary ConvLSTM has the limitation of predictable range more
than 3-timesteps in real-world precipitation forecasting owing to the
irreducible bias between target prediction and ground-truth value. This paper
proposes a rain-code approach for spatiotemporal precipitation code-to-code
forecasting. We propose a novel rainy feature that represents a temporal rainy
process using multi-frame fusion for the timestep reduction. We perform
rain-code studies with various term ranges based on the standard ConvLSTM. We
applied to a dam region within the Japanese rainy term hourly precipitation
data, under 2006 to 2019 approximately 127 thousands hours, every year from May
to October. We apply the radar analysis hourly data on the central broader
region with an area of 136 x 148 km2 . Finally we have provided sensitivity
studies between the rain-code size and hourly accuracy within the several
forecasting range.
</p>
<a href="http://arxiv.org/abs/2009.14573" target="_blank">arXiv:2009.14573</a> [<a href="http://arxiv.org/pdf/2009.14573" target="_blank">pdf</a>]

<h2>Learning Social Learning. (arXiv:2010.00581v2 [cs.LG] UPDATED)</h2>
<h3>Kamal Ndousse, Douglas Eck, Sergey Levine, Natasha Jaques</h3>
<p>Social learning is a key component of human and animal intelligence. By
taking cues from the behavior of experts in their environment, social learners
can acquire sophisticated behavior and rapidly adapt to new circumstances. This
paper investigates whether independent reinforcement learning (RL) agents in a
multi-agent environment can use social learning to improve their performance
using cues from other agents. We find that in most circumstances, vanilla
model-free RL agents do not use social learning, even in environments in which
individual exploration is expensive. We analyze the reasons for this
deficiency, and show that by introducing a model-based auxiliary loss we are
able to train agents to lever-age cues from experts to solve hard exploration
tasks. The generalized social learning policy learned by these agents allows
them to not only outperform the experts with which they trained, but also
achieve better zero-shot transfer performance than solo learners when deployed
to novel environments with experts. In contrast, agents that have not learned
to rely on social learning generalize poorly and do not succeed in the transfer
task. Further,we find that by mixing multi-agent and solo training, we can
obtain agents that use social learning to out-perform agents trained alone,
even when experts are not avail-able. This demonstrates that social learning
has helped improve agents' representation of the task itself. Our results
indicate that social learning can enable RL agents to not only improve
performance on the task at hand, but improve generalization to novel
environments.
</p>
<a href="http://arxiv.org/abs/2010.00581" target="_blank">arXiv:2010.00581</a> [<a href="http://arxiv.org/pdf/2010.00581" target="_blank">pdf</a>]

<h2>Bongard-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning. (arXiv:2010.00763v3 [cs.AI] UPDATED)</h2>
<h3>Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu, Animashree Anandkumar</h3>
<p>Humans have an inherent ability to learn novel concepts from only a few
samples and generalize these concepts to different situations. Even though
today's machine learning models excel with a plethora of training data on
standard recognition tasks, a considerable gap exists between machine-level
pattern recognition and human-level concept learning. To narrow this gap, the
Bongard Problems (BPs) were introduced as an inspirational challenge for visual
cognition in intelligent systems. Despite new advances in representation
learning and learning to learn, BPs remain a daunting challenge for modern AI.
Inspired by the original one hundred BPs, we propose a new benchmark
Bongard-LOGO for human-level concept learning and reasoning. We develop a
program-guided generation technique to produce a large set of
human-interpretable visual cognition problems in action-oriented LOGO language.
Our benchmark captures three core properties of human cognition: 1)
context-dependent perception, in which the same object may have disparate
interpretations given different contexts; 2) analogy-making perception, in
which some meaningful concepts are traded off for other meaningful concepts;
and 3) perception with a few samples but infinite vocabulary. In experiments,
we show that the state-of-the-art deep learning methods perform substantially
worse than human subjects, implying that they fail to capture core human
cognition properties. Finally, we discuss research directions towards a general
architecture for visual reasoning to tackle this benchmark.
</p>
<a href="http://arxiv.org/abs/2010.00763" target="_blank">arXiv:2010.00763</a> [<a href="http://arxiv.org/pdf/2010.00763" target="_blank">pdf</a>]

<h2>Interpretable Machine Learning for COVID-19: An Empirical Study on Severity Prediction Task. (arXiv:2010.02006v4 [cs.LG] UPDATED)</h2>
<h3>Han Wu, Wenjie Ruan, Jiangtao Wang, Dingchang Zheng, Yayuan Gen, Shaolin Li, Jian Chen, Kunwei Li, Xiangfei Chai, Sumi Helal</h3>
<p>Black-box nature hinders the deployment of many high-accuracy models in
medical diagnosis. It is risky to put one's life in the hands of models that
medical researchers do not trust. However, to understand the mechanism of a new
virus, such as COVID-19, machine learning models may catch important symptoms
that medical practitioners do not notice due to the surge of infected patients
during a pandemic.

In this work, the interpretation of machine learning models reveals that a
high C-reactive protein (CRP) corresponds to severe infection, and severe
patients usually go through a cardiac injury, which is consistent with
well-established medical knowledge. Additionally, through the interpretation of
machine learning models, we find phlegm and diarrhea are two important
symptoms, without which indicate a high risk of turning severe. These two
symptoms are not recognized at the early stage of the outbreak, whereas our
findings are corroborated by later autopsies of COVID-19 patients. We find
patients with a high N-terminal pro B-type natriuretic peptide (NTproBNP) have
a significantly increased risk of death which does not receive much attention
initially but proves true by the following-up study. Thus, we suggest
interpreting machine learning models can offer help to diagnosis at the early
stage of an outbreak.
</p>
<a href="http://arxiv.org/abs/2010.02006" target="_blank">arXiv:2010.02006</a> [<a href="http://arxiv.org/pdf/2010.02006" target="_blank">pdf</a>]

<h2>Federated learning using a mixture of experts. (arXiv:2010.02056v2 [cs.LG] UPDATED)</h2>
<h3>Edvin Listo Zec, Olof Mogren, John Martinsson, Leon Ren&#xe9; S&#xfc;tfeld, Daniel Gillblad</h3>
<p>Federated learning has received attention for its efficiency and privacy
benefits, in settings where data is distributed among devices. Although
federated learning shows significant promise as a key approach when data cannot
be shared or centralized, current incarnations show limited privacy properties
and have shortcomings when applied to common real-world scenarios. One such
scenario is heterogeneous data among devices, where data may come from
different generating distributions. In this paper, we propose a federated
learning framework using a mixture of experts to balance the specialist nature
of a locally trained model with the generalist knowledge of a global model in a
federated learning setting. Our results show that the mixture of experts model
is better suited as a personalized model for devices when data is
heterogeneous, outperforming both global and local models. Furthermore, our
framework gives strict privacy guarantees, which allows clients to select parts
of their data that may be excluded from the federation. The evaluation shows
that the proposed solution is robust to the setting where some users require a
strict privacy setting and do not disclose their models to a central server at
all, opting out from the federation partially or entirely. The proposed
framework is general enough to include any kind of machine learning models, and
can even use combinations of different kinds.
</p>
<a href="http://arxiv.org/abs/2010.02056" target="_blank">arXiv:2010.02056</a> [<a href="http://arxiv.org/pdf/2010.02056" target="_blank">pdf</a>]

<h2>Disentangle-based Continual Graph Representation Learning. (arXiv:2010.02565v3 [cs.LG] UPDATED)</h2>
<h3>Xiaoyu Kou, Yankai Lin, Shaobo Liu, Peng Li, Jie Zhou, Yan Zhang</h3>
<p>Graph embedding (GE) methods embed nodes (and/or edges) in graph into a
low-dimensional semantic space, and have shown its effectiveness in modeling
multi-relational data. However, existing GE models are not practical in
real-world applications since it overlooked the streaming nature of incoming
data. To address this issue, we study the problem of continual graph
representation learning which aims to continually train a GE model on new data
to learn incessantly emerging multi-relational data while avoiding
catastrophically forgetting old learned knowledge. Moreover, we propose a
disentangle-based continual graph representation learning (DiCGRL) framework
inspired by the human's ability to learn procedural knowledge. The experimental
results show that DiCGRL could effectively alleviate the catastrophic
forgetting problem and outperform state-of-the-art continual learning models.
</p>
<a href="http://arxiv.org/abs/2010.02565" target="_blank">arXiv:2010.02565</a> [<a href="http://arxiv.org/pdf/2010.02565" target="_blank">pdf</a>]

<h2>YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors. (arXiv:2010.03320v2 [cs.CV] UPDATED)</h2>
<h3>Kamil Kowol, Matthias Rottmann, Stefan Bracke, Hanno Gottschalk</h3>
<p>In this work, we present an uncertainty-based method for sensor fusion with
camera and radar data. The outputs of two neural networks, one processing
camera and the other one radar data, are combined in an uncertainty aware
manner. To this end, we gather the outputs and corresponding meta information
for both networks. For each predicted object, the gathered information is
post-processed by a gradient boosting method to produce a joint prediction of
both networks. In our experiments we combine the YOLOv3 object detection
network with a customized $1D$ radar segmentation network and evaluate our
method on the nuScenes dataset. In particular we focus on night scenes, where
the capability of object detection networks based on camera data is potentially
handicapped. Our experiments show, that this approach of uncertainty aware
fusion, which is also of very modular nature, significantly gains performance
compared to single sensor baselines and is in range of specifically tailored
deep learning based fusion approaches.
</p>
<a href="http://arxiv.org/abs/2010.03320" target="_blank">arXiv:2010.03320</a> [<a href="http://arxiv.org/pdf/2010.03320" target="_blank">pdf</a>]

<h2>Accelerating Simulation of Stiff Nonlinear Systems using Continuous-Time Echo State Networks. (arXiv:2010.04004v3 [cs.LG] UPDATED)</h2>
<h3>Ranjan Anantharaman, Yingbo Ma, Shashi Gowda, Chris Laughman, Viral Shah, Alan Edelman, Chris Rackauckas</h3>
<p>Modern design, control, and optimization often requires simulation of highly
nonlinear models, leading to prohibitive computational costs. These costs can
be amortized by evaluating a cheap surrogate of the full model. Here we present
a general data-driven method, the continuous-time echo state network (CTESN),
for generating surrogates of nonlinear ordinary differential equations with
dynamics at widely separated timescales. We empirically demonstrate
near-constant time performance using our CTESNs on a physically motivated
scalable model of a heating system whose full execution time increases
exponentially, while maintaining relative error of within 0.2 %. We also show
that our model captures fast transients as well as slow dynamics effectively,
while other techniques such as physics informed neural networks have
difficulties trying to train and predict the highly nonlinear behavior of these
models.
</p>
<a href="http://arxiv.org/abs/2010.04004" target="_blank">arXiv:2010.04004</a> [<a href="http://arxiv.org/pdf/2010.04004" target="_blank">pdf</a>]

<h2>Generating Novel Glyph without Human Data by Learning to Communicate. (arXiv:2010.04402v2 [cs.CV] UPDATED)</h2>
<h3>Seung-won Park</h3>
<p>In this paper, we present Neural Glyph, a system that generates novel glyph
without any training data. The generator and the classifier are trained to
communicate via visual symbols as a medium, which enforces the generator to
come up with a set of distinctive symbols. Our method results in glyphs that
resemble the human-made glyphs, which may imply that the visual appearances of
existing glyphs can be attributed to constraints of communication via writing.
Important tricks that enable this framework are described and the code is made
available.
</p>
<a href="http://arxiv.org/abs/2010.04402" target="_blank">arXiv:2010.04402</a> [<a href="http://arxiv.org/pdf/2010.04402" target="_blank">pdf</a>]

<h2>Gini in a Bottleneck: Sparse Molecular Representations for Graph Convolutional Neural Networks. (arXiv:2010.04535v2 [cs.LG] UPDATED)</h2>
<h3>Ryan Henderson, Djork-Arn&#xe9; Clevert, Floriane Montanari</h3>
<p>Due to the nature of deep learning approaches, it is inherently difficult to
understand which aspects of a molecular graph drive the predictions of the
network. As a mitigation strategy, we constrain certain weights in a multi-task
graph convolutional neural network according to the Gini index to maximize the
"inequality" of the learned representations. We show that this constraint does
not degrade evaluation metrics for some targets, and allows us to combine the
outputs of the graph convolutional operation in a visually interpretable way.
We then perform a proof-of-concept experiment on quantum chemistry targets on
the public QM9 dataset, and a larger experiment on ADMET targets on proprietary
drug-like molecules. Since a benchmark of explainability in the latter case is
difficult, we informally surveyed medicinal chemists within our organization to
check for agreement between regions of the molecule they and the model
identified as relevant to the properties in question.
</p>
<a href="http://arxiv.org/abs/2010.04535" target="_blank">arXiv:2010.04535</a> [<a href="http://arxiv.org/pdf/2010.04535" target="_blank">pdf</a>]

<h2>Domain Agnostic Learning for Unbiased Authentication. (arXiv:2010.05250v2 [stat.ML] UPDATED)</h2>
<h3>Jian Liang, Yuren Cao, Shuang Li, Bing Bai, Hao Li, Fei Wang, Kun Bai</h3>
<p>Authentication is the task of confirming the matching relationship between a
data instance and a given identity. Typical examples of authentication problems
include face recognition and person re-identification. Data-driven
authentication could be affected by undesired biases, i.e., the models are
often trained in one domain (e.g., for people wearing spring outfits) while
applied in other domains (e.g., they change the clothes to summer outfits).
Previous works have made efforts to eliminate domain-difference. They typically
assume domain annotations are provided, and all the domains share classes.
However, for authentication, there could be a large number of domains shared by
different identities/classes, and it is impossible to annotate these domains
exhaustively. It could make domain-difference challenging to model and
eliminate. In this paper, we propose a domain-agnostic method that eliminates
domain-difference without domain labels. We alternately perform latent domain
discovery and domain-difference elimination until our model no longer detects
domain-difference. In our approach, the latent domains are discovered by
learning the heterogeneous predictive relationships between inputs and outputs.
Then domain-difference is eliminated in both class-dependent and
class-independent spaces to improve robustness of elimination. We further
extend our method to a meta-learning framework to pursue more thorough
domain-difference elimination. Comprehensive empirical evaluation results are
provided to demonstrate the effectiveness and superiority of our proposed
method.
</p>
<a href="http://arxiv.org/abs/2010.05250" target="_blank">arXiv:2010.05250</a> [<a href="http://arxiv.org/pdf/2010.05250" target="_blank">pdf</a>]

<h2>Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains. (arXiv:2010.05334v3 [cs.CV] UPDATED)</h2>
<h3>Justin N. M. Pinkney, Doron Adler</h3>
<p>GANs can generate photo-realistic images from the domain of their training
data. However, those wanting to use them for creative purposes often want to
generate imagery from a truly novel domain, a task which GANs are inherently
unable to do. It is also desirable to have a level of control so that there is
a degree of artistic direction rather than purely curation of random results.
Here we present a method for interpolating between generative models of the
StyleGAN architecture in a resolution dependent manner. This allows us to
generate images from an entirely novel domain and do this with a degree of
control over the nature of the output.
</p>
<a href="http://arxiv.org/abs/2010.05334" target="_blank">arXiv:2010.05334</a> [<a href="http://arxiv.org/pdf/2010.05334" target="_blank">pdf</a>]

<h2>On the Power of Abstention and Data-Driven Decision Making for Adversarial Robustness. (arXiv:2010.06154v2 [cs.LG] UPDATED)</h2>
<h3>Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, Hongyang Zhang</h3>
<p>We formally define a feature-space attack where the adversary can perturb
datapoints by arbitrary amounts but in restricted directions. By restricting
the attack to a small random subspace, our model provides a clean abstraction
for non-Lipschitz networks which map small input movements to large feature
movements. We prove that classifiers with the ability to abstain are provably
more powerful than those that cannot in this setting. Specifically, we show
that no matter how well-behaved the natural data is, any classifier that cannot
abstain will be defeated by such an adversary. However, by allowing abstention,
we give a parameterized algorithm with provably good performance against such
an adversary when classes are reasonably well-separated in feature space and
the dimension of the feature space is high. We further use a data-driven method
to set our algorithm parameters to optimize over the accuracy vs. abstention
trade-off with strong theoretical guarantees. Our theory has direct
applications to the technique of contrastive learning, where we empirically
demonstrate the ability of our algorithms to obtain high robust accuracy with
only small amounts of abstention in both supervised and self-supervised
settings. Our results provide a first formal abstention-based gap, and a first
provable optimization for the induced trade-off in an adversarial defense
setting.
</p>
<a href="http://arxiv.org/abs/2010.06154" target="_blank">arXiv:2010.06154</a> [<a href="http://arxiv.org/pdf/2010.06154" target="_blank">pdf</a>]

<h2>Simultaneously forecasting global geomagnetic activity using Recurrent Networks. (arXiv:2010.06487v2 [cs.LG] UPDATED)</h2>
<h3>Charles Topliff, Morris Cohen, William Bristow</h3>
<p>Many systems used by society are extremely vulnerable to space weather events
such as solar flares and geomagnetic storms which could potentially cause
catastrophic damage. In recent years, many works have emerged to provide early
warning to such systems by forecasting these events through some proxy, but
these approaches have largely focused on a specific phenomenon. We present a
sequence-to-sequence learning approach to the problem of forecasting global
space weather conditions at an hourly resolution. This approach improves upon
other work in this field by simultaneously forecasting several key proxies for
geomagnetic activity up to 6 hours in advance. We demonstrate an improvement
over the best currently known predictor of geomagnetic storms, and an
improvement over a persistence baseline several hours in advance.
</p>
<a href="http://arxiv.org/abs/2010.06487" target="_blank">arXiv:2010.06487</a> [<a href="http://arxiv.org/pdf/2010.06487" target="_blank">pdf</a>]

<h2>GreedyFool: Multi-Factor Imperceptibility and Its Application to Designing Black-box Adversarial Example Attack. (arXiv:2010.06855v3 [cs.LG] UPDATED)</h2>
<h3>Hui Liu, Bo Zhao, Jiabao Guo, Yang An, Peng Liu</h3>
<p>Deep neural networks (DNNs) are inherently vulnerable to well-designed input
samples called adversarial examples. The adversary can easily fool DNNs by
adding slight perturbations to the input. In this paper, we propose a novel
black-box adversarial example attack named GreedyFool, which synthesizes
adversarial examples based on the differential evolution and the greedy
approximation. The differential evolution is utilized to evaluate the effects
of perturbed pixels on the confidence of the DNNs-based classifier. The greedy
approximation is an approximate optimization algorithm to automatically get
adversarial perturbations. Existing works synthesize the adversarial examples
by leveraging simple metrics to penalize the perturbations, which lack
sufficient consideration of the human visual system (HVS), resulting in
noticeable artifacts. In order to sufficient imperceptibility, we launch a lot
of investigations into the HVS and design an integrated metric considering just
noticeable distortion (JND), Weber-Fechner law, texture masking and channel
modulation, which is proven to be a better metric to measure the perceptual
distance between the benign examples and the adversarial ones. The experimental
results demonstrate that the GreedyFool has several remarkable properties
including black-box, 100% success rate, flexibility, automation and can
synthesize the more imperceptible adversarial examples than the
state-of-the-art pixel-wise methods.
</p>
<a href="http://arxiv.org/abs/2010.06855" target="_blank">arXiv:2010.06855</a> [<a href="http://arxiv.org/pdf/2010.06855" target="_blank">pdf</a>]

<h2>AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. (arXiv:2010.07468v3 [cs.LG] UPDATED)</h2>
<h3>Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, James S. Duncan</h3>
<p>Most popular optimizers for deep learning can be broadly categorized as
adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient
descent (SGD) with momentum). For many models such as convolutional neural
networks (CNNs), adaptive methods typically converge faster but generalize
worse compared to SGD; for complex settings such as generative adversarial
networks (GANs), adaptive methods are typically the default because of their
stability.We propose AdaBelief to simultaneously achieve three goals: fast
convergence as in adaptive methods, good generalization as in SGD, and training
stability. The intuition for AdaBelief is to adapt the stepsize according to
the "belief" in the current gradient direction. Viewing the exponential moving
average (EMA) of the noisy gradient as the prediction of the gradient at the
next time step, if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step; if the observed
gradient is close to the prediction, we trust it and take a large step. We
validate AdaBelief in extensive experiments, showing that it outperforms other
methods with fast convergence and high accuracy on image classification and
language modeling. Specifically, on ImageNet, AdaBelief achieves comparable
accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief
demonstrates high stability and improves the quality of generated samples
compared to a well-tuned Adam optimizer. Code is available at
https://github.com/juntang-zhuang/Adabelief-Optimizer
</p>
<a href="http://arxiv.org/abs/2010.07468" target="_blank">arXiv:2010.07468</a> [<a href="http://arxiv.org/pdf/2010.07468" target="_blank">pdf</a>]

<h2>Multi-Resolution 3D Mapping with Explicit Free Space Representation for Fast and Accurate Mobile Robot Motion Planning. (arXiv:2010.07929v4 [cs.RO] UPDATED)</h2>
<h3>Nils Funk, Juan Tarrio, Sotiris Papatheodorou, Marija Popovic, Pablo F. Alcantarilla, Stefan Leutenegger</h3>
<p>With the aim of bridging the gap between high quality reconstruction and
mobile robot motion planning, we propose an efficient system that leverages the
concept of adaptive-resolution volumetric mapping, which naturally integrates
with the hierarchical decomposition of space in an octree data structure.
Instead of a Truncated Signed Distance Function (TSDF), we adopt mapping of
occupancy probabilities in log-odds representation, which allows to represent
both surfaces, as well as the entire free, i.e. observed space, as opposed to
unobserved space. We introduce a method for choosing resolution -- on the fly
-- in real-time by means of a multi-scale max-min pooling of the input depth
image. The notion of explicit free space mapping paired with the spatial
hierarchy in the data structure, as well as map resolution, allows for
collision queries, as needed for robot motion planning, at unprecedented speed.
We quantitatively evaluate mapping accuracy, memory, runtime performance, and
planning performance showing improvements over the state of the art,
particularly in cases requiring high resolution maps.
</p>
<a href="http://arxiv.org/abs/2010.07929" target="_blank">arXiv:2010.07929</a> [<a href="http://arxiv.org/pdf/2010.07929" target="_blank">pdf</a>]

<h2>Robot Design With Neural Networks, MILP Solvers and Active Learning. (arXiv:2010.09842v2 [cs.AI] UPDATED)</h2>
<h3>Sanjai Narain, Emily Mak, Dana Chee, Todd Huster, Jeremy Cohen, Kishore Pochiraju, Brendan Englot, Niraj K. Jha, Karthik Narayan</h3>
<p>Central to the design of many robot systems and their controllers is solving
a constrained blackbox optimization problem. This paper presents CNMA, a new
method of solving this problem that is conservative in the number of
potentially expensive blackbox function evaluations; allows specifying complex,
even recursive constraints directly rather than as hard-to-design penalty or
barrier functions; and is resilient to the non-termination of function
evaluations. CNMA leverages the ability of neural networks to approximate any
continuous function, their transformation into equivalent mixed integer linear
programs (MILPs) and their optimization subject to constraints with industrial
strength MILP solvers. A new learning-from-failure step guides the learning to
be relevant to solving the constrained optimization problem. Thus, the amount
of learning is orders of magnitude smaller than that needed to learn functions
over their entire domains. CNMA is illustrated with the design of several
robotic systems: wave-energy propelled boat, lunar lander, hexapod, cartpole,
acrobot and parallel parking. These range from 6 real-valued dimensions to 36.
We show that CNMA surpasses the Nelder-Mead, Gaussian and Random Search
optimization methods against the metric of number of function evaluations.
</p>
<a href="http://arxiv.org/abs/2010.09842" target="_blank">arXiv:2010.09842</a> [<a href="http://arxiv.org/pdf/2010.09842" target="_blank">pdf</a>]

<h2>Learning from Suboptimal Demonstration via Self-Supervised Reward Regression. (arXiv:2010.11723v3 [cs.RO] UPDATED)</h2>
<h3>Letian Chen, Rohan Paleja, Matthew Gombolay</h3>
<p>Learning from Demonstration (LfD) seeks to democratize robotics by enabling
non-roboticist end-users to teach robots to perform a task by providing a human
demonstration. However, modern LfD techniques, e.g. inverse reinforcement
learning (IRL), assume users provide at least stochastically optimal
demonstrations. This assumption fails to hold in most real-world scenarios.
Recent attempts to learn from sub-optimal demonstration leverage pairwise
rankings and following the Luce-Shepard rule. However, we show these approaches
make incorrect assumptions and thus suffer from brittle, degraded performance.
We overcome these limitations in developing a novel approach that bootstraps
off suboptimal demonstrations to synthesize optimality-parameterized data to
train an idealized reward function. We empirically validate we learn an
idealized reward function with ~0.95 correlation with ground-truth reward
versus ~0.75 for prior work. We can then train policies achieving ~200%
improvement over the suboptimal demonstration and ~90% improvement over prior
work. We present a physical demonstration of teaching a robot a topspin strike
in table tennis that achieves 32% faster returns and 40% more topspin than user
demonstration.
</p>
<a href="http://arxiv.org/abs/2010.11723" target="_blank">arXiv:2010.11723</a> [<a href="http://arxiv.org/pdf/2010.11723" target="_blank">pdf</a>]

<h2>Formally Verified SAT-Based AI Planning. (arXiv:2010.14648v3 [cs.AI] UPDATED)</h2>
<h3>Mohammad Abdulaziz, Friedrich Kurz</h3>
<p>We present an executable formally verified SAT encoding of classical AI
planning. We use the theorem prover Isabelle/HOL to perform the verification.
We experimentally test the verified encoding and show that it can be used for
reasonably sized standard planning benchmarks. We also use it as a reference to
test a state-of-the-art SAT-based planner, showing that it sometimes falsely
claims that problems have no solutions of certain lengths.
</p>
<a href="http://arxiv.org/abs/2010.14648" target="_blank">arXiv:2010.14648</a> [<a href="http://arxiv.org/pdf/2010.14648" target="_blank">pdf</a>]

<h2>Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective. (arXiv:2010.15750v3 [cs.LG] UPDATED)</h2>
<h3>Vu Nguyen, Vaden Masrani, Rob Brekelmans, Michael A. Osborne, Frank Wood</h3>
<p>Achieving the full promise of the Thermodynamic Variational Objective (TVO),
a recently proposed variational lower bound on the log evidence involving a
one-dimensional Riemann integral approximation, requires choosing a "schedule"
of sorted discretization points. This paper introduces a bespoke Gaussian
process bandit optimization method for automatically choosing these points. Our
approach not only automates their one-time selection, but also dynamically
adapts their positions over the course of optimization, leading to improved
model learning and inference. We provide theoretical guarantees that our bandit
optimization converges to the regret-minimizing choice of integration points.
Empirical validation of our algorithm is provided in terms of improved learning
and inference in Variational Autoencoders and Sigmoid Belief Networks.
</p>
<a href="http://arxiv.org/abs/2010.15750" target="_blank">arXiv:2010.15750</a> [<a href="http://arxiv.org/pdf/2010.15750" target="_blank">pdf</a>]

<h2>Dynamics Randomization Revisited:A Case Study for Quadrupedal Locomotion. (arXiv:2011.02404v2 [cs.RO] UPDATED)</h2>
<h3>Zhaoming Xie, Xingye Da, Michiel van de Panne, Buck Babich, Animesh Garg</h3>
<p>Understanding the gap between simulation and reality is critical for
reinforcement learning with legged robots, which are largely trained in
simulation. However, recent work has resulted in sometimes conflicting
conclusions with regard to which factors are important for success, including
the role of dynamics randomization. In this paper, we aim to provide clarity
and understanding on the role of dynamics randomization in learning robust
locomotion policies for the Laikago quadruped robot. Surprisingly, in contrast
to prior work with the same robot model, we find that direct sim-to-real
transfer is possible without dynamics randomization or on-robot adaptation
schemes. We conduct extensive ablation studies in a sim-to-sim setting to
understand the key issues underlying successful policy transfer, including
other design decisions that can impact policy robustness. We further ground our
conclusions via sim-to-real experiments with various gaits, speeds, and
stepping frequencies. Additional Details:
https://www.pair.toronto.edu/understanding-dr/.
</p>
<a href="http://arxiv.org/abs/2011.02404" target="_blank">arXiv:2011.02404</a> [<a href="http://arxiv.org/pdf/2011.02404" target="_blank">pdf</a>]

<h2>DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation. (arXiv:2011.02709v3 [cs.CV] UPDATED)</h2>
<h3>Zhenxing Zhang, Lambert Schomaker</h3>
<p>Most existing text-to-image generation methods adopt a multi-stage modular
architecture which has three significant problems: 1) Training multiple
networks increases the run time and affects the convergence and stability of
the generative model; 2) These approaches ignore the quality of early-stage
generator images; 3) Many discriminators need to be trained. To this end, we
propose the Dual Attention Generative Adversarial Network (DTGAN) which can
synthesize high-quality and semantically consistent images only employing a
single generator/discriminator pair. The proposed model introduces
channel-aware and pixel-aware attention modules that can guide the generator to
focus on text-relevant channels and pixels based on the global sentence vector
and to fine-tune original feature maps using attention weights. Also,
Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to
help our attention modules flexibly control the amount of change in shape and
texture by the input natural-language description. Furthermore, a new type of
visual loss is utilized to enhance the image resolution by ensuring vivid shape
and perceptually uniform color distributions of generated images. Experimental
results on benchmark datasets demonstrate the superiority of our proposed
method compared to the state-of-the-art models with a multi-stage framework.
Visualization of the attention maps shows that the channel-aware attention
module is able to localize the discriminative regions, while the pixel-aware
attention module has the ability to capture the globally visual contents for
the generation of an image.
</p>
<a href="http://arxiv.org/abs/2011.02709" target="_blank">arXiv:2011.02709</a> [<a href="http://arxiv.org/pdf/2011.02709" target="_blank">pdf</a>]

<h2>B-GAP: Behavior-Guided Action Prediction for Autonomous Navigation. (arXiv:2011.03748v2 [cs.RO] UPDATED)</h2>
<h3>Angelos Mavrogiannis, Rohan Chandra, Dinesh Manocha</h3>
<p>We present a novel learning algorithm for action prediction and local
navigation for autonomous driving. Our approach classifies the driver behavior
of other vehicles or road-agents (aggressive or conservative) and takes that
into account for decision making and safe driving. We present a behavior-driven
simulator that can generate trajectories corresponding to different levels of
aggressive behaviors and use our simulator to train a policy using graph
convolutional networks. We use a reinforcement learning-based navigation scheme
that uses a proximity graph of traffic agents and computes a safe trajectory
for the ego-vehicle that accounts for aggressive driver maneuvers such as
overtaking, over-speeding, weaving, and sudden lane changes. We have integrated
our algorithm with OpenAI gym-based "Highway-Env" simulator and demonstrate the
benefits in terms of improved navigation in different scenarios.
</p>
<a href="http://arxiv.org/abs/2011.03748" target="_blank">arXiv:2011.03748</a> [<a href="http://arxiv.org/pdf/2011.03748" target="_blank">pdf</a>]

<h2>Deep Learning Analysis and Age Prediction from Shoeprints. (arXiv:2011.03794v2 [cs.CV] UPDATED)</h2>
<h3>Muhammad Hassan (1), Yan Wang (1), Di Wang (2), Daixi Li (3), Yanchun Liang (1), You Zhou (1,2), Dong Xu (4) ((1) Computer Science and Technology, Jilin University, Changchun, (2) Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly, Nanyang Technological University, Singapore, (3) Everspray Science and Technology Company Ltd., (4) Department of Electrical Engineering and Computer Science, University of Missouri, Columbia)</h3>
<p>Human walking and gaits involve several complex body parts and are influenced
by personality, mood, social and cultural traits, and aging. These factors are
reflected in shoeprints, which in turn can be used to predict age, a problem
not systematically addressed using any computational approach. We collected
100,000 shoeprints of subjects ranging from 7 to 80 years old and used the data
to develop a deep learning end-to-end model ShoeNet to analyze age-related
patterns and predict age. The model integrates various convolutional neural
network models together using a skip mechanism to extract age-related features,
especially in pressure and abrasion regions from pair-wise shoeprints. The
results show that 40.23% of the subjects had prediction errors within 5-years
of age and the prediction accuracy for gender classification reached 86.07%.
Interestingly, the age-related features mostly reside in the asymmetric
differences between left and right shoeprints. The analysis also reveals
interesting age-related and gender-related patterns in the pressure
distributions on shoeprints; in particular, the pressure forces spread from the
middle of the toe toward outside regions over age with gender-specific
variations on heel regions. Such statistics provide insight into new methods
for forensic investigations, medical studies of gait-pattern disorders,
biometrics, and sport studies.
</p>
<a href="http://arxiv.org/abs/2011.03794" target="_blank">arXiv:2011.03794</a> [<a href="http://arxiv.org/pdf/2011.03794" target="_blank">pdf</a>]

<h2>Dirichlet Pruning for Neural Network Compression. (arXiv:2011.05985v2 [cs.LG] UPDATED)</h2>
<h3>Kamil Adamczewski, Mijung Park</h3>
<p>We introduce Dirichlet pruning, a novel post-processing technique to
transform a large neural network model into a compressed one. Dirichlet pruning
is a form of structured pruning which assigns the Dirichlet distribution over
each layer's channels in convolutional layers (or neurons in fully-connected
layers), and estimates the parameters of the distribution over these units
using variational inference. The learned distribution allows us to remove
unimportant units, resulting in a compact architecture containing only crucial
features for a task at hand. Our method is extremely fast to train. The number
of newly introduced Dirichlet parameters is only linear in the number of
channels, which allows for rapid training, requiring as little as one epoch to
converge. We perform extensive experiments, in particular on larger
architectures such as VGG and WideResNet (45% and 52% compression rate,
respectively) where our method achieves the state-of-the-art compression
performance and provides interpretable features as a by-product.
</p>
<a href="http://arxiv.org/abs/2011.05985" target="_blank">arXiv:2011.05985</a> [<a href="http://arxiv.org/pdf/2011.05985" target="_blank">pdf</a>]

<h2>Griddly: A platform for AI research in games. (arXiv:2011.06363v2 [cs.AI] UPDATED)</h2>
<h3>Chris Bamford, Shengyi Huang, Simon Lucas</h3>
<p>In recent years, there have been immense breakthroughs in Game AI research,
particularly with Reinforcement Learning (RL). Despite their success, the
underlying games are usually implemented with their own preset environments and
game mechanics, thus making it difficult for researchers to prototype different
game environments. However, testing the RL agents against a variety of game
environments is critical for recent effort to study generalization in RL and
avoid the problem of overfitting that may otherwise occur. In this paper, we
present Griddly as a new platform for Game AI research that provides a unique
combination of highly configurable games, different observer types and an
efficient C++ core engine. Additionally, we present a series of baseline
experiments to study the effect of different observation configurations and
generalization ability of RL agents.
</p>
<a href="http://arxiv.org/abs/2011.06363" target="_blank">arXiv:2011.06363</a> [<a href="http://arxiv.org/pdf/2011.06363" target="_blank">pdf</a>]

<h2>Implicit bias of any algorithm: bounding bias via margin. (arXiv:2011.06550v4 [stat.ML] UPDATED)</h2>
<h3>Elvis Dohmatob</h3>
<p>Consider $n$ points $x_1,\ldots,x_n$ in finite-dimensional euclidean space,
each having one of two colors. Suppose there exists a separating hyperplane
(identified with its unit normal vector $w)$ for the points, i.e a hyperplane
such that points of same color lie on the same side of the hyperplane. We
measure the quality of such a hyperplane by its margin $\gamma(w)$, defined as
minimum distance between any of the points $x_i$ and the hyperplane. In this
paper, we prove that the margin function $\gamma$ satisfies a nonsmooth
Kurdyka-Lojasiewicz inequality with exponent $1/2$. This result has
far-reaching consequences. For example, let $\gamma^{opt}$ be the maximum
possible margin for the problem and let $w^{opt}$ be the parameter for the
hyperplane which attains this value. Given any other separating hyperplane with
parameter $w$, let $d(w):=\|w-w^{opt}\|$ be the euclidean distance between $w$
and $w^{opt}$, also called the bias of $w$. From the previous KL-inequality, we
deduce that $(\gamma^{opt}-\gamma(w)) / R \le d(w) \le
2\sqrt{(\gamma^{opt}-\gamma(w))/\gamma^{opt}}$, where $R:=\max_i \|x_i\|$ is
the maximum distance of the points $x_i$ from the origin. Consequently, for any
optimization algorithm (gradient-descent or not), the bias of the iterates
converges at least as fast as the square-root of the rate of their convergence
of the margin. Thus, our work provides a generic tool for analyzing the
implicit bias of any algorithm in terms of its margin, in situations where a
specialized analysis might not be available: it is sufficient to establish a
good rate for converge of the margin, a task which is usually much easier.
</p>
<a href="http://arxiv.org/abs/2011.06550" target="_blank">arXiv:2011.06550</a> [<a href="http://arxiv.org/pdf/2011.06550" target="_blank">pdf</a>]

<h2>Reward Biased Maximum Likelihood Estimation for Reinforcement Learning. (arXiv:2011.07738v2 [cs.LG] UPDATED)</h2>
<h3>Akshay Mete, Rahul Singh, P.R. Kumar</h3>
<p>The Reward-Biased Maximum Likelihood Estimate (RBMLE) for adaptive control of
Markov chains was proposed in (Kumar and Becker, 1982) to overcome the central
obstacle of what is called the "closed-identifiability problem" of adaptive
control, the "dual control problem" by Feldbaum (Feldbaum, 1960a,b), or the
"exploration vs. exploitation problem". It exploited the key observation that
since the maximum likelihood parameter estimator can asymptotically identify
the closed-transition probabilities under a certainty equivalent approach
(Borkar and Varaiya, 1979), the limiting parameter estimates must necessarily
have an optimal reward that is less than the optimal reward for the true but
unknown system. Hence it proposed a bias in favor of parameters with larger
optimal rewards, providing a carefully structured solution to above problem. It
thereby proposed an optimistic approach of favoring parameters with larger
optimal rewards, now known as "optimism in the face of uncertainty." The RBMLE
approach has been proved to be longterm average reward optimal in a variety of
contexts including controlled Markov chains, linear quadratic Gaussian systems,
some nonlinear systems, and diffusions. However, modern attention is focused on
the much finer notion of "regret," or finite-time performance for all time,
espoused by (Lai and Robbins, 1985). Recent analysis of RBMLE for multi-armed
stochastic bandits (Liu et al., 2020) and linear contextual bandits (Hung et
al., 2020) has shown that it has state-of-the-art regret and exhibits empirical
performance comparable to or better than the best current contenders. Motivated
by this, we examine the finite-time performance of RBMLE for reinforcement
learning tasks of optimal control of unknown Markov Decision Processes. We show
that it has a regret of $O(\log T)$ after $T$ steps, similar to state-of-art
algorithms.
</p>
<a href="http://arxiv.org/abs/2011.07738" target="_blank">arXiv:2011.07738</a> [<a href="http://arxiv.org/pdf/2011.07738" target="_blank">pdf</a>]

<h2>No-Regret Reinforcement Learning with Value Function Approximation: a Kernel Embedding Approach. (arXiv:2011.07881v2 [cs.LG] UPDATED)</h2>
<h3>Sayak Ray Chowdhury, Rafael Oliveira</h3>
<p>We consider the regret minimization problem in reinforcement learning (RL) in
the episodic setting. In many real-world RL environments, the state and action
spaces are continuous or very large. Existing approaches establish regret
guarantees by either a low-dimensional representation of the stochastic
transition model or an approximation of the $Q$-functions. However, the
understanding of function approximation schemes for state-value functions
largely remains missing. In this paper, we propose an online model-based RL
algorithm, namely the CME-RL, that learns representations of transition
distributions as embeddings in a reproducing kernel Hilbert space while
carefully balancing the exploitation-exploration tradeoff. We demonstrate the
efficiency of our algorithm by proving a frequentist (worst-case) regret bound
that is of order $\tilde{O}\big(H\gamma_N\sqrt{N}\big)$, where $H$ is the
episode length, $N$ is the total number of time steps and $\gamma_N$ is an
information theoretic quantity relating the effective dimension of the
state-action feature space. Our method bypasses the need for estimating
transition probabilities and applies to any domain on which kernels can be
defined. It also brings new insights into the general theory of kernel methods
for approximate inference and RL regret minimization.
</p>
<a href="http://arxiv.org/abs/2011.07881" target="_blank">arXiv:2011.07881</a> [<a href="http://arxiv.org/pdf/2011.07881" target="_blank">pdf</a>]

<h2>High-level Prior-based Loss Functions for Medical Image Segmentation: A Survey. (arXiv:2011.08018v2 [cs.CV] UPDATED)</h2>
<h3>Rosana El Jurdi, Caroline Petitjean, Paul Honeine, Veronika Cheplygina, Fahed Abdallah</h3>
<p>Today, deep convolutional neural networks (CNNs) have demonstrated state of
the art performance for supervised medical image segmentation, across various
imaging modalities and tasks. Despite early success, segmentation networks may
still generate anatomically aberrant segmentations, with holes or inaccuracies
near the object boundaries. To mitigate this effect, recent research works have
focused on incorporating spatial information or prior knowledge to enforce
anatomically plausible segmentation. If the integration of prior knowledge in
image segmentation is not a new topic in classical optimization approaches, it
is today an increasing trend in CNN based image segmentation, as shown by the
growing literature on the topic. In this survey, we focus on high level prior,
embedded at the loss function level. We categorize the articles according to
the nature of the prior: the object shape, size, topology, and the
inter-regions constraints. We highlight strengths and limitations of current
approaches, discuss the challenge related to the design and the integration of
prior-based losses, and the optimization strategies, and draw future research
directions.
</p>
<a href="http://arxiv.org/abs/2011.08018" target="_blank">arXiv:2011.08018</a> [<a href="http://arxiv.org/pdf/2011.08018" target="_blank">pdf</a>]

<h2>A comparative study of semi- and self-supervised semantic segmentation of biomedical microscopy data. (arXiv:2011.08076v2 [cs.CV] UPDATED)</h2>
<h3>Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian Wagner, Ingo Roeder, Nico Scherf</h3>
<p>In recent years, Convolutional Neural Networks (CNNs) have become the
state-of-the-art method for biomedical image analysis. However, these networks
are usually trained in a supervised manner, requiring large amounts of labelled
training data. These labelled data sets are often difficult to acquire in the
biomedical domain. In this work, we validate alternative ways to train CNNs
with fewer labels for biomedical image segmentation using. We adapt two semi-
and self-supervised image classification methods and analyse their performance
for semantic segmentation of biomedical microscopy images.
</p>
<a href="http://arxiv.org/abs/2011.08076" target="_blank">arXiv:2011.08076</a> [<a href="http://arxiv.org/pdf/2011.08076" target="_blank">pdf</a>]

<h2>Cinematic-L1 Video Stabilization with a Log-Homography Model. (arXiv:2011.08144v2 [cs.CV] UPDATED)</h2>
<h3>Arwen Bradley, Jason Klivington, Joseph Triscari, Rudolph van der Merwe</h3>
<p>We present a method for stabilizing handheld video that simulates the camera
motions cinematographers achieve with equipment like tripods, dollies, and
Steadicams. We formulate a constrained convex optimization problem minimizing
the $\ell_1$-norm of the first three derivatives of the stabilized motion. Our
approach extends the work of Grundmann et al. [9] by solving with full
homographies (rather than affinities) in order to correct perspective,
preserving linearity by working in log-homography space. We also construct crop
constraints that preserve field-of-view; model the problem as a quadratic
(rather than linear) program to allow for an $\ell_2$ term encouraging fidelity
to the original trajectory; and add constraints and objectives to reduce
distortion. Furthermore, we propose new methods for handling salient objects
via both inclusion constraints and centering objectives. Finally, we describe a
windowing strategy to approximate the solution in linear time and bounded
memory. Our method is computationally efficient, running at 300fps on an iPhone
XS, and yields high-quality results, as we demonstrate with a collection of
stabilized videos, quantitative and qualitative comparisons to [9] and other
methods, and an ablation study.
</p>
<a href="http://arxiv.org/abs/2011.08144" target="_blank">arXiv:2011.08144</a> [<a href="http://arxiv.org/pdf/2011.08144" target="_blank">pdf</a>]

<h2>Learning Efficient GANs using Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v2 [cs.CV] UPDATED)</h2>
<h3>Shaojie Li, Mingbao Lin, Yan Wang, Mingliang Xu, Feiyue Huang, Yongjian Wu, Ling Shao, Rongrong Ji</h3>
<p>Generative Adversarial Networks (GANs) have been widely-used in image
translation, but their high computational and storage costs impede the
deployment on mobile devices. Prevalent methods for CNN compression cannot be
directly applied to GANs due to the complicated generator architecture and the
unstable adversarial training. To solve these, in this paper, we introduce a
novel GAN compression method, termed DMAD, by proposing a Differentiable Mask
and a co-Attention Distillation. The former searches for a light-weight
generator architecture in a training-adaptive manner. To overcome channel
inconsistency when pruning the residual connections, an adaptive cross-block
group sparsity is further incorporated. The latter simultaneously distills
informative attention maps from both the generator and discriminator of a
pre-trained model to the searched generator, effectively stabilizing the
adversarial training of our light-weight model. Experiments show that DMAD can
reduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13x and that of
Pix2Pix by 4x while retaining a comparable performance against the full model.
Code is available at https://github.com/SJLeo/DMAD.
</p>
<a href="http://arxiv.org/abs/2011.08382" target="_blank">arXiv:2011.08382</a> [<a href="http://arxiv.org/pdf/2011.08382" target="_blank">pdf</a>]

<h2>Slender Object Detection: Diagnoses and Improvements. (arXiv:2011.08529v2 [cs.CV] UPDATED)</h2>
<h3>Zhaoyi Wan, Yimin Chen, Sutao Deng, Cong Yao, Jiebo Luo</h3>
<p>In this paper, we are concerned with the detection of a particular type of
objects with extreme aspect ratios, namely slender objects. In real-world
scenarios as well as widely-used datasets (such as COCO), slender objects are
actually very common. However, this type of object has been largely overlooked
by previous object detection algorithms. Upon our investigation, for a
classical object detection method, a drastic drop of 18.9% mAP on COCO is
observed, if solely evaluated on slender objects. Therefore, We systematically
study the problem of slender object detection in this work. Accordingly, an
analytical framework with carefully designed benchmark and evaluation protocols
is established, in which different algorithms and modules can be inspected and
compared. Our key findings include: 1) the essential role of anchors in label
assignment; 2) the descriptive capability of the 2-point representation; 3) the
crucial strategies for improving the detection of slender objects and regular
objects. Our work identifies and extends the insights of existing methods that
are previously underexploited. Furthermore, we propose a feature adaption
strategy that achieves clear and consistent improvements over current
representative object detection methods. In particular, a natural and effective
extension of the center prior, which leads to a significant improvement on
slender objects, is devised. We believe this work opens up new opportunities
and calibrates ablation standards for future research in the field of object
detection.
</p>
<a href="http://arxiv.org/abs/2011.08529" target="_blank">arXiv:2011.08529</a> [<a href="http://arxiv.org/pdf/2011.08529" target="_blank">pdf</a>]

<h2>Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature Layers. (arXiv:2011.08692v2 [cs.CV] UPDATED)</h2>
<h3>Nina Varney, Vijayan K. Asari, Quinn Graehling</h3>
<p>We present a method to learn a diverse group of object categories from an
unordered point set. We propose our Pyramid Point network, which uses a dense
pyramid structure instead of the traditional 'U' shape, typically seen in
semantic segmentation networks. This pyramid structure gives a second look,
allowing the network to revisit different layers simultaneously, increasing the
contextual information by creating additional layers with less noise. We
introduce a Focused Kernel Point convolution (FKP Conv), which expands on the
traditional point convolutions by adding an attention mechanism to the kernel
outputs. This FKP Conv increases our feature quality and allows us to weigh the
kernel outputs dynamically. These FKP Convs are the central part of our
Recurrent FKP Bottleneck block, which makes up the backbone of our encoder.
With this distinct network, we demonstrate competitive performance on three
benchmark data sets. We also perform an ablation study to show the positive
effects of each element in our FKP Conv.
</p>
<a href="http://arxiv.org/abs/2011.08692" target="_blank">arXiv:2011.08692</a> [<a href="http://arxiv.org/pdf/2011.08692" target="_blank">pdf</a>]

<h2>Uncertainty Modelling in Deep Neural Networks for Image Data. (arXiv:2011.08712v2 [cs.CV] UPDATED)</h2>
<h3>Aria Khoshsirat</h3>
<p>Quantifying uncertainty in a model's predictions is important as it enables,
for example, the safety of an AI system to be increased by acting on the
model's output in an informed manner. We cannot expect a system to be 100%
accurate or perfect at its task, however, we can equip the system with some
tools to inform us if it is not certain about a prediction. This way, a second
check can be performed, or the task can be passed to a human specialist. This
is crucial for applications where the cost of an error is high, such as in
autonomous vehicle control, medical image analysis, financial estimations or
legal fields. Deep Neural Networks are powerful black box predictors that have
recently achieved impressive performance on a wide spectrum of tasks.
Quantifying predictive uncertainty in DNNs is a challenging and yet on-going
problem. Although there have been many efforts to equip NNs with tools to
estimate uncertainty, such as Monte Carlo Dropout, most of the previous methods
only focus on one of the three types of model, data or distributional
uncertainty. In this paper we propose a complete framework to capture and
quantify all of these three types of uncertainties in DNNs for image
classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.
</p>
<a href="http://arxiv.org/abs/2011.08712" target="_blank">arXiv:2011.08712</a> [<a href="http://arxiv.org/pdf/2011.08712" target="_blank">pdf</a>]

<h2>Deep Active Surface Models. (arXiv:2011.08826v3 [cs.CV] UPDATED)</h2>
<h3>Udaranga Wickramasinghe, Graham Knott, Pascal Fua</h3>
<p>Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.
</p>
<a href="http://arxiv.org/abs/2011.08826" target="_blank">arXiv:2011.08826</a> [<a href="http://arxiv.org/pdf/2011.08826" target="_blank">pdf</a>]

<h2>Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis. (arXiv:2011.09055v2 [cs.CV] UPDATED)</h2>
<h3>Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, Shenghua Gao</h3>
<p>We tackle human image synthesis, including human motion imitation, appearance
transfer, and novel view synthesis, within a unified framework. It means that
the model, once being trained, can be used to handle all these tasks. The
existing task-specific methods mainly use 2D keypoints to estimate the human
body structure. However, they only express the position information with no
abilities to characterize the personalized shape of the person and model the
limb rotations. In this paper, we propose to use a 3D body mesh recovery module
to disentangle the pose and shape. It can not only model the joint location and
rotation but also characterize the personalized body shape. To preserve the
source information, such as texture, style, color, and face identity, we
propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block
(AttLWB) that propagates the source information in both image and feature
spaces to the synthesized reference. Specifically, the source features are
extracted by a denoising convolutional auto-encoder for characterizing the
source identity well. Furthermore, our proposed method can support a more
flexible warping from multiple sources. To further improve the generalization
ability of the unseen source images, a one/few-shot adversarial learning is
applied. In detail, it firstly trains a model in an extensive training set.
Then, it finetunes the model by one/few-shot unseen image(s) in a
self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024)
results. Also, we build a new dataset, namely iPER dataset, for the evaluation
of human motion imitation, appearance transfer, and novel view synthesis.
Extensive experiments demonstrate the effectiveness of our methods in terms of
preserving face identity, shape consistency, and clothes details. All codes and
dataset are available on
https://impersonator.org/work/impersonator-plus-plus.html.
</p>
<a href="http://arxiv.org/abs/2011.09055" target="_blank">arXiv:2011.09055</a> [<a href="http://arxiv.org/pdf/2011.09055" target="_blank">pdf</a>]

<h2>MUST-GAN: Multi-level Statistics Transfer for Self-driven Person Image Generation. (arXiv:2011.09084v2 [cs.CV] UPDATED)</h2>
<h3>Tianxiang Ma, Bo Peng, Wei Wang, Jing Dong</h3>
<p>Pose-guided person image generation usually involves using paired
source-target images to supervise the training, which significantly increases
the data preparation effort and limits the application of the models. To deal
with this problem, we propose a novel multi-level statistics transfer model,
which disentangles and transfers multi-level appearance features from person
images and merges them with pose features to reconstruct the source person
images themselves. So that the source images can be used as supervision for
self-driven person image generation. Specifically, our model extracts
multi-level features from the appearance encoder and learns the optimal
appearance representation through attention mechanism and attributes
statistics. Then we transfer them to a pose-guided generator for re-fusion of
appearance and pose. Our approach allows for flexible manipulation of person
appearance and pose properties to perform pose transfer and clothes style
transfer tasks. Experimental results on the DeepFashion dataset demonstrate our
method's superiority compared with state-of-the-art supervised and unsupervised
methods. In addition, our approach also performs well in the wild.
</p>
<a href="http://arxiv.org/abs/2011.09084" target="_blank">arXiv:2011.09084</a> [<a href="http://arxiv.org/pdf/2011.09084" target="_blank">pdf</a>]

<h2>EWareNet: Emotion Aware Human Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation. (arXiv:2011.09438v2 [cs.RO] UPDATED)</h2>
<h3>Venkatraman Narayanan, Bala Murali Manoghar, Rama Prashanth RV, Aniket Bera</h3>
<p>We present EWareNet, a novel intent-aware social robot navigation algorithm
among pedestrians. Our approach predicts the trajectory-based pedestrian intent
from historical gaits, which is then used for intent-guided navigation taking
into account social and proxemic constraints. To predict pedestrian intent, we
propose a transformer-based model that works on a commodity RGB-D camera
mounted onto a moving robot. Our intent prediction routine is integrated into a
mapless navigation scheme and makes no assumptions about the environment of
pedestrian motion. Our navigation scheme consists of a novel obstacle profile
representation methodology that is dynamically adjusted based on the pedestrian
pose, intent, and emotion. The navigation scheme is based on a reinforcement
learning algorithm that takes into consideration human intent and robot's
impact on human intent, in addition to the environmental configuration. We
outperform current state-of-art algorithms for intent prediction from 3D gaits.
</p>
<a href="http://arxiv.org/abs/2011.09438" target="_blank">arXiv:2011.09438</a> [<a href="http://arxiv.org/pdf/2011.09438" target="_blank">pdf</a>]

<h2>Detecting Hierarchical Changes in Latent Variable Models. (arXiv:2011.09465v3 [stat.ML] UPDATED)</h2>
<h3>Shintaro Fukushima, Kenji Yamanishi</h3>
<p>This paper addresses the issue of detecting hierarchical changes in latent
variable models (HCDL) from data streams. There are three different levels of
changes for latent variable models: 1) the first level is the change in data
distribution for fixed latent variables, 2) the second one is that in the
distribution over latent variables, and 3) the third one is that in the number
of latent variables. It is important to detect these changes because we can
analyze the causes of changes by identifying which level a change comes from
(change interpretability). This paper proposes an information-theoretic
framework for detecting changes of the three levels in a hierarchical way. The
key idea to realize it is to employ the MDL (minimum description length) change
statistics for measuring the degree of change, in combination with DNML
(decomposed normalized maximum likelihood) code-length calculation. We give a
theoretical basis for making reliable alarms for changes. Focusing on
stochastic block models, we employ synthetic and benchmark datasets to
empirically demonstrate the effectiveness of our framework in terms of change
interpretability as well as change detection.
</p>
<a href="http://arxiv.org/abs/2011.09465" target="_blank">arXiv:2011.09465</a> [<a href="http://arxiv.org/pdf/2011.09465" target="_blank">pdf</a>]

<h2>Gradient Starvation: A Learning Proclivity in Neural Networks. (arXiv:2011.09468v2 [cs.LG] UPDATED)</h2>
<h3>Mohammad Pezeshki, S&#xe9;kou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, Guillaume Lajoie</h3>
<p>We identify and formalize a fundamental gradient descent phenomenon resulting
in a learning proclivity in over-parameterized neural networks. Gradient
Starvation arises when cross-entropy loss is minimized by capturing only a
subset of features relevant for the task, despite the presence of other
predictive features that fail to be discovered. This work provides a
theoretical explanation for the emergence of such feature imbalance in neural
networks. Using tools from Dynamical Systems theory, we identify simple
properties of learning dynamics during gradient descent that lead to this
imbalance, and prove that such a situation can be expected given certain
statistical structure in training data. Based on our proposed formalism, we
develop guarantees for a novel regularization method aimed at decoupling
feature learning dynamics, improving accuracy and robustness in cases hindered
by gradient starvation. We illustrate our findings with simple and real-world
out-of-distribution (OOD) generalization experiments.
</p>
<a href="http://arxiv.org/abs/2011.09468" target="_blank">arXiv:2011.09468</a> [<a href="http://arxiv.org/pdf/2011.09468" target="_blank">pdf</a>]

<h2>StressNet: Detecting Stress in Thermal Videos. (arXiv:2011.09540v2 [cs.CV] UPDATED)</h2>
<h3>Satish Kumar, A S M Iftekhar, Michael Goebel, Tom Bullock, Mary H. MacLean, Michael B. Miller, Tyler Santander, Barry Giesbrecht, Scott T. Grafton, B.S. Manjunath</h3>
<p>Precise measurement of physiological signals is critical for the effective
monitoring of human vital signs. Recent developments in computer vision have
demonstrated that signals such as pulse rate and respiration rate can be
extracted from digital video of humans, increasing the possibility of
contact-less monitoring. This paper presents a novel approach to obtaining
physiological signals and classifying stress states from thermal video. The
proposed network--"StressNet"--features a hybrid emission representation model
that models the direct emission and absorption of heat by the skin and
underlying blood vessels. This results in an information-rich feature
representation of the face, which is used by spatio-temporal network for
reconstructing the ISTI ( Initial Systolic Time Interval: a measure of change
in cardiac sympathetic activity that is considered to be a quantitative index
of stress in humans ). The reconstructed ISTI signal is fed into a
stress-detection model to detect and classify the individual's stress state (
i.e. stress or no stress ). A detailed evaluation demonstrates that StressNet
achieves estimated the ISTI signal with 95% accuracy and detect stress with
average precision of 0.842. The source code is available on Github.
</p>
<a href="http://arxiv.org/abs/2011.09540" target="_blank">arXiv:2011.09540</a> [<a href="http://arxiv.org/pdf/2011.09540" target="_blank">pdf</a>]

<h2>Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video. (arXiv:2011.09846v3 [cs.CV] UPDATED)</h2>
<h3>Ben Saunders, Necati Cihan Camgoz, Richard Bowden</h3>
<p>To be truly understandable and accepted by Deaf communities, an automatic
Sign Language Production (SLP) system must generate a photo-realistic signer.
Prior approaches based on graphical avatars have proven unpopular, whereas
recent neural SLP works that produce skeleton pose sequences have been shown to
be not understandable to Deaf viewers.

In this paper, we propose SignGAN, the first SLP model to produce
photo-realistic continuous sign language videos directly from spoken language.
We employ a transformer architecture with a Mixture Density Network (MDN)
formulation to handle the translation from spoken language to skeletal pose. A
pose-conditioned human synthesis model is then introduced to generate a
photo-realistic sign language video from the skeletal pose sequence. This
allows the photo-realistic production of sign videos directly translated from
written text.

We further propose a novel keypoint-based loss function, which significantly
improves the quality of synthesized hand images, operating in the keypoint
space to avoid issues caused by motion blur. In addition, we introduce a method
for controllable video generation, enabling training on large, diverse sign
language datasets and providing the ability to control the signer appearance at
inference.

Using a dataset of eight different sign language interpreters extracted from
broadcast footage, we show that SignGAN significantly outperforms all baseline
methods for quantitative metrics and human perceptual studies.
</p>
<a href="http://arxiv.org/abs/2011.09846" target="_blank">arXiv:2011.09846</a> [<a href="http://arxiv.org/pdf/2011.09846" target="_blank">pdf</a>]

<h2>Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v2 [cs.CV] UPDATED)</h2>
<h3>Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon</h3>
<p>Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to remote sensing, where unlabeled data is often
abundant but labeled data is scarce. We first show that due to their different
characteristics, a non-trivial gap persists between contrastive and supervised
learning on standard benchmarks. To close the gap, we propose novel training
methods that exploit the spatiotemporal structure of remote sensing data. We
leverage spatially aligned images over time to construct temporal positive
pairs in contrastive learning and geo-location to design pre-text tasks. Our
experiments show that our proposed method closes the gap between contrastive
and supervised learning on image classification, object detection and semantic
segmentation for remote sensing and other geo-tagged image datasets
</p>
<a href="http://arxiv.org/abs/2011.09980" target="_blank">arXiv:2011.09980</a> [<a href="http://arxiv.org/pdf/2011.09980" target="_blank">pdf</a>]

<h2>Multi-Plane Program Induction with 3D Box Priors. (arXiv:2011.10007v2 [cs.CV] UPDATED)</h2>
<h3>Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B. Tenenbaum, Noah Snavely, Jiajun Wu</h3>
<p>We consider two important aspects in understanding and editing images:
modeling regular, program-like texture or patterns in 2D planes, and 3D posing
of these planes in the scene. Unlike prior work on image-based program
synthesis, which assumes the image contains a single visible 2D plane, we
present Box Program Induction (BPI), which infers a program-like scene
representation that simultaneously models repeated structure on multiple 2D
planes, the 3D position and orientation of the planes, and camera parameters,
all from a single image. Our model assumes a box prior, i.e., that the image
captures either an inner view or an outer view of a box in 3D. It uses neural
networks to infer visual cues such as vanishing points, wireframe lines to
guide a search-based algorithm to find the program that best explains the
image. Such a holistic, structured scene representation enables 3D-aware
interactive image editing operations such as inpainting missing pixels,
changing camera parameters, and extrapolate the image contents.
</p>
<a href="http://arxiv.org/abs/2011.10007" target="_blank">arXiv:2011.10007</a> [<a href="http://arxiv.org/pdf/2011.10007" target="_blank">pdf</a>]

<h2>Sequential Targeting: an incremental learning approach for data imbalance in text classification. (arXiv:2011.10216v2 [cs.LG] UPDATED)</h2>
<h3>Joel Jang, Yoonjeon Kim, Kyoungho Choi, Sungho Suh</h3>
<p>Classification tasks require a balanced distribution of data to ensure the
learner to be trained to generalize over all classes. In real-world datasets,
however, the number of instances vary substantially among classes. This
typically leads to a learner that promotes bias towards the majority group due
to its dominating property. Therefore, methods to handle imbalanced datasets
are crucial for alleviating distributional skews and fully utilizing the
under-represented data, especially in text classification. While addressing the
imbalance in text data, most methods utilize sampling methods on the numerical
representation of the data, which limits its efficiency on how effective the
representation is. We propose a novel training method, Sequential
Targeting(ST), independent of the effectiveness of the representation method,
which enforces an incremental learning setting by splitting the data into
mutually exclusive subsets and training the learner adaptively. To address
problems that arise within incremental learning, we apply elastic weight
consolidation. We demonstrate the effectiveness of our method through
experiments on simulated benchmark datasets (IMDB) and data collected from
NAVER.
</p>
<a href="http://arxiv.org/abs/2011.10216" target="_blank">arXiv:2011.10216</a> [<a href="http://arxiv.org/pdf/2011.10216" target="_blank">pdf</a>]

<h2>Learning Synthetic to Real Transfer for Localization and Navigational Tasks. (arXiv:2011.10274v2 [cs.RO] UPDATED)</h2>
<h3>Maxime Pietrantoni, Boris Chidlovskii, Tomi Silander</h3>
<p>Autonomous navigation consists in an agent being able to navigate without
human intervention or supervision, it affects both high level planning and low
level control. Navigation is at the crossroad of multiple disciplines, it
combines notions of computer vision, robotics and control. This work aimed at
creating, in a simulation, a navigation pipeline whose transfer to the real
world could be done with as few efforts as possible. Given the limited time and
the wide range of problematic to be tackled, absolute navigation performances
while important was not the main objective. The emphasis was rather put on
studying the sim2real gap which is one the major bottlenecks of modern robotics
and autonomous navigation. To design the navigation pipeline four main
challenges arise; environment, localization, navigation and planning. The
iGibson simulator is picked for its photo-realistic textures and physics
engine. A topological approach to tackle space representation was picked over
metric approaches because they generalize better to new environments and are
less sensitive to change of conditions. The navigation pipeline is decomposed
as a localization module, a planning module and a local navigation module.
These modules utilize three different networks, an image representation
extractor, a passage detector and a local policy. The laters are trained on
specifically tailored tasks with some associated datasets created for those
specific tasks. Localization is the ability for the agent to localize itself
against a specific space representation. It must be reliable, repeatable and
robust to a wide variety of transformations. Localization is tackled as an
image retrieval task using a deep neural network trained on an auxiliary task
as a feature descriptor extractor. The local policy is trained with behavioral
cloning from expert trajectories gathered with ROS navigation stack.
</p>
<a href="http://arxiv.org/abs/2011.10274" target="_blank">arXiv:2011.10274</a> [<a href="http://arxiv.org/pdf/2011.10274" target="_blank">pdf</a>]

<h2>No-Regret Prediction in Marginally Stable Systems. (arXiv:2002.02064v3 [cs.LG] CROSS LISTED)</h2>
<h3>Udaya Ghai, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang</h3>
<p>We consider the problem of online prediction in a marginally stable linear
dynamical system subject to bounded adversarial or (non-isotropic) stochastic
perturbations. This poses two challenges. Firstly, the system is in general
unidentifiable, so recent and classical results on parameter recovery do not
apply. Secondly, because we allow the system to be marginally stable, the state
can grow polynomially with time; this causes standard regret bounds in online
convex optimization to be vacuous. In spite of these challenges, we show that
the online least-squares algorithm achieves sublinear regret (improvable to
polylogarithmic in the stochastic setting), with polynomial dependence on the
system's parameters. This requires a refined regret analysis, including a
structural lemma showing the current state of the system to be a small linear
combination of past states, even if the state grows polynomially. By applying
our techniques to learning an autoregressive filter, we also achieve
logarithmic regret in the partially observed setting under Gaussian noise, with
polynomial dependence on the memory of the associated Kalman filter.
</p>
<a href="http://arxiv.org/abs/2002.02064" target="_blank">arXiv:2002.02064</a> [<a href="http://arxiv.org/pdf/2002.02064" target="_blank">pdf</a>]

<h2>Pareto-efficient Acquisition Functions for Cost-Aware Bayesian Optimization. (arXiv:2011.11456v1 [cs.LG])</h2>
<h3>Gauthier Guinet, Valerio Peronne, C&#xe9;dric Archambeau</h3>
<p>Bayesian optimization (BO) is a popular method to optimize expensive
black-box functions. It efficiently tunes machine learning algorithms under the
implicit assumption that hyperparameter evaluations cost approximately the
same. In reality, the cost of evaluating different hyperparameters, be it in
terms of time, dollars or energy, can span several orders of magnitude of
difference. While a number of heuristics have been proposed to make BO
cost-aware, none of these have been proven to work robustly. In this work, we
reformulate cost-aware BO in terms of Pareto efficiency and introduce the cost
Pareto Front, a mathematical object allowing us to highlight the shortcomings
of commonly used acquisition functions. Based on this, we propose a novel
Pareto-efficient adaptation of the expected improvement. On 144 real-world
black-box function optimization problems we show that our Pareto-efficient
acquisition functions significantly outperform previous solutions, bringing up
to 50% speed-ups while providing finer control over the cost-accuracy
trade-off. We also revisit the common choice of Gaussian process cost models,
showing that simple, low-variance cost models predict training times
effectively.
</p>
<a href="http://arxiv.org/abs/2011.11456" target="_blank">arXiv:2011.11456</a> [<a href="http://arxiv.org/pdf/2011.11456" target="_blank">pdf</a>]

<h2>Generative Adversarial Simulator. (arXiv:2011.11472v1 [cs.LG])</h2>
<h3>Jonathan Raiman</h3>
<p>Knowledge distillation between machine learning models has opened many new
avenues for parameter count reduction, performance improvements, or amortizing
training time when changing architectures between the teacher and student
network. In the case of reinforcement learning, this technique has also been
applied to distill teacher policies to students. Until now, policy distillation
required access to a simulator or real world trajectories.

In this paper we introduce a simulator-free approach to knowledge
distillation in the context of reinforcement learning. A key challenge is
having the student learn the multiplicity of cases that correspond to a given
action. While prior work has shown that data-free knowledge distillation is
possible with supervised learning models by generating synthetic examples,
these approaches to are vulnerable to only producing a single prototype example
for each class. We propose an extension to explicitly handle multiple
observations per output class that seeks to find as many exemplars as possible
for a given output class by reinitializing our data generator and making use of
an adversarial loss.

To the best of our knowledge, this is the first demonstration of
simulator-free knowledge distillation between a teacher and a student policy.
This new approach improves over the state of the art on data-free learning of
student networks on benchmark datasets (MNIST, Fashion-MNIST, CIFAR-10), and we
also demonstrate that it specifically tackles issues with multiple input modes.
We also identify open problems when distilling agents trained in high
dimensional environments such as Pong, Breakout, or Seaquest.
</p>
<a href="http://arxiv.org/abs/2011.11472" target="_blank">arXiv:2011.11472</a> [<a href="http://arxiv.org/pdf/2011.11472" target="_blank">pdf</a>]

<h2>Dimensionality reduction, regularization, and generalization in overparameterized regressions. (arXiv:2011.11477v1 [stat.ML])</h2>
<h3>Ningyuan (Teresa) Huang, David W. Hogg, Soledad Villar</h3>
<p>Overparameterization in deep learning is powerful: Very large models fit the
training data perfectly and yet generalize well. This realization brought back
the study of linear models for regression, including ordinary least squares
(OLS), which, like deep learning, shows a "double descent" behavior. This
involves two features: (1) The risk (out-of-sample prediction error) can grow
arbitrarily when the number of samples $n$ approaches the number of parameters
$p$, and (2) the risk decreases with $p$ at $p&gt;n$, sometimes achieving a lower
value than the lowest risk at $p&lt;n$. The divergence of the risk for OLS at
$p\approx n$ is related to the condition number of the empirical covariance in
the feature set. For this reason, it can be avoided with regularization. In
this work we show that it can also be avoided with a PCA-based dimensionality
reduction. We provide a finite upper bound for the risk of the PCA-based
estimator. This result is in contrast to recent work that shows that a
different form of dimensionality reduction -- one based on the population
covariance instead of the empirical covariance -- does not avoid the
divergence. We connect these results to an analysis of adversarial attacks,
which become more effective as they raise the condition number of the empirical
covariance of the features. We show that OLS is arbitrarily susceptible to
data-poisoning attacks in the overparameterized regime -- unlike the
underparameterized regime -- and that regularization and dimensionality
reduction improve the robustness.
</p>
<a href="http://arxiv.org/abs/2011.11477" target="_blank">arXiv:2011.11477</a> [<a href="http://arxiv.org/pdf/2011.11477" target="_blank">pdf</a>]

<h2>Social Determinants of Recidivism: A Machine Learning Solution. (arXiv:2011.11483v1 [cs.LG])</h2>
<h3>Vik Shirvaikar, Choudur Lakshminarayan</h3>
<p>In this study, we propose advancements in criminal justice analytics along
three dimensions. First, for the long-standing problem of recidivism risk
assessment, we shift the focus from predicting the likelihood of recidivism to
identifying its underlying determinants within distinct subgroups. Second, to
achieve this, we introduce a machine learning pipeline that combines
unsupervised and supervised techniques to identify homogeneous clusters of
individuals and find statistically significant determinants of recidivism
within each cluster. We demonstrate useful heuristics to address key challenges
in this pipeline related to parameter selection and data processing. Third, we
use these results to compare outcomes across subgroups, enabling a more nuanced
understanding of the root factors that lead to differences in recidivism.
Overall, this approach aims to explore new ways of addressing long-standing
criminal justice challenges, providing a reliable framework for informed policy
intervention.
</p>
<a href="http://arxiv.org/abs/2011.11483" target="_blank">arXiv:2011.11483</a> [<a href="http://arxiv.org/pdf/2011.11483" target="_blank">pdf</a>]

<h2>condLSTM-Q: A novel deep learning model for predicting Covid-19 mortality in fine geographical Scale. (arXiv:2011.11507v1 [cs.LG])</h2>
<h3>HyeongChan Jo (1), Juhyun Kim (2), Tzu-Chen Huang (3), Yu-Li Ni (1) ((1) Division of Biology and Biological Engineering, Caltech, (2) The Division of Physics Mathematics and Astronomy, Caltech, (3) Walter Burke Institute for Theoretical Physics, Caltech)</h3>
<p>Predictive models with a focus on different spatial-temporal scales benefit
governments and healthcare systems to combat the COVID-19 pandemic. Here we
present the conditional Long Short-Term Memory networks with Quantile output
(condLSTM-Q), a well-performing model for making quantile predictions on
COVID-19 death tolls at the county level with a two-week forecast window. This
fine geographical scale is a rare but useful feature in publicly available
predictive models, which would especially benefit state-level officials to
coordinate resources within the state. The quantile predictions from condLSTM-Q
inform people about the distribution of the predicted death tolls, allowing
better evaluation of possible trajectories of the severity. Given the
scalability and generalizability of neural network models, this model could
incorporate additional data sources with ease, and could be further developed
to generate other useful predictions such as new cases or hospitalizations
intuitively.
</p>
<a href="http://arxiv.org/abs/2011.11507" target="_blank">arXiv:2011.11507</a> [<a href="http://arxiv.org/pdf/2011.11507" target="_blank">pdf</a>]

<h2>Logarithmic Regret for Reinforcement Learning with Linear Function Approximation. (arXiv:2011.11566v1 [cs.LG])</h2>
<h3>Jiafan He, Dongruo Zhou, Quanquan Gu</h3>
<p>Reinforcement learning (RL) with linear function approximation has received
increasing attention recently. However, existing work has focused on obtaining
$\sqrt{T}$-type regret bound, where $T$ is the number of steps. In this paper,
we show that logarithmic regret is attainable under two recently proposed
linear MDP assumptions provided that there exists a positive sub-optimality gap
for the optimal action-value function. In specific, under the linear MDP
assumption (Jin et al. 2019), the LSVI-UCB algorithm can achieve
$\tilde{O}(d^{3}H^5/\text{gap}_{\text{min}}\cdot \log(T))$ regret; and under
the linear mixture model assumption (Ayoub et al. 2020), the UCRL-VTR algorithm
can achieve $\tilde{O}(d^{2}H^5/\text{gap}_{\text{min}}\cdot \log^3(T))$
regret, where $d$ is the dimension of feature mapping, $H$ is the length of
episode, and $\text{gap}_{\text{min}}$ is the minimum of sub-optimality gap. To
the best of our knowledge, these are the first logarithmic regret bounds for RL
with linear function approximation.
</p>
<a href="http://arxiv.org/abs/2011.11566" target="_blank">arXiv:2011.11566</a> [<a href="http://arxiv.org/pdf/2011.11566" target="_blank">pdf</a>]

<h2>Conjecturing-Based Computational Discovery of Patterns in Data. (arXiv:2011.11576v1 [cs.LG])</h2>
<h3>J.P. Brooks, D.J. Edwards, C.E. Larson, N. Van Cleemput</h3>
<p>Modern machine learning methods are designed to exploit complex patterns in
data regardless of their form, while not necessarily revealing them to the
investigator. Here we demonstrate situations where modern machine learning
methods are ill-equipped to reveal feature interaction effects and other
nonlinear relationships. We propose the use of a conjecturing machine that
generates feature relationships in the form of bounds for numerical features
and boolean expressions for nominal features that are ignored by machine
learning algorithms. The proposed framework is demonstrated for a
classification problem with an interaction effect and a nonlinear regression
problem. In both settings, true underlying relationships are revealed and
generalization performance improves. The framework is then applied to
patient-level data regarding COVID-19 outcomes to suggest possible risk
factors.
</p>
<a href="http://arxiv.org/abs/2011.11576" target="_blank">arXiv:2011.11576</a> [<a href="http://arxiv.org/pdf/2011.11576" target="_blank">pdf</a>]

