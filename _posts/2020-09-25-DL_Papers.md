---
title: Latest Deep Learning Papers
date: 2020-10-29 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed</h1>
<h2>On the Optimality and Convergence Properties of the Learning Model Predictive Controller. (arXiv:2010.15153v1 [math.OC])</h2>
<h3>Ugo Rosolia, Yingzhao Lian, Emilio T. Maddalena, Giancarlo Ferrari-Trecate, Colin N. Jones</h3>
<p>In this technical note we analyse the performance improvement and optimality
properties of the Learning Model Predictive Control (LMPC) strategy for linear
deterministic systems. The LMPC framework is a policy iteration scheme where
closed-loop trajectories are used to update the control policy for the next
execution of the control task. We show that, when a Linear Independence
Constraint Qualification (LICQ) condition holds, the LMPC scheme guarantees
strict iterative performance improvement and optimality, meaning that the
closed-loop cost evaluated over the entire task converges asymptotically to the
optimal cost of the infinite-horizon control problem. Compared to previous
works this sufficient LICQ condition can be easily checked, it holds for a
larger class of systems and it can be used to adaptively select the prediction
horizon of the controller, as demonstrated by a numerical example.
</p>
<a href="http://arxiv.org/abs/2010.15153" target="_blank">arXiv:2010.15153</a> [<a href="http://arxiv.org/pdf/2010.15153" target="_blank">pdf</a>]

<h2>Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks. (arXiv:2010.15288v1 [cs.LG])</h2>
<h3>Masood S. Mortazavi</h3>
<p>Semantically-aligned $(speech, image)$ datasets can be used to explore
"visually-grounded speech". In a majority of existing investigations, features
of an image signal are extracted using neural networks "pre-trained" on other
tasks (e.g., classification on ImageNet). In still others, pre-trained networks
are used to extract audio features prior to semantic embedding. Without
"transfer learning" through pre-trained initialization or pre-trained feature
extraction, previous results have tended to show low rates of recall in $speech
\rightarrow image$ and $image \rightarrow speech$ queries.

Choosing appropriate neural architectures for encoders in the speech and
image branches and using large datasets, one can obtain competitive recall
rates without any reliance on any pre-trained initialization or feature
extraction: $(speech,image)$ semantic alignment and $speech \rightarrow image$
and $image \rightarrow speech$ retrieval are canonical tasks worthy of
independent investigation of their own and allow one to explore other
questions---e.g., the size of the audio embedder can be reduced significantly
with little loss of recall rates in $speech \rightarrow image$ and $image
\rightarrow speech$ queries.
</p>
<a href="http://arxiv.org/abs/2010.15288" target="_blank">arXiv:2010.15288</a> [<a href="http://arxiv.org/pdf/2010.15288" target="_blank">pdf</a>]

<h2>Learning Centric Wireless Resource Allocation for Edge Computing: Algorithm and Experiment. (arXiv:2010.15371v1 [cs.IT])</h2>
<h3>Liangkai Zhou, Yuncong Hong, Shuai Wang, Ruihua Han, Dachuan Li, Rui Wang, Qi Hao</h3>
<p>Edge intelligence is an emerging network architecture that integrates
sensing, communication, computing components, and supports various machine
learning applications, where a fundamental communication question is: how to
allocate the limited wireless resources (such as time, energy) to the
simultaneous model training of heterogeneous learning tasks? Existing methods
ignore two important facts: 1) different models have heterogeneous demands on
training data; 2) there is a mismatch between the simulated environment and the
real-world environment. As a result, they could lead to low learning
performance in practice. This paper proposes the learning centric wireless
resource allocation (LCWRA) scheme that maximizes the worst learning
performance of multiple classification tasks. Analysis shows that the optimal
transmission time has an inverse power relationship with respect to the
classification error. Finally, both simulation and experimental results are
provided to verify the performance of the proposed LCWRA scheme and its
robustness in real implementation.
</p>
<a href="http://arxiv.org/abs/2010.15371" target="_blank">arXiv:2010.15371</a> [<a href="http://arxiv.org/pdf/2010.15371" target="_blank">pdf</a>]

<h2>Robustifying Binary Classification to Adversarial Perturbation. (arXiv:2010.15391v1 [cs.LG])</h2>
<h3>Fariborz Salehi, Babak Hassibi</h3>
<p>Despite the enormous success of machine learning models in various
applications, most of these models lack resilience to (even small)
perturbations in their input data. Hence, new methods to robustify machine
learning models seem very essential. To this end, in this paper we consider the
problem of binary classification with adversarial perturbations. Investigating
the solution to a min-max optimization (which considers the worst-case loss in
the presence of adversarial perturbations) we introduce a generalization to the
max-margin classifier which takes into account the power of the adversary in
manipulating the data. We refer to this classifier as the "Robust Max-margin"
(RM) classifier. Under some mild assumptions on the loss function, we
theoretically show that the gradient descent iterates (with sufficiently small
step size) converge to the RM classifier in its direction. Therefore, the RM
classifier can be studied to compute various performance measures (e.g.
generalization error) of binary classification with adversarial perturbations.
</p>
<a href="http://arxiv.org/abs/2010.15391" target="_blank">arXiv:2010.15391</a> [<a href="http://arxiv.org/pdf/2010.15391" target="_blank">pdf</a>]

<h2>Channel Estimation and Equalization for CP-OFDM-based OTFS in Fractional Doppler Channels. (arXiv:2010.15396v1 [cs.IT])</h2>
<h3>Noriyuki Hashimoto, Noboru Osawa, Kosuke Yamazaki, Shinsuke Ibi</h3>
<p>Orthogonal time frequency and space (OTFS) modulation is a promising
technology that satisfies high Doppler requirements for future mobile systems.
OTFS modulation encodes information symbols and pilot symbols into the
two-dimensional (2D) delay-Doppler (DD) domain. The received symbols suffer
from inter-Doppler interference (IDI) in the fading channels with fractional
Doppler shifts that are sampled at noninteger indices in the DD domain. IDI has
been treated as an unavoidable effect because the fractional Doppler shifts
cannot be obtained directly from the received pilot symbols. In this paper, we
provide a solution to channel estimation for fractional Doppler channels. The
proposed estimation provides new insight into the OTFS input-output relation in
the DD domain as a 2D circular convolution with a small approximation.
According to the input-output relation, we also provide a low-complexity
channel equalization method using the estimated channel information. We
demonstrate the error performance of the proposed channel estimation and
equalization in several channels by simulations. The simulation results show
that in high-mobility environments, the total system utilizing the proposed
methods outperforms orthogonal frequency division multiplexing (OFDM) with
ideal channel estimation and a conventional channel estimation method using a
pseudo sequence.
</p>
<a href="http://arxiv.org/abs/2010.15396" target="_blank">arXiv:2010.15396</a> [<a href="http://arxiv.org/pdf/2010.15396" target="_blank">pdf</a>]

<h2>Orthogonality relations for deep level Deligne--Lusztig schemes of Coxeter type. (arXiv:2010.15489v1 [math.RT])</h2>
<h3>Olivier Dudas, Alexander B. Ivanov</h3>
<p>In this paper we prove some orthogonality relations for representations
arising from deep level Deligne--Lusztig schemes of Coxeter type. This
generalizes previous results of Lusztig (2004), and of Chan and the second
author (2019). Potential applications include the study of unipotent
representations arising from such deep level Deligne--Lusztig schemes, as well
as their geometry, in the spirit of the work of Lusztig (1976).
</p>
<a href="http://arxiv.org/abs/2010.15489" target="_blank">arXiv:2010.15489</a> [<a href="http://arxiv.org/pdf/2010.15489" target="_blank">pdf</a>]

<h2>Self-Learning Threshold-Based Load Balancing. (arXiv:2010.15525v1 [cs.PF])</h2>
<h3>Diego Goldsztajn (1), Sem C. Borst (1), Johan S. H. van Leeuwaarden (2), Debankur Mukherjee (3), Philip A. Whiting (4) ((1) Eindhoven University of Technology, (2) Tilburg University, (3) Georgia Institute of Technology, (4) Macquarie University)</h3>
<p>We consider a large-scale service system where incoming tasks have to be
instantaneously dispatched to one out of many parallel server pools. The
dispatcher uses a threshold for balancing the load and keeping the maximum
number of concurrent tasks across server pools low. We demonstrate that such a
policy is optimal on the fluid and diffusion scales for a suitable threshold
value, while only involving a small communication overhead. In order to set the
threshold optimally, it is important, however, to learn the load of the system,
which may be uncertain or even time-varying. For that purpose, we design a
control rule for tuning the threshold in an online manner. We provide
conditions which guarantee that this adaptive threshold settles at the optimal
value, along with estimates for the time until this happens.
</p>
<a href="http://arxiv.org/abs/2010.15525" target="_blank">arXiv:2010.15525</a> [<a href="http://arxiv.org/pdf/2010.15525" target="_blank">pdf</a>]

<h2>Every integer can be written as a square plus a squarefree. (arXiv:2010.15580v1 [math.NT])</h2>
<h3>Jorge Urroz</h3>
<p>In the paper we can prove that every integer can be written as the sum of two
integers, one perfect square and one squarefree. We also establish the
asympotic formula for the number of representations of an integer in this form.
The result is deeply related with the divisor function. In the course of our
study we get an independent result about it. Concretely we are able to deduce a
new upper bound for the divisor function valid for any integer and fully
explicit.
</p>
<a href="http://arxiv.org/abs/2010.15580" target="_blank">arXiv:2010.15580</a> [<a href="http://arxiv.org/pdf/2010.15580" target="_blank">pdf</a>]

<h2>Shared Space Transfer Learning for analyzing multi-site fMRI data. (arXiv:2010.15594v1 [cs.LG])</h2>
<h3>Muhammad Yousefnezhad, Alessandro Selvitella, Daoqiang Zhang, Andrew J. Greenshaw, Russell Greiner</h3>
<p>Multi-voxel pattern analysis (MVPA) learns predictive models from task-based
functional magnetic resonance imaging (fMRI) data, for distinguishing when
subjects are performing different cognitive tasks -- e.g., watching movies or
making decisions. MVPA works best with a well-designed feature set and an
adequate sample size. However, most fMRI datasets are noisy, high-dimensional,
expensive to collect, and with small sample sizes. Further, training a robust,
generalized predictive model that can analyze homogeneous cognitive tasks
provided by multi-site fMRI datasets has additional challenges. This paper
proposes the Shared Space Transfer Learning (SSTL) as a novel transfer learning
(TL) approach that can functionally align homogeneous multi-site fMRI datasets,
and so improve the prediction performance in every site. SSTL first extracts a
set of common features for all subjects in each site. It then uses TL to map
these site-specific features to a site-independent shared space in order to
improve the performance of the MVPA. SSTL uses a scalable optimization
procedure that works effectively for high-dimensional fMRI datasets. The
optimization procedure extracts the common features for each site by using a
single-iteration algorithm and maps these site-specific common features to the
site-independent shared space. We evaluate the effectiveness of the proposed
method for transferring between various cognitive tasks. Our comprehensive
experiments validate that SSTL achieves superior performance to other
state-of-the-art analysis techniques.
</p>
<a href="http://arxiv.org/abs/2010.15594" target="_blank">arXiv:2010.15594</a> [<a href="http://arxiv.org/pdf/2010.15594" target="_blank">pdf</a>]

<h2>Generalization bounds for deep thresholding networks. (arXiv:2010.15658v1 [math.ST])</h2>
<h3>Arash Behboodi, Holger Rauhut, Ekkehard Schnoor</h3>
<p>We consider compressive sensing in the scenario where the sparsity basis
(dictionary) is not known in advance, but needs to be learned from examples.
Motivated by the well-known iterative soft thresholding algorithm for the
reconstruction, we define deep networks parametrized by the dictionary, which
we call deep thresholding networks. Based on training samples, we aim at
learning the optimal sparsifying dictionary and thereby the optimal network
that reconstructs signals from their low-dimensional linear measurements. The
dictionary learning is performed via minimizing the empirical risk. We derive
generalization bounds by analyzing the Rademacher complexity of hypothesis
classes consisting of such deep networks. We obtain estimates of the sample
complexity that depend only linearly on the dimensions and on the depth.
</p>
<a href="http://arxiv.org/abs/2010.15658" target="_blank">arXiv:2010.15658</a> [<a href="http://arxiv.org/pdf/2010.15658" target="_blank">pdf</a>]

<h2>Analyzing the tree-layer structure of Deep Forests. (arXiv:2010.15690v1 [cs.LG])</h2>
<h3>Ludovic Arnould (LPSM UMR 8001), Claire Boyer (LPSM UMR 8001), Erwan Scornet (CMAP)</h3>
<p>Random forests on the one hand, and neural networks on the other hand, have
met great success in the machine learning community for their predictive
performance. Combinations of both have been proposed in the literature, notably
leading to the so-called deep forests (DF) [25]. In this paper, we investigate
the mechanisms at work in DF and outline that DF architecture can generally be
simplified into more simple and computationally efficient shallow forests
networks. Despite some instability, the latter may outperform standard
predictive tree-based methods. In order to precisely quantify the improvement
achieved by these light network configurations over standard tree learners, we
theoretically study the performance of a shallow tree network made of two
layers, each one composed of a single centered tree. We provide tight
theoretical lower and upper bounds on its excess risk. These theoretical
results show the interest of tree-network architectures for well-structured
data provided that the first layer, acting as a data encoder, is rich enough.
</p>
<a href="http://arxiv.org/abs/2010.15690" target="_blank">arXiv:2010.15690</a> [<a href="http://arxiv.org/pdf/2010.15690" target="_blank">pdf</a>]

<h2>Learning interaction kernels in mean-field equations of 1st-order systems of interacting particles. (arXiv:2010.15694v1 [stat.ML])</h2>
<h3>Quanjun Lang, Fei Lu</h3>
<p>We introduce a nonparametric algorithm to learn interaction kernels of
mean-field equations for 1st-order systems of interacting particles. The data
consist of discrete space-time observations of the solution. By least squares
with regularization, the algorithm learns the kernel on data-adaptive
hypothesis spaces efficiently. A key ingredient is a probabilistic error
functional derived from the likelihood of the mean-field equation's diffusion
process. The estimator converges, in a reproducing kernel Hilbert space and an
L2 space under an identifiability condition, at a rate optimal in the sense
that it equals the numerical integrator's order. We demonstrate our algorithm
on three typical examples: the opinion dynamics with a piecewise linear kernel,
the granular media model with a quadratic kernel, and the aggregation-diffusion
with a repulsive-attractive kernel.
</p>
<a href="http://arxiv.org/abs/2010.15694" target="_blank">arXiv:2010.15694</a> [<a href="http://arxiv.org/pdf/2010.15694" target="_blank">pdf</a>]

<h2>Constrained Online Learning to Mitigate Distortion Effects in Pulse-Agile Cognitive Radar. (arXiv:2010.15698v1 [cs.IT])</h2>
<h3>Charles E. Thornton, R. Michael Buehrer, Anthony F. Martone</h3>
<p>Pulse-agile radar systems have demonstrated favorable performance in dynamic
electromagnetic scenarios. However, the use of non-identical waveforms within a
radar's coherent processing interval may lead to harmful distortion effects
when pulse-Doppler processing is used. This paper presents an online learning
framework to optimize detection performance while mitigating harmful sidelobe
levels. The radar waveform selection process is formulated as a linear
contextual bandit problem, within which waveform adaptations which exceed a
tolerable level of expected distortion are eliminated. The constrained online
learning approach is effective and computationally feasible, evidenced by
simulations in a radar-communication coexistence scenario and in the presence
of intentional adaptive jamming. This approach is applied to both stochastic
and adversarial contextual bandit learning models and the detection performance
in dynamic scenarios is evaluated.
</p>
<a href="http://arxiv.org/abs/2010.15698" target="_blank">arXiv:2010.15698</a> [<a href="http://arxiv.org/pdf/2010.15698" target="_blank">pdf</a>]

<h2>A deep neural network algorithm for semilinear elliptic PDEs with applications in insurance mathematics. (arXiv:2010.15757v1 [q-fin.MF])</h2>
<h3>Stefan Kremsner, Alexander Steinicke, Michaela Sz&#xf6;lgyenyi</h3>
<p>In insurance mathematics optimal control problems over an infinite time
horizon arise when computing risk measures. Their solutions correspond to
solutions of deterministic semilinear (degenerate) elliptic partial
differential equations. In this paper we propose a deep neural network
algorithm for solving such partial differential equations in high dimensions.
The algorithm is based on the correspondence of elliptic partial differential
equations to backward stochastic differential equations with random terminal
time.
</p>
<a href="http://arxiv.org/abs/2010.15757" target="_blank">arXiv:2010.15757</a> [<a href="http://arxiv.org/pdf/2010.15757" target="_blank">pdf</a>]

<h2>Identifying Transition States of Chemical Kinetic Systems using Network Embedding Techniques. (arXiv:2010.15760v1 [math.NA])</h2>
<h3>Paula Mercurio, Di Liu</h3>
<p>Using random walk sampling methods for feature learning on networks, we
develop a method for generating low-dimensional node embeddings for directed
graphs and identifying transition states of stochastic chemical reacting
systems. We modified objective functions adopted in existing random walk based
network embedding methods to handle directed graphs and neighbors of different
degrees. Through optimization via gradient ascent, we embed the weighted graph
vertices into a low-dimensional vector space Rd while preserving the
neighborhood of each node. We then demonstrate the effectiveness of the method
on dimension reduction through several examples regarding identification of
transition states of chemical reactions, especially for entropic systems.
</p>
<a href="http://arxiv.org/abs/2010.15760" target="_blank">arXiv:2010.15760</a> [<a href="http://arxiv.org/pdf/2010.15760" target="_blank">pdf</a>]

<h2>Domain adaptation under structural causal models. (arXiv:2010.15764v1 [stat.ML])</h2>
<h3>Yuansi Chen, Peter B&#xfc;hlmann</h3>
<p>Domain adaptation (DA) arises as an important problem in statistical machine
learning when the source data used to train a model is different from the
target data used to test the model. Recent advances in DA have mainly been
application-driven and have largely relied on the idea of a common subspace for
source and target data. To understand the empirical successes and failures of
DA methods, we propose a theoretical framework via structural causal models
that enables analysis and comparison of the prediction performance of DA
methods. This framework also allows us to itemize the assumptions needed for
the DA methods to have a low target error. Additionally, with insights from our
theory, we propose a new DA method called CIRM that outperforms existing DA
methods when both the covariates and label distributions are perturbed in the
target data. We complement the theoretical analysis with extensive simulations
to show the necessity of the devised assumptions. Reproducible synthetic and
real data experiments are also provided to illustrate the strengths and
weaknesses of DA methods when parts of the assumptions of our theory are
violated.
</p>
<a href="http://arxiv.org/abs/2010.15764" target="_blank">arXiv:2010.15764</a> [<a href="http://arxiv.org/pdf/2010.15764" target="_blank">pdf</a>]

<h2>A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems. (arXiv:2010.15768v1 [math.OC])</h2>
<h3>Jiawei Zhang, Peijun Xiao, Ruoyu Sun, Zhi-Quan Luo</h3>
<p>Nonconvex-concave min-max problem arises in many machine learning
applications including minimizing a pointwise maximum of a set of nonconvex
functions and robust adversarial training of neural networks. A popular
approach to solve this problem is the gradient descent-ascent (GDA) algorithm
which unfortunately can exhibit oscillation in case of nonconvexity. In this
paper, we introduce a "smoothing" scheme which can be combined with GDA to
stabilize the oscillation and ensure convergence to a stationary solution. We
prove that the stabilized GDA algorithm can achieve an $O(1/\epsilon^2)$
iteration complexity for minimizing the pointwise maximum of a finite
collection of nonconvex functions. Moreover, the smoothed GDA algorithm
achieves an $O(1/\epsilon^4)$ iteration complexity for general
nonconvex-concave problems. Extensions of this stabilized GDA algorithm to
multi-block cases are presented. To the best of our knowledge, this is the
first algorithm to achieve $O(1/\epsilon^2)$ for a class of nonconvex-concave
problem. We illustrate the practical efficiency of the stabilized GDA algorithm
on robust training.
</p>
<a href="http://arxiv.org/abs/2010.15768" target="_blank">arXiv:2010.15768</a> [<a href="http://arxiv.org/pdf/2010.15768" target="_blank">pdf</a>]

<h2>Quantum advantage for differential equation analysis. (arXiv:2010.15776v1 [quant-ph])</h2>
<h3>Bobak T. Kiani, Giacomo De Palma, Dirk Englund, William Kaminsky, Milad Marvian, Seth Lloyd</h3>
<p>Quantum algorithms for both differential equation solving and for machine
learning potentially offer an exponential speedup over all known classical
algorithms. However, there also exist obstacles to obtaining this potential
speedup in useful problem instances. The essential obstacle for quantum
differential equation solving is that outputting useful information may require
difficult post-processing, and the essential obstacle for quantum machine
learning is that inputting the training set is a difficult task just by itself.
In this paper, we demonstrate, when combined, these difficulties solve one
another. We show how the output of quantum differential equation solving can
serve as the input for quantum machine learning, allowing dynamical analysis in
terms of principal components, power spectra, and wavelet decompositions. To
illustrate this, we consider continuous time Markov processes on
epidemiological and social networks. These quantum algorithms provide an
exponential advantage over existing classical Monte Carlo methods.
</p>
<a href="http://arxiv.org/abs/2010.15776" target="_blank">arXiv:2010.15776</a> [<a href="http://arxiv.org/pdf/2010.15776" target="_blank">pdf</a>]

<h2>A solution to Erd\H{o}s and Hajnal's odd cycle problem. (arXiv:2010.15802v1 [math.CO])</h2>
<h3>Hong Liu, Richard Montgomery</h3>
<p>In 1981, Erd\H{o}s and Hajnal asked whether the sum of the reciprocals of the
odd cycle lengths in a graph with infinite chromatic number is necessarily
infinite. Let $\mathcal{C}(G)$ be the set of cycle lengths in a graph $G$ and
let $\mathcal{C}_\text{odd}(G)$ be the set of odd numbers in $\mathcal{C}(G)$.
We prove that, if $G$ has chromatic number $k$, then $\sum_{\ell\in
\mathcal{C}_\text{odd}(G)}1/\ell\geq (1/2-o_k(1))\log k$. This solves Erd\H{o}s
and Hajnal's odd cycle problem, and, furthermore, this bound is asymptotically
optimal.

In 1984, Erd\H{o}s asked whether there is some $d$ such that each graph with
chromatic number at least $d$ (or perhaps even only average degree at least
$d$) has a cycle whose length is a power of 2. We show that an average degree
condition is sufficient for this problem, solving it with methods that apply to
a wide range of sequences in addition to the powers of 2.

Finally, we use our methods to show that, for every $k$, there is some $d$ so
that every graph with average degree at least $d$ has a subdivision of the
complete graph $K_k$ in which each edge is subdivided the same number of times.
This confirms a conjecture of Thomassen from 1984.
</p>
<a href="http://arxiv.org/abs/2010.15802" target="_blank">arXiv:2010.15802</a> [<a href="http://arxiv.org/pdf/2010.15802" target="_blank">pdf</a>]

<h2>Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations. (arXiv:2003.02960v3 [cs.LG] UPDATED)</h2>
<h3>Aditya Golatkar, Alessandro Achille, Stefano Soatto</h3>
<p>We describe a procedure for removing dependency on a cohort of training data
from a trained deep network that improves upon and generalizes previous methods
to different readout functions and can be extended to ensure forgetting in the
activations of the network. We introduce a new bound on how much information
can be extracted per query about the forgotten cohort from a black-box network
for which only the input-output behavior is observed. The proposed forgetting
procedure has a deterministic part derived from the differential equations of a
linearized version of the model, and a stochastic part that ensures information
destruction by adding noise tailored to the geometry of the loss landscape. We
exploit the connections between the activation and weight dynamics of a DNN
inspired by Neural Tangent Kernels to compute the information in the
activations.
</p>
<a href="http://arxiv.org/abs/2003.02960" target="_blank">arXiv:2003.02960</a> [<a href="http://arxiv.org/pdf/2003.02960" target="_blank">pdf</a>]

<h2>Multivariate Quasi-tight Framelets with High Balancing Orders Derived from Any Compactly Supported Refinable Vector Functions. (arXiv:2005.12451v2 [math.FA] UPDATED)</h2>
<h3>Bin Han, Ran Lu</h3>
<p>Generalizing wavelets by adding desired redundancy and flexibility,framelets
are of interest and importance in many applications such as image processing
and numerical algorithms. Several key properties of framelets are high
vanishing moments for sparse multiscale representation, fast framelet
transforms for numerical efficiency, and redundancy for robustness. However, it
is a challenging problem to study and construct multivariate nonseparable
framelets, mainly due to their intrinsic connections to factorization and
syzygy modules of multivariate polynomial matrices. In this paper, we
circumvent the above difficulties through the approach of quasi-tight
framelets, which behave almost identically to tight framelets. Employing the
popular oblique extension principle (OEP), from an arbitrary compactly
supported $\dm$-refinable vector function $\phi$ with multiplicity greater than
one, we prove that we can always derive from $\phi$ a compactly supported
multivariate quasi-tight framelet such that (i) all the framelet generators
have the highest possible order of vanishing moments;(ii) its associated fast
framelet transform is compact with the highest balancing order.For a refinable
scalar function $\phi$, the above item (ii) often cannot be achieved
intrinsically but we show that we can always construct a compactly supported
OEP-based multivariate quasi-tight framelet derived from $\phi$ satisfying item
(i).This paper provides a comprehensive investigation on OEP-based multivariate
quasi-tight multiframelets and their associated framelet transforms with high
balancing orders. This deepens our theoretical understanding of multivariate
quasi-tight multiframelets and their associated fast multiframelet transforms.
</p>
<a href="http://arxiv.org/abs/2005.12451" target="_blank">arXiv:2005.12451</a> [<a href="http://arxiv.org/pdf/2005.12451" target="_blank">pdf</a>]

<h2>A mathematical model for automatic differentiation in machine learning. (arXiv:2006.02080v2 [cs.LG] UPDATED)</h2>
<h3>Jerome Bolte (TSE), Edouard Pauwels (IRIT-ADRIA)</h3>
<p>Automatic differentiation, as implemented today, does not have a simple
mathematical model adapted to the needs of modern machine learning. In this
work we articulate the relationships between differentiation of programs as
implemented in practice and differentiation of nonsmooth functions. To this end
we provide a simple class of functions, a nonsmooth calculus, and show how they
apply to stochastic approximation methods. We also evidence the issue of
artificial critical points created by algorithmic differentiation and show how
usual methods avoid these points with probability one.
</p>
<a href="http://arxiv.org/abs/2006.02080" target="_blank">arXiv:2006.02080</a> [<a href="http://arxiv.org/pdf/2006.02080" target="_blank">pdf</a>]

<h2>Graph Neural Networks for Scalable Radio Resource Management: Architecture Design and Theoretical Analysis. (arXiv:2007.07632v2 [cs.IT] UPDATED)</h2>
<h3>Yifei Shen, Yuanming Shi, Jun Zhang, Khaled B. Letaief</h3>
<p>Deep learning has recently emerged as a disruptive technology to solve
challenging radio resource management problems in wireless networks. However,
the neural network architectures adopted by existing works suffer from poor
scalability, generalization, and lack of interpretability. A long-standing
approach to improve scalability and generalization is to incorporate the
structures of the target task into the neural network architecture. In this
paper, we propose to apply graph neural networks (GNNs) to solve large-scale
radio resource management problems, supported by effective neural network
architecture design and theoretical analysis. Specifically, we first
demonstrate that radio resource management problems can be formulated as graph
optimization problems that enjoy a universal permutation equivariance property.
We then identify a class of neural networks, named \emph{message passing graph
neural networks} (MPGNNs). It is demonstrated that they not only satisfy the
permutation equivariance property, but also can generalize to large-scale
problems while enjoying a high computational efficiency. For interpretablity
and theoretical guarantees, we prove the equivalence between MPGNNs and a class
of distributed optimization algorithms, which is then used to analyze the
performance and generalization of MPGNN-based methods. Extensive simulations,
with power control and beamforming as two examples, will demonstrate that the
proposed method, trained in an unsupervised manner with unlabeled samples,
matches or even outperforms classic optimization-based algorithms without
domain-specific knowledge. Remarkably, the proposed method is highly scalable
and can solve the beamforming problem in an interference channel with $1000$
transceiver pairs within $6$ milliseconds on a single GPU.
</p>
<a href="http://arxiv.org/abs/2007.07632" target="_blank">arXiv:2007.07632</a> [<a href="http://arxiv.org/pdf/2007.07632" target="_blank">pdf</a>]

<h2>A Matrix Chernoff Bound for Markov Chains and Its Application to Co-occurrence Matrices. (arXiv:2008.02464v2 [stat.ML] UPDATED)</h2>
<h3>Jiezhong Qiu, Chi Wang, Ben Liao, Richard Peng, Jie Tang</h3>
<p>We prove a Chernoff-type bound for sums of matrix-valued random variables
sampled via a regular (aperiodic and irreducible) finite Markov chain.
Specially, consider a random walk on a regular Markov chain and a Hermitian
matrix-valued function on its state space. Our result gives exponentially
decreasing bounds on the tail distributions of the extreme eigenvalues of the
sample mean matrix. Our proof is based on the matrix expander (regular
undirected graph) Chernoff bound [Garg et al. STOC '18] and scalar
Chernoff-Hoeffding bounds for Markov chains [Chung et al. STACS '12].

Our matrix Chernoff bound for Markov chains can be applied to analyze the
behavior of co-occurrence statistics for sequential data, which have been
common and important data signals in machine learning. We show that given a
regular Markov chain with $n$ states and mixing time $\tau$, we need a
trajectory of length $O(\tau (\log{(n)}+\log{(\tau)})/\epsilon^2)$ to achieve
an estimator of the co-occurrence matrix with error bound $\epsilon$. We
conduct several experiments and the experimental results are consistent with
the exponentially fast convergence rate from theoretical analysis. Our result
gives the first bound on the convergence rate of the co-occurrence matrix and
the first sample complexity analysis in graph representation learning.
</p>
<a href="http://arxiv.org/abs/2008.02464" target="_blank">arXiv:2008.02464</a> [<a href="http://arxiv.org/pdf/2008.02464" target="_blank">pdf</a>]

<h2>Multi-scale Deep Neural Network (MscaleDNN) Methods for Oscillatory Stokes Flows in Complex Domains. (arXiv:2009.12729v2 [math.NA] UPDATED)</h2>
<h3>Bo Wang, Wenzhong Zhang, Wei Cai</h3>
<p>In this paper, we study a multi-scale deep neural network (MscaleDNN) as a
meshless numerical method for computing oscillatory Stokes flows in complex
domains. The MscaleDNN employs a multi-scale structure in the design of its DNN
using radial scalings to convert the approximation of high frequency components
of the highly oscillatory Stokes solution to one of lower frequencies. The
MscaleDNN solution to the Stokes problem is obtained by minimizing a loss
function in terms of L2 normof the residual of the Stokes equation. Three forms
of loss functions are investigated based on vorticity-velocity-pressure,
velocity-stress-pressure, and velocity-gradient of velocity-pressure
formulations of the Stokes equation. We first conduct a systematic study of the
MscaleDNN methods with various loss functions on the Kovasznay flow in
comparison with normal fully connected DNNs. Then, Stokes flows with highly
oscillatory solutions in a 2-D domain with six randomly placed holes are
simulated by the MscaleDNN. The results show that MscaleDNN has faster
convergence and consistent error decays in the simulation of Kovasznay flow for
all four tested loss functions. More importantly, the MscaleDNN is capable of
learning highly oscillatory solutions when the normal DNNs fail to converge.
</p>
<a href="http://arxiv.org/abs/2009.12729" target="_blank">arXiv:2009.12729</a> [<a href="http://arxiv.org/pdf/2009.12729" target="_blank">pdf</a>]

<h2>Improved Analysis of Clipping Algorithms for Non-convex Optimization. (arXiv:2010.02519v2 [cs.LG] UPDATED)</h2>
<h3>Bohang Zhang, Jikai Jin, Cong Fang, Liwei Wang</h3>
<p>Gradient clipping is commonly used in training deep neural networks partly
due to its practicability in relieving the exploding gradient problem.
Recently, \citet{zhang2019gradient} show that clipped (stochastic) Gradient
Descent (GD) converges faster than vanilla GD/SGD via introducing a new
assumption called $(L_0, L_1)$-smoothness, which characterizes the violent
fluctuation of gradients typically encountered in deep neural networks.
However, their iteration complexities on the problem-dependent parameters are
rather pessimistic, and theoretical justification of clipping combined with
other crucial techniques, e.g. momentum acceleration, are still lacking. In
this paper, we bridge the gap by presenting a general framework to study the
clipping algorithms, which also takes momentum methods into consideration. We
provide convergence analysis of the framework in both deterministic and
stochastic setting, and demonstrate the tightness of our results by comparing
them with existing lower bounds. Our results imply that the efficiency of
clipping methods will not degenerate even in highly non-smooth regions of the
landscape. Experiments confirm the superiority of clipping-based methods in
deep learning tasks.
</p>
<a href="http://arxiv.org/abs/2010.02519" target="_blank">arXiv:2010.02519</a> [<a href="http://arxiv.org/pdf/2010.02519" target="_blank">pdf</a>]

<h2>Raw Audio for Depression Detection Can Be More Robust Against Gender Imbalance than Mel-Spectrogram Features. (arXiv:2010.15120v1 [cs.SD])</h2>
<h3>Andrew Bailey, Mark D. Plumbley</h3>
<p>Depression is a large-scale mental health problem and a challenging area for
machine learning researchers in terms of the detection of depression. Datasets
such as the Distress Analysis Interview Corpus - Wizard of Oz have been created
to aid research in this area. However, on top of the challenges inherent in
accurately detecting depression, biases in datasets may result in skewed
classification performance. In this paper we examine gender bias in the
DAIC-WOZ dataset using audio-based deep neural networks. We show that gender
biases in DAIC-WOZ can lead to an overreporting of performance, which has been
overlooked in the past due to the same gender biases being present in the test
set. By using raw audio and different concepts from Fair Machine Learning, such
as data re-distribution, we can mitigate against the harmful effects of bias.
</p>
<a href="http://arxiv.org/abs/2010.15120" target="_blank">arXiv:2010.15120</a> [<a href="http://arxiv.org/pdf/2010.15120" target="_blank">pdf</a>]

<h2>Diagnostic data integration using deep neural networks for real-time plasma analysis. (arXiv:2010.15156v1 [physics.comp-ph])</h2>
<h3>A. Rigoni Garola, R. Cavazzana, M. Gobbin, R.S. Delogu, G. Manduchi, C. Taliercio, A. Luchetta</h3>
<p>Recent advances in acquisition equipment is providing experiments with
growing amounts of precise yet affordable sensors. At the same time an improved
computational power, coming from new hardware resources (GPU, FPGA, ACAP), has
been made available at relatively low costs. This led us to explore the
possibility of completely renewing the chain of acquisition for a fusion
experiment, where many high-rate sources of data, coming from different
diagnostics, can be combined in a wide framework of algorithms. If on one hand
adding new data sources with different diagnostics enriches our knowledge about
physical aspects, on the other hand the dimensions of the overall model grow,
making relations among variables more and more opaque. A new approach for the
integration of such heterogeneous diagnostics, based on composition of deep
\textit{variational autoencoders}, could ease this problem, acting as a
structural sparse regularizer. This has been applied to RFX-mod experiment
data, integrating the soft X-ray linear images of plasma temperature with the
magnetic state.

However to ensure a real-time signal analysis, those algorithmic techniques
must be adapted to run in well suited hardware. In particular it is shown that,
attempting a quantization of neurons transfer functions, such models can be
modified to create an embedded firmware. This firmware, approximating the deep
inference model to a set of simple operations, fits well with the simple logic
units that are largely abundant in FPGAs. This is the key factor that permits
the use of affordable hardware with complex deep neural topology and operates
them in real-time.
</p>
<a href="http://arxiv.org/abs/2010.15156" target="_blank">arXiv:2010.15156</a> [<a href="http://arxiv.org/pdf/2010.15156" target="_blank">pdf</a>]

<h2>Panoster: End-to-end Panoptic Segmentation of LiDAR Point Clouds. (arXiv:2010.15157v1 [cs.CV])</h2>
<h3>Stefano Gasperini, Mohammad-Ali Nikouei Mahani, Alvaro Marcos-Ramiro, Nassir Navab, Federico Tombari</h3>
<p>Panoptic segmentation has recently unified semantic and instance
segmentation, previously addressed separately, thus taking a step further
towards creating more comprehensive and efficient perception systems. In this
paper, we present Panoster, a novel proposal-free panoptic segmentation method
for point clouds. Unlike previous approaches relying on several steps to group
pixels or points into objects, Panoster proposes a simplified framework
incorporating a learning-based clustering solution to identify instances. At
inference time, this acts as a class-agnostic semantic segmentation, allowing
Panoster to be fast, while outperforming prior methods in terms of accuracy.
Additionally, we showcase how our approach can be flexibly and effectively
applied on diverse existing semantic architectures to deliver panoptic
predictions.
</p>
<a href="http://arxiv.org/abs/2010.15157" target="_blank">arXiv:2010.15157</a> [<a href="http://arxiv.org/pdf/2010.15157" target="_blank">pdf</a>]

<h2>CNN Profiler on Polar Coordinate Images for Tropical Cyclone Structure Analysis. (arXiv:2010.15158v1 [cs.CV])</h2>
<h3>Boyo Chen, Buo-Fu Chen, Chun-Min Hsiao</h3>
<p>Convolutional neural networks (CNN) have achieved great success in analyzing
tropical cyclones (TC) with satellite images in several tasks, such as TC
intensity estimation. In contrast, TC structure, which is conventionally
described by a few parameters estimated subjectively by meteorology
specialists, is still hard to be profiled objectively and routinely. This study
applies CNN on satellite images to create the entire TC structure profiles,
covering all the structural parameters. By utilizing the meteorological domain
knowledge to construct TC wind profiles based on historical structure
parameters, we provide valuable labels for training in our newly released
benchmark dataset. With such a dataset, we hope to attract more attention to
this crucial issue among data scientists. Meanwhile, a baseline is established
with a specialized convolutional model operating on polar-coordinates. We
discovered that it is more feasible and physically reasonable to extract
structural information on polar-coordinates, instead of Cartesian coordinates,
according to a TC's rotational and spiral natures. Experimental results on the
released benchmark dataset verified the robustness of the proposed model and
demonstrated the potential for applying deep learning techniques for this
barely developed yet important topic.
</p>
<a href="http://arxiv.org/abs/2010.15158" target="_blank">arXiv:2010.15158</a> [<a href="http://arxiv.org/pdf/2010.15158" target="_blank">pdf</a>]

<h2>Sizeless: Predicting the optimal size of serverless functions. (arXiv:2010.15162v1 [cs.DC])</h2>
<h3>Simon Eismann, Long Bui, Johannes Grohmann, Cristina L. Abad, Nikolas Herbst, Samuel Kounev</h3>
<p>Serverless functions are a cloud computing paradigm that reduces operational
overheads for developers, because the cloud provider takes care of resource
management tasks such as resource provisioning, deployment, and auto-scaling.
The only resource management task that developers are still in charge of is
resource sizing, that is, selecting how much resources are allocated to each
worker instance. However, due to the challenging nature of resource sizing,
developers often neglect it despite its significant cost and performance
benefits. Existing approaches aiming to automate serverless functions resource
sizing require dedicated performance tests, which are time consuming to
implement and maintain.

In this paper, we introduce Sizeless -- an approach to predict the optimal
resource size of a serverless function using monitoring data from a single
resource size. As our approach requires only production monitoring data,
developers no longer need to implement and maintain representative performance
tests. Furthermore, it enables cloud providers, which cannot engage in testing
the performance of user functions, to implement resource sizing on a platform
level and automate the last resource management task associated with serverless
functions. In our evaluation, Sizeless was able to predict the execution time
of the serverless functions of a realistic server-less application with a
median prediction accuracy of 93.1%. Using Sizeless to optimize the memory size
of this application results in a speedup of 16.7% while simultaneously
decreasing costs by 2.5%.
</p>
<a href="http://arxiv.org/abs/2010.15162" target="_blank">arXiv:2010.15162</a> [<a href="http://arxiv.org/pdf/2010.15162" target="_blank">pdf</a>]

<h2>Polymer Informatics with Multi-Task Learning. (arXiv:2010.15166v1 [cond-mat.mtrl-sci])</h2>
<h3>Christopher K&#xfc;nneth, Arunkumar Chitteth Rajan, Huan Tran, Lihua Chen, Chiho Kim, Rampi Ramprasad</h3>
<p>Modern data-driven tools are transforming application-specific polymer
development cycles. Surrogate models that can be trained to predict the
properties of new polymers are becoming commonplace. Nevertheless, these models
do not utilize the full breadth of the knowledge available in datasets, which
are oftentimes sparse; inherent correlations between different property
datasets are disregarded. Here, we demonstrate the potency of multi-task
learning approaches that exploit such inherent correlations effectively,
particularly when some property dataset sizes are small. Data pertaining to 36
different properties of over $13, 000$ polymers (corresponding to over $23,000$
data points) are coalesced and supplied to deep-learning multi-task
architectures. Compared to conventional single-task learning models (that are
trained on individual property datasets independently), the multi-task approach
is accurate, efficient, scalable, and amenable to transfer learning as more
data on the same or different properties become available. Moreover, these
models are interpretable. Chemical rules, that explain how certain features
control trends in specific property values, emerge from the present work,
paving the way for the rational design of application specific polymers meeting
desired property or performance objectives.
</p>
<a href="http://arxiv.org/abs/2010.15166" target="_blank">arXiv:2010.15166</a> [<a href="http://arxiv.org/pdf/2010.15166" target="_blank">pdf</a>]

<h2>Improving Perceptual Quality by Phone-Fortified Perceptual Loss for Speech Enhancement. (arXiv:2010.15174v1 [cs.SD])</h2>
<h3>Tsun-An Hsieh, Cheng Yu, Szu-Wei Fu, Xugang Lu, Yu Tsao</h3>
<p>Speech enhancement (SE) aims to improve speech quality and intelligibility,
which are both related to a smooth transition in speech segments that may carry
linguistic information, e.g. phones and syllables. In this study, we took
phonetic characteristics into account in the SE training process. Hence, we
designed a phone-fortified perceptual (PFP) loss, and the training of our SE
model was guided by PFP loss. In PFP loss, phonetic characteristics are
extracted by wav2vec, an unsupervised learning model based on the contrastive
predictive coding (CPC) criterion. Different from previous deep-feature-based
approaches, the proposed approach explicitly uses the phonetic information in
the deep feature extraction process to guide the SE model training. To test the
proposed approach, we first confirmed that the wav2vec representations carried
clear phonetic information using a t-distributed stochastic neighbor embedding
(t-SNE) analysis. Next, we observed that the proposed PFP loss was more
strongly correlated with the perceptual evaluation metrics than point-wise and
signal-level losses, thus achieving higher scores for standardized quality and
intelligibility evaluation metrics in the Voice Bank--DEMAND dataset.
</p>
<a href="http://arxiv.org/abs/2010.15174" target="_blank">arXiv:2010.15174</a> [<a href="http://arxiv.org/pdf/2010.15174" target="_blank">pdf</a>]

<h2>A Study on Efficiency in Continual Learning Inspired by Human Learning. (arXiv:2010.15187v1 [cs.LG])</h2>
<h3>Philip J. Ball, Yingzhen Li, Angus Lamb, Cheng Zhang</h3>
<p>Humans are efficient continual learning systems; we continually learn new
skills from birth with finite cells and resources. Our learning is highly
optimized both in terms of capacity and time while not suffering from
catastrophic forgetting. In this work we study the efficiency of continual
learning systems, taking inspiration from human learning. In particular,
inspired by the mechanisms of sleep, we evaluate popular pruning-based
continual learning algorithms, using PackNet as a case study. First, we
identify that weight freezing, which is used in continual learning without
biological justification, can result in over $2\times$ as many weights being
used for a given level of performance. Secondly, we note the similarity in
human day and night time behaviors to the training and pruning phases
respectively of PackNet. We study a setting where the pruning phase is given a
time budget, and identify connections between iterative pruning and multiple
sleep cycles in humans. We show there exists an optimal choice of iteration
v.s. epochs given different tasks.
</p>
<a href="http://arxiv.org/abs/2010.15187" target="_blank">arXiv:2010.15187</a> [<a href="http://arxiv.org/pdf/2010.15187" target="_blank">pdf</a>]

<h2>Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in First-person Simulated 3D Environments. (arXiv:2010.15195v1 [cs.LG])</h2>
<h3>Wilka Carvalho, Anthony Liang, Kimin Lee, Sungryull Sohn, Honglak Lee, Richard L. Lewis, Satinder Singh</h3>
<p>First-person object-interaction tasks in high-fidelity, 3D, simulated
environments such as the AI2Thor virtual home-environment pose significant
sample-efficiency challenges for reinforcement learning (RL) agents learning
from sparse task rewards. To alleviate these challenges, prior work has
provided extensive supervision via a combination of reward-shaping,
ground-truth object-information, and expert demonstrations. In this work, we
show that one can learn object-interaction tasks from scratch without
supervision by learning an attentive object-model as an auxiliary task during
task learning with an object-centric relational RL agent. Our key insight is
that learning an object-model that incorporates object-attention into forward
prediction provides a dense learning signal for unsupervised representation
learning of both objects and their relationships. This, in turn, enables faster
policy learning for an object-centric relational RL agent. We demonstrate our
agent by introducing a set of challenging object-interaction tasks in the
AI2Thor environment where learning with our attentive object-model is key to
strong performance. Specifically, we compare our agent and relational RL agents
with alternative auxiliary tasks to a relational RL agent equipped with
ground-truth object-information, and show that learning with our object-model
best closes the performance gap in terms of both learning speed and maximum
success rate. Additionally, we find that incorporating object-attention into an
object-model's forward predictions is key to learning representations which
capture object-category and object-state.
</p>
<a href="http://arxiv.org/abs/2010.15195" target="_blank">arXiv:2010.15195</a> [<a href="http://arxiv.org/pdf/2010.15195" target="_blank">pdf</a>]

<h2>Rosella: A Self-Driving Distributed Scheduler for Heterogeneous Clusters. (arXiv:2010.15206v1 [cs.DC])</h2>
<h3>Qiong Wu, Sunil Manandhar, Zhenming Liu</h3>
<p>Large-scale interactive web services and advanced AI applications make
sophisticated decisions in real-time, based on executing a massive amount of
computation tasks on thousands of servers. Task schedulers, which often operate
in heterogeneous and volatile environments, require high throughput, i.e.,
scheduling millions of tasks per second, and low latency, i.e., incurring
minimal scheduling delays for millisecond-level tasks. Scheduling is further
complicated by other users' workloads in a shared system, other background
activities, and the diverse hardware configurations inside datacenters.

We present Rosella, a new self-driving, distributed approach for task
scheduling in heterogeneous clusters. Our system automatically learns the
compute environment and adjust its scheduling policy in real-time. The solution
provides high throughput and low latency simultaneously, because it runs in
parallel on multiple machines with minimum coordination and only performs
simple operations for each scheduling decision. Our learning module monitors
total system load, and uses the information to dynamically determine optimal
estimation strategy for the backends' compute-power. Our scheduling policy
generalizes power-of-two-choice algorithms to handle heterogeneous workers,
reducing the max queue length of $O(\log n)$ obtained by prior algorithms to
$O(\log \log n)$. We implement a Rosella prototype and evaluate it with a
variety of workloads. Experimental results show that Rosella significantly
reduces task response times, and adapts to environment changes quickly.
</p>
<a href="http://arxiv.org/abs/2010.15206" target="_blank">arXiv:2010.15206</a> [<a href="http://arxiv.org/pdf/2010.15206" target="_blank">pdf</a>]

<h2>A Visuospatial Dataset for Naturalistic Verb Learning. (arXiv:2010.15225v1 [cs.CL])</h2>
<h3>Dylan Ebert, Ellie Pavlick</h3>
<p>We introduce a new dataset for training and evaluating grounded language
models. Our data is collected within a virtual reality environment and is
designed to emulate the quality of language data to which a pre-verbal child is
likely to have access: That is, naturalistic, spontaneous speech paired with
richly grounded visuospatial context. We use the collected data to compare
several distributional semantics models for verb learning. We evaluate neural
models based on 2D (pixel) features as well as feature-engineered models based
on 3D (symbolic, spatial) features, and show that neither modeling approach
achieves satisfactory performance. Our results are consistent with evidence
from child language acquisition that emphasizes the difficulty of learning
verbs from naive distributional data. We discuss avenues for future work on
cognitively-inspired grounded language learning, and release our corpus with
the intent of facilitating research on the topic.
</p>
<a href="http://arxiv.org/abs/2010.15225" target="_blank">arXiv:2010.15225</a> [<a href="http://arxiv.org/pdf/2010.15225" target="_blank">pdf</a>]

<h2>Speech-Based Emotion Recognition using Neural Networks and Information Visualization. (arXiv:2010.15229v1 [cs.HC])</h2>
<h3>Jumana Almahmoud, Kruthika Kikkeri</h3>
<p>Emotions recognition is commonly employed for health assessment. However, the
typical metric for evaluation in therapy is based on patient-doctor appraisal.
This process can fall into the issue of subjectivity, while also requiring
healthcare professionals to deal with copious amounts of information. Thus,
machine learning algorithms can be a useful tool for the classification of
emotions. While several models have been developed in this domain, there is a
lack of userfriendly representations of the emotion classification systems for
therapy. We propose a tool which enables users to take speech samples and
identify a range of emotions (happy, sad, angry, surprised, neutral, clam,
disgust, and fear) from audio elements through a machine learning model. The
dashboard is designed based on local therapists' needs for intuitive
representations of speech data in order to gain insights and informative
analyses of their sessions with their patients.
</p>
<a href="http://arxiv.org/abs/2010.15229" target="_blank">arXiv:2010.15229</a> [<a href="http://arxiv.org/pdf/2010.15229" target="_blank">pdf</a>]

<h2>Accurate Prostate Cancer Detection and Segmentation on Biparametric MRI using Non-local Mask R-CNN with Histopathological Ground Truth. (arXiv:2010.15233v1 [eess.IV])</h2>
<h3>Zhenzhen Dai, Ivan Jambor, Pekka Taimen, Milan Pantelic, Mohamed Elshaikh, Craig Rogers, Otto Ettala, Peter Bostr&#xf6;m, Hannu Aronen, Harri Merisaari, Ning Wen</h3>
<p>Purpose: We aimed to develop deep machine learning (DL) models to improve the
detection and segmentation of intraprostatic lesions (IL) on bp-MRI by using
whole amount prostatectomy specimen-based delineations. We also aimed to
investigate whether transfer learning and self-training would improve results
with small amount labelled data.

Methods: 158 patients had suspicious lesions delineated on MRI based on
bp-MRI, 64 patients had ILs delineated on MRI based on whole mount
prostatectomy specimen sections, 40 patients were unlabelled. A non-local Mask
R-CNN was proposed to improve the segmentation accuracy. Transfer learning was
investigated by fine-tuning a model trained using MRI-based delineations with
prostatectomy-based delineations. Two label selection strategies were
investigated in self-training. The performance of models was evaluated by 3D
detection rate, dice similarity coefficient (DSC), 95 percentile Hausdrauff (95
HD, mm) and true positive ratio (TPR).

Results: With prostatectomy-based delineations, the non-local Mask R-CNN with
fine-tuning and self-training significantly improved all evaluation metrics.
For the model with the highest detection rate and DSC, 80.5% (33/41) of lesions
in all Gleason Grade Groups (GGG) were detected with DSC of 0.548[0.165], 95 HD
of 5.72[3.17] and TPR of 0.613[0.193]. Among them, 94.7% (18/19) of lesions
with GGG &gt; 2 were detected with DSC of 0.604[0.135], 95 HD of 6.26[3.44] and
TPR of 0.580[0.190].

Conclusion: DL models can achieve high prostate cancer detection and
segmentation accuracy on bp-MRI based on annotations from histologic images. To
further improve the performance, more data with annotations of both MRI and
whole amount prostatectomy specimens are required.
</p>
<a href="http://arxiv.org/abs/2010.15233" target="_blank">arXiv:2010.15233</a> [<a href="http://arxiv.org/pdf/2010.15233" target="_blank">pdf</a>]

<h2>Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions. (arXiv:2010.15234v1 [cs.LG])</h2>
<h3>Kartik Ahuja, Karthikeyan Shanmugam, Amit Dhurandhar</h3>
<p>Recently, invariant risk minimization (IRM) (Arjovsky et al.) was proposed as
a promising solution to address out-of-distribution (OOD) generalization. In
Ahuja et al., it was shown that solving for the Nash equilibria of a new class
of "ensemble-games" is equivalent to solving IRM. In this work, we extend the
framework in Ahuja et al. for linear regressions by projecting the
ensemble-game on an $\ell_{\infty}$ ball. We show that such projections help
achieve non-trivial OOD guarantees despite not achieving perfect invariance.
For linear models with confounders, we prove that Nash equilibria of these
games are closer to the ideal OOD solutions than the standard empirical risk
minimization (ERM) and we also provide learning algorithms that provably
converge to these Nash Equilibria. Empirical comparisons of the proposed
approach with the state-of-the-art show consistent gains in achieving OOD
solutions in several settings involving anti-causal variables and confounders.
</p>
<a href="http://arxiv.org/abs/2010.15234" target="_blank">arXiv:2010.15234</a> [<a href="http://arxiv.org/pdf/2010.15234" target="_blank">pdf</a>]

<h2>Test Set Optimization by Machine Learning Algorithms. (arXiv:2010.15240v1 [cs.LG])</h2>
<h3>Kaiming Fu, Yulu Jin, Zhousheng Chen</h3>
<p>Diagnosis results are highly dependent on the volume of test set. To derive
the most efficient test set, we propose several machine learning based methods
to predict the minimum amount of test data that produces relatively accurate
diagnosis. By collecting outputs from failing circuits, the feature matrix and
label vector are generated, which involves the inference information of the
test termination point. Thus we develop a prediction model to fit the data and
determine when to terminate testing. The considered methods include LASSO and
Support Vector Machine(SVM) where the relationship between goals(label) and
predictors(feature matrix) are considered to be linear in LASSO and nonlinear
in SVM. Numerical results show that SVM reaches a diagnosis accuracy of 90.4%
while deducting the volume of test set by 35.24%.
</p>
<a href="http://arxiv.org/abs/2010.15240" target="_blank">arXiv:2010.15240</a> [<a href="http://arxiv.org/pdf/2010.15240" target="_blank">pdf</a>]

<h2>Deep Shells: Unsupervised Shape Correspondence with Optimal Transport. (arXiv:2010.15261v1 [cs.CV])</h2>
<h3>Marvin Eisenberger, Aysim Toker, Laura Leal-Taix&#xe9;, Daniel Cremers</h3>
<p>We propose a novel unsupervised learning approach to 3D shape correspondence
that builds a multiscale matching pipeline into a deep neural network. This
approach is based on smooth shells, the current state-of-the-art axiomatic
correspondence method, which requires an a priori stochastic search over the
space of initial poses. Our goal is to replace this costly preprocessing step
by directly learning good initializations from the input surfaces. To that end,
we systematically derive a fully differentiable, hierarchical matching pipeline
from entropy regularized optimal transport. This allows us to combine it with a
local feature extractor based on smooth, truncated spectral convolution
filters. Finally, we show that the proposed unsupervised method significantly
improves over the state-of-the-art on multiple datasets, even in comparison to
the most recent supervised methods. Moreover, we demonstrate compelling
generalization results by applying our learned filters to examples that
significantly deviate from the training set.
</p>
<a href="http://arxiv.org/abs/2010.15261" target="_blank">arXiv:2010.15261</a> [<a href="http://arxiv.org/pdf/2010.15261" target="_blank">pdf</a>]

<h2>Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greedification in Reinforcement Learning. (arXiv:2010.15268v1 [cs.LG])</h2>
<h3>Kenny Young, Richard S. Sutton</h3>
<p>Despite empirical success, the theory of reinforcement learning (RL) with
value function approximation remains fundamentally incomplete. Prior work has
identified a variety of pathological behaviours that arise in RL algorithms
that combine approximate on-policy evaluation and greedification. One prominent
example is policy oscillation, wherein an algorithm may cycle indefinitely
between policies, rather than converging to a fixed point. What is not well
understood however is the quality of the policies in the region of oscillation.
In this paper we present simple examples illustrating that in addition to
policy oscillation and multiple fixed points -- the same basic issue can lead
to convergence to the worst possible policy for a given approximation. Such
behaviours can arise when algorithms optimize evaluation accuracy weighted by
the distribution of states that occur under the current policy, but greedify
based on the value of states which are rare or nonexistent under this
distribution. This means the values used for greedification are unreliable and
can steer the policy in undesirable directions. Our observation that this can
lead to the worst possible policy shows that in a general sense such algorithms
are unreliable. The existence of such examples helps to narrow the kind of
theoretical guarantees that are possible and the kind of algorithmic ideas that
are likely to be helpful. We demonstrate analytically and experimentally that
such pathological behaviours can impact a wide range of RL and dynamic
programming algorithms; such behaviours can arise both with and without
bootstrapping, and with linear function approximation as well as with more
complex parameterized functions like neural networks.
</p>
<a href="http://arxiv.org/abs/2010.15268" target="_blank">arXiv:2010.15268</a> [<a href="http://arxiv.org/pdf/2010.15268" target="_blank">pdf</a>]

<h2>GloFlow: Global Image Alignment for Creation of Whole Slide Images for Pathology from Video. (arXiv:2010.15269v1 [eess.IV])</h2>
<h3>Viswesh Krishna, Anirudh Joshi, Philip L. Bulterys, Eric Yang, Andrew Y. Ng, Pranav Rajpurkar</h3>
<p>The application of deep learning to pathology assumes the existence of
digital whole slide images of pathology slides. However, slide digitization is
bottlenecked by the high cost of precise motor stages in slide scanners that
are needed for position information used for slide stitching. We propose
GloFlow, a two-stage method for creating a whole slide image using optical
flow-based image registration with global alignment using a computationally
tractable graph-pruning approach. In the first stage, we train an optical flow
predictor to predict pairwise translations between successive video frames to
approximate a stitch. In the second stage, this approximate stitch is used to
create a neighborhood graph to produce a corrected stitch. On a simulated
dataset of video scans of WSIs, we find that our method outperforms known
approaches to slide-stitching, and stitches WSIs resembling those produced by
slide scanners.
</p>
<a href="http://arxiv.org/abs/2010.15269" target="_blank">arXiv:2010.15269</a> [<a href="http://arxiv.org/pdf/2010.15269" target="_blank">pdf</a>]

<h2>Representation learning for improved interpretability and classification accuracy of clinical factors from EEG. (arXiv:2010.15274v1 [cs.LG])</h2>
<h3>Garrett Honke, Irina Higgins, Nina Thigpen, Vladimir Miskovic, Katie Link, Pramod Gupta, Julia Klawohn, Greg Hajcak</h3>
<p>Despite extensive standardization, diagnostic interviews for mental health
disorders encompass substantial subjective judgment. Previous studies have
demonstrated that EEG-based neural measures can function as reliable objective
correlates of depression, or even predictors of depression and its course.
However, their clinical utility has not been fully realized because of 1) the
lack of automated ways to deal with the inherent noise associated with EEG data
at scale, and 2) the lack of knowledge of which aspects of the EEG signal may
be markers of a clinical disorder. Here we adapt an unsupervised pipeline from
the recent deep representation learning literature to address these problems by
1) learning a disentangled representation using $\beta$-VAE to denoise the
signal, and 2) extracting interpretable features associated with a sparse set
of clinical labels using a Symbol-Concept Association Network (SCAN). We
demonstrate that our method is able to outperform the canonical hand-engineered
baseline classification method on a number of factors, including participant
age and depression diagnosis. Furthermore, our method recovers a representation
that can be used to automatically extract denoised Event Related Potentials
(ERPs) from novel, single EEG trajectories, and supports fast supervised
re-mapping to various clinical labels, allowing clinicians to re-use a single
EEG representation regardless of updates to the standardized diagnostic system.
Finally, single factors of the learned disentangled representations often
correspond to meaningful markers of clinical factors, as automatically detected
by SCAN, allowing for human interpretability and post-hoc expert analysis of
the recommendations made by the model.
</p>
<a href="http://arxiv.org/abs/2010.15274" target="_blank">arXiv:2010.15274</a> [<a href="http://arxiv.org/pdf/2010.15274" target="_blank">pdf</a>]

<h2>Class-incremental learning: survey and performance evaluation. (arXiv:2010.15277v1 [cs.LG])</h2>
<h3>Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D. Bagdanov, Joost van de Weijer</h3>
<p>For future learning systems incremental learning is desirable, because it
allows for: efficient resource usage by eliminating the need to retrain from
scratch at the arrival of new data; reduced memory usage by preventing or
limiting the amount of data required to be stored -- also important when
privacy limitations are imposed; and learning that more closely resembles human
learning. The main challenge for incremental learning is catastrophic
forgetting, which refers to the precipitous drop in performance on previously
learned tasks after learning a new one. Incremental learning of deep neural
networks has seen explosive growth in recent years. Initial work focused on
task incremental learning, where a task-ID is provided at inference time.
Recently we have seen a shift towards class-incremental learning where the
learner must classify at inference time between all classes seen in previous
tasks without recourse to a task-ID. In this paper, we provide a complete
survey of existing methods for incremental learning, and in particular we
perform an extensive experimental evaluation on twelve class-incremental
methods. We consider several new experimental scenarios, including a comparison
of class-incremental methods on multiple large-scale datasets, investigation
into small and large domain shifts, and comparison on various network
architectures.
</p>
<a href="http://arxiv.org/abs/2010.15277" target="_blank">arXiv:2010.15277</a> [<a href="http://arxiv.org/pdf/2010.15277" target="_blank">pdf</a>]

<h2>GENs: Generative Encoding Networks. (arXiv:2010.15283v1 [cs.LG])</h2>
<h3>Surojit Saha, Shireen Elhabian, Ross T. Whitaker</h3>
<p>Mapping data from and/or onto a known family of distributions has become an
important topic in machine learning and data analysis. Deep generative models
(e.g., generative adversarial networks ) have been used effectively to match
known and unknown distributions. Nonetheless, when the form of the target
distribution is known, analytical methods are advantageous in providing robust
results with provable properties. In this paper, we propose and analyze the use
of nonparametric density methods to estimate the Jensen-Shannon divergence for
matching unknown data distributions to known target distributions, such
Gaussian or mixtures of Gaussians, in latent spaces. This analytical method has
several advantages: better behavior when training sample quantity is low,
provable convergence properties, and relatively few parameters, which can be
derived analytically. Using the proposed method, we enforce the latent
representation of an autoencoder to match a target distribution in a learning
framework that we call a {\em generative encoding network}. Here, we present
the numerical methods; derive the expected distribution of the data in the
latent space; evaluate the properties of the latent space, sample
reconstruction, and generated samples; show the advantages over the adversarial
counterpart; and demonstrate the application of the method in real world.
</p>
<a href="http://arxiv.org/abs/2010.15283" target="_blank">arXiv:2010.15283</a> [<a href="http://arxiv.org/pdf/2010.15283" target="_blank">pdf</a>]

<h2>Link inference of noisy delay-coupled networks: Machine learning and opto-electronic experimental tests. (arXiv:2010.15289v1 [nlin.AO])</h2>
<h3>Amitava Banerjee, Joseph D. Hart, Rajarshi Roy, Edward Ott</h3>
<p>We devise a machine learning technique to solve the general problem of
inferring network links that have time-delays. The goal is to do this purely
from time-series data of the network nodal states. This task has applications
in fields ranging from applied physics and engineering to neuroscience and
biology. To achieve this, we first train a type of machine learning system
known as reservoir computing to mimic the dynamics of the unknown network. We
formulate and test a technique that uses the trained parameters of the
reservoir system output layer to deduce an estimate of the unknown network
structure. Our technique, by its nature, is non-invasive, but is motivated by
the widely-used invasive network inference method whereby the responses to
active perturbations applied to the network are observed and employed to infer
network links (e.g., knocking down genes to infer gene regulatory networks). We
test this technique on experimental and simulated data from delay-coupled
opto-electronic oscillator networks. We show that the technique often yields
very good results particularly if the system does not exhibit synchrony. We
also find that the presence of dynamical noise can strikingly enhance the
accuracy and ability of our technique, especially in networks that exhibit
synchrony.
</p>
<a href="http://arxiv.org/abs/2010.15289" target="_blank">arXiv:2010.15289</a> [<a href="http://arxiv.org/pdf/2010.15289" target="_blank">pdf</a>]

<h2>Fact or Factitious? Contextualized Opinion Spam Detection. (arXiv:2010.15296v1 [cs.AI])</h2>
<h3>Stefan Kennedy, Niall Walsh, Kirils Sloka, Jennifer Foster, Andrew McCarren</h3>
<p>In this paper we perform an analytic comparison of a number of techniques
used to detect fake and deceptive online reviews. We apply a number machine
learning approaches found to be effective, and introduce our own approach by
fine-tuning state of the art contextualised embeddings. The results we obtain
show the potential of contextualised embeddings for fake review detection, and
lay the groundwork for future research in this area.
</p>
<a href="http://arxiv.org/abs/2010.15296" target="_blank">arXiv:2010.15296</a> [<a href="http://arxiv.org/pdf/2010.15296" target="_blank">pdf</a>]

<h2>Uncovering Latent Biases in Text: Method and Application to Peer Review. (arXiv:2010.15300v1 [cs.CL])</h2>
<h3>Emaad Manzoor, Nihar B. Shah</h3>
<p>Quantifying systematic disparities in numerical quantities such as employment
rates and wages between population subgroups provides compelling evidence for
the existence of societal biases. However, biases in the text written for
members of different subgroups (such as in recommendation letters for male and
non-male candidates), though widely reported anecdotally, remain challenging to
quantify. In this work, we introduce a novel framework to quantify bias in text
caused by the visibility of subgroup membership indicators. We develop a
nonparametric estimation and inference procedure to estimate this bias. We then
formalize an identification strategy to causally link the estimated bias to the
visibility of subgroup membership indicators, provided observations from time
periods both before and after an identity-hiding policy change. We identify an
application wherein "ground truth" bias can be inferred to evaluate our
framework, instead of relying on synthetic or secondary data. Specifically, we
apply our framework to quantify biases in the text of peer reviews from a
reputed machine learning conference before and after the conference adopted a
double-blind reviewing policy. We show evidence of biases in the review ratings
that serves as "ground truth", and show that our proposed framework accurately
detects these biases from the review text without having access to the review
ratings.
</p>
<a href="http://arxiv.org/abs/2010.15300" target="_blank">arXiv:2010.15300</a> [<a href="http://arxiv.org/pdf/2010.15300" target="_blank">pdf</a>]

<h2>Point Cloud Attribute Compression via Successive Subspace Graph Transform. (arXiv:2010.15302v1 [cs.CV])</h2>
<h3>Yueru Chen, Yiting Shao, Jing Wang, Ge Li, C.-C. Jay Kuo</h3>
<p>Inspired by the recently proposed successive subspace learning (SSL)
principles, we develop a successive subspace graph transform (SSGT) to address
point cloud attribute compression in this work. The octree geometry structure
is utilized to partition the point cloud, where every node of the octree
represents a point cloud subspace with a certain spatial size. We design a
weighted graph with self-loop to describe the subspace and define a graph
Fourier transform based on the normalized graph Laplacian. The transforms are
applied to large point clouds from the leaf nodes to the root node of the
octree recursively, while the represented subspace is expanded from the
smallest one to the whole point cloud successively. It is shown by experimental
results that the proposed SSGT method offers better R-D performances than the
previous Region Adaptive Haar Transform (RAHT) method.
</p>
<a href="http://arxiv.org/abs/2010.15302" target="_blank">arXiv:2010.15302</a> [<a href="http://arxiv.org/pdf/2010.15302" target="_blank">pdf</a>]

<h2>Automatic joint damage quantification using computer vision and deep learning. (arXiv:2010.15303v1 [cs.CV])</h2>
<h3>Quang Tran, Jeffery R. Roesler</h3>
<p>Joint raveled or spalled damage (henceforth called joint damage) can affect
the safety and long-term performance of concrete pavements. It is important to
assess and quantify the joint damage over time to assist in building action
plans for maintenance, predicting maintenance costs, and maximize the concrete
pavement service life. A framework for the accurate, autonomous, and rapid
quantification of joint damage with a low-cost camera is proposed using a
computer vision technique with a deep learning (DL) algorithm. The DL model is
employed to train 263 images of sawcuts with joint damage. The trained DL model
is used for pixel-wise color-masking joint damage in a series of query 2D
images, which are used to reconstruct a 3D image using open-source structure
from motion algorithm. Another damage quantification algorithm using a color
threshold is applied to detect and compute the surface area of the damage in
the 3D reconstructed image. The effectiveness of the framework was validated
through inspecting joint damage at four transverse contraction joints in
Illinois, USA, including three acceptable joints and one unacceptable joint by
visual inspection. The results show the framework achieves 76% recall and 10%
error.
</p>
<a href="http://arxiv.org/abs/2010.15303" target="_blank">arXiv:2010.15303</a> [<a href="http://arxiv.org/pdf/2010.15303" target="_blank">pdf</a>]

<h2>DeviceTTS: A Small-Footprint, Fast, Stable Network for On-Device Text-to-Speech. (arXiv:2010.15311v1 [eess.AS])</h2>
<h3>Zhiying Huang, Hao Li, Ming Lei</h3>
<p>With the number of smart devices increasing, the demand for on-device
text-to-speech (TTS) increases rapidly. In recent years, many prominent
End-to-End TTS methods have been proposed, and have greatly improved the
quality of synthesized speech. However, to ensure the qualified speech, most
TTS systems depend on large and complex neural network models, and it's hard to
deploy these TTS systems on-device. In this paper, a small-footprint, fast,
stable network for on-device TTS is proposed, named as DeviceTTS. DeviceTTS
makes use of a duration predictor as a bridge between encoder and decoder so as
to avoid the problem of words skipping and repeating in Tacotron. As we all
know, model size is a key factor for on-device TTS. For DeviceTTS, Deep
Feedforward Sequential Memory Network (DFSMN) is used as the basic component.
Moreover, to speed up inference, mix-resolution decoder is proposed for balance
the inference speed and speech quality. Experiences are done with WORLD and
LPCNet vocoder. Finally, with only 1.4 million model parameters and 0.099
GFLOPS, DeviceTTS achieves comparable performance with Tacotron and FastSpeech.
As far as we know, the DeviceTTS can meet the needs of most of the devices in
practical application.
</p>
<a href="http://arxiv.org/abs/2010.15311" target="_blank">arXiv:2010.15311</a> [<a href="http://arxiv.org/pdf/2010.15311" target="_blank">pdf</a>]

<h2>Recurrent neural circuits for contour detection. (arXiv:2010.15314v1 [cs.CV])</h2>
<h3>Drew Linsley, Junkyung Kim, Alekh Ashok, Thomas Serre</h3>
<p>We introduce a deep recurrent neural network architecture that approximates
visual cortical circuits. We show that this architecture, which we refer to as
the gamma-net, learns to solve contour detection tasks with better sample
efficiency than state-of-the-art feedforward networks, while also exhibiting a
classic perceptual illusion, known as the orientation-tilt illusion. Correcting
this illusion significantly reduces gamma-net contour detection accuracy by
driving it to prefer low-level edges over high-level object boundary contours.
Overall, our study suggests that the orientation-tilt illusion is a byproduct
of neural circuits that help biological visual systems achieve robust and
efficient contour detection, and that incorporating these circuits in
artificial neural networks can improve computer vision.
</p>
<a href="http://arxiv.org/abs/2010.15314" target="_blank">arXiv:2010.15314</a> [<a href="http://arxiv.org/pdf/2010.15314" target="_blank">pdf</a>]

<h2>Exploring Generative Adversarial Networks for Image-to-Image Translation in STEM Simulation. (arXiv:2010.15315v1 [cs.CV])</h2>
<h3>Nick Lawrence, Mingren Shen, Ruiqi Yin, Cloris Feng, Dane Morgan</h3>
<p>The use of accurate scanning transmission electron microscopy (STEM) image
simulation methods require large computation times that can make their use
infeasible for the simulation of many images. Other simulation methods based on
linear imaging models, such as the convolution method, are much faster but are
too inaccurate to be used in application. In this paper, we explore deep
learning models that attempt to translate a STEM image produced by the
convolution method to a prediction of the high accuracy multislice image. We
then compare our results to those of regression methods. We find that using the
deep learning model Generative Adversarial Network (GAN) provides us with the
best results and performs at a similar accuracy level to previous regression
models on the same dataset. Codes and data for this project can be found in
this GitHub repository, https://github.com/uw-cmg/GAN-STEM-Conv2MultiSlice.
</p>
<a href="http://arxiv.org/abs/2010.15315" target="_blank">arXiv:2010.15315</a> [<a href="http://arxiv.org/pdf/2010.15315" target="_blank">pdf</a>]

<h2>Gaussian Processes Model-based Control of Underactuated Balance Robots. (arXiv:2010.15320v1 [cs.RO])</h2>
<h3>Kuo Chen, Jingang Yi, Dezhen Song</h3>
<p>Ranging from cart-pole systems and autonomous bicycles to bipedal robots,
control of these underactuated balance robots aims to achieve both external
(actuated) subsystem trajectory tracking and internal (unactuated) subsystem
balancing tasks with limited actuation authority. This paper proposes a
learning model-based control framework for underactuated balance robots. The
key idea to simultaneously achieve tracking and balancing tasks is to design
control strategies in slow- and fast-time scales, respectively. In slow-time
scale, model predictive control (MPC) is used to generate the desired internal
subsystem trajectory that encodes the external subsystem tracking performance
and control input. In fast-time scale, the actual internal trajectory is
stabilized to the desired internal trajectory by using an inverse dynamics
controller. The coupling effects between the external and internal subsystems
are captured through the planned internal trajectory profile and the dual
structural properties of the robotic systems. The control design is based on
Gaussian processes (GPs) regression model that are learned from experiments
without need of priori knowledge about the robot dynamics nor successful
balance demonstration. The GPs provide estimates of modeling uncertainties of
the robotic systems and these uncertainty estimations are incorporated in the
MPC design to enhance the control robustness to modeling errors. The
learning-based control design is analyzed with guaranteed stability and
performance. The proposed design is demonstrated by experiments on a Furuta
pendulum and an autonomous bikebot.
</p>
<a href="http://arxiv.org/abs/2010.15320" target="_blank">arXiv:2010.15320</a> [<a href="http://arxiv.org/pdf/2010.15320" target="_blank">pdf</a>]

<h2>Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth. (arXiv:2010.15327v1 [cs.LG])</h2>
<h3>Thao Nguyen, Maithra Raghu, Simon Kornblith</h3>
<p>A key factor in the success of deep neural networks is the ability to scale
models to improve performance by varying the architecture depth and width. This
simple property of neural network design has resulted in highly effective
architectures for a variety of tasks. Nevertheless, there is limited
understanding of effects of depth and width on the learned representations. In
this paper, we study this fundamental question. We begin by investigating how
varying depth and width affects model hidden representations, finding a
characteristic block structure in the hidden representations of larger capacity
(wider or deeper) models. We demonstrate that this block structure arises when
model capacity is large relative to the size of the training set, and is
indicative of the underlying layers preserving and propagating the dominant
principal component of their representations. This discovery has important
ramifications for features learned by different models, namely, representations
outside the block structure are often similar across architectures with varying
widths and depths, but the block structure is unique to each model. We analyze
the output predictions of different model architectures, finding that even when
the overall accuracy is similar, wide and deep models exhibit distinctive error
patterns and variations across classes.
</p>
<a href="http://arxiv.org/abs/2010.15327" target="_blank">arXiv:2010.15327</a> [<a href="http://arxiv.org/pdf/2010.15327" target="_blank">pdf</a>]

<h2>Learning Sampling Distributions Using Local 3D Workspace Decompositions for Motion Planning in High Dimensions. (arXiv:2010.15335v1 [cs.RO])</h2>
<h3>Constantinos Chamzas, Zachary Kingston, Carlos Quintero-Pe&#xf1;a, Anshumali Shrivastava, Lydia E. Kavraki</h3>
<p>Earlier work has shown that reusing experience from prior motion planning
problems can improve the efficiency of similar, future motion planning queries.
However, for robots with many degrees-of-freedom, these methods exhibit poor
generalization across different environments and often require large datasets
that are impractical to gather. We present SPARK and FLAME , two
experience-based frameworks for sampling-based planning applicable to complex
manipulators in 3 D environments. Both combine samplers associated with
features from a workspace decomposition into a global biased sampling
distribution. SPARK decomposes the environment based on exact geometry while
FLAME is more general, and uses an octree-based decomposition obtained from
sensor data. We demonstrate the effectiveness of SPARK and FLAME on a Fetch
robot tasked with challenging pick-and-place manipulation problems. Our
approaches can be trained incrementally and significantly improve performance
with only a handful of examples, generalizing better over diverse tasks and
environments as compared to prior approaches.
</p>
<a href="http://arxiv.org/abs/2010.15335" target="_blank">arXiv:2010.15335</a> [<a href="http://arxiv.org/pdf/2010.15335" target="_blank">pdf</a>]

<h2>Identifying safe intersection design through unsupervised feature extraction from satellite imagery. (arXiv:2010.15343v1 [cs.CV])</h2>
<h3>Jasper S. Wijnands, Haifeng Zhao, Kerry A. Nice, Jason Thompson, Katherine Scully, Jingqiu Guo, Mark Stevenson</h3>
<p>The World Health Organization has listed the design of safer intersections as
a key intervention to reduce global road trauma. This article presents the
first study to systematically analyze the design of all intersections in a
large country, based on aerial imagery and deep learning. Approximately 900,000
satellite images were downloaded for all intersections in Australia and
customized computer vision techniques emphasized the road infrastructure. A
deep autoencoder extracted high-level features, including the intersection's
type, size, shape, lane markings, and complexity, which were used to cluster
similar designs. An Australian telematics data set linked infrastructure design
to driving behaviors captured during 66 million kilometers of driving. This
showed more frequent hard acceleration events (per vehicle) at four- than
three-way intersections, relatively low hard deceleration frequencies at
T-intersections, and consistently low average speeds on roundabouts. Overall,
domain-specific feature extraction enabled the identification of infrastructure
improvements that could result in safer driving behaviors, potentially reducing
road trauma.
</p>
<a href="http://arxiv.org/abs/2010.15343" target="_blank">arXiv:2010.15343</a> [<a href="http://arxiv.org/pdf/2010.15343" target="_blank">pdf</a>]

<h2>Sea-Net: Squeeze-And-Excitation Attention Net For Diabetic Retinopathy Grading. (arXiv:2010.15344v1 [cs.CV])</h2>
<h3>Ziyuan Zhao, Kartik Chopra, Zeng Zeng, Xiaoli Li</h3>
<p>Diabetes is one of the most common disease in individuals. \textit{Diabetic
retinopathy} (DR) is a complication of diabetes, which could lead to blindness.
Automatic DR grading based on retinal images provides a great diagnostic and
prognostic value for treatment planning. However, the subtle differences among
severity levels make it difficult to capture important features using
conventional methods. To alleviate the problems, a new deep learning
architecture for robust DR grading is proposed, referred to as SEA-Net, in
which, spatial attention and channel attention are alternatively carried out
and boosted with each other, improving the classification performance. In
addition, a hybrid loss function is proposed to further maximize the
inter-class distance and reduce the intra-class variability. Experimental
results have shown the effectiveness of the proposed architecture.
</p>
<a href="http://arxiv.org/abs/2010.15344" target="_blank">arXiv:2010.15344</a> [<a href="http://arxiv.org/pdf/2010.15344" target="_blank">pdf</a>]

<h2>Developing Augmented Reality based Gaming Model to Teach Ethical Education in Primary Schools. (arXiv:2010.15346v1 [cs.CY])</h2>
<h3>Mohammad Ali</h3>
<p>Education sector is adopting new technologies for both teaching and learning
pedagogy. Augmented Reality (AR) is a new technology that can be used in the
educational pedagogy to enhance the engagement with students. Students interact
with AR-based educational material for more visualization and explanation.
Therefore, the use of AR in education is becoming more popular. However, most
researches narrate the use of AR technologies in the field of English, Maths,
Science, Culture, Arts, and History education but the absence of ethical
education is visible. In our paper, we design the system and develop an
AR-based mobile game model in the field of Ethical education for pre-primary
students. Students from pre-primary require more interactive lessons than
theoretical concepts. So, we use AR technology to develop a game which offers
interactive procedures where students can learn with fun and engage with the
context. Finally, we develop a prototype that works with our research
objective. We conclude our paper with future works.
</p>
<a href="http://arxiv.org/abs/2010.15346" target="_blank">arXiv:2010.15346</a> [<a href="http://arxiv.org/pdf/2010.15346" target="_blank">pdf</a>]

<h2>Financial ticket intelligent recognition system based on deep learning. (arXiv:2010.15356v1 [cs.LG])</h2>
<h3>Fukang Tian, Haiyu Wu, Bo Xu</h3>
<p>Facing the rapid growth in the issuance of financial tickets (or bills,
invoices etc.), traditional manual invoice reimbursement and financial
accounting system are imposing an increasing burden on financial accountants
and consuming excessive manpower. To solve this problem, we proposes an
iterative self-learning Framework of Financial Ticket intelligent Recognition
System (FFTRS), which can support the fast iterative updating and extensibility
of the algorithm model, which are the fundamental requirements for a practical
financial accounting system. In addition, we designed a simple yet efficient
Financial Ticket Faster Detection network (FTFDNet) and an intelligent data
warehouse of financial ticket are designed to strengthen its efficiency and
performance. At present, the system can recognize 194 kinds of financial
tickets and has an automatic iterative optimization mechanism, which means,
with the increase of application time, the types of tickets supported by the
system will continue to increase, and the accuracy of recognition will continue
to improve. Experimental results show that the average recognition accuracy of
the system is 97.07%, and the average running time for a single ticket is
175.67ms. The practical value of the system has been tested in a commercial
application, which makes a beneficial attempt for the deep learning technology
in financial accounting work.
</p>
<a href="http://arxiv.org/abs/2010.15356" target="_blank">arXiv:2010.15356</a> [<a href="http://arxiv.org/pdf/2010.15356" target="_blank">pdf</a>]

<h2>Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection. (arXiv:2010.15360v1 [cs.CL])</h2>
<h3>Shaolei Wang, Zhongyuan Wang, Wanxiang Che, Ting Liu</h3>
<p>Most existing approaches to disfluency detection heavily rely on
human-annotated corpora, which is expensive to obtain in practice. There have
been several proposals to alleviate this issue with, for instance,
self-supervised learning techniques, but they still require human-annotated
corpora. In this work, we explore the unsupervised learning paradigm which can
potentially work with unlabeled text corpora that are cheaper and easier to
obtain. Our model builds upon the recent work on Noisy Student Training, a
semi-supervised learning approach that extends the idea of self-training.
Experimental results on the commonly used English Switchboard test set show
that our approach achieves competitive performance compared to the previous
state-of-the-art supervised systems using contextualized word embeddings (e.g.
BERT and ELECTRA).
</p>
<a href="http://arxiv.org/abs/2010.15360" target="_blank">arXiv:2010.15360</a> [<a href="http://arxiv.org/pdf/2010.15360" target="_blank">pdf</a>]

<h2>Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System. (arXiv:2010.15363v1 [cs.IR])</h2>
<h3>Tianxin Wei, Fuli Feng, Jiawei Chen, Chufeng Shi, Ziwei Wu, Jinfeng Yi, Xiangnan He</h3>
<p>The general aim of the recommender system is to provide personalized
suggestions to users, which is opposed to suggesting popular items. However,
the normal training paradigm, i.e., fitting a recommender model to recover the
user behavior data with pointwise or pairwise loss, makes the model biased
towards popular items. This results in the terrible Matthew effect, making
popular items be more frequently recommended and become even more popular.
Existing work addresses this issue with Inverse Propensity Weighting (IPW),
which decreases the impact of popular items on the training and increases the
impact of long-tail items. Although theoretically sound, IPW methods are highly
sensitive to the weighting strategy, which is notoriously difficult to tune.

In this work, we explore the popularity bias issue from a novel and
fundamental perspective -- cause-effect. We identify that popularity bias lies
in the direct effect from the item node to the ranking score, such that an
item's intrinsic property is the cause of mistakenly assigning it a higher
ranking score. To eliminate popularity bias, it is essential to answer the
counterfactual question that what the ranking score would be if the model only
uses item property. To this end, we formulate a causal graph to describe the
important cause-effect relations in the recommendation process. During
training, we perform multi-task learning to achieve the contribution of each
cause; during testing, we perform counterfactual inference to remove the effect
of item popularity. Remarkably, our solution amends the learning process of
recommendation which is agnostic to a wide range of models. We demonstrate it
on Matrix Factorization (MF) and LightGCN, which are representative of the
conventional and state-of-the-art model for collaborative filtering.
Experiments on five real-world datasets demonstrate the effectiveness of our
method.
</p>
<a href="http://arxiv.org/abs/2010.15363" target="_blank">arXiv:2010.15363</a> [<a href="http://arxiv.org/pdf/2010.15363" target="_blank">pdf</a>]

<h2>Learning Personalized Discretionary Lane-Change Initiation for Fully Autonomous Driving Based on Reinforcement Learning. (arXiv:2010.15372v1 [cs.HC])</h2>
<h3>Zhuoxi Liu, Zheng Wang, Bo Yang, Kimihiko Nakano</h3>
<p>In this article, the authors present a novel method to learn the personalized
tactic of discretionary lane-change initiation for fully autonomous vehicles
through human-computer interactions. Instead of learning from human-driving
demonstrations, a reinforcement learning technique is employed to learn how to
initiate lane changes from traffic context, the action of a self-driving
vehicle, and in-vehicle user feedback. The proposed offline algorithm rewards
the action-selection strategy when the user gives positive feedback and
penalizes it when negative feedback. Also, a multi-dimensional driving scenario
is considered to represent a more realistic lane-change trade-off. The results
show that the lane-change initiation model obtained by this method can
reproduce the personal lane-change tactic, and the performance of the
customized models (average accuracy 86.1%) is much better than that of the
non-customized models (average accuracy 75.7%). This method allows continuous
improvement of customization for users during fully autonomous driving even
without human-driving experience, which will significantly enhance the user
acceptance of high-level autonomy of self-driving vehicles.
</p>
<a href="http://arxiv.org/abs/2010.15372" target="_blank">arXiv:2010.15372</a> [<a href="http://arxiv.org/pdf/2010.15372" target="_blank">pdf</a>]

<h2>Solving Sparse Linear Inverse Problems in Communication Systems: A Deep Learning Approach With Adaptive Depth. (arXiv:2010.15376v1 [eess.SP])</h2>
<h3>Wei Chen, Bowen Zhang, Shi Jin, Bo Ai, Zhangdui Zhong</h3>
<p>Sparse signal recovery problems from noisy linear measurements appear in many
areas of wireless communications. In recent years, deep learning (DL) based
approaches have attracted interests of researchers to solve the sparse linear
inverse problem by unfolding iterative algorithms as neural networks.
Typically, research concerning DL assume a fixed number of network layers.
However, it ignores a key character in traditional iterative algorithms, where
the number of iterations required for convergence changes with varying sparsity
levels. By investigating on the projected gradient descent, we unveil the
drawbacks of the existing DL methods with fixed depth. Then we propose an
end-to-end trainable DL architecture, which involves an extra halting score at
each layer. Therefore, the proposed method learns how many layers to execute to
emit an output, and the network depth is dynamically adjusted for each task in
the inference phase. We conduct experiments using both synthetic data and
applications including random access in massive MTC and massive MIMO channel
estimation, and the results demonstrate the improved efficiency for the
proposed approach.
</p>
<a href="http://arxiv.org/abs/2010.15376" target="_blank">arXiv:2010.15376</a> [<a href="http://arxiv.org/pdf/2010.15376" target="_blank">pdf</a>]

<h2>Collaborative Method for Incremental Learning on Classification and Generation. (arXiv:2010.15378v1 [cs.CV])</h2>
<h3>Byungju Kim, Jaeyoung Lee, Kyungsu Kim, Sungjin Kim, Junmo Kim</h3>
<p>Although well-trained deep neural networks have shown remarkable performance
on numerous tasks, they rapidly forget what they have learned as soon as they
begin to learn with additional data with the previous data stop being provided.
In this paper, we introduce a novel algorithm, Incremental Class Learning with
Attribute Sharing (ICLAS), for incremental class learning with deep neural
networks. As one of its component, we also introduce a generative model,
incGAN, which can generate images with increased variety compared with the
training data. Under challenging environment of data deficiency, ICLAS
incrementally trains classification and the generation networks. Since ICLAS
trains both networks, our algorithm can perform multiple times of incremental
class learning. The experiments on MNIST dataset demonstrate the advantages of
our algorithm.
</p>
<a href="http://arxiv.org/abs/2010.15378" target="_blank">arXiv:2010.15378</a> [<a href="http://arxiv.org/pdf/2010.15378" target="_blank">pdf</a>]

<h2>Learning to Actively Learn: A Robust Approach. (arXiv:2010.15382v1 [cs.LG])</h2>
<h3>Jifan Zhang, Kevin Jamieson</h3>
<p>This work proposes a procedure for designing algorithms for specific adaptive
data collection tasks like active learning and pure-exploration multi-armed
bandits. Unlike the design of traditional adaptive algorithms that rely on
concentration of measure and careful analysis to justify the correctness and
sample complexity of the procedure, our adaptive algorithm is learned via
adversarial training over equivalence classes of problems derived from
information theoretic lower bounds. In particular, a single adaptive learning
algorithm is learned that competes with the best adaptive algorithm learned for
each equivalence class. Our procedure takes as input just the available
queries, set of hypotheses, loss function, and total query budget. This is in
contrast to existing meta-learning work that learns an adaptive algorithm
relative to an explicit, user-defined subset or prior distribution over
problems which can be challenging to define and be mismatched to the instance
encountered at test time. This work is particularly focused on the regime when
the total query budget is very small, such as a few dozen, which is much
smaller than those budgets typically considered by theoretically derived
algorithms. We perform synthetic experiments to justify the stability and
effectiveness of the training procedure, and then evaluate the method on tasks
derived from real data including a noisy 20 Questions game and a joke
recommendation task.
</p>
<a href="http://arxiv.org/abs/2010.15382" target="_blank">arXiv:2010.15382</a> [<a href="http://arxiv.org/pdf/2010.15382" target="_blank">pdf</a>]

<h2>Prediction-Based Power Oversubscription in Cloud Platforms. (arXiv:2010.15388v1 [cs.DC])</h2>
<h3>Alok Kumbhare, Reza Azimi, Ioannis Manousakis, Anand Bonde, Felipe Frujeri, Nithish Mahalingam, Pulkit Misra, Seyyed Ahmad Javadi, Bianca Schroeder, Marcus Fontoura, Ricardo Bianchini</h3>
<p>Datacenter designers rely on conservative estimates of IT equipment power
draw to provision resources. This leaves resources underutilized and requires
more datacenters to be built. Prior work has used power capping to shave the
rare power peaks and add more servers to the datacenter, thereby
oversubscribing its resources and lowering capital costs. This works well when
the workloads and their server placements are known. Unfortunately, these
factors are unknown in public clouds, forcing providers to limit the
oversubscription so that performance is never impacted.

In this paper, we argue that providers can use predictions of workload
performance criticality and virtual machine (VM) resource utilization to
increase oversubscription. This poses many challenges, such as identifying the
performance-critical workloads from black-box VMs, creating support for
criticality-aware power management, and increasing oversubscription while
limiting the impact of capping. We address these challenges for the hardware
and software infrastructures of Microsoft Azure. The results show that we
enable a 2x increase in oversubscription with minimum impact to critical
workloads.
</p>
<a href="http://arxiv.org/abs/2010.15388" target="_blank">arXiv:2010.15388</a> [<a href="http://arxiv.org/pdf/2010.15388" target="_blank">pdf</a>]

<h2>Learning Audio Embeddings with User Listening Data for Content-based Music Recommendation. (arXiv:2010.15389v1 [cs.SD])</h2>
<h3>Ke Chen, Beici Liang, Xiaoshuan Ma, Minwei Gu</h3>
<p>Personalized recommendation on new track releases has always been a
challenging problem in the music industry. To combat this problem, we first
explore user listening history and demographics to construct a user embedding
representing the user's music preference. With the user embedding and audio
data from user's liked and disliked tracks, an audio embedding can be obtained
for each track using metric learning with Siamese networks. For a new track, we
can decide the best group of users to recommend by computing the similarity
between the track's audio embedding and different user embeddings,
respectively. The proposed system yields state-of-the-art performance on
content-based music recommendation tested with millions of users and tracks.
Also, we extract audio embeddings as features for music genre classification
tasks. The results show the generalization ability of our audio embeddings.
</p>
<a href="http://arxiv.org/abs/2010.15389" target="_blank">arXiv:2010.15389</a> [<a href="http://arxiv.org/pdf/2010.15389" target="_blank">pdf</a>]

<h2>Multitask Bandit Learning through Heterogeneous Feedback Aggregation. (arXiv:2010.15390v1 [cs.LG])</h2>
<h3>Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel D. Riek, Kamalika Chaudhuri</h3>
<p>In many real-world applications, multiple agents seek to learn how to perform
highly related yet slightly different tasks in an online bandit learning
protocol. We formulate this problem as the $\epsilon$-multi-player multi-armed
bandit problem, in which a set of players concurrently interact with a set of
arms, and for each arm, the reward distributions for all players are similar
but not necessarily identical. We develop an upper confidence bound-based
algorithm, RobustAgg$(\epsilon)$, that adaptively aggregates rewards collected
by different players. In the setting where an upper bound on the pairwise
similarities of reward distributions between players is known, we achieve
instance-dependent regret guarantees that depend on the amenability of
information sharing across players. We complement these upper bounds with
nearly matching lower bounds. In the setting where pairwise similarities are
unknown, we provide a lower bound, as well as an algorithm that trades off
minimax regret guarantees for adaptivity to unknown similarity structure.
</p>
<a href="http://arxiv.org/abs/2010.15390" target="_blank">arXiv:2010.15390</a> [<a href="http://arxiv.org/pdf/2010.15390" target="_blank">pdf</a>]

<h2>Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v1 [cs.LG])</h2>
<h3>Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, Chelsea Finn</h3>
<p>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, na\"ive formulations often
degrade performance and in particular, identifying the tasks that would benefit
from co-training remains a challenging design question. In this paper, we
analyze the dynamics of information transfer, or transference, across tasks
throughout training. Specifically, we develop a similarity measure that can
quantify transference among tasks and use this quantity to both better
understand the optimization dynamics of multi-task learning as well as improve
overall learning performance. In the latter case, we propose two methods to
leverage our transference metric. The first operates at a macro-level by
selecting which tasks should train together while the second functions at a
micro-level by determining how to combine task gradients at each training step.
We find these methods can lead to significant improvement over prior work on
three supervised multi-task learning benchmarks and one multi-task
reinforcement learning paradigm.
</p>
<a href="http://arxiv.org/abs/2010.15413" target="_blank">arXiv:2010.15413</a> [<a href="http://arxiv.org/pdf/2010.15413" target="_blank">pdf</a>]

<h2>A Novel Anomaly Detection Algorithm for Hybrid Production Systems based on Deep Learning and Timed Automata. (arXiv:2010.15415v1 [cs.LG])</h2>
<h3>Nemanja Hranisavljevic, Oliver Niggemann, Alexander Maier</h3>
<p>Performing anomaly detection in hybrid systems is a challenging task since it
requires analysis of timing behavior and mutual dependencies of both discrete
and continuous signals. Typically, it requires modeling system behavior, which
is often accomplished manually by human engineers. Using machine learning for
creating a behavioral model from observations has advantages, such as lower
development costs and fewer requirements for specific knowledge about the
system. The paper presents DAD:DeepAnomalyDetection, a new approach for
automatic model learning and anomaly detection in hybrid production systems. It
combines deep learning and timed automata for creating behavioral model from
observations. The ability of deep belief nets to extract binary features from
real-valued inputs is used for transformation of continuous to discrete
signals. These signals, together with the original discrete signals are than
handled in an identical way. Anomaly detection is performed by the comparison
of actual and predicted system behavior. The algorithm has been applied to few
data sets including two from real systems and has shown promising results.
</p>
<a href="http://arxiv.org/abs/2010.15415" target="_blank">arXiv:2010.15415</a> [<a href="http://arxiv.org/pdf/2010.15415" target="_blank">pdf</a>]

<h2>ProCAN: Progressive Growing Channel Attentive Non-Local Network for Lung Nodule Classification. (arXiv:2010.15417v1 [eess.IV])</h2>
<h3>Mundher Al-Shabi, Kelvin Shak, Maxine Tan</h3>
<p>Lung cancer classification in screening computed tomography (CT) scans is one
of the most crucial tasks for early detection of this disease. Many lives can
be saved if we are able to accurately classify malignant/ cancerous lung
nodules. Consequently, several deep learning based models have been proposed
recently to classify lung nodules as malignant or benign. Nevertheless, the
large variation in the size and heterogeneous appearance of the nodules makes
this task an extremely challenging one. We propose a new Progressive Growing
Channel Attentive Non-Local (ProCAN) network for lung nodule classification.
The proposed method addresses this challenge from three different aspects.
First, we enrich the Non-Local network by adding channel-wise attention
capability to it. Second, we apply Curriculum Learning principles, whereby we
first train our model on easy examples before hard/ difficult ones. Third, as
the classification task gets harder during the Curriculum learning, our model
is progressively grown to increase its capability of handling the task at hand.
We examined our proposed method on two different public datasets and compared
its performance with state-of-the-art methods in the literature. The results
show that the ProCAN model outperforms state-of-the-art methods and achieves an
AUC of 98.05% and accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we
conducted extensive ablation studies to analyze the contribution and effects of
each new component of our proposed method.
</p>
<a href="http://arxiv.org/abs/2010.15417" target="_blank">arXiv:2010.15417</a> [<a href="http://arxiv.org/pdf/2010.15417" target="_blank">pdf</a>]

<h2>Scalable Graph Neural Networks via Bidirectional Propagation. (arXiv:2010.15421v1 [cs.LG])</h2>
<h3>Ming Chen, Zhewei Wei, Bolin Ding, Yaliang Li, Ye Yuan, Xiaoyong Du, Ji-Rong Wen</h3>
<p>Graph Neural Networks (GNN) is an emerging field for learning on
non-Euclidean data. Recently, there has been increased interest in designing
GNN that scales to large graphs. Most existing methods use "graph sampling" or
"layer-wise sampling" techniques to reduce training time. However, these
methods still suffer from degrading performance and scalability problems when
applying to graphs with billions of edges. This paper presents GBP, a scalable
GNN that utilizes a localized bidirectional propagation process from both the
feature vectors and the training/testing nodes. Theoretical analysis shows that
GBP is the first method that achieves sub-linear time complexity for both the
precomputation and the training phases. An extensive empirical study
demonstrates that GBP achieves state-of-the-art performance with significantly
less training/testing time. Most notably, GBP can deliver superior performance
on a graph with over 60 million nodes and 1.8 billion edges in less than half
an hour on a single machine.
</p>
<a href="http://arxiv.org/abs/2010.15421" target="_blank">arXiv:2010.15421</a> [<a href="http://arxiv.org/pdf/2010.15421" target="_blank">pdf</a>]

<h2>Detection of asteroid trails in Hubble Space Telescope images using Deep Learning. (arXiv:2010.15425v1 [astro-ph.IM])</h2>
<h3>Andrei A. Parfeni, Laurentiu I. Caramete, Andreea M. Dobre, Nguyen Tran Bach</h3>
<p>We present an application of Deep Learning for the image recognition of
asteroid trails in single-exposure photos taken by the Hubble Space Telescope.
Using algorithms based on multi-layered deep Convolutional Neural Networks, we
report accuracies of above 80% on the validation set. Our project was motivated
by the Hubble Asteroid Hunter project on Zooniverse, which focused on
identifying these objects in order to localize and better characterize them. We
aim to demonstrate that Machine Learning techniques can be very useful in
trying to solve problems that are closely related to Astronomy and
Astrophysics, but that they are still not developed enough for very specific
tasks.
</p>
<a href="http://arxiv.org/abs/2010.15425" target="_blank">arXiv:2010.15425</a> [<a href="http://arxiv.org/pdf/2010.15425" target="_blank">pdf</a>]

<h2>Physics-informed deep learning for flow and deformation in poroelastic media. (arXiv:2010.15426v1 [cs.CE])</h2>
<h3>Yared W. Bekele</h3>
<p>A physics-informed neural network is presented for poroelastic problems with
coupled flow and deformation processes. The governing equilibrium and mass
balance equations are discussed and specific derivations for two-dimensional
cases are presented. A fully-connected deep neural network is used for
training. Barry and Mercer's source problem with time-dependent fluid
injection/extraction in an idealized poroelastic medium, which has an exact
analytical solution, is used as a numerical example. A random sample from the
analytical solution is used as training data and the performance of the model
is tested by predicting the solution on the entire domain after training. The
deep learning model predicts the horizontal and vertical deformations well
while the error in the predicted pore pressure predictions is slightly higher
because of the sparsity of the pore pressure values.
</p>
<a href="http://arxiv.org/abs/2010.15426" target="_blank">arXiv:2010.15426</a> [<a href="http://arxiv.org/pdf/2010.15426" target="_blank">pdf</a>]

<h2>Self-paced Data Augmentation for Training Neural Networks. (arXiv:2010.15434v1 [cs.LG])</h2>
<h3>Tomoumi Takase, Ryo Karakida, Hideki Asoh</h3>
<p>Data augmentation is widely used for machine learning; however, an effective
method to apply data augmentation has not been established even though it
includes several factors that should be tuned carefully. One such factor is
sample suitability, which involves selecting samples that are suitable for data
augmentation. A typical method that applies data augmentation to all training
samples disregards sample suitability, which may reduce classifier performance.
To address this problem, we propose the self-paced augmentation (SPA) to
automatically and dynamically select suitable samples for data augmentation
when training a neural network. The proposed method mitigates the deterioration
of generalization performance caused by ineffective data augmentation. We
discuss two reasons the proposed SPA works relative to curriculum learning and
desirable changes to loss function instability. Experimental results
demonstrate that the proposed SPA can improve the generalization performance,
particularly when the number of training samples is small. In addition, the
proposed SPA outperforms the state-of-the-art RandAugment method.
</p>
<a href="http://arxiv.org/abs/2010.15434" target="_blank">arXiv:2010.15434</a> [<a href="http://arxiv.org/pdf/2010.15434" target="_blank">pdf</a>]

<h2>FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements. (arXiv:2010.15440v1 [eess.IV])</h2>
<h3>Salman S. Khan, Varun Sundar, Vivek Boominathan, Ashok Veeraraghavan, Kaushik Mitra</h3>
<p>Lensless imaging has emerged as a potential solution towards realizing
ultra-miniature cameras by eschewing the bulky lens in a traditional camera.
Without a focusing lens, the lensless cameras rely on computational algorithms
to recover the scenes from multiplexed measurements. However, the current
iterative-optimization-based reconstruction algorithms produce noisier and
perceptually poorer images. In this work, we propose a non-iterative deep
learning based reconstruction approach that results in orders of magnitude
improvement in image quality for lensless reconstructions. Our approach, called
$\textit{FlatNet}$, lays down a framework for reconstructing high-quality
photorealistic images from mask-based lensless cameras, where the camera's
forward model formulation is known. FlatNet consists of two stages: (1) an
inversion stage that maps the measurement into a space of intermediate
reconstruction by learning parameters within the forward model formulation, and
(2) a perceptual enhancement stage that improves the perceptual quality of this
intermediate reconstruction. These stages are trained together in an end-to-end
manner. We show high-quality reconstructions by performing extensive
experiments on real and challenging scenes using two different types of
lensless prototypes: one which uses a separable forward model and another,
which uses a more general non-separable cropped-convolution model. Our
end-to-end approach is fast, produces photorealistic reconstructions, and is
easy to adopt for other mask-based lensless cameras.
</p>
<a href="http://arxiv.org/abs/2010.15440" target="_blank">arXiv:2010.15440</a> [<a href="http://arxiv.org/pdf/2010.15440" target="_blank">pdf</a>]

<h2>Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection. (arXiv:2010.15441v1 [cs.LG])</h2>
<h3>Divya Thekke Kanapram, Pablo Marin-Plaza, Lucio Marcenaro, David Martin, Arturo de la Escalera, Carlo Regazzoni</h3>
<p>The evolution of Intelligent Transportation Systems in recent times
necessitates the development of self-awareness in agents. Before the intensive
use of Machine Learning, the detection of abnormalities was manually programmed
by checking every variable and creating huge nested conditions that are very
difficult to track. This paper aims to introduce a novel method to develop
self-awareness in autonomous vehicles that mainly focuses on detecting abnormal
situations around the considered agents. Multi-sensory time-series data from
the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN)
models used for future state prediction and the detection of dynamic
abnormalities. Moreover, an initial level collective awareness model that can
perform joint anomaly detection in co-operative tasks is proposed. The GNG
algorithm learns the DBN models' discrete node variables; probabilistic
transition links connect the node variables. A Markov Jump Particle Filter
(MJPF) is applied to predict future states and detect when the vehicle is
potentially misbehaving using learned DBNs as filter parameters. In this paper,
datasets from real experiments of autonomous vehicles performing various tasks
used to learn and test a set of switching DBN models.
</p>
<a href="http://arxiv.org/abs/2010.15441" target="_blank">arXiv:2010.15441</a> [<a href="http://arxiv.org/pdf/2010.15441" target="_blank">pdf</a>]

<h2>Advanced Python Performance Monitoring with Score-P. (arXiv:2010.15444v1 [cs.DC])</h2>
<h3>Andreas Gocht, Robert Sch&#xf6;ne, Jan Frenzel</h3>
<p>Within the last years, Python became more prominent in the scientific
community and is now used for simulations, machine learning, and data analysis.
All these tasks profit from additional compute power offered by parallelism and
offloading. In the domain of High Performance Computing (HPC), we can look back
to decades of experience exploiting different levels of parallelism on the
core, node or inter-node level, as well as utilising accelerators. By using
performance analysis tools to investigate all these levels of parallelism, we
can tune applications for unprecedented performance. Unfortunately, standard
Python performance analysis tools cannot cope with highly parallel programs.
Since the development of such software is complex and error-prone, we
demonstrate an easy-to-use solution based on an existing tool infrastructure
for performance analysis. In this paper, we describe how to apply the
established instrumentation framework \scorep to trace Python applications. We
finish with a study of the overhead that users can expect for instrumenting
their applications.
</p>
<a href="http://arxiv.org/abs/2010.15444" target="_blank">arXiv:2010.15444</a> [<a href="http://arxiv.org/pdf/2010.15444" target="_blank">pdf</a>]

<h2>Scalable Federated Learning over Passive Optical Networks. (arXiv:2010.15454v1 [cs.NI])</h2>
<h3>Jun Li, Lei Chen, Jiajia Chen</h3>
<p>Two-step aggregation is introduced to facilitate scalable federated learning
(SFL) over passive optical networks (PONs). Results reveal that the SFL keeps
the required PON upstream bandwidth constant regardless of the number of
involved clients, while bringing ~10% learning accuracy improvement.
</p>
<a href="http://arxiv.org/abs/2010.15454" target="_blank">arXiv:2010.15454</a> [<a href="http://arxiv.org/pdf/2010.15454" target="_blank">pdf</a>]

<h2>Multilayer Clustered Graph Learning. (arXiv:2010.15456v1 [cs.LG])</h2>
<h3>Mireille El Gheche, Pascal Frossard</h3>
<p>Multilayer graphs are appealing mathematical tools for modeling multiple
types of relationship in the data. In this paper, we aim at analyzing
multilayer graphs by properly combining the information provided by individual
layers, while preserving the specific structure that allows us to eventually
identify communities or clusters that are crucial in the analysis of graph
data. To do so, we learn a clustered representative graph by solving an
optimization problem that involves a data fidelity term to the observed layers,
and a regularization pushing for a sparse and community-aware graph. We use the
contrastive loss as a data fidelity term, in order to properly aggregate the
observed layers into a representative graph. The regularization is based on a
measure of graph sparsification called "effective resistance", coupled with a
penalization of the first few eigenvalues of the representative graph Laplacian
matrix to favor the formation of communities. The proposed optimization problem
is nonconvex but fully differentiable, and thus can be solved via the projected
gradient method. Experiments show that our method leads to a significant
improvement w.r.t. state-of-the-art multilayer graph learning algorithms for
solving clustering problems.
</p>
<a href="http://arxiv.org/abs/2010.15456" target="_blank">arXiv:2010.15456</a> [<a href="http://arxiv.org/pdf/2010.15456" target="_blank">pdf</a>]

<h2>FiGLearn: Filter and Graph Learning using Optimal Transport. (arXiv:2010.15457v1 [cs.LG])</h2>
<h3>Matthias Minder, Zahra Farsijani, Dhruti Shah, Mireille El Gheche, Pascal Frossard</h3>
<p>In many applications, a dataset can be considered as a set of observed
signals that live on an unknown underlying graph structure. Some of these
signals may be seen as white noise that has been filtered on the graph topology
by a graph filter. Hence, the knowledge of the filter and the graph provides
valuable information about the underlying data generation process and the
complex interactions that arise in the dataset. We hence introduce a novel
graph signal processing framework for jointly learning the graph and its
generating filter from signal observations. We cast a new optimisation problem
that minimises the Wasserstein distance between the distribution of the signal
observations and the filtered signal distribution model. Our proposed method
outperforms state-of-the-art graph learning frameworks on synthetic data. We
then apply our method to a temperature anomaly dataset, and further show how
this framework can be used to infer missing values if only very little
information is available.
</p>
<a href="http://arxiv.org/abs/2010.15457" target="_blank">arXiv:2010.15457</a> [<a href="http://arxiv.org/pdf/2010.15457" target="_blank">pdf</a>]

<h2>Self-Supervised Video Representation Using Pretext-Contrastive Learning. (arXiv:2010.15464v1 [cs.CV])</h2>
<h3>Li Tao, Xueting Wang, Toshihiko Yamasaki</h3>
<p>Pretext tasks and contrastive learning have been successful in
self-supervised learning for video retrieval and recognition. In this study, we
analyze their optimization targets and utilize the hyper-sphere feature space
to explore the connections between them, indicating the compatibility and
consistency of these two different learning methods. Based on the analysis, we
propose a self-supervised training method, referred as Pretext-Contrastive
Learning (PCL), to learn video representations. Extensive experiments based on
different combinations of pretext task baselines and contrastive losses confirm
the strong agreement with their self-supervised learning targets, demonstrating
the effectiveness and the generality of PCL. The combination of pretext tasks
and contrastive losses showed significant improvements in both video retrieval
and recognition over the corresponding baselines. And we can also outperform
current state-of-the-art methods in the same manner. Further, our PCL is
flexible and can be applied to almost all existing pretext task methods.
</p>
<a href="http://arxiv.org/abs/2010.15464" target="_blank">arXiv:2010.15464</a> [<a href="http://arxiv.org/pdf/2010.15464" target="_blank">pdf</a>]

<h2>Emergence of Spatial Coordinates via Exploration. (arXiv:2010.15469v1 [cs.LG])</h2>
<h3>Alban Laflaqui&#xe8;re</h3>
<p>Spatial knowledge is a fundamental building block for the development of
advanced perceptive and cognitive abilities. Traditionally, in robotics, the
Euclidean (x,y,z) coordinate system and the agent's forward model are defined a
priori. We show that a naive agent can autonomously build an internal
coordinate system, with the same dimension and metric regularity as the
external space, simply by learning to predict the outcome of sensorimotor
transitions in a self-supervised way.
</p>
<a href="http://arxiv.org/abs/2010.15469" target="_blank">arXiv:2010.15469</a> [<a href="http://arxiv.org/pdf/2010.15469" target="_blank">pdf</a>]

<h2>Beyond cross-entropy: learning highly separable feature distributions for robust and accurate classification. (arXiv:2010.15487v1 [cs.CV])</h2>
<h3>Arslan Ali, Andrea Migliorati, Tiziano Bianchi, Enrico Magli</h3>
<p>Deep learning has shown outstanding performance in several applications
including image classification. However, deep classifiers are known to be
highly vulnerable to adversarial attacks, in that a minor perturbation of the
input can easily lead to an error. Providing robustness to adversarial attacks
is a very challenging task especially in problems involving a large number of
classes, as it typically comes at the expense of an accuracy decrease. In this
work, we propose the Gaussian class-conditional simplex (GCCS) loss: a novel
approach for training deep robust multiclass classifiers that provides
adversarial robustness while at the same time achieving or even surpassing the
classification accuracy of state-of-the-art methods. Differently from other
frameworks, the proposed method learns a mapping of the input classes onto
target distributions in a latent space such that the classes are linearly
separable. Instead of maximizing the likelihood of target labels for individual
samples, our objective function pushes the network to produce feature
distributions yielding high inter-class separation. The mean values of the
distributions are centered on the vertices of a simplex such that each class is
at the same distance from every other class. We show that the regularization of
the latent space based on our approach yields excellent classification accuracy
and inherently provides robustness to multiple adversarial attacks, both
targeted and untargeted, outperforming state-of-the-art approaches over
challenging datasets.
</p>
<a href="http://arxiv.org/abs/2010.15487" target="_blank">arXiv:2010.15487</a> [<a href="http://arxiv.org/pdf/2010.15487" target="_blank">pdf</a>]

<h2>"What, not how" -- Solving an under-actuated insertion task from scratch. (arXiv:2010.15492v1 [cs.RO])</h2>
<h3>Giulia Vezzani, Michael Neunert, Markus Wulfmeier, Rae Jeong, Thomas Lampe, Noah Siegel, Roland Hafner, Abbas Abdolmaleki, Martin Riedmiller, Francesco Nori</h3>
<p>Robot manipulation requires a complex set of skills that need to be carefully
combined and coordinated to solve a task. Yet, most ReinforcementLearning (RL)
approaches in robotics study tasks which actually consist only of a single
manipulation skill, such as grasping an object or inserting a pre-grasped
object. As a result the skill ('how' to solve the task) but not the actual goal
of a complete manipulation ('what' to solve) is specified. In contrast, we
study a complex manipulation goal that requires an agent to learn and combine
diverse manipulation skills. We propose a challenging, highly under-actuated
peg-in-hole task with a free, rotational asymmetrical peg, requiring a broad
range of manipulation skills. While correct peg (re-)orientation is a
requirement for successful insertion, there is no reward associated with it.
Hence an agent needs to understand this pre-condition and learn the skill to
fulfil it. The final insertion reward is sparse, allowing freedom in the
solution and leading to complex emerging behaviour not envisioned during the
task design. We tackle the problem in a multi-task RL framework using Scheduled
Auxiliary Control (SAC-X) combined with Regularized Hierarchical Policy
Optimization (RHPO) which successfully solves the task in simulation and from
scratch on a single robot where data is severely limited.
</p>
<a href="http://arxiv.org/abs/2010.15492" target="_blank">arXiv:2010.15492</a> [<a href="http://arxiv.org/pdf/2010.15492" target="_blank">pdf</a>]

<h2>Dynamic Resource-aware Corner Detection for Bio-inspired Vision Sensors. (arXiv:2010.15507v1 [cs.CV])</h2>
<h3>Sherif A.S. Mohamed, Jawad N. Yasin, Mohammad-hashem Haghbayan, Antonio Miele, Jukka Heikkonen, Hannu Tenhunen, Juha Plosila</h3>
<p>Event-based cameras are vision devices that transmit only brightness changes
with low latency and ultra-low power consumption. Such characteristics make
event-based cameras attractive in the field of localization and object tracking
in resource-constrained systems. Since the number of generated events in such
cameras is huge, the selection and filtering of the incoming events are
beneficial from both increasing the accuracy of the features and reducing the
computational load. In this paper, we present an algorithm to detect
asynchronous corners from a stream of events in real-time on embedded systems.
The algorithm is called the Three Layer Filtering-Harris or TLF-Harris
algorithm. The algorithm is based on an events' filtering strategy whose
purpose is 1) to increase the accuracy by deliberately eliminating some
incoming events, i.e., noise, and 2) to improve the real-time performance of
the system, i.e., preserving a constant throughput in terms of input events per
second, by discarding unnecessary events with a limited accuracy loss. An
approximation of the Harris algorithm, in turn, is used to exploit its
high-quality detection capability with a low-complexity implementation to
enable seamless real-time performance on embedded computing platforms. The
proposed algorithm is capable of selecting the best corner candidate among
neighbors and achieves an average execution time savings of 59 % compared with
the conventional Harris score. Moreover, our approach outperforms the competing
methods, such as eFAST, eHarris, and FA-Harris, in terms of real-time
performance, and surpasses Arc* in terms of accuracy.
</p>
<a href="http://arxiv.org/abs/2010.15507" target="_blank">arXiv:2010.15507</a> [<a href="http://arxiv.org/pdf/2010.15507" target="_blank">pdf</a>]

<h2>Night vision obstacle detection and avoidance based on Bio-Inspired Vision Sensors. (arXiv:2010.15509v1 [cs.CV])</h2>
<h3>Jawad N. Yasin, Sherif A.S. Mohamed, Mohammad-hashem Haghbayan, Jukka Heikkonen, Hannu Tenhunen, Muhammad Mehboob Yasin, Juha Plosila</h3>
<p>Moving towards autonomy, unmanned vehicles rely heavily on state-of-the-art
collision avoidance systems (CAS). However, the detection of obstacles
especially during night-time is still a challenging task since the lighting
conditions are not sufficient for traditional cameras to function properly.
Therefore, we exploit the powerful attributes of event-based cameras to perform
obstacle detection in low lighting conditions. Event cameras trigger events
asynchronously at high output temporal rate with high dynamic range of up to
120 $dB$. The algorithm filters background activity noise and extracts objects
using robust Hough transform technique. The depth of each detected object is
computed by triangulating 2D features extracted utilising LC-Harris. Finally,
asynchronous adaptive collision avoidance (AACA) algorithm is applied for
effective avoidance. Qualitative evaluation is compared using event-camera and
traditional camera.
</p>
<a href="http://arxiv.org/abs/2010.15509" target="_blank">arXiv:2010.15509</a> [<a href="http://arxiv.org/pdf/2010.15509" target="_blank">pdf</a>]

<h2>Asynchronous Corner Tracking Algorithm based on Lifetime of Events for DAVIS Cameras. (arXiv:2010.15510v1 [cs.CV])</h2>
<h3>Sherif A.S. Mohamed, Jawad N. Yasin, Mohammad-Hashem Haghbayan, Antonio Miele, Jukka Heikkonen, Hannu Tenhunen, Juha Plosila</h3>
<p>Event cameras, i.e., the Dynamic and Active-pixel Vision Sensor (DAVIS) ones,
capture the intensity changes in the scene and generates a stream of events in
an asynchronous fashion. The output rate of such cameras can reach up to 10
million events per second in high dynamic environments. DAVIS cameras use novel
vision sensors that mimic human eyes. Their attractive attributes, such as high
output rate, High Dynamic Range (HDR), and high pixel bandwidth, make them an
ideal solution for applications that require high-frequency tracking. Moreover,
applications that operate in challenging lighting scenarios can exploit the
high HDR of event cameras, i.e., 140 dB compared to 60 dB of traditional
cameras. In this paper, a novel asynchronous corner tracking method is proposed
that uses both events and intensity images captured by a DAVIS camera. The
Harris algorithm is used to extract features, i.e., frame-corners from
keyframes, i.e., intensity images. Afterward, a matching algorithm is used to
extract event-corners from the stream of events. Events are solely used to
perform asynchronous tracking until the next keyframe is captured. Neighboring
events, within a window size of 5x5 pixels around the event-corner, are used to
calculate the velocity and direction of extracted event-corners by fitting the
2D planar using a randomized Hough transform algorithm. Experimental evaluation
showed that our approach is able to update the location of the extracted
corners up to 100 times during the blind time of traditional cameras, i.e.,
between two consecutive intensity images.
</p>
<a href="http://arxiv.org/abs/2010.15510" target="_blank">arXiv:2010.15510</a> [<a href="http://arxiv.org/pdf/2010.15510" target="_blank">pdf</a>]

<h2>UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-noise Ratio Condition. (arXiv:2010.15521v1 [eess.AS])</h2>
<h3>Xiang Hao, Xiangdong Su, Zhiyu Wang, Hui Zhang, Batushiren</h3>
<p>Speech enhancement at extremely low signal-to-noise ratio (SNR) condition is
a very challenging problem and rarely investigated in previous works. This
paper proposes a robust speech enhancement approach (UNetGAN) based on U-Net
and generative adversarial learning to deal with this problem. This approach
consists of a generator network and a discriminator network, which operate
directly in the time domain. The generator network adopts a U-Net like
structure and employs dilated convolution in the bottleneck of it. We evaluate
the performance of the UNetGAN at low SNR conditions (up to -20dB) on the
public benchmark. The result demonstrates that it significantly improves the
speech quality and substantially outperforms the representative deep learning
models, including SEGAN, cGAN fo SE, Bidirectional LSTM using phase-sensitive
spectrum approximation cost function (PSA-BLSTM) and Wave-U-Net regarding
Short-Time Objective Intelligibility (STOI) and Perceptual evaluation of speech
quality (PESQ).
</p>
<a href="http://arxiv.org/abs/2010.15521" target="_blank">arXiv:2010.15521</a> [<a href="http://arxiv.org/pdf/2010.15521" target="_blank">pdf</a>]

<h2>A comparison of automatic multi-tissue segmentation methods of the human fetal brain using the FeTA Dataset. (arXiv:2010.15526v1 [eess.IV])</h2>
<h3>Kelly Payette, Priscille de Dumast, Hamza Kebiri, Ivan Ezhov, Johannes C. Paetzold, Suprosanna Shit, Asim Iqbal, Romesa Khan, Raimund Kottke, Patrice Grehten, Hui Ji, Levente Lanczi, Marianna Nagy, Monika Beresova, Thi Dao Nguyen, Giancarlo Natalucci, Theofanis Karayannis, Bjoern Menze, Meritxell Bach Cuadra, Andras Jakab</h3>
<p>It is critical to quantitatively analyse the developing human fetal brain in
order to fully understand neurodevelopment in both normal fetuses and those
with congenital disorders. To facilitate this analysis, automatic multi-tissue
fetal brain segmentation algorithms are needed, which in turn requires open
databases of segmented fetal brains. Here we introduce a publicly available
database of 50 manually segmented pathological and non-pathological fetal
magnetic resonance brain volume reconstructions across a range of gestational
ages (20 to 33 weeks) into 7 different tissue categories (external
cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep
grey matter, brainstem/spinal cord). In addition, we quantitatively evaluate
the accuracy of several automatic multi-tissue segmentation algorithms of the
developing human fetal brain. Four research groups participated, submitting a
total of 10 algorithms, demonstrating the benefits the database for the
development of automatic algorithms.
</p>
<a href="http://arxiv.org/abs/2010.15526" target="_blank">arXiv:2010.15526</a> [<a href="http://arxiv.org/pdf/2010.15526" target="_blank">pdf</a>]

<h2>On the robustness of kernel-based pairwise learning. (arXiv:2010.15527v1 [stat.ML])</h2>
<h3>Patrick Gensler, Andreas Christmann</h3>
<p>It is shown that many results on the statistical robustness of kernel-based
pairwise learning can be derived under basically no assumptions on the input
and output spaces. In particular neither moment conditions on the conditional
distribution of Y given X = x nor the boundedness of the output space is
needed. We obtain results on the existence and boundedness of the influence
function and show qualitative robustness of the kernel-based estimator. The
present paper generalizes results by Christmann and Zhou (2016) by allowing the
prediction function to take two arguments and can thus be applied in a variety
of situations such as ranking.
</p>
<a href="http://arxiv.org/abs/2010.15527" target="_blank">arXiv:2010.15527</a> [<a href="http://arxiv.org/pdf/2010.15527" target="_blank">pdf</a>]

<h2>An End to End Network Architecture for Fundamental Matrix Estimation. (arXiv:2010.15528v1 [cs.CV])</h2>
<h3>Yesheng Zhang, Xu Zhao, Dahong Qian</h3>
<p>In this paper, we present a novel end-to-end network architecture to estimate
fundamental matrix directly from stereo images. To establish a complete working
pipeline, different deep neural networks in charge of finding correspondences
in images, performing outlier rejection and calculating fundamental matrix, are
integrated into an end-to-end network architecture.

To well train the network and preserve geometry properties of fundamental
matrix, a new loss function is introduced. To evaluate the accuracy of
estimated fundamental matrix more reasonably, we design a new evaluation metric
which is highly consistent with visualization result. Experiments conducted on
both outdoor and indoor data-sets show that this network outperforms
traditional methods as well as previous deep learning based methods on various
metrics and achieves significant performance improvements.
</p>
<a href="http://arxiv.org/abs/2010.15528" target="_blank">arXiv:2010.15528</a> [<a href="http://arxiv.org/pdf/2010.15528" target="_blank">pdf</a>]

<h2>How do Offline Measures for Exploration in Reinforcement Learning behave?. (arXiv:2010.15533v1 [cs.LG])</h2>
<h3>Jakob J. Hollenstein, Sayantan Auddy, Matteo Saveriano, Erwan Renaudo, Justus Piater</h3>
<p>Sufficient exploration is paramount for the success of a reinforcement
learning agent. Yet, exploration is rarely assessed in an algorithm-independent
way. We compare the behavior of three data-based, offline exploration metrics
described in the literature on intuitive simple distributions and highlight
problems to be aware of when using them. We propose a fourth metric,uniform
relative entropy, and implement it using either a k-nearest-neighbor or a
nearest-neighbor-ratio estimator, highlighting that the implementation choices
have a profound impact on these measures.
</p>
<a href="http://arxiv.org/abs/2010.15533" target="_blank">arXiv:2010.15533</a> [<a href="http://arxiv.org/pdf/2010.15533" target="_blank">pdf</a>]

<h2>Matern Gaussian Processes on Graphs. (arXiv:2010.15538v1 [stat.ML])</h2>
<h3>Viacheslav Borovitskiy, Iskander Azangulov, Alexander Terenin, Peter Mostowsky, Marc Peter Deisenroth, Nicolas Durrande</h3>
<p>Gaussian processes are a versatile framework for learning unknown functions
in a manner that permits one to utilize prior information about their
properties. Although many different Gaussian process models are readily
available when the input space is Euclidean, the choice is much more limited
for Gaussian processes whose input space is an undirected graph. In this work,
we leverage the stochastic partial differential equation characterization of
Mat\'{e}rn Gaussian processes - a widely-used model class in the Euclidean
setting - to study their analog for undirected graphs. We show that the
resulting Gaussian processes inherit various attractive properties of their
Euclidean and Riemannian analogs and provide techniques that allow them to be
trained using standard methods, such as inducing points. This enables graph
Mat\'{e}rn Gaussian processes to be employed in mini-batch and non-conjugate
settings, thereby making them more accessible to practitioners and easier to
deploy within larger learning frameworks.
</p>
<a href="http://arxiv.org/abs/2010.15538" target="_blank">arXiv:2010.15538</a> [<a href="http://arxiv.org/pdf/2010.15538" target="_blank">pdf</a>]

<h2>Multi-Constitutive Neural Network for Large Deformation Poromechanics Problem. (arXiv:2010.15549v1 [cs.LG])</h2>
<h3>Qi Zhang, Yilin Chen, Ziyi Yang, Eric Darve</h3>
<p>In this paper, we study the problem of large-strain consolidation in
poromechanics with deep neural networks. Given different material properties
and different loading conditions, the goal is to predict pore pressure and
settlement. We propose a novel method "multi-constitutive neural network"
(MCNN) such that one model can solve several different constitutive laws. We
introduce a one-hot encoding vector as an additional input vector, which is
used to label the constitutive law we wish to solve. Then we build a DNN which
takes as input (X, t) along with a constitutive model label and outputs the
corresponding solution. It is the first time, to our knowledge, that we can
evaluate multi-constitutive laws through only one training process while still
obtaining good accuracies. We found that MCNN trained to solve multiple PDEs
outperforms individual neural network solvers trained with PDE.
</p>
<a href="http://arxiv.org/abs/2010.15549" target="_blank">arXiv:2010.15549</a> [<a href="http://arxiv.org/pdf/2010.15549" target="_blank">pdf</a>]

<h2>ADABOOK & MULTIBOOK: Adaptive Boosting with Chance Correction. (arXiv:2010.15550v1 [cs.LG])</h2>
<h3>David M. W. Powers</h3>
<p>There has been considerable interest in boosting and bagging, including the
combination of the adaptive techniques of AdaBoost with the random selection
with replacement techniques of Bagging. At the same time there has been a
revisiting of the way we evaluate, with chance-corrected measures like Kappa,
Informedness, Correlation or ROC AUC being advocated. This leads to the
question of whether learning algorithms can do better by optimizing an
appropriate chance corrected measure. Indeed, it is possible for a weak learner
to optimize Accuracy to the detriment of the more reaslistic chance-corrected
measures, and when this happens the booster can give up too early. This
phenomenon is known to occur with conventional Accuracy-based AdaBoost, and the
MultiBoost algorithm has been developed to overcome such problems using restart
techniques based on bagging. This paper thus complements the theoretical work
showing the necessity of using chance-corrected measures for evaluation, with
empirical work showing how use of a chance-corrected measure can improve
boosting. We show that the early surrender problem occurs in MultiBoost too, in
multiclass situations, so that chance-corrected AdaBook and Multibook can beat
standard Multiboost or AdaBoost, and we further identify which chance-corrected
measures to use when.
</p>
<a href="http://arxiv.org/abs/2010.15550" target="_blank">arXiv:2010.15550</a> [<a href="http://arxiv.org/pdf/2010.15550" target="_blank">pdf</a>]

<h2>Investigating the Robustness of Artificial Intelligent Algorithms with Mixture Experiments. (arXiv:2010.15551v1 [stat.ML])</h2>
<h3>Jiayi Lian, Laura Freeman, Yili Hong, Xinwei Deng</h3>
<p>Artificial intelligent (AI) algorithms, such as deep learning and XGboost,
are used in numerous applications including computer vision, autonomous
driving, and medical diagnostics. The robustness of these AI algorithms is of
great interest as inaccurate prediction could result in safety concerns and
limit the adoption of AI systems. In this paper, we propose a framework based
on design of experiments to systematically investigate the robustness of AI
classification algorithms. A robust classification algorithm is expected to
have high accuracy and low variability under different application scenarios.
The robustness can be affected by a wide range of factors such as the imbalance
of class labels in the training dataset, the chosen prediction algorithm, the
chosen dataset of the application, and a change of distribution in the training
and test datasets. To investigate the robustness of AI classification
algorithms, we conduct a comprehensive set of mixture experiments to collect
prediction performance results. Then statistical analyses are conducted to
understand how various factors affect the robustness of AI classification
algorithms. We summarize our findings and provide suggestions to practitioners
in AI applications.
</p>
<a href="http://arxiv.org/abs/2010.15551" target="_blank">arXiv:2010.15551</a> [<a href="http://arxiv.org/pdf/2010.15551" target="_blank">pdf</a>]

<h2>Modulation Pattern Detection Using Complex Convolutions in Deep Learning. (arXiv:2010.15556v1 [cs.LG])</h2>
<h3>Jakob Krzyston, Rajib Bhattacharjea, Andrew Stark</h3>
<p>Transceivers used for telecommunications transmit and receive specific
modulation patterns that are represented as sequences of complex numbers.
Classifying modulation patterns is challenging because noise and channel
impairments affect the signals in complicated ways such that the received
signal bears little resemblance to the transmitted signal. Although deep
learning approaches have shown great promise over statistical methods in this
problem space, deep learning frameworks continue to lag in support for
complex-valued data. To address this gap, we study the implementation and use
of complex convolutions in a series of convolutional neural network
architectures. Replacement of data structure and convolution operations by
their complex generalization in an architecture improves performance, with
statistical significance, at recognizing modulation patterns in complex-valued
signals with high SNR after being trained on low SNR signals. This suggests
complex-valued convolutions enables networks to learn more meaningful
representations. We investigate this hypothesis by comparing the features
learned in each experiment by visualizing the inputs that results in one-hot
modulation pattern classification for each network.
</p>
<a href="http://arxiv.org/abs/2010.15556" target="_blank">arXiv:2010.15556</a> [<a href="http://arxiv.org/pdf/2010.15556" target="_blank">pdf</a>]

<h2>Genetic U-Net: Automatically Designing Lightweight U-shaped CNN Architectures Using the Genetic Algorithm for Retinal Vessel Segmentation. (arXiv:2010.15560v1 [eess.IV])</h2>
<h3>Jiahong Wei, Zhun Fan</h3>
<p>Many previous works based on deep learning for retinal vessel segmentation
have achieved promising performance by manually designing U-shaped
convolutional neural networks (CNNs). However, the manual design of these CNNs
is time-consuming and requires extensive empirical knowledge. To address this
problem, we propose a novel method using genetic algorithms (GAs) to
automatically design a lightweight U-shaped CNN for retinal vessel
segmentation, called Genetic U-Net. Here we first design a special search space
containing the structure of U-Net and its corresponding operations, and then
use genetic algorithm to search for superior architectures in this search
space. Experimental results show that the proposed method outperforms the
existing methods on three public datasets, DRIVE, CHASE_DB1 and STARE. In
addition, the architectures obtained by the proposed method are more
lightweight but robust than the state-of-the-art models.
</p>
<a href="http://arxiv.org/abs/2010.15560" target="_blank">arXiv:2010.15560</a> [<a href="http://arxiv.org/pdf/2010.15560" target="_blank">pdf</a>]

<h2>Federated Transfer Learning: concept and applications. (arXiv:2010.15561v1 [cs.LG])</h2>
<h3>Sudipan Saha, Tahir Ahmad</h3>
<p>Development of Artificial Intelligence (AI) is inherently tied to the
development of data. However, in most industries data exists in form of
isolated islands, with limited scope of sharing between different
organizations. This is an hindrance to the further development of AI. Federated
learning has emerged as a possible solution to this problem in the last few
years without compromising user privacy. Among different variants of the
federated learning, noteworthy is federated transfer learning (FTL) that allows
knowledge to be transferred across domains that do not have many overlapping
features and users. In this work we provide a comprehensive survey of the
existing works on this topic. In more details, we study the background of FTL
and its different existing applications. We further analyze FTL from privacy
and machine learning perspective.
</p>
<a href="http://arxiv.org/abs/2010.15561" target="_blank">arXiv:2010.15561</a> [<a href="http://arxiv.org/pdf/2010.15561" target="_blank">pdf</a>]

<h2>Overcoming The Limitations of Neural Networks in Composite-Pattern Learning with Architopes. (arXiv:2010.15571v1 [cs.NE])</h2>
<h3>Anastasis Kratsios, Behnoosh Zamanlooy</h3>
<p>The effectiveness of neural networks in solving complex problems is well
recognized; however, little is known about their limitations. We demonstrate
that the feed-forward architecture, for most commonly used activation
functions, is incapable of approximating functions comprised of multiple
sub-patterns while simultaneously respecting their composite-pattern structure.
We overcome this bottleneck with a simple architecture modification that
reallocates the neurons of any single feed-forward network across several
smaller sub-networks, each specialized on a distinct part of the input-space.
The modified architecture, called an Architope, is more expressive on two
fronts. First, it is dense in an associated space of piecewise continuous
functions in which the feed-forward architecture is not dense. Second, it
achieves the same approximation rate as the feed-forward networks while only
requiring $\mathscr{O}(N^{-1})$ fewer parameters in its hidden layers.
Moreover, the architecture achieves these approximation improvements while
preserving the target's composite-pattern structure.
</p>
<a href="http://arxiv.org/abs/2010.15571" target="_blank">arXiv:2010.15571</a> [<a href="http://arxiv.org/pdf/2010.15571" target="_blank">pdf</a>]

<h2>Import test questions into Moodle LMS. (arXiv:2010.15577v1 [cs.CY])</h2>
<h3>Iryna S. Mintii, Svitlana V. Shokaliuk, Tetiana A. Vakaliuk, Mykhailo M. Mintii, Vladimir N. Soloviev</h3>
<p>The purpose of the study is to highlight the theoretical and methodological
aspects of preparing the test questions of the most common types in the form of
text files for further import into learning management system (LMS) Moodle. The
subject of the research is the automated filling of the Moodle LMS test
database. The objectives of the study: to analyze the import files of test
questions, their advantages and disadvantages; to develop guidelines for the
preparation of test questions of common types in the form of text files for
further import into Moodle LMS. The action algorithms for importing questions
and instructions for submitting question files in such formats as Aiken, GIFT,
Moodle XML, "True/False" questions, "Multiple Choice" (one of many and many of
many), "Matching", with an open answer - "Numerical" or "Short answer" and
"Essay" are offered in this article. The formats for submitting questions,
examples of its designing and developed questions were demonstrated in view
mode in Moodle LMS.
</p>
<a href="http://arxiv.org/abs/2010.15577" target="_blank">arXiv:2010.15577</a> [<a href="http://arxiv.org/pdf/2010.15577" target="_blank">pdf</a>]

<h2>Modeling biomedical breathing signals with convolutional deep probabilistic autoencoders. (arXiv:2010.15579v1 [cs.LG])</h2>
<h3>Oscar Pastor-Serrano, Danny Lathouwers, Zolt&#xe1;n Perk&#xf3;</h3>
<p>One of the main problems with biomedical signals is the limited amount of
patient-specific data and the significant amount of time needed to record a
sufficient number of samples for diagnostic and treatment purposes. We explore
the use of Variational Autoencoder (VAE) and Adversarial Autoencoder (AAE)
algorithms based on one-dimensional convolutional neural networks in order to
build generative models able to capture and represent the variability of a set
of unlabeled quasi-periodic signals using as few as 10 parameters. Furthermore,
we introduce a modified AAE architecture that allows simultaneous
semi-supervised classification and generation of different types of signals.
Our study is based on physical breathing signals, i.e. time series describing
the position of chest markers, generally used to describe respiratory motion.
The time series are discretized into a vector of periods, with each period
containing 6 time and position values. These vectors can be transformed back
into time series through an additional reconstruction neural network and allow
to generate extended signals while simplifying the modeling task. The obtained
models can be used to generate realistic breathing realizations from patient or
population data and to classify new recordings. We show that by incorporating
the labels from around 10-15\% of the dataset during training, the model can be
guided to group data according to the patient it belongs to, or based on the
presence of different types of breathing irregularities such as baseline
shifts. Our specific motivation is to model breathing motion during
radiotherapy lung cancer treatments, for which the developed model serves as an
efficient tool to robustify plans against breathing uncertainties. However, the
same methodology can in principle be applied to any other kind of
quasi-periodic biomedical signal, representing a generically applicable tool.
</p>
<a href="http://arxiv.org/abs/2010.15579" target="_blank">arXiv:2010.15579</a> [<a href="http://arxiv.org/pdf/2010.15579" target="_blank">pdf</a>]

<h2>The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research. (arXiv:2010.15581v1 [cs.CY])</h2>
<h3>Nur Ahmed, Muntasir Wahed</h3>
<p>Increasingly, modern Artificial Intelligence (AI) research has become more
computationally intensive. However, a growing concern is that due to unequal
access to computing power, only certain firms and elite universities have
advantages in modern AI research. Using a novel dataset of 171394 papers from
57 prestigious computer science conferences, we document that firms, in
particular, large technology firms and elite universities have increased
participation in major AI conferences since deep learning's unanticipated rise
in 2012. The effect is concentrated among elite universities, which are ranked
1-50 in the QS World University Rankings. Further, we find two strategies
through which firms increased their presence in AI research: first, they have
increased firm-only publications; and second, firms are collaborating primarily
with elite universities. Consequently, this increased presence of firms and
elite universities in AI research has crowded out mid-tier (QS ranked 201-300)
and lower-tier (QS ranked 301-500) universities. To provide causal evidence
that deep learning's unanticipated rise resulted in this divergence, we
leverage the generalized synthetic control method, a data-driven counterfactual
estimator. Using machine learning based text analysis methods, we provide
additional evidence that the divergence between these two groups - large firms
and non-elite universities - is driven by access to computing power or compute,
which we term as the "compute divide". This compute divide between large firms
and non-elite universities increases concerns around bias and fairness within
AI technology, and presents an obstacle towards "democratizing" AI. These
results suggest that a lack of access to specialized equipment such as compute
can de-democratize knowledge production.
</p>
<a href="http://arxiv.org/abs/2010.15581" target="_blank">arXiv:2010.15581</a> [<a href="http://arxiv.org/pdf/2010.15581" target="_blank">pdf</a>]

<h2>Improving Accuracy of Federated Learning in Non-IID Settings. (arXiv:2010.15582v1 [cs.LG])</h2>
<h3>Mustafa Safa Ozdayi, Murat Kantarcioglu, Rishabh Iyer</h3>
<p>Federated Learning (FL) is a decentralized machine learning protocol that
allows a set of participating agents to collaboratively train a model without
sharing their data. This makes FL particularly suitable for settings where data
privacy is desired. However, it has been observed that the performance of FL is
closely tied with the local data distributions of agents. Particularly, in
settings where local data distributions vastly differ among agents, FL performs
rather poorly with respect to the centralized training. To address this
problem, we hypothesize the reasons behind the performance degradation, and
develop some techniques to address these reasons accordingly. In this work, we
identify four simple techniques that can improve the performance of trained
models without incurring any additional communication overhead to FL, but
rather, some light computation overhead either on the client, or the
server-side. In our experimental analysis, combination of our techniques
improved the validation accuracy of a model trained via FL by more than 12%
with respect to our baseline. This is about 5% less than the accuracy of the
model trained on centralized data.
</p>
<a href="http://arxiv.org/abs/2010.15582" target="_blank">arXiv:2010.15582</a> [<a href="http://arxiv.org/pdf/2010.15582" target="_blank">pdf</a>]

<h2>Future Directions of the Cyberinfrastructure for Sustained Scientific Innovation (CSSI) Program. (arXiv:2010.15584v1 [cs.CY])</h2>
<h3>Ritu Arora (1), Xiaosong Li (2), Bonnie Hurwitz (3), Daniel Fay (4), Dhabaleswar K. Panda (5), Edward Valeev (6), Shaowen Wang (7), Shirley Moore (8), Sunita Chandrasekaran (9), Ting Cao (2), Holly Bik (10), Matthew Curry (11), Tanzima Islam (12) ((1) Texas Advanced Computing Center, (2) University of Washington, (3) University of Arizona, (4) Microsoft, (5) The Ohio State University, (6) Virginia Tech University, (7) University of Illinois, (8) Oak Ridge National Lab, (9) University of Delaware, (10) University of California, Riverside, (11) Sandia National Lab, (12) Texas State University)</h3>
<p>The CSSI 2019 workshop was held on October 28-29, 2019, in Austin, Texas. The
main objectives of this workshop were to (1) understand the impact of the CSSI
program on the community over the last 9 years, (2) engage workshop
participants in identifying gaps and opportunities in the current CSSI
landscape, (3) gather ideas on the cyberinfrastructure needs and expectations
of the community with respect to the CSSI program, and (4) prepare a report
summarizing the feedback gathered from the community that can inform the future
solicitations of the CSSI program. The workshop brought together different
stakeholders interested in provisioning sustainable cyberinfrastructure that
can power discoveries impacting the various fields of science and technology
and maintaining the nation's competitiveness in the areas such as scientific
software, HPC, networking, cybersecurity, and data/information science. The
workshop served as a venue for gathering the community-feedback on the current
state of the CSSI program and its future directions.
</p>
<a href="http://arxiv.org/abs/2010.15584" target="_blank">arXiv:2010.15584</a> [<a href="http://arxiv.org/pdf/2010.15584" target="_blank">pdf</a>]

<h2>Panel: Economic Policy and Governance during Pandemics using AI. (arXiv:2010.15585v1 [cs.CY])</h2>
<h3>Feras A. Batarseh, Munisamy Gopinath</h3>
<p>The global food supply chain (starting at farms and ending with consumers)
has been seriously disrupted by many outlier events such as trade wars, the
China demand shock, natural disasters, and pandemics. Outlier events create
uncertainty along the entire supply chain in addition to intervening policy
responses to mitigate their adverse effects. Artificial Intelligence (AI)
methods (i.e. machine/reinforcement/deep learning) provide an opportunity to
better understand outcomes during outlier events by identifying regular,
irregular and contextual components. Employing AI can provide guidance to
decision making suppliers, farmers, processors, wholesalers, and retailers
along the supply chain, and policy makers to facilitate welfare-improving
outcomes. This panel discusses these issues.
</p>
<a href="http://arxiv.org/abs/2010.15585" target="_blank">arXiv:2010.15585</a> [<a href="http://arxiv.org/pdf/2010.15585" target="_blank">pdf</a>]

<h2>Event-Driven Learning of Systematic Behaviours in Stock Markets. (arXiv:2010.15586v1 [q-fin.ST])</h2>
<h3>Xianchao Wu</h3>
<p>It is reported that financial news, especially financial events expressed in
news, provide information to investors' long/short decisions and influence the
movements of stock markets. Motivated by this, we leverage financial event
streams to train a classification neural network that detects latent
event-stock linkages and stock markets' systematic behaviours in the U.S. stock
market. Our proposed pipeline includes (1) a combined event extraction method
that utilizes Open Information Extraction and neural co-reference resolution,
(2) a BERT/ALBERT enhanced representation of events, and (3) an extended
hierarchical attention network that includes attentions on event, news and
temporal levels. Our pipeline achieves significantly better accuracies and
higher simulated annualized returns than state-of-the-art models when being
applied to predicting Standard\&amp;Poor 500, Dow Jones, Nasdaq indices and 10
individual stocks.
</p>
<a href="http://arxiv.org/abs/2010.15586" target="_blank">arXiv:2010.15586</a> [<a href="http://arxiv.org/pdf/2010.15586" target="_blank">pdf</a>]

<h2>Enhancing reinforcement learning by a finite reward response filter with a case study in intelligent structural control. (arXiv:2010.15597v1 [cs.LG])</h2>
<h3>Hamid Radmard Rahmani, Carsten Koenke, Marco A. Wiering</h3>
<p>In many reinforcement learning (RL) problems, it takes some time until a
taken action by the agent reaches its maximum effect on the environment and
consequently the agent receives the reward corresponding to that action by a
delay called action-effect delay. Such delays reduce the performance of the
learning algorithm and increase the computational costs, as the reinforcement
learning agent values the immediate rewards more than the future reward that is
more related to the taken action. This paper addresses this issue by
introducing an applicable enhanced Q-learning method in which at the beginning
of the learning phase, the agent takes a single action and builds a function
that reflects the environments response to that action, called the reflexive
$\gamma$ - function. During the training phase, the agent utilizes the created
reflexive $\gamma$- function to update the Q-values. We have applied the
developed method to a structural control problem in which the goal of the agent
is to reduce the vibrations of a building subjected to earthquake excitations
with a specified delay. Seismic control problems are considered as a complex
task in structural engineering because of the stochastic and unpredictable
nature of earthquakes and the complex behavior of the structure. Three
scenarios are presented to study the effects of zero, medium, and long
action-effect delays and the performance of the Enhanced method is compared to
the standard Q-learning method. Both RL methods use neural network to learn to
estimate the state-action value function that is used to control the structure.
The results show that the enhanced method significantly outperforms the
performance of the original method in all cases, and also improves the
stability of the algorithm in dealing with action-effect delays.
</p>
<a href="http://arxiv.org/abs/2010.15597" target="_blank">arXiv:2010.15597</a> [<a href="http://arxiv.org/pdf/2010.15597" target="_blank">pdf</a>]

<h2>Using a Binary Classification Model to Predict the Likelihood of Enrolment to the Undergraduate Program of a Philippine University. (arXiv:2010.15601v1 [cs.CY])</h2>
<h3>Dr.Joseph A. Esquivel, Dr. James A. Esquivel</h3>
<p>With the recent implementation of the K to 12 Program, academic institutions,
specifically, Colleges and Universities in the Philippines have been faced with
difficulties in determining projected freshmen enrollees vis-a-vis
decision-making factors for efficient resource management. Enrollment targets
directly impacts success factors of Higher Education Institutions. This study
covered an analysis of various characteristics of freshmen applicants affecting
their admission status in a Philippine university. A predictive model was
developed using Logistic Regression to evaluate the probability that an
admitted student will pursue to enroll in the Institution or not. The dataset
used was acquired from the University Admissions Office. The office designed an
online application form to capture applicants' details. The online form was
distributed to all student applicants, and most often, students, tend to
provide incomplete information. Despite this fact, student characteristics, as
well as geographic and demographic data based on the students' location are
significant predictors of enrollment decision. The results of the study show
that given limited information about prospective students, Higher Education
Institutions can implement machine learning techniques to supplement management
decisions and provide estimates of class sizes, in this way, it will allow the
institution to optimize the allocation of resources and will have better
control over net tuition revenue.
</p>
<a href="http://arxiv.org/abs/2010.15601" target="_blank">arXiv:2010.15601</a> [<a href="http://arxiv.org/pdf/2010.15601" target="_blank">pdf</a>]

<h2>Designing learning experiences for online teaching and learning. (arXiv:2010.15602v1 [cs.CY])</h2>
<h3>Nachamma Sockalingam, Junhua Liu</h3>
<p>Teaching is about constantly innovating strategies, ways and means to engage
diverse students in active and meaningful learning. In line with this, SUTD
adopts various student-centric teaching and learning teaching methods and
approaches. This means that our graduate/undergraduate instructors have to be
ready to teach using these student student-centric teaching and learning
pedagogies. In this article, I share my experiences of redesigning this
teaching course that is typically conducted face-to-face to a synchronous
online course and also invite one of the participant in this course to reflect
on his experience as a student.
</p>
<a href="http://arxiv.org/abs/2010.15602" target="_blank">arXiv:2010.15602</a> [<a href="http://arxiv.org/pdf/2010.15602" target="_blank">pdf</a>]

<h2>Suppressing Mislabeled Data via Grouping and Self-Attention. (arXiv:2010.15603v1 [cs.CV])</h2>
<h3>Xiaojiang Peng, Kai Wang, Zhaoyang Zeng, Qing Li, Jianfei Yang, Yu Qiao</h3>
<p>Deep networks achieve excellent results on large-scale clean data but degrade
significantly when learning from noisy labels. To suppressing the impact of
mislabeled data, this paper proposes a conceptually simple yet efficient
training block, termed as Attentive Feature Mixup (AFM), which allows paying
more attention to clean samples and less to mislabeled ones via sample
interactions in small groups. Specifically, this plug-and-play AFM first
leverages a \textit{group-to-attend} module to construct groups and assign
attention weights for group-wise samples, and then uses a \textit{mixup} module
with the attention weights to interpolate massive noisy-suppressed samples. The
AFM has several appealing benefits for noise-robust deep learning. (i) It does
not rely on any assumptions and extra clean subset. (ii) With massive
interpolations, the ratio of useless samples is reduced dramatically compared
to the original noisy ratio. (iii) \pxj{It jointly optimizes the interpolation
weights with classifiers, suppressing the influence of mislabeled data via low
attention weights. (iv) It partially inherits the vicinal risk minimization of
mixup to alleviate over-fitting while improves it by sampling fewer
feature-target vectors around mislabeled data from the mixup vicinal
distribution.} Extensive experiments demonstrate that AFM yields
state-of-the-art results on two challenging real-world noisy datasets: Food101N
and Clothing1M. The code will be available at
https://github.com/kaiwang960112/AFM.
</p>
<a href="http://arxiv.org/abs/2010.15603" target="_blank">arXiv:2010.15603</a> [<a href="http://arxiv.org/pdf/2010.15603" target="_blank">pdf</a>]

<h2>Autoregressive Asymmetric Linear Gaussian Hidden Markov Models. (arXiv:2010.15604v1 [cs.LG])</h2>
<h3>Carlos Puerto-Santana, Pedro Larra&#xf1;aga, Concha Bielza</h3>
<p>In a real life process evolving over time, the relationship between its
relevant variables may change. Therefore, it is advantageous to have different
inference models for each state of the process. Asymmetric hidden Markov models
fulfil this dynamical requirement and provide a framework where the trend of
the process can be expressed as a latent variable. In this paper, we modify
these recent asymmetric hidden Markov models to have an asymmetric
autoregressive component, allowing the model to choose the order of
autoregression that maximizes its penalized likelihood for a given training
set. Additionally, we show how inference, hidden states decoding and parameter
learning must be adapted to fit the proposed model. Finally, we run experiments
with synthetic and real data to show the capabilities of this new model.
</p>
<a href="http://arxiv.org/abs/2010.15604" target="_blank">arXiv:2010.15604</a> [<a href="http://arxiv.org/pdf/2010.15604" target="_blank">pdf</a>]

<h2>Manifold learning-based feature extraction for structural defect reconstruction. (arXiv:2010.15605v1 [cs.CE])</h2>
<h3>Qi Li, Dianzi Liu, Zhenghua Qian</h3>
<p>Data-driven quantitative defect reconstructions using ultrasonic guided waves
has recently demonstrated great potential in the area of non-destructive
testing. In this paper, we develop an efficient deep learning-based defect
reconstruction framework, called NetInv, which recasts the inverse guided wave
scattering problem as a data-driven supervised learning progress that realizes
a mapping between reflection coefficients in wavenumber domain and defect
profiles in the spatial domain. The superiorities of the proposed NetInv over
conventional reconstruction methods for defect reconstruction have been
demonstrated by several examples. Results show that NetInv has the ability to
achieve the higher quality of defect profiles with remarkable efficiency and
provides valuable insight into the development of effective data driven
structural health monitoring and defect reconstruction using machine learning.
</p>
<a href="http://arxiv.org/abs/2010.15605" target="_blank">arXiv:2010.15605</a> [<a href="http://arxiv.org/pdf/2010.15605" target="_blank">pdf</a>]

<h2>An Overview Of 3D Object Detection. (arXiv:2010.15614v1 [cs.CV])</h2>
<h3>Yilin Wang, Jiayi Ye</h3>
<p>Point cloud 3D object detection has recently received major attention and
becomes an active research topic in 3D computer vision community. However,
recognizing 3D objects in LiDAR (Light Detection and Ranging) is still a
challenge due to the complexity of point clouds. Objects such as pedestrians,
cyclists, or traffic cones are usually represented by quite sparse points,
which makes the detection quite complex using only point cloud. In this
project, we propose a framework that uses both RGB and point cloud data to
perform multiclass object recognition. We use existing 2D detection models to
localize the region of interest (ROI) on the RGB image, followed by a pixel
mapping strategy in the point cloud, and finally, lift the initial 2D bounding
box to 3D space. We use the recently released nuScenes dataset---a large-scale
dataset contains many data formats---to training and evaluate our proposed
architecture.
</p>
<a href="http://arxiv.org/abs/2010.15614" target="_blank">arXiv:2010.15614</a> [<a href="http://arxiv.org/pdf/2010.15614" target="_blank">pdf</a>]

<h2>Abstract Value Iteration for Hierarchical Reinforcement Learning. (arXiv:2010.15638v1 [cs.LG])</h2>
<h3>Kishor Jothimurugan, Osbert Bastani, Rajeev Alur</h3>
<p>We propose a novel hierarchical reinforcement learning framework for control
with continuous state and action spaces. In our framework, the user specifies
subgoal regions which are subsets of states; then, we (i) learn options that
serve as transitions between these subgoal regions, and (ii) construct a
high-level plan in the resulting abstract decision process (ADP). A key
challenge is that the ADP may not be Markov, which we address by proposing two
algorithms for planning in the ADP. Our first algorithm is conservative,
allowing us to prove theoretical guarantees on its performance, which help
inform the design of subgoal regions. Our second algorithm is a practical one
that interweaves planning at the abstract level and learning at the concrete
level. In our experiments, we demonstrate that our approach outperforms
state-of-the-art hierarchical reinforcement learning algorithms on several
challenging benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.15638" target="_blank">arXiv:2010.15638</a> [<a href="http://arxiv.org/pdf/2010.15638" target="_blank">pdf</a>]

<h2>Teaching a GAN What Not to Learn. (arXiv:2010.15639v1 [stat.ML])</h2>
<h3>Siddarth Asokan, Chandra Sekhar Seelamantula</h3>
<p>Generative adversarial networks (GANs) were originally envisioned as
unsupervised generative models that learn to follow a target distribution.
Variants such as conditional GANs, auxiliary-classifier GANs (ACGANs) project
GANs on to supervised and semi-supervised learning frameworks by providing
labelled data and using multi-class discriminators. In this paper, we approach
the supervised GAN problem from a different perspective, one that is motivated
by the philosophy of the famous Persian poet Rumi who said, "The art of knowing
is knowing what to ignore." In the GAN framework, we not only provide the GAN
positive data that it must learn to model, but also present it with so-called
negative samples that it must learn to avoid - we call this "The Rumi
Framework." This formulation allows the discriminator to represent the
underlying target distribution better by learning to penalize generated samples
that are undesirable - we show that this capability accelerates the learning
process of the generator. We present a reformulation of the standard GAN (SGAN)
and least-squares GAN (LSGAN) within the Rumi setting. The advantage of the
reformulation is demonstrated by means of experiments conducted on MNIST,
Fashion MNIST, CelebA, and CIFAR-10 datasets. Finally, we consider an
application of the proposed formulation to address the important problem of
learning an under-represented class in an unbalanced dataset. The Rumi approach
results in substantially lower FID scores than the standard GAN frameworks
while possessing better generalization capability.
</p>
<a href="http://arxiv.org/abs/2010.15639" target="_blank">arXiv:2010.15639</a> [<a href="http://arxiv.org/pdf/2010.15639" target="_blank">pdf</a>]

<h2>Free-Form Image Inpainting via Contrastive Attention Network. (arXiv:2010.15643v1 [cs.CV])</h2>
<h3>Xin Ma, Xiaoqiang Zhou, Huaibo Huang, Zhenhua Chai, Xiaolin Wei, Ran He</h3>
<p>Most deep learning based image inpainting approaches adopt autoencoder or its
variants to fill missing regions in images. Encoders are usually utilized to
learn powerful representational spaces, which are important for dealing with
sophisticated learning tasks. Specifically, in image inpainting tasks, masks
with any shapes can appear anywhere in images (i.e., free-form masks) which
form complex patterns. It is difficult for encoders to capture such powerful
representations under this complex situation. To tackle this problem, we
propose a self-supervised Siamese inference network to improve the robustness
and generalization. It can encode contextual semantics from full resolution
images and obtain more discriminative representations. we further propose a
multi-scale decoder with a novel dual attention fusion module (DAF), which can
combine both the restored and known regions in a smooth way. This multi-scale
architecture is beneficial for decoding discriminative representations learned
by encoders into images layer by layer. In this way, unknown regions will be
filled naturally from outside to inside. Qualitative and quantitative
experiments on multiple datasets, including facial and natural datasets (i.e.,
Celeb-HQ, Pairs Street View, Places2 and ImageNet), demonstrate that our
proposed method outperforms state-of-the-art methods in generating high-quality
inpainting results.
</p>
<a href="http://arxiv.org/abs/2010.15643" target="_blank">arXiv:2010.15643</a> [<a href="http://arxiv.org/pdf/2010.15643" target="_blank">pdf</a>]

<h2>Reliable Graph Neural Networks via Robust Aggregation. (arXiv:2010.15651v1 [cs.LG])</h2>
<h3>Simon Geisler, Daniel Z&#xfc;gner, Stephan G&#xfc;nnemann</h3>
<p>Perturbations targeting the graph structure have proven to be extremely
effective in reducing the performance of Graph Neural Networks (GNNs), and
traditional defenses such as adversarial training do not seem to be able to
improve robustness. This work is motivated by the observation that
adversarially injected edges effectively can be viewed as additional samples to
a node's neighborhood aggregation function, which results in distorted
aggregations accumulating over the layers. Conventional GNN aggregation
functions, such as a sum or mean, can be distorted arbitrarily by a single
outlier. We propose a robust aggregation function motivated by the field of
robust statistics. Our approach exhibits the largest possible breakdown point
of 0.5, which means that the bias of the aggregation is bounded as long as the
fraction of adversarial edges of a node is less than 50\%. Our novel
aggregation function, Soft Medoid, is a fully differentiable generalization of
the Medoid and therefore lends itself well for end-to-end deep learning.
Equipping a GNN with our aggregation improves the robustness with respect to
structure perturbations on Cora ML by a factor of 3 (and 5.5 on Citeseer) and
by a factor of 8 for low-degree nodes.
</p>
<a href="http://arxiv.org/abs/2010.15651" target="_blank">arXiv:2010.15651</a> [<a href="http://arxiv.org/pdf/2010.15651" target="_blank">pdf</a>]

<h2>Semi-Supervised Speech Recognition via Graph-based Temporal Classification. (arXiv:2010.15653v1 [cs.LG])</h2>
<h3>Niko Moritz, Takaaki Hori, Jonathan Le Roux</h3>
<p>Semi-supervised learning has demonstrated promising results in automatic
speech recognition (ASR) by self-training using a seed ASR model with
pseudo-labels generated for unlabeled data. The effectiveness of this approach
largely relies on the pseudo-label accuracy, for which typically only the
1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best
list can provide more accurate labels for an unlabeled speech utterance and
also reflect uncertainties of the seed ASR model. In this paper, we propose a
generalized form of the connectionist temporal classification (CTC) objective
that accepts a graph representation of the training targets. The newly proposed
graph-based temporal classification (GTC) objective is applied for
self-training with WFST-based supervision, which is generated from an N-best
list of pseudo-labels. In this setup, GTC is used to learn not only a temporal
alignment, similarly to CTC, but also a label alignment to obtain the optimal
pseudo-label sequence from the weighted graph. Results show that this approach
can effectively exploit an N-best list of pseudo-labels with associated scores,
outperforming standard pseudo-labeling by a large margin, with ASR results
close to an oracle experiment in which the best hypotheses of the N-best lists
are selected manually.
</p>
<a href="http://arxiv.org/abs/2010.15653" target="_blank">arXiv:2010.15653</a> [<a href="http://arxiv.org/pdf/2010.15653" target="_blank">pdf</a>]

<h2>Identification of complex mixtures for Raman spectroscopy using a novel scheme based on a new multi-label deep neural network. (arXiv:2010.15654v1 [eess.SP])</h2>
<h3>Liangrui Pan, Pronthep Pipitsunthonsan, Chalongrat Daengngam, Mitchai Chongcheawchamnan</h3>
<p>With noisy environment caused by fluoresence and additive white noise as well
as complicated spectrum fingerprints, the identification of complex mixture
materials remains a major challenge in Raman spectroscopy application. In this
paper, we propose a new scheme based on a constant wavelet transform (CWT) and
a deep network for classifying complex mixture. The scheme first transforms the
noisy Raman spectrum to a two-dimensional scale map using CWT. A multi-label
deep neural network model (MDNN) is then applied for classifying material. The
proposed model accelerates the feature extraction and expands the feature graph
using the global averaging pooling layer. The Sigmoid function is implemented
in the last layer of the model. The MDNN model was trained, validated and
tested with data collected from the samples prepared from substances in palm
oil. During training and validating process, data augmentation is applied to
overcome the imbalance of data and enrich the diversity of Raman spectra. From
the test results, it is found that the MDNN model outperforms previously
proposed deep neural network models in terms of Hamming loss, one error,
coverage, ranking loss, average precision, F1 macro averaging and F1 micro
averaging, respectively. The average detection time obtained from our model is
5.31 s, which is much faster than the detection time of the previously proposed
models.
</p>
<a href="http://arxiv.org/abs/2010.15654" target="_blank">arXiv:2010.15654</a> [<a href="http://arxiv.org/pdf/2010.15654" target="_blank">pdf</a>]

<h2>Machine Learning Based Demand Modelling for On-Demand Transit Services: A Case Study of Belleville, Ontario. (arXiv:2010.15673v1 [cs.CY])</h2>
<h3>Nael Alsaleh, Bilal Farooq</h3>
<p>The use of mobile applications apps and GPS service on smartphones for
transportation management applications has enabled the new "on-demand mobility"
service, where the transportation supply is following the users' schedule and
routes. In September 2018, the City of Belleville in Canada and Pantonium
operationalized the same idea, but for the public transit service in the city
to develop an on-demand transit (ODT) service. An existing fixed route (RT 11)
public transit service was converted into an on-demand service during the night
as a pilot project to maintain a higher demand sensitivity and highest
operation cost efficiency per trip. In this study, Random Forest (RF), Bagging,
Artificial Neural Network (ANN), and Deep Neural Network (DNN) machine learning
algorithms were adopted to develop a pickup demand model (trip generation) and
a trip demand model (trip distribution model) for Belleville ODT service based
on the dissemination areas' demographic characteristics and the existing trip
characteristics. The developed models aim to explain the demand behavior,
investigate the main factors affecting the trip pattern and their relative
importance, and to predict the number of generated trips from any dissemination
area as well as between any two dissemination areas. The results indicate that
the developed models can predict 63% and 70% of the pickup and trip demand
levels, respectively. Both models are most affected by the month of the year
and the day of the week variables. In addition, the population density has a
higher impact on the ODT service pickup demand levels than the other
demographic characteristics followed by the working age percentages and median
income characteristics. Whereas, the distribution of the trips depends on the
demographic characteristics of the destination area more than the origin area.
</p>
<a href="http://arxiv.org/abs/2010.15673" target="_blank">arXiv:2010.15673</a> [<a href="http://arxiv.org/pdf/2010.15673" target="_blank">pdf</a>]

<h2>Deep DA for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos. (arXiv:2010.15675v1 [cs.CV])</h2>
<h3>Gnana Praveen R, Eric Granger, Patrick Cardinal</h3>
<p>Automatic estimation of pain intensity from facial expressions in videos has
an immense potential in health care applications. However, domain adaptation
(DA) is needed to alleviate the problem of domain shifts that typically occurs
between video data captured in source and target do-mains. Given the laborious
task of collecting and annotating videos, and the subjective bias due to
ambiguity among adjacent intensity levels, weakly-supervised learning (WSL)is
gaining attention in such applications. Yet, most state-of-the-art WSL models
are typically formulated as regression problems, and do not leverage the
ordinal relation between intensity levels, nor the temporal coherence of
multiple consecutive frames. This paper introduces a new deep learn-ing model
for weakly-supervised DA with ordinal regression(WSDA-OR), where videos in
target domain have coarse la-bels provided on a periodic basis. The WSDA-OR
model enforces ordinal relationships among the intensity levels as-signed to
the target sequences, and associates multiple relevant frames to sequence-level
labels (instead of a single frame). In particular, it learns discriminant and
domain-invariant feature representations by integrating multiple in-stance
learning with deep adversarial DA, where soft Gaussian labels are used to
efficiently represent the weak ordinal sequence-level labels from the target
domain. The proposed approach was validated on the RECOLA video dataset as
fully-labeled source domain, and UNBC-McMaster video data as weakly-labeled
target domain. We have also validated WSDA-OR on BIOVID and Fatigue (private)
datasets for sequence level estimation. Experimental results indicate that our
approach can provide a significant improvement over the state-of-the-art
models, allowing to achieve a greater localization accuracy.
</p>
<a href="http://arxiv.org/abs/2010.15675" target="_blank">arXiv:2010.15675</a> [<a href="http://arxiv.org/pdf/2010.15675" target="_blank">pdf</a>]

<h2>Deep Autofocus for Synthetic Aperture Sonar. (arXiv:2010.15687v1 [eess.IV])</h2>
<h3>Isaac Gerg, Vishal Monga</h3>
<p>Synthetic aperture sonar (SAS) requires precise positional and environmental
information to produce well-focused output during the image reconstruction
step. However, errors in these measurements are commonly present resulting in
defocused imagery. To overcome these issues, an \emph{autofocus} algorithm is
employed as a post-processing step after image reconstruction for the purpose
of improving image quality using the image content itself. These algorithms are
usually iterative and metric-based in that they seek to optimize an image
sharpness metric. In this letter, we demonstrate the potential of machine
learning, specifically deep learning, to address the autofocus problem. We
formulate the problem as a self-supervised, phase error estimation task using a
deep network we call Deep Autofocus. Our formulation has the advantages of
being non-iterative (and thus fast) and not requiring ground truth
focused-defocused images pairs as often required by other deblurring deep
learning methods. We compare our technique against a set of common sharpness
metrics optimized using gradient descent over a real-world dataset. Our results
demonstrate Deep Autofocus can produce imagery that is perceptually as good as
benchmark iterative techniques but at a substantially lower computational cost.
We conclude that our proposed Deep Autofocus can provide a more favorable
cost-quality trade-off than state-of-the-art alternatives with significant
potential of future research.
</p>
<a href="http://arxiv.org/abs/2010.15687" target="_blank">arXiv:2010.15687</a> [<a href="http://arxiv.org/pdf/2010.15687" target="_blank">pdf</a>]

<h2>Learning Deep Interleaved Networks with Asymmetric Co-Attention for Image Restoration. (arXiv:2010.15689v1 [cs.CV])</h2>
<h3>Feng Li, Runmin Cong, Huihui Bai, Yifan He, Yao Zhao, Ce Zhu</h3>
<p>Recently, convolutional neural network (CNN) has demonstrated significant
success for image restoration (IR) tasks (e.g., image super-resolution, image
deblurring, rain streak removal, and dehazing). However, existing CNN based
models are commonly implemented as a single-path stream to enrich feature
representations from low-quality (LQ) input space for final predictions, which
fail to fully incorporate preceding low-level contexts into later high-level
features within networks, thereby producing inferior results. In this paper, we
present a deep interleaved network (DIN) that learns how information at
different states should be combined for high-quality (HQ) images
reconstruction. The proposed DIN follows a multi-path and multi-branch pattern
allowing multiple interconnected branches to interleave and fuse at different
states. In this way, the shallow information can guide deep representative
features prediction to enhance the feature expression ability. Furthermore, we
propose asymmetric co-attention (AsyCA) which is attached at each interleaved
node to model the feature dependencies. Such AsyCA can not only adaptively
emphasize the informative features from different states, but also improves the
discriminative ability of networks. Our presented DIN can be trained end-to-end
and applied to various IR tasks. Comprehensive evaluations on public benchmarks
and real-world datasets demonstrate that the proposed DIN perform favorably
against the state-of-the-art methods quantitatively and qualitatively.
</p>
<a href="http://arxiv.org/abs/2010.15689" target="_blank">arXiv:2010.15689</a> [<a href="http://arxiv.org/pdf/2010.15689" target="_blank">pdf</a>]

<h2>Generalized Insider Attack Detection Implementation using NetFlow Data. (arXiv:2010.15697v1 [cs.CR])</h2>
<h3>Yash Samtani, Jesse Elwell</h3>
<p>Insider Attack Detection in commercial networks is a critical problem that
does not have any good solutions at this current time. The problem is
challenging due to the lack of visibility into live networks and a lack of a
standard feature set to distinguish between different attacks. In this paper,
we study an approach centered on using network data to identify attacks. Our
work builds on unsupervised machine learning techniques such as One-Class SVM
and bi-clustering as weak indicators of insider network attacks. We combine
these techniques to limit the number of false positives to an acceptable level
required for real-world deployments by using One-Class SVM to check for
anomalies detected by the proposed Bi-clustering algorithm. We present a
prototype implementation in Python and associated results for two different
real-world representative data sets. We show that our approach is a promising
tool for insider attack detection in realistic settings.
</p>
<a href="http://arxiv.org/abs/2010.15697" target="_blank">arXiv:2010.15697</a> [<a href="http://arxiv.org/pdf/2010.15697" target="_blank">pdf</a>]

<h2>Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks. (arXiv:2010.15703v1 [cs.CV])</h2>
<h3>Julieta Martinez, Jashan Shewakramani, Ting Wei Liu, Ioan Andrei B&#xe2;rsan, Wenyuan Zeng, Raquel Urtasun</h3>
<p>Compressing large neural networks is an important step for their deployment
in resource-constrained computational platforms. In this context, vector
quantization is an appealing framework that expresses multiple parameters using
a single code, and has recently achieved state-of-the-art network compression
on a range of core vision and natural language processing tasks. Key to the
success of vector quantization is deciding which parameter groups should be
compressed together. Previous work has relied on heuristics that group the
spatial dimension of individual convolutional filters, but a general solution
remains unaddressed. This is desirable for pointwise convolutions (which
dominate modern architectures), linear layers (which have no notion of spatial
dimension), and convolutions (when more than one filter is compressed to the
same codeword). In this paper we make the observation that the weights of two
adjacent layers can be permuted while expressing the same function. We then
establish a connection to rate-distortion theory and search for permutations
that result in networks that are easier to compress. Finally, we rely on an
annealed quantization algorithm to better compress the network and achieve
higher final accuracy. We show results on image classification, object
detection, and segmentation, reducing the gap with the uncompressed model by 40
to 70% with respect to the current state of the art.
</p>
<a href="http://arxiv.org/abs/2010.15703" target="_blank">arXiv:2010.15703</a> [<a href="http://arxiv.org/pdf/2010.15703" target="_blank">pdf</a>]

<h2>What can we learn from gradients?. (arXiv:2010.15718v1 [cs.CR])</h2>
<h3>Jia Qian, Lars Kai Hansen</h3>
<p>Recent work (\cite{zhu2019deep}) has shown that it is possible to reconstruct
the input (image) from the gradient of a neural network. In this paper, our aim
is to better understand the limits to reconstruction and to speed up image
reconstruction by imposing prior image information and improved initialization.
Firstly, we show that for the \textbf{non-linear} neural network,
gradient-based reconstruction approximates to solving a high-dimension
\textbf{linear} equations for both fully-connected neural network and
convolutional neural network. Exploring the theoretical limits of input
reconstruction, we show that a fully-connected neural network with a
\textbf{one} hidden node is enough to reconstruct a \textbf{single} input
image, regardless of the number of nodes in the output layer. Then we
generalize this result to a gradient averaged over mini-batches of size B. In
this case, the full mini-batch can be reconstructed in a fully-connected
network if the number of hidden units exceeds B. For a convolutional neural
network, the required number of filters in the first convolutional layer again
is decided by the batch size B, however, in this case, input width d and the
width after filter $d^{'}$ also play the role $h=(\frac{d}{d^{'}})^2BC$, where
C is channel number of input. Finally, we validate and underpin our theoretical
analysis on bio-medical data (fMRI, ECG signals, and cell images) and on
benchmark data (MNIST, CIFAR100, and face images).
</p>
<a href="http://arxiv.org/abs/2010.15718" target="_blank">arXiv:2010.15718</a> [<a href="http://arxiv.org/pdf/2010.15718" target="_blank">pdf</a>]

<h2>Attentive Clustering Processes. (arXiv:2010.15727v1 [stat.ML])</h2>
<h3>Ari Pakman, Yueqi Wang, Yoonho Lee, Pallab Basu, Juho Lee, Yee Whye Teh, Liam Paninski</h3>
<p>Amortized approaches to clustering have recently received renewed attention
thanks to novel objective functions that exploit the expressiveness of deep
learning models. In this work we revisit a recent proposal for fast amortized
probabilistic clustering, the Clusterwise Clustering Process (CCP), which
yields samples from the posterior distribution of cluster labels for sets of
arbitrary size using only O(K) forward network evaluations, where K is an
arbitrary number of clusters. While adequate in simple datasets, we show that
the model can severely underfit complex datasets, and hypothesize that this
limitation can be traced back to the implicit assumption that the probability
of a point joining a cluster is equally sensitive to all the points available
to join the same cluster. We propose an improved model, the Attentive
Clustering Process (ACP), that selectively pays more attention to relevant
points while preserving the invariance properties of the generative model. We
illustrate the advantages of the new model in applications to spike-sorting in
multi-electrode arrays and community discovery in networks. The latter case
combines the ACP model with graph convolutional networks, and to our knowledge
is the first deep learning model that handles an arbitrary number of
communities.
</p>
<a href="http://arxiv.org/abs/2010.15727" target="_blank">arXiv:2010.15727</a> [<a href="http://arxiv.org/pdf/2010.15727" target="_blank">pdf</a>]

<h2>Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v1 [cs.CL])</h2>
<h3>Hang Dong, V&#xed;ctor Su&#xe1;rez-Paniagua, William Whiteley, Honghan Wu</h3>
<p>Diagnostic or procedural coding of clinical notes aims to derive a coded
summary of disease-related information about patients. Such coding is usually
done manually in hospitals but could potentially be automated to improve the
efficiency and accuracy of medical coding. Recent studies on deep learning for
automated medical coding achieved promising performances. However, the
explainability of these models is usually poor, preventing them to be used
confidently in supporting clinical practice. Another limitation is that these
models mostly assume independence among labels, ignoring the complex
correlation among medical codes which can potentially be exploited to improve
the performance. We propose a Hierarchical Label-wise Attention Network (HLAN),
which aimed to interpret the model by quantifying importance (as attention
weights) of words and sentences related to each of the labels. Secondly, we
propose to enhance the major deep learning models with a label embedding (LE)
initialisation approach, which learns a dense, continuous vector representation
and then injects the representation into the final layers and the label-wise
attention layers in the models. We evaluated the methods using three settings
on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS
COVID-19 shielding codes. Experiments were conducted to compare HLAN and LE
initialisation to the state-of-the-art neural network based methods. HLAN
achieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and
comparable results on the NHS COVID-19 shielding code prediction to other
models. By highlighting the most salient words and sentences for each label,
HLAN showed more meaningful and comprehensive model interpretation compared to
its downgraded baselines and the CNN-based models. LE initialisation
consistently boosted most deep learning models for automated medical coding.
</p>
<a href="http://arxiv.org/abs/2010.15728" target="_blank">arXiv:2010.15728</a> [<a href="http://arxiv.org/pdf/2010.15728" target="_blank">pdf</a>]

<h2>Causal variables from reinforcement learning using generalized Bellman equations. (arXiv:2010.15745v1 [cs.LG])</h2>
<h3>Tue Herlau</h3>
<p>Many open problems in machine learning are intrinsically related to
causality, however, the use of causal analysis in machine learning is still in
its early stage. Within a general reinforcement learning setting, we consider
the problem of building a general reinforcement learning agent which uses
experience to construct a causal graph of the environment, and use this graph
to inform its policy. Our approach has three characteristics: First, we learn a
simple, coarse-grained causal graph, in which the variables reflect states at
many time instances, and the interventions happen at the level of policies,
rather than individual actions. Secondly, we use mediation analysis to obtain
an optimization target. By minimizing this target, we define the causal
variables. Thirdly, our approach relies on estimating conditional expectations
rather the familiar expected return from reinforcement learning, and we
therefore apply a generalization of Bellman's equations. We show the method can
learn a plausible causal graph in a grid-world environment, and the agent
obtains an improvement in performance when using the causally informed policy.
To our knowledge, this is the first attempt to apply causal analysis in a
reinforcement learning setting without strict restrictions on the number of
states. We have observed that mediation analysis provides a promising avenue
for transforming the problem of causal acquisition into one of cost-function
minimization, but importantly one which involves estimating conditional
expectations. This is a new challenge, and we think that causal reinforcement
learning will involve development methods suited for online estimation of such
conditional expectations. Finally, a benefit of our approach is the use of very
simple causal models, which are arguably a more natural model of human causal
understanding.
</p>
<a href="http://arxiv.org/abs/2010.15745" target="_blank">arXiv:2010.15745</a> [<a href="http://arxiv.org/pdf/2010.15745" target="_blank">pdf</a>]

<h2>Gaussian Process Bandit Optimization of theThermodynamic Variational Objective. (arXiv:2010.15750v1 [cs.LG])</h2>
<h3>Vu Nguyen, Vaden Masrani, Rob Brekelmans, Michael A. Osborne, Frank Wood</h3>
<p>Achieving the full promise of the Thermodynamic Variational Objective (TVO),a
recently proposed variational lower bound on the log evidence involving a
one-dimensional Riemann integral approximation, requires choosing a "schedule"
ofsorted discretization points. This paper introduces a bespoke Gaussian
processbandit optimization method for automatically choosing these points. Our
approach not only automates their one-time selection, but also dynamically
adaptstheir positions over the course of optimization, leading to improved
model learning and inference. We provide theoretical guarantees that our bandit
optimizationconverges to the regret-minimizing choice of integration points.
Empirical validation of our algorithm is provided in terms of improved learning
and inference inVariational Autoencoders and Sigmoid Belief Networks.
</p>
<a href="http://arxiv.org/abs/2010.15750" target="_blank">arXiv:2010.15750</a> [<a href="http://arxiv.org/pdf/2010.15750" target="_blank">pdf</a>]

<h2>A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. (arXiv:2010.15761v1 [physics.comp-ph])</h2>
<h3>Antonio Stanziola, Simon R. Arridge, Ben T. Cox, Bradley E. Treeby</h3>
<p>Transcranial ultrasound therapy is increasingly used for the non-invasive
treatment of brain disorders. However, conventional numerical wave solvers are
currently too computationally expensive to be used online during treatments to
predict the acoustic field passing through the skull (e.g., to account for
subject-specific dose and targeting variations). As a step towards real-time
predictions, in the current work, a fast iterative solver for the heterogeneous
Helmholtz equation in 2D is developed using a fully-learned optimizer. The
lightweight network architecture is based on a modified UNet that includes a
learned hidden state. The network is trained using a physics-based loss
function and a set of idealized sound speed distributions with fully
unsupervised training (no knowledge of the true solution is required). The
learned optimizer shows excellent performance on the test set, and is capable
of generalization well outside the training examples, including to much larger
computational domains, and more complex source and sound speed distributions,
for example, those derived from x-ray computed tomography images of the skull.
</p>
<a href="http://arxiv.org/abs/2010.15761" target="_blank">arXiv:2010.15761</a> [<a href="http://arxiv.org/pdf/2010.15761" target="_blank">pdf</a>]

<h2>WaveTransform: Crafting Adversarial Examples via Input Decomposition. (arXiv:2010.15773v1 [cs.CV])</h2>
<h3>Divyam Anshumaan, Akshay Agarwal, Mayank Vatsa, Richa Singh</h3>
<p>Frequency spectrum has played a significant role in learning unique and
discriminating features for object recognition. Both low and high frequency
information present in images have been extracted and learnt by a host of
representation learning techniques, including deep learning. Inspired by this
observation, we introduce a novel class of adversarial attacks, namely
`WaveTransform', that creates adversarial noise corresponding to low-frequency
and high-frequency subbands, separately (or in combination). The frequency
subbands are analyzed using wavelet decomposition; the subbands are corrupted
and then used to construct an adversarial example. Experiments are performed
using multiple databases and CNN models to establish the effectiveness of the
proposed WaveTransform attack and analyze the importance of a particular
frequency component. The robustness of the proposed attack is also evaluated
through its transferability and resiliency against a recent adversarial defense
algorithm. Experiments show that the proposed attack is effective against the
defense algorithm and is also transferable across CNNs.
</p>
<a href="http://arxiv.org/abs/2010.15773" target="_blank">arXiv:2010.15773</a> [<a href="http://arxiv.org/pdf/2010.15773" target="_blank">pdf</a>]

<h2>Understanding the Failure Modes of Out-of-Distribution Generalization. (arXiv:2010.15775v1 [cs.LG])</h2>
<h3>Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur</h3>
<p>Empirical studies suggest that machine learning models often rely on
features, such as the background, that may be spuriously correlated with the
label only during training time, resulting in poor accuracy during test-time.
In this work, we identify the fundamental factors that give rise to this
behavior, by explaining why models fail this way {\em even} in easy-to-learn
tasks where one would expect these models to succeed. In particular, through a
theoretical study of gradient-descent-trained linear classifiers on some
easy-to-learn tasks, we uncover two complementary failure modes. These modes
arise from how spurious correlations induce two kinds of skews in the data: one
geometric in nature, and another, statistical in nature. Finally, we construct
natural modifications of image classification datasets to understand when these
failure modes can arise in practice. We also design experiments to isolate the
two failure modes when training modern neural networks on these datasets.
</p>
<a href="http://arxiv.org/abs/2010.15775" target="_blank">arXiv:2010.15775</a> [<a href="http://arxiv.org/pdf/2010.15775" target="_blank">pdf</a>]

<h2>A Framework for Learning Predator-prey Agents from Simulation to Real World. (arXiv:2010.15792v1 [cs.RO])</h2>
<h3>Jiunhan Chen, Zhenyu Gao</h3>
<p>In this paper, we propose an evolutionary predatorprey robot system which can
be generally implemented from simulation to the real world. We design the
closed-loop robot system with camera and infrared sensors as inputs of
controller. Both the predators and prey are co-evolved by NeuroEvolution of
Augmenting Topologies (NEAT) to learn the expected behaviours. We design a
framework that integrate Gym of OpenAI, Robot Operating System (ROS), Gazebo.
In such a framework, users only need to focus on algorithms without being
worried about the detail of manipulating robots in both simulation and the real
world. Combining simulations, real-world evolution, and robustness analysis, it
can be applied to develop the solutions for the predator-prey tasks. For the
convenience of users, the source code and videos of the simulated and real
world are published on Github.
</p>
<a href="http://arxiv.org/abs/2010.15792" target="_blank">arXiv:2010.15792</a> [<a href="http://arxiv.org/pdf/2010.15792" target="_blank">pdf</a>]

<h2>Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search. (arXiv:2010.15821v1 [cs.CV])</h2>
<h3>Houwen Peng, Hao Du, Hongyuan Yu, Qi Li, Jing Liao, Jianlong Fu</h3>
<p>One-shot weight sharing methods have recently drawn great attention in neural
architecture search due to high efficiency and competitive performance.
However, weight sharing across models has an inherent deficiency, i.e.,
insufficient training of subnetworks in the hypernetwork. To alleviate this
problem, we present a simple yet effective architecture distillation method.
The central idea is that subnetworks can learn collaboratively and teach each
other throughout the training process, aiming to boost the convergence of
individual models. We introduce the concept of prioritized path, which refers
to the architecture candidates exhibiting superior performance during training.
Distilling knowledge from the prioritized paths is able to boost the training
of subnetworks. Since the prioritized paths are changed on the fly depending on
their performance and complexity, the final obtained paths are the cream of the
crop. We directly select the most promising one from the prioritized paths as
the final architecture, without using other complex search methods, such as
reinforcement learning or evolution algorithms. The experiments on ImageNet
verify such path distillation method can improve the convergence ratio and
performance of the hypernetwork, as well as boosting the training of
subnetworks. The discovered architectures achieve superior performance compared
to the recent MobileNetV3 and EfficientNet families under aligned settings.
Moreover, the experiments on object detection and more challenging search space
show the generality and robustness of the proposed method. Code and models are
available at https://github.com/microsoft/cream.git.
</p>
<a href="http://arxiv.org/abs/2010.15821" target="_blank">arXiv:2010.15821</a> [<a href="http://arxiv.org/pdf/2010.15821" target="_blank">pdf</a>]

<h2>Passport-aware Normalization for Deep Model Protection. (arXiv:2010.15824v1 [cs.CV])</h2>
<h3>Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu</h3>
<p>Despite tremendous success in many application scenarios, deep learning faces
serious intellectual property (IP) infringement threats. Considering the cost
of designing and training a good model, infringements will significantly
infringe the interests of the original model owner. Recently, many impressive
works have emerged for deep model IP protection. However, they either are
vulnerable to ambiguity attacks, or require changes in the target network
structure by replacing its original normalization layers and hence cause
significant performance drops. To this end, we propose a new passport-aware
normalization formulation, which is generally applicable to most existing
normalization layers and only needs to add another passport-aware branch for IP
protection. This new branch is jointly trained with the target model but
discarded in the inference stage. Therefore it causes no structure change in
the target model. Only when the model IP is suspected to be stolen by someone,
the private passport-aware branch is added back for ownership verification.
Through extensive experiments, we verify its effectiveness in both image and 3D
point recognition models. It is demonstrated to be robust not only to common
attack techniques like fine-tuning and model compression, but also to ambiguity
attacks. By further combining it with trigger-set based methods, both black-box
and white-box verification can be achieved for enhanced security of deep
learning models deployed in real systems. Code can be found at
https://github.com/ZJZAC/Passport-aware-Normalization.
</p>
<a href="http://arxiv.org/abs/2010.15824" target="_blank">arXiv:2010.15824</a> [<a href="http://arxiv.org/pdf/2010.15824" target="_blank">pdf</a>]

<h2>Proceedings 9th International Workshop on Theorem Proving Components for Educational Software. (arXiv:2010.15832v1 [cs.AI])</h2>
<h3>Pedro Quaresma (University of Coimbra, Portugal), Walther Neuper (JKU Johannes Kepler University, Linz, Austria), Jo&#xe3;o Marcos (UFRN, Brazil)</h3>
<p>The 9th International Workshop on Theorem-Proving Components for Educational
Software (ThEdu'20) was scheduled to happen on June 29 as a satellite of the
IJCAR-FSCD 2020 joint meeting, in Paris. The COVID-19 pandemic came by
surprise, though, and the main conference was virtualised. Fearing that an
online meeting would not allow our community to fully reproduce the usual
face-to-face networking opportunities of the ThEdu initiative, the Steering
Committee of ThEdu decided to cancel our workshop. Given that many of us had
already planned and worked for that moment, we decided that ThEdu'20 could
still live in the form of an EPTCS volume. The EPTCS concurred with us,
recognising this very singular situation, and accepted our proposal of
organising a special issue with papers submitted to ThEdu'20. An open call for
papers was then issued, and attracted five submissions, all of which have been
accepted by our reviewers, who produced three careful reports on each of the
contributions. The resulting revised papers are collected in the present
volume. We, the volume editors, hope that this collection of papers will help
further promoting the development of theorem-proving-based software, and that
it will collaborate to improve the mutual understanding between computer
mathematicians and stakeholders in education. With some luck, we would actually
expect that the very special circumstances set up by the worst sanitary crisis
in a century will happen to reinforce the need for the application of certified
components and of verification methods for the production of educational
software that would be available even when the traditional on-site learning
experiences turn out not to be recommendable.
</p>
<a href="http://arxiv.org/abs/2010.15832" target="_blank">arXiv:2010.15832</a> [<a href="http://arxiv.org/pdf/2010.15832" target="_blank">pdf</a>]

<h2>Property Checking Without Invariant Generation. (arXiv:1602.05829v3 [cs.LO] UPDATED)</h2>
<h3>Eugene Goldberg</h3>
<p>We introduce a procedure for proving safety properties. This procedure is
based on a technique called Partial Quantifier Elimination (PQE). In contrast
to complete quantifier elimination, in PQE, only a part of the formula is taken
out of the scope of quantifiers. So, PQE can be dramatically more efficient
than complete quantifier elimination. The appeal of our procedure is twofold.
First, it can prove a property without generating an inductive invariant.
Second, it employs depth-first search and so can be used to find deep bugs.
</p>
<a href="http://arxiv.org/abs/1602.05829" target="_blank">arXiv:1602.05829</a> [<a href="http://arxiv.org/pdf/1602.05829" target="_blank">pdf</a>]

<h2>Type-two polynomial-time and restricted lookahead. (arXiv:1801.07485v2 [cs.CC] UPDATED)</h2>
<h3>Bruce M. Kapron, Florian Steinberg</h3>
<p>This paper provides an alternate characterization of type-two polynomial-time
computability, with the goal of making second-order complexity theory more
approachable. We rely on the usual oracle machines to model programs with
subroutine calls. In contrast to previous results, the use of higher-order
objects as running times is avoided, either explicitly or implicitly. Instead,
regular polynomials are used. This is achieved by refining the notion of
oracle-polynomial-time introduced by Cook. We impose a further restriction on
the oracle interactions to force feasibility. Both the restriction as well as
its purpose are very simple: it is well-known that Cook's model allows
polynomial depth iteration of functional inputs with no restrictions on size,
and thus does not guarantee that polynomial-time computability is preserved. To
mend this we restrict the number of lookahead revisions, that is the number of
times a query can be asked that is bigger than any of the previous queries. We
prove that this leads to a class of feasible functionals and that all feasible
problems can be solved within this class if one is allowed to separate a task
into efficiently solvable subtasks. Formally put: the closure of our class
under lambda-abstraction and application includes all feasible operations. We
also revisit the very similar class of strongly polynomial-time computable
operators previously introduced by Kawamura and Steinberg. We prove it to be
strictly included in our class and, somewhat surprisingly, to have the same
closure property. This can be attributed to properties of the limited recursion
operator: It is not strongly polynomial-time computable but decomposes into two
such operations and lies in our class.
</p>
<a href="http://arxiv.org/abs/1801.07485" target="_blank">arXiv:1801.07485</a> [<a href="http://arxiv.org/pdf/1801.07485" target="_blank">pdf</a>]

<h2>Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces. (arXiv:1902.06626v5 [cs.CR] UPDATED)</h2>
<h3>Mohammad Saidur Rahman, Mohsen Imani, Nate Mathews, Matthew Wright</h3>
<p>Website Fingerprinting (WF) is a type of traffic analysis attack that enables
a local passive eavesdropper to infer the victim's activity, even when the
traffic is protected by a VPN or an anonymity system like Tor. Leveraging a
deep-learning classifier, a WF attacker can gain over 98% accuracy on Tor
traffic. In this paper, we explore a novel defense, Mockingbird, based on the
idea of adversarial examples that have been shown to undermine machine-learning
classifiers in other domains. Since the attacker gets to design and train his
attack classifier based on the defense, we first demonstrate that at a
straightforward technique for generating adversarial-example based traces fails
to protect against an attacker using adversarial training for robust
classification. We then propose Mockingbird, a technique for generating traces
that resists adversarial training by moving randomly in the space of viable
traces and not following more predictable gradients. The technique drops the
accuracy of the state-of-the-art attack hardened with adversarial training from
98% to 42-58% while incurring only 58% bandwidth overhead. The attack accuracy
is generally lower than state-of-the-art defenses, and much lower when
considering Top-2 accuracy, while incurring lower bandwidth overheads.
</p>
<a href="http://arxiv.org/abs/1902.06626" target="_blank">arXiv:1902.06626</a> [<a href="http://arxiv.org/pdf/1902.06626" target="_blank">pdf</a>]

<h2>ATRW: A Benchmark for Amur Tiger Re-identification in the Wild. (arXiv:1906.05586v4 [cs.CV] UPDATED)</h2>
<h3>Shuyuan Li, Jianguo Li, Hanlin Tang, Rui Qian, Weiyao Lin</h3>
<p>Monitoring the population and movements of endangered species is an important
task to wildlife conversation. Traditional tagging methods do not scale to
large populations, while applying computer vision methods to camera sensor data
requires re-identification (re-ID) algorithms to obtain accurate counts and
moving trajectory of wildlife. However, existing re-ID methods are largely
targeted at persons and cars, which have limited pose variations and
constrained capture environments. This paper tries to fill the gap by
introducing a novel large-scale dataset, the Amur Tiger Re-identification in
the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur
tigers, with bounding box, pose keypoint, and tiger identity annotations. In
contrast to typical re-ID datasets, the tigers are captured in a diverse set of
unconstrained poses and lighting conditions. We demonstrate with a set of
baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we
propose a novel method for tiger re-identification, which introduces precise
pose parts modeling in deep neural networks to handle large pose variation of
tigers, and reaches notable performance improvement over existing re-ID
methods. The dataset is public available at https://cvwc2019.github.io/ .
</p>
<a href="http://arxiv.org/abs/1906.05586" target="_blank">arXiv:1906.05586</a> [<a href="http://arxiv.org/pdf/1906.05586" target="_blank">pdf</a>]

<h2>A Simple Local Minimal Intensity Prior and An Improved Algorithm for Blind Image Deblurring. (arXiv:1906.06642v5 [eess.IV] UPDATED)</h2>
<h3>Fei Wen, Rendong Ying, Yipeng Liu, Peilin Liu, Trieu-Kien Truong</h3>
<p>Blind image deblurring is a long standing challenging problem in image
processing and low-level vision. Recently, sophisticated priors such as dark
channel prior, extreme channel prior, and local maximum gradient prior, have
shown promising effectiveness. However, these methods are computationally
expensive. Meanwhile, since these priors involved subproblems cannot be solved
explicitly, approximate solution is commonly used, which limits the best
exploitation of their capability. To address these problems, this work firstly
proposes a simplified sparsity prior of local minimal pixels, namely patch-wise
minimal pixels (PMP). The PMP of clear images is much more sparse than that of
blurred ones, and hence is very effective in discriminating between clear and
blurred images. Then, a novel algorithm is designed to efficiently exploit the
sparsity of PMP in deblurring. The new algorithm flexibly imposes sparsity
inducing on the PMP under the MAP framework rather than directly uses the half
quadratic splitting algorithm. By this, it avoids non-rigorous approximation
solution in existing algorithms, while being much more computationally
efficient. Extensive experiments demonstrate that the proposed algorithm can
achieve better practical stability compared with state-of-the-arts. In terms of
deblurring quality, robustness and computational efficiency, the new algorithm
is superior to state-of-the-arts. Code for reproducing the results of the new
method is available at https://github.com/FWen/deblur-pmp.git.
</p>
<a href="http://arxiv.org/abs/1906.06642" target="_blank">arXiv:1906.06642</a> [<a href="http://arxiv.org/pdf/1906.06642" target="_blank">pdf</a>]

<h2>Dimensional Reweighting Graph Convolutional Networks. (arXiv:1907.02237v3 [cs.LG] UPDATED)</h2>
<h3>Xu Zou, Qiuye Jia, Jianwei Zhang, Chang Zhou, Hongxia Yang, Jie Tang</h3>
<p>Graph Convolution Networks (GCNs) are becoming more and more popular for
learning node representations on graphs. Though there exist various
developments on sampling and aggregation to accelerate the training process and
improve the performances, limited works focus on dealing with the dimensional
information imbalance of node representations. To bridge the gap, we propose a
method named Dimensional reweighting Graph Convolution Network (DrGCN). We
theoretically prove that our DrGCN can guarantee to improve the stability of
GCNs via mean field theory. Our dimensional reweighting method is very flexible
and can be easily combined with most sampling and aggregation techniques for
GCNs. Experimental results demonstrate its superior performances on several
challenging transductive and inductive node classification benchmark datasets.
Our DrGCN also outperforms existing models on an industrial-sized Alibaba
recommendation dataset.
</p>
<a href="http://arxiv.org/abs/1907.02237" target="_blank">arXiv:1907.02237</a> [<a href="http://arxiv.org/pdf/1907.02237" target="_blank">pdf</a>]

<h2>Developing an Unsupervised Real-time Anomaly Detection Scheme for Time Series with Multi-seasonality. (arXiv:1908.01146v2 [cs.LG] UPDATED)</h2>
<h3>Wentai Wu, Ligang He, Weiwei Lin, Yi Su, Yuhua Cui, Carsten Maple, Stephen Jarvis</h3>
<p>On-line detection of anomalies in time series is a key technique used in
various event-sensitive scenarios such as robotic system monitoring, smart
sensor networks and data center security. However, the increasing diversity of
data sources and the variety of demands make this task more challenging than
ever. Firstly, the rapid increase in unlabeled data means supervised learning
is becoming less suitable in many cases. Secondly, a large portion of time
series data have complex seasonality features. Thirdly, on-line anomaly
detection needs to be fast and reliable. In light of this, we have developed a
prediction-driven, unsupervised anomaly detection scheme, which adopts a
backbone model combining the decomposition and the inference of time series
data. Further, we propose a novel metric, Local Trend Inconsistency (LTI), and
an efficient detection algorithm that computes LTI in a real-time manner and
scores each data point robustly in terms of its probability of being anomalous.
We have conducted extensive experimentation to evaluate our algorithm with
several datasets from both public repositories and production environments. The
experimental results show that our scheme outperforms existing representative
anomaly detection algorithms in terms of the commonly used metric, Area Under
Curve (AUC), while achieving the desired efficiency.
</p>
<a href="http://arxiv.org/abs/1908.01146" target="_blank">arXiv:1908.01146</a> [<a href="http://arxiv.org/pdf/1908.01146" target="_blank">pdf</a>]

<h2>Optimal Machine Intelligence at the Edge of Chaos. (arXiv:1909.05176v2 [cs.LG] UPDATED)</h2>
<h3>Ling Feng, Lin Zhang, Choy Heng Lai</h3>
<p>It has long been suggested that the biological brain operates at some
critical point between two different phases, possibly order and chaos. Despite
many indirect empirical evidence from the brain and analytical indication on
simple neural networks, the foundation of this hypothesis on generic non-linear
systems remains unclear. Here we develop a general theory that reveals the
exact edge of chaos is the boundary between the chaotic phase and the
(pseudo)periodic phase arising from Neimark-Sacker bifurcation. This edge is
analytically determined by the asymptotic Jacobian norm values of the
non-linear operator and influenced by the dimensionality of the system. The
optimality at the edge of chaos is associated with the highest information
transfer between input and output at this point similar to that of the logistic
map. As empirical validations, our experiments on the various deep learning
models in computer vision demonstrate the optimality of the models near the
edge of chaos, and we observe that the state-of-art training algorithms push
the models towards such edge as they become more accurate. We further
establishes the theoretical understanding of deep learning model generalization
through asymptotic stability.
</p>
<a href="http://arxiv.org/abs/1909.05176" target="_blank">arXiv:1909.05176</a> [<a href="http://arxiv.org/pdf/1909.05176" target="_blank">pdf</a>]

<h2>Noisy Batch Active Learning with Deterministic Annealing. (arXiv:1909.12473v2 [cs.LG] UPDATED)</h2>
<h3>Gaurav Gupta, Anit Kumar Sahu, Wan-Yi Lin</h3>
<p>We study the problem of training machine learning models incrementally with
batches of samples annotated with noisy oracles. We select each batch of
samples that are important and also diverse via clustering and importance
sampling. More importantly, we incorporate model uncertainty into the sampling
probability to compensate for poor estimation of the importance scores when the
training data is too small to build a meaningful model. Experiments on
benchmark image classification datasets (MNIST, SVHN, CIFAR10, and EMNIST) show
improvement over existing active learning strategies. We introduce an extra
denoising layer to deep networks to make active learning robust to label noises
and show significant improvements.
</p>
<a href="http://arxiv.org/abs/1909.12473" target="_blank">arXiv:1909.12473</a> [<a href="http://arxiv.org/pdf/1909.12473" target="_blank">pdf</a>]

<h2>ProxIQA: A Proxy Approach to Perceptual Optimization of Learned Image Compression. (arXiv:1910.08845v2 [eess.IV] UPDATED)</h2>
<h3>Li-Heng Chen, Christos G. Bampis, Zhi Li, Andrey Norkin, Alan C. Bovik</h3>
<p>The use of $\ell_p$ $(p=1,2)$ norms has largely dominated the measurement of
loss in neural networks due to their simplicity and analytical properties.
However, when used to assess the loss of visual information, these simple norms
are not very consistent with human perception. Here, we describe a different
"proximal" approach to optimize image analysis networks against quantitative
perceptual models. Specifically, we construct a proxy network, broadly termed
ProxIQA, which mimics the perceptual model while serving as a loss layer of the
network. We experimentally demonstrate how this optimization framework can be
applied to train an end-to-end optimized image compression network. By building
on top of an existing deep image compression model, we are able to demonstrate
a bitrate reduction of as much as $31\%$ over MSE optimization, given a
specified perceptual quality (VMAF) level.
</p>
<a href="http://arxiv.org/abs/1910.08845" target="_blank">arXiv:1910.08845</a> [<a href="http://arxiv.org/pdf/1910.08845" target="_blank">pdf</a>]

<h2>Federated Learning over Wireless Networks: Convergence Analysis and Resource Allocation. (arXiv:1910.13067v4 [cs.LG] UPDATED)</h2>
<h3>Canh T. Dinh, Nguyen H. Tran, Minh N. H. Nguyen, Choong Seon Hong, Wei Bao, Albert Y. Zomaya, Vincent Gramoli</h3>
<p>There is an increasing interest in a fast-growing machine learning technique
called Federated Learning, in which the model training is distributed over
mobile user equipments (UEs), exploiting UEs' local computation and training
data. Despite its advantages in data privacy-preserving, Federated Learning
(FL) still has challenges in heterogeneity across UEs' data and physical
resources. We first propose a FL algorithm which can handle the heterogeneous
UEs' data challenge without further assumptions except strongly convex and
smooth loss functions. We provide the convergence rate characterizing the
trade-off between local computation rounds of UE to update its local model and
global communication rounds to update the FL global model. We then employ the
proposed FL algorithm in wireless networks as a resource allocation
optimization problem that captures the trade-off between the FL convergence
wall clock time and energy consumption of UEs with heterogeneous computing and
power resources. Even though the wireless resource allocation problem of FL is
non-convex, we exploit this problem's structure to decompose it into three
sub-problems and analyze their closed-form solutions as well as insights to
problem design. Finally, we illustrate the theoretical analysis for the new
algorithm with Tensorflow experiments and extensive numerical results for the
wireless resource allocation sub-problems. The experiment results not only
verify the theoretical convergence but also show that our proposed algorithm
outperforms the vanilla FedAvg algorithm in terms of convergence rate and
testing accuracy.
</p>
<a href="http://arxiv.org/abs/1910.13067" target="_blank">arXiv:1910.13067</a> [<a href="http://arxiv.org/pdf/1910.13067" target="_blank">pdf</a>]

<h2>Minimalistic Attacks: How Little it Takes to Fool a Deep Reinforcement Learning Policy. (arXiv:1911.03849v5 [cs.LG] UPDATED)</h2>
<h3>Xinghua Qu, Zhu Sun, Yew-Soon Ong, Abhishek Gupta, Pengfei Wei</h3>
<p>Recent studies have revealed that neural network-based policies can be easily
fooled by adversarial examples. However, while most prior works analyze the
effects of perturbing every pixel of every frame assuming white-box policy
access, in this paper we take a more restrictive view towards adversary
generation - with the goal of unveiling the limits of a model's vulnerability.
In particular, we explore minimalistic attacks by defining three key settings:
(1) black-box policy access: where the attacker only has access to the input
(state) and output (action probability) of an RL policy; (2) fractional-state
adversary: where only several pixels are perturbed, with the extreme case being
a single-pixel adversary; and (3) tactically-chanced attack: where only
significant frames are tactically chosen to be attacked. We formulate the
adversarial attack by accommodating the three key settings and explore their
potency on six Atari games by examining four fully trained state-of-the-art
policies. In Breakout, for example, we surprisingly find that: (i) all policies
showcase significant performance degradation by merely modifying 0.01% of the
input state, and (ii) the policy trained by DQN is totally deceived by
perturbation to only 1% frames.
</p>
<a href="http://arxiv.org/abs/1911.03849" target="_blank">arXiv:1911.03849</a> [<a href="http://arxiv.org/pdf/1911.03849" target="_blank">pdf</a>]

<h2>Privacy-Preserving Gradient Boosting Decision Trees. (arXiv:1911.04209v3 [cs.LG] UPDATED)</h2>
<h3>Qinbin Li, Zhaomin Wu, Zeyi Wen, Bingsheng He</h3>
<p>The Gradient Boosting Decision Tree (GBDT) is a popular machine learning
model for various tasks in recent years. In this paper, we study how to improve
model accuracy of GBDT while preserving the strong guarantee of differential
privacy. Sensitivity and privacy budget are two key design aspects for the
effectiveness of differential private models. Existing solutions for GBDT with
differential privacy suffer from the significant accuracy loss due to too loose
sensitivity bounds and ineffective privacy budget allocations (especially
across different trees in the GBDT model). Loose sensitivity bounds lead to
more noise to obtain a fixed privacy level. Ineffective privacy budget
allocations worsen the accuracy loss especially when the number of trees is
large. Therefore, we propose a new GBDT training algorithm that achieves
tighter sensitivity bounds and more effective noise allocations. Specifically,
by investigating the property of gradient and the contribution of each tree in
GBDTs, we propose to adaptively control the gradients of training data for each
iteration and leaf node clipping in order to tighten the sensitivity bounds.
Furthermore, we design a novel boosting framework to allocate the privacy
budget between trees so that the accuracy loss can be further reduced. Our
experiments show that our approach can achieve much better model accuracy than
other baselines.
</p>
<a href="http://arxiv.org/abs/1911.04209" target="_blank">arXiv:1911.04209</a> [<a href="http://arxiv.org/pdf/1911.04209" target="_blank">pdf</a>]

<h2>Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning. (arXiv:1912.02290v4 [stat.ML] UPDATED)</h2>
<h3>Samuel Kessler, Vu Nguyen, Stefan Zohren, Stephen Roberts</h3>
<p>We place an Indian Buffet process (IBP) prior over the structure of a
Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to
increase and decrease automatically. We further extend this model such that the
prior on the structure of each hidden layer is shared globally across all
layers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of
resource allocation in Continual Learning (CL) where new tasks occur and the
network requires extra resources. Our model uses online variational inference
with reparameterisation of the Bernoulli and Beta distributions, which
constitute the IBP and H-IBP priors. As we automatically learn the number of
weights in each layer of the BNN, overfitting and underfitting problems are
largely overcome. We show empirically that our approach offers a competitive
edge over existing methods in CL.
</p>
<a href="http://arxiv.org/abs/1912.02290" target="_blank">arXiv:1912.02290</a> [<a href="http://arxiv.org/pdf/1912.02290" target="_blank">pdf</a>]

<h2>What it Thinks is Important is Important: Robustness Transfers through Input Gradients. (arXiv:1912.05699v3 [cs.LG] UPDATED)</h2>
<h3>Alvin Chan, Yi Tay, Yew-Soon Ong</h3>
<p>Adversarial perturbations are imperceptible changes to input pixels that can
change the prediction of deep learning models. Learned weights of models robust
to such perturbations are previously found to be transferable across different
tasks but this applies only if the model architecture for the source and target
tasks is the same. Input gradients characterize how small changes at each input
pixel affect the model output. Using only natural images, we show here that
training a student model's input gradients to match those of a robust teacher
model can gain robustness close to a strong baseline that is robustly trained
from scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and
Tiny-ImageNet, we show that our proposed method, input gradient adversarial
matching, can transfer robustness across different tasks and even across
different model architectures. This demonstrates that directly targeting the
semantics of input gradients is a feasible way towards adversarial robustness.
</p>
<a href="http://arxiv.org/abs/1912.05699" target="_blank">arXiv:1912.05699</a> [<a href="http://arxiv.org/pdf/1912.05699" target="_blank">pdf</a>]

<h2>Deep Automodulators. (arXiv:1912.10321v4 [cs.LG] UPDATED)</h2>
<h3>Ari Heljakka, Yuxin Hou, Juho Kannala, Arno Solin</h3>
<p>We introduce a new category of generative autoencoders called automodulators.
These networks can faithfully reproduce individual real-world input images like
regular autoencoders, but also generate a fused sample from an arbitrary
combination of several such images, allowing instantaneous 'style-mixing' and
other new applications. An automodulator decouples the data flow of decoder
operations from statistical properties thereof and uses the latent vector to
modulate the former by the latter, with a principled approach for mutual
disentanglement of decoder layers. Prior work has explored similar decoder
architecture with GANs, but their focus has been on random sampling. A
corresponding autoencoder could operate on real input images. For the first
time, we show how to train such a general-purpose model with sharp outputs in
high resolution, using novel training techniques, demonstrated on four image
data sets. Besides style-mixing, we show state-of-the-art results in
autoencoder comparison, and visual image quality nearly indistinguishable from
state-of-the-art GANs. We expect the automodulator variants to become a useful
building block for image applications and other data domains.
</p>
<a href="http://arxiv.org/abs/1912.10321" target="_blank">arXiv:1912.10321</a> [<a href="http://arxiv.org/pdf/1912.10321" target="_blank">pdf</a>]

<h2>Statistical Limits of Supervised Quantum Learning. (arXiv:2001.10477v3 [quant-ph] UPDATED)</h2>
<h3>Carlo Ciliberto, Andrea Rocchetto, Alessandro Rudi, Leonard Wossnig</h3>
<p>Within the framework of statistical learning theory it is possible to bound
the minimum number of samples required by a learner to reach a target accuracy.
We show that if the bound on the accuracy is taken into account, quantum
machine learning algorithms for supervised learning---for which statistical
guarantees are available---cannot achieve polylogarithmic runtimes in the input
dimension. We conclude that, when no further assumptions on the problem are
made, quantum machine learning algorithms for supervised learning can have at
most polynomial speedups over efficient classical algorithms, even in cases
where quantum access to the data is naturally available.
</p>
<a href="http://arxiv.org/abs/2001.10477" target="_blank">arXiv:2001.10477</a> [<a href="http://arxiv.org/pdf/2001.10477" target="_blank">pdf</a>]

<h2>An implicit function learning approach for parametric modal regression. (arXiv:2002.06195v2 [stat.ML] UPDATED)</h2>
<h3>Yangchen Pan, Ehsan Imani, Martha White, Amir-massoud Farahmand</h3>
<p>For multi-valued functions---such as when the conditional distribution on
targets given the inputs is multi-modal---standard regression approaches are
not always desirable because they provide the conditional mean. Modal
regression algorithms address this issue by instead finding the conditional
mode(s). Most, however, are nonparametric approaches and so can be difficult to
scale. Further, parametric approximators, like neural networks, facilitate
learning complex relationships between inputs and targets. In this work, we
propose a parametric modal regression algorithm. We use the implicit function
theorem to develop an objective, for learning a joint function over inputs and
targets. We empirically demonstrate on several synthetic problems that our
method (i) can learn multi-valued functions and produce the conditional modes,
(ii) scales well to high-dimensional inputs, and (iii) can even be more
effective for certain uni-modal problems, particularly for high-frequency
functions. We demonstrate that our method is competitive in a real-world modal
regression problem and two regular regression datasets.
</p>
<a href="http://arxiv.org/abs/2002.06195" target="_blank">arXiv:2002.06195</a> [<a href="http://arxiv.org/pdf/2002.06195" target="_blank">pdf</a>]

<h2>Learning Global Transparent Models Consistent with Local Contrastive Explanations. (arXiv:2002.08247v4 [cs.LG] UPDATED)</h2>
<h3>Tejaswini Pedapati, Avinash Balakrishnan, Karthikeyan Shanmugam, Amit Dhurandhar</h3>
<p>There is a rich and growing literature on producing local
contrastive/counterfactual explanations for black-box models (e.g. neural
networks).

In these methods, for an input, an explanation is in the form of a contrast
point differing in very few features from the original input and lying in a
different class. Other works try to build globally interpretable models like
decision trees and rule lists based on the data using actual labels or based on
the black-box models predictions. Although these interpretable global models
can be useful, they may not be consistent with local explanations from a
specific black-box of choice. In this work, we explore the question: Can we
produce a transparent global model that is simultaneously accurate and
consistent with the local (contrastive) explanations of the black-box model? We
introduce a natural local consistency metric that quantifies if the local
explanations and predictions of the black-box model are also consistent with
the proxy global transparent model. Based on a key insight we propose a novel
method where we create custom boolean features from sparse local contrastive
explanations of the black-box model and then train a globally transparent model
on just these, and showcase empirically that such models have higher local
consistency compared with other known strategies, while still being close in
performance to models that are trained with access to the original data.
</p>
<a href="http://arxiv.org/abs/2002.08247" target="_blank">arXiv:2002.08247</a> [<a href="http://arxiv.org/pdf/2002.08247" target="_blank">pdf</a>]

<h2>Curriculum By Smoothing. (arXiv:2003.01367v3 [cs.LG] UPDATED)</h2>
<h3>Samarth Sinha, Animesh Garg, Hugo Larochelle</h3>
<p>Convolutional Neural Networks (CNNs) have shown impressive performance in
computer vision tasks such as image classification, detection, and
segmentation. Moreover, recent work in Generative Adversarial Networks (GANs)
has highlighted the importance of learning by progressively increasing the
difficulty of a learning task [26]. When learning a network from scratch, the
information propagated within the network during the earlier stages of training
can contain distortion artifacts due to noise which can be detrimental to
training. In this paper, we propose an elegant curriculum based scheme that
smoothes the feature embedding of a CNN using anti-aliasing or low-pass
filters. We propose to augment the train-ing of CNNs by controlling the amount
of high frequency information propagated within the CNNs as training
progresses, by convolving the output of a CNN feature map of each layer with a
Gaussian kernel. By decreasing the variance of the Gaussian kernel, we
gradually increase the amount of high-frequency information available within
the network for inference. As the amount of information in the feature maps
increases during training, the network is able to progressively learn better
representations of the data. Our proposed augmented training scheme
significantly improves the performance of CNNs on various vision tasks without
either adding additional trainable parameters or an auxiliary regularization
objective. The generality of our method is demonstrated through empirical
performance gains in CNN architectures across four different tasks: transfer
learning, cross-task transfer learning, and generative models.
</p>
<a href="http://arxiv.org/abs/2003.01367" target="_blank">arXiv:2003.01367</a> [<a href="http://arxiv.org/pdf/2003.01367" target="_blank">pdf</a>]

<h2>No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting with Adversarial Attacks. (arXiv:2003.03824v2 [eess.IV] UPDATED)</h2>
<h3>Siqi Liu, Arnaud Arindra Adiyoso Setio, Florin C. Ghesu, Eli Gibson, Sasa Grbic, Bogdan Georgescu, Dorin Comaniciu</h3>
<p>Detecting malignant pulmonary nodules at an early stage can allow medical
interventions which may increase the survival rate of lung cancer patients.
Using computer vision techniques to detect nodules can improve the sensitivity
and the speed of interpreting chest CT for lung cancer screening. Many studies
have used CNNs to detect nodule candidates. Though such approaches have been
shown to outperform the conventional image processing based methods regarding
the detection accuracy, CNNs are also known to be limited to generalize on
under-represented samples in the training set and prone to imperceptible noise
perturbations. Such limitations can not be easily addressed by scaling up the
dataset or the models. In this work, we propose to add adversarial synthetic
nodules and adversarial attack samples to the training data to improve the
generalization and the robustness of the lung nodule detection systems. To
generate hard examples of nodules from a differentiable nodule synthesizer, we
use projected gradient descent (PGD) to search the latent code within a bounded
neighbourhood that would generate nodules to decrease the detector response. To
make the network more robust to unanticipated noise perturbations, we use PGD
to search for noise patterns that can trigger the network to give
over-confident mistakes. By evaluating on two different benchmark datasets
containing consensus annotations from three radiologists, we show that the
proposed techniques can improve the detection performance on real CT data. To
understand the limitations of both the conventional networks and the proposed
augmented networks, we also perform stress-tests on the false positive
reduction networks by feeding different types of artificially produced patches.
We show that the augmented networks are more robust to both under-represented
nodules as well as resistant to noise perturbations.
</p>
<a href="http://arxiv.org/abs/2003.03824" target="_blank">arXiv:2003.03824</a> [<a href="http://arxiv.org/pdf/2003.03824" target="_blank">pdf</a>]

<h2>Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule. (arXiv:2003.03977v4 [cs.LG] UPDATED)</h2>
<h3>Nikhil Iyer, V Thejas, Nipun Kwatra, Ramachandran Ramjee, Muthian Sivathanu</h3>
<p>Several papers argue that wide minima generalize better than narrow minima.
In this paper, through detailed experiments that not only corroborate the
generalization properties of wide minima, we also provide empirical evidence
for a new hypothesis that the density of wide minima is likely lower than the
density of narrow minima. Further, motivated by this hypothesis, we design a
novel explore-exploit learning rate schedule. On a variety of image and natural
language datasets, compared to their original hand-tuned learning rate
baselines, we show that our explore-exploit schedule can result in either up to
0.84% higher absolute accuracy using the original training budget or up to 57%
reduced training time while achieving the original reported accuracy. For
example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) and
WMT'14 (DE-EN) datasets by just modifying the learning rate schedule of a high
performing model.
</p>
<a href="http://arxiv.org/abs/2003.03977" target="_blank">arXiv:2003.03977</a> [<a href="http://arxiv.org/pdf/2003.03977" target="_blank">pdf</a>]

<h2>On Calibration of Mixup Training for Deep Neural Networks. (arXiv:2003.09946v3 [cs.LG] UPDATED)</h2>
<h3>Juan Maro&#xf1;as, Daniel Ramos, Roberto Paredes</h3>
<p>Deep Neural Networks (DNN) represent the state of the art in many tasks.
However, due to their overparameterization, their generalization capabilities
are in doubt and still a field under study. Consequently, DNN can overfit and
assign overconfident predictions -- effects that have been shown to affect the
calibration of the confidences assigned to unseen data. Data Augmentation (DA)
strategies have been proposed to regularize these models, being Mixup one of
the most popular due to its ability to improve the accuracy, the uncertainty
quantification and the calibration of DNN. In this work however we argue and
provide empirical evidence that, due to its fundamentals, Mixup does not
necessarily improve calibration. Based on our observations we propose a new
loss function that improves the calibration, and also sometimes the accuracy,
of DNN trained with this DA technique. Our loss is inspired by Bayes decision
theory and introduces a new training framework for designing losses for
probabilistic modelling. We provide state-of-the-art accuracy with consistent
improvements in calibration performance. Appendix and code are provided here:
https://github.com/jmaronas/calibration_MixupDNN_ARCLoss.pytorch.git
</p>
<a href="http://arxiv.org/abs/2003.09946" target="_blank">arXiv:2003.09946</a> [<a href="http://arxiv.org/pdf/2003.09946" target="_blank">pdf</a>]

<h2>Supervised Contrastive Learning. (arXiv:2004.11362v2 [cs.LG] UPDATED)</h2>
<h3>Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan</h3>
<p>Contrastive learning applied to self-supervised representation learning has
seen a resurgence in recent years, leading to state of the art performance in
the unsupervised training of deep image models. Modern batch contrastive
approaches subsume or significantly outperform traditional contrastive losses
such as triplet, max-margin and the N-pairs loss. In this work, we extend the
self-supervised batch contrastive approach to the fully-supervised setting,
allowing us to effectively leverage label information. Clusters of points
belonging to the same class are pulled together in embedding space, while
simultaneously pushing apart clusters of samples from different classes. We
analyze two possible versions of the supervised contrastive (SupCon) loss,
identifying the best-performing formulation of the loss. On ResNet-200, we
achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above
the best number reported for this architecture. We show consistent
outperformance over cross-entropy on other datasets and two ResNet variants.
The loss shows benefits for robustness to natural corruptions and is more
stable to hyperparameter settings such as optimizers and data augmentations. In
reduced data settings, it outperforms cross-entropy significantly. Our loss
function is simple to implement, and reference TensorFlow code is released at
https://t.ly/supcon.
</p>
<a href="http://arxiv.org/abs/2004.11362" target="_blank">arXiv:2004.11362</a> [<a href="http://arxiv.org/pdf/2004.11362" target="_blank">pdf</a>]

<h2>InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs. (arXiv:2005.09635v2 [cs.CV] UPDATED)</h2>
<h3>Yujun Shen, Ceyuan Yang, Xiaoou Tang, Bolei Zhou</h3>
<p>Although Generative Adversarial Networks (GANs) have made significant
progress in face synthesis, there lacks enough understanding of what GANs have
learned in the latent representation to map a random code to a photo-realistic
image. In this work, we propose a framework called InterFaceGAN to interpret
the disentangled face representation learned by the state-of-the-art GAN models
and study the properties of the facial semantics encoded in the latent space.
We first find that GANs learn various semantics in some linear subspaces of the
latent space. After identifying these subspaces, we can realistically
manipulate the corresponding facial attributes without retraining the model. We
then conduct a detailed study on the correlation between different semantics
and manage to better disentangle them via subspace projection, resulting in
more precise control of the attribute manipulation. Besides manipulating the
gender, age, expression, and presence of eyeglasses, we can even alter the face
pose and fix the artifacts accidentally made by GANs. Furthermore, we perform
an in-depth face identity analysis and a layer-wise analysis to evaluate the
editing results quantitatively. Finally, we apply our approach to real face
editing by employing GAN inversion approaches and explicitly training
feed-forward models based on the synthetic data established by InterFaceGAN.
Extensive experimental results suggest that learning to synthesize faces
spontaneously brings a disentangled and controllable face representation.
</p>
<a href="http://arxiv.org/abs/2005.09635" target="_blank">arXiv:2005.09635</a> [<a href="http://arxiv.org/pdf/2005.09635" target="_blank">pdf</a>]

<h2>Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery. (arXiv:2006.03267v2 [eess.IV] UPDATED)</h2>
<h3>Christina Corbane, Vasileios Syrris, Filip Sabo, Panagiotis Politis, Michele Melchiorri, Martino Pesaresi, Pierre Soille, Thomas Kemper</h3>
<p>Spatially consistent and up-to-date maps of human settlements are crucial for
addressing policies related to urbanization and sustainability, especially in
the era of an increasingly urbanized world.The availability of open and free
Sentinel-2 data of the Copernicus Earth Observation program offers a new
opportunity for wall-to-wall mapping of human settlements at a global
scale.This paper presents a deep-learning-based framework for a fully automated
extraction of built-up areas at a spatial resolution of 10 m from a global
composite of Sentinel-2 imagery.A multi-neuro modeling methodology building on
a simple Convolution Neural Networks architecture for pixel-wise image
classification of built-up areas is developed.The core features of the proposed
model are the image patch of size 5 x 5 pixels adequate for describing built-up
areas from Sentinel-2 imagery and the lightweight topology with a total number
of 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened
layers.The deployment of the model on the global Sentinel-2 image composite
provides the most detailed and complete map reporting about built-up areas for
reference year 2018. The validation of the results with an independent
reference data-set of building footprints covering 277 sites across the world
establishes the reliability of the built-up layer produced by the proposed
framework and the model robustness.
</p>
<a href="http://arxiv.org/abs/2006.03267" target="_blank">arXiv:2006.03267</a> [<a href="http://arxiv.org/pdf/2006.03267" target="_blank">pdf</a>]

<h2>3D Self-Supervised Methods for Medical Imaging. (arXiv:2006.03829v2 [cs.CV] UPDATED)</h2>
<h3>Aiham Taleb, Winfried Loetzsch, Noel Danz, Julius Severin, Thomas Gaertner, Benjamin Bergner, Christoph Lippert</h3>
<p>Self-supervised learning methods have witnessed a recent surge of interest
after proving successful in multiple application fields. In this work, we
leverage these techniques, and we propose 3D versions for five different
self-supervised methods, in the form of proxy tasks. Our methods facilitate
neural network feature learning from unlabeled 3D images, aiming to reduce the
required cost for expert annotation. The developed algorithms are 3D
Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles,
Relative 3D patch location, and 3D Exemplar networks. Our experiments show that
pretraining models with our 3D tasks yields more powerful semantic
representations, and enables solving downstream tasks more accurately and
efficiently, compared to training the models from scratch and to pretraining
them on 2D slices. We demonstrate the effectiveness of our methods on three
downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation
from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic
Retinopathy Detection from 2D Fundus images. In each task, we assess the gains
in data-efficiency, performance, and speed of convergence. Interestingly, we
also find gains when transferring the learned representations, by our methods,
from a large unlabeled 3D corpus to a small downstream-specific dataset. We
achieve results competitive to state-of-the-art solutions at a fraction of the
computational expense. We publish our implementations for the developed
algorithms (both 3D and 2D versions) as an open-source library, in an effort to
allow other researchers to apply and extend our methods on their datasets.
</p>
<a href="http://arxiv.org/abs/2006.03829" target="_blank">arXiv:2006.03829</a> [<a href="http://arxiv.org/pdf/2006.03829" target="_blank">pdf</a>]

<h2>Truthful Data Acquisition via Peer Prediction. (arXiv:2006.03992v2 [cs.GT] UPDATED)</h2>
<h3>Yiling Chen, Yiheng Shen, Shuran Zheng</h3>
<p>We consider the problem of purchasing data for machine learning or
statistical estimation. The data analyst has a budget to purchase datasets from
multiple data providers. She does not have any test data that can be used to
evaluate the collected data and can assign payments to data providers solely
based on the collected datasets. We consider the problem in the standard
Bayesian paradigm and in two settings: (1) data are only collected once; (2)
data are collected repeatedly and each day's data are drawn independently from
the same distribution. For both settings, our mechanisms guarantee that
truthfully reporting one's dataset is always an equilibrium by adopting
techniques from peer prediction: pay each provider the mutual information
between his reported data and other providers' reported data. Depending on the
data distribution, the mechanisms can also discourage misreports that would
lead to inaccurate predictions. Our mechanisms also guarantee individual
rationality and budget feasibility for certain underlying distributions in the
first setting and for all distributions in the second setting.
</p>
<a href="http://arxiv.org/abs/2006.03992" target="_blank">arXiv:2006.03992</a> [<a href="http://arxiv.org/pdf/2006.03992" target="_blank">pdf</a>]

<h2>Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction. (arXiv:2006.06648v3 [cs.LG] UPDATED)</h2>
<h3>Jinheon Baek, Dong Bok Lee, Sung Ju Hwang</h3>
<p>Many practical graph problems, such as knowledge graph construction and
drug-drug interaction prediction, require to handle multi-relational graphs.
However, handling real-world multi-relational graphs with Graph Neural Networks
(GNNs) is often challenging due to their evolving nature, as new entities
(nodes) can emerge over time. Moreover, newly emerged entities often have few
links, which makes the learning even more difficult. Motivated by this
challenge, we introduce a realistic problem of few-shot out-of-graph link
prediction, where we not only predict the links between the seen and unseen
nodes as in a conventional out-of-knowledge link prediction task but also
between the unseen nodes, with only few edges per node. We tackle this problem
with a novel transductive meta-learning framework which we refer to as Graph
Extrapolation Networks (GEN). GEN meta-learns both the node embedding network
for inductive inference (seen-to-unseen) and the link prediction network for
transductive inference (unseen-to-unseen). For transductive link prediction, we
further propose a stochastic embedding layer to model uncertainty in the link
prediction between unseen entities. We validate our model on multiple benchmark
datasets for knowledge graph completion and drug-drug interaction prediction.
The results show that our model significantly outperforms relevant baselines
for out-of-graph link prediction tasks.
</p>
<a href="http://arxiv.org/abs/2006.06648" target="_blank">arXiv:2006.06648</a> [<a href="http://arxiv.org/pdf/2006.06648" target="_blank">pdf</a>]

<h2>Sparse and Continuous Attention Mechanisms. (arXiv:2006.07214v3 [cs.LG] UPDATED)</h2>
<h3>Andr&#xe9; F. T. Martins, Ant&#xf3;nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro M. Q. Aguiar, M&#xe1;rio A. T. Figueiredo</h3>
<p>Exponential families are widely used in machine learning; they include many
distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,
Poisson, and categorical distributions via the softmax transformation).
Distributions in each of these families have fixed support. In contrast, for
finite domains, there has been recent work on sparse alternatives to softmax
(e.g. sparsemax and alpha-entmax), which have varying support, being able to
assign zero probability to irrelevant categories. This paper expands that work
in two directions: first, we extend alpha-entmax to continuous domains,
revealing a link with Tsallis statistics and deformed exponential families.
Second, we introduce continuous-domain attention mechanisms, deriving efficient
gradient backpropagation algorithms for alpha in {1,2}. Experiments on
attention-based text classification, machine translation, and visual question
answering illustrate the use of continuous attention in 1D and 2D, showing that
it allows attending to time intervals and compact regions.
</p>
<a href="http://arxiv.org/abs/2006.07214" target="_blank">arXiv:2006.07214</a> [<a href="http://arxiv.org/pdf/2006.07214" target="_blank">pdf</a>]

<h2>Learning Latent Space Energy-Based Prior Model. (arXiv:2006.08205v2 [stat.ML] UPDATED)</h2>
<h3>Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, Ying Nian Wu</h3>
<p>We propose to learn energy-based model (EBM) in the latent space of a
generator model, so that the EBM serves as a prior model that stands on the
top-down network of the generator model. Both the latent space EBM and the
top-down network can be learned jointly by maximum likelihood, which involves
short-run MCMC sampling from both the prior and posterior distributions of the
latent vector. Due to the low dimensionality of the latent space and the
expressiveness of the top-down network, a simple EBM in latent space can
capture regularities in the data effectively, and MCMC sampling in latent space
is efficient and mixes well. We show that the learned model exhibits strong
performances in terms of image and text generation and anomaly detection. The
one-page code can be found in supplementary materials.
</p>
<a href="http://arxiv.org/abs/2006.08205" target="_blank">arXiv:2006.08205</a> [<a href="http://arxiv.org/pdf/2006.08205" target="_blank">pdf</a>]

<h2>ContraGAN: Contrastive Learning for Conditional Image Generation. (arXiv:2006.12681v2 [cs.CV] UPDATED)</h2>
<h3>Minguk Kang, Jaesik Park</h3>
<p>Conditional image generation is the task of generating diverse images using
class label information. Although many conditional Generative Adversarial
Networks (GAN) have shown realistic results, such methods consider pairwise
relations between the embedding of an image and the embedding of the
corresponding label (data-to-class relations) as the conditioning losses. In
this paper, we propose ContraGAN that considers relations between multiple
image embeddings in the same batch (data-to-data relations) as well as the
data-to-class relations by using a conditional contrastive loss. The
discriminator of ContraGAN discriminates the authenticity of given samples and
minimizes a contrastive objective to learn the relations between training
images. Simultaneously, the generator tries to generate realistic images that
deceive the authenticity and have a low contrastive loss. The experimental
results show that ContraGAN outperforms state-of-the-art-models by 7.3% and
7.7% on Tiny ImageNet and ImageNet datasets, respectively. Besides, we
experimentally demonstrate that ContraGAN helps to relieve the overfitting of
the discriminator. For a fair comparison, we re-implement twelve
state-of-the-art GANs using the PyTorch library. The software package is
available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.
</p>
<a href="http://arxiv.org/abs/2006.12681" target="_blank">arXiv:2006.12681</a> [<a href="http://arxiv.org/pdf/2006.12681" target="_blank">pdf</a>]

<h2>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization. (arXiv:2006.13258v2 [cs.LG] UPDATED)</h2>
<h3>Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai</h3>
<p>Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.
</p>
<a href="http://arxiv.org/abs/2006.13258" target="_blank">arXiv:2006.13258</a> [<a href="http://arxiv.org/pdf/2006.13258" target="_blank">pdf</a>]

<h2>Relative Deviation Margin Bounds. (arXiv:2006.14950v2 [cs.LG] UPDATED)</h2>
<h3>Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh</h3>
<p>We present a series of new and more favorable margin-based learning
guarantees that depend on the empirical margin loss of a predictor. We give two
types of learning bounds, both distribution-dependent and valid for general
families, in terms of the Rademacher complexity or the empirical $\ell_\infty$
covering number of the hypothesis set used. Furthermore, using our relative
deviation margin bounds, we derive distribution-dependent generalization bounds
for unbounded loss functions under the assumption of a finite moment. We also
briefly highlight several applications of these bounds and discuss their
connection with existing results.
</p>
<a href="http://arxiv.org/abs/2006.14950" target="_blank">arXiv:2006.14950</a> [<a href="http://arxiv.org/pdf/2006.14950" target="_blank">pdf</a>]

<h2>Robustness against Relational Adversary. (arXiv:2007.00772v2 [cs.LG] UPDATED)</h2>
<h3>Yizhen Wang, Xiaozhu Meng, Ke Wang, Mihai Christodorescu, Somesh Jha</h3>
<p>Test-time adversarial attacks have posed serious challenges to the robustness
of machine-learning models, and in many settings the adversarial perturbation
need not be bounded by small $\ell_p$-norms. Motivated by the
semantics-preserving attacks in vision and security domain, we investigate
$\textit{relational adversaries}$, a broad class of attackers who create
adversarial examples that are in a reflexive-transitive closure of a logical
relation. We analyze the conditions for robustness and propose
$\textit{normalize-and-predict}$ -- a learning framework with provable
robustness guarantee. We compare our approach with adversarial training and
derive an unified framework that provides benefits of both approaches. Guided
by our theoretical findings, we apply our framework to image classification and
malware detection. Results of both tasks show that attacks using relational
adversaries frequently fool existing models, but our unified framework can
significantly enhance their robustness.
</p>
<a href="http://arxiv.org/abs/2007.00772" target="_blank">arXiv:2007.00772</a> [<a href="http://arxiv.org/pdf/2007.00772" target="_blank">pdf</a>]

<h2>Information Theoretic Lower Bounds for Feed-Forward Fully-Connected Deep Networks. (arXiv:2007.00796v2 [stat.ML] UPDATED)</h2>
<h3>Xiaochen Yang, Jean Honorio</h3>
<p>In this paper, we study the sample complexity lower bounds for the exact
recovery of parameters and for a positive excess risk of a feed-forward,
fully-connected neural network for binary classification, using
information-theoretic tools. We prove these lower bounds by the existence of a
generative network characterized by a backwards data generating process, where
the input is generated based on the binary output, and the network is
parametrized by weight parameters for the hidden layers. The sample complexity
lower bound for the exact recovery of parameters is $\Omega(d r \log(r) + p )$
and for a positive excess risk is $\Omega(r \log(r) + p )$, where $p$ is the
dimension of the input, $r$ reflects the rank of the weight matrices and $d$ is
the number of hidden layers. To the best of our knowledge, our results are the
first information theoretic lower bounds.
</p>
<a href="http://arxiv.org/abs/2007.00796" target="_blank">arXiv:2007.00796</a> [<a href="http://arxiv.org/pdf/2007.00796" target="_blank">pdf</a>]

<h2>Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning. (arXiv:2007.01293v2 [cs.LG] UPDATED)</h2>
<h3>Zhongzheng Ren, Raymond A. Yeh, Alexander G. Schwing</h3>
<p>Existing semi-supervised learning (SSL) algorithms use a single weight to
balance the loss of labeled and unlabeled examples, i.e., all unlabeled
examples are equally weighted. But not all unlabeled data are equal. In this
paper we study how to use a different weight for every unlabeled example.
Manual tuning of all those weights -- as done in prior work -- is no longer
possible. Instead, we adjust those weights via an algorithm based on the
influence function, a measure of a model's dependency on one training example.
To make the approach efficient, we propose a fast and effective approximation
of the influence function. We demonstrate that this technique outperforms
state-of-the-art methods on semi-supervised image and language classification
tasks.
</p>
<a href="http://arxiv.org/abs/2007.01293" target="_blank">arXiv:2007.01293</a> [<a href="http://arxiv.org/pdf/2007.01293" target="_blank">pdf</a>]

<h2>Self-Supervised Graph Transformer on Large-Scale Molecular Data. (arXiv:2007.02835v2 [q-bio.BM] UPDATED)</h2>
<h3>Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, Junzhou Huang</h3>
<p>How to obtain informative representations of molecules is a crucial
prerequisite in AI-driven drug design and discovery. Recent researches abstract
molecules as graphs and employ Graph Neural Networks (GNNs) for molecular
representation learning. Nevertheless, two issues impede the usage of GNNs in
real scenarios: (1) insufficient labeled molecules for supervised training; (2)
poor generalization capability to new-synthesized molecules. To address them
both, we propose a novel framework, GROVER, which stands for Graph
Representation frOm self-superVised mEssage passing tRansformer. With carefully
designed self-supervised tasks in node-, edge- and graph-level, GROVER can
learn rich structural and semantic information of molecules from enormous
unlabelled molecular data. Rather, to encode such complex information, GROVER
integrates Message Passing Networks into the Transformer-style architecture to
deliver a class of more expressive encoders of molecules. The flexibility of
GROVER allows it to be trained efficiently on large-scale molecular dataset
without requiring any supervision, thus being immunized to the two issues
mentioned above. We pre-train GROVER with 100 million parameters on 10 million
unlabelled molecules -- the biggest GNN and the largest training dataset in
molecular representation learning. We then leverage the pre-trained GROVER for
molecular property prediction followed by task-specific fine-tuning, where we
observe a huge improvement (more than 6% on average) from current
state-of-the-art methods on 11 challenging benchmarks. The insights we gained
are that well-designed self-supervision losses and largely-expressive
pre-trained models enjoy the significant potential on performance boosting.
</p>
<a href="http://arxiv.org/abs/2007.02835" target="_blank">arXiv:2007.02835</a> [<a href="http://arxiv.org/pdf/2007.02835" target="_blank">pdf</a>]

<h2>RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning. (arXiv:2007.06271v2 [cs.CV] UPDATED)</h2>
<h3>Riccardo Del Chiaro, Bart&#x142;omiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer</h3>
<p>Research on continual learning has led to a variety of approaches to
mitigating catastrophic forgetting in feed-forward classification networks.
Until now surprisingly little attention has been focused on continual learning
of recurrent models applied to problems like image captioning. In this paper we
take a systematic look at continual learning of LSTM-based models for image
captioning. We propose an attention-based approach that explicitly accommodates
the transient nature of vocabularies in continual image captioning tasks --
i.e. that task vocabularies are not disjoint. We call our method Recurrent
Attention to Transient Tasks (RATT), and also show how to adapt continual
learning approaches based on weight egularization and knowledge distillation to
recurrent continual learning problems. We apply our approaches to incremental
image captioning problem on two new continual learning benchmarks we define
using the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is
able to sequentially learn five captioning tasks while incurring no forgetting
of previously learned ones.
</p>
<a href="http://arxiv.org/abs/2007.06271" target="_blank">arXiv:2007.06271</a> [<a href="http://arxiv.org/pdf/2007.06271" target="_blank">pdf</a>]

<h2>Temporal Pointwise Convolutional Networks for Length of Stay Prediction in the Intensive Care Unit. (arXiv:2007.09483v2 [cs.LG] UPDATED)</h2>
<h3>Emma Rocheteau, Pietro Li&#xf2;, Stephanie Hyland</h3>
<p>The pressure of ever-increasing patient demand and budget restrictions make
hospital bed management a daily challenge for clinical staff. Most critical is
the efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to
the patients who need life support. Central to solving this problem is knowing
for how long the current set of ICU patients are likely to stay in the unit. In
this work, we propose a new deep learning model based on the combination of
temporal convolution and pointwise (1x1) convolution, to solve the length of
stay prediction task on the eICU critical care dataset. The model - which we
refer to as Temporal Pointwise Convolution (TPC) - is specifically designed to
mitigate for common challenges with Electronic Health Records, such as
skewness, irregular sampling and missing data. In doing so, we have achieved
significant performance benefits of 18-51% (metric dependent) over the commonly
used Long-Short Term Memory (LSTM) network, and the multi-head self-attention
network known as the Transformer.
</p>
<a href="http://arxiv.org/abs/2007.09483" target="_blank">arXiv:2007.09483</a> [<a href="http://arxiv.org/pdf/2007.09483" target="_blank">pdf</a>]

<h2>CovidDeep: SARS-CoV-2/COVID-19 Test Based on Wearable Medical Sensors and Efficient Neural Networks. (arXiv:2007.10497v3 [cs.HC] UPDATED)</h2>
<h3>Shayan Hassantabar, Novati Stefano, Vishweshwar Ghanakota, Alessandra Ferrari, Gregory N. Nicola, Raffaele Bruno, Ignazio R. Marino, Kenza Hamidouche, Niraj K. Jha</h3>
<p>The novel coronavirus (SARS-CoV-2) has led to a pandemic. The current testing
regime based on Reverse Transcription-Polymerase Chain Reaction for SARS-CoV-2
has been unable to keep up with testing demands, and also suffers from a
relatively low positive detection rate in the early stages of the resultant
COVID-19 disease. Hence, there is a need for an alternative approach for
repeated large-scale testing of SARS-CoV-2/COVID-19. We propose a framework
called CovidDeep that combines efficient DNNs with commercially available WMSs
for pervasive testing of the virus. We collected data from 87 individuals,
spanning three cohorts including healthy, asymptomatic, and symptomatic
patients. We trained DNNs on various subsets of the features automatically
extracted from six WMS and questionnaire categories to perform ablation studies
to determine which subsets are most efficacious in terms of test accuracy for a
three-way classification. The highest test accuracy obtained was 98.1%. We also
augmented the real training dataset with a synthetic training dataset drawn
from the same probability distribution to impose a prior on DNN weights and
leveraged a grow-and-prune synthesis paradigm to learn both DNN architecture
and weights. This boosted the accuracy of the various DNNs further and
simultaneously reduced their size and floating-point operations.
</p>
<a href="http://arxiv.org/abs/2007.10497" target="_blank">arXiv:2007.10497</a> [<a href="http://arxiv.org/pdf/2007.10497" target="_blank">pdf</a>]

<h2>YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart Camera Applications. (arXiv:2007.13404v2 [cs.CV] UPDATED)</h2>
<h3>Christos Kyrkou</h3>
<p>Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.
</p>
<a href="http://arxiv.org/abs/2007.13404" target="_blank">arXiv:2007.13404</a> [<a href="http://arxiv.org/pdf/2007.13404" target="_blank">pdf</a>]

<h2>A Composable Specification Language for Reinforcement Learning Tasks. (arXiv:2008.09293v2 [cs.LG] UPDATED)</h2>
<h3>Kishor Jothimurugan, Rajeev Alur, Osbert Bastani</h3>
<p>Reinforcement learning is a promising approach for learning control policies
for robot tasks. However, specifying complex tasks (e.g., with multiple
objectives and safety constraints) can be challenging, since the user must
design a reward function that encodes the entire task. Furthermore, the user
often needs to manually shape the reward to ensure convergence of the learning
algorithm. We propose a language for specifying complex control tasks, along
with an algorithm that compiles specifications in our language into a reward
function and automatically performs reward shaping. We implement our approach
in a tool called SPECTRL, and show that it outperforms several state-of-the-art
baselines.
</p>
<a href="http://arxiv.org/abs/2008.09293" target="_blank">arXiv:2008.09293</a> [<a href="http://arxiv.org/pdf/2008.09293" target="_blank">pdf</a>]

<h2>Gravilon: Applications of a New Gradient Descent Method to Machine Learning. (arXiv:2008.11370v2 [cs.LG] UPDATED)</h2>
<h3>Chad Kelterborn, Marcin Mazur, Bogdan V. Petrenko</h3>
<p>Gradient descent algorithms have been used in countless applications since
the inception of Newton's method. The explosion in the number of applications
of neural networks has re-energized efforts in recent years to improve the
standard gradient descent method in both efficiency and accuracy. These methods
modify the effect of the gradient in updating the values of the parameters.
These modifications often incorporate hyperparameters: additional variables
whose values must be specified at the outset of the program. We provide, below,
a novel gradient descent algorithm, called Gravilon, that uses the geometry of
the hypersurface to modify the length of the step in the direction of the
gradient. Using neural networks, we provide promising experimental results
comparing the accuracy and efficiency of the Gravilon method against commonly
used gradient descent algorithms on MNIST digit classification.
</p>
<a href="http://arxiv.org/abs/2008.11370" target="_blank">arXiv:2008.11370</a> [<a href="http://arxiv.org/pdf/2008.11370" target="_blank">pdf</a>]

<h2>On the model-based stochastic value gradient for continuous reinforcement learning. (arXiv:2008.12775v2 [cs.LG] UPDATED)</h2>
<h3>Brandon Amos, Samuel Stanton, Denis Yarats, Andrew Gordon Wilson</h3>
<p>Model-based reinforcement learning approaches add explicit domain knowledge
to agents in hopes of improving the sample-efficiency in comparison to
model-free agents. However, in practice model-based methods are unable to
achieve the same asymptotic performance on challenging continuous control tasks
due to the complexity of learning and controlling an explicit world model. In
this paper we investigate the stochastic value gradient (SVG), which is a
well-known family of methods for controlling continuous systems which includes
model-based approaches that distill a model-based value expansion into a
model-free policy. We consider a variant of the model-based SVG that scales to
larger systems and uses 1) an entropy regularization to help with exploration,
2) a learned deterministic world model to improve the short-horizon value
estimate, and 3) a learned model-free value estimate after the model's rollout.
This SVG variation captures the model-free soft actor-critic method as an
instance when the model rollout horizon is zero, and otherwise uses
short-horizon model rollouts to improve the value estimate for the policy
update. We surpass the asymptotic performance of other model-based methods on
the proprioceptive MuJoCo locomotion tasks from the OpenAI gym, including a
humanoid. We notably achieve these results with a simple deterministic world
model without requiring an ensemble.
</p>
<a href="http://arxiv.org/abs/2008.12775" target="_blank">arXiv:2008.12775</a> [<a href="http://arxiv.org/pdf/2008.12775" target="_blank">pdf</a>]

<h2>Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning. (arXiv:2009.00142v4 [cs.LG] UPDATED)</h2>
<h3>Pan Li, Yanbang Wang, Hongwei Wang, Jure Leskovec</h3>
<p>Learning representations of sets of nodes in a graph is crucial for
applications ranging from node-role discovery to link prediction and molecule
classification. Graph Neural Networks (GNNs) have achieved great success in
graph representation learning. However, expressive power of GNNs is limited by
the 1-Weisfeiler-Lehman (WL) test and thus GNNs generate identical
representations for graph substructures that may in fact be very different.
More powerful GNNs, proposed recently by mimicking higher-order-WL tests, only
focus on representing entire graphs and they are computationally inefficient as
they cannot utilize sparsity of the underlying graph. Here we propose and
mathematically analyze a general class of structure-related features, termed
Distance Encoding (DE). DE assists GNNs in representing any set of nodes, while
providing strictly more expressive power than the 1-WL test. DE captures the
distance between the node set whose representation is to be learned and each
node in the graph. To capture the distance DE can apply various graph-distance
measures such as shortest path distance or generalized PageRank scores. We
propose two ways for GNNs to use DEs (1) as extra node features, and (2) as
controllers of message aggregation in GNNs. Both approaches can utilize the
sparse structure of the underlying graph, which leads to computational
efficiency and scalability. We also prove that DE can distinguish node sets
embedded in almost all regular graphs where traditional GNNs always fail. We
evaluate DE on three tasks over six real networks: structural role prediction,
link prediction, and triangle prediction. Results show that our models
outperform GNNs without DE by up-to 15\% in accuracy and AUROC. Furthermore,
our models also significantly outperform other state-of-the-art methods
especially designed for the above tasks.
</p>
<a href="http://arxiv.org/abs/2009.00142" target="_blank">arXiv:2009.00142</a> [<a href="http://arxiv.org/pdf/2009.00142" target="_blank">pdf</a>]

<h2>Accelerated reactive transport simulations in heterogeneous porous media using Reaktoro and Firedrake. (arXiv:2009.01194v2 [cs.CE] UPDATED)</h2>
<h3>Svetlana Kyas, Diego Volpatto, Martin O. Saar, Allan M. M. Leal</h3>
<p>This work investigates the performance of the on-demand machine learning
(ODML) algorithm introduced in Leal et al. (2020) when applied to different
reactive transport problems in heterogeneous porous media. ODML was devised to
accelerate the computationally expensive geochemical reaction calculations in
reactive transport simulations. We demonstrate that the ODML algorithm speeds
up these calculations by one to three orders of magnitude. Such acceleration,
in turn, significantly accelerates the entire reactive transport simulation.
The numerical experiments are performed by implementing the coupling of two
open-source software packages: Reaktoro (Leal, 2015) and Firedrake (Rathgeber
et al., 2016).
</p>
<a href="http://arxiv.org/abs/2009.01194" target="_blank">arXiv:2009.01194</a> [<a href="http://arxiv.org/pdf/2009.01194" target="_blank">pdf</a>]

<h2>Physically Embedded Planning Problems: New Challenges for Reinforcement Learning. (arXiv:2009.05524v2 [cs.AI] UPDATED)</h2>
<h3>Mehdi Mirza, Andrew Jaegle, Jonathan J. Hunt, Arthur Guez, Saran Tunyasuvunakool, Alistair Muldal, Th&#xe9;ophane Weber, Peter Karkus, S&#xe9;bastien Racani&#xe8;re, Lars Buesing, Timothy Lillicrap, Nicolas Heess</h3>
<p>Recent work in deep reinforcement learning (RL) has produced algorithms
capable of mastering challenging games such as Go, chess, or shogi. In these
works the RL agent directly observes the natural state of the game and controls
that state directly with its actions. However, when humans play such games,
they do not just reason about the moves but also interact with their physical
environment. They understand the state of the game by looking at the physical
board in front of them and modify it by manipulating pieces using touch and
fine-grained motor control. Mastering complicated physical systems with
abstract goals is a central challenge for artificial intelligence, but it
remains out of reach for existing RL algorithms. To encourage progress towards
this goal we introduce a set of physically embedded planning problems and make
them publicly available. We embed challenging symbolic tasks (Sokoban,
tic-tac-toe, and Go) in a physics engine to produce a set of tasks that require
perception, reasoning, and motor control over long time horizons. Although
existing RL algorithms can tackle the symbolic versions of these tasks, we find
that they struggle to master even the simplest of their physically embedded
counterparts. As a first step towards characterizing the space of solution to
these tasks, we introduce a strong baseline that uses a pre-trained expert game
player to provide hints in the abstract space to an RL agent's policy while
training it on the full sensorimotor control task. The resulting agent solves
many of the tasks, underlining the need for methods that bridge the gap between
abstract planning and embodied control. See illustrating video at
https://youtu.be/RwHiHlym_1k.
</p>
<a href="http://arxiv.org/abs/2009.05524" target="_blank">arXiv:2009.05524</a> [<a href="http://arxiv.org/pdf/2009.05524" target="_blank">pdf</a>]

<h2>CorDEL: A Contrastive Deep Learning Approach for Entity Linkage. (arXiv:2009.07203v2 [cs.DB] UPDATED)</h2>
<h3>Zhengyang Wang, Bunyamin Sisman, Hao Wei, Xin Luna Dong, Shuiwang Ji</h3>
<p>Entity linkage (EL) is a critical problem in data cleaning and integration.
In the past several decades, EL has typically been done by rule-based systems
or traditional machine learning models with hand-curated features, both of
which heavily depend on manual human inputs. With the ever-increasing growth of
new data, deep learning (DL) based approaches have been proposed to alleviate
the high cost of EL associated with the traditional models. Existing
exploration of DL models for EL strictly follows the well-known twin-network
architecture. However, we argue that the twin-network architecture is
sub-optimal to EL, leading to inherent drawbacks of existing models. In order
to address the drawbacks, we propose a novel and generic contrastive DL
framework for EL. The proposed framework is able to capture both syntactic and
semantic matching signals and pays attention to subtle but critical
differences. Based on the framework, we develop a contrastive DL approach for
EL, called CorDEL, with three powerful variants. We evaluate CorDEL with
extensive experiments conducted on both public benchmark datasets and a
real-world dataset. CorDEL outperforms previous state-of-the-art models by 5.2%
on public benchmark datasets. Moreover, CorDEL yields a 2.4% improvement over
the current best DL model on the real-world dataset, while reducing the number
of training parameters by 97.6%.
</p>
<a href="http://arxiv.org/abs/2009.07203" target="_blank">arXiv:2009.07203</a> [<a href="http://arxiv.org/pdf/2009.07203" target="_blank">pdf</a>]

<h2>Autoregressive Knowledge Distillation through Imitation Learning. (arXiv:2009.07253v2 [cs.CL] UPDATED)</h2>
<h3>Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei</h3>
<p>The performance of autoregressive models on natural language generation tasks
has dramatically improved due to the adoption of deep, self-attentive
architectures. However, these gains have come at the cost of hindering
inference speed, making state-of-the-art models cumbersome to deploy in
real-world, time-sensitive settings. We develop a compression technique for
autoregressive models that is driven by an imitation learning perspective on
knowledge distillation. The algorithm is designed to address the exposure bias
problem. On prototypical language generation tasks such as translation and
summarization, our method consistently outperforms other distillation
algorithms, such as sequence-level knowledge distillation. Student models
trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those
trained from scratch, while increasing inference speed by up to 14 times in
comparison to the teacher model.
</p>
<a href="http://arxiv.org/abs/2009.07253" target="_blank">arXiv:2009.07253</a> [<a href="http://arxiv.org/pdf/2009.07253" target="_blank">pdf</a>]

<h2>Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization. (arXiv:2009.12829v3 [cs.CV] UPDATED)</h2>
<h3>Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, Alex C. Kot</h3>
<p>Recently, we have witnessed great progress in the field of medical imaging
classification by adopting deep neural networks. However, the recent advanced
models still require accessing sufficiently large and representative datasets
for training, which is often unfeasible in clinically realistic environments.
When trained on limited datasets, the deep neural network is lack of
generalization capability, as the trained deep neural network on data within a
certain distribution (e.g. the data captured by a certain device vendor or
patient population) may not be able to generalize to the data with another
distribution.

In this paper, we introduce a simple but effective approach to improve the
generalization capability of deep neural networks in the field of medical
imaging classification. Motivated by the observation that the domain
variability of the medical images is to some extent compact, we propose to
learn a representative feature space through variational encoding with a novel
linear-dependency regularization term to capture the shareable information
among medical data collected from different domains. As a result, the trained
neural network is expected to equip with better generalization capability to
the "unseen" medical data. Experimental results on two challenging medical
imaging classification tasks indicate that our method can achieve better
cross-domain generalization capability compared with state-of-the-art
baselines.
</p>
<a href="http://arxiv.org/abs/2009.12829" target="_blank">arXiv:2009.12829</a> [<a href="http://arxiv.org/pdf/2009.12829" target="_blank">pdf</a>]

<h2>Pretrained Language Model Embryology: The Birth of ALBERT. (arXiv:2010.02480v2 [cs.CL] UPDATED)</h2>
<h3>Cheng-Han Chiang, Sung-Feng Huang, Hung-yi Lee</h3>
<p>While behaviors of pretrained language models (LMs) have been thoroughly
examined, what happened during pretraining is rarely studied. We thus
investigate the developmental process from a set of randomly initialized
parameters to a totipotent language model, which we refer to as the embryology
of a pretrained language model. Our results show that ALBERT learns to
reconstruct and predict tokens of different parts of speech (POS) in different
learning speeds during pretraining. We also find that linguistic knowledge and
world knowledge do not generally improve as pretraining proceeds, nor do
downstream tasks' performance. These findings suggest that knowledge of a
pretrained model varies during pretraining, and having more pretrain steps does
not necessarily provide a model with more comprehensive knowledge. We will
provide source codes and pretrained models to reproduce our results at
https://github.com/d223302/albert-embryology.
</p>
<a href="http://arxiv.org/abs/2010.02480" target="_blank">arXiv:2010.02480</a> [<a href="http://arxiv.org/pdf/2010.02480" target="_blank">pdf</a>]

<h2>Improving Local Identifiability in Probabilistic Box Embeddings. (arXiv:2010.04831v2 [cs.LG] UPDATED)</h2>
<h3>Shib Sankar Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Lorraine Li, Andrew McCallum</h3>
<p>Geometric embeddings have recently received attention for their natural
ability to represent transitive asymmetric relations via containment. Box
embeddings, where objects are represented by n-dimensional hyperrectangles, are
a particularly promising example of such an embedding as they are closed under
intersection and their volume can be calculated easily, allowing them to
naturally represent calibrated probability distributions. The benefits of
geometric embeddings also introduce a problem of local identifiability,
however, where whole neighborhoods of parameters result in equivalent loss
which impedes learning. Prior work addressed some of these issues by using an
approximation to Gaussian convolution over the box parameters, however, this
intersection operation also increases the sparsity of the gradient. In this
work, we model the box parameters with min and max Gumbel distributions, which
were chosen such that space is still closed under the operation of the
intersection. The calculation of the expected intersection volume involves all
parameters, and we demonstrate experimentally that this drastically improves
the ability of such models to learn.
</p>
<a href="http://arxiv.org/abs/2010.04831" target="_blank">arXiv:2010.04831</a> [<a href="http://arxiv.org/pdf/2010.04831" target="_blank">pdf</a>]

<h2>Neural-Symbolic Reasoning on Knowledge Graphs. (arXiv:2010.05446v3 [cs.AI] UPDATED)</h2>
<h3>Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, Haipeng Ding</h3>
<p>Knowledge graph reasoning is the fundamental component to support machine
learning applications such as information extraction, information retrieval and
recommendation. Since knowledge graph can be viewed as the discrete symbolic
representations of knowledge, reasoning on knowledge graphs can naturally
leverage the symbolic techniques. However, symbolic reasoning is intolerant of
the ambiguous and noisy data. On the contrary, the recent advances of deep
learning promote neural reasoning on knowledge graphs, which is robust to the
ambiguous and noisy data, but lacks interpretability compared to symbolic
reasoning. Considering the advantages and disadvantages of both methodologies,
recent efforts have been made on combining the two reasoning methods. In this
survey, we take a thorough look at the development of the symbolic reasoning,
neural reasoning and the neural-symbolic reasoning on knowledge graphs. We
survey two specific reasoning tasks, knowledge graph completion and question
answering on knowledge graphs, and explain them in a unified reasoning
framework. We also briefly discuss the future directions for knowledge graph
reasoning.
</p>
<a href="http://arxiv.org/abs/2010.05446" target="_blank">arXiv:2010.05446</a> [<a href="http://arxiv.org/pdf/2010.05446" target="_blank">pdf</a>]

<h2>CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations. (arXiv:2010.06351v3 [cs.CL] UPDATED)</h2>
<h3>Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun</h3>
<p>Pre-trained self-supervised models such as BERT have achieved striking
success in learning sequence representations, especially for natural language
processing. These models typically corrupt the given sequences with certain
types of noise, such as masking, shuffling, or substitution, and then try to
recover the original input. However, such pre-training approaches are prone to
learning representations that are covariant with the noise, leading to the
discrepancy between the pre-training and fine-tuning stage. To remedy this, we
present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence
representations. The proposed CAPT encourages the consistency between
representations of the original sequence and its corrupted version via
unsupervised instance-wise training signals. In this way, it not only
alleviates the pretrain-finetune discrepancy induced by the noise of
pre-training, but also aids the pre-trained model in better capturing global
semantics of the input via more effective sentence-level supervision. Different
from most prior work that focuses on a particular modality, comprehensive
empirical evidence on 11 natural language understanding and cross-modal tasks
illustrates that CAPT is applicable for both language and vision-language
tasks, and obtains surprisingly consistent improvement, including 0.6% absolute
gain on GLUE benchmarks and 0.8% absolute increment on NLVR.
</p>
<a href="http://arxiv.org/abs/2010.06351" target="_blank">arXiv:2010.06351</a> [<a href="http://arxiv.org/pdf/2010.06351" target="_blank">pdf</a>]

<h2>Spherical Knowledge Distillation. (arXiv:2010.07485v2 [cs.LG] UPDATED)</h2>
<h3>Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, Deng Cai</h3>
<p>Knowledge distillation aims at obtaining a small but effective deep model by
transferring knowledge from a much larger one. The previous approaches try to
reach this goal by simply "logit-supervised" information transferring between
the teacher and student, which somehow can be subsequently decomposed as the
transfer of normalized logits and $l^2$ norm. We argue that the norm of logits
is actually interference, which damages the efficiency in the transfer process.
To address this problem, we propose Spherical Knowledge Distillation (SKD).
Specifically, we project the teacher and the student's logits into a unit
sphere, and then we can efficiently perform knowledge distillation on the
sphere. We verify our argument via theoretical analysis and ablation study.
Extensive experiments have demonstrated the superiority and scalability of our
method over the SOTAs.
</p>
<a href="http://arxiv.org/abs/2010.07485" target="_blank">arXiv:2010.07485</a> [<a href="http://arxiv.org/pdf/2010.07485" target="_blank">pdf</a>]

<h2>Learning Accurate Entropy Model with Global Reference for Image Compression. (arXiv:2010.08321v2 [eess.IV] UPDATED)</h2>
<h3>Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhenhong Sun, Hao Li, Rong Jin</h3>
<p>In recent deep image compression neural networks, the entropy model plays a
critical role in estimating the prior distribution of deep image encodings.
Existing methods combine hyperprior with local context in the entropy
estimation function. This greatly limits their performance due to the absence
of a global vision. In this work, we propose a novel Global Reference Model for
image compression to effectively leverage both the local and the global context
information, leading to an enhanced compression rate. The proposed method scans
decoded latents and then finds the most relevant latent to assist the
distribution estimating of the current latent. A by-product of this work is the
innovation of a mean-shifting GDN module that further improves the performance.
Experimental results demonstrate that the proposed model outperforms the
rate-distortion performance of most of the state-of-the-art methods in the
industry.
</p>
<a href="http://arxiv.org/abs/2010.08321" target="_blank">arXiv:2010.08321</a> [<a href="http://arxiv.org/pdf/2010.08321" target="_blank">pdf</a>]

<h2>A Grid-based Representation for Human Action Recognition. (arXiv:2010.08841v2 [cs.CV] UPDATED)</h2>
<h3>Soufiane Lamghari, Guillaume-Alexandre Bilodeau, Nicolas Saunier</h3>
<p>Human action recognition (HAR) in videos is a fundamental research topic in
computer vision. It consists mainly in understanding actions performed by
humans based on a sequence of visual observations. In recent years, HAR have
witnessed significant progress, especially with the emergence of deep learning
models. However, most of existing approaches for action recognition rely on
information that is not always relevant for this task, and are limited in the
way they fuse the temporal information. In this paper, we propose a novel
method for human action recognition that encodes efficiently the most
discriminative appearance information of an action with explicit attention on
representative pose features, into a new compact grid representation. Our GRAR
(Grid-based Representation for Action Recognition) method is tested on several
benchmark datasets demonstrating that our model can accurately recognize human
actions, despite intra-class appearance variations and occlusion challenges.
</p>
<a href="http://arxiv.org/abs/2010.08841" target="_blank">arXiv:2010.08841</a> [<a href="http://arxiv.org/pdf/2010.08841" target="_blank">pdf</a>]

<h2>What breach? Measuring online awareness of security incidents by studying real-world browsing behavior. (arXiv:2010.09843v2 [cs.CR] UPDATED)</h2>
<h3>Sruti Bhagavatula, Lujo Bauer, Apu Kapadia</h3>
<p>Awareness about security and privacy risks is important for developing good
security habits. Learning about real-world security incidents and data breaches
can alert people to the ways in which their information is vulnerable online,
thus playing a significant role in encouraging safe security behavior. This
paper examines 1) how often people read about security incidents online, 2) of
those people, whether and to what extent they follow up with an action, e.g.,
by trying to read more about the incident, and 3) what influences the
likelihood that they will read about an incident and take some action. We study
this by quantitatively examining real-world internet-browsing data from 303
participants.

Our findings present a bleak view of awareness of security incidents. Only
17% of participants visited any web pages related to six widely publicized
large-scale security incidents; few read about one even when an incident was
likely to have affected them (e.g., the Equifax breach almost universally
affected people with Equifax credit reports). We further found that more severe
incidents as well as articles that constructively spoke about the incident
inspired more action. We conclude with recommendations for specific future
research and for enabling useful security incident information to reach more
people.
</p>
<a href="http://arxiv.org/abs/2010.09843" target="_blank">arXiv:2010.09843</a> [<a href="http://arxiv.org/pdf/2010.09843" target="_blank">pdf</a>]

<h2>Model selection in reconciling hierarchical time series. (arXiv:2010.10742v2 [cs.LG] UPDATED)</h2>
<h3>Mahdi Abolghasemi, Rob J Hyndman, Evangelos Spiliotis, Christoph Bergmeir</h3>
<p>Model selection has been proven an effective strategy for improving accuracy
in time series forecasting applications. However, when dealing with
hierarchical time series, apart from selecting the most appropriate forecasting
model, forecasters have also to select a suitable method for reconciling the
base forecasts produced for each series to make sure they are coherent.
Although some hierarchical forecasting methods like minimum trace are strongly
supported both theoretically and empirically for reconciling the base
forecasts, there are still circumstances under which they might not produce the
most accurate results, being outperformed by other methods. In this paper we
propose an approach for dynamically selecting the most appropriate hierarchical
forecasting method and succeeding better forecasting accuracy along with
coherence. The approach, to be called conditional hierarchical forecasting, is
based on Machine Learning classification methods and uses time series features
as leading indicators for performing the selection for each hierarchy examined
considering a variety of alternatives. Our results suggest that conditional
hierarchical forecasting leads to significantly more accurate forecasts than
standard approaches, especially at lower hierarchical levels.
</p>
<a href="http://arxiv.org/abs/2010.10742" target="_blank">arXiv:2010.10742</a> [<a href="http://arxiv.org/pdf/2010.10742" target="_blank">pdf</a>]

<h2>The Polynomial Method is Universal for Distribution-Free Correlational SQ Learning. (arXiv:2010.11925v2 [cs.DS] UPDATED)</h2>
<h3>Aravind Gollakota, Sushrut Karmalkar, Adam Klivans</h3>
<p>We consider the problem of distribution-free learning for Boolean function
classes in the PAC and agnostic models. Generalizing a recent beautiful work of
Malach and Shalev-Shwartz (2020) who gave the first tight correlational SQ
(CSQ) lower bounds for learning DNF formulas, we show that lower bounds on the
threshold or approximate degree of any function class directly imply CSQ lower
bounds for PAC or agnostic learning respectively. These match corresponding
positive results using upper bounds on the threshold or approximate degree in
the SQ model for PAC or agnostic learning. Many of these results were implicit
in earlier works of Feldman and Sherstov.
</p>
<a href="http://arxiv.org/abs/2010.11925" target="_blank">arXiv:2010.11925</a> [<a href="http://arxiv.org/pdf/2010.11925" target="_blank">pdf</a>]

<h2>Adaptive In-network Collaborative Caching for Enhanced Ensemble Deep Learning at Edge. (arXiv:2010.12899v3 [cs.NI] UPDATED)</h2>
<h3>Yana Qin, Danye Wu, Zhiwei Xu, Jie Tian, Yujun Zhang</h3>
<p>To enhance the quality and speed of data processing and protect the privacy
and security of the data, edge computing has been extensively applied to
support data-intensive intelligent processing services at edge. Among these
data-intensive services, ensemble learning-based services can in natural
leverage the distributed computation and storage resources at edge devices to
achieve efficient data collection, processing, analysis.

Collaborative caching has been applied in edge computing to support services
close to the data source, in order to take the limited resources at edge
devices to support high-performance ensemble learning solutions. To achieve
this goal, we propose an adaptive in-network collaborative caching scheme for
ensemble learning at edge. First, an efficient data representation structure is
proposed to record cached data among different nodes. In addition, we design a
collaboration scheme to facilitate edge nodes to cache valuable data for local
ensemble learning, by scheduling local caching according to a summarization of
data representations from different edge nodes. Our extensive simulations
demonstrate the high performance of the proposed collaborative caching scheme,
which significantly reduces the learning latency and the transmission overhead.
</p>
<a href="http://arxiv.org/abs/2010.12899" target="_blank">arXiv:2010.12899</a> [<a href="http://arxiv.org/pdf/2010.12899" target="_blank">pdf</a>]

<h2>Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional Similarity-Based Approach. (arXiv:2010.13273v2 [cs.IR] UPDATED)</h2>
<h3>Yuyang Dong, Kunihiro Takeoka, Chuan Xiao, Masafumi Oyamada</h3>
<p>Finding joinable tables in data lakes is key procedure in many applications
such as data integration, data augmentation, data analysis, and data market.
Traditional approaches that find equi-joinable tables are unable to deal with
misspellings and different formats, nor do they capture any semantic joins. In
this paper, we propose PEXESO, a framework for joinable table discovery in data
lakes. We embed textual values as high-dimensional vectors and join columns
under similarity predicates on high-dimensional vectors, hence to address the
limitations of equi-join approaches and identify more meaningful results. To
efficiently find joinable tables with similarity, we propose a block-and-verify
method that utilizes pivot-based filtering. A partitioning technique is
developed to cope with the case when the data lake is large and the index
cannot fit in main memory. An experimental evaluation on real datasets shows
that our solution identifies substantially more tables than equi-joins and
outperforms other similarity-based options, and the join results are useful in
data enrichment for machine learning tasks. The experiments also demonstrate
the efficiency of the proposed method.
</p>
<a href="http://arxiv.org/abs/2010.13273" target="_blank">arXiv:2010.13273</a> [<a href="http://arxiv.org/pdf/2010.13273" target="_blank">pdf</a>]

<h2>Malicious Requests Detection with Improved Bidirectional Long Short-term Memory Neural Networks. (arXiv:2010.13285v2 [cs.LG] UPDATED)</h2>
<h3>Wenhao Li, Bincheng Zhang, Jiajie Zhang</h3>
<p>Detecting and intercepting malicious requests are one of the most widely used
ways against attacks in the network security. Most existing detecting
approaches, including matching blacklist characters and machine learning
algorithms have all shown to be vulnerable to sophisticated attacks. To address
the above issues, a more general and rigorous detection method is required. In
this paper, we formulate the problem of detecting malicious requests as a
temporal sequence classification problem, and propose a novel deep learning
model namely Convolutional Neural Network-Bidirectional Long Short-term
Memory-Convolutional Neural Network (CNN-BiLSTM-CNN). By connecting the shadow
and deep feature maps of the convolutional layers, the malicious feature
extracting ability is improved on more detailed functionality. Experimental
results on HTTP dataset CSIC 2010 have demonstrated the effectiveness of the
proposed method when compared with the state-of-the-arts.
</p>
<a href="http://arxiv.org/abs/2010.13285" target="_blank">arXiv:2010.13285</a> [<a href="http://arxiv.org/pdf/2010.13285" target="_blank">pdf</a>]

<h2>Memory Optimization for Deep Networks. (arXiv:2010.14501v2 [cs.LG] UPDATED)</h2>
<h3>Aashaka Shah, Chao-Yuan Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Kr&#xe4;henb&#xfc;hl</h3>
<p>Deep learning is slowly, but steadily, hitting a memory bottleneck. While the
tensor computation in top-of-the-line GPUs increased by 32x over the last five
years, the total available memory only grew by 2.5x. This prevents researchers
from exploring larger architectures, as training large networks requires more
memory for storing intermediate outputs. In this paper, we present MONeT, an
automatic framework that minimizes both the memory footprint and computational
overhead of deep networks. MONeT jointly optimizes the checkpointing schedule
and the implementation of various operators. MONeT is able to outperform all
prior hand-tuned operations as well as automated checkpointing. MONeT reduces
the overall memory requirement by 3x for various PyTorch models, with a 9-16%
overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x
less memory than current state-of-the-art automated checkpointing frameworks.
Our code is available at https://github.com/utsaslab/MONeT.
</p>
<a href="http://arxiv.org/abs/2010.14501" target="_blank">arXiv:2010.14501</a> [<a href="http://arxiv.org/pdf/2010.14501" target="_blank">pdf</a>]

<h2>Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports. (arXiv:2010.14584v2 [cs.CL] UPDATED)</h2>
<h3>Aleksandra Edwards, David Rogers, Jose Camacho-Collados, H&#xe9;l&#xe8;ne de Ribaupierre, Alun Preece</h3>
<p>The task of text and sentence classification is associated with the need for
large amounts of labelled training data. The acquisition of high volumes of
labelled datasets can be expensive or unfeasible, especially for
highly-specialised domains for which documents are hard to obtain. Research on
the application of supervised classification based on small amounts of training
data is limited. In this paper, we address the combination of state-of-the-art
deep learning and classification methods and provide an insight into what
combination of methods fit the needs of small, domain-specific, and
terminologically-rich corpora. We focus on a real-world scenario related to a
collection of safeguarding reports comprising learning experiences and
reflections on tackling serious incidents involving children and vulnerable
adults. The relatively small volume of available reports and their use of
highly domain-specific terminology makes the application of automated
approaches difficult. We focus on the problem of automatically identifying the
main themes in a safeguarding report using supervised classification
approaches. Our results show the potential of deep learning models to simulate
subject-expert behaviour even for complex tasks with limited labelled data.
</p>
<a href="http://arxiv.org/abs/2010.14584" target="_blank">arXiv:2010.14584</a> [<a href="http://arxiv.org/pdf/2010.14584" target="_blank">pdf</a>]

<h2>Batch Reinforcement Learning with a Nonparametric Off-Policy Policy Gradient. (arXiv:2010.14771v2 [cs.LG] UPDATED)</h2>
<h3>Samuele Tosatto, Jo&#xe3;o Carvalho, Jan Peters</h3>
<p>Off-policy Reinforcement Learning (RL) holds the promise of better data
efficiency as it allows sample reuse and potentially enables safe interaction
with the environment. Current off-policy policy gradient methods either suffer
from high bias or high variance, delivering often unreliable estimates. The
price of inefficiency becomes evident in real-world scenarios such as
interaction-driven robot learning, where the success of RL has been rather
limited, and a very high sample cost hinders straightforward application. In
this paper, we propose a nonparametric Bellman equation, which can be solved in
closed form. The solution is differentiable w.r.t the policy parameters and
gives access to an estimation of the policy gradient. In this way, we avoid the
high variance of importance sampling approaches, and the high bias of
semi-gradient methods. We empirically analyze the quality of our gradient
estimate against state-of-the-art methods, and show that it outperforms the
baselines in terms of sample efficiency on classical control tasks.
</p>
<a href="http://arxiv.org/abs/2010.14771" target="_blank">arXiv:2010.14771</a> [<a href="http://arxiv.org/pdf/2010.14771" target="_blank">pdf</a>]

<h2>Transferable Universal Adversarial Perturbations Using Generative Models. (arXiv:2010.14919v2 [cs.CV] UPDATED)</h2>
<h3>Atiye Sadat Hashemi, Andreas B&#xe4;r, Saeed Mozaffari, Tim Fingscheidt</h3>
<p>Deep neural networks tend to be vulnerable to adversarial perturbations,
which by adding to a natural image can fool a respective model with high
confidence. Recently, the existence of image-agnostic perturbations, also known
as universal adversarial perturbations (UAPs), were discovered. However,
existing UAPs still lack a sufficiently high fooling rate, when being applied
to an unknown target model. In this paper, we propose a novel deep learning
technique for generating more transferable UAPs. We utilize a perturbation
generator and some given pretrained networks so-called source models to
generate UAPs using the ImageNet dataset. Due to the similar feature
representation of various model architectures in the first layer, we propose a
loss formulation that focuses on the adversarial energy only in the respective
first layer of the source models. This supports the transferability of our
generated UAPs to any other target model. We further empirically analyze our
generated UAPs and demonstrate that these perturbations generalize very well
towards different target models. Surpassing the current state of the art in
both, fooling rate and model-transferability, we can show the superiority of
our proposed approach. Using our generated non-targeted UAPs, we obtain an
average fooling rate of 93.36% on the source models (state of the art: 82.16%).
Generating our UAPs on the deep ResNet-152, we obtain about a 12% absolute
fooling rate advantage vs. cutting-edge methods on VGG-16 and VGG-19 target
models.
</p>
<a href="http://arxiv.org/abs/2010.14919" target="_blank">arXiv:2010.14919</a> [<a href="http://arxiv.org/pdf/2010.14919" target="_blank">pdf</a>]

<h2>Benchmarking Parallelism in FaaS Platforms. (arXiv:2010.15032v2 [cs.DC] UPDATED)</h2>
<h3>Daniel Barcelona-Pons, Pedro Garc&#xed;a-L&#xf3;pez</h3>
<p>Serverless computing has seen a myriad of work exploring its potential. Some
systems tackle Function-as-a-Service (FaaS) properties on automatic elasticity
and scale to run highly-parallel computing jobs. However, they focus on
specific platforms and convey that their ideas can be extrapolated to any FaaS
runtime.

An important question arises: do all FaaS platforms fit parallel
computations? In this paper, we argue that not all of them provide the
necessary means to host highly-parallel applications. To validate our
hypothesis, we create a comparative framework and categorize the architectures
of four cloud FaaS offerings, with emphasis on parallel performance. We attest
and extend this description with an empirical experiment that consists in
plotting in deep detail the evolution of a parallel computing job on each
service.

The analysis of our results evinces that FaaS is not inherently good for
parallel computations and architectural differences across platforms are
decisive to categorize their performance. A key insight is the importance of
virtualization technologies and the scheduling approach of FaaS platforms.
Parallelism improves with lighter virtualization and proactive scheduling due
to finer resource allocation and faster elasticity. This causes some platforms
like AWS and IBM to perform well for highly-parallel computations, while others
such as Azure present difficulties to achieve the required parallelism degree.
Consequently, the information in this paper becomes of special interest to help
users choose the most adequate infrastructure for their parallel applications.
</p>
<a href="http://arxiv.org/abs/2010.15032" target="_blank">arXiv:2010.15032</a> [<a href="http://arxiv.org/pdf/2010.15032" target="_blank">pdf</a>]

<h2>Continuous Chaotic Nonlinear System and Lyapunov controller Optimization using Deep Learning. (arXiv:2010.14746v1 [eess.SY] CROSS LISTED)</h2>
<h3>Amr Mahmoud, Youmna Ismaeil, Mohamed Zohdy</h3>
<p>The introduction of unexpected system disturbances and new system dynamics
does not allow initially selected static system and controller parameters to
guarantee continued system stability and performance. In this research we
present a novel approach for detecting early failure indicators of non-linear
highly chaotic system and accordingly predict the best parameter calibrations
to offset such instability using deep machine learning regression model. The
approach proposed continuously monitors the system and controller signals. The
Re-calibration of the system and controller parameters is triggered according
to a set of conditions designed to maintain system stability without compromise
to the system speed, intended outcome or required processing power. The deep
neural model predicts the parameter values that would best counteract the
expected system in-stability. To demonstrate the effectiveness of the proposed
approach, it is applied to the non-linear complex combination of Duffing Van
der pol oscillators. The approach is also tested under different scenarios the
system and controller parameters are initially chosen incorrectly or the system
parameters are changed while running or new system dynamics are introduced
while running to measure effectiveness and reaction time.
</p>
<a href="http://arxiv.org/abs/2010.14746" target="_blank">arXiv:2010.14746</a> [<a href="http://arxiv.org/pdf/2010.14746" target="_blank">pdf</a>]

<h2>Spatiotemporal effects of the causal factors on COVID-19 incidences in the contiguous United States. (arXiv:2010.15754v1 [stat.AP])</h2>
<h3>Arabinda Maiti, Qi Zhang, Srikanta Sannigrahi, Suvamoy Pramanik, Suman Chakraborti, Francesco Pilla</h3>
<p>Since December 2019, the world has been witnessing the gigantic effect of an
unprecedented global pandemic called Severe Acute Respiratory Syndrome
Coronavirus (SARS-CoV-2) - COVID-19. So far, 38,619,674 confirmed cases and
1,093,522 confirmed deaths due to COVID-19 have been reported. In the United
States (US), the cases and deaths are recorded as 7,833,851 and 215,199.
Several timely researches have discussed the local and global effects of the
confounding factors on COVID-19 casualties in the US. However, most of these
studies considered little about the time varying associations between and among
these factors, which are crucial for understanding the outbreak of the present
pandemic. Therefore, this study adopts various relevant approaches, including
local and global spatial regression models and machine learning to explore the
causal effects of the confounding factors on COVID-19 counts in the contiguous
US. Totally five spatial regression models, spatial lag model (SLM), ordinary
least square (OLS), spatial error model (SEM), geographically weighted
regression (GWR) and multiscale geographically weighted regression (MGWR), are
performed at the county scale to take into account the scale effects on
modelling. For COVID-19 cases, ethnicity, crime, and income factors are found
to be the strongest covariates and explain the maximum model variances. For
COVID-19 deaths, both (domestic and international) migration and income factors
play a crucial role in explaining spatial differences of COVID-19 death counts
across counties. The local coefficient of determination (R2) values derived
from the GWR and MGWR models are found very high over the
Wisconsin-Indiana-Michigan (the Great Lake) region, as well as several parts of
Texas, California, Mississippi and Arkansas.
</p>
<a href="http://arxiv.org/abs/2010.15754" target="_blank">arXiv:2010.15754</a> [<a href="http://arxiv.org/pdf/2010.15754" target="_blank">pdf</a>]

<h2>Learning Bayesian Networks from Ordinal Data. (arXiv:2010.15808v1 [stat.ME])</h2>
<h3>Xiang Ge Luo, Giusi Moffa, Jack Kuipers</h3>
<p>Bayesian networks are a powerful framework for studying the dependency
structure of variables in a complex system. The problem of learning Bayesian
networks is tightly associated with the given data type. Ordinal data, such as
stages of cancer, rating scale survey questions, and letter grades for exams,
are ubiquitous in applied research. However, existing solutions are mainly for
continuous and nominal data. In this work, we propose an iterative
score-and-search method - called the Ordinal Structural EM (OSEM) algorithm -
for learning Bayesian networks from ordinal data. Unlike traditional approaches
designed for nominal data, we explicitly respect the ordering amongst the
categories. More precisely, we assume that the ordinal variables originate from
marginally discretizing a set of Gaussian variables, whose structural
dependence in the latent space follows a directed acyclic graph. Then, we adopt
the Structural EM algorithm and derive closed-form scoring functions for
efficient graph searching. Through simulation studies, we illustrate the
superior performance of the OSEM algorithm compared to the alternatives and
analyze various factors that may influence the learning accuracy. Finally, we
demonstrate the practicality of our method with a real-world application on
psychological survey data from 408 patients with co-morbid symptoms of
obsessive-compulsive disorder and depression.
</p>
<a href="http://arxiv.org/abs/2010.15808" target="_blank">arXiv:2010.15808</a> [<a href="http://arxiv.org/pdf/2010.15808" target="_blank">pdf</a>]

<h2>Expanding the scope of statistical computing: Training statisticians to be software engineers. (arXiv:1912.13076v3 [stat.CO] UPDATED)</h2>
<h3>Alex Reinhart, Christopher R. Genovese</h3>
<p>Traditionally, statistical computing courses have taught the syntax of a
particular programming language or specific statistical computation methods.
Since the publication of Nolan and Temple Lang (2010), we have seen a greater
emphasis on data wrangling, reproducible research, and visualization. This
shift better prepares students for careers working with complex datasets and
producing analyses for multiple audiences. But, we argue, statisticians are now
often called upon to develop statistical software, not just analyses, such as R
packages implementing new analysis methods or machine learning systems
integrated into commercial products. This demands different skills.

We describe a graduate course that we developed to meet this need by focusing
on four themes: programming practices; software design; important algorithms
and data structures; and essential tools and methods. Through code review and
revision, and a semester-long software project, students practice all the
skills of software engineering. The course allows students to expand their
understanding of computing as applied to statistical problems while building
expertise in the kind of software development that is increasingly the province
of the working statistician. We see this as a model for the future evolution of
the computing curriculum in statistics and data science.
</p>
<a href="http://arxiv.org/abs/1912.13076" target="_blank">arXiv:1912.13076</a> [<a href="http://arxiv.org/pdf/1912.13076" target="_blank">pdf</a>]

<h2>Machine learning for weather and climate are worlds apart. (arXiv:2008.10679v2 [physics.ao-ph] UPDATED)</h2>
<h3>Duncan Watson-Parris</h3>
<p>Modern weather and climate models share a common heritage, and often even
components, however they are used in different ways to answer fundamentally
different questions. As such, attempts to emulate them using machine learning
should reflect this. While the use of machine learning to emulate weather
forecast models is a relatively new endeavour there is a rich history of
climate model emulation. This is primarily because while weather modelling is
an initial condition problem which intimately depends on the current state of
the atmosphere, climate modelling is predominantly a boundary condition
problem. In order to emulate the response of the climate to different drivers
therefore, representation of the full dynamical evolution of the atmosphere is
neither necessary, or in many cases, desirable. Climate scientists are
typically interested in different questions also. Indeed emulating the
steady-state climate response has been possible for many years and provides
significant speed increases that allow solving inverse problems for e.g.
parameter estimation. Nevertheless, the large datasets, non-linear
relationships and limited training data make Climate a domain which is rich in
interesting machine learning challenges.

Here I seek to set out the current state of climate model emulation and
demonstrate how, despite some challenges, recent advances in machine learning
provide new opportunities for creating useful statistical models of the
climate.
</p>
<a href="http://arxiv.org/abs/2008.10679" target="_blank">arXiv:2008.10679</a> [<a href="http://arxiv.org/pdf/2008.10679" target="_blank">pdf</a>]

