---
title: Latest Deep Learning Papers
date: 2020-12-22 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (172 Articles)</h1>
<h2>Exploring and Analyzing Machine Commonsense Benchmarks. (arXiv:2012.11634v1 [cs.AI])</h2>
<h3>Henrique Santos, Minor Gordon, Zhicheng Liang, Gretchen Forbush, Deborah L. McGuinness</h3>
<p>Commonsense question-answering (QA) tasks, in the form of benchmarks, are
constantly being introduced for challenging and comparing commonsense QA
systems. The benchmarks provide question sets that systems' developers can use
to train and test new models before submitting their implementations to
official leaderboards. Although these tasks are created to evaluate systems in
identified dimensions (e.g. topic, reasoning type), this metadata is limited
and largely presented in an unstructured format or completely not present.
Because machine common sense is a fast-paced field, the problem of fully
assessing current benchmarks and systems with regards to these evaluation
dimensions is aggravated. We argue that the lack of a common vocabulary for
aligning these approaches' metadata limits researchers in their efforts to
understand systems' deficiencies and in making effective choices for future
tasks. In this paper, we first discuss this MCS ecosystem in terms of its
elements and their metadata. Then, we present how we are supporting the
assessment of approaches by initially focusing on commonsense benchmarks. We
describe our initial MCS Benchmark Ontology, an extensible common vocabulary
that formalizes benchmark metadata, and showcase how it is supporting the
development of a Benchmark tool that enables benchmark exploration and
analysis.
</p>
<a href="http://arxiv.org/abs/2012.11634" target="_blank">arXiv:2012.11634</a> [<a href="http://arxiv.org/pdf/2012.11634" target="_blank">pdf</a>]

<h2>Unsupervised in-distribution anomaly detection of new physics through conditional density estimation. (arXiv:2012.11638v1 [cs.LG])</h2>
<h3>George Stein, Uros Seljak, Biwei Dai</h3>
<p>Anomaly detection is a key application of machine learning, but is generally
focused on the detection of outlying samples in the low probability density
regions of data. Here we instead present and motivate a method for unsupervised
in-distribution anomaly detection using a conditional density estimator,
designed to find unique, yet completely unknown, sets of samples residing in
high probability density regions. We apply this method towards the detection of
new physics in simulated Large Hadron Collider (LHC) particle collisions as
part of the 2020 LHC Olympics blind challenge, and show how we detected a new
particle appearing in only 0.08% of 1 million collision events. The results we
present are our original blind submission to the 2020 LHC Olympics, where it
achieved the state-of-the-art performance.
</p>
<a href="http://arxiv.org/abs/2012.11638" target="_blank">arXiv:2012.11638</a> [<a href="http://arxiv.org/pdf/2012.11638" target="_blank">pdf</a>]

<h2>Multi-Agent Reinforcement Learning for Dynamic Ocean Monitoring by a Swarm of Buoys. (arXiv:2012.11641v1 [cs.RO])</h2>
<h3>Maryam Kouzehgar, Malika Meghjani, Roland Bouffanais</h3>
<p>Autonomous marine environmental monitoring problem traditionally encompasses
an area coverage problem which can only be effectively carried out by a
multi-robot system. In this paper, we focus on robotic swarms that are
typically operated and controlled by means of simple swarming behaviors
obtained from a subtle, yet ad hoc combination of bio-inspired strategies. We
propose a novel and structured approach for area coverage using multi-agent
reinforcement learning (MARL) which effectively deals with the non-stationarity
of environmental features. Specifically, we propose two dynamic area coverage
approaches: (1) swarm-based MARL, and (2) coverage-range-based MARL. The former
is trained using the multi-agent deep deterministic policy gradient (MADDPG)
approach whereas, a modified version of MADDPG is introduced for the latter
with a reward function that intrinsically leads to a collective behavior. Both
methods are tested and validated with different geometric shaped regions with
equal surface area (square vs. rectangle) yielding acceptable area coverage,
and benefiting from the structured learning in non-stationary environments.
Both approaches are advantageous compared to a na\"{i}ve swarming method.
However, coverage-range-based MARL outperforms the swarm-based MARL with
stronger convergence features in learning criteria and higher spreading of
agents for area coverage.
</p>
<a href="http://arxiv.org/abs/2012.11641" target="_blank">arXiv:2012.11641</a> [<a href="http://arxiv.org/pdf/2012.11641" target="_blank">pdf</a>]

<h2>myGym: Modular Toolkit for Visuomotor Robotic Tasks. (arXiv:2012.11643v1 [cs.RO])</h2>
<h3>Michal Vavrecka, Nikita Sokovnin, Megi Mejdrechova, Gabriela Sejnova, Marek Otahal</h3>
<p>We introduce a novel virtual robotic toolkit myGym, developed for
reinforcement learning (RL), intrinsic motivation and imitation learning tasks
trained in a 3D simulator. The trained tasks can then be easily transferred to
real-world robotic scenarios. The modular structure of the simulator enables
users to train and validate their algorithms on a large number of scenarios
with various robots, environments and tasks. Compared to existing toolkits
(e.g. OpenAI Gym, Roboschool) which are suitable for classical RL, myGym is
also prepared for visuomotor (combining vision &amp; movement) unsupervised tasks
that require intrinsic motivation, i.e. the robots are able to generate their
own goals. There are also collaborative scenarios intended for human-robot
interaction. The toolkit provides pretrained visual modules for visuomotor
tasks allowing rapid prototyping, and, moreover, users can customize the visual
submodules and retrain with their own set of objects. In practice, the user
selects the desired environment, robot, objects, task and type of reward as
simulation parameters, and the training, visualization and testing themselves
are handled automatically. The user can thus fully focus on development of the
neural network architecture while controlling the behaviour of the environment
using predefined parameters.
</p>
<a href="http://arxiv.org/abs/2012.11643" target="_blank">arXiv:2012.11643</a> [<a href="http://arxiv.org/pdf/2012.11643" target="_blank">pdf</a>]

<h2>Fast Physical Activity Suggestions: Efficient Hyperparameter Learning in Mobile Health. (arXiv:2012.11646v1 [cs.LG])</h2>
<h3>Marianne Menictas, Sabina Tomkins, Susan Murphy</h3>
<p>Users can be supported to adopt healthy behaviors, such as regular physical
activity, via relevant and timely suggestions on their mobile devices.
Recently, reinforcement learning algorithms have been found to be effective for
learning the optimal context under which to provide suggestions. However, these
algorithms are not necessarily designed for the constraints posed by mobile
health (mHealth) settings, that they be efficient, domain-informed and
computationally affordable. We propose an algorithm for providing physical
activity suggestions in mHealth settings. Using domain-science, we formulate a
contextual bandit algorithm which makes use of a linear mixed effects model. We
then introduce a procedure to efficiently perform hyper-parameter updating,
using far less computational resources than competing approaches. Not only is
our approach computationally efficient, it is also easily implemented with
closed form matrix algebraic updates and we show improvements over state of the
art approaches both in speed and accuracy of up to 99% and 56% respectively.
</p>
<a href="http://arxiv.org/abs/2012.11646" target="_blank">arXiv:2012.11646</a> [<a href="http://arxiv.org/pdf/2012.11646" target="_blank">pdf</a>]

<h2>Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. (arXiv:2012.11654v1 [stat.ML])</h2>
<h3>Quynh Nguyen, Marco Mondelli, Guido Montufar</h3>
<p>A recent line of work has analyzed the theoretical properties of deep neural
networks via the Neural Tangent Kernel (NTK). In particular, the smallest
eigenvalue of the NTK has been related to memorization capacity, convergence of
gradient descent algorithms and generalization of deep nets. However, existing
results either provide bounds in the two-layer setting or assume that the
spectrum of the NTK is bounded away from 0 for multi-layer networks. In this
paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for
deep ReLU networks, both in the limiting case of infinite widths and for finite
widths. In the finite-width setting, the network architectures we consider are
quite general: we require the existence of a wide layer with roughly order of
$N$ neurons, $N$ being the number of data samples; and the scaling of the
remaining widths is arbitrary (up to logarithmic factors). To obtain our
results, we analyze various quantities of independent interest: we give lower
bounds on the smallest singular value of feature matrices, and upper bounds on
the Lipschitz constant of input-output feature maps.
</p>
<a href="http://arxiv.org/abs/2012.11654" target="_blank">arXiv:2012.11654</a> [<a href="http://arxiv.org/pdf/2012.11654" target="_blank">pdf</a>]

<h2>Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised Video Object Segmentation. (arXiv:2012.11655v1 [cs.CV])</h2>
<h3>Hyojin Park, Jayeon Yoo, Seohyeong Jeong, Ganesh Venkatesh, Nojun Kwak</h3>
<p>Current state-of-the-art approaches for Semi-supervised Video Object
Segmentation (Semi-VOS) propagates information from previous frames to generate
segmentation mask for the current frame. This results in high-quality
segmentation across challenging scenarios such as changes in appearance and
occlusion. But it also leads to unnecessary computations for stationary or
slow-moving objects where the change across frames is minimal. In this work, we
exploit this observation by using temporal information to quickly identify
frames with minimal change and skip the heavyweight mask generation step. To
realize this efficiency, we propose a novel dynamic network that estimates
change across frames and decides which path -- computing a full network or
reusing previous frame's feature -- to choose depending on the expected
similarity. Experimental results show that our approach significantly improves
inference speed without much accuracy degradation on challenging Semi-VOS
datasets -- DAVIS 16, DAVIS 17, and YouTube-VOS. Furthermore, our approach can
be applied to multiple Semi-VOS methods demonstrating its generality.
</p>
<a href="http://arxiv.org/abs/2012.11655" target="_blank">arXiv:2012.11655</a> [<a href="http://arxiv.org/pdf/2012.11655" target="_blank">pdf</a>]

<h2>Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning. (arXiv:2012.11662v1 [cs.LG])</h2>
<h3>Sean Gillen, Katie Byl</h3>
<p>A key limitation in using various modern methods of machine learning in
developing feedback control policies is the lack of appropriate methodologies
to analyze their long-term dynamics, in terms of making any sort of guarantees
(even statistically) about robustness. The central reasons for this are largely
due to the so-called curse of dimensionality, combined with the black-box
nature of the resulting control policies themselves. This paper aims at the
first of these issues. Although the full state space of a system may be quite
large in dimensionality, it is a common feature of most model-based control
methods that the resulting closed-loop systems demonstrate dominant dynamics
that are rapidly driven to some lower-dimensional sub-space within. In this
work we argue that the dimensionality of this subspace is captured by tools
from fractal geometry, namely various notions of a fractional dimension. We
then show that the dimensionality of trajectories induced by model free
reinforcement learning agents can be influenced adding a post processing
function to the agents reward signal. We verify that the dimensionality
reduction is robust to noise being added to the system and show that that the
modified agents are more actually more robust to noise and push disturbances in
general for the systems we examined.
</p>
<a href="http://arxiv.org/abs/2012.11662" target="_blank">arXiv:2012.11662</a> [<a href="http://arxiv.org/pdf/2012.11662" target="_blank">pdf</a>]

<h2>Combining Deep Reinforcement Learning And Local Control For The Acrobot Swing-up And Balance Task. (arXiv:2012.11663v1 [cs.RO])</h2>
<h3>Sean Gillen, Marco Molnar, Katie Byl</h3>
<p>In this work we present a novel extension of soft actor critic, a state of
the art deep reinforcement algorithm. Our method allows us to combine
traditional controllers with learned neural network policies. This combination
allows us to leverage both our own domain knowledge and some of the advantages
of model free reinforcement learning. We demonstrate our algorithm by combining
a hand designed linear quadratic regulator with a learned controller for the
acrobot problem. We show that our technique outperforms other state of the art
reinforcement learning algorithms in this setting.
</p>
<a href="http://arxiv.org/abs/2012.11663" target="_blank">arXiv:2012.11663</a> [<a href="http://arxiv.org/pdf/2012.11663" target="_blank">pdf</a>]

<h2>Smoothed Gaussian Mixture Models for Video Classification and Recommendation. (arXiv:2012.11673v1 [cs.CV])</h2>
<h3>Sirjan Kafle, Aman Gupta, Xue Xia, Ananth Sankar, Xi Chen, Di Wen, Liang Zhang</h3>
<p>Cluster-and-aggregate techniques such as Vector of Locally Aggregated
Descriptors (VLAD), and their end-to-end discriminatively trained equivalents
like NetVLAD have recently been popular for video classification and action
recognition tasks. These techniques operate by assigning video frames to
clusters and then representing the video by aggregating residuals of frames
with respect to the mean of each cluster. Since some clusters may see very
little video-specific data, these features can be noisy. In this paper, we
propose a new cluster-and-aggregate method which we call smoothed Gaussian
mixture model (SGMM), and its end-to-end discriminatively trained equivalent,
which we call deep smoothed Gaussian mixture model (DSGMM). SGMM represents
each video by the parameters of a Gaussian mixture model (GMM) trained for that
video. Low-count clusters are addressed by smoothing the video-specific
estimates with a universal background model (UBM) trained on a large number of
videos. The primary benefit of SGMM over VLAD is smoothing which makes it less
sensitive to small number of training samples. We show, through extensive
experiments on the YouTube-8M classification task, that SGMM/DSGMM is
consistently better than VLAD/NetVLAD by a small but statistically significant
margin. We also show results using a dataset created at LinkedIn to predict if
a member will watch an uploaded video.
</p>
<a href="http://arxiv.org/abs/2012.11673" target="_blank">arXiv:2012.11673</a> [<a href="http://arxiv.org/pdf/2012.11673" target="_blank">pdf</a>]

<h2>Natural vs Balanced Distribution in Deep Learning on Whole Slide Images for Cancer Detection. (arXiv:2012.11684v1 [cs.CV])</h2>
<h3>Ismat Ara Reshma, Sylvain Cussat-Blanc, Radu Tudor Ionescu, Herv&#xe9; Luga, Josiane Mothe</h3>
<p>The class distribution of data is one of the factors that regulates the
performance of machine learning models. However, investigations on the impact
of different distributions available in the literature are very few, sometimes
absent for domain-specific tasks. In this paper, we analyze the impact of
natural and balanced distributions of the training set in deep learning (DL)
models applied on histological images, also known as whole slide images (WSIs).
WSIs are considered as the gold standard for cancer diagnosis. In recent years,
researchers have turned their attention to DL models to automate and accelerate
the diagnosis process. In the training of such DL models, filtering out the
non-regions-of-interest from the WSIs and adopting an artificial distribution
(usually, a balanced distribution) is a common trend. In our analysis, we show
that keeping the WSIs data in their usual distribution (which we call natural
distribution) for DL training produces fewer false positives (FPs) with
comparable false negatives (FNs) than the artificially-obtained balanced
distribution. We conduct an empirical comparative study with 10 random folds
for each distribution, comparing the resulting average performance levels in
terms of five different evaluation metrics. Experimental results show the
effectiveness of the natural distribution over the balanced one across all the
evaluation metrics.
</p>
<a href="http://arxiv.org/abs/2012.11684" target="_blank">arXiv:2012.11684</a> [<a href="http://arxiv.org/pdf/2012.11684" target="_blank">pdf</a>]

<h2>Encoding Syntactic Knowledge in Transformer Encoder for Intent Detection and Slot Filling. (arXiv:2012.11689v1 [cs.AI])</h2>
<h3>Jixuan Wang, Kai Wei, Martin Radfar, Weiwei Zhang, Clement Chung</h3>
<p>We propose a novel Transformer encoder-based architecture with syntactical
knowledge encoded for intent detection and slot filling. Specifically, we
encode syntactic knowledge into the Transformer encoder by jointly training it
to predict syntactic parse ancestors and part-of-speech of each token via
multi-task learning. Our model is based on self-attention and feed-forward
layers and does not require external syntactic information to be available at
inference time. Experiments show that on two benchmark datasets, our models
with only two Transformer encoder layers achieve state-of-the-art results.
Compared to the previously best performed model without pre-training, our
models achieve absolute F1 score and accuracy improvement of 1.59% and 0.85%
for slot filling and intent detection on the SNIPS dataset, respectively. Our
models also achieve absolute F1 score and accuracy improvement of 0.1% and
0.34% for slot filling and intent detection on the ATIS dataset, respectively,
over the previously best performed model. Furthermore, the visualization of the
self-attention weights illustrates the benefits of incorporating syntactic
information during training.
</p>
<a href="http://arxiv.org/abs/2012.11689" target="_blank">arXiv:2012.11689</a> [<a href="http://arxiv.org/pdf/2012.11689" target="_blank">pdf</a>]

<h2>Alleviating Noisy Data in Image Captioning with Cooperative Distillation. (arXiv:2012.11691v1 [cs.CV])</h2>
<h3>Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit Padhi, Mattia Rigotti, Jarret Ross, Yair Schiff</h3>
<p>Image captioning systems have made substantial progress, largely due to the
availability of curated datasets like Microsoft COCO or Vizwiz that have
accurate descriptions of their corresponding images. Unfortunately, scarce
availability of such cleanly labeled data results in trained algorithms
producing captions that can be terse and idiosyncratically specific to details
in the image. We propose a new technique, cooperative distillation that
combines clean curated datasets with the web-scale automatically extracted
captions of the Google Conceptual Captions dataset (GCC), which can have poor
descriptions of images, but is abundant in size and therefore provides a rich
vocabulary resulting in more expressive captions.
</p>
<a href="http://arxiv.org/abs/2012.11691" target="_blank">arXiv:2012.11691</a> [<a href="http://arxiv.org/pdf/2012.11691" target="_blank">pdf</a>]

<h2>Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge. (arXiv:2012.11696v1 [cs.CV])</h2>
<h3>Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit Padhi, Mattia Rigotti, Jarret Ross, Yair Schiff, Richard A. Young, Brian Belgodere</h3>
<p>Image captioning has recently demonstrated impressive progress largely owing
to the introduction of neural network algorithms trained on curated dataset
like MS-COCO. Often work in this field is motivated by the promise of
deployment of captioning systems in practical applications. However, the
scarcity of data and contexts in many competition datasets renders the utility
of systems trained on these datasets limited as an assistive technology in
real-world settings, such as helping visually impaired people navigate and
accomplish everyday tasks. This gap motivated the introduction of the novel
VizWiz dataset, which consists of images taken by the visually impaired and
captions that have useful, task-oriented information. In an attempt to help the
machine learning computer vision field realize its promise of producing
technologies that have positive social impact, the curators of the VizWiz
dataset host several competitions, including one for image captioning. This
work details the theory and engineering from our winning submission to the 2020
captioning competition. Our work provides a step towards improved assistive
image captioning systems.
</p>
<a href="http://arxiv.org/abs/2012.11696" target="_blank">arXiv:2012.11696</a> [<a href="http://arxiv.org/pdf/2012.11696" target="_blank">pdf</a>]

<h2>HDNET: Exploiting HD Maps for 3D Object Detection. (arXiv:2012.11704v1 [cs.CV])</h2>
<h3>Bin Yang, Ming Liang, Raquel Urtasun</h3>
<p>In this paper we show that High-Definition (HD) maps provide strong priors
that can boost the performance and robustness of modern 3D object detectors.
Towards this goal, we design a single stage detector that extracts geometric
and semantic features from the HD maps. As maps might not be available
everywhere, we also propose a map prediction module that estimates the map on
the fly from raw LiDAR data. We conduct extensive experiments on KITTI as well
as a large-scale 3D detection benchmark containing 1 million frames, and show
that the proposed map-aware detector consistently outperforms the
state-of-the-art in both mapped and un-mapped scenarios. Importantly the whole
framework runs at 20 frames per second.
</p>
<a href="http://arxiv.org/abs/2012.11704" target="_blank">arXiv:2012.11704</a> [<a href="http://arxiv.org/pdf/2012.11704" target="_blank">pdf</a>]

<h2>Taking Principles Seriously: A Hybrid Approach to Value Alignment. (arXiv:2012.11705v1 [cs.AI])</h2>
<h3>Tae Wan Kim, John Hooker, Thomas Donaldson</h3>
<p>An important step in the development of value alignment (VA) systems in AI is
understanding how VA can reflect valid ethical principles. We propose that
designers of VA systems incorporate ethics by utilizing a hybrid approach in
which both ethical reasoning and empirical observation play a role. This, we
argue, avoids committing the "naturalistic fallacy," which is an attempt to
derive "ought" from "is," and it provides a more adequate form of ethical
reasoning when the fallacy is not committed. Using quantified model logic, we
precisely formulate principles derived from deontological ethics and show how
they imply particular "test propositions" for any given action plan in an AI
rule base. The action plan is ethical only if the test proposition is
empirically true, a judgment that is made on the basis of empirical VA. This
permits empirical VA to integrate seamlessly with independently justified
ethical principles.
</p>
<a href="http://arxiv.org/abs/2012.11705" target="_blank">arXiv:2012.11705</a> [<a href="http://arxiv.org/pdf/2012.11705" target="_blank">pdf</a>]

<h2>Off-Policy Optimization of Portfolio Allocation Policies under Constraints. (arXiv:2012.11715v1 [cs.AI])</h2>
<h3>Nymisha Bandi, Theja Tulabandhula</h3>
<p>The dynamic portfolio optimization problem in finance frequently requires
learning policies that adhere to various constraints, driven by investor
preferences and risk. We motivate this problem of finding an allocation policy
within a sequential decision making framework and study the effects of: (a)
using data collected under previously employed policies, which may be
sub-optimal and constraint-violating, and (b) imposing desired constraints
while computing near-optimal policies with this data. Our framework relies on
solving a minimax objective, where one player evaluates policies via off-policy
estimators, and the opponent uses an online learning strategy to control
constraint violations. We extensively investigate various choices for
off-policy estimation and their corresponding optimization sub-routines, and
quantify their impact on computing constraint-aware allocation policies. Our
study shows promising results for constructing such policies when back-tested
on historical equities data, under various regimes of operation, dimensionality
and constraints.
</p>
<a href="http://arxiv.org/abs/2012.11715" target="_blank">arXiv:2012.11715</a> [<a href="http://arxiv.org/pdf/2012.11715" target="_blank">pdf</a>]

<h2>Social NCE: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2012.11717v1 [cs.LG])</h2>
<h3>Yuejiang Liu, Qi Yan, Alexandre Alahi</h3>
<p>Learning socially-aware motion representations is at the core of recent
advances in human trajectory forecasting and robot navigation in crowded
spaces. Yet existing methods often struggle to generalize to challenging
scenarios and even output unacceptable solutions (e.g., collisions). In this
work, we propose to address this issue via contrastive learning. Concretely, we
introduce a social contrastive loss that encourages the encoded motion
representation to preserve sufficient information for distinguishing a positive
future event from a set of negative ones. We explicitly draw these negative
samples based on our domain knowledge about socially unfavorable scenarios in
the multi-agent context. Experimental results show that the proposed method
consistently boosts the performance of previous trajectory forecasting,
behavioral cloning, and reinforcement learning algorithms in various settings.
Our method makes little assumptions about neural architecture designs, and
hence can be used as a generic way to incorporate negative data augmentation
into motion representation learning.
</p>
<a href="http://arxiv.org/abs/2012.11717" target="_blank">arXiv:2012.11717</a> [<a href="http://arxiv.org/pdf/2012.11717" target="_blank">pdf</a>]

<h2>Cross-Domain Latent Modulation for Variational Transfer Learning. (arXiv:2012.11727v1 [cs.LG])</h2>
<h3>Jinyong Hou, Jeremiah D. Deng, Stephen Cranefield, Xuejie Ding</h3>
<p>We propose a cross-domain latent modulation mechanism within a variational
autoencoders (VAE) framework to enable improved transfer learning. Our key idea
is to procure deep representations from one data domain and use it as
perturbation to the reparameterization of the latent variable in another
domain. Specifically, deep representations of the source and target domains are
first extracted by a unified inference model and aligned by employing gradient
reversal. Second, the learned deep representations are cross-modulated to the
latent encoding of the alternate domain. The consistency between the
reconstruction from the modulated latent encoding and the generation using deep
representation samples is then enforced in order to produce inter-class
alignment in the latent space. We apply the proposed model to a number of
transfer learning tasks including unsupervised domain adaptation and
image-toimage translation. Experimental results show that our model gives
competitive performance.
</p>
<a href="http://arxiv.org/abs/2012.11727" target="_blank">arXiv:2012.11727</a> [<a href="http://arxiv.org/pdf/2012.11727" target="_blank">pdf</a>]

<h2>A Fast Edge-Based Synchronizer for Tasks in Real-Time Artificial Intelligence Applications. (arXiv:2012.11731v1 [cs.AI])</h2>
<h3>Richard Olaniyan, Muthucumaru Maheswaran</h3>
<p>Real-time artificial intelligence (AI) applications mapped onto edge
computing need to perform data capture, process data, and device actuation
within given bounds while using the available devices. Task synchronization
across the devices is an important problem that affects the timely progress of
an AI application by determining the quality of the captured data, time to
process the data, and the quality of actuation. In this paper, we develop a
fast edge-based synchronization scheme that can time align the execution of
input-output tasks as well compute tasks. The primary idea of the fast
synchronizer is to cluster the devices into groups that are highly synchronized
in their task executions and statically determine few synchronization points
using a game-theoretic solver. The cluster of devices use a late notification
protocol to select the best point among the pre-computed synchronization points
to reach a time aligned task execution as quickly as possible. We evaluate the
performance of our synchronization scheme using trace-driven simulations and we
compare the performance with existing distributed synchronization schemes for
real-time AI application tasks. We implement our synchronization scheme and
compare its training accuracy and training time with other parameter server
synchronization frameworks.
</p>
<a href="http://arxiv.org/abs/2012.11731" target="_blank">arXiv:2012.11731</a> [<a href="http://arxiv.org/pdf/2012.11731" target="_blank">pdf</a>]

<h2>Predicting the Critical Number of Layers for Hierarchical Support Vector Regression. (arXiv:2012.11734v1 [cs.LG])</h2>
<h3>Ryan Mohr, Maria Fonoberova, Zlatko Drma&#x10d;, Iva Manojlovi&#x107;, Igor Mezi&#x107;</h3>
<p>Hierarchical support vector regression (HSVR) models a function from data as
a linear combination of SVR models at a range of scales, starting at a coarse
scale and moving to finer scales as the hierarchy continues. In the original
formulation of HSVR, there were no rules for choosing the depth of the model.
In this paper, we observe in a number of models a phase transition in the
training error -- the error remains relatively constant as layers are added,
until a critical scale is passed, at which point the training error drops close
to zero and remains nearly constant for added layers. We introduce a method to
predict this critical scale a priori with the prediction based on the support
of either a Fourier transform of the data or the Dynamic Mode Decomposition
(DMD) spectrum. This allows us to determine the required number of layers prior
to training any models.
</p>
<a href="http://arxiv.org/abs/2012.11734" target="_blank">arXiv:2012.11734</a> [<a href="http://arxiv.org/pdf/2012.11734" target="_blank">pdf</a>]

<h2>Cost-sensitive Semi-supervised Classification for Fraud Applications. (arXiv:2012.11743v1 [cs.LG])</h2>
<h3>Sulaf Elshaar, Samira Sadaoui</h3>
<p>This research explores Cost-Sensitive Learning (CSL) in the fraud detection
domain to decrease the fraud class's incorrect predictions and increase its
accuracy. Notably, we concentrate on shill bidding fraud that is challenging to
detect because the behavior of shill and legitimate bidders are similar. We
investigate CSL within the Semi-Supervised Classification (SSC) framework to
address the scarcity of labeled fraud data. Our paper is the first attempt to
integrate CSL with SSC for fraud detection. We adopt a meta-CSL approach to
manage the costs of misclassification errors, while SSC algorithms are trained
with imbalanced data. Using an actual shill bidding dataset, we assess the
performance of several hybrid models of CSL and SSC and then compare their
misclassification error and accuracy rates statistically. The most efficient
CSL+SSC model was able to detect 99% of fraudsters and with the lowest total
cost.
</p>
<a href="http://arxiv.org/abs/2012.11743" target="_blank">arXiv:2012.11743</a> [<a href="http://arxiv.org/pdf/2012.11743" target="_blank">pdf</a>]

<h2>Training DNNs in O(1) memory with MEM-DFA using Random Matrices. (arXiv:2012.11745v1 [cs.CV])</h2>
<h3>Tien Chu, Kamil Mykitiuk, Miron Szewczyk, Adam Wiktor, Zbigniew Wojna</h3>
<p>This work presents a method for reducing memory consumption to a constant
complexity when training deep neural networks. The algorithm is based on the
more biologically plausible alternatives of the backpropagation (BP): direct
feedback alignment (DFA) and feedback alignment (FA), which use random matrices
to propagate error. The proposed method, memory-efficient direct feedback
alignment (MEM-DFA), uses higher independence of layers in DFA and allows
avoiding storing at once all activation vectors, unlike standard BP, FA, and
DFA. Thus, our algorithm's memory usage is constant regardless of the number of
layers in a neural network. The method increases the computational cost only by
a constant factor of one extra forward pass.

The MEM-DFA, BP, FA, and DFA were evaluated along with their memory profiles
on MNIST and CIFAR-10 datasets on various neural network models. Our
experiments agree with our theoretical results and show a significant decrease
in the memory cost of MEM-DFA compared to the other algorithms.
</p>
<a href="http://arxiv.org/abs/2012.11745" target="_blank">arXiv:2012.11745</a> [<a href="http://arxiv.org/pdf/2012.11745" target="_blank">pdf</a>]

<h2>Informer: Transformer Likes Informed Attention. (arXiv:2012.11747v1 [cs.LG])</h2>
<h3>Ruining He, Anirudh Ravula, Bhargav Kanagal, Joshua Ainslie</h3>
<p>Transformer is the backbone of modern NLP models. In this paper, we propose
Informer, a simple architecture that significantly outperforms canonical
Transformers on a spectrum of tasks including Masked Language Modeling, GLUE,
and SQuAD. Qualitatively, Informer is easy to implement and requires minimal
hyper-parameter tuning. It also stabilizes training and leads to models with
sparser attentions. Code will be open-sourced upon paper acceptance.
</p>
<a href="http://arxiv.org/abs/2012.11747" target="_blank">arXiv:2012.11747</a> [<a href="http://arxiv.org/pdf/2012.11747" target="_blank">pdf</a>]

<h2>Contraband Materials Detection Within Volumetric 3D Computed Tomography Baggage Security Screening Imagery. (arXiv:2012.11753v1 [cs.CV])</h2>
<h3>Qian Wang, Toby P. Breckon</h3>
<p>Automatic prohibited object detection within 2D/3D X-ray Computed Tomography
(CT) has been studied in literature to enhance the aviation security screening
at checkpoints. Deep Convolutional Neural Networks (CNN) have demonstrated
superior performance in 2D X-ray imagery. However, there exists very limited
proof of how deep neural networks perform in materials detection within
volumetric 3D CT baggage screening imagery. We attempt to close this gap by
applying Deep Neural Networks in 3D contraband substance detection based on
their material signatures. Specifically, we formulate it as a 3D semantic
segmentation problem to identify material types for all voxels based on which
contraband materials can be detected. To this end, we firstly investigate 3D
CNN based semantic segmentation algorithms such as 3D U-Net and its variants.
In contrast to the original dense representation form of volumetric 3D CT data,
we propose to convert the CT volumes into sparse point clouds which allows the
use of point cloud processing approaches such as PointNet++ towards more
efficient processing. Experimental results on a publicly available dataset (NEU
ATR) demonstrate the effectiveness of both 3D U-Net and PointNet++ in materials
detection in 3D CT imagery for baggage security screening.
</p>
<a href="http://arxiv.org/abs/2012.11753" target="_blank">arXiv:2012.11753</a> [<a href="http://arxiv.org/pdf/2012.11753" target="_blank">pdf</a>]

<h2>Bounding the Complexity of Formally Verifying Neural Networks: A Geometric Approach. (arXiv:2012.11761v1 [cs.LG])</h2>
<h3>James Ferlez, Yasser Shoukry</h3>
<p>In this paper, we consider the computational complexity of formally verifying
the input/output behavior of Rectified Linear Unit (ReLU) Neural Networks
(NNs): that is we consider the complexity of determining whether the output of
a NN lies in a specific convex polytopic region (in its range) whenever its
input lies in a specific polytopic region (in its domain). Specifically, we
show that for two different NN architectures -- shallow NNs and Two-Level
Lattice (TLL) NNs -- the verification problem with polytopic constraints is
polynomial in the number of neurons in the NN to be verified, when all other
aspects of the verification problem held fixed. We achieve these complexity
results by exhibiting an explicit verification algorithm for each type of
architecture. Nevertheless, both algorithms share a commonality in structure.
First, they efficiently translate the NN parameters into a partitioning of the
NN's input space by means of hyperplanes; this has the effect of partitioning
the original verification problem into sub-verification problems derived from
the geometry of the NN itself. These partitionings have two further important
properties. First, the number of these hyperplanes is polynomially related to
the number of neurons, and hence so is the number of sub-verification problems.
Second, each of the subproblems is solvable in polynomial time by means of a
Linear Program (LP). Thus, to attain an overall polynomial time algorithm for
the original verification problem, it is only necessary to enumerate these
subproblems in polynomial time. For this, we also contribute a novel algorithm
to enumerate the regions in a hyperplane arrangement in polynomial time; our
algorithm is based on a poset ordering of the regions for which poset
successors are polynomially easy to compute.
</p>
<a href="http://arxiv.org/abs/2012.11761" target="_blank">arXiv:2012.11761</a> [<a href="http://arxiv.org/pdf/2012.11761" target="_blank">pdf</a>]

<h2>Deep Multi-attribute Graph Representation Learning on Protein Structures. (arXiv:2012.11762v1 [cs.LG])</h2>
<h3>Tian Xia, Wei-Shinn Ku</h3>
<p>Graphs as a type of data structure have recently attracted significant
attention. Representation learning of geometric graphs has achieved great
success in many fields including molecular, social, and financial networks. It
is natural to present proteins as graphs in which nodes represent the residues
and edges represent the pairwise interactions between residues. However, 3D
protein structures have rarely been studied as graphs directly. The challenges
include: 1) Proteins are complex macromolecules composed of thousands of atoms
making them much harder to model than micro-molecules. 2) Capturing the
long-range pairwise relations for protein structure modeling remains
under-explored. 3) Few studies have focused on learning the different
attributes of proteins together. To address the above challenges, we propose a
new graph neural network architecture to represent the proteins as 3D graphs
and predict both distance geometric graph representation and dihedral geometric
graph representation together. This gives a significant advantage because this
network opens a new path from the sequence to structure. We conducted extensive
experiments on four different datasets and demonstrated the effectiveness of
the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.11762" target="_blank">arXiv:2012.11762</a> [<a href="http://arxiv.org/pdf/2012.11762" target="_blank">pdf</a>]

<h2>Self-Progressing Robust Training. (arXiv:2012.11769v1 [cs.LG])</h2>
<h3>Minhao Cheng, Pin-Yu Chen, Sijia Liu, Shiyu Chang, Cho-Jui Hsieh, Payel Das</h3>
<p>Enhancing model robustness under new and even adversarial environments is a
crucial milestone toward building trustworthy machine learning systems. Current
robust training methods such as adversarial training explicitly uses an
"attack" (e.g., $\ell_{\infty}$-norm bounded perturbation) to generate
adversarial examples during model training for improving adversarial
robustness. In this paper, we take a different perspective and propose a new
framework called SPROUT, self-progressing robust training. During model
training, SPROUT progressively adjusts training label distribution via our
proposed parametrized label smoothing technique, making training free of attack
generation and more scalable. We also motivate SPROUT using a general
formulation based on vicinity risk minimization, which includes many robust
training methods as special cases. Compared with state-of-the-art adversarial
training methods (PGD-l_inf and TRADES) under l_inf-norm bounded attacks and
various invariance tests, SPROUT consistently attains superior performance and
is more scalable to large neural networks. Our results shed new light on
scalable, effective and attack-independent robust training methods.
</p>
<a href="http://arxiv.org/abs/2012.11769" target="_blank">arXiv:2012.11769</a> [<a href="http://arxiv.org/pdf/2012.11769" target="_blank">pdf</a>]

<h2>Power-SLIC: Diagram-based superpixel generation. (arXiv:2012.11772v1 [cs.CV])</h2>
<h3>Maximilian Fiedler, Andreas Alpers</h3>
<p>Superpixel algorithms, which group pixels similar in color and other
low-level properties, are increasingly used for pre-processing in image
segmentation. Commonly important criteria for the computation of superpixels
are boundary adherence, speed, and regularity.

Boundary adherence and regularity are typically contradictory goals. Most
recent algorithms have focused on improving boundary adherence. Motivated by
improving superpixel regularity, we propose a diagram-based superpixel
generation method called Power-SLIC.

On the BSDS500 data set, Power-SLIC outperforms other state-of-the-art
algorithms in terms of compactness and boundary precision, and its boundary
adherence is the most robust against varying levels of Gaussian noise. In terms
of speed, Power-SLIC is competitive with SLIC.
</p>
<a href="http://arxiv.org/abs/2012.11772" target="_blank">arXiv:2012.11772</a> [<a href="http://arxiv.org/pdf/2012.11772" target="_blank">pdf</a>]

<h2>Differentially Private Synthetic Medical Data Generation using Convolutional GANs. (arXiv:2012.11774v1 [cs.LG])</h2>
<h3>Amirsina Torfi, Edward A. Fox, Chandan K. Reddy</h3>
<p>Deep learning models have demonstrated superior performance in several
application problems, such as image classification and speech processing.
However, creating a deep learning model using health record data requires
addressing certain privacy challenges that bring unique concerns to researchers
working in this domain. One effective way to handle such private data issues is
to generate realistic synthetic data that can provide practically acceptable
data quality and correspondingly the model performance. To tackle this
challenge, we develop a differentially private framework for synthetic data
generation using R\'enyi differential privacy. Our approach builds on
convolutional autoencoders and convolutional generative adversarial networks to
preserve some of the critical characteristics of the generated synthetic data.
In addition, our model can also capture the temporal information and feature
correlations that might be present in the original data. We demonstrate that
our model outperforms existing state-of-the-art models under the same privacy
budget using several publicly available benchmark medical datasets in both
supervised and unsupervised settings.
</p>
<a href="http://arxiv.org/abs/2012.11774" target="_blank">arXiv:2012.11774</a> [<a href="http://arxiv.org/pdf/2012.11774" target="_blank">pdf</a>]

<h2>MailLeak: Obfuscation-Robust Character Extraction Using Transfer Learning. (arXiv:2012.11775v1 [cs.LG])</h2>
<h3>Wei Wang, Emily Sallenback, Zeyu Ning, Hugues Nelson Iradukunda, Wenxi Lu, Qingquan Zhang, Ting Zhu</h3>
<p>The following work presents a new algorithm for character recognition from
obfuscated images. The presented method is an example of a potential threat to
current postal services. This paper both analyzes the efficiency of the given
algorithm and suggests countermeasures to prevent such threats from occurring.
</p>
<a href="http://arxiv.org/abs/2012.11775" target="_blank">arXiv:2012.11775</a> [<a href="http://arxiv.org/pdf/2012.11775" target="_blank">pdf</a>]

<h2>SERV-CT: A disparity dataset from CT for validation of endoscopic 3D reconstruction. (arXiv:2012.11779v1 [cs.CV])</h2>
<h3>P.J. &quot;Eddie&#x27;&#x27; Edwards, Dimitris Psychogyios, Stefanie Speidel, Lena Maier-Hein, Danail Stoyanov</h3>
<p>In computer vision, reference datasets have been highly successful in
promoting algorithmic development in stereo reconstruction. Surgical scenes
gives rise to specific problems, including the lack of clear corner features,
highly specular surfaces and the presence of blood and smoke. Publicly
available datasets have been produced using CT and either phantom images or
biological tissue samples covering a relatively small region of the endoscope
field-of-view. We present a stereo-endoscopic reconstruction validation dataset
based on CT (SERV-CT). Two {\it ex vivo} small porcine full torso cadavers were
placed within the view of the endoscope with both the endoscope and target
anatomy visible in the CT scan. Orientation of the endoscope was manually
aligned to the stereoscopic view. Reference disparities and occlusions were
calculated for 8 stereo pairs from each sample. For the second sample an RGB
surface was acquired to aid alignment of smooth, featureless surfaces. Repeated
manual alignments showed an RMS disparity accuracy of ~2 pixels and a depth
accuracy of ~2mm. The reference dataset includes endoscope image pairs with
corresponding calibration, disparities, depths and occlusions covering the
majority of the endoscopic image and a range of tissue types. Smooth specular
surfaces and images with significant variation of depth are included. We
assessed the performance of various stereo algorithms from online available
repositories. There is a significant variation between algorithms, highlighting
some of the challenges of surgical endoscopic images. The SERV-CT dataset
provides an easy to use stereoscopic validation for surgical applications with
smooth reference disparities and depths with coverage over the majority of the
endoscopic images. This complements existing resources well and we hope will
aid the development of surgical endoscopic anatomical reconstruction
algorithms.
</p>
<a href="http://arxiv.org/abs/2012.11779" target="_blank">arXiv:2012.11779</a> [<a href="http://arxiv.org/pdf/2012.11779" target="_blank">pdf</a>]

<h2>Towards an Automatic System for Extracting Planar Orientations from Software Generated Point Clouds. (arXiv:2012.11780v1 [cs.LG])</h2>
<h3>J. Kissi-Ameyaw, K. McIsaac, X. Wang, G. R. Osinski</h3>
<p>In geology, a key activity is the characterisation of geological structures
(surface formation topology and rock units) using Planar Orientation
measurements such as Strike, Dip and Dip Direction. In general these
measurements are collected manually using basic equipment; usually a
compass/clinometer and a backboard, recorded on a map by hand. Various
computing techniques and technologies, such as Lidar, have been utilised in
order to automate this process and update the collection paradigm for these
types of measurements. Techniques such as Structure from Motion (SfM)
reconstruct of scenes and objects by generating a point cloud from input
images, with detailed reconstruction possible on the decimetre scale. SfM-type
techniques provide advantages in areas of cost and usability in more varied
environmental conditions, while sacrificing the extreme levels of data
fidelity. Here is presented a methodology of data acquisition and a Machine
Learning-based software system: GeoStructure, developed to automate the
measurement of orientation measurements. Rather than deriving measurements
using a method applied to the input images, such as the Hough Transform, this
method takes measurements directly from the reconstructed point cloud surfaces.
Point cloud noise is mitigated using a Mahalanobis distance implementation.
Significant structure is characterised using a k-nearest neighbour region
growing algorithm, and final surface orientations are quantified using the
plane, and normal direction cosines.
</p>
<a href="http://arxiv.org/abs/2012.11780" target="_blank">arXiv:2012.11780</a> [<a href="http://arxiv.org/pdf/2012.11780" target="_blank">pdf</a>]

<h2>Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization. (arXiv:2012.11782v1 [cs.LG])</h2>
<h3>Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike, Kento Uemura, Hiroki Arimura</h3>
<p>Post-hoc explanation methods for machine learning models have been widely
used to support decision-making. One of the popular methods is Counterfactual
Explanation (CE), which provides a user with a perturbation vector of features
that alters the prediction result. Given a perturbation vector, a user can
interpret it as an "action" for obtaining one's desired decision result. In
practice, however, showing only a perturbation vector is often insufficient for
users to execute the action. The reason is that if there is an asymmetric
interaction among features, such as causality, the total cost of the action is
expected to depend on the order of changing features. Therefore, practical CE
methods are required to provide an appropriate order of changing features in
addition to a perturbation vector. For this purpose, we propose a new framework
called Ordered Counterfactual Explanation (OrdCE). We introduce a new objective
function that evaluates a pair of an action and an order based on feature
interaction. To extract an optimal pair, we propose a mixed-integer linear
optimization approach with our objective function. Numerical experiments on
real datasets demonstrated the effectiveness of our OrdCE in comparison with
unordered CE methods.
</p>
<a href="http://arxiv.org/abs/2012.11782" target="_blank">arXiv:2012.11782</a> [<a href="http://arxiv.org/pdf/2012.11782" target="_blank">pdf</a>]

<h2>Can I Still Trust You?: Understanding the Impact of Distribution Shifts on Algorithmic Recourses. (arXiv:2012.11788v1 [cs.LG])</h2>
<h3>Kaivalya Rawal, Ece Kamar, Himabindu Lakkaraju</h3>
<p>As predictive models are being increasingly deployed to make a variety of
consequential decisions ranging from hiring decisions to loan approvals, there
is growing emphasis on designing algorithms that can provide reliable recourses
to affected individuals. To this end, several recourse generation algorithms
have been proposed in recent literature. However, there is little to no work on
systematically assessing if these algorithms are actually generating recourses
that are reliable. In this work, we assess the reliability of algorithmic
recourses through the lens of distribution shifts i.e., we empirically and
theoretically study if and what kind of recourses generated by state-of-the-art
algorithms are robust to distribution shifts. To the best of our knowledge,
this work makes the first attempt at addressing this critical question. We
experiment with multiple synthetic and real world datasets capturing different
kinds of distribution shifts including temporal shifts, geospatial shifts, and
shifts due to data corrections. Our results demonstrate that all the
aforementioned distribution shifts could potentially invalidate the recourses
generated by state-of-the-art algorithms. In addition, we also find that
recourse interventions themselves may cause distribution shifts which in turn
invalidate previously prescribed recourses. Our theoretical results establish
that the recourses (counterfactuals) that are close to the model decision
boundary are more likely to be invalidated upon model updation. However,
state-of-the-art algorithms tend to prefer exactly these recourses because
their cost functions penalize recourses (counterfactuals) that require large
modifications to the original instance. Our findings not only expose
fundamental flaws in recourse finding strategies but also pave new way for
rethinking the design and development of recourse generation algorithms.
</p>
<a href="http://arxiv.org/abs/2012.11788" target="_blank">arXiv:2012.11788</a> [<a href="http://arxiv.org/pdf/2012.11788" target="_blank">pdf</a>]

<h2>Dynamic penalty function approach for constraints handling in reinforcement learning. (arXiv:2012.11790v1 [cs.LG])</h2>
<h3>Haeun Yoo, Victor M. Zavala, Jay H. Lee</h3>
<p>Reinforcement learning (RL) is attracting attentions as an effective way to
solve sequential optimization problems involving high dimensional state/action
space and stochastic uncertainties. Many of such problems involve constraints
expressed by inequalities. This study focuses on using RL to solve such
constrained optimal control problems. Most of RL application studies have
considered inequality constraints as soft constraints by adding penalty terms
for violating the constraints to the reward function. However, while training
neural networks to represent the value (or Q) function, a key step in RL, one
can run into computational issues caused by the sharp change in the function
value at the constraint boundary due to the large penalty imposed. This
difficulty during training can lead to convergence problems and ultimately poor
closed-loop performance. To address this problem, this study suggests the use
of a dynamic penalty function which gradually and systematically increases the
penalty factor during training as the iteration episodes proceed. First, we
examined the ability of a neural network to represent an artificial value
function when uniform, linear, or dynamic penalty functions are added to
prevent constraint violation. The agent trained by a Deep Q Network (DQN)
algorithm with the dynamic penalty function approach was compared with agents
with other constant penalty functions in a simple vehicle control problem.
Results show that the dynamic penalty approach can improve the neural network's
approximation accuracy and that brings faster convergence to a solution closer
to the optimal solution.
</p>
<a href="http://arxiv.org/abs/2012.11790" target="_blank">arXiv:2012.11790</a> [<a href="http://arxiv.org/pdf/2012.11790" target="_blank">pdf</a>]

<h2>Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning. (arXiv:2012.11792v1 [cs.AI])</h2>
<h3>Mehrdad Zakershahrak, Samira Ghodratnama</h3>
<p>Providing explanations is considered an imperative ability for an AI agent in
a human-robot teaming framework. The right explanation provides the rationale
behind an AI agent's decision making. However, to maintain the human teammate's
cognitive demand to comprehend the provided explanations, prior works have
focused on providing explanations in a specific order or intertwining the
explanation generation with plan execution. These approaches, however, do not
consider the degree of details they share throughout the provided explanations.
In this work, we argue that the explanations, especially the complex ones,
should be abstracted to be aligned with the level of details the teammate
desires to maintain the cognitive load of the recipient. The challenge here is
to learn a hierarchical model of explanations and details the agent requires to
yield the explanations as an objective. Moreover, the agent needs to follow a
high-level plan in a task domain such that the agent can transfer learned
teammate preferences to a scenario where lower-level control policies are
different, while the high-level plan remains the same. Results confirmed our
hypothesis that the process of understanding an explanation was a dynamic
hierarchical process. The human preference that reflected this aspect
corresponded exactly to creating and employing abstraction for knowledge
assimilation hidden deeper in our cognitive process. We showed that
hierarchical explanations achieved better task performance and behavior
interpretability while reduced cognitive load. These results shed light on
designing explainable agents utilizing reinforcement learning and planning
across various domains.
</p>
<a href="http://arxiv.org/abs/2012.11792" target="_blank">arXiv:2012.11792</a> [<a href="http://arxiv.org/pdf/2012.11792" target="_blank">pdf</a>]

<h2>Multiple-Perspective Clustering of Passive Wi-Fi Sensing Trajectory Data. (arXiv:2012.11796v1 [cs.LG])</h2>
<h3>Zann Koh, Yuren Zhou, Billy Pik Lik Lau, Chau Yuen, Bige Tuncer, Keng Hua Chong</h3>
<p>Information about the spatiotemporal flow of humans within an urban context
has a wide plethora of applications. Currently, although there are many
different approaches to collect such data, there lacks a standardized framework
to analyze it. The focus of this paper is on the analysis of the data collected
through passive Wi-Fi sensing, as such passively collected data can have a wide
coverage at low cost. We propose a systematic approach by using unsupervised
machine learning methods, namely k-means clustering and hierarchical
agglomerative clustering (HAC) to analyze data collected through such a passive
Wi-Fi sniffing method. We examine three aspects of clustering of the data,
namely by time, by person, and by location, and we present the results obtained
by applying our proposed approach on a real-world dataset collected over five
months.
</p>
<a href="http://arxiv.org/abs/2012.11796" target="_blank">arXiv:2012.11796</a> [<a href="http://arxiv.org/pdf/2012.11796" target="_blank">pdf</a>]

<h2>Time Series Domain Adaptation via Sparse Associative Structure Alignment. (arXiv:2012.11797v1 [cs.LG])</h2>
<h3>Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, Zhuozhang Li, Xiaoyan Yang, Zhenjie Zhang</h3>
<p>Domain adaptation on time series data is an important but challenging task.
Most of the existing works in this area are based on the learning of the
domain-invariant representation of the data with the help of restrictions like
MMD. However, such extraction of the domain-invariant representation is a
non-trivial task for time series data, due to the complex dependence among the
timestamps. In detail, in the fully dependent time series, a small change of
the time lags or the offsets may lead to difficulty in the domain invariant
extraction. Fortunately, the stability of the causality inspired us to explore
the domain invariant structure of the data. To reduce the difficulty in the
discovery of causal structure, we relax it to the sparse associative structure
and propose a novel sparse associative structure alignment model for domain
adaptation. First, we generate the segment set to exclude the obstacle of
offsets. Second, the intra-variables and inter-variables sparse attention
mechanisms are devised to extract associative structure time-series data with
considering time lags. Finally, the associative structure alignment is used to
guide the transfer of knowledge from the source domain to the target one.
Experimental studies not only verify the good performance of our methods on
three real-world datasets but also provide some insightful discoveries on the
transferred knowledge.
</p>
<a href="http://arxiv.org/abs/2012.11797" target="_blank">arXiv:2012.11797</a> [<a href="http://arxiv.org/pdf/2012.11797" target="_blank">pdf</a>]

<h2>Modeling Deep Learning Based Privacy Attacks on Physical Mail. (arXiv:2012.11803v1 [cs.CV])</h2>
<h3>Bingyao Huang, Ruyi Lian, Dimitris Samaras, Haibin Ling</h3>
<p>Mail privacy protection aims to prevent unauthorized access to hidden content
within an envelope since normal paper envelopes are not as safe as we think. In
this paper, for the first time, we show that with a well designed deep learning
model, the hidden content may be largely recovered without opening the
envelope. We start by modeling deep learning-based privacy attacks on physical
mail content as learning the mapping from the camera-captured envelope front
face image to the hidden content, then we explicitly model the mapping as a
combination of perspective transformation, image dehazing and denoising using a
deep convolutional neural network, named Neural-STE (See-Through-Envelope). We
show experimentally that hidden content details, such as texture and image
structure, can be clearly recovered. Finally, our formulation and model allow
us to design envelopes that can counter deep learning-based privacy attacks on
physical mail.
</p>
<a href="http://arxiv.org/abs/2012.11803" target="_blank">arXiv:2012.11803</a> [<a href="http://arxiv.org/pdf/2012.11803" target="_blank">pdf</a>]

<h2>To Talk or to Work: Flexible Communication Compression for Energy Efficient Federated Learning over Heterogeneous Mobile Edge Devices. (arXiv:2012.11804v1 [cs.LG])</h2>
<h3>Liang Li, Dian Shi, Ronghui Hou, Hui Li, Miao Pan, Zhu Han</h3>
<p>Recent advances in machine learning, wireless communication, and mobile
hardware technologies promisingly enable federated learning (FL) over massive
mobile edge devices, which opens new horizons for numerous intelligent mobile
applications. Despite the potential benefits, FL imposes huge communication and
computation burdens on participating devices due to periodical global
synchronization and continuous local training, raising great challenges to
battery constrained mobile devices. In this work, we target at improving the
energy efficiency of FL over mobile edge networks to accommodate heterogeneous
participating devices without sacrificing the learning performance. To this
end, we develop a convergence-guaranteed FL algorithm enabling flexible
communication compression. Guided by the derived convergence bound, we design a
compression control scheme to balance the energy consumption of local computing
(i.e., "working") and wireless communication (i.e., "talking") from the
long-term learning perspective. In particular, the compression parameters are
elaborately chosen for FL participants adapting to their computing and
communication environments. Extensive simulations are conducted using various
datasets to validate our theoretical analysis, and the results also demonstrate
the efficacy of the proposed scheme in energy saving.
</p>
<a href="http://arxiv.org/abs/2012.11804" target="_blank">arXiv:2012.11804</a> [<a href="http://arxiv.org/pdf/2012.11804" target="_blank">pdf</a>]

<h2>Graph and Temporal Convolutional Networks for 3D Multi-person Pose Estimation in Monocular Videos. (arXiv:2012.11806v1 [cs.CV])</h2>
<h3>Yu Cheng, Bo Wang, Bo Yang, Robby T. Tan</h3>
<p>Despite the recent progress, 3D multi-person pose estimation from monocular
videos is still challenging due to the commonly encountered problem of missing
information caused by occlusion, partially out-of-frame target persons, and
inaccurate person detection.To tackle this problem, we propose a novel
framework integrating graph convolutional networks (GCNs) and temporal
convolutional networks (TCNs) to robustly estimate camera-centric multi-person
3D poses that do not require camera parameters. In particular, we introduce a
human-joint GCN, which unlike the existing GCN, is based on a directed graph
that employs the 2D pose estimator's confidence scores to improve the pose
estimation results. We also introduce a human-bone GCN, which models the bone
connections and provides more information beyond human joints. The two GCNs
work together to estimate the spatial frame-wise 3D poses and can make use of
both visible joint and bone information in the target frame to estimate the
occluded or missing human-part information. To further refine the 3D pose
estimation, we use our temporal convolutional networks (TCNs) to enforce the
temporal and human-dynamics constraints. We use a joint-TCN to estimate
person-centric 3D poses across frames, and propose a velocity-TCN to estimate
the speed of 3D joints to ensure the consistency of the 3D pose estimation in
consecutive frames. Finally, to estimate the 3D human poses for multiple
persons, we propose a root-TCN that estimates camera-centric 3D poses without
requiring camera parameters. Quantitative and qualitative evaluations
demonstrate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.11806" target="_blank">arXiv:2012.11806</a> [<a href="http://arxiv.org/pdf/2012.11806" target="_blank">pdf</a>]

<h2>Learning Disentangled Semantic Representation for Domain Adaptation. (arXiv:2012.11807v1 [cs.CV])</h2>
<h3>Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, Zhifeng Hao</h3>
<p>Domain adaptation is an important but challenging task. Most of the existing
domain adaptation methods struggle to extract the domain-invariant
representation on the feature space with entangling domain information and
semantic information. Different from previous efforts on the entangled feature
space, we aim to extract the domain invariant semantic information in the
latent disentangled semantic representation (DSR) of the data. In DSR, we
assume the data generation process is controlled by two independent sets of
variables, i.e., the semantic latent variables and the domain latent variables.
Under the above assumption, we employ a variational auto-encoder to reconstruct
the semantic latent variables and domain latent variables behind the data. We
further devise a dual adversarial network to disentangle these two sets of
reconstructed latent variables. The disentangled semantic latent variables are
finally adapted across the domains. Experimental studies testify that our model
yields state-of-the-art performance on several domain adaptation benchmark
datasets.
</p>
<a href="http://arxiv.org/abs/2012.11807" target="_blank">arXiv:2012.11807</a> [<a href="http://arxiv.org/pdf/2012.11807" target="_blank">pdf</a>]

<h2>Progressive One-shot Human Parsing. (arXiv:2012.11810v1 [cs.CV])</h2>
<h3>Haoyu He, Jing Zhang, Bhavani Thuraisingham, Dacheng Tao</h3>
<p>Prior human parsing models are limited to parsing humans into classes
pre-defined in the training data, which is not flexible to generalize to unseen
classes, e.g., new clothing in fashion analysis. In this paper, we propose a
new problem named one-shot human parsing (OSHP) that requires to parse human
into an open set of reference classes defined by any single reference example.
During training, only base classes defined in the training set are exposed,
which can overlap with part of reference classes. In this paper, we devise a
novel Progressive One-shot Parsing network (POPNet) to address two critical
challenges , i.e., testing bias and small sizes. POPNet consists of two
collaborative metric learning modules named Attention Guidance Module and
Nearest Centroid Module, which can learn representative prototypes for base
classes and quickly transfer the ability to unseen classes during testing,
thereby reducing testing bias. Moreover, POPNet adopts a progressive human
parsing framework that can incorporate the learned knowledge of parent classes
at the coarse granularity to help recognize the descendant classes at the fine
granularity, thereby handling the small sizes issue. Experiments on the ATR-OS
benchmark tailored for OSHP demonstrate POPNet outperforms other representative
one-shot segmentation models by large margins and establishes a strong
baseline. Source code can be found at
https://github.com/Charleshhy/One-shot-Human-Parsing.
</p>
<a href="http://arxiv.org/abs/2012.11810" target="_blank">arXiv:2012.11810</a> [<a href="http://arxiv.org/pdf/2012.11810" target="_blank">pdf</a>]

<h2>Subject-independent Human Pose Image Construction with Commodity Wi-Fi. (arXiv:2012.11812v1 [cs.CV])</h2>
<h3>Shuang Zhou, Lingchao Guo, Zhaoming Lu, Xiangming Wen, Wei Zheng, Yiming Wang</h3>
<p>Recently, commodity Wi-Fi devices have been shown to be able to construct
human pose images, i.e., human skeletons, as fine-grained as cameras. Existing
papers achieve good results when constructing the images of subjects who are in
the prior training samples. However, the performance drops when it comes to new
subjects, i.e., the subjects who are not in the training samples. This paper
focuses on solving the subject-generalization problem in human pose image
construction. To this end, we define the subject as the domain. Then we design
a Domain-Independent Neural Network (DINN) to extract subject-independent
features and convert them into fine-grained human pose images. We also propose
a novel training method to train the DINN and it has no re-training overhead
comparing with the domain-adversarial approach. We build a prototype system and
experimental results demonstrate that our system can construct fine-grained
human pose images of new subjects with commodity Wi-Fi in both the visible and
through-wall scenarios, which shows the effectiveness and the
subject-generalization ability of our model.
</p>
<a href="http://arxiv.org/abs/2012.11812" target="_blank">arXiv:2012.11812</a> [<a href="http://arxiv.org/pdf/2012.11812" target="_blank">pdf</a>]

<h2>Molecular CT: Unifying Geometry and Representation Learning for Molecules at Different Scales. (arXiv:2012.11816v1 [cs.LG])</h2>
<h3>Jun Zhang, Yaqiang Zhou, Yao-Kun Lei, Yi Isaac Yang, Yi Qin Gao</h3>
<p>Deep learning is changing many areas in molecular physics, and it has shown
great potential to deliver new solutions to challenging molecular modeling
problems. Along with this trend arises the increasing demand of expressive and
versatile neural network architectures which are compatible with molecular
systems. A new deep neural network architecture, Molecular Configuration
Transformer (Molecular CT), is introduced for this purpose. Molecular CT is
composed of a relation-aware encoder module and a computationally universal
geometry learning unit, thus able to account for the relational constraints
between particles meanwhile scalable to different particle numbers and
invariant w.r.t. the trans-rotational transforms. The computational efficiency
and universality makes Molecular CT versatile for a variety of molecular
learning scenarios and especially appealing for transferable representation
learning across different molecular systems. As examples, we show that
Molecular CT enables representational learning for molecular systems at
different scales, and achieves comparable or improved results on common
benchmarks using a more light-weighted structure compared to baseline models.
</p>
<a href="http://arxiv.org/abs/2012.11816" target="_blank">arXiv:2012.11816</a> [<a href="http://arxiv.org/pdf/2012.11816" target="_blank">pdf</a>]

<h2>Fast and Robust Localization of Surgical Array using Kalman Filter. (arXiv:2012.11819v1 [cs.RO])</h2>
<h3>Md Ashikuzzaman, Noushin Jafarpisheh, Sunil Rottoo, Pierre Brisson, Hassan Rivaz</h3>
<p>Intraoperative tracking of surgical instruments is an inevitable task of
computer-assisted surgery. An optical tracking system often fails to precisely
reconstruct the dynamic location and pose of a surgical tool due to the
acquisition noise and measurement variance. Embedding a Kalman Filter (KF) or
any of its extensions such as extended and unscented Kalman filters with the
optical tracker resolves this issue by reducing the estimation variance and
regularizing the temporal behavior. However, the current rigid-body KF
implementations are computationally burdensome and hence, takes long execution
time which hinders real-time surgical tracking. This paper introduces a fast
and computationally efficient implementation of linear KF to improve the
measurement accuracy of an optical tracking system with high temporal
resolution. Instead of the surgical tool as a whole, our KF framework tracks
each individual fiducial mounted on it using a Newtonian model. In addition to
simulated dataset, we validate our technique against real data obtained from a
high frame-rate commercial optical tracking system. The proposed KF framework
substantially stabilizes the tracking behavior in all of our experiments and
reduces the mean-squared error (MSE) from the order of $10^{-2}$ $mm^{2}$ to
$10^{-4}$ $mm^{2}$.
</p>
<a href="http://arxiv.org/abs/2012.11819" target="_blank">arXiv:2012.11819</a> [<a href="http://arxiv.org/pdf/2012.11819" target="_blank">pdf</a>]

<h2>NetReAct: Interactive Learning for Network Summarization. (arXiv:2012.11821v1 [cs.LG])</h2>
<h3>Sorour E. Amiri, Bijaya Adhikari, John Wenskovitch, Alexander Rodriguez, Michelle Dowling, Chris North, B. Aditya Prakash</h3>
<p>Generating useful network summaries is a challenging and important problem
with several applications like sensemaking, visualization, and compression.
However, most of the current work in this space do not take human feedback into
account while generating summaries. Consider an intelligence analysis scenario,
where the analyst is exploring a similarity network between documents. The
analyst can express her agreement/disagreement with the visualization of the
network summary via iterative feedback, e.g. closing or moving documents
("nodes") together. How can we use this feedback to improve the network summary
quality? In this paper, we present NetReAct, a novel interactive network
summarization algorithm which supports the visualization of networks induced by
text corpora to perform sensemaking. NetReAct incorporates human feedback with
reinforcement learning to summarize and visualize document networks. Using
scenarios from two datasets, we show how NetReAct is successful in generating
high-quality summaries and visualizations that reveal hidden patterns better
than other non-trivial baselines.
</p>
<a href="http://arxiv.org/abs/2012.11821" target="_blank">arXiv:2012.11821</a> [<a href="http://arxiv.org/pdf/2012.11821" target="_blank">pdf</a>]

<h2>Dual-encoder Bidirectional Generative Adversarial Networks for Anomaly Detection. (arXiv:2012.11834v1 [cs.LG])</h2>
<h3>Teguh Budianto, Tomohiro Nakai, Kazunori Imoto, Takahiro Takimoto, Kosuke Haruki</h3>
<p>Generative adversarial networks (GANs) have shown promise for various
problems including anomaly detection. When anomaly detection is performed using
GAN models that learn only the features of normal data samples, data that are
not similar to normal data are detected as abnormal samples. The present
approach is developed by employing a dual-encoder in a bidirectional GAN
architecture that is trained simultaneously with a generator and a
discriminator network. Through the learning mechanism, the proposed method aims
to reduce the problem of bad cycle consistency, in which a bidirectional GAN
might not be able to reproduce samples with a large difference between normal
and abnormal samples. We assume that bad cycle consistency occurs when the
method does not preserve enough information of the sample data. We show that
our proposed method performs well in capturing the distribution of normal
samples, thereby improving anomaly detection on GAN-based models. Experiments
are reported in which our method is applied to publicly available datasets,
including application to a brain magnetic resonance imaging anomaly detection
system.
</p>
<a href="http://arxiv.org/abs/2012.11834" target="_blank">arXiv:2012.11834</a> [<a href="http://arxiv.org/pdf/2012.11834" target="_blank">pdf</a>]

<h2>Multi-shot NAS for Discovering Adversarially Robust Convolutional Neural Architectures at Targeted Capacities. (arXiv:2012.11835v1 [cs.AI])</h2>
<h3>Xuefei Ning, Junbo Zhao, Wenshuo Li, Tianchen Zhao, Huazhong Yang, Yu Wang</h3>
<p>Convolutional neural networks (CNNs) are vulnerable to adversarial examples,
and studies show that increasing the model capacity of an architecture topology
(e.g., width expansion) can bring consistent robustness improvements. This
reveals a clear robustness-efficiency trade-off that should be considered in
architecture design. Recent studies have employed one-shot neural architecture
search (NAS) to discover adversarially robust architectures. However, since the
capacities of different topologies cannot be easily aligned during the search
process, current one-shot NAS methods might favor topologies with larger
capacity in the supernet. And the discovered topology might be sub-optimal when
aligned to the targeted capacity. This paper proposes a novel multi-shot NAS
method to explicitly search for adversarially robust architectures at a certain
targeted capacity. Specifically, we estimate the reward at the targeted
capacity using interior extra-polation of the rewards from multiple supernets.
Experimental results demonstrate the effectiveness of the proposed method. For
instance, at the targeted FLOPs of 1560M, the discovered MSRobNet-1560 (clean
84.8%, PGD100 52.9%) outperforms the recent NAS-discovered architecture
RobNet-free (clean 82.8%, PGD100 52.6%) with similar FLOPs.
</p>
<a href="http://arxiv.org/abs/2012.11835" target="_blank">arXiv:2012.11835</a> [<a href="http://arxiv.org/pdf/2012.11835" target="_blank">pdf</a>]

<h2>Residual Matrix Product State for Machine Learning. (arXiv:2012.11841v1 [cs.LG])</h2>
<h3>Ye-Ming Meng, Jing Zhang, Peng Zhang, Chao Gao, Shi-Ju Ran</h3>
<p>Tensor network (TN), which originates from quantum physics, shows broad
prospects in classical and quantum machine learning (ML). However, there still
exists a considerable gap of accuracy between TN and the sophisticated neural
network (NN) models for classical ML. It is still elusive how far TN ML can be
improved by, e.g., borrowing the techniques from NN. In this work, we propose
the residual matrix product state (ResMPS) by combining the ideas of matrix
product state (MPS) and residual NN. ResMPS can be treated as a network where
its layers map the "hidden" features to the outputs (e.g., classifications),
and the variational parameters of the layers are the functions of the features
of samples (e.g., pixels of images). This is essentially different from NN,
where the layers map feed-forwardly the features to the output. ResMPS can
naturally incorporate with the non-linear activations and dropout layers, and
outperforms the state-of-the-art TN models on the efficiency, stability, and
expression power. Besides, ResMPS is interpretable from the perspective of
polynomial expansion, where the factorization and exponential machines
naturally emerge. Our work contributes to connecting and hybridizing neural and
tensor networks, which is crucial to understand the working mechanisms further
and improve both models' performances.
</p>
<a href="http://arxiv.org/abs/2012.11841" target="_blank">arXiv:2012.11841</a> [<a href="http://arxiv.org/pdf/2012.11841" target="_blank">pdf</a>]

<h2>Adversarial Multiscale Feature Learning for Overlapping Chromosome Segmentation. (arXiv:2012.11847v1 [cs.CV])</h2>
<h3>Liye Mei, Yalan Yu, Yueyun Weng, Xiaopeng Guo, Yan Liu, Du Wang, Sheng Liu, Fuling Zhou, Cheng Lei</h3>
<p>Chromosome karyotype analysis is of great clinical importance in the
diagnosis and treatment of diseases, especially for genetic diseases. Since
manual analysis is highly time and effort consuming, computer-assisted
automatic chromosome karyotype analysis based on images is routinely used to
improve the efficiency and accuracy of the analysis. Due to the strip shape of
the chromosomes, they easily get overlapped with each other when imaged,
significantly affecting the accuracy of the analysis afterward. Conventional
overlapping chromosome segmentation methods are usually based on manually
tagged features, hence, the performance of which is easily affected by the
quality, such as resolution and brightness, of the images. To address the
problem, in this paper, we present an adversarial multiscale feature learning
framework to improve the accuracy and adaptability of overlapping chromosome
segmentation. Specifically, we first adopt the nested U-shape network with
dense skip connections as the generator to explore the optimal representation
of the chromosome images by exploiting multiscale features. Then we use the
conditional generative adversarial network (cGAN) to generate images similar to
the original ones, the training stability of which is enhanced by applying the
least-square GAN objective. Finally, we employ Lovasz-Softmax to help the model
converge in a continuous optimization setting. Comparing with the established
algorithms, the performance of our framework is proven superior by using public
datasets in eight evaluation criteria, showing its great potential in
overlapping chromosome segmentation
</p>
<a href="http://arxiv.org/abs/2012.11847" target="_blank">arXiv:2012.11847</a> [<a href="http://arxiv.org/pdf/2012.11847" target="_blank">pdf</a>]

<h2>Selective Forgetting of Deep Networks at a Finer Level than Samples. (arXiv:2012.11849v1 [stat.ML])</h2>
<h3>Tomohiro Hayase, Suguru Yasutomi, Takashi Katoh</h3>
<p>Selective forgetting or removing information from deep neural networks (DNNs)
is essential for continuous learning and is challenging in controlling the
DNNs. Such forgetting is crucial also in a practical sense since the deployed
DNNs may be trained on the data with outliers, poisoned by attackers, or with
leaked/sensitive information. In this paper, we formulate selective forgetting
for classification tasks at a finer level than the samples' level. We specify
the finer level based on four datasets distinguished by two conditions: whether
they contain information to be forgotten and whether they are available for the
forgetting procedure. Additionally, we reveal the need for such formulation
with the datasets by showing concrete and practical situations. Moreover, we
introduce the forgetting procedure as an optimization problem on three
criteria; the forgetting, the correction, and the remembering term.
Experimental results show that the proposed methods can make the model forget
to use specific information for classification. Notably, in specific cases, our
methods improved the model's accuracy on the datasets, which contains
information to be forgotten but is unavailable in the forgetting procedure.
Such data are unexpectedly found and misclassified in actual situations.
</p>
<a href="http://arxiv.org/abs/2012.11849" target="_blank">arXiv:2012.11849</a> [<a href="http://arxiv.org/pdf/2012.11849" target="_blank">pdf</a>]

<h2>Predicting Online Video Advertising Effects with Multimodal Deep Learning. (arXiv:2012.11851v1 [cs.CV])</h2>
<h3>Jun Ikeda, Hiroyuki Seshime, Xueting Wang, Toshihiko Yamasaki</h3>
<p>With expansion of the video advertising market, research to predict the
effects of video advertising is getting more attention. Although effect
prediction of image advertising has been explored a lot, prediction for video
advertising is still challenging with seldom research. In this research, we
propose a method for predicting the click through rate (CTR) of video
advertisements and analyzing the factors that determine the CTR. In this paper,
we demonstrate an optimized framework for accurately predicting the effects by
taking advantage of the multimodal nature of online video advertisements
including video, text, and metadata features. In particular, the two types of
metadata, i.e., categorical and continuous, are properly separated and
normalized. To avoid overfitting, which is crucial in our task because the
training data are not very rich, additional regularization layers are inserted.
Experimental results show that our approach can achieve a correlation
coefficient as high as 0.695, which is a significant improvement from the
baseline (0.487).
</p>
<a href="http://arxiv.org/abs/2012.11851" target="_blank">arXiv:2012.11851</a> [<a href="http://arxiv.org/pdf/2012.11851" target="_blank">pdf</a>]

<h2>A Second-Order Approach to Learning with Instance-Dependent Label Noise. (arXiv:2012.11854v1 [cs.LG])</h2>
<h3>Zhaowei Zhu, Tongliang Liu, Yang Liu</h3>
<p>The presence of label noise often misleads the training of deep neural
networks. Departing from the recent literature which largely assumes the label
noise rate is only determined by the true class, the errors in human-annotated
labels are more likely to be dependent on the difficulty levels of tasks,
resulting in settings with instance-dependent label noise. We show
theoretically that the heterogeneous instance-dependent label noise is
effectively down-weighting the examples with higher noise rates in a
non-uniform way and thus causes imbalances, rendering the strategy of directly
applying methods for class-dependent label noise questionable. In this paper,
we propose and study the potentials of a second-order approach that leverages
the estimation of several covariance terms defined between the
instance-dependent noise rates and the Bayes optimal label. We show that this
set of second-order information successfully captures the induced imbalances.
We further proceed to show that with the help of the estimated second-order
information, we identify a new loss function whose expected risk of a
classifier under instance-dependent label noise can be shown to be equivalent
to a new problem with only class-dependent label noise. This fact allows us to
develop effective loss functions to correctly evaluate models. We provide an
efficient procedure to perform the estimations without accessing either ground
truth labels or prior knowledge of the noise rates. Experiments on CIFAR10 and
CIFAR100 with synthetic instance-dependent label noise and Clothing1M with
real-world human label noise verify our approach.
</p>
<a href="http://arxiv.org/abs/2012.11854" target="_blank">arXiv:2012.11854</a> [<a href="http://arxiv.org/pdf/2012.11854" target="_blank">pdf</a>]

<h2>GuidedStyle: Attribute Knowledge Guided Style Manipulation for Semantic Face Editing. (arXiv:2012.11856v1 [cs.CV])</h2>
<h3>Xianxu Hou, Xiaokang Zhang, Linlin Shen, Zhihui Lai, Jun Wan</h3>
<p>Although significant progress has been made in synthesizing high-quality and
visually realistic face images by unconditional Generative Adversarial Networks
(GANs), there still lacks of control over the generation process in order to
achieve semantic face editing. In addition, it remains very challenging to
maintain other face information untouched while editing the target attributes.
In this paper, we propose a novel learning framework, called GuidedStyle, to
achieve semantic face editing on StyleGAN by guiding the image generation
process with a knowledge network. Furthermore, we allow an attention mechanism
in StyleGAN generator to adaptively select a single layer for style
manipulation. As a result, our method is able to perform disentangled and
controllable edits along various attributes, including smiling, eyeglasses,
gender, mustache and hair color. Both qualitative and quantitative results
demonstrate the superiority of our method over other competing methods for
semantic face editing. Moreover, we show that our model can be also applied to
different types of real and artistic face editing, demonstrating strong
generalization ability.
</p>
<a href="http://arxiv.org/abs/2012.11856" target="_blank">arXiv:2012.11856</a> [<a href="http://arxiv.org/pdf/2012.11856" target="_blank">pdf</a>]

<h2>Gaussian Process Regression constrained by Boundary Value Problems. (arXiv:2012.11857v1 [cs.LG])</h2>
<h3>Mamikon Gulian, Ari Frankel, Laura Swiler</h3>
<p>We develop a framework for Gaussian processes regression constrained by
boundary value problems. The framework may be applied to infer the solution of
a well-posed boundary value problem with a known second-order differential
operator and boundary conditions, but for which only scattered observations of
the source term are available. Scattered observations of the solution may also
be used in the regression. The framework combines co-kriging with the linear
transformation of a Gaussian process together with the use of kernels given by
spectral expansions in eigenfunctions of the boundary value problem. Thus, it
benefits from a reduced-rank property of covariance matrices. We demonstrate
that the resulting framework yields more accurate and stable solution inference
as compared to physics-informed Gaussian process regression without boundary
condition constraints.
</p>
<a href="http://arxiv.org/abs/2012.11857" target="_blank">arXiv:2012.11857</a> [<a href="http://arxiv.org/pdf/2012.11857" target="_blank">pdf</a>]

<h2>Salient Bundle Adjustment for Visual SLAM. (arXiv:2012.11863v1 [cs.RO])</h2>
<h3>Ke Wang, Sai Ma, Junlan Chen, Jianbo Lu</h3>
<p>Recently, the philosophy of visual saliency and attention has started to gain
popularity in the robotics community. Therefore, this paper aims to mimic this
mechanism in SLAM framework by using saliency prediction model. Comparing with
traditional SLAM that treated all feature points as equal important in
optimization process, we think that the salient feature points should play more
important role in optimization process. Therefore, we proposed a saliency model
to predict the saliency map, which can capture both scene semantic and
geometric information. Then, we proposed Salient Bundle Adjustment by using the
value of saliency map as the weight of the feature points in traditional Bundle
Adjustment approach. Exhaustive experiments conducted with the state-of-the-art
algorithm in KITTI and EuRoc datasets show that our proposed algorithm
outperforms existing algorithms in both indoor and outdoor environments.
Finally, we will make our saliency dataset and relevant source code open-source
for enabling future research.
</p>
<a href="http://arxiv.org/abs/2012.11863" target="_blank">arXiv:2012.11863</a> [<a href="http://arxiv.org/pdf/2012.11863" target="_blank">pdf</a>]

<h2>Human Action Recognition from Various Data Modalities: A Review. (arXiv:2012.11866v1 [cs.CV])</h2>
<h3>Zehua Sun, Jun Liu, Qiuhong Ke, Hossein Rahmani</h3>
<p>Human Action Recognition (HAR), aiming to understand human behaviors and then
assign category labels, has a wide range of applications, and thus has been
attracting increasing attention in the field of computer vision. Generally,
human actions can be represented using various data modalities, such as RGB,
skeleton, depth, infrared sequence, point cloud, event stream, audio,
acceleration, radar, and WiFi, etc., which encode different sources of useful
yet distinct information and have various advantages and application scenarios.
Consequently, lots of existing works have attempted to investigate different
types of approaches for HAR using various modalities. In this paper, we give a
comprehensive survey for HAR from the perspective of the input data modalities.
Specifically, we review both the hand-crafted feature-based and deep
learning-based methods for single data modalities, and also review the methods
based on multiple modalities, including the fusion-based frameworks and the
co-learning-based approaches. The current benchmark datasets for HAR are also
introduced. Finally, we discuss some potentially important research directions
in this area.
</p>
<a href="http://arxiv.org/abs/2012.11866" target="_blank">arXiv:2012.11866</a> [<a href="http://arxiv.org/pdf/2012.11866" target="_blank">pdf</a>]

<h2>A Survey of Methods for Managing the Classification and Solution of Data Imbalance Problem. (arXiv:2012.11870v1 [cs.LG])</h2>
<h3>Khan Md. Hasib, Md. Sadiq Iqbal, Faisal Muhammad Shah, Jubayer Al Mahmud, Mahmudul Hasan Popel, Md. Imran Hossain Showrov, Shakil Ahmed, Obaidur Rahman</h3>
<p>The problem of class imbalance is extensive for focusing on numerous
applications in the real world. In such a situation, nearly all of the examples
are labeled as one class called majority class, while far fewer examples are
labeled as the other class usually, the more important class is called
minority. Over the last few years, several types of research have been carried
out on the issue of class imbalance, including data sampling, cost-sensitive
analysis, Genetic Programming based models, bagging, boosting, etc.
Nevertheless, in this survey paper, we enlisted the 24 related studies in the
years 2003, 2008, 2010, 2012 and 2014 to 2019, focusing on the architecture of
single, hybrid, and ensemble method design to understand the current status of
improving classification output in machine learning techniques to fix problems
with class imbalances. This survey paper also includes a statistical analysis
of the classification algorithms under various methods and several other
experimental conditions, as well as datasets used in different research papers.
</p>
<a href="http://arxiv.org/abs/2012.11870" target="_blank">arXiv:2012.11870</a> [<a href="http://arxiv.org/pdf/2012.11870" target="_blank">pdf</a>]

<h2>FcaNet: Frequency Channel Attention Networks. (arXiv:2012.11879v1 [cs.CV])</h2>
<h3>Zequn Qin, Pengyi Zhang, Fei Wu, Xi Li</h3>
<p>Attention mechanism, especially channel attention, has gained great success
in the computer vision field. Many works focus on how to design efficient
channel attention mechanisms while ignoring a fundamental problem, i.e., using
global average pooling (GAP) as the unquestionable pre-processing method. In
this work, we start from a different view and rethink channel attention using
frequency analysis. Based on the frequency analysis, we mathematically prove
that the conventional GAP is a special case of the feature decomposition in the
frequency domain. With the proof, we naturally generalize the pre-processing of
channel attention mechanism in the frequency domain and propose FcaNet with
novel multi-spectral channel attention. The proposed method is simple but
effective. We can change only one line of code in the calculation to implement
our method within existing channel attention methods. Moreover, the proposed
method achieves state-of-the-art results compared with other channel attention
methods on image classification, object detection, and instance segmentation
tasks. Our method could improve by 1.8% in terms of Top-1 accuracy on ImageNet
compared with the baseline SENet-50, with the same number of parameters and the
same computational cost. Our code and models will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.11879" target="_blank">arXiv:2012.11879</a> [<a href="http://arxiv.org/pdf/2012.11879" target="_blank">pdf</a>]

<h2>Fast and Accurate $k$-means++ via Rejection Sampling. (arXiv:2012.11891v1 [cs.LG])</h2>
<h3>Vincent Cohen-Addad, Silvio Lattanzi, Ashkan Norouzi-Fard, Christian Sohler, Ola Svensson</h3>
<p>$k$-means++ \cite{arthur2007k} is a widely used clustering algorithm that is
easy to implement, has nice theoretical guarantees and strong empirical
performance. Despite its wide adoption, $k$-means++ sometimes suffers from
being slow on large data-sets so a natural question has been to obtain more
efficient algorithms with similar guarantees. In this paper, we present a near
linear time algorithm for $k$-means++ seeding. Interestingly our algorithm
obtains the same theoretical guarantees as $k$-means++ and significantly
improves earlier results on fast $k$-means++ seeding. Moreover, we show
empirically that our algorithm is significantly faster than $k$-means++ and
obtains solutions of equivalent quality.
</p>
<a href="http://arxiv.org/abs/2012.11891" target="_blank">arXiv:2012.11891</a> [<a href="http://arxiv.org/pdf/2012.11891" target="_blank">pdf</a>]

<h2>Graph Autoencoders with Deconvolutional Networks. (arXiv:2012.11898v1 [cs.LG])</h2>
<h3>Jia Li, Tomas Yu, Da-Cheng Juan, Arjun Gopalan, Hong Cheng, Andrew Tomkins</h3>
<p>Recent studies have indicated that Graph Convolutional Networks (GCNs) act as
a \emph{low pass} filter in spectral domain and encode smoothed node
representations. In this paper, we consider their opposite, namely Graph
Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed
node representations. We motivate the design of Graph Deconvolutional Networks
via a combination of inverse filters in spectral domain and de-noising layers
in wavelet domain, as the inverse operation results in a \emph{high pass}
filter and may amplify the noise. Based on the proposed GDN, we further propose
a graph autoencoder framework that first encodes smoothed graph representations
with GCN and then decodes accurate graph signals with GDN. We demonstrate the
effectiveness of the proposed method on several tasks including unsupervised
graph-level representation , social recommendation and graph generation
</p>
<a href="http://arxiv.org/abs/2012.11898" target="_blank">arXiv:2012.11898</a> [<a href="http://arxiv.org/pdf/2012.11898" target="_blank">pdf</a>]

<h2>This is not the Texture you are looking for! Introducing Novel Counterfactual Explanations for Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v1 [cs.LG])</h2>
<h3>Silvan Mertes, Tobias Huber, Katharina Weitz, Alexander Heimerl, Elisabeth Andr&#xe9;</h3>
<p>With the ongoing rise of machine learning, the need for methods for
explaining decisions made by artificial intelligence systems is becoming a more
and more important topic. Especially for image classification tasks, many
state-of-the-art tools to explain such classifiers rely on visual highlighting
of important areas of the input data. Contrary, counterfactual explanation
systems try to enable a counterfactual reasoning by modifying the input image
in a way such that the classifier would have made a different prediction. By
doing so, the users of counterfactual explanation systems are equipped with a
completely different kind of explanatory information. However, methods for
generating realistic counterfactual explanations for image classifiers are
still rare. In this work, we present a novel approach to generate such
counterfactual image explanations based on adversarial image-to-image
translation techniques. Additionally, we conduct a user study to evaluate our
approach in a use case which was inspired by a healthcare scenario. Our results
show that our approach leads to significantly better results regarding mental
models, explanation satisfaction, trust, emotions, and self-efficacy than two
state-of-the art systems that work with saliency maps, namely LIME and LRP.
</p>
<a href="http://arxiv.org/abs/2012.11905" target="_blank">arXiv:2012.11905</a> [<a href="http://arxiv.org/pdf/2012.11905" target="_blank">pdf</a>]

<h2>A Hybrid VDV Model for Automatic Diagnosis of Pneumothorax using Class-Imbalanced Chest X-rays Dataset. (arXiv:2012.11911v1 [cs.CV])</h2>
<h3>Tahira Iqbal, Arslan Shaukat, Usman Akram, Zartasha Mustansar, Yung-Cheol Byun</h3>
<p>Pneumothorax, a life threatening disease, needs to be diagnosed immediately
and efficiently. The prognosis in this case is not only time consuming but also
prone to human errors. So an automatic way of accurate diagnosis using chest
X-rays is the utmost requirement. To-date, most of the available medical images
datasets have class-imbalance issue. The main theme of this study is to solve
this problem along with proposing an automated way of detecting pneumothorax.
We first compare the existing approaches to tackle the class-imbalance issue
and find that data-level-ensemble (i.e. ensemble of subsets of dataset)
outperforms other approaches. Thus, we propose a novel framework named as VDV
model, which is a complex model-level-ensemble of data-level-ensembles and uses
three convolutional neural networks (CNN) including VGG16, VGG-19 and
DenseNet-121 as fixed feature extractors. In each data-level-ensemble features
extracted from one of the pre-defined CNN are fed to support vector machine
(SVM) classifier, and output from each data-level-ensemble is calculated using
voting method. Once outputs from the three data-level-ensembles with three
different CNN architectures are obtained, then, again, voting method is used to
calculate the final prediction. Our proposed framework is tested on SIIM ACR
Pneumothorax dataset and Random Sample of NIH Chest X-ray dataset (RS-NIH). For
the first dataset, 85.17% Recall with 86.0% Area under the Receiver Operating
Characteristic curve (AUC) is attained. For the second dataset, 90.9% Recall
with 95.0% AUC is achieved with random split of data while 85.45% recall with
77.06% AUC is obtained with patient-wise split of data. For RS-NIH, the
obtained results are higher as compared to previous results from literature
However, for first dataset, direct comparison cannot be made, since this
dataset has not been used earlier for Pneumothorax classification.
</p>
<a href="http://arxiv.org/abs/2012.11911" target="_blank">arXiv:2012.11911</a> [<a href="http://arxiv.org/pdf/2012.11911" target="_blank">pdf</a>]

<h2>Interpreting Deep Learning Models for Epileptic Seizure Detection on EEG signals. (arXiv:2012.11933v1 [cs.LG])</h2>
<h3>Valentin Gabeff, Tomas Teijeiro, Marina Zapater, Leila Cammoun, Sylvain Rheims, Philippe Ryvlin, David Atienza</h3>
<p>While Deep Learning (DL) is often considered the state-of-the art for
Artificial Intelligence-based medical decision support, it remains sparsely
implemented in clinical practice and poorly trusted by clinicians due to
insufficient interpretability of neural network models. We have tackled this
issue by developing interpretable DL models in the context of online detection
of epileptic seizure, based on EEG signal. This has conditioned the preparation
of the input signals, the network architecture, and the post-processing of the
output in line with the domain knowledge. Specifically, we focused the
discussion on three main aspects: 1) how to aggregate the classification
results on signal segments provided by the DL model into a larger time scale,
at the seizure-level; 2) what are the relevant frequency patterns learned in
the first convolutional layer of different models, and their relation with the
delta, theta, alpha, beta and gamma frequency bands on which the visual
interpretation of EEG is based; and 3) the identification of the signal
waveforms with larger contribution towards the ictal class, according to the
activation differences highlighted using the DeepLIFT method. Results show that
the kernel size in the first layer determines the interpretability of the
extracted features and the sensitivity of the trained models, even though the
final performance is very similar after post-processing. Also, we found that
amplitude is the main feature leading to an ictal prediction, suggesting that a
larger patient population would be required to learn more complex frequency
patterns. Still, our methodology was successfully able to generalize patient
inter-variability for the majority of the studied population with a
classification F1-score of 0.873 and detecting 90% of the seizures.
</p>
<a href="http://arxiv.org/abs/2012.11933" target="_blank">arXiv:2012.11933</a> [<a href="http://arxiv.org/pdf/2012.11933" target="_blank">pdf</a>]

<h2>Knowledge Graphs Evolution and Preservation -- A Technical Report from ISWS 2019. (arXiv:2012.11936v1 [cs.AI])</h2>
<h3>Nacira Abbas, Kholoud Alghamdi, Mortaza Alinam, Francesca Alloatti, Glenda Amaral, Claudia d&#x27;Amato, Luigi Asprino, Martin Beno, Felix Bensmann, Russa Biswas, Ling Cai, Riley Capshaw, Valentina Anita Carriero, Irene Celino, Amine Dadoun, Stefano De Giorgis, Harm Delva, John Domingue, Michel Dumontier, Vincent Emonet, Marieke van Erp, Paola Espinoza Arias, Omaima Fallatah, Sebasti&#xe1;n Ferrada, Marc Gallofr&#xe9; Oca&#xf1;a, Michalis Georgiou, Genet Asefa Gesese, Frances Gillis-Webber, Francesca Giovannetti, Mar&#xec;a Granados Buey, Ismail Harrando, Ivan Heibi, Vitor Horta, Laurine Huber, Federico Igne, Mohamad Yaser Jaradeh, Neha Keshan, Aneta Koleva, Bilal Koteich, Kabul Kurniawan, Mengya Liu, Chuangtao Ma, Lientje Maas, Martin Mansfield, Fabio Mariani, Eleonora Marzi, Sepideh Mesbah, et al. (27 additional authors not shown)</h3>
<p>One of the grand challenges discussed during the Dagstuhl Seminar "Knowledge
Graphs: New Directions for Knowledge Representation on the Semantic Web" and
described in its report is that of a: "Public FAIR Knowledge Graph of
Everything: We increasingly see the creation of knowledge graphs that capture
information about the entirety of a class of entities. [...] This grand
challenge extends this further by asking if we can create a knowledge graph of
"everything" ranging from common sense concepts to location based entities.
This knowledge graph should be "open to the public" in a FAIR manner
democratizing this mass amount of knowledge." Although linked open data (LOD)
is one knowledge graph, it is the closest realisation (and probably the only
one) to a public FAIR Knowledge Graph (KG) of everything. Surely, LOD provides
a unique testbed for experimenting and evaluating research hypotheses on open
and FAIR KG. One of the most neglected FAIR issues about KGs is their ongoing
evolution and long term preservation. We want to investigate this problem, that
is to understand what preserving and supporting the evolution of KGs means and
how these problems can be addressed. Clearly, the problem can be approached
from different perspectives and may require the development of different
approaches, including new theories, ontologies, metrics, strategies,
procedures, etc. This document reports a collaborative effort performed by 9
teams of students, each guided by a senior researcher as their mentor,
attending the International Semantic Web Research School (ISWS 2019). Each team
provides a different perspective to the problem of knowledge graph evolution
substantiated by a set of research questions as the main subject of their
investigation. In addition, they provide their working definition for KG
preservation and evolution.
</p>
<a href="http://arxiv.org/abs/2012.11936" target="_blank">arXiv:2012.11936</a> [<a href="http://arxiv.org/pdf/2012.11936" target="_blank">pdf</a>]

<h2>3D Point-to-Keypoint Voting Network for 6D Pose Estimation. (arXiv:2012.11938v1 [cs.CV])</h2>
<h3>Weitong Hua, Jiaxin Guo, Yue Wang, Rong Xiong</h3>
<p>Object 6D pose estimation is an important research topic in the field of
computer vision due to its wide application requirements and the challenges
brought by complexity and changes in the real-world. We think fully exploring
the characteristics of spatial relationship between points will help to improve
the pose estimation performance, especially in the scenes of background clutter
and partial occlusion. But this information was usually ignored in previous
work using RGB image or RGB-D data. In this paper, we propose a framework for
6D pose estimation from RGB-D data based on spatial structure characteristics
of 3D keypoints. We adopt point-wise dense feature embedding to vote for 3D
keypoints, which makes full use of the structure information of the rigid body.
After the direction vectors pointing to the keypoints are predicted by CNN, we
use RANSAC voting to calculate the coordinate of the 3D keypoints, then the
pose transformation can be easily obtained by the least square method. In
addition, a spatial dimension sampling strategy for points is employed, which
makes the method achieve excellent performance on small training sets. The
proposed method is verified on two benchmark datasets, LINEMOD and OCCLUSION
LINEMOD. The experimental results show that our method outperforms the
state-of-the-art approaches, achieves ADD(-S) accuracy of 98.7\% on LINEMOD
dataset and 52.6\% on OCCLUSION LINEMOD dataset in real-time.
</p>
<a href="http://arxiv.org/abs/2012.11938" target="_blank">arXiv:2012.11938</a> [<a href="http://arxiv.org/pdf/2012.11938" target="_blank">pdf</a>]

<h2>Robotic Process Automation -- A Systematic Literature Review and Assessment Framework. (arXiv:2012.11951v1 [cs.RO])</h2>
<h3>Judith Wewerka, Manfred Reichert</h3>
<p>Robotic Process Automation (RPA) is the automation of rule-based routine
processes to increase efficiency and to reduce costs. Due to the utmost
importance of process automation in industry, RPA attracts increasing attention
in the scientific field as well. This paper presents the state-of-the-art in
the RPA field by means of a Systematic Literature Review (SLR). In this SLR, 63
publications are identified, categorised, and analysed along well-defined
research questions. From the SLR findings, moreover, a framework for
systematically analysing, assessing, and comparing existing as well as upcoming
RPA works is derived. The discovered thematic clusters advise further
investigations in order to develop an even more detailed structural research
approach for RPA.
</p>
<a href="http://arxiv.org/abs/2012.11951" target="_blank">arXiv:2012.11951</a> [<a href="http://arxiv.org/pdf/2012.11951" target="_blank">pdf</a>]

<h2>Generalized Relation Learning with Semantic Correlation Awareness for Link Prediction. (arXiv:2012.11957v1 [cs.AI])</h2>
<h3>Yao Zhang, Xu Zhang, Jun Wang, Hongru Liang, Wenqiang Lei, Zhe Sun, Adam Jatowt, Zhenglu Yang</h3>
<p>Developing link prediction models to automatically complete knowledge graphs
has recently been the focus of significant research interest. The current
methods for the link prediction taskhavetwonaturalproblems:1)the relation
distributions in KGs are usually unbalanced, and 2) there are many unseen
relations that occur in practical situations. These two problems limit the
training effectiveness and practical applications of the existing link
prediction models. We advocate a holistic understanding of KGs and we propose
in this work a unified Generalized Relation Learning framework GRL to address
the above two problems, which can be plugged into existing link prediction
models. GRL conducts a generalized relation learning, which is aware of
semantic correlations between relations that serve as a bridge to connect
semantically similar relations. After training with GRL, the closeness of
semantically similar relations in vector space and the discrimination of
dissimilar relations are improved. We perform comprehensive experiments on six
benchmarks to demonstrate the superior capability of GRL in the link prediction
task. In particular, GRL is found to enhance the existing link prediction
models making them insensitive to unbalanced relation distributions and capable
of learning unseen relations.
</p>
<a href="http://arxiv.org/abs/2012.11957" target="_blank">arXiv:2012.11957</a> [<a href="http://arxiv.org/pdf/2012.11957" target="_blank">pdf</a>]

<h2>Unsupervised Functional Data Analysis via Nonlinear Dimension Reduction. (arXiv:2012.11987v1 [stat.ML])</h2>
<h3>Moritz Herrmann, Fabian Scheipl</h3>
<p>In recent years, manifold methods have moved into focus as tools for
dimension reduction. Assuming that the high-dimensional data actually lie on or
close to a low-dimensional nonlinear manifold, these methods have shown
convincing results in several settings. This manifold assumption is often
reasonable for functional data, i.e., data representing continuously observed
functions, as well. However, the performance of manifold methods recently
proposed for tabular or image data has not been systematically assessed in the
case of functional data yet. Moreover, it is unclear how to evaluate the
quality of learned embeddings that do not yield invertible mappings, since the
reconstruction error cannot be used as a performance measure for such
representations. In this work, we describe and investigate the specific
challenges for nonlinear dimension reduction posed by the functional data
setting. The contributions of the paper are three-fold: First of all, we define
a theoretical framework which allows to systematically assess specific
challenges that arise in the functional data context, transfer several
nonlinear dimension reduction methods for tabular and image data to functional
data, and show that manifold methods can be used successfully in this setting.
Secondly, we subject performance assessment and tuning strategies to a thorough
and systematic evaluation based on several different functional data settings
and point out some previously undescribed weaknesses and pitfalls which can
jeopardize reliable judgment of embedding quality. Thirdly, we propose a
nuanced approach to make trustworthy decisions for or against competing
nonconforming embeddings more objectively.
</p>
<a href="http://arxiv.org/abs/2012.11987" target="_blank">arXiv:2012.11987</a> [<a href="http://arxiv.org/pdf/2012.11987" target="_blank">pdf</a>]

<h2>Self-Imitation Advantage Learning. (arXiv:2012.11989v1 [cs.LG])</h2>
<h3>Johan Ferret, Olivier Pietquin, Matthieu Geist</h3>
<p>Self-imitation learning is a Reinforcement Learning (RL) method that
encourages actions whose returns were higher than expected, which helps in hard
exploration and sparse reward problems. It was shown to improve the performance
of on-policy actor-critic methods in several discrete control tasks.
Nevertheless, applying self-imitation to the mostly action-value based
off-policy RL methods is not straightforward. We propose SAIL, a novel
generalization of self-imitation learning for off-policy RL, based on a
modification of the Bellman optimality operator that we connect to Advantage
Learning. Crucially, our method mitigates the problem of stale returns by
choosing the most optimistic return estimate between the observed return and
the current action-value for self-imitation. We demonstrate the empirical
effectiveness of SAIL on the Arcade Learning Environment, with a focus on hard
exploration games.
</p>
<a href="http://arxiv.org/abs/2012.11989" target="_blank">arXiv:2012.11989</a> [<a href="http://arxiv.org/pdf/2012.11989" target="_blank">pdf</a>]

<h2>Multiple Instance Segmentation in Brachial Plexus Ultrasound Image Using BPMSegNet. (arXiv:2012.12012v1 [cs.CV])</h2>
<h3>Yi Ding, Qiqi Yang, Guozheng Wu, Jian Zhang, Zhiguang Qin</h3>
<p>The identification of nerve is difficult as structures of nerves are
challenging to image and to detect in ultrasound images. Nevertheless, the
nerve identification in ultrasound images is a crucial step to improve
performance of regional anesthesia. In this paper, a network called Brachial
Plexus Multi-instance Segmentation Network (BPMSegNet) is proposed to identify
different tissues (nerves, arteries, veins, muscles) in ultrasound images. The
BPMSegNet has three novel modules. The first is the spatial local contrast
feature, which computes contrast features at different scales. The second one
is the self-attention gate, which reweighs the channels in feature maps by
their importance. The third is the addition of a skip concatenation with
transposed convolution within a feature pyramid network. The proposed BPMSegNet
is evaluated by conducting experiments on our constructed Ultrasound Brachial
Plexus Dataset (UBPD). Quantitative experimental results show the proposed
network can segment multiple tissues from the ultrasound images with a good
performance.
</p>
<a href="http://arxiv.org/abs/2012.12012" target="_blank">arXiv:2012.12012</a> [<a href="http://arxiv.org/pdf/2012.12012" target="_blank">pdf</a>]

<h2>Do We Really Need Scene-specific Pose Encoders?. (arXiv:2012.12014v1 [cs.CV])</h2>
<h3>Yoli Shavit, Ron Ferens</h3>
<p>Visual pose regression models estimate the camera pose from a query image
with a single forward pass. Current models learn pose encoding from an image
using deep convolutional networks which are trained per scene. The resulting
encoding is typically passed to a multi-layer perceptron in order to regress
the pose. In this work, we propose that scene-specific pose encoders are not
required for pose regression and that encodings trained for visual similarity
can be used instead. In order to test our hypothesis, we take a shallow
architecture of several fully connected layers and train it with pre-computed
encodings from a generic image retrieval model. We find that these encodings
are not only sufficient to regress the camera pose, but that, when provided to
a branching fully connected architecture, a trained model can achieve
competitive results and even surpass current \textit{state-of-the-art} pose
regressors in some cases. Moreover, we show that for outdoor localization, the
proposed architecture is the only pose regressor, to date, consistently
localizing in under 2 meters and 5 degrees.
</p>
<a href="http://arxiv.org/abs/2012.12014" target="_blank">arXiv:2012.12014</a> [<a href="http://arxiv.org/pdf/2012.12014" target="_blank">pdf</a>]

<h2>Data Assimilation in the Latent Space of a Neural Network. (arXiv:2012.12056v1 [cs.LG])</h2>
<h3>Maddalena Amendola, Rossella Arcucci, Laetitia Mottet, Cesar Quilodran Casas, Shiwei Fan, Christopher Pain, Paul Linden, Yi-Ke Guo</h3>
<p>There is an urgent need to build models to tackle Indoor Air Quality issue.
Since the model should be accurate and fast, Reduced Order Modelling technique
is used to reduce the dimensionality of the problem. The accuracy of the model,
that represent a dynamic system, is improved integrating real data coming from
sensors using Data Assimilation techniques. In this paper, we formulate a new
methodology called Latent Assimilation that combines Data Assimilation and
Machine Learning. We use a Convolutional neural network to reduce the
dimensionality of the problem, a Long-Short-Term-Memory to build a surrogate
model of the dynamic system and an Optimal Interpolated Kalman Filter to
incorporate real data. Experimental results are provided for CO2 concentration
within an indoor space. This methodology can be used for example to predict in
real-time the load of virus, such as the SARS-COV-2, in the air by linking it
to the concentration of CO2.
</p>
<a href="http://arxiv.org/abs/2012.12056" target="_blank">arXiv:2012.12056</a> [<a href="http://arxiv.org/pdf/2012.12056" target="_blank">pdf</a>]

<h2>QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning. (arXiv:2012.12062v1 [cs.LG])</h2>
<h3>Pascal Leroy, Damien Ernst, Pierre Geurts, Gilles Louppe, Jonathan Pisane, Matthia Sabatelli</h3>
<p>This paper introduces four new algorithms that can be used for tackling
multi-agent reinforcement learning (MARL) problems occurring in cooperative
settings. All algorithms are based on the Deep Quality-Value (DQV) family of
algorithms, a set of techniques that have proven to be successful when dealing
with single-agent reinforcement learning problems (SARL). The key idea of DQV
algorithms is to jointly learn an approximation of the state-value function
$V$, alongside an approximation of the state-action value function $Q$. We
follow this principle and generalise these algorithms by introducing two fully
decentralised MARL algorithms (IQV and IQV-Max) and two algorithms that are
based on the centralised training with decentralised execution training
paradigm (QVMix and QVMix-Max). We compare our algorithms with state-of-the-art
MARL techniques on the popular StarCraft Multi-Agent Challenge (SMAC)
environment. We show competitive results when QVMix and QVMix-Max are compared
to well-known MARL techniques such as QMIX and MAVEN and show that QVMix can
even outperform them on some of the tested environments, being the algorithm
which performs best overall. We hypothesise that this is due to the fact that
QVMix suffers less from the overestimation bias of the $Q$ function.
</p>
<a href="http://arxiv.org/abs/2012.12062" target="_blank">arXiv:2012.12062</a> [<a href="http://arxiv.org/pdf/2012.12062" target="_blank">pdf</a>]

<h2>Disentangling images with Lie group transformations and sparse coding. (arXiv:2012.12071v1 [cs.CV])</h2>
<h3>Ho Yin Chau, Frank Qiu, Yubei Chen, Bruno Olshausen</h3>
<p>Discrete spatial patterns and their continuous transformations are two
important regularities contained in natural signals. Lie groups and
representation theory are mathematical tools that have been used in previous
works to model continuous image transformations. On the other hand, sparse
coding is an important tool for learning dictionaries of patterns in natural
signals. In this paper, we combine these ideas in a Bayesian generative model
that learns to disentangle spatial patterns and their continuous
transformations in a completely unsupervised manner. Images are modeled as a
sparse superposition of shape components followed by a transformation that is
parameterized by n continuous variables. The shape components and
transformations are not predefined, but are instead adapted to learn the
symmetries in the data, with the constraint that the transformations form a
representation of an n-dimensional torus. Training the model on a dataset
consisting of controlled geometric transformations of specific MNIST digits
shows that it can recover these transformations along with the digits. Training
on the full MNIST dataset shows that it can learn both the basic digit shapes
and the natural transformations such as shearing and stretching that are
contained in this data.
</p>
<a href="http://arxiv.org/abs/2012.12071" target="_blank">arXiv:2012.12071</a> [<a href="http://arxiv.org/pdf/2012.12071" target="_blank">pdf</a>]

<h2>MetaAugment: Sample-Aware Data Augmentation Policy Learning. (arXiv:2012.12076v1 [cs.LG])</h2>
<h3>Fengwei Zhou, Jiawei Li, Chuanlong Xie, Fei Chen, Lanqing Hong, Rui Sun, Zhenguo Li</h3>
<p>Automated data augmentation has shown superior performance in image
recognition. Existing works search for dataset-level augmentation policies
without considering individual sample variations, which are likely to be
sub-optimal. On the other hand, learning different policies for different
samples naively could greatly increase the computing cost. In this paper, we
learn a sample-aware data augmentation policy efficiently by formulating it as
a sample reweighting problem. Specifically, an augmentation policy network
takes a transformation and the corresponding augmented image as inputs, and
outputs a weight to adjust the augmented image loss computed by a task network.
At training stage, the task network minimizes the weighted losses of augmented
training images, while the policy network minimizes the loss of the task
network on a validation set via meta-learning. We theoretically prove the
convergence of the training procedure and further derive the exact convergence
rate. Superior performance is achieved on widely-used benchmarks including
CIFAR-10/100, Omniglot, and ImageNet.
</p>
<a href="http://arxiv.org/abs/2012.12076" target="_blank">arXiv:2012.12076</a> [<a href="http://arxiv.org/pdf/2012.12076" target="_blank">pdf</a>]

<h2>Unsupervised Machine learning methods for city vitality index. (arXiv:2012.12082v1 [cs.LG])</h2>
<h3>Jean-S&#xe9;bastien Dessureault, Jonathan Simard, Daniel Massicotte</h3>
<p>This paper concerns the challenge to evaluate and predict a district vitality
index (VI) over the years. There is no standard method to do it, and it is even
more complicated to do it retroactively in the last decades. Although, it is
essential to evaluate and learn features of the past to predict a VI in the
future. This paper proposes a method to evaluate such a VI, based on a k-mean
clustering algorithm. The meta parameters of this unsupervised machine learning
technique are optimized by a genetic algorithm method. Based on the resulting
clusters and VI, a linear regression is applied to predict the VI of each
district of a city. The weights of each feature used in the clustering are
calculated using a random forest regressor algorithm. This method can be a
powerful insight for urbanists and inspire the redaction of a city plan in the
smart city context.
</p>
<a href="http://arxiv.org/abs/2012.12082" target="_blank">arXiv:2012.12082</a> [<a href="http://arxiv.org/pdf/2012.12082" target="_blank">pdf</a>]

<h2>Limitation of Acyclic Oriented Graphs Matching as Cell Tracking Accuracy Measure when Evaluating Mitosis. (arXiv:2012.12084v1 [cs.CV])</h2>
<h3>Ye Chen, Yuankai Huo</h3>
<p>Multi-object tracking (MOT) in computer vision and cell tracking in
biomedical image analysis are two similar research fields, whose common aim is
to achieve instance level object detection/segmentation and associate such
objects across different video frames. However, one major difference between
these two tasks is that cell tracking also aim to detect mitosis (cell
division), which is typically not considered in MOT tasks. Therefore, the
acyclic oriented graphs matching (AOGM) has been used as de facto standard
evaluation metrics for cell tracking, rather than directly using the evaluation
metrics in computer vision, such as multiple object tracking accuracy (MOTA),
ID Switches (IDS), ID F1 Score (IDF1) etc. However, based on our experiments,
we realized that AOGM did not always function as expected for mitosis events.
In this paper, we exhibit the limitations of evaluating mitosis with AOGM using
both simulated and real cell tracking data.
</p>
<a href="http://arxiv.org/abs/2012.12084" target="_blank">arXiv:2012.12084</a> [<a href="http://arxiv.org/pdf/2012.12084" target="_blank">pdf</a>]

<h2>Estimating Crop Primary Productivity with Sentinel-2 and Landsat 8 using Machine Learning Methods Trained with Radiative Transfer Simulations. (arXiv:2012.12101v1 [cs.CV])</h2>
<h3>Aleksandra Wolanin, Gustau Camps-Valls, Luis G&#xf3;mez-Chova, Gonzalo Mateo-Garc&#xed;a, Christiaan van der Tol, Yongguang Zhang, Luis Guanter</h3>
<p>Satellite remote sensing has been widely used in the last decades for
agricultural applications, {both for assessing vegetation condition and for
subsequent yield prediction.} Existing remote sensing-based methods to estimate
gross primary productivity (GPP), which is an important variable to indicate
crop photosynthetic function and stress, typically rely on empirical or
semi-empirical approaches, which tend to over-simplify photosynthetic
mechanisms. In this work, we take advantage of all parallel developments in
mechanistic photosynthesis modeling and satellite data availability for
advanced monitoring of crop productivity. In particular, we combine
process-based modeling with the soil-canopy energy balance radiative transfer
model (SCOPE) with Sentinel-2 {and Landsat 8} optical remote sensing data and
machine learning methods in order to estimate crop GPP. Our model successfully
estimates GPP across a variety of C3 crop types and environmental conditions
even though it does not use any local information from the corresponding sites.
This highlights its potential to map crop productivity from new satellite
sensors at a global scale with the help of current Earth observation cloud
computing platforms.
</p>
<a href="http://arxiv.org/abs/2012.12101" target="_blank">arXiv:2012.12101</a> [<a href="http://arxiv.org/pdf/2012.12101" target="_blank">pdf</a>]

<h2>Predicting survival outcomes using topological features of tumor pathology images. (arXiv:2012.12102v1 [cs.CV])</h2>
<h3>Chul Moon, Qiwei Li, Guanghua Xiao</h3>
<p>Tumor shape and size have been used as important markers for cancer diagnosis
and treatment. Recent developments in medical imaging technology enable more
detailed segmentation of tumor regions in high resolution. This paper proposes
a topological feature to characterize tumor progression from digital pathology
images and examine its effect on the time-to-event data. We develop distance
transform for pathology images and show that a topological summary statistic
computed by persistent homology quantifies tumor shape, size, distribution, and
connectivity. The topological features are represented in functional space and
used as functional predictors in a functional Cox regression model. A case
study is conducted using non-small cell lung cancer pathology images. The
results show that the topological features predict survival prognosis after
adjusting for age, sex, smoking status, stage, and size of tumors. Also, the
topological features with non-zero effects correspond to the shapes that are
known to be related to tumor progression. Our study provides a new perspective
for understanding tumor shape and patient prognosis.
</p>
<a href="http://arxiv.org/abs/2012.12102" target="_blank">arXiv:2012.12102</a> [<a href="http://arxiv.org/pdf/2012.12102" target="_blank">pdf</a>]

<h2>A Deep Reinforcement Learning Approach for Ramp Metering Based on Traffic Video Data. (arXiv:2012.12104v1 [cs.CV])</h2>
<h3>Bing Liu (1), Yu Tang (2), Yuxiong Ji (1), Yu Shen (1), Yuchuan Du (1) ((1) Key Laboratory of Road and Traffic Engineering of the Ministry of Education, Tongji University, Shanghai, China, (2) Tandon School of Engineering, New York University, New York, USA)</h3>
<p>Ramp metering that uses traffic signals to regulate vehicle flows from the
on-ramps has been widely implemented to improve vehicle mobility of the
freeway. Previous studies generally update signal timings in real-time based on
predefined traffic measures collected by point detectors, such as traffic
volumes and occupancies. Comparing with point detectors, traffic cameras-which
have been increasingly deployed on road networks-could cover larger areas and
provide more detailed traffic information. In this work, we propose a deep
reinforcement learning (DRL) method to explore the potential of traffic video
data in improving the efficiency of ramp metering. The proposed method uses
traffic video frames as inputs and learns the optimal control strategies
directly from the high-dimensional visual inputs. A real-world case study
demonstrates that, in comparison with a state-of-the-practice method, the
proposed DRL method results in 1) lower travel times in the mainline, 2)
shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream
of the merging area. The results suggest that the proposed method is able to
extract useful information from the video data for better ramp metering
controls.
</p>
<a href="http://arxiv.org/abs/2012.12104" target="_blank">arXiv:2012.12104</a> [<a href="http://arxiv.org/pdf/2012.12104" target="_blank">pdf</a>]

<h2>Warped Gaussian Processes in Remote Sensing Parameter Estimation and Causal Inference. (arXiv:2012.12105v1 [cs.CV])</h2>
<h3>Anna Mateo-Sanchis, Jordi Mu&#xf1;oz-Mar&#xed;, Adri&#xe1;n P&#xe9;rez-Suay, Gustau Camps-Valls</h3>
<p>This paper introduces warped Gaussian processes (WGP) regression in remote
sensing applications. WGP models output observations as a parametric nonlinear
transformation of a GP. The parameters of such prior model are then learned via
standard maximum likelihood. We show the good performance of the proposed model
for the estimation of oceanic chlorophyll content from multispectral data,
vegetation parameters (chlorophyll, leaf area index, and fractional vegetation
cover) from hyperspectral data, and in the detection of the causal direction in
a collection of 28 bivariate geoscience and remote sensing causal problems. The
model consistently performs better than the standard GP and the more advanced
heteroscedastic GP model, both in terms of accuracy and more sensible
confidence intervals.
</p>
<a href="http://arxiv.org/abs/2012.12105" target="_blank">arXiv:2012.12105</a> [<a href="http://arxiv.org/pdf/2012.12105" target="_blank">pdf</a>]

<h2>Convolutional Neural Networks from Image Markers. (arXiv:2012.12108v1 [cs.CV])</h2>
<h3>Barbara C. Benato, Italos E. de Souza, Felipe L. Galv&#xe3;o, Alexandre X. Falc&#xe3;o</h3>
<p>A technique named Feature Learning from Image Markers (FLIM) was recently
proposed to estimate convolutional filters, with no backpropagation, from
strokes drawn by a user on very few images (e.g., 1-3) per class, and
demonstrated for coconut-tree image classification. This paper extends FLIM for
fully connected layers and demonstrates it on different image classification
problems. The work evaluates marker selection from multiple users and the
impact of adding a fully connected layer. The results show that FLIM-based
convolutional neural networks can outperform the same architecture trained from
scratch by backpropagation.
</p>
<a href="http://arxiv.org/abs/2012.12108" target="_blank">arXiv:2012.12108</a> [<a href="http://arxiv.org/pdf/2012.12108" target="_blank">pdf</a>]

<h2>Noise-Equipped Convolutional Neural Networks. (arXiv:2012.12109v1 [cs.CV])</h2>
<h3>Menghan Xia, Tien-Tsin Wong</h3>
<p>As a generic modeling tool, Convolutional Neural Network (CNN) has been
widely employed in image synthesis and translation tasks. However, when a CNN
model is fed with a flat input, the transformation degrades into a scaling
operation due to the spatial sharing nature of convolution kernels. This
inherent problem has been barely studied nor raised as an application
restriction. In this paper, we point out that such convolution degradation
actually hinders some specific image generation tasks that expect value-variant
output from a flat input. We study the cause behind it and propose a generic
solution to tackle it. Our key idea is to break the flat input condition
through a proxy input module that perturbs the input data symmetrically with a
noise map and reassembles them in feature domain. We call it noise-equipped CNN
model and study its behavior through multiple analysis. Our experiments show
that our model is free of degradation and hence serves as a superior
alternative to standard CNN models. We further demonstrate improved
performances of applying our model to existing applications, e.g. semantic
photo synthesis and color-encoded grayscale generation.
</p>
<a href="http://arxiv.org/abs/2012.12109" target="_blank">arXiv:2012.12109</a> [<a href="http://arxiv.org/pdf/2012.12109" target="_blank">pdf</a>]

<h2>MOCCA: Multi-Layer One-Class Classification for Anomaly Detection. (arXiv:2012.12111v1 [cs.CV])</h2>
<h3>Fabio Valerio Massoli, Fabrizio Falchi, Alperen Kantarci, &#x15e;eymanur Akti, Hazim Kemal Ekenel, Giuseppe Amato</h3>
<p>Anomalies are ubiquitous in all scientific fields and can express an
unexpected event due to incomplete knowledge about the data distribution or an
unknown process that suddenly comes into play and distorts the observations.
Due to such events' rarity, it is common to train deep learning models on
"normal", i.e. non-anomalous, datasets only, thus letting the neural network to
model the distribution beneath the input data. In this context, we propose our
deep learning approach to the anomaly detection problem named
Multi-LayerOne-Class Classification (MOCCA). We explicitly leverage the
piece-wise nature of deep neural networks by exploiting information extracted
at different depths to detect abnormal data instances. We show how combining
the representations extracted from multiple layers of a model leads to higher
discrimination performance than typical approaches proposed in the literature
that are based neural networks' final output only. We propose to train the
model by minimizing the $L_2$ distance between the input representation and a
reference point, the anomaly-free training data centroid, at each considered
layer. We conduct extensive experiments on publicly available datasets for
anomaly detection, namely CIFAR10, MVTec AD, and ShanghaiTech, considering both
the single-image and video-based scenarios. We show that our method reaches
superior performances compared to the state-of-the-art approaches available in
the literature. Moreover, we provide a model analysis to give insight on how
our approach works.
</p>
<a href="http://arxiv.org/abs/2012.12111" target="_blank">arXiv:2012.12111</a> [<a href="http://arxiv.org/pdf/2012.12111" target="_blank">pdf</a>]

<h2>Explainable Abstract Trains Dataset. (arXiv:2012.12115v1 [cs.CV])</h2>
<h3>Manuel de Sousa Ribeiro, Ludwig Krippahl, Joao Leite</h3>
<p>The Explainable Abstract Trains Dataset is an image dataset containing
simplified representations of trains. It aims to provide a platform for the
application and research of algorithms for justification and explanation
extraction. The dataset is accompanied by an ontology that conceptualizes and
classifies the depicted trains based on their visual characteristics, allowing
for a precise understanding of how each train was labeled. Each image in the
dataset is annotated with multiple attributes describing the trains' features
and with bounding boxes for the train elements.
</p>
<a href="http://arxiv.org/abs/2012.12115" target="_blank">arXiv:2012.12115</a> [<a href="http://arxiv.org/pdf/2012.12115" target="_blank">pdf</a>]

<h2>Projected Stochastic Gradient Langevin Algorithms for Constrained Sampling and Non-Convex Learning. (arXiv:2012.12137v1 [cs.LG])</h2>
<h3>Andrew Lamperski</h3>
<p>Langevin algorithms are gradient descent methods with additive noise. They
have been used for decades in Markov chain Monte Carlo (MCMC) sampling,
optimization, and learning. Their convergence properties for unconstrained
non-convex optimization and learning problems have been studied widely in the
last few years. Other work has examined projected Langevin algorithms for
sampling from log-concave distributions restricted to convex compact sets. For
learning and optimization, log-concave distributions correspond to convex
losses. In this paper, we analyze the case of non-convex losses with compact
convex constraint sets and IID external data variables. We term the resulting
method the projected stochastic gradient Langevin algorithm (PSGLA). We show
the algorithm achieves a deviation of $O(T^{-1/4}(\log T)^{1/2})$ from its
target distribution in 1-Wasserstein distance. For optimization and learning,
we show that the algorithm achieves $\epsilon$-suboptimal solutions, on
average, provided that it is run for a time that is polynomial in
$\epsilon^{-1}$ and slightly super-exponential in the problem dimension.
</p>
<a href="http://arxiv.org/abs/2012.12137" target="_blank">arXiv:2012.12137</a> [<a href="http://arxiv.org/pdf/2012.12137" target="_blank">pdf</a>]

<h2>Projection-Free Bandit Optimization with Privacy Guarantees. (arXiv:2012.12138v1 [cs.LG])</h2>
<h3>Alina Ene, Huy L. Nguyen, Adrian Vladu</h3>
<p>We design differentially private algorithms for the bandit convex
optimization problem in the projection-free setting. This setting is important
whenever the decision set has a complex geometry, and access to it is done
efficiently only through a linear optimization oracle, hence Euclidean
projections are unavailable (e.g. matroid polytope, submodular base polytope).
This is the first differentially-private algorithm for projection-free bandit
optimization, and in fact our bound of $\widetilde{O}(T^{3/4})$ matches the
best known non-private projection-free algorithm (Garber-Kretzu, AISTATS `20)
and the best known private algorithm, even for the weaker setting when
projections are available (Smith-Thakurta, NeurIPS `13).
</p>
<a href="http://arxiv.org/abs/2012.12138" target="_blank">arXiv:2012.12138</a> [<a href="http://arxiv.org/pdf/2012.12138" target="_blank">pdf</a>]

<h2>Image to Bengali Caption Generation Using Deep CNN and Bidirectional Gated Recurrent Unit. (arXiv:2012.12139v1 [cs.CV])</h2>
<h3>Al Momin Faruk, Hasan Al Faraby, Md. Muzahidul Azad, Md. Riduyan Fedous, Md. Kishor Morol</h3>
<p>There is very little notable research on generating descriptions of the
Bengali language. About 243 million people speak in Bengali, and it is the 7th
most spoken language on the planet. The purpose of this research is to propose
a CNN and Bidirectional GRU based architecture model that generates natural
language captions in the Bengali language from an image. Bengali people can use
this research to break the language barrier and better understand each other's
perspectives. It will also help many blind people with their everyday lives.
This paper used an encoder-decoder approach to generate captions. We used a
pre-trained Deep convolutional neural network (DCNN) called InceptonV3image
embedding model as the encoder for analysis, classification, and annotation of
the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the
decoder to generate captions. Argmax and Beam search is used to produce the
highest possible quality of the captions. A new dataset called BNATURE is used,
which comprises 8000 images with five captions per image. It is used for
training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3,
BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.
</p>
<a href="http://arxiv.org/abs/2012.12139" target="_blank">arXiv:2012.12139</a> [<a href="http://arxiv.org/pdf/2012.12139" target="_blank">pdf</a>]

<h2>Learning to Initialize Gradient Descent Using Gradient Descent. (arXiv:2012.12141v1 [cs.LG])</h2>
<h3>Kartik Ahuja, Amit Dhurandhar, Kush R. Varshney</h3>
<p>Non-convex optimization problems are challenging to solve; the success and
computational expense of a gradient descent algorithm or variant depend heavily
on the initialization strategy. Often, either random initialization is used or
initialization rules are carefully designed by exploiting the nature of the
problem class. As a simple alternative to hand-crafted initialization rules, we
propose an approach for learning "good" initialization rules from previous
solutions. We provide theoretical guarantees that establish conditions that are
sufficient in all cases and also necessary in some under which our approach
performs better than random initialization. We apply our methodology to various
non-convex problems such as generating adversarial examples, generating post
hoc explanations for black-box machine learning models, and allocating
communication spectrum, and show consistent gains over other initialization
techniques.
</p>
<a href="http://arxiv.org/abs/2012.12141" target="_blank">arXiv:2012.12141</a> [<a href="http://arxiv.org/pdf/2012.12141" target="_blank">pdf</a>]

<h2>High-Speed Robot Navigation using Predicted Occupancy Maps. (arXiv:2012.12142v1 [cs.RO])</h2>
<h3>Kapil D. Katyal (1 and 2), Adam Polevoy (1), Joseph Moore (1), Craig Knuth (1), Katie M. Popek (1) ((1) Johns Hopkins University Applied Physics Lab, (2) Dept. of Comp. Sci., Johns Hopkins University)</h3>
<p>Safe and high-speed navigation is a key enabling capability for real world
deployment of robotic systems. A significant limitation of existing approaches
is the computational bottleneck associated with explicit mapping and the
limited field of view (FOV) of existing sensor technologies. In this paper, we
study algorithmic approaches that allow the robot to predict spaces extending
beyond the sensor horizon for robust planning at high speeds. We accomplish
this using a generative neural network trained from real-world data without
requiring human annotated labels. Further, we extend our existing control
algorithms to support leveraging the predicted spaces to improve collision-free
planning and navigation at high speeds. Our experiments are conducted on a
physical robot based on the MIT race car using an RGBD sensor where were able
to demonstrate improved performance at 4 m/s compared to a controller not
operating on predicted regions of the map.
</p>
<a href="http://arxiv.org/abs/2012.12142" target="_blank">arXiv:2012.12142</a> [<a href="http://arxiv.org/pdf/2012.12142" target="_blank">pdf</a>]

<h2>Convolutional Recurrent Network for Road Boundary Extraction. (arXiv:2012.12160v1 [cs.CV])</h2>
<h3>Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Shenlong Wang, Raquel Urtasun</h3>
<p>Creating high definition maps that contain precise information of static
elements of the scene is of utmost importance for enabling self driving cars to
drive safely. In this paper, we tackle the problem of drivable road boundary
extraction from LiDAR and camera imagery. Towards this goal, we design a
structured model where a fully convolutional network obtains deep features
encoding the location and direction of road boundaries and then, a
convolutional recurrent network outputs a polyline representation for each one
of them. Importantly, our method is fully automatic and does not require a user
in the loop. We showcase the effectiveness of our method on a large North
American city where we obtain perfect topology of road boundaries 99.3% of the
time at a high precision and recall.
</p>
<a href="http://arxiv.org/abs/2012.12160" target="_blank">arXiv:2012.12160</a> [<a href="http://arxiv.org/pdf/2012.12160" target="_blank">pdf</a>]

<h2>C-Watcher: A Framework for Early Detection of High-Risk Neighborhoods Ahead of COVID-19 Outbreak. (arXiv:2012.12169v1 [cs.LG])</h2>
<h3>Congxi Xiao, Jingbo Zhou, Jizhou Huang, An Zhuo, Ji Liu, Haoyi Xiong, Dejing Dou</h3>
<p>The novel coronavirus disease (COVID-19) has crushed daily routines and is
still rampaging through the world. Existing solution for nonpharmaceutical
interventions usually needs to timely and precisely select a subset of
residential urban areas for containment or even quarantine, where the spatial
distribution of confirmed cases has been considered as a key criterion for the
subset selection. While such containment measure has successfully stopped or
slowed down the spread of COVID-19 in some countries, it is criticized for
being inefficient or ineffective, as the statistics of confirmed cases are
usually time-delayed and coarse-grained. To tackle the issues, we propose
C-Watcher, a novel data-driven framework that aims at screening every
neighborhood in a target city and predicting infection risks, prior to the
spread of COVID-19 from epicenters to the city. In terms of design, C-Watcher
collects large-scale long-term human mobility data from Baidu Maps, then
characterizes every residential neighborhood in the city using a set of
features based on urban mobility patterns. Furthermore, to transfer the
firsthand knowledge (witted in epicenters) to the target city before local
outbreaks, we adopt a novel adversarial encoder framework to learn
"city-invariant" representations from the mobility-related features for precise
early detection of high-risk neighborhoods, even before any confirmed cases
known, in the target city. We carried out extensive experiments on C-Watcher
using the real-data records in the early stage of COVID-19 outbreaks, where the
results demonstrate the efficiency and effectiveness of C-Watcher for early
detection of high-risk neighborhoods from a large number of cities.
</p>
<a href="http://arxiv.org/abs/2012.12169" target="_blank">arXiv:2012.12169</a> [<a href="http://arxiv.org/pdf/2012.12169" target="_blank">pdf</a>]

<h2>Latent Feature Representation via Unsupervised Learning for Pattern Discovery in Massive Electron Microscopy Image Volumes. (arXiv:2012.12175v1 [cs.CV])</h2>
<h3>Gary B Huang, Huei-Fang Yang, Shin-ya Takemura, Pat Rivlin, Stephen M Plaza</h3>
<p>We propose a method to facilitate exploration and analysis of new large data
sets. In particular, we give an unsupervised deep learning approach to learning
a latent representation that captures semantic similarity in the data set. The
core idea is to use data augmentations that preserve semantic meaning to
generate synthetic examples of elements whose feature representations should be
close to one another.

We demonstrate the utility of our method applied to nano-scale electron
microscopy data, where even relatively small portions of animal brains can
require terabytes of image data. Although supervised methods can be used to
predict and identify known patterns of interest, the scale of the data makes it
difficult to mine and analyze patterns that are not known a priori. We show the
ability of our learned representation to enable query by example, so that if a
scientist notices an interesting pattern in the data, they can be presented
with other locations with matching patterns. We also demonstrate that
clustering of data in the learned space correlates with biologically-meaningful
distinctions. Finally, we introduce a visualization tool and software ecosystem
to facilitate user-friendly interactive analysis and uncover interesting
biological patterns. In short, our work opens possible new avenues in
understanding of and discovery in large data sets, arising in domains such as
EM analysis.
</p>
<a href="http://arxiv.org/abs/2012.12175" target="_blank">arXiv:2012.12175</a> [<a href="http://arxiv.org/pdf/2012.12175" target="_blank">pdf</a>]

<h2>Quantum Convolutional Neural Networks for High Energy Physics Data Analysis. (arXiv:2012.12177v1 [cs.LG])</h2>
<h3>Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, Shinjae Yoo</h3>
<p>This work presents a quantum convolutional neural network (QCNN) for the
classification of high energy physics events. The proposed model is tested
using a simulated dataset from the Deep Underground Neutrino Experiment. The
proposed architecture demonstrates the quantum advantage of learning faster
than the classical convolutional neural networks (CNNs) under a similar number
of parameters. In addition to faster convergence, the QCNN achieves greater
test accuracy compared to CNNs. Based on experimental results, it is a
promising direction to study the application of QCNN and other quantum machine
learning models in high energy physics and additional scientific fields.
</p>
<a href="http://arxiv.org/abs/2012.12177" target="_blank">arXiv:2012.12177</a> [<a href="http://arxiv.org/pdf/2012.12177" target="_blank">pdf</a>]

<h2>Learning to Play Imperfect-Information Games by Imitating an Oracle Planner. (arXiv:2012.12186v1 [cs.AI])</h2>
<h3>Rinu Boney, Alexander Ilin, Juho Kannala, Jarno Sepp&#xe4;nen</h3>
<p>We consider learning to play multiplayer imperfect-information games with
simultaneous moves and large state-action spaces. Previous attempts to tackle
such challenging games have largely focused on model-free learning methods,
often requiring hundreds of years of experience to produce competitive agents.
Our approach is based on model-based planning. We tackle the problem of partial
observability by first building an (oracle) planner that has access to the full
state of the environment and then distilling the knowledge of the oracle to a
(follower) agent which is trained to play the imperfect-information game by
imitating the oracle's choices. We experimentally show that planning with naive
Monte Carlo tree search does not perform very well in large combinatorial
action spaces. We therefore propose planning with a fixed-depth tree search and
decoupled Thompson sampling for action selection. We show that the planner is
able to discover efficient playing strategies in the games of Clash Royale and
Pommerman and the follower policy successfully learns to implement them by
training on a few hundred battles.
</p>
<a href="http://arxiv.org/abs/2012.12186" target="_blank">arXiv:2012.12186</a> [<a href="http://arxiv.org/pdf/2012.12186" target="_blank">pdf</a>]

<h2>Query Answering via Decentralized Search. (arXiv:2012.12192v1 [cs.AI])</h2>
<h3>Liang Ma, Mudhakar Srivatsa, Derya Cansever, Xifeng Yan, Sue Kase, Michelle Vanni</h3>
<p>Expert networks are formed by a group of expert-professionals with different
specialties to collaboratively resolve specific queries posted to the network.
In such networks, when a query reaches an expert who does not have sufficient
expertise, this query needs to be routed to other experts for further
processing until it is completely solved; therefore, query answering efficiency
is sensitive to the underlying query routing mechanism being used. Among all
possible query routing mechanisms, decentralized search, operating purely on
each expert's local information without any knowledge of network global
structure, represents the most basic and scalable routing mechanism, which is
applicable to any network scenarios even in dynamic networks. However, there is
still a lack of fundamental understanding of the efficiency of decentralized
search in expert networks. In this regard, we investigate decentralized search
by quantifying its performance under a variety of network settings. Our key
findings reveal the existence of network conditions, under which decentralized
search can achieve significantly short query routing paths (i.e., between
$O(\log n)$ and $O(\log^2 n)$ hops, $n$: total number of experts in the
network). Based on such theoretical foundation, we further study how the unique
properties of decentralized search in expert networks is related to the
anecdotal small-world phenomenon. In addition, we demonstrate that
decentralized search is robust against estimation errors introduced by
misinterpreting the required expertise levels. To the best of our knowledge,
this is the first work studying fundamental behaviors of decentralized search
in expert networks. The developed performance bounds, confirmed by real
datasets, are able to assist in predicting network performance and designing
complex expert networks.
</p>
<a href="http://arxiv.org/abs/2012.12192" target="_blank">arXiv:2012.12192</a> [<a href="http://arxiv.org/pdf/2012.12192" target="_blank">pdf</a>]

<h2>Labels Are Not Perfect: Inferring Spatial Uncertainty in Object Detection. (arXiv:2012.12195v1 [cs.CV])</h2>
<h3>Di Feng, Zining Wang, Yiyang Zhou, Lars Rosenbaum, Fabian Timm, Klaus Dietmayer, Masayoshi Tomizuka, Wei Zhan</h3>
<p>The availability of many real-world driving datasets is a key reason behind
the recent progress of object detection algorithms in autonomous driving.
However, there exist ambiguity or even failures in object labels due to
error-prone annotation process or sensor observation noise. Current public
object detection datasets only provide deterministic object labels without
considering their inherent uncertainty, as does the common training process or
evaluation metrics for object detectors. As a result, an in-depth evaluation
among different object detection methods remains challenging, and the training
process of object detectors is sub-optimal, especially in probabilistic object
detection. In this work, we infer the uncertainty in bounding box labels from
LiDAR point clouds based on a generative model, and define a new representation
of the probabilistic bounding box through a spatial uncertainty distribution.
Comprehensive experiments show that the proposed model reflects complex
environmental noises in LiDAR perception and the label quality. Furthermore, we
propose Jaccard IoU (JIoU) as a new evaluation metric that extends IoU by
incorporating label uncertainty. We conduct an in-depth comparison among
several LiDAR-based object detectors using the JIoU metric. Finally, we
incorporate the proposed label uncertainty in a loss function to train a
probabilistic object detector and to improve its detection accuracy. We verify
our proposed methods on two public datasets (KITTI, Waymo), as well as on
simulation data. Code is released at https://bit.ly/2W534yo.
</p>
<a href="http://arxiv.org/abs/2012.12195" target="_blank">arXiv:2012.12195</a> [<a href="http://arxiv.org/pdf/2012.12195" target="_blank">pdf</a>]

<h2>Autonomous sPOMDP Environment Modeling With Partial Model Exploitation. (arXiv:2012.12203v1 [cs.LG])</h2>
<h3>Andrew Wilhelm, Aaron Wilhelm, Garrett Fosdick</h3>
<p>A state space representation of an environment is a classic and yet powerful
tool used by many autonomous robotic systems for efficient and often optimal
solution planning. However, designing these representations with high
performance is laborious and costly, necessitating an effective and versatile
tool for autonomous generation of state spaces for autonomous robots. We
present a novel state space exploration algorithm by extending the original
surprise-based partially-observable Markov Decision Processes (sPOMDP), and
demonstrate its effective long-term exploration planning performance in various
environments. Through extensive simulation experiments, we show the proposed
model significantly increases efficiency and scalability of the original sPOMDP
learning techniques with a range of 31-63% gain in training speed while
improving robustness in environments with less deterministic transitions. Our
results pave the way for extending sPOMDP solutions to a broader set of
environments.
</p>
<a href="http://arxiv.org/abs/2012.12203" target="_blank">arXiv:2012.12203</a> [<a href="http://arxiv.org/pdf/2012.12203" target="_blank">pdf</a>]

<h2>FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations. (arXiv:2012.12206v1 [cs.LG])</h2>
<h3>Yichi Zhang, Junhao Pan, Xinheng Liu, Hongzheng Chen, Deming Chen, Zhiru Zhang</h3>
<p>Binary neural networks (BNNs) have 1-bit weights and activations. Such
networks are well suited for FPGAs, as their dominant computations are bitwise
arithmetic and the memory requirement is also significantly reduced. However,
compared to start-of-the-art compact convolutional neural network (CNN) models,
BNNs tend to produce a much lower accuracy on realistic datasets such as
ImageNet. In addition, the input layer of BNNs has gradually become a major
compute bottleneck, because it is conventionally excluded from binarization to
avoid a large accuracy loss. This work proposes FracBNN, which exploits
fractional activations to substantially improve the accuracy of BNNs.
Specifically, our approach employs a dual-precision activation scheme to
compute features with up to two bits, using an additional sparse binary
convolution. We further binarize the input layer using a novel thermometer
encoding. Overall, FracBNN preserves the key benefits of conventional BNNs,
where all convolutional layers are computed in pure binary MAC operations
(BMACs). We design an efficient FPGA-based accelerator for our novel BNN model
that supports the fractional activations. To evaluate the performance of
FracBNN under a resource-constrained scenario, we implement the entire
optimized network architecture on an embedded FPGA (Xilinx Ultra96v2). Our
experiments on ImageNet show that FracBNN achieves an accuracy comparable to
MobileNetV2, surpassing the best-known BNN design on FPGAs with an increase of
28.9% in top-1 accuracy and a 2.5x reduction in model size. FracBNN also
outperforms a recently introduced BNN model with an increase of 2.4% in top-1
accuracy while using the same model size. On the embedded FPGA device, FracBNN
demonstrates the ability of real-time image classification.
</p>
<a href="http://arxiv.org/abs/2012.12206" target="_blank">arXiv:2012.12206</a> [<a href="http://arxiv.org/pdf/2012.12206" target="_blank">pdf</a>]

<h2>Emergent Hand Morphology and Control from Optimizing Robust Grasps of Diverse Objects. (arXiv:2012.12209v1 [cs.RO])</h2>
<h3>Xinlei Pan, Animesh Garg, Animashree Anandkumar, Yuke Zhu</h3>
<p>Evolution in nature illustrates that the creatures' biological structure and
their sensorimotor skills adapt to the environmental changes for survival.
Likewise, the ability to morph and acquire new skills can facilitate an
embodied agent to solve tasks of varying complexities. In this work, we
introduce a data-driven approach where effective hand designs naturally emerge
for the purpose of grasping diverse objects. Jointly optimizing morphology and
control imposes computational challenges since it requires constant evaluation
of a black-box function that measures the performance of a combination of
embodiment and behavior. We develop a novel Bayesian Optimization algorithm
that efficiently co-designs the morphology and grasping skills jointly through
learned latent-space representations. We design the grasping tasks based on a
taxonomy of three human grasp types: power grasp, pinch grasp, and lateral
grasp. Through experimentation and comparative study, we demonstrate the
effectiveness of our approach in discovering robust and cost-efficient hand
morphologies for grasping novel objects.
</p>
<a href="http://arxiv.org/abs/2012.12209" target="_blank">arXiv:2012.12209</a> [<a href="http://arxiv.org/pdf/2012.12209" target="_blank">pdf</a>]

<h2>Geometric robust descriptor for 3D point cloud. (arXiv:2012.12215v1 [cs.CV])</h2>
<h3>Seung Hwan Jung, Yeong-Gil Shin, Minyoung Chung</h3>
<p>We propose rotation robust and density robust local geometric descriptor.
Local geometric feature of point cloud is used in many applications, for
example, to find correspondences in 3D registration and to segment local
regions. Usually, application accuracy depends on the discriminative power of
the local geometric features. However, there are some problems such as point
sparsity, rotated point cloud, and so on. In this paper, we present new local
feature generation method to make a rotation robust and density robust
descriptor. First, we place kernels aligned around each point and align them to
the normal of the point. To avoid the sign problem of the normal vector, we use
symmetric kernel point distribution with respect to the tangent plane. Next,
from each kernel point, we estimate geometric information which is rotation
robust and discriminative. Finally, we operate convolution process with
consideration of kernel point structure, and aggregate all kernel features. We
experiment our local descriptors on the ModelNet40 dataset for registration and
classification, and the ShapeNet part dataset for segmentation. Our descriptor
shows discriminative power regardless of point distribution.
</p>
<a href="http://arxiv.org/abs/2012.12215" target="_blank">arXiv:2012.12215</a> [<a href="http://arxiv.org/pdf/2012.12215" target="_blank">pdf</a>]

<h2>BKT-LSTM: Efficient Student Modeling for knowledge tracing and student performance prediction. (arXiv:2012.12218v1 [cs.AI])</h2>
<h3>Sein Minn</h3>
<p>Recently, we have seen a rapid rise in usage of online educational platforms.
The personalized education became crucially important in future learning
environments. Knowledge tracing (KT) refers to the detection of students'
knowledge states and predict future performance given their past outcomes for
providing adaptive solution to Intelligent Tutoring Systems (ITS). Bayesian
Knowledge Tracing (BKT) is a model to capture mastery level of each skill
independently with psychologically meaningful parameters and widely used in
successful tutoring systems. However it has lower efficiency in student
performance prediction and is unable to detect learning transfer across skills.
While recent KT models based on deep neural networks shows impressive student
performance prediction but it came with a price. Ten of thousands of parameters
in neural networks are unable to provide psychologically meaningful
interpretation that reflect to cognitive theory. In this paper, we proposed an
efficient knowledge tracing model (BKT-LSTM) that is able to provide meaningful
interpretation (where individual \textit{skill mastery} of a student is learned
by BKT and \textit{learning transfer} (across skills) is detected by k-means
clustering) and then \textit{problem difficulty} is taken into account in
student performance prediction by leveraging predictive power of LSTM. BKT-LSTM
outperforms state-of-the-art student models in term of predictive power whilst
considering skill mastery with psychologically meaningful interpretation of
BKT.
</p>
<a href="http://arxiv.org/abs/2012.12218" target="_blank">arXiv:2012.12218</a> [<a href="http://arxiv.org/pdf/2012.12218" target="_blank">pdf</a>]

<h2>Training Convolutional Neural Networks With Hebbian Principal Component Analysis. (arXiv:2012.12229v1 [cs.CV])</h2>
<h3>Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro</h3>
<p>Recent work has shown that biologically plausible Hebbian learning can be
integrated with backpropagation learning (backprop), when training deep
convolutional neural networks. In particular, it has been shown that Hebbian
learning can be used for training the lower or the higher layers of a neural
network. For instance, Hebbian learning is effective for re-training the higher
layers of a pre-trained deep neural network, achieving comparable accuracy
w.r.t. SGD, while requiring fewer training epochs, suggesting potential
applications for transfer learning. In this paper we build on these results and
we further improve Hebbian learning in these settings, by using a nonlinear
Hebbian Principal Component Analysis (HPCA) learning rule, in place of the
Hebbian Winner Takes All (HWTA) strategy used in previous work. We test this
approach in the context of computer vision. In particular, the HPCA rule is
used to train Convolutional Neural Networks in order to extract relevant
features from the CIFAR-10 image dataset. The HPCA variant that we explore
further improves the previous results, motivating further interest towards
biologically plausible learning algorithms.
</p>
<a href="http://arxiv.org/abs/2012.12229" target="_blank">arXiv:2012.12229</a> [<a href="http://arxiv.org/pdf/2012.12229" target="_blank">pdf</a>]

<h2>Unadversarial Examples: Designing Objects for Robust Vision. (arXiv:2012.12235v1 [cs.CV])</h2>
<h3>Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry, Ashish Kapoor</h3>
<p>We study a class of realistic computer vision settings wherein one can
influence the design of the objects being recognized. We develop a framework
that leverages this capability to significantly improve vision models'
performance and robustness. This framework exploits the sensitivity of modern
machine learning algorithms to input perturbations in order to design "robust
objects," i.e., objects that are explicitly optimized to be confidently
detected or classified. We demonstrate the efficacy of the framework on a wide
variety of vision-based tasks ranging from standard benchmarks, to
(in-simulation) robotics, to real-world experiments. Our code can be found at
https://git.io/unadversarial .
</p>
<a href="http://arxiv.org/abs/2012.12235" target="_blank">arXiv:2012.12235</a> [<a href="http://arxiv.org/pdf/2012.12235" target="_blank">pdf</a>]

<h2>Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Deforming Scene from Monocular Video. (arXiv:2012.12247v1 [cs.CV])</h2>
<h3>Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh&#xf6;fer, Christoph Lassner, Christian Theobalt</h3>
<p>In this tech report, we present the current state of our ongoing work on
reconstructing Neural Radiance Fields (NERF) of general non-rigid scenes via
ray bending. Non-rigid NeRF (NR-NeRF) takes RGB images of a deforming object
(e.g., from a monocular video) as input and then learns a geometry and
appearance representation that not only allows to reconstruct the input
sequence but also to re-render any time step into novel camera views with high
fidelity. In particular, we show that a consumer-grade camera is sufficient to
synthesize convincing bullet-time videos of short and simple scenes. In
addition, the resulting representation enables correspondence estimation across
views and time, and provides rigidity scores for each point in the scene. We
urge the reader to watch the supplemental videos for qualitative results.
</p>
<a href="http://arxiv.org/abs/2012.12247" target="_blank">arXiv:2012.12247</a> [<a href="http://arxiv.org/pdf/2012.12247" target="_blank">pdf</a>]

<h2>Underwater image filtering: methods, datasets and evaluation. (arXiv:2012.12258v1 [cs.CV])</h2>
<h3>Chau Yi Li, Riccardo Mazzon, Andrea Cavallaro</h3>
<p>Underwater images are degraded by the selective attenuation of light that
distorts colours and reduces contrast. The degradation extent depends on the
water type, the distance between an object and the camera, and the depth under
the water surface the object is at. Underwater image filtering aims to restore
or to enhance the appearance of objects captured in an underwater image.
Restoration methods compensate for the actual degradation, whereas enhancement
methods improve either the perceived image quality or the performance of
computer vision algorithms. The growing interest in underwater image filtering
methods--including learning-based approaches used for both restoration and
enhancement--and the associated challenges call for a comprehensive review of
the state of the art. In this paper, we review the design principles of
filtering methods and revisit the oceanology background that is fundamental to
identify the degradation causes. We discuss image formation models and the
results of restoration methods in various water types. Furthermore, we present
task-dependent enhancement methods and categorise datasets for training neural
networks and for method evaluation. Finally, we discuss evaluation strategies,
including subjective tests and quality assessment measures. We complement this
survey with a platform ( https://puiqe.eecs.qmul.ac.uk/ ), which hosts
state-of-the-art underwater filtering methods and facilitates comparisons.
</p>
<a href="http://arxiv.org/abs/2012.12258" target="_blank">arXiv:2012.12258</a> [<a href="http://arxiv.org/pdf/2012.12258" target="_blank">pdf</a>]

<h2>YolactEdge: Real-time Instance Segmentation on the Edge (Jetson AGX Xavier: 30 FPS, RTX 2080 Ti: 170 FPS). (arXiv:2012.12259v1 [cs.CV])</h2>
<h3>Haotian Liu, Rafael A. Rivera Soto, Fanyi Xiao, Yong Jae Lee</h3>
<p>We propose YolactEdge, the first competitive instance segmentation approach
that runs on small edge devices at real-time speeds. Specifically, YolactEdge
runs at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti)
with a ResNet-101 backbone on 550x550 resolution images. To achieve this, we
make two improvements to the state-of-the-art image-based real-time method
YOLACT: (1) TensorRT optimization while carefully trading off speed and
accuracy, and (2) a novel feature warping module to exploit temporal redundancy
in videos. Experiments on the YouTube VIS and MS COCO datasets demonstrate that
YolactEdge produces a 3-5x speed up over existing real-time methods while
producing competitive mask and box detection accuracy. We also conduct ablation
studies to dissect our design choices and modules. Code and models are
available at https://github.com/haotian-liu/yolact_edge.
</p>
<a href="http://arxiv.org/abs/2012.12259" target="_blank">arXiv:2012.12259</a> [<a href="http://arxiv.org/pdf/2012.12259" target="_blank">pdf</a>]

<h2>Time-Travel Rephotography. (arXiv:2012.12261v1 [cs.CV])</h2>
<h3>Xuan Luo, Xuaner Zhang, Paul Yoo, Ricardo Martin-Brualla, Jason Lawrence, Steven M. Seitz</h3>
<p>Many historical people are captured only in old, faded, black and white
photos, that have been distorted by the limitations of early cameras and the
passage of time. This paper simulates traveling back in time with a modern
camera to rephotograph famous subjects. Unlike conventional image restoration
filters which apply independent operations like denoising, colorization, and
superresolution, we leverage the StyleGAN2 framework to project old photos into
the space of modern high-resolution photos, achieving all of these effects in a
unified framework. A unique challenge with this approach is capturing the
identity and pose of the photo's subject and not the many artifacts in
low-quality antique photos. Our comparisons to current state-of-the-art
restoration filters show significant improvements and compelling results for a
variety of important historical people.
</p>
<a href="http://arxiv.org/abs/2012.12261" target="_blank">arXiv:2012.12261</a> [<a href="http://arxiv.org/pdf/2012.12261" target="_blank">pdf</a>]

<h2>The Last State of Artificial Intelligence in Project Management. (arXiv:2012.12262v1 [cs.AI])</h2>
<h3>Mohammad Reza Davahli</h3>
<p>Artificial intelligence (AI) has been used to advance different fields, such
as education, healthcare, and finance. However, the application of AI in the
field of project management (PM) has not progressed equally. This paper reports
on a systematic review of the published studies used to investigate the
application of AI in PM. This systematic review identified relevant papers
using Web of Science, Science Direct, and Google Scholar databases. Of the 652
articles found, 58 met the predefined criteria and were included in the review.
Included papers were classified per the following dimensions: PM knowledge
areas, PM processes, and AI techniques. The results indicated that the
application of AI in PM was in its early stages and AI models have not applied
for multiple PM processes especially in processes groups of project stakeholder
management, project procurements management, and project communication
management. However, the most popular PM processes among included papers were
project effort prediction and cost estimation, and the most popular AI
techniques were support vector machines, neural networks, and genetic
algorithms.
</p>
<a href="http://arxiv.org/abs/2012.12262" target="_blank">arXiv:2012.12262</a> [<a href="http://arxiv.org/pdf/2012.12262" target="_blank">pdf</a>]

<h2>Compressive Sensing Using Iterative Hard Thresholding with Low Precision Data Representation: Theory and Applications. (arXiv:1802.04907v4 [stat.ML] UPDATED)</h2>
<h3>Nezihe Merve G&#xfc;rel, Kaan Kara, Alen Stojanov, Tyler Smith, Thomas Lemmin, Dan Alistarh, Markus P&#xfc;schel, Ce Zhang</h3>
<p>Modern scientific instruments produce vast amounts of data, which can
overwhelm the processing ability of computer systems. Lossy compression of data
is an intriguing solution, but comes with its own drawbacks, such as potential
signal loss, and the need for careful optimization of the compression ratio. In
this work, we focus on a setting where this problem is especially acute:
compressive sensing frameworks for interferometry and medical imaging. We ask
the following question: can the precision of the data representation be lowered
for all inputs, with recovery guarantees and practical performance? Our first
contribution is a theoretical analysis of the normalized Iterative Hard
Thresholding (IHT) algorithm when all input data, meaning both the measurement
matrix and the observation vector are quantized aggressively. We present a
variant of low precision normalized {IHT} that, under mild conditions, can
still provide recovery guarantees. The second contribution is the application
of our quantization framework to radio astronomy and magnetic resonance
imaging. We show that lowering the precision of the data can significantly
accelerate image recovery. We evaluate our approach on telescope data and
samples of brain images using CPU and FPGA implementations achieving up to a 9x
speed-up with negligible loss of recovery quality.
</p>
<a href="http://arxiv.org/abs/1802.04907" target="_blank">arXiv:1802.04907</a> [<a href="http://arxiv.org/pdf/1802.04907" target="_blank">pdf</a>]

<h2>Convex Programming Based Spectral Clustering. (arXiv:1805.04246v3 [cs.LG] UPDATED)</h2>
<h3>Tomohiko Mizutani</h3>
<p>Clustering is a fundamental task in data analysis, and spectral clustering
has been recognized as a promising approach to it. Given a graph describing the
relationship between data, spectral clustering explores the underlying cluster
structure in two stages. The first stage embeds the nodes of the graph in real
space, and the second stage groups the embedded nodes into several clusters.
The use of the $k$-means method in the grouping stage is currently standard
practice. We present a spectral clustering algorithm that uses convex
programming in the grouping stage and study how well it works. This algorithm
is designed based on the following observation. If a graph is well-clustered,
then the nodes with the largest degree in each cluster can be found by
computing an enclosing ellipsoid of the nodes embedded in real space, and the
clusters can be identified by using those nodes. We show that, for
well-clustered graphs, the algorithm can find clusters of nodes with minimal
conductance. We also give an experimental assessment of the algorithm's
performance.
</p>
<a href="http://arxiv.org/abs/1805.04246" target="_blank">arXiv:1805.04246</a> [<a href="http://arxiv.org/pdf/1805.04246" target="_blank">pdf</a>]

<h2>Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. (arXiv:1906.01529v5 [cs.LG] UPDATED)</h2>
<h3>Zhengwei Wang, Qi She, Tomas E. Ward</h3>
<p>Generative adversarial networks (GANs) have been extensively studied in the
past few years. Arguably their most significant impact has been in the area of
computer vision where great advances have been made in challenges such as
plausible image generation, image-to-image translation, facial attribute
manipulation and similar domains. Despite the significant successes achieved to
date, applying GANs to real-world problems still poses significant challenges,
three of which we focus on here. These are: (1) the generation of high quality
images, (2) diversity of image generation, and (3) stable training. Focusing on
the degree to which popular GAN technologies have made progress against these
challenges, we provide a detailed review of the state of the art in GAN-related
research in the published scientific literature. We further structure this
review through a convenient taxonomy we have adopted based on variations in GAN
architectures and loss functions. While several reviews for GANs have been
presented to date, none have considered the status of this field based on their
progress towards addressing practical challenges relevant to computer vision.
Accordingly, we review and critically discuss the most popular
architecture-variant, and loss-variant GANs, for tackling these challenges. Our
objective is to provide an overview as well as a critical analysis of the
status of GAN research in terms of relevant progress towards important computer
vision application requirements. As we do this we also discuss the most
compelling applications in computer vision in which GANs have demonstrated
considerable success along with some suggestions for future research
directions. Code related to GAN-variants studied in this work is summarized on
https://github.com/sheqi/GAN_Review.
</p>
<a href="http://arxiv.org/abs/1906.01529" target="_blank">arXiv:1906.01529</a> [<a href="http://arxiv.org/pdf/1906.01529" target="_blank">pdf</a>]

<h2>PuzzleFlex: kinematic motion of chains with loose joints. (arXiv:1906.08708v2 [cs.RO] UPDATED)</h2>
<h3>Samuel Lensgraf, Karim Itani, Yinan Zhang, Zezhou Sun, Yijia Wu, Alberto Quattrini Li, Bo Zhu, Emily Whiting, Weifu Wang, Devin Balkcom</h3>
<p>This paper presents a method of computing free motions of a planar assembly
of rigid bodies connected by loose joints. Joints are modeled using local
distance constraints, which are then linearized with respect to configuration
space velocities, yielding a linear programming formulation that allows
analysis of systems with thousands of rigid bodies. Potential applications
include analysis of collections of modular robots, structural stability
perturbation analysis, tolerance analysis for mechanical systems, and formation
control of mobile robots.
</p>
<a href="http://arxiv.org/abs/1906.08708" target="_blank">arXiv:1906.08708</a> [<a href="http://arxiv.org/pdf/1906.08708" target="_blank">pdf</a>]

<h2>FLAML: A Fast and Lightweight AutoML Library. (arXiv:1911.04706v2 [cs.LG] UPDATED)</h2>
<h3>Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu</h3>
<p>We study the problem of using low computational cost to automate the choices
of learners and hyperparameters for an ad-hoc training dataset and error
metric, by conducting trials of different configurations on the given training
data. We investigate the joint impact of multiple factors on both trial cost
and model error, and propose several design guidelines. Following them, we
build a fast and lightweight library FLAML which optimizes for low
computational resource in finding accurate models. FLAML integrates several
simple but effective search strategies into an adaptive system. It
significantly outperforms top-ranked AutoML libraries on a large open source
AutoML benchmark under equal, or sometimes orders of magnitude smaller budget
constraints.
</p>
<a href="http://arxiv.org/abs/1911.04706" target="_blank">arXiv:1911.04706</a> [<a href="http://arxiv.org/pdf/1911.04706" target="_blank">pdf</a>]

<h2>What You See is What You Get: Exploiting Visibility for 3D Object Detection. (arXiv:1912.04986v3 [cs.CV] UPDATED)</h2>
<h3>Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan</h3>
<p>Recent advances in 3D sensing have created unique challenges for computer
vision. One fundamental challenge is finding a good representation for 3D
sensor data. Most popular representations (such as PointNet) are proposed in
the context of processing truly 3D data (e.g. points sampled from mesh models),
ignoring the fact that 3D sensored data such as a LiDAR sweep is in fact 2.5D.
We argue that representing 2.5D data as collections of (x, y, z) points
fundamentally destroys hidden information about freespace. In this paper, we
demonstrate such knowledge can be efficiently recovered through 3D raycasting
and readily incorporated into batch-based gradient learning. We describe a
simple approach to augmenting voxel-based networks with visibility: we add a
voxelized visibility map as an additional input stream. In addition, we show
that visibility can be combined with two crucial modifications common to
state-of-the-art 3D detectors: synthetic data augmentation of virtual objects
and temporal aggregation of LiDAR sweeps over multiple time frames. On the
NuScenes 3D detection benchmark, we show that, by adding an additional stream
for visibility input, we can significantly improve the overall detection
accuracy of a state-of-the-art 3D detector.
</p>
<a href="http://arxiv.org/abs/1912.04986" target="_blank">arXiv:1912.04986</a> [<a href="http://arxiv.org/pdf/1912.04986" target="_blank">pdf</a>]

<h2>Neighborhood Structure Assisted Non-negative Matrix Factorization and its Application in Unsupervised Point-wise Anomaly Detection. (arXiv:2001.06541v2 [cs.LG] UPDATED)</h2>
<h3>Imtiaz Ahmed, Xia Ben Hu, Mithun P. Acharya, Yu Ding</h3>
<p>Dimensionality reduction is considered as an important step for ensuring
competitive performance in unsupervised learning such as anomaly detection.
Non-negative matrix factorization (NMF) is a popular and widely used method to
accomplish this goal. But NMF do not have the provision to include the
neighborhood structure information and, as a result, may fail to provide
satisfactory performance in presence of nonlinear manifold structure. To
address that shortcoming, we propose to consider and incorporate the
neighborhood structural similarity information within the NMF framework by
modeling the data through a minimum spanning tree. We label the resulting
method as the neighborhood structure assisted NMF. We further devise both
offline and online algorithmic versions of the proposed method. Empirical
comparisons using twenty benchmark datasets as well as an industrial dataset
extracted from a hydropower plant demonstrate the superiority of the
neighborhood structure assisted NMF and support our claim of merit. Looking
closer into the formulation and properties of the neighborhood structure
assisted NMF with other recent, enhanced versions of NMF reveals that inclusion
of the neighborhood structure information using MST plays a key role in
attaining the enhanced performance in anomaly detection.
</p>
<a href="http://arxiv.org/abs/2001.06541" target="_blank">arXiv:2001.06541</a> [<a href="http://arxiv.org/pdf/2001.06541" target="_blank">pdf</a>]

<h2>Gradient Surgery for Multi-Task Learning. (arXiv:2001.06782v4 [cs.LG] UPDATED)</h2>
<h3>Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn</h3>
<p>While deep learning and deep reinforcement learning (RL) systems have
demonstrated impressive results in domains such as image classification, game
playing, and robotic control, data efficiency remains a major challenge.
Multi-task learning has emerged as a promising approach for sharing structure
across multiple tasks to enable more efficient learning. However, the
multi-task setting presents a number of optimization challenges, making it
difficult to realize large efficiency gains compared to learning tasks
independently. The reasons why multi-task learning is so challenging compared
to single-task learning are not fully understood. In this work, we identify a
set of three conditions of the multi-task optimization landscape that cause
detrimental gradient interference, and develop a simple yet general approach
for avoiding such interference between task gradients. We propose a form of
gradient surgery that projects a task's gradient onto the normal plane of the
gradient of any other task that has a conflicting gradient. On a series of
challenging multi-task supervised and multi-task RL problems, this approach
leads to substantial gains in efficiency and performance. Further, it is
model-agnostic and can be combined with previously-proposed multi-task
architectures for enhanced performance.
</p>
<a href="http://arxiv.org/abs/2001.06782" target="_blank">arXiv:2001.06782</a> [<a href="http://arxiv.org/pdf/2001.06782" target="_blank">pdf</a>]

<h2>Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? -- A Neural Tangent Kernel Perspective. (arXiv:2002.06262v2 [cs.LG] UPDATED)</h2>
<h3>Kaixuan Huang, Yuqing Wang, Molei Tao, Tuo Zhao</h3>
<p>Deep residual networks (ResNets) have demonstrated better generalization
performance than deep feedforward networks (FFNets). However, the theory behind
such a phenomenon is still largely unknown. This paper studies this fundamental
problem in deep learning from a so-called "neural tangent kernel" perspective.
Specifically, we first show that under proper conditions, as the width goes to
infinity, training deep ResNets can be viewed as learning reproducing kernel
functions with some kernel function. We then compare the kernel of deep ResNets
with that of deep FFNets and discover that the class of functions induced by
the kernel of FFNets is asymptotically not learnable, as the depth goes to
infinity. In contrast, the class of functions induced by the kernel of ResNets
does not exhibit such degeneracy. Our discovery partially justifies the
advantages of deep ResNets over deep FFNets in generalization abilities.
Numerical results are provided to support our claim.
</p>
<a href="http://arxiv.org/abs/2002.06262" target="_blank">arXiv:2002.06262</a> [<a href="http://arxiv.org/pdf/2002.06262" target="_blank">pdf</a>]

<h2>Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability. (arXiv:2002.07399v3 [stat.ML] UPDATED)</h2>
<h3>Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai Chen, Shaojie Tang, Zhihua Wu</h3>
<p>Federated learning is a new distributed machine learning framework, where a
bunch of heterogeneous clients collaboratively train a model without sharing
training data. In this work, we consider a practical and ubiquitous issue when
deploying federated learning in mobile environments: intermittent client
availability, where the set of eligible clients may change during the training
process. Such intermittent client availability would seriously deteriorate the
performance of the classical Federated Averaging algorithm (FedAvg for short).
Thus, we propose a simple distributed non-convex optimization algorithm, called
Federated Latest Averaging (FedLaAvg for short), which leverages the latest
gradients of all clients, even when the clients are not available, to jointly
update the global model in each iteration. Our theoretical analysis shows that
FedLaAvg attains the convergence rate of $O(E^{1/2}/(N^{1/4} T^{1/2}))$,
achieving a sublinear speedup with respect to the total number of clients. We
implement FedLaAvg along with several baselines and evaluate them over the
benchmarking MNIST and Sentiment140 datasets. The evaluation results
demonstrate that FedLaAvg achieves more stable training than FedAvg in both
convex and non-convex settings and indeed reaches a sublinear speedup.
</p>
<a href="http://arxiv.org/abs/2002.07399" target="_blank">arXiv:2002.07399</a> [<a href="http://arxiv.org/pdf/2002.07399" target="_blank">pdf</a>]

<h2>Coded Federated Learning. (arXiv:2002.09574v2 [cs.LG] UPDATED)</h2>
<h3>Sagar Dhakal, Saurav Prakash, Yair Yona, Shilpa Talwar, Nageen Himayat</h3>
<p>Federated learning is a method of training a global model from decentralized
data distributed across client devices. Here, model parameters are computed
locally by each client device and exchanged with a central server, which
aggregates the local models for a global view, without requiring sharing of
training data. The convergence performance of federated learning is severely
impacted in heterogeneous computing platforms such as those at the wireless
edge, where straggling computations and communication links can significantly
limit timely model parameter updates. This paper develops a novel coded
computing technique for federated learning to mitigate the impact of
stragglers. In the proposed Coded Federated Learning (CFL) scheme, each client
device privately generates parity training data and shares it with the central
server only once at the start of the training phase. The central server can
then preemptively perform redundant gradient computations on the composite
parity data to compensate for the erased or delayed parameter updates. Our
results show that CFL allows the global model to converge nearly four times
faster when compared to an uncoded approach
</p>
<a href="http://arxiv.org/abs/2002.09574" target="_blank">arXiv:2002.09574</a> [<a href="http://arxiv.org/pdf/2002.09574" target="_blank">pdf</a>]

<h2>Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study. (arXiv:2003.06152v3 [cs.LG] UPDATED)</h2>
<h3>Assaf Dauber, Meir Feder, Tomer Koren, Roi Livni</h3>
<p>The notion of implicit bias, or implicit regularization, has been suggested
as a means to explain the surprising generalization ability of modern-days
overparameterized learning algorithms. This notion refers to the tendency of
the optimization algorithm towards a certain structured solution that often
generalizes well. Recently, several papers have studied implicit regularization
and were able to identify this phenomenon in various scenarios. We revisit this
paradigm in arguably the simplest non-trivial setup, and study the implicit
bias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex
Optimization. As a first step, we provide a simple construction that rules out
the existence of a \emph{distribution-independent} implicit regularizer that
governs the generalization ability of SGD. We then demonstrate a learning
problem that rules out a very general class of \emph{distribution-dependent}
implicit regularizers from explaining generalization, which includes strongly
convex regularizers as well as non-degenerate norm-based regularizations.
Certain aspects of our constructions point out to significant difficulties in
providing a comprehensive explanation of an algorithm's generalization
performance by solely arguing about its implicit regularization properties.
</p>
<a href="http://arxiv.org/abs/2003.06152" target="_blank">arXiv:2003.06152</a> [<a href="http://arxiv.org/pdf/2003.06152" target="_blank">pdf</a>]

<h2>Continual Learning with Node-Importance based Adaptive Group Sparse Regularization. (arXiv:2003.13726v3 [cs.LG] UPDATED)</h2>
<h3>Sangwon Jung, Hongjoon Ahn, Sungmin Cha, Taesup Moon</h3>
<p>We propose a novel regularization-based continual learning method, dubbed as
Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group
sparsity-based penalties. Our method selectively employs the two penalties when
learning each node based its the importance, which is adaptively updated after
learning each new task. By utilizing the proximal gradient descent method for
learning, the exact sparsity and freezing of the model is guaranteed, and thus,
the learner can explicitly control the model capacity as the learning
continues. Furthermore, as a critical detail, we re-initialize the weights
associated with unimportant nodes after learning each task in order to prevent
the negative transfer that causes the catastrophic forgetting and facilitate
efficient learning of new tasks. Throughout the extensive experimental results,
we show that our AGS-CL uses much less additional memory space for storing the
regularization parameters, and it significantly outperforms several
state-of-the-art baselines on representative continual learning benchmarks for
both supervised and reinforcement learning tasks.
</p>
<a href="http://arxiv.org/abs/2003.13726" target="_blank">arXiv:2003.13726</a> [<a href="http://arxiv.org/pdf/2003.13726" target="_blank">pdf</a>]

<h2>Adaptive Partial Scanning Transmission Electron Microscopy with Reinforcement Learning. (arXiv:2004.02786v4 [cs.LG] UPDATED)</h2>
<h3>Jeffrey M. Ede</h3>
<p>Compressed sensing can decrease scanning transmission electron microscopy
electron dose and scan time with minimal information loss. Traditionally,
sparse scans used in compressed sensing sample a static set of probing
locations. However, dynamic scans that adapt to specimens are expected to be
able to match or surpass the performance of static scans as static scans are a
subset of possible dynamic scans. Thus, we present a prototype for a contiguous
sparse scan system that piecewise adapts scan paths to specimens as they are
scanned. Sampling directions for scan segments are chosen by a recurrent neural
network based on previously observed scan segments. The recurrent neural
network is trained by reinforcement learning to cooperate with a feedforward
convolutional neural network that completes the sparse scans. This paper
presents our learning policy, experiments, and example partial scans, and
discusses future research directions. Source code, pretrained models, and
training data is openly accessible at
https://github.com/Jeffrey-Ede/adaptive-scans
</p>
<a href="http://arxiv.org/abs/2004.02786" target="_blank">arXiv:2004.02786</a> [<a href="http://arxiv.org/pdf/2004.02786" target="_blank">pdf</a>]

<h2>A survey on modern trainable activation functions. (arXiv:2005.00817v2 [cs.LG] UPDATED)</h2>
<h3>Andrea Apicella, Francesco Donnarumma, Francesco Isgr&#xf2;, Roberto Prevete</h3>
<p>In neural networks literature, there is a strong interest in identifying and
defining activation functions which can improve neural network performance. In
recent years there has been a renovated interest of the scientific community in
investigating activation functions which can be trained during the learning
process, usually referred to as "trainable", "learnable" or "adaptable"
activation functions. They appear to lead to better network performance.
Diverse and heterogeneous models of trainable activation function have been
proposed in the literature. In this paper, we present a survey of these models.
Starting from a discussion on the use of the term "activation function" in
literature, we propose a taxonomy of trainable activation functions, highlight
common and distinctive proprieties of recent and past models, and discuss main
advantages and limitations of this type of approach. We show that many of the
proposed approaches are equivalent to adding neuron layers which use fixed
(non-trainable) activation functions and some simple local rule that
constraints the corresponding weight layers.
</p>
<a href="http://arxiv.org/abs/2005.00817" target="_blank">arXiv:2005.00817</a> [<a href="http://arxiv.org/pdf/2005.00817" target="_blank">pdf</a>]

<h2>Double Generative Adversarial Networks for Conditional Independence Testing. (arXiv:2006.02615v2 [stat.ML] UPDATED)</h2>
<h3>Chengchun Shi, Tianlin Xu, Wicher Bergsma, Lexin Li</h3>
<p>In this article, we consider the problem of high-dimensional conditional
independence testing, which is a key building block in statistics and machine
learning. We propose a double generative adversarial networks (GANs)-based
inference procedure. We first introduce a double GANs framework to learn two
generators, and integrate the two generators to construct a doubly-robust test
statistic. We next consider multiple generalized covariance measures, and take
their maximum as our test statistic. Finally, we obtain the empirical
distribution of our test statistic through multiplier bootstrap. We show that
our test controls type-I error, while the power approaches one asymptotically.
More importantly, these theoretical guarantees are obtained under much weaker
and practically more feasible conditions compared to existing tests. We
demonstrate the efficacy of our test through both synthetic and real datasets.
</p>
<a href="http://arxiv.org/abs/2006.02615" target="_blank">arXiv:2006.02615</a> [<a href="http://arxiv.org/pdf/2006.02615" target="_blank">pdf</a>]

<h2>Stealing Deep Reinforcement Learning Models for Fun and Profit. (arXiv:2006.05032v2 [cs.LG] UPDATED)</h2>
<h3>Kangjie Chen, Shangwei Guo, Tianwei Zhang, Xiaofei Xie, Yang Liu</h3>
<p>This paper presents the first model extraction attack against Deep
Reinforcement Learning (DRL), which enables an external adversary to precisely
recover a black-box DRL model only from its interaction with the environment.
Model extraction attacks against supervised Deep Learning models have been
widely studied. However, those techniques cannot be applied to the
reinforcement learning scenario due to DRL models' high complexity,
stochasticity and limited observable information. We propose a novel
methodology to overcome the above challenges. The key insight of our approach
is that the process of DRL model extraction is equivalent to imitation
learning, a well-established solution to learn sequential decision-making
policies. Based on this observation, our methodology first builds a classifier
to reveal the training algorithm family of the targeted black-box DRL model
only based on its predicted actions, and then leverages state-of-the-art
imitation learning techniques to replicate the model from the identified
algorithm family. Experimental results indicate that our methodology can
effectively recover the DRL models with high fidelity and accuracy. We also
demonstrate two use cases to show that our model extraction attack can (1)
significantly improve the success rate of adversarial attacks, and (2) steal
DRL models stealthily even they are protected by DNN watermarks. These pose a
severe threat to the intellectual property and privacy protection of DRL
applications.
</p>
<a href="http://arxiv.org/abs/2006.05032" target="_blank">arXiv:2006.05032</a> [<a href="http://arxiv.org/pdf/2006.05032" target="_blank">pdf</a>]

<h2>Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty. (arXiv:2006.06015v2 [cs.CV] UPDATED)</h2>
<h3>Miguel Monteiro, Lo&#xef;c Le Folgoc, Daniel Coelho de Castro, Nick Pawlowski, Bernardo Marques, Konstantinos Kamnitsas, Mark van der Wilk, Ben Glocker</h3>
<p>In image segmentation, there is often more than one plausible solution for a
given input. In medical imaging, for example, experts will often disagree about
the exact location of object boundaries. Estimating this inherent uncertainty
and predicting multiple plausible hypotheses is of great interest in many
applications, yet this ability is lacking in most current deep learning
methods. In this paper, we introduce stochastic segmentation networks (SSNs),
an efficient probabilistic method for modelling aleatoric uncertainty with any
image segmentation network architecture. In contrast to approaches that produce
pixel-wise estimates, SSNs model joint distributions over entire label maps and
thus can generate multiple spatially coherent hypotheses for a single image. By
using a low-rank multivariate normal distribution over the logit space to model
the probability of the label map given the image, we obtain a spatially
consistent probability distribution that can be efficiently computed by a
neural network without any changes to the underlying architecture. We tested
our method on the segmentation of real-world medical data, including lung
nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform
state-of-the-art for modelling correlated uncertainty in ambiguous images while
being much simpler, more flexible, and more efficient.
</p>
<a href="http://arxiv.org/abs/2006.06015" target="_blank">arXiv:2006.06015</a> [<a href="http://arxiv.org/pdf/2006.06015" target="_blank">pdf</a>]

<h2>Asymptotic Errors for Teacher-Student Convex Generalized Linear Models (or : How to Prove Kabashima's Replica Formula). (arXiv:2006.06581v4 [stat.ML] UPDATED)</h2>
<h3>Cedric Gerbelot, Alia Abbara, Florent Krzakala</h3>
<p>There has been a recent surge of interest in the study of asymptotic
reconstruction performance in various cases of generalized linear estimation
problems in the teacher-student setting, especially for the case of i.i.d
standard normal matrices. Here, we go beyond these matrices, and prove an
analytical formula for the reconstruction performance of convex generalized
linear models with rotationally-invariant data matrices with arbitrary bounded
spectrum, rigorously confirming a conjecture originally derived using the
replica method from statistical physics. The formula includes many problems
such as compressed sensing or sparse logistic classification. The proof is
achieved by leveraging on message passing algorithms and the statistical
properties of their iterates, allowing to characterize the asymptotic empirical
distribution of the estimator. Our proof is crucially based on the construction
of converging sequences of an oracle multi-layer vector approximate message
passing algorithm, where the convergence analysis is done by checking the
stability of an equivalent dynamical system. We illustrate our claim with
numerical examples on mainstream learning methods such as sparse logistic
regression and linear support vector classifiers, showing excellent agreement
between moderate size simulation and the asymptotic prediction.
</p>
<a href="http://arxiv.org/abs/2006.06581" target="_blank">arXiv:2006.06581</a> [<a href="http://arxiv.org/pdf/2006.06581" target="_blank">pdf</a>]

<h2>Categorical Normalizing Flows via Continuous Transformations. (arXiv:2006.09790v2 [cs.LG] UPDATED)</h2>
<h3>Phillip Lippe, Efstratios Gavves</h3>
<p>Despite their popularity, to date, the application of normalizing flows on
categorical data stays limited. The current practice of using dequantization to
map discrete data to a continuous space is inapplicable as categorical data has
no intrinsic order. Instead, categorical data have complex and latent relations
that must be inferred, like the synonymy between words. In this paper, we
investigate \emph{Categorical Normalizing Flows}, that is normalizing flows for
categorical data. By casting the encoding of categorical data in continuous
space as a variational inference problem, we jointly optimize the continuous
representation and the model likelihood. Using a factorized decoder, we
introduce an inductive bias to model any interactions in the normalizing flow.
As a consequence, we do not only simplify the optimization compared to having a
joint decoder, but also make it possible to scale up to a large number of
categories that is currently impossible with discrete normalizing flows. Based
on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant
generative model on graphs. GraphCNF implements a three step approach modeling
the nodes, edges and adjacency matrix stepwise to increase efficiency. On
molecule generation, GraphCNF outperforms both one-shot and autoregressive
flow-based state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2006.09790" target="_blank">arXiv:2006.09790</a> [<a href="http://arxiv.org/pdf/2006.09790" target="_blank">pdf</a>]

<h2>Approximate Simulation for Template-Based Whole-Body Control. (arXiv:2006.09921v3 [cs.RO] UPDATED)</h2>
<h3>Vince Kurtz, Patrick M. Wensing, Hai Lin</h3>
<p>Reduced-order template models are widely used to control high
degree-of-freedom legged robots, but existing methods for template-based
whole-body control rely heavily on heuristics and often suffer from robustness
issues. In this letter, we propose a template-based whole-body control method
grounded in the formal framework of approximate simulation. Our central
contribution is to demonstrate how the Hamiltonian structure of rigid-body
dynamics can be exploited to establish approximate simulation for a
high-dimensional nonlinear system. The resulting controller is passive, more
robust to push disturbances, uneven terrain, and modeling errors than standard
QP-based methods, and naturally enables high center of mass walking. Our
theoretical results are supported by simulation experiments with a 30
degree-of-freedom Valkyrie humanoid model.
</p>
<a href="http://arxiv.org/abs/2006.09921" target="_blank">arXiv:2006.09921</a> [<a href="http://arxiv.org/pdf/2006.09921" target="_blank">pdf</a>]

<h2>Noticing Motion Patterns: Temporal CNN with a Novel Convolution Operator for Human Trajectory Prediction. (arXiv:2007.00862v3 [cs.CV] UPDATED)</h2>
<h3>Dapeng Zhao, Jean Oh</h3>
<p>We propose a Convolutional Neural Network-based approach to learn, detect,and
extract patterns in sequential trajectory data, known here as Social Pattern
Extraction Convolution (Social-PEC). A set of experiments carried out on the
human trajectory prediction problem shows that our model performs comparably to
the state of the art and outperforms in some cases. More importantly,the
proposed approach unveils the obscurity in the previous use of pooling layer,
presenting a way to intuitively explain the decision-making process.
</p>
<a href="http://arxiv.org/abs/2007.00862" target="_blank">arXiv:2007.00862</a> [<a href="http://arxiv.org/pdf/2007.00862" target="_blank">pdf</a>]

<h2>Region-based Non-local Operation for Video Classification. (arXiv:2007.09033v4 [cs.CV] UPDATED)</h2>
<h3>Guoxi Huang, Adrian G. Bors</h3>
<p>Convolutional Neural Networks (CNNs) model long-range dependencies by deeply
stacking convolution operations with small window sizes, which makes the
optimizations difficult. This paper presents region-based non-local (RNL)
operations as a family of self-attention mechanisms, which can directly capture
long-range dependencies without using a deep stack of local operations. Given
an intermediate feature map, our method recalibrates the feature at a position
by aggregating the information from the neighboring regions of all positions.
By combining a channel attention module with the proposed RNL, we design an
attention chain, which can be integrated into the off-the-shelf CNNs for
end-to-end training. We evaluate our method on two video classification
benchmarks. The experimental results of our method outperform other attention
mechanisms, and we achieve state-of-the-art performance on the
Something-Something V1 dataset.
</p>
<a href="http://arxiv.org/abs/2007.09033" target="_blank">arXiv:2007.09033</a> [<a href="http://arxiv.org/pdf/2007.09033" target="_blank">pdf</a>]

<h2>A Novel Framework for Spatio-Temporal Prediction of Environmental Data Using Deep Learning. (arXiv:2007.11836v2 [stat.ML] UPDATED)</h2>
<h3>Federico Amato, Fabian Guignard, Sylvain Robert, Mikhail Kanevski</h3>
<p>As the role played by statistical and computational sciences in climate and
environmental modelling and prediction becomes more important, Machine Learning
researchers are becoming more aware of the relevance of their work to help
tackle the climate crisis. Indeed, being universal nonlinear function
approximation tools, Machine Learning algorithms are efficient in analysing and
modelling spatially and temporally variable environmental data. While Deep
Learning models have proved to be able to capture spatial, temporal, and
spatio-temporal dependencies through their automatic feature representation
learning, the problem of the interpolation of continuous spatio-temporal fields
measured on a set of irregular points in space is still under-investigated. To
fill this gap, we introduce here a framework for spatio-temporal prediction of
climate and environmental data using deep learning. Specifically, we show how
spatio-temporal processes can be decomposed in terms of a sum of products of
temporally referenced basis functions, and of stochastic spatial coefficients
which can be spatially modelled and mapped on a regular grid, allowing the
reconstruction of the complete spatio-temporal signal. Applications on two case
studies based on simulated and real-world data will show the effectiveness of
the proposed framework in modelling coherent spatio-temporal fields.
</p>
<a href="http://arxiv.org/abs/2007.11836" target="_blank">arXiv:2007.11836</a> [<a href="http://arxiv.org/pdf/2007.11836" target="_blank">pdf</a>]

<h2>A Self-Training Approach for Point-Supervised Object Detection and Counting in Crowds. (arXiv:2007.12831v2 [cs.CV] UPDATED)</h2>
<h3>Yi Wang, Junhui Hou, Xinyu Hou, Lap-Pui Chau</h3>
<p>In this paper, we propose a novel self-training approach that enables a
typical object detector trained only with point-level annotations (i.e.,
objects are labeled with points) to estimate both the center points and sizes
of crowded objects. Specifically, during training, we utilize the available
point annotations to supervise the estimation of the center points of objects
directly. Based on a locally-uniform distribution assumption, we initialize
pseudo object sizes from the point-level supervisory information, which are
then leveraged to guide the regression of object sizes via a crowdedness-aware
loss. Meanwhile, we propose a confidence and order-aware refinement scheme to
continuously refine the initial pseudo object sizes such that the ability of
the detector is increasingly boosted to detect and count objects in crowds
simultaneously. Moreover, to address extremely crowded scenes, we propose an
effective decoding method to improve the detector's representation ability.
Experimental results on the WiderFace benchmark show that our approach
significantly outperforms state-of-the-art point-supervised methods under both
detection and counting tasks, i.e., our method improves the average precision
by more than 10\% and reduces the counting error by 31.2\%. Besides, our method
obtains the best results on the crowd counting and localization datasets (i.e.,
ShanghaiTech and NWPU-Crowd) and vehicle counting datasets (i.e., CARPK and
PUCPR+) compared with state-of-the-art counting-by-detection methods.
</p>
<a href="http://arxiv.org/abs/2007.12831" target="_blank">arXiv:2007.12831</a> [<a href="http://arxiv.org/pdf/2007.12831" target="_blank">pdf</a>]

<h2>Deep Active Learning for Solvability Prediction in Power Systems. (arXiv:2007.13250v2 [cs.LG] UPDATED)</h2>
<h3>Yichen Zhang, Jianzhe Liu, Feng Qiu, Tianqi Hong, Rui Yao</h3>
<p>Traditional methods for solvability region analysis can only have inner
approximations with inconclusive conservatism. Machine learning methods have
been proposed to approach the real region. In this letter, we propose a deep
active learning framework for power system solvability prediction. Compared
with the passive learning methods where the training is performed after all
instances are labeled, the active learning selects most informative instances
to be label and therefore significantly reduce the size of labeled dataset for
training. In the active learning framework, the acquisition functions, which
correspond to different sampling strategies, are defined in terms of the
on-the-fly posterior probability from the classifier. The IEEE 39-bus system is
employed to validate the proposed framework, where a two-dimensional case is
illustrated to visualize the effectiveness of the sampling method followed by
the full-dimensional numerical experiments.
</p>
<a href="http://arxiv.org/abs/2007.13250" target="_blank">arXiv:2007.13250</a> [<a href="http://arxiv.org/pdf/2007.13250" target="_blank">pdf</a>]

<h2>An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites. (arXiv:2008.01882v3 [cs.CV] UPDATED)</h2>
<h3>Giovanni Pasqualino, Antonino Furnari, Giovanni Signorello, Giovanni Maria Farinella</h3>
<p>Recognizing artworks in a cultural site using images acquired from the user's
point of view (First Person Vision) allows to build interesting applications
for both the visitors and the site managers. However, current object detection
algorithms working in fully supervised settings need to be trained with large
quantities of labeled data, whose collection requires a lot of times and high
costs in order to achieve good performance. Using synthetic data generated from
the 3D model of the cultural site to train the algorithms can reduce these
costs. On the other hand, when these models are tested with real images, a
significant drop in performance is observed due to the differences between real
and synthetic images. In this study we consider the problem of Unsupervised
Domain Adaptation for object detection in cultural sites. To address this
problem, we created a new dataset containing both synthetic and real images of
16 different artworks. We hence investigated different domain adaptation
techniques based on one-stage and two-stage object detector, image-to-image
translation and feature alignment. Based on the observation that single-stage
detectors are more robust to the domain shift in the considered settings, we
proposed a new method which builds on RetinaNet and feature alignment that we
called DA-RetinaNet. The proposed approach achieves better results than
compared methods on the proposed dataset and on Cityscapes. To support research
in this field we release the dataset at the following link
https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the proposed
architecture at https://github.com/fpv-iplab/DA-RetinaNet.
</p>
<a href="http://arxiv.org/abs/2008.01882" target="_blank">arXiv:2008.01882</a> [<a href="http://arxiv.org/pdf/2008.01882" target="_blank">pdf</a>]

<h2>Quaternion Graph Neural Networks. (arXiv:2008.05089v3 [cs.LG] UPDATED)</h2>
<h3>Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung</h3>
<p>Recently, graph neural networks (GNNs) become a principal research direction
to learn low-dimensional continuous embeddings of nodes and graphs to predict
node and graph labels, respectively. However, Euclidean embeddings have high
distortion when using GNNs to model complex graphs such as social networks.
Furthermore, existing GNNs are not very efficient with the high number of model
parameters when increasing the number of hidden layers. Therefore, we move
beyond the Euclidean space to a hyper-complex vector space to improve graph
representation quality and reduce the number of model parameters. To this end,
we propose quaternion graph neural networks (QGNN) to generalize GCNs within
the Quaternion space to learn quaternion embeddings for nodes and graphs. The
Quaternion space, a hyper-complex vector space, provides highly meaningful
computations through Hamilton product compared to the Euclidean and complex
vector spaces. As a result, our QGNN can reduce the model size up to four times
and enhance learning better graph representations. Experimental results show
that the proposed QGNN produces state-of-the-art accuracies on a range of
well-known benchmark datasets for three downstream tasks, including graph
classification, semi-supervised node classification, and text (node)
classification. Our code is available at: https://github.com/daiquocnguyen/QGNN
</p>
<a href="http://arxiv.org/abs/2008.05089" target="_blank">arXiv:2008.05089</a> [<a href="http://arxiv.org/pdf/2008.05089" target="_blank">pdf</a>]

<h2>How to Put Users in Control of their Data in Federated Top-N Recommendation with Learning to Rank. (arXiv:2008.07192v4 [cs.LG] UPDATED)</h2>
<h3>Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara, Fedelucio Narducci</h3>
<p>Recommendation services are extensively adopted in several user-centered
applications as a tool to alleviate the information overload problem and help
users in orienteering in a vast space of possible choices. In such scenarios,
data ownership is a crucial concern since users may not be willing to share
their sensitive preferences (e.g., visited locations) with a central server.
Unfortunately, data harvesting and collection is at the basis of modern,
state-of-the-art approaches to recommendation. To address this issue, we
present FPL, an architecture in which users collaborate in training a central
factorization model while controlling the amount of sensitive data leaving
their devices. The proposed approach implements pair-wise learning-to-rank
optimization by following the Federated Learning principles, originally
conceived to mitigate the privacy risks of traditional machine learning. The
public implementation is available at https://split.to/sisinflab-fpl.
</p>
<a href="http://arxiv.org/abs/2008.07192" target="_blank">arXiv:2008.07192</a> [<a href="http://arxiv.org/pdf/2008.07192" target="_blank">pdf</a>]

<h2>A deep active inference model of the rubber-hand illusion. (arXiv:2008.07408v2 [cs.AI] UPDATED)</h2>
<h3>Thomas Rood, Marcel van Gerven, Pablo Lanillos</h3>
<p>Understanding how perception and action deal with sensorimotor conflicts,
such as the rubber-hand illusion (RHI), is essential to understand how the body
adapts to uncertain situations. Recent results in humans have shown that the
RHI not only produces a change in the perceived arm location, but also causes
involuntary forces. Here, we describe a deep active inference agent in a
virtual environment, which we subjected to the RHI, that is able to account for
these results. We show that our model, which deals with visual high-dimensional
inputs, produces similar perceptual and force patterns to those found in
humans.
</p>
<a href="http://arxiv.org/abs/2008.07408" target="_blank">arXiv:2008.07408</a> [<a href="http://arxiv.org/pdf/2008.07408" target="_blank">pdf</a>]

<h2>Stochastic Markov Gradient Descent and Training Low-Bit Neural Networks. (arXiv:2008.11117v2 [cs.LG] UPDATED)</h2>
<h3>Jonathan Ashbrock, Alexander M. Powell</h3>
<p>The massive size of modern neural networks has motivated substantial recent
interest in neural network quantization. We introduce Stochastic Markov
Gradient Descent (SMGD), a discrete optimization method applicable to training
quantized neural networks. The SMGD algorithm is designed for settings where
memory is highly constrained during training. We provide theoretical guarantees
of algorithm performance as well as encouraging numerical results.
</p>
<a href="http://arxiv.org/abs/2008.11117" target="_blank">arXiv:2008.11117</a> [<a href="http://arxiv.org/pdf/2008.11117" target="_blank">pdf</a>]

<h2>Adaptive Exploitation of Pre-trained Deep Convolutional Neural Networks for Robust Visual Tracking. (arXiv:2008.13015v2 [cs.CV] UPDATED)</h2>
<h3>Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei</h3>
<p>Due to the automatic feature extraction procedure via multi-layer nonlinear
transformations, the deep learning-based visual trackers have recently achieved
great success in challenging scenarios for visual tracking purposes. Although
many of those trackers utilize the feature maps from pre-trained convolutional
neural networks (CNNs), the effects of selecting different models and
exploiting various combinations of their feature maps are still not compared
completely. To the best of our knowledge, all those methods use a fixed number
of convolutional feature maps without considering the scene attributes (e.g.,
occlusion, deformation, and fast motion) that might occur during tracking. As a
pre-requisition, this paper proposes adaptive discriminative correlation
filters (DCF) based on the methods that can exploit CNN models with different
topologies. First, the paper provides a comprehensive analysis of four commonly
used CNN models to determine the best feature maps of each model. Second, with
the aid of analysis results as attribute dictionaries, adaptive exploitation of
deep features is proposed to improve the accuracy and robustness of visual
trackers regarding video characteristics. Third, the generalization of the
proposed method is validated on various tracking datasets as well as CNN models
with similar architectures. Finally, extensive experimental results demonstrate
the effectiveness of the proposed adaptive method compared with
state-of-the-art visual tracking methods.
</p>
<a href="http://arxiv.org/abs/2008.13015" target="_blank">arXiv:2008.13015</a> [<a href="http://arxiv.org/pdf/2008.13015" target="_blank">pdf</a>]

<h2>On Communication Compression for Distributed Optimization on Heterogeneous Data. (arXiv:2009.02388v2 [cs.LG] UPDATED)</h2>
<h3>Sebastian U. Stich</h3>
<p>Lossy gradient compression, with either unbiased or biased compressors, has
become a key tool to avoid the communication bottleneck in centrally
coordinated distributed training of machine learning models. We analyze the
performance of two standard and general types of methods: (i) distributed
quantized SGD (D-QSGD) with arbitrary unbiased quantizers and (ii) distributed
SGD with error-feedback and biased compressors (D-EF-SGD) in the heterogeneous
(non-iid) data setting. Our results indicate that D-EF-SGD is much less
affected than D-QSGD by non-iid data, but both methods can suffer a slowdown if
data-skewness is high. We further study two alternatives that are not (or much
less) affected by heterogenous data distributions: first, a recently proposed
method that is effective on strongly convex problems, and secondly, we point
out a more general approach that is applicable to linear compressors only but
effective in all considered scenarios.
</p>
<a href="http://arxiv.org/abs/2009.02388" target="_blank">arXiv:2009.02388</a> [<a href="http://arxiv.org/pdf/2009.02388" target="_blank">pdf</a>]

<h2>Effective Federated Adaptive Gradient Methods with Non-IID Decentralized Data. (arXiv:2009.06557v2 [cs.LG] UPDATED)</h2>
<h3>Qianqian Tong, Guannan Liang, Jinbo Bi</h3>
<p>Federated learning allows loads of edge computing devices to collaboratively
learn a global model without data sharing. The analysis with partial device
participation under non-IID and unbalanced data reflects more reality. In this
work, we propose federated learning versions of adaptive gradient methods -
Federated AGMs - which employ both the first-order and second-order momenta, to
alleviate generalization performance deterioration caused by dissimilarity of
data population among devices. To further improve the test performance, we
compare several schemes of calibration for the adaptive learning rate,
including the standard Adam calibrated by $\epsilon$, $p$-Adam, and one
calibrated by an activation function. Our analysis provides the first set of
theoretical results that the proposed (calibrated) Federated AGMs converge to a
first-order stationary point under non-IID and unbalanced data settings for
nonconvex optimization. We perform extensive experiments to compare these
federated learning methods with the state-of-the-art FedAvg, FedMomentum and
SCAFFOLD and to assess the different calibration schemes and the advantages of
AGMs over the current federated learning methods.
</p>
<a href="http://arxiv.org/abs/2009.06557" target="_blank">arXiv:2009.06557</a> [<a href="http://arxiv.org/pdf/2009.06557" target="_blank">pdf</a>]

<h2>Experimental Design for Overparameterized Learning with Application to Single Shot Deep Active Learning. (arXiv:2009.12820v2 [cs.LG] UPDATED)</h2>
<h3>Neta Shoham, Haim Avron</h3>
<p>The impressive performance exhibited by modern machine learning models hinges
on the ability to train such models on a very large amount of labeled data.
However, since access to large volumes of labeled data is often limited or
expensive, it is desirable to alleviate this bottleneck by carefully curating
the training set. Optimal experimental design is a well-established paradigm
for selecting data point to be labeled so to maximally inform the learning
process. Unfortunately, classical theory on optimal experimental design focuses
on selecting examples in order to learn underparameterized (and thus,
non-interpolative) models, while modern machine learning models such as deep
neural networks are overparameterized, and oftentimes are trained to be
interpolative. As such, classical experimental design methods are not
applicable in many modern learning setups. Indeed, the predictive performance
of underparameterized models tends to be variance dominated, so classical
experimental design focuses on variance reduction, while the predictive
performance of overparameterized models can also be, as is shown in this paper,
bias dominated or of mixed nature. In this paper we propose a design strategy
that is well suited for overparameterized regression and interpolation. We
demonstrate the applicability of our method in the context of deep learning by
proposing a new algorithm for single-shot deep active learning
</p>
<a href="http://arxiv.org/abs/2009.12820" target="_blank">arXiv:2009.12820</a> [<a href="http://arxiv.org/pdf/2009.12820" target="_blank">pdf</a>]

<h2>Mastering Atari with Discrete World Models. (arXiv:2010.02193v2 [cs.LG] UPDATED)</h2>
<h3>Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba</h3>
<p>Intelligent agents need to generalize from past experience to achieve goals
in complex environments. World models facilitate such generalization and allow
learning behaviors from imagined outcomes to increase sample-efficiency. While
learning world models from image inputs has recently become feasible for some
tasks, modeling Atari games accurately enough to derive successful behaviors
has remained an open challenge for many years. We introduce DreamerV2, a
reinforcement learning agent that learns behaviors purely from predictions in
the compact latent space of a powerful world model. The world model uses
discrete representations and is trained separately from the policy. DreamerV2
constitutes the first agent that achieves human-level performance on the Atari
benchmark of 55 tasks by learning behaviors inside a separately trained world
model. With the same computational budget and wall-clock time, DreamerV2
reaches 200M frames and exceeds the final performance of the top single-GPU
agents IQN and Rainbow.
</p>
<a href="http://arxiv.org/abs/2010.02193" target="_blank">arXiv:2010.02193</a> [<a href="http://arxiv.org/pdf/2010.02193" target="_blank">pdf</a>]

<h2>Trajectory Inspection: A Method for Iterative Clinician-Driven Design of Reinforcement Learning Studies. (arXiv:2010.04279v2 [cs.LG] UPDATED)</h2>
<h3>Christina X. Ji, Michael Oberst, Sanjat Kanjilal, David Sontag</h3>
<p>Reinforcement learning (RL) has the potential to significantly improve
clinical decision making. However, treatment policies learned via RL from
observational data are sensitive to subtle choices in study design. We
highlight a simple approach, trajectory inspection, to bring clinicians into an
iterative design process for model-based RL studies. We identify where the
model recommends unexpectedly aggressive treatments or expects surprisingly
positive outcomes from its recommendations. Then, we examine clinical
trajectories simulated with the learned model and policy alongside the actual
hospital course. Applying this approach to recent work on RL for sepsis
management, we uncover a model bias towards discharge, a preference for high
vasopressor doses that may be linked to small sample sizes, and clinically
implausible expectations of discharge without weaning off vasopressors. We hope
that iterations of detecting and addressing the issues unearthed by our method
will result in RL policies that inspire more confidence in deployment.
</p>
<a href="http://arxiv.org/abs/2010.04279" target="_blank">arXiv:2010.04279</a> [<a href="http://arxiv.org/pdf/2010.04279" target="_blank">pdf</a>]

<h2>Boosting Image-based Mutual Gaze Detection using Pseudo 3D Gaze. (arXiv:2010.07811v2 [cs.CV] UPDATED)</h2>
<h3>Bardia Doosti, Ching-Hui Chen, Raviteja Vemulapalli, Xuhui Jia, Yukun Zhu, Bradley Green</h3>
<p>Mutual gaze detection, i.e., predicting whether or not two people are looking
at each other, plays an important role in understanding human interactions. In
this work, we focus on the task of image-based mutual gaze detection, and
propose a simple and effective approach to boost the performance by using an
auxiliary 3D gaze estimation task during the training phase. We achieve the
performance boost without additional labeling cost by training the 3D gaze
estimation branch using pseudo 3D gaze labels deduced from mutual gaze labels.
By sharing the head image encoder between the 3D gaze estimation and the mutual
gaze detection branches, we achieve better head features than learned by
training the mutual gaze detection branch alone. Experimental results on three
image datasets show that the proposed approach improves the detection
performance significantly without additional annotations. This work also
introduces a new image dataset that consists of 33.1K pairs of humans annotated
with mutual gaze labels in 29.2K images.
</p>
<a href="http://arxiv.org/abs/2010.07811" target="_blank">arXiv:2010.07811</a> [<a href="http://arxiv.org/pdf/2010.07811" target="_blank">pdf</a>]

<h2>Multimodal Research in Vision and Language: A Review of Current and Emerging Trends. (arXiv:2010.09522v2 [cs.CV] UPDATED)</h2>
<h3>Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar, Soujanya Poria, Roger Zimmermann, Amir Zadeh</h3>
<p>Deep Learning and its applications have cascaded impactful research and
development with a diverse range of modalities present in the real-world data.
More recently, this has enhanced research interests in the intersection of the
Vision and Language arena with its numerous applications and fast-paced growth.
In this paper, we present a detailed overview of the latest trends in research
pertaining to visual and language modalities. We look at its applications in
their task formulations and how to solve various problems related to semantic
perception and content generation. We also address task-specific trends, along
with their evaluation strategies and upcoming challenges. Moreover, we shed
some light on multi-disciplinary patterns and insights that have emerged in the
recent past, directing this field towards more modular and transparent
intelligent systems. This survey identifies key trends gravitating recent
literature in VisLang research and attempts to unearth directions that the
field is heading towards.
</p>
<a href="http://arxiv.org/abs/2010.09522" target="_blank">arXiv:2010.09522</a> [<a href="http://arxiv.org/pdf/2010.09522" target="_blank">pdf</a>]

<h2>ENSURE: Ensemble Stein's Unbiased Risk Estimator for Unsupervised Learning. (arXiv:2010.10631v2 [cs.CV] UPDATED)</h2>
<h3>Hemant Kumar Aggarwal, Aniket Pramanik, Mathews Jacob</h3>
<p>Deep learning algorithms are emerging as powerful alternatives to compressed
sensing methods, offering improved image quality and computational efficiency.
Unfortunately, fully sampled training images may not be available or are
difficult to acquire in several applications, including high-resolution and
dynamic imaging. Previous studies in image reconstruction have utilized Stein's
Unbiased Risk Estimator (SURE) as a mean square error (MSE) estimate for the
image denoising step in an unrolled network. Unfortunately, the end-to-end
training of a network using SURE remains challenging since the projected SURE
loss is a poor approximation to the MSE, especially in the heavily undersampled
setting. We propose an ENsemble SURE (ENSURE) approach to train a deep network
only from undersampled measurements. In particular, we show that training a
network using an ensemble of images, each acquired with a different sampling
pattern, can closely approximate the MSE. Our preliminary experimental results
show that the proposed ENSURE approach gives comparable reconstruction quality
to supervised learning and a recent unsupervised learning method.
</p>
<a href="http://arxiv.org/abs/2010.10631" target="_blank">arXiv:2010.10631</a> [<a href="http://arxiv.org/pdf/2010.10631" target="_blank">pdf</a>]

<h2>Differentially Private Weighted Sampling. (arXiv:2010.13048v2 [cs.LG] UPDATED)</h2>
<h3>Edith Cohen, Ofir Geri, Tamas Sarlos, Uri Stemmer</h3>
<p>Common datasets have the form of elements with keys (e.g., transactions and
products) and the goal is to perform analytics on the aggregated form of key
and frequency pairs. A weighted sample of keys by (a function of) frequency is
a highly versatile summary that provides a sparse set of representative keys
and supports approximate evaluations of query statistics. We propose private
weighted sampling (PWS): A method that ensures element-level differential
privacy while retaining, to the extent possible, the utility of a respective
non-private weighted sample. PWS maximizes the reporting probabilities of keys
and estimation quality of a broad family of statistics. PWS improves over the
state of the art also for the well-studied special case of private histograms,
when no sampling is performed. We empirically demonstrate significant
performance gains compared with prior baselines: 20%-300% increase in key
reporting for common Zipfian frequency distributions and accuracy for $\times
2$-$ 8$ lower frequencies in estimation tasks. Moreover, PWS is applied as a
simple post-processing of a non-private sample, without requiring the original
data. This allows for seamless integration with existing implementations of
non-private schemes and retaining the efficiency of schemes designed for
resource-constrained settings such as massive distributed or streamed data. We
believe that due to practicality and performance, PWS may become a method of
choice in applications where privacy is desired.
</p>
<a href="http://arxiv.org/abs/2010.13048" target="_blank">arXiv:2010.13048</a> [<a href="http://arxiv.org/pdf/2010.13048" target="_blank">pdf</a>]

<h2>Learning from the Crowd with Pairwise Comparison. (arXiv:2011.01104v2 [cs.LG] UPDATED)</h2>
<h3>Shiwei Zeng, Jie Shen</h3>
<p>Efficient learning of halfspaces is arguably one of the most important
problems in machine learning and statistics. With the unprecedented growth of
large-scale data sets, it has become ubiquitous to appeal to crowd for data
annotation, and the central problem that attracts a surge of recent interests
is how one can provably learn the underlying halfspace from the highly noisy
crowd feedback. On the other hand, a large body of recent works have been
dedicated to the problem of learning with not only labels, but also pairwise
comparisons, since in many cases it is easier to compare than to label. In this
paper we study the problem of learning halfspaces from the crowd under the
realizable PAC learning setting, and we assume that the crowd workers can
provide (noisy) labels or pairwise comparison tags upon request. We show that
with a powerful boosting framework, together with our novel design of a
filtering process, the overhead (to be defined) of the crowd acts as a
constant, whereas the natural extension of standard approaches to crowd setting
leads to an overhead growing with the size of the data sets.
</p>
<a href="http://arxiv.org/abs/2011.01104" target="_blank">arXiv:2011.01104</a> [<a href="http://arxiv.org/pdf/2011.01104" target="_blank">pdf</a>]

<h2>Automated Hyperparameter Selection for the PC Algorithm. (arXiv:2011.01889v2 [stat.ML] UPDATED)</h2>
<h3>Eric V. Strobl</h3>
<p>The PC algorithm infers causal relations using conditional independence tests
that require a pre-specified Type I $\alpha$ level. PC is however unsupervised,
so we cannot tune $\alpha$ using traditional cross-validation. We therefore
propose AutoPC, a fast procedure that optimizes $\alpha$ directly for a user
chosen metric. We in particular force PC to double check its output by
executing a second run on the recovered graph. We choose the final output as
the one which maximizes stability between the two runs. AutoPC consistently
outperforms the state of the art across multiple metrics.
</p>
<a href="http://arxiv.org/abs/2011.01889" target="_blank">arXiv:2011.01889</a> [<a href="http://arxiv.org/pdf/2011.01889" target="_blank">pdf</a>]

<h2>Performance Analysis of Optimizers for Plant Disease Classification with Convolutional Neural Networks. (arXiv:2011.04056v2 [cs.CV] UPDATED)</h2>
<h3>Shreyas Rajesh Labhsetwar, Soumya Haridas, Riyali Panmand, Rutuja Deshpande, Piyush Arvind Kolte, Sandhya Pati</h3>
<p>Crop failure owing to pests &amp; diseases are inherent within Indian
agriculture, leading to annual losses of 15 to 25% of productivity, resulting
in a huge economic loss. This research analyzes the performance of various
optimizers for predictive analysis of plant diseases with deep learning
approach. The research uses Convolutional Neural Networks for classification of
farm or plant leaf samples of 3 crops into 15 classes. The various optimizers
used in this research include RMSprop, Adam and AMSgrad. Optimizers Performance
is visualised by plotting the Training and Validation Accuracy and Loss curves,
ROC curves and Confusion Matrix. The best performance is achieved using Adam
optimizer, with the maximum validation accuracy being 98%. This paper focuses
on the research analysis proving that plant diseases can be predicted and
pre-empted using deep learning methodology with the help of satellite, drone
based or mobile based images that result in reducing crop failure and
agricultural losses.
</p>
<a href="http://arxiv.org/abs/2011.04056" target="_blank">arXiv:2011.04056</a> [<a href="http://arxiv.org/pdf/2011.04056" target="_blank">pdf</a>]

<h2>Analysis of Dimensional Influence of Convolutional Neural Networks for Histopathological Cancer Classification. (arXiv:2011.04057v2 [cs.CV] UPDATED)</h2>
<h3>Shreyas Rajesh Labhsetwar, Alistair Michael Baretto, Raj Sunil Salvi, Piyush Arvind Kolte, Veerasai Subramaniam Venkatesh</h3>
<p>Convolutional Neural Networks can be designed with different levels of
complexity depending upon the task at hand. This paper analyzes the effect of
dimensional changes to the CNN architecture on its performance on the task of
Histopathological Cancer Classification. The research starts with a baseline
10-layer CNN model with (3 X 3) convolution filters. Thereafter, the baseline
architecture is scaled in multiple dimensions including width, depth,
resolution and a combination of all of these. Width scaling involves
inculcating greater number of neurons per CNN layer, whereas depth scaling
involves deepening the hierarchical layered structure. Resolution scaling is
performed by increasing the dimensions of the input image, and compound scaling
involves a hybrid combination of width, depth and resolution scaling. The
results indicate that histopathological cancer scans are very complex in nature
and hence require high resolution images fed to a large hierarchy of
Convolution, MaxPooling, Dropout and Batch Normalization layers to extract all
the intricacies and perform perfect classification. Since compound scaling the
baseline model ensures that all the three dimensions: width, depth and
resolution are scaled, the best performance is obtained with compound scaling.
This research shows that better performance of CNN models is achieved by
compound scaling of the baseline model for the task of Histopathological Cancer
Classification.
</p>
<a href="http://arxiv.org/abs/2011.04057" target="_blank">arXiv:2011.04057</a> [<a href="http://arxiv.org/pdf/2011.04057" target="_blank">pdf</a>]

<h2>A Neuro-Inspired Autoencoding Defense Against Adversarial Perturbations. (arXiv:2011.10867v2 [cs.LG] UPDATED)</h2>
<h3>Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow</h3>
<p>Deep Neural Networks (DNNs) are vulnerable to adversarial attacks: carefully
constructed perturbations to an image can seriously impair classification
accuracy, while being imperceptible to humans. While there has been a
significant amount of research on defending against such attacks, most defenses
based on systematic design principles have been defeated by appropriately
modified attacks. For a fixed set of data, the most effective current defense
is to train the network using adversarially perturbed examples. In this paper,
we investigate a radically different, neuro-inspired defense mechanism,
starting from the observation that human vision is virtually unaffected by
adversarial examples designed for machines. We aim to reject L^inf bounded
adversarial perturbations before they reach a classifier DNN, using an encoder
with characteristics commonly observed in biological vision: sparse
overcomplete representations, randomness due to synaptic noise, and drastic
nonlinearities. Encoder training is unsupervised, using standard dictionary
learning. A CNN-based decoder restores the size of the encoder output to that
of the original image, enabling the use of a standard CNN for classification.
Our nominal design is to train the decoder and classifier together in standard
supervised fashion, but we also consider unsupervised decoder training based on
a regression objective (as in a conventional autoencoder) with separate
supervised training of the classifier. Unlike adversarial training, all
training is based on clean images.

Our experiments on the CIFAR-10 show performance competitive with
state-of-the-art defenses based on adversarial training, and point to the
promise of neuro-inspired techniques for the design of robust neural networks.
In addition, we provide results for a subset of the Imagenet dataset to verify
that our approach scales to larger images.
</p>
<a href="http://arxiv.org/abs/2011.10867" target="_blank">arXiv:2011.10867</a> [<a href="http://arxiv.org/pdf/2011.10867" target="_blank">pdf</a>]

<h2>Towards Playing Full MOBA Games with Deep Reinforcement Learning. (arXiv:2011.12692v3 [cs.AI] UPDATED)</h2>
<h3>Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu</h3>
<p>MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand
challenges to AI systems such as multi-agent, enormous state-action space,
complex action control, etc. Developing AI for playing MOBA games has raised
much attention accordingly. However, existing work falls short in handling the
raw game complexity caused by the explosion of agent combinations, i.e.,
lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the
play to a pool of only 17 heroes. As a result, full MOBA games without
restrictions are far from being mastered by any existing AI system. In this
paper, we propose a MOBA AI learning paradigm that methodologically enables
playing full MOBA games with deep reinforcement learning. Specifically, we
develop a combination of novel and existing learning techniques, including
curriculum self-play learning, policy distillation, off-policy adaption,
multi-head value estimation, and Monte-Carlo tree-search, in training and
playing a large pool of heroes, meanwhile addressing the scalability issue
skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build
superhuman AI agents that can defeat top esports players. The superiority of
our AI is demonstrated by the first large-scale performance test of MOBA AI
agent in the literature.
</p>
<a href="http://arxiv.org/abs/2011.12692" target="_blank">arXiv:2011.12692</a> [<a href="http://arxiv.org/pdf/2011.12692" target="_blank">pdf</a>]

<h2>Image-based plant disease diagonasis with unsupervised anomaly detection based on reconstructability of colors. (arXiv:2011.14306v3 [cs.CV] UPDATED)</h2>
<h3>Ryoya Katafuchi, Terumasa Tokunaga</h3>
<p>This paper proposes an unsupervised anomaly detection technique for
image-based plant disease diagnosis. A construction of large and openly
available data set on labeled images of healthy and diseased crop plants has
led to growing interest in computer vision techniques for automatic plant
disease diagnosis. Although supervised image classifiers based on deep learning
could be a powerful tool to identify plant diseases, they require huge amount
of data sets that have been labeled as healthy and diseased. While, data mining
techniques called "anomaly detection" include unsupervised approaches that not
require rare samples for training classifiers. The proposed method in this
study focuses on the reconstructability of colors on plant images. We expect
that a deep encoder decoder network trained for reconstructing colors of
healthy plant images certainly fails to reconstruct colors of symptomatic
regions. The main contributions of this work are as follows: (i) we propose a
new image-based plant disease detection framework utilizing a conditional
adversarial network called pix2pix, (ii) we introduce a new anomaly score based
on CIEDE2000 color difference. Through experiments using PlantVillage dataset,
we demonstrate that our method is superior to an existing anomaly detector
called AnoGAN for identifying diseased crop images in terms of accuracy,
interpretability and computational efficiency.
</p>
<a href="http://arxiv.org/abs/2011.14306" target="_blank">arXiv:2011.14306</a> [<a href="http://arxiv.org/pdf/2011.14306" target="_blank">pdf</a>]

<h2>A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints. (arXiv:2011.14858v2 [cs.CV] UPDATED)</h2>
<h3>Puranjay Mohan, Aditya Jyoti Paul, Abhay Chirania</h3>
<p>The world is going through one of the most dangerous pandemics of all time
with the rapid spread of the novel coronavirus (COVID-19). According to the
World Health Organisation, the most effective way to thwart the transmission of
coronavirus is to wear medical face masks. Monitoring the use of face masks in
public places has been a challenge because manual monitoring could be unsafe.
This paper proposes an architecture for detecting medical face masks for
deployment on resource-constrained endpoints having extremely low memory
footprints. A small development board with an ARM Cortex-M7 microcontroller
clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for
the deployment of the model. Using the TensorFlow Lite framework, the model is
quantized to further reduce its size. The proposed model is 138 KB post
quantization and runs at the inference speed of 30 FPS.
</p>
<a href="http://arxiv.org/abs/2011.14858" target="_blank">arXiv:2011.14858</a> [<a href="http://arxiv.org/pdf/2011.14858" target="_blank">pdf</a>]

<h2>Cross-modal Retrieval and Synthesis (X-MRS): Closing the modality gap in shared subspace. (arXiv:2012.01345v2 [cs.CV] UPDATED)</h2>
<h3>Ricardo Guerrero, Hai Xuan Pham, Vladimir Pavlovic</h3>
<p>Computational food analysis (CFA), a broad set of methods that attempt to
automate food understanding, naturally requires analysis of multi-modal
evidence of a particular food or dish, e.g. images, recipe text, preparation
video, nutrition labels, etc. A key to making CFA possible is multi-modal
shared subspace learning, which in turn can be used for cross-modal retrieval
and/or synthesis, particularly, between food images and their corresponding
textual recipes. In this work we propose a simple yet novel architecture for
shared subspace learning, which is used to tackle the food image-to-recipe
retrieval problem. Our proposed method employs an effective transformer based
multilingual recipe encoder coupled with a traditional image embedding
architecture. Experimental analysis on the public Recipe1M dataset shows that
the subspace learned via the proposed method outperforms the current
state-of-the-arts (SoTA) in food retrieval by a large margin, obtaining
recall@1 of 0.64. Furthermore, in order to demonstrate the representational
power of the learned subspace, we propose a generative food image synthesis
model conditioned on the embeddings of recipes. Synthesized images can
effectively reproduce the visual appearance of paired samples, achieving R@1 of
0.68 in the image-to-recipe retrieval experiment, thus effectively capturing
the semantics of the textual recipe.
</p>
<a href="http://arxiv.org/abs/2012.01345" target="_blank">arXiv:2012.01345</a> [<a href="http://arxiv.org/pdf/2012.01345" target="_blank">pdf</a>]

<h2>Origin-Aware Next Destination Recommendation with Personalized Preference Attention. (arXiv:2012.01915v2 [cs.AI] UPDATED)</h2>
<h3>Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh, Renrong Weng, Rui Tan</h3>
<p>Next destination recommendation is an important task in the transportation
domain of taxi and ride-hailing services, where users are recommended with
personalized destinations given their current origin location. However, recent
recommendation works do not satisfy this origin-awareness property, and only
consider learning from historical destination locations, without origin
information. Thus, the resulting approaches are unable to learn and predict
origin-aware recommendations based on the user's current location, leading to
sub-optimal performance and poor real-world practicality. Hence, in this work,
we study the origin-aware next destination recommendation task. We propose the
Spatial-Temporal Origin-Destination Personalized Preference Attention
(STOD-PPA) encoder-decoder model to learn origin-origin (OO),
destination-destination (DD), and origin-destination (OD) relationships by
first encoding both origin and destination sequences with spatial and temporal
factors in local and global views, then decoding them through personalized
preference attention to predict the next destination. Experimental results on
seven real-world user trajectory taxi datasets show that our model
significantly outperforms baseline and state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.01915" target="_blank">arXiv:2012.01915</a> [<a href="http://arxiv.org/pdf/2012.01915" target="_blank">pdf</a>]

<h2>Disentangled Information Bottleneck. (arXiv:2012.07372v3 [cs.LG] UPDATED)</h2>
<h3>Ziqi Pan, Li Niu, Jianfu Zhang, Liqing Zhang</h3>
<p>The information bottleneck (IB) method is a technique for extracting
information that is relevant for predicting the target random variable from the
source random variable, which is typically implemented by optimizing the IB
Lagrangian that balances the compression and prediction terms. However, the IB
Lagrangian is hard to optimize, and multiple trials for tuning values of
Lagrangian multiplier are required. Moreover, we show that the prediction
performance strictly decreases as the compression gets stronger during
optimizing the IB Lagrangian. In this paper, we implement the IB method from
the perspective of supervised disentangling. Specifically, we introduce
Disentangled Information Bottleneck (DisenIB) that is consistent on compressing
source maximally without target prediction performance loss (maximum
compression). Theoretical and experimental results demonstrate that our method
is consistent on maximum compression, and performs well in terms of
generalization, robustness to adversarial attack, out-of-distribution
detection, and supervised disentangling.
</p>
<a href="http://arxiv.org/abs/2012.07372" target="_blank">arXiv:2012.07372</a> [<a href="http://arxiv.org/pdf/2012.07372" target="_blank">pdf</a>]

<h2>A case for new neural network smoothness constraints. (arXiv:2012.07969v2 [stat.ML] UPDATED)</h2>
<h3>Mihaela Rosca, Theophane Weber, Arthur Gretton, Shakir Mohamed</h3>
<p>How sensitive should machine learning models be to input changes? We tackle
the question of model smoothness and show that it is a useful inductive bias
which aids generalization, adversarial robustness, generative modeling and
reinforcement learning. We explore current methods of imposing smoothness
constraints and observe they lack the flexibility to adapt to new tasks, they
don't account for data modalities, they interact with losses, architectures and
optimization in ways not yet fully understood. We conclude that new advances in
the field are hinging on finding ways to incorporate data, tasks and learning
into our definitions of smoothness.
</p>
<a href="http://arxiv.org/abs/2012.07969" target="_blank">arXiv:2012.07969</a> [<a href="http://arxiv.org/pdf/2012.07969" target="_blank">pdf</a>]

<h2>FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction. (arXiv:2012.07999v2 [cs.CV] UPDATED)</h2>
<h3>ShahRukh Athar, Albert Pumarola, Francesc Moreno-Noguer, Dimitris Samaras</h3>
<p>Facial Expressions induce a variety of high-level details on the 3D face
geometry. For example, a smile causes the wrinkling of cheeks or the formation
of dimples, while being angry often causes wrinkling of the forehead. Morphable
Models (3DMMs) of the human face fail to capture such fine details in their
PCA-based representations and consequently cannot generate such details when
used to edit expressions. In this work, we introduce FaceDet3D, a
first-of-its-kind method that generates - from a single image - geometric
facial details that are consistent with any desired target expression. The
facial details are represented as a vertex displacement map and used then by a
Neural Renderer to photo-realistically render novel images of any single image
in any desired expression and view. The project website is:
this http URL
</p>
<a href="http://arxiv.org/abs/2012.07999" target="_blank">arXiv:2012.07999</a> [<a href="http://arxiv.org/pdf/2012.07999" target="_blank">pdf</a>]

<h2>TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask). (arXiv:2012.09302v2 [cs.LG] UPDATED)</h2>
<h3>Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Ting Wang</h3>
<p>Neural backdoors represent one primary threat to the security of deep
learning systems. The intensive research on this subject has produced a
plethora of attacks/defenses, resulting in a constant arms race. However, due
to the lack of evaluation benchmarks, many critical questions remain largely
unexplored: (i) How effective, evasive, or transferable are different attacks?
(ii) How robust, utility-preserving, or generic are different defenses? (iii)
How do various factors (e.g., model architectures) impact their performance?
(iv) What are the best practices (e.g., optimization strategies) to operate
such attacks/defenses? (v) How can the existing attacks/defenses be further
improved?

To bridge the gap, we design and implement TROJANZOO, the first open-source
platform for evaluating neural backdoor attacks/defenses in a unified,
holistic, and practical manner. Thus, it has incorporated 12 representative
attacks, 15 state-of-the-art defenses, 6 attack performance metrics, 10 defense
utility metrics, as well as rich tools for in-depth analysis of attack-defense
interactions. Leveraging TROJANZOO, we conduct a systematic study of existing
attacks/defenses, leading to a number of interesting findings: (i) different
attacks manifest various trade-offs among multiple desiderata (e.g.,
effectiveness, evasiveness, and transferability); (ii) one-pixel triggers often
suffice; (iii) optimizing trigger patterns and trojan models jointly improves
both attack effectiveness and evasiveness; (iv) sanitizing trojan models often
introduces new vulnerabilities; (v) most defenses are ineffective against
adaptive attacks, but integrating complementary ones significantly enhances
defense robustness. We envision that such findings will help users select the
right defense solutions and facilitate future research on neural backdoors.
</p>
<a href="http://arxiv.org/abs/2012.09302" target="_blank">arXiv:2012.09302</a> [<a href="http://arxiv.org/pdf/2012.09302" target="_blank">pdf</a>]

<h2>Fair for All: Best-effort Fairness Guarantees for Classification. (arXiv:2012.10216v2 [cs.LG] UPDATED)</h2>
<h3>Anilesh K. Krishnaswamy, Zhihao Jiang, Kangning Wang, Yu Cheng, Kamesh Munagala</h3>
<p>Standard approaches to group-based notions of fairness, such as \emph{parity}
and \emph{equalized odds}, try to equalize absolute measures of performance
across known groups (based on race, gender, etc.). Consequently, a group that
is inherently harder to classify may hold back the performance on other groups;
and no guarantees can be provided for unforeseen groups. Instead, we propose a
fairness notion whose guarantee, on each group $g$ in a class $\mathcal{G}$, is
relative to the performance of the best classifier on $g$. We apply this notion
to broad classes of groups, in particular, where (a) $\mathcal{G}$ consists of
all possible groups (subsets) in the data, and (b) $\mathcal{G}$ is more
streamlined.

For the first setting, which is akin to groups being completely unknown, we
devise the {\sc PF} (Proportional Fairness) classifier, which guarantees, on
any possible group $g$, an accuracy that is proportional to that of the optimal
classifier for $g$, scaled by the relative size of $g$ in the data set. Due to
including all possible groups, some of which could be too complex to be
relevant, the worst-case theoretical guarantees here have to be proportionally
weaker for smaller subsets.

For the second setting, we devise the {\sc BeFair} (Best-effort Fair)
framework which seeks an accuracy, on every $g \in \mathcal{G}$, which
approximates that of the optimal classifier on $g$, independent of the size of
$g$. Aiming for such a guarantee results in a non-convex problem, and we design
novel techniques to get around this difficulty when $\mathcal{G}$ is the set of
linear hypotheses. We test our algorithms on real-world data sets, and present
interesting comparative insights on their performance.
</p>
<a href="http://arxiv.org/abs/2012.10216" target="_blank">arXiv:2012.10216</a> [<a href="http://arxiv.org/pdf/2012.10216" target="_blank">pdf</a>]

<h2>PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection. (arXiv:2012.10412v3 [cs.CV] UPDATED)</h2>
<h3>Yanan Zhang, Di Huang, Yunhong Wang</h3>
<p>LiDAR-based 3D object detection is an important task for autonomous driving
and current approaches suffer from sparse and partial point clouds of distant
and occluded objects. In this paper, we propose a novel two-stage approach,
namely PC-RGNN, dealing with such challenges by two specific solutions. On the
one hand, we introduce a point cloud completion module to recover high-quality
proposals of dense points and entire views with original structures preserved.
On the other hand, a graph neural network module is designed, which
comprehensively captures relations among points through a local-global
attention mechanism as well as multi-scale graph based context aggregation,
substantially strengthening encoded features. Extensive experiments on the
KITTI benchmark show that the proposed approach outperforms the previous
state-of-the-art baselines by remarkable margins, highlighting its
effectiveness.
</p>
<a href="http://arxiv.org/abs/2012.10412" target="_blank">arXiv:2012.10412</a> [<a href="http://arxiv.org/pdf/2012.10412" target="_blank">pdf</a>]

<h2>GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning. (arXiv:2012.10630v2 [cs.LG] UPDATED)</h2>
<h3>Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Rishabh Iyer</h3>
<p>Large scale machine learning and deep models are extremely data-hungry.
Unfortunately, obtaining large amounts of labeled data is expensive, and
training state-of-the-art models (with hyperparameter tuning) requires
significant computing resources and time. Secondly, real-world data is noisy
and imbalanced. As a result, several recent papers try to make the training
process more efficient and robust. However, most existing work either focuses
on robustness or efficiency, but not both. In this work, we introduce Glister,
a GeneraLIzation based data Subset selecTion for Efficient and Robust learning
framework. We formulate Glister as a mixed discrete-continuous bi-level
optimization problem to select a subset of the training data, which maximizes
the log-likelihood on a held-out validation set. Next, we propose an iterative
online algorithm Glister-Online, which performs data selection iteratively
along with the parameter updates and can be applied to any loss-based learning
algorithm. We then show that for a rich class of loss functions including
cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete
data selection is an instance of (weakly) submodular optimization, and we
analyze conditions for which Glister-Online reduces the validation loss and
converges. Finally, we propose Glister-Active, an extension to batch active
learning, and we empirically demonstrate the performance of Glister on a wide
range of tasks including, (a) data selection to reduce training time, (b)
robust learning under label noise and imbalance settings, and (c) batch-active
learning with several deep and shallow models. We show that our framework
improves upon state of the art both in efficiency and accuracy (in cases (a)
and (c)) and is more efficient compared to other state-of-the-art robust
learning algorithms in case (b).
</p>
<a href="http://arxiv.org/abs/2012.10630" target="_blank">arXiv:2012.10630</a> [<a href="http://arxiv.org/pdf/2012.10630" target="_blank">pdf</a>]

<h2>The COVID-19 pandemic: socioeconomic and health disparities. (arXiv:2012.11399v2 [cs.LG] UPDATED)</h2>
<h3>Behzad Javaheri</h3>
<p>Disadvantaged groups around the world have suffered and endured higher
mortality during the current COVID-19 pandemic. This contrast disparity
suggests that socioeconomic and health-related factors may drive inequality in
disease outcome. To identify these factors correlated with COVID-19 outcome,
country aggregate data provided by the Lancet COVID-19 Commission subjected to
correlation analysis. Socioeconomic and health-related variables were used to
predict mortality in the top 5 most affected countries using ridge regression
and extreme gradient boosting (XGBoost) models. Our data reveal that predictors
related to demographics and social disadvantage correlate with COVID-19
mortality per million and that XGBoost performed better than ridge regression.
Taken together, our findings suggest that the health consequence of the current
pandemic is not just confined to indiscriminate impact of a viral infection but
that these preventable effects are amplified based on pre-existing health and
socioeconomic inequalities.
</p>
<a href="http://arxiv.org/abs/2012.11399" target="_blank">arXiv:2012.11399</a> [<a href="http://arxiv.org/pdf/2012.11399" target="_blank">pdf</a>]

<h2>Accelerated Parallel Non-conjugate Sampling for Bayesian Non-parametric Models. (arXiv:1705.07178v5 [stat.ML] UPDATED)</h2>
<h3>Michael Minyi Zhang, Sinead A. Williamson, Fernando Perez-Cruz</h3>
<p>Inference of latent feature models in the Bayesian nonparametric setting is
generally difficult, especially in high dimensional settings, because it
usually requires proposing features from some prior distribution. In special
cases, where the integration is tractable, we can sample new feature
assignments according to a predictive likelihood. However, this still may not
be efficient in high dimensions. We present a novel method to accelerate the
mixing of latent variable model inference by proposing feature locations based
on the data, as opposed to the prior. First, we introduce an accelerated
feature proposal mechanism that we show is a valid Bayesian inference
algorithm. Next, we propose an approximate inference strategy to perform
accelerated inference in parallel. A two-stage algorithm that combines the two
approaches provides a computationally attractive method that exhibits good
mixing behavior and converges to the posterior distribution of our model, while
allowing us to exploit parallelization.
</p>
<a href="http://arxiv.org/abs/1705.07178" target="_blank">arXiv:1705.07178</a> [<a href="http://arxiv.org/pdf/1705.07178" target="_blank">pdf</a>]

