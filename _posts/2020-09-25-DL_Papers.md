---
title: Latest Deep Learning Papers
date: 2020-12-08 21:00:02 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (205 Articles)</h1>
<h2>Shape From Tracing: Towards Reconstructing 3D Object Geometry and SVBRDF Material from Images via Differentiable Path Tracing. (arXiv:2012.03939v1 [cs.CV])</h2>
<h3>Purvi Goel, Loudon Cohen, James Guesman, Vikas Thamizharasan, James Tompkin, Daniel Ritchie</h3>
<p>Reconstructing object geometry and material from multiple views typically
requires optimization. Differentiable path tracing is an appealing framework as
it can reproduce complex appearance effects. However, it is difficult to use
due to high computational cost. In this paper, we explore how to use
differentiable ray tracing to refine an initial coarse mesh and per-mesh-facet
material representation. In simulation, we find that it is possible to
reconstruct fine geometric and material detail from low resolution input views,
allowing high-quality reconstructions in a few hours despite the expense of
path tracing. The reconstructions successfully disambiguate shading, shadow,
and global illumination effects such as diffuse interreflection from material
properties. We demonstrate the impact of different geometry initializations,
including space carving, multi-view stereo, and 3D neural networks. Finally,
with input captured using smartphone video and a consumer 360? camera for
lighting estimation, we also show how to refine initial reconstructions of
real-world objects in unconstrained environments.
</p>
<a href="http://arxiv.org/abs/2012.03939" target="_blank">arXiv:2012.03939</a> [<a href="http://arxiv.org/pdf/2012.03939" target="_blank">pdf</a>]

<h2>GenScan: A Generative Method for Populating Parametric 3D Scan Datasets. (arXiv:2012.03998v1 [cs.CV])</h2>
<h3>Mohammad Keshavarzi, Oladapo Afolabi, Luisa Caldas, Allen Y. Yang, Avideh Zakhor</h3>
<p>The availability of rich 3D datasets corresponding to the geometrical
complexity of the built environments is considered an ongoing challenge for 3D
deep learning methodologies. To address this challenge, we introduce GenScan, a
generative system that populates synthetic 3D scan datasets in a parametric
fashion. The system takes an existing captured 3D scan as an input and outputs
alternative variations of the building layout including walls, doors, and
furniture with corresponding textures. GenScan is a fully automated system that
can also be manually controlled by a user through an assigned user interface.
Our proposed system utilizes a combination of a hybrid deep neural network and
a parametrizer module to extract and transform elements of a given 3D scan.
GenScan takes advantage of style transfer techniques to generate new textures
for the generated scenes. We believe our system would facilitate data
augmentation to expand the currently limited 3D geometry datasets commonly used
in 3D computer vision, generative design, and general 3D deep learning tasks.
</p>
<a href="http://arxiv.org/abs/2012.03998" target="_blank">arXiv:2012.03998</a> [<a href="http://arxiv.org/pdf/2012.03998" target="_blank">pdf</a>]

<h2>Battery Model Calibration with Deep Reinforcement Learning. (arXiv:2012.04010v1 [cs.LG])</h2>
<h3>Ajaykumar Unagar, Yuan Tian, Manuel Arias-Chao, Olga Fink</h3>
<p>Lithium-Ion (Li-I) batteries have recently become pervasive and are used in
many physical assets. To enable a good prediction of the end of discharge of
batteries, detailed electrochemical Li-I battery models have been developed.
Their parameters are typically calibrated before they are taken into operation
and are typically not re-calibrated during operation. However, since battery
performance is affected by aging, the reality gap between the computational
battery models and the real physical systems leads to inaccurate predictions. A
supervised machine learning algorithm would require an extensive representative
training dataset mapping the observation to the ground truth calibration
parameters. This may be infeasible for many practical applications. In this
paper, we implement a Reinforcement Learning-based framework for reliably and
efficiently inferring calibration parameters of battery models. The framework
enables real-time inference of the computational model parameters in order to
compensate the reality-gap from the observations. Most importantly, the
proposed methodology does not need any labeled data samples, (samples of
observations and the ground truth calibration parameters). Furthermore, the
framework does not require any information on the underlying physical model.
The experimental results demonstrate that the proposed methodology is capable
of inferring the model parameters with high accuracy and high robustness. While
the achieved results are comparable to those obtained with supervised machine
learning, they do not rely on the ground truth information during training.
</p>
<a href="http://arxiv.org/abs/2012.04010" target="_blank">arXiv:2012.04010</a> [<a href="http://arxiv.org/pdf/2012.04010" target="_blank">pdf</a>]

<h2>Learning an Animatable Detailed 3D Face Model from In-The-Wild Images. (arXiv:2012.04012v1 [cs.CV])</h2>
<h3>Yao Feng, Haiwen Feng, Michael J. Black, Timo Bolkart</h3>
<p>While current monocular 3D face reconstruction methods can recover fine
geometric details, they suffer several limitations. Some methods produce faces
that cannot be realistically animated because they do not model how wrinkles
vary with expression. Other methods are trained on high-quality face scans and
do not generalize well to in-the-wild images. We present the first approach to
jointly learn a model with animatable detail and a detailed 3D face regressor
from in-the-wild images that recovers shape details as well as their
relationship to facial expressions. Our DECA (Detailed Expression Capture and
Animation) model is trained to robustly produce a UV displacement map from a
low-dimensional latent representation that consists of person-specific detail
parameters and generic expression parameters, while a regressor is trained to
predict detail, shape, albedo, expression, pose and illumination parameters
from a single image. We introduce a novel detail-consistency loss to
disentangle person-specific details and expression-dependent wrinkles. This
disentanglement allows us to synthesize realistic person-specific wrinkles by
controlling expression parameters while keeping person-specific details
unchanged. DECA achieves state-of-the-art shape reconstruction accuracy on two
benchmarks. Qualitative results on in-the-wild data demonstrate DECA's
robustness and its ability to disentangle identity and expression dependent
details enabling animation of reconstructed faces. The model and code are
publicly available at https://github.com/YadiraF/DECA.
</p>
<a href="http://arxiv.org/abs/2012.04012" target="_blank">arXiv:2012.04012</a> [<a href="http://arxiv.org/pdf/2012.04012" target="_blank">pdf</a>]

<h2>Generating unseen complex scenes: are we there yet?. (arXiv:2012.04027v1 [cs.CV])</h2>
<h3>Arantxa Casanova, Michal Drozdzal, Adriana Romero-Soriano</h3>
<p>Although recent complex scene conditional generation models generate
increasingly appealing scenes, it is very hard to assess which models perform
better and why. This is often due to models being trained to fit different data
splits, and defining their own experimental setups. In this paper, we propose a
methodology to compare complex scene conditional generation models, and provide
an in-depth analysis that assesses the ability of each model to (1) fit the
training distribution and hence perform well on seen conditionings, (2) to
generalize to unseen conditionings composed of seen object combinations, and
(3) generalize to unseen conditionings composed of unseen object combinations.
As a result, we observe that recent methods are able to generate recognizable
scenes given seen conditionings, and exploit compositionality to generalize to
unseen conditionings with seen object combinations. However, all methods suffer
from noticeable image quality degradation when asked to generate images from
conditionings composed of unseen object combinations. Moreover, through our
analysis, we identify the advantages of different pipeline components, and find
that (1) encouraging compositionality through instance-wise spatial
conditioning normalizations increases robustness to both types of unseen
conditionings, (2) using semantically aware losses such as the scene-graph
perceptual similarity helps improve some dimensions of the generation process,
and (3) enhancing the quality of generated masks and the quality of the
individual objects are crucial steps to improve robustness to both types of
unseen conditionings.
</p>
<a href="http://arxiv.org/abs/2012.04027" target="_blank">arXiv:2012.04027</a> [<a href="http://arxiv.org/pdf/2012.04027" target="_blank">pdf</a>]

<h2>On-Road Motion Planning for Automated Vehicles at Ulm University. (arXiv:2012.04028v1 [cs.RO])</h2>
<h3>Maximilian Graf, Oliver Speidel, Jona Ruof, Klaus Dietmayer</h3>
<p>The Institute of Measurement, Control and Microtechnology of the University
of Ulm investigates advanced driver assistance systems for decades and
concentrates large parts on autonomous driving. It is well known: Motion
planning is a key technology for autonomous driving. It is first and foremost
responsible for the safety of the vehicle passengers as well as of all
surrounding traffic participants. However, a further task consists also in
providing a smooth and comfortable driving behavior. In Ulm, we have the
grateful opportunity to test our algorithms under real conditions in public
traffic and diversified scenarios. In this paper, we would like to give the
readers an insight of our work, about the vehicle, the test track, as well as
of the related problems, challenges and solutions. Therefore, we will describe
the motion planning system and explain the implemented functionalities.
Furthermore, we will show how our vehicle moves through public road traffic and
how it deals with challenging scenarios like e.g. driving through roundabouts
and intersections.
</p>
<a href="http://arxiv.org/abs/2012.04028" target="_blank">arXiv:2012.04028</a> [<a href="http://arxiv.org/pdf/2012.04028" target="_blank">pdf</a>]

<h2>Statistical Mechanics of Deep Linear Neural Networks: The Back-Propagating Renormalization Group. (arXiv:2012.04030v1 [cs.LG])</h2>
<h3>Qianyi Li, Haim Sompolinsky</h3>
<p>The success of deep learning in many real-world tasks has triggered an effort
to theoretically understand the power and limitations of deep learning in
training and generalization of complex tasks, so far with limited progress. In
this work, we study the statistical mechanics of learning in Deep Linear Neural
Networks (DLNNs) in which the input-output function of an individual unit is
linear. Despite the linearity of the units, learning in DLNNs is highly
nonlinear, hence studying its properties reveals some of the essential features
of nonlinear Deep Neural Networks (DNNs). We solve exactly the network
properties following supervised learning using an equilibrium Gibbs
distribution in the weight space. To do this, we introduce the Back-Propagating
Renormalization Group (BPRG) which allows for the incremental integration of
the network weights layer by layer from the network output layer and
progressing backward. This procedure allows us to evaluate important network
properties such as its generalization error, the role of network width and
depth, the impact of the size of the training set, and the effects of weight
regularization and learning stochasticity. Furthermore, by performing partial
integration of layers, BPRG allows us to compute the emergent properties of the
neural representations across the different hidden layers. We have proposed a
heuristic extension of the BPRG to nonlinear DNNs with rectified linear units
(ReLU). Surprisingly, our numerical simulations reveal that despite the
nonlinearity, the predictions of our theory are largely shared by ReLU networks
with modest depth, in a wide regime of parameters. Our work is the first exact
statistical mechanical study of learning in a family of Deep Neural Networks,
and the first development of the Renormalization Group approach to the weight
space of these systems.
</p>
<a href="http://arxiv.org/abs/2012.04030" target="_blank">arXiv:2012.04030</a> [<a href="http://arxiv.org/pdf/2012.04030" target="_blank">pdf</a>]

<h2>ATOM3D: Tasks On Molecules in Three Dimensions. (arXiv:2012.04035v1 [cs.LG])</h2>
<h3>Raphael J.L. Townshend, Martin V&#xf6;gele, Patricia Suriana, Alexander Derry, Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Brandon Anderson, Stephan Eismann, Risi Kondor, Russ B. Altman, Ron O. Dror</h3>
<p>Computational methods that operate directly on three-dimensional molecular
structure hold large potential to solve important questions in biology and
chemistry. In particular deep neural networks have recently gained significant
attention. In this work we present ATOM3D, a collection of both novel and
existing datasets spanning several key classes of biomolecules, to
systematically assess such learning methods. We develop three-dimensional
molecular learning networks for each of these tasks, finding that they
consistently improve performance relative to one- and two-dimensional methods.
The specific choice of architecture proves to be critical for performance, with
three-dimensional convolutional networks excelling at tasks involving complex
geometries, while graph networks perform well on systems requiring detailed
positional information. Furthermore, equivariant networks show significant
promise. Our results indicate many molecular problems stand to gain from
three-dimensional molecular learning. All code and datasets can be accessed via
https://www.atom3d.ai .
</p>
<a href="http://arxiv.org/abs/2012.04035" target="_blank">arXiv:2012.04035</a> [<a href="http://arxiv.org/pdf/2012.04035" target="_blank">pdf</a>]

<h2>An autoencoder wavelet based deep neural network with attention mechanism for multistep prediction of plant growth. (arXiv:2012.04041v1 [cs.LG])</h2>
<h3>Bashar Alhnaity, Stefanos Kollias, Georgios Leontidis, Shouyong Jiang, Bert Schamp, Simon Pearson</h3>
<p>Multi-step prediction is considered of major significance for time series
analysis in many real life problems. Existing methods mainly focus on
one-step-ahead forecasting, since multiple step forecasting generally fails due
to accumulation of prediction errors. This paper presents a novel approach for
predicting plant growth in agriculture, focusing on prediction of plant Stem
Diameter Variations (SDV). The proposed approach consists of three main steps.
At first, wavelet decomposition is applied to the original data, as to
facilitate model fitting and reduce noise in them. Then an encoder-decoder
framework is developed using Long Short Term Memory (LSTM) and used for
appropriate feature extraction from the data. Finally, a recurrent neural
network including LSTM and an attention mechanism is proposed for modelling
long-term dependencies in the time series data. Experimental results are
presented which illustrate the good performance of the proposed approach and
that it significantly outperforms the existing models, in terms of error
criteria such as RMSE, MAE and MAPE.
</p>
<a href="http://arxiv.org/abs/2012.04041" target="_blank">arXiv:2012.04041</a> [<a href="http://arxiv.org/pdf/2012.04041" target="_blank">pdf</a>]

<h2>Rotation-Invariant Point Convolution With Multiple Equivariant Alignments. (arXiv:2012.04048v1 [cs.CV])</h2>
<h3>Hugues Thomas</h3>
<p>Recent attempts at introducing rotation invariance or equivariance in 3D deep
learning approaches have shown promising results, but these methods still
struggle to reach the performances of standard 3D neural networks. In this work
we study the relation between equivariance and invariance in 3D point
convolutions. We show that using rotation-equivariant alignments, it is
possible to make any convolutional layer rotation-invariant. Furthermore, we
improve this simple alignment procedure by using the alignment themselves as
features in the convolution, and by combining multiple alignments together.
With this core layer, we design rotation-invariant architectures which improve
state-of-the-art results in both object classification and semantic
segmentation and reduces the gap between rotation-invariant and standard 3D
deep learning approaches.
</p>
<a href="http://arxiv.org/abs/2012.04048" target="_blank">arXiv:2012.04048</a> [<a href="http://arxiv.org/pdf/2012.04048" target="_blank">pdf</a>]

<h2>Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition. (arXiv:2012.04053v1 [cs.LG])</h2>
<h3>Liyu Chen, Haipeng Luo, Chen-Yu Wei</h3>
<p>We study the stochastic shortest path problem with adversarial costs and
known transition, and show that the minimax regret is
$\widetilde{O}(\sqrt{DT^\star K})$ and $\widetilde{O}(\sqrt{DT^\star SA K})$
for the full-information setting and the bandit feedback setting respectively,
where $D$ is the diameter, $T^\star$ is the expected hitting time of the
optimal policy, $S$ is the number of states, $A$ is the number of actions, and
$K$ is the number of episodes. Our results significantly improve upon the
existing work of (Rosenberg and Mansour, 2020) which only considers the
full-information setting and achieves suboptimal regret. Our work is also the
first to consider bandit feedback with adversarial costs.

Our algorithms are built on top of the Online Mirror Descent framework with a
variety of new techniques that might be of independent interest, including an
improved multi-scale expert algorithm, a reduction from general stochastic
shortest path to a special loop-free case, a skewed occupancy measure space,
%the usage of log-barrier with an increasing learning rate schedule, and a
novel correction term added to the cost estimators. Interestingly, the last two
elements reduce the variance of the learner via positive bias and the variance
of the optimal policy via negative bias respectively, and having them
simultaneously is critical for obtaining the optimal high-probability bound in
the bandit feedback setting.
</p>
<a href="http://arxiv.org/abs/2012.04053" target="_blank">arXiv:2012.04053</a> [<a href="http://arxiv.org/pdf/2012.04053" target="_blank">pdf</a>]

<h2>Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search. (arXiv:2012.04060v1 [cs.CV])</h2>
<h3>Andrey Kurenkov, Roberto Mart&#xed;n-Mart&#xed;n, Jeff Ichnowski, Ken Goldberg, Silvio Savarese</h3>
<p>Searching for objects in indoor organized environments such as homes or
offices is part of our everyday activities. When looking for a target object,
we jointly reason about the rooms and containers the object is likely to be in;
the same type of container will have a different probability of having the
target depending on the room it is in. We also combine geometric and semantic
information to infer what container is best to search, or what other objects
are best to move, if the target object is hidden from view. We propose to use a
3D scene graph representation to capture the hierarchical, semantic, and
geometric aspects of this problem. To exploit this representation in a search
process, we introduce Hierarchical Mechanical Search (HMS), a method that
guides an agent's actions towards finding a target object specified with a
natural language description. HMS is based on a novel neural network
architecture that uses neural message passing of vectors with visual,
geometric, and linguistic information to allow HMS to reason across layers of
the graph while combining semantic and geometric cues. HMS is evaluated on a
novel dataset of 500 3D scene graphs with dense placements of semantically
related objects in storage locations, and is shown to be significantly better
than several baselines at finding objects and close to the oracle policy in
terms of the median number of actions required. Additional qualitative results
can be found at https://ai.stanford.edu/mech-search/hms.
</p>
<a href="http://arxiv.org/abs/2012.04060" target="_blank">arXiv:2012.04060</a> [<a href="http://arxiv.org/pdf/2012.04060" target="_blank">pdf</a>]

<h2>Improved Convergence Rates for Non-Convex Federated Learning with Compression. (arXiv:2012.04061v1 [stat.ML])</h2>
<h3>Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S. Dhillon</h3>
<p>Federated learning is a new distributed learning paradigm that enables
efficient training of emerging large-scale machine learning models. In this
paper, we consider federated learning on non-convex objectives with compressed
communication from the clients to the central server. We propose a novel
first-order algorithm (\texttt{FedSTEPH2}) that employs compressed
communication and achieves the optimal iteration complexity of
$\mathcal{O}(1/\epsilon^{1.5})$ to reach an $\epsilon$-stationary point (i.e.
$\mathbb{E}[\|\nabla f(\bm{x})\|^2] \leq \epsilon$) on smooth non-convex
objectives. The proposed scheme is the first algorithm that attains the
aforementioned optimal complexity {with compressed communication}. The key idea
of \texttt{FedSTEPH2} that enables attaining this optimal complexity is
applying judicious momentum terms both in the local client updates and the
global server update. As a prequel to \texttt{FedSTEPH2}, we propose
\texttt{FedSTEPH} which involves a momentum term only in the local client
updates. We establish that \texttt{FedSTEPH} enjoys improved convergence rates
under various non-convex settings (such as the Polyak-\L{}ojasiewicz condition)
and with fewer assumptions than prior work.
</p>
<a href="http://arxiv.org/abs/2012.04061" target="_blank">arXiv:2012.04061</a> [<a href="http://arxiv.org/pdf/2012.04061" target="_blank">pdf</a>]

<h2>Cost-effective Machine Learning Inference Offload for Edge Computing. (arXiv:2012.04063v1 [cs.LG])</h2>
<h3>Christian Makaya, Amalendu Iyer, Jonathan Salfity, Madhu Athreya, M Anthony Lewis</h3>
<p>Computing at the edge is increasingly important since a massive amount of
data is generated. This poses challenges in transporting all that data to the
remote data centers and cloud, where they can be processed and analyzed. On the
other hand, harnessing the edge data is essential for offering data-driven and
machine learning-based applications, if the challenges, such as device
capabilities, connectivity, and heterogeneity can be mitigated. Machine
learning applications are very compute-intensive and require processing of
large amount of data. However, edge devices are often resources-constrained, in
terms of compute resources, power, storage, and network connectivity. Hence,
limiting their potential to run efficiently and accurately state-of-the art
deep neural network (DNN) models, which are becoming larger and more complex.
This paper proposes a novel offloading mechanism by leveraging installed-base
on-premises (edge) computational resources. The proposed mechanism allows the
edge devices to offload heavy and compute-intensive workloads to edge nodes
instead of using remote cloud. Our offloading mechanism has been prototyped and
tested with state-of-the art person and object detection DNN models for mobile
robots and video surveillance applications. The performance shows a significant
gain compared to cloud-based offloading strategies in terms of accuracy and
latency.
</p>
<a href="http://arxiv.org/abs/2012.04063" target="_blank">arXiv:2012.04063</a> [<a href="http://arxiv.org/pdf/2012.04063" target="_blank">pdf</a>]

<h2>A New Window Loss Function for Bone Fracture Detection and Localization in X-ray Images with Point-based Annotation. (arXiv:2012.04066v1 [cs.CV])</h2>
<h3>Xinyu Zhang, Yirui Wang, Chi-Tung Cheng, Le Lu, Jing Xiao, Chien-Hung Liao, Shun Miao</h3>
<p>Object detection methods are widely adopted for computer-aided diagnosis
using medical images. Anomalous findings are usually treated as objects that
are described by bounding boxes. Yet, many pathological findings, e.g., bone
fractures, cannot be clearly defined by bounding boxes, owing to considerable
instance, shape and boundary ambiguities. This makes bounding box annotations,
and their associated losses, highly ill-suited. In this work, we propose a new
bone fracture detection method for X-ray images, based on a labor effective and
flexible annotation scheme suitable for abnormal findings with no clear
object-level spatial extents or boundaries. Our method employs a simple,
intuitive, and informative point-based annotation protocol to mark localized
pathology information. To address the uncertainty in the fracture scales
annotated via point(s), we convert the annotations into pixel-wise supervision
that uses lower and upper bounds with positive, negative, and uncertain
regions. A novel Window Loss is subsequently proposed to only penalize the
predictions outside of the uncertain regions. Our method has been extensively
evaluated on 4410 pelvic X-ray images of unique patients. Experiments
demonstrate that our method outperforms previous state-of-the-art image
classification and object detection baselines by healthy margins, with an AUROC
of 0.983 and FROC score of 89.6%.
</p>
<a href="http://arxiv.org/abs/2012.04066" target="_blank">arXiv:2012.04066</a> [<a href="http://arxiv.org/pdf/2012.04066" target="_blank">pdf</a>]

<h2>Efficient Attitude Estimators: A Tutorial and Survey. (arXiv:2012.04075v1 [cs.RO])</h2>
<h3>Hussein Al-Jlailaty, Mohammad M. Mansour</h3>
<p>Inertial sensors based on micro-electromechanical systems (MEMS) technology,
such as accelerometers and angular rate sensors, are cost-effective solutions
used in inertial navigation systems in a broad spectrum of applications that
estimate position, velocity and orientation of a system with respect to an
inertial reference frame. The task of an orientation filter is to compute an
optimal solution for the attitude state, consisting of roll, pitch and yaw,
through the fusion of angular rate, accelerometer, and magnetometer
measurements. The aim of this paper is threefold: first, it serves researchers
and practitioners in the signal processing community seeking the most
appropriate attitude estimators that fulfills their needs, shedding light on
the drawbacks and the advantages of a wide variety of designs. Second, it
serves as a survey and tutorial for existing estimator designs in the
literature, assessing their design aspects and components, and dissecting their
hidden details for the benefit of researchers. Third, a comprehensive list of
algorithms is discussed for a fully functional inertial navigation system,
starting from the navigation equations and ending with the filter equations,
keeping in mind their suitability for power limited embedded processors. The
source code of all algorithms is published, with the aim of it being an
out-of-box solution for researchers in the field. The reader will take away the
following concepts from this article: understand the key concepts of an
inertial navigation system; be able to implement and test a complete stand
alone solution; be able to evaluate and understand different algorithms;
understand the trade-offs between different filter architectures and
techniques; and understand efficient embedded processing techniques, trends and
opportunities.
</p>
<a href="http://arxiv.org/abs/2012.04075" target="_blank">arXiv:2012.04075</a> [<a href="http://arxiv.org/pdf/2012.04075" target="_blank">pdf</a>]

<h2>Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately. (arXiv:2012.04104v1 [cs.LG])</h2>
<h3>Fereshte Khani, Percy Liang</h3>
<p>The presence of spurious features interferes with the goal of obtaining
robust models that perform well across many groups within the population. A
natural remedy is to remove spurious features from the model. However, in this
work we show that removal of spurious features can decrease accuracy due to the
inductive biases of overparameterized models. We completely characterize how
the removal of spurious features affects accuracy across different groups (more
generally, test distributions) in noiseless overparameterized linear
regression. In addition, we show that removal of spurious feature can decrease
the accuracy even in balanced datasets -- each target co-occurs equally with
each spurious feature; and it can inadvertently make the model more susceptible
to other spurious features. Finally, we show that robust self-training can
remove spurious features without affecting the overall accuracy. Experiments on
the Toxic-Comment-Detectoin and CelebA datasets show that our results hold in
non-linear models.
</p>
<a href="http://arxiv.org/abs/2012.04104" target="_blank">arXiv:2012.04104</a> [<a href="http://arxiv.org/pdf/2012.04104" target="_blank">pdf</a>]

<h2>The Tribes of Machine Learning and the Realm of Computer Architecture. (arXiv:2012.04105v1 [cs.LG])</h2>
<h3>Ayaz Akram, Jason Lowe-Power</h3>
<p>Machine learning techniques have influenced the field of computer
architecture like many other fields. This paper studies how the fundamental
machine learning techniques can be applied towards computer architecture
problems. We also provide a detailed survey of computer architecture research
that employs different machine learning methods. Finally, we present some
future opportunities and the outstanding challenges that need to be overcome to
exploit full potential of machine learning for computer architecture.
</p>
<a href="http://arxiv.org/abs/2012.04105" target="_blank">arXiv:2012.04105</a> [<a href="http://arxiv.org/pdf/2012.04105" target="_blank">pdf</a>]

<h2>Deformable Gabor Feature Networks for Biomedical Image Classification. (arXiv:2012.04109v1 [cs.CV])</h2>
<h3>Xuan Gong, Xin Xia, Wentao Zhu, Baochang Zhang, David Doermann, Lian Zhuo</h3>
<p>In recent years, deep learning has dominated progress in the field of medical
image analysis. We find however, that the ability of current deep learning
approaches to represent the complex geometric structures of many medical images
is insufficient. One limitation is that deep learning models require a
tremendous amount of data, and it is very difficult to obtain a sufficient
amount with the necessary detail. A second limitation is that there are
underlying features of these medical images that are well established, but the
black-box nature of existing convolutional neural networks (CNNs) do not allow
us to exploit them. In this paper, we revisit Gabor filters and introduce a
deformable Gabor convolution (DGConv) to expand deep networks interpretability
and enable complex spatial variations. The features are learned at deformable
sampling locations with adaptive Gabor convolutions to improve
representativeness and robustness to complex objects. The DGConv replaces
standard convolutional layers and is easily trained end-to-end, resulting in
deformable Gabor feature network (DGFN) with few additional parameters and
minimal additional training cost. We introduce DGFN for addressing deep
multi-instance multi-label classification on the INbreast dataset for
mammograms and on the ChestX-ray14 dataset for pulmonary x-ray images.
</p>
<a href="http://arxiv.org/abs/2012.04109" target="_blank">arXiv:2012.04109</a> [<a href="http://arxiv.org/pdf/2012.04109" target="_blank">pdf</a>]

<h2>SuperFront: From Low-resolution to High-resolution Frontal Face Synthesis. (arXiv:2012.04111v1 [cs.CV])</h2>
<h3>Yu Yin, Joseph P. Robinson, Songyao Jiang, Yue Bai, Can Qin, Yun Fu</h3>
<p>Advances in face rotation, along with other face-based generative tasks, are
more frequent as we advance further in topics of deep learning. Even as
impressive milestones are achieved in synthesizing faces, the importance of
preserving identity is needed in practice and should not be overlooked. Also,
the difficulty should not be more for data with obscured faces, heavier poses,
and lower quality. Existing methods tend to focus on samples with variation in
pose, but with the assumption data is high in quality. We propose a generative
adversarial network (GAN) -based model to generate high-quality, identity
preserving frontal faces from one or multiple low-resolution (LR) faces with
extreme poses. Specifically, we propose SuperFront-GAN (SF-GAN) to synthesize a
high-resolution (HR), frontal face from one-to-many LR faces with various poses
and with the identity-preserved. We integrate a super-resolution (SR) side-view
module into SF-GAN to preserve identity information and fine details of the
side-views in HR space, which helps model reconstruct high-frequency
information of faces (i.e., periocular, nose, and mouth regions). Moreover,
SF-GAN accepts multiple LR faces as input, and improves each added sample. We
squeeze additional gain in performance with an orthogonal constraint in the
generator to penalize redundant latent representations and, hence, diversify
the learned features space. Quantitative and qualitative results demonstrate
the superiority of SF-GAN over others.
</p>
<a href="http://arxiv.org/abs/2012.04111" target="_blank">arXiv:2012.04111</a> [<a href="http://arxiv.org/pdf/2012.04111" target="_blank">pdf</a>]

<h2>Generalization bounds for deep learning. (arXiv:2012.04115v1 [stat.ML])</h2>
<h3>Guillermo Valle-P&#xe9;rez, Ard A. Louis</h3>
<p>Generalization in deep learning has been the topic of much recent theoretical
and empirical research. Here we introduce desiderata for techniques that
predict generalization errors for deep learning models in supervised learning.
Such predictions should 1) scale correctly with data complexity; 2) scale
correctly with training set size; 3) capture differences between architectures;
4) capture differences between optimization algorithms; 5) be quantitatively
not too far from the true error (in particular, be non-vacuous); 6) be
efficiently computable; and 7) be rigorous. We focus on generalization error
upper bounds and bounds, and introduce a categorisation of bounds depending on
assumptions on the algorithm and data. We review a wide range of existing
approaches, from classical VC dimension to recent PAC-Bayesian bounds,
commenting on how well they perform against the desiderata.

We next use a function-based picture to derive a marginal-likelihood
PAC-Bayesian bound. This bound is, by one definition, optimal up to a
multiplicative constant in the asymptotic limit of large training sets, as long
as the learning curve follows a power law, which is typically found in practice
for deep learning problems. Extensive empirical analysis demonstrates that our
marginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results
for the 6 and 7 are promising, but not yet fully conclusive, while only
desideratum 5 is currently beyond the scope of our bound. Finally, we comment
on why this function-based bound performs significantly better than current
parameter-based PAC-Bayes bounds.
</p>
<a href="http://arxiv.org/abs/2012.04115" target="_blank">arXiv:2012.04115</a> [<a href="http://arxiv.org/pdf/2012.04115" target="_blank">pdf</a>]

<h2>Parameter Efficient Multimodal Transformers for Video Representation Learning. (arXiv:2012.04124v1 [cs.CV])</h2>
<h3>Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, Yale Song</h3>
<p>The recent success of Transformers in the language domain has motivated
adapting it to a multimodal setting, where a new visual model is trained in
tandem with an already pretrained language model. However, due to the excessive
memory requirements from Transformers, existing work typically fixes the
language model and train only the vision module, which limits its ability to
learn cross-modal information in an end-to-end manner. In this work, we focus
on reducing the parameters of multimodal Transformers in the context of
audio-visual video representation learning. We alleviate the high memory
requirement by sharing the weights of Transformers across layers and
modalities; we decompose the Transformer into modality-specific and
modality-shared parts so that the model learns the dynamics of each modality
both individually and together, and propose a novel parameter sharing scheme
based on low-rank approximation. We show that our approach reduces parameters
up to 80$\%$, allowing us to train our model end-to-end from scratch. We also
propose a negative sampling approach based on an instance similarity measured
on the CNN embedding space that our model learns with the Transformers. To
demonstrate our approach, we pretrain our model on 30-second clips from
Kinetics-700 and transfer it to audio-visual classification tasks.
</p>
<a href="http://arxiv.org/abs/2012.04124" target="_blank">arXiv:2012.04124</a> [<a href="http://arxiv.org/pdf/2012.04124" target="_blank">pdf</a>]

<h2>Performance Analysis of Keypoint Detectors and Binary Descriptors Under Varying Degrees of Photometric and Geometric Transformations. (arXiv:2012.04135v1 [cs.CV])</h2>
<h3>Shuvo Kumar Paul, Pourya Hoseini, Mircea Nicolescu, Monica Nicolescu</h3>
<p>Detecting image correspondences by feature matching forms the basis of
numerous computer vision applications. Several detectors and descriptors have
been presented in the past, addressing the efficient generation of features
from interest points (keypoints) in an image. In this paper, we investigate
eight binary descriptors (AKAZE, BoostDesc, BRIEF, BRISK, FREAK, LATCH, LUCID,
and ORB) and eight interest point detector (AGAST, AKAZE, BRISK, FAST,
HarrisLapalce, KAZE, ORB, and StarDetector). We have decoupled the detection
and description phase to analyze the interest point detectors and then evaluate
the performance of the pairwise combination of different detectors and
descriptors. We conducted experiments on a standard dataset and analyzed the
comparative performance of each method under different image transformations.
We observed that: (1) the FAST, AGAST, ORB detectors were faster and detected
more keypoints, (2) the AKAZE and KAZE detectors performed better under
photometric changes while ORB was more robust against geometric changes, (3) in
general, descriptors performed better when paired with the KAZE and AKAZE
detectors, (4) the BRIEF, LUCID, ORB descriptors were relatively faster, and
(5) none of the descriptors did particularly well under geometric
transformations, only BRISK, FREAK, and AKAZE showed reasonable resiliency.
</p>
<a href="http://arxiv.org/abs/2012.04135" target="_blank">arXiv:2012.04135</a> [<a href="http://arxiv.org/pdf/2012.04135" target="_blank">pdf</a>]

<h2>Deep Energy-Based NARX Models. (arXiv:2012.04136v1 [cs.LG])</h2>
<h3>Johannes N. Hendriks, Fredrik K. Gustafsson, Ant&#xf4;nio H. Ribeiro, Adrian G. Wills, Thomas B. Sch&#xf6;n</h3>
<p>This paper is directed towards the problem of learning nonlinear ARX models
based on system input--output data. In particular, our interest is in learning
a conditional distribution of the current output based on a finite window of
past inputs and outputs. To achieve this, we consider the use of so-called
energy-based models, which have been developed in allied fields for learning
unknown distributions based on data. This energy-based model relies on a
general function to describe the distribution, and here we consider a deep
neural network for this purpose. The primary benefit of this approach is that
it is capable of learning both simple and highly complex noise models, which we
demonstrate on simulated and experimental data.
</p>
<a href="http://arxiv.org/abs/2012.04136" target="_blank">arXiv:2012.04136</a> [<a href="http://arxiv.org/pdf/2012.04136" target="_blank">pdf</a>]

<h2>Improved Swarm Engineering: Aligning Intuition and Analysis. (arXiv:2012.04144v1 [cs.RO])</h2>
<h3>John Harwell, Maria Gini</h3>
<p>When designing swarm-robotic systems, systematic comparison of swarm control
algorithms is necessary to determine which can scale up to handle the target
problem size and operating conditions. Qualitative predictions of performance
based on algorithm descriptions are often incorrect, and can lead to costly
design processes for swarm-robotic systems. We propose a set of quantitative
measures for swarm scalability, emergence, flexibility, and robustness which
enable swarm control algorithms analysis and comparison, swarm performance of a
given control algorithm, collectively enabling quicker and more confident
design decisions. We demonstrate the utility of our proposed measurements as
modeling and design tools for real-world scenarios by analyzing two common
problems, indoor warehouse object transport and search and rescue, and present
experimental results obtained in simulation.
</p>
<a href="http://arxiv.org/abs/2012.04144" target="_blank">arXiv:2012.04144</a> [<a href="http://arxiv.org/pdf/2012.04144" target="_blank">pdf</a>]

<h2>Dynamic Anchor Learning for Arbitrary-Oriented Object Detection. (arXiv:2012.04150v1 [cs.CV])</h2>
<h3>Qi Ming, Zhiqiang Zhou, Lingjuan Miao, Hongwei Zhang, Linhao Li</h3>
<p>Arbitrary-oriented objects widely appear in natural scenes, aerial
photographs, remote sensing images, etc., thus arbitrary-oriented object
detection has received considerable attention. Many current rotation detectors
use plenty of anchors with different orientations to achieve spatial alignment
with ground truth boxes, then Intersection-over-Union (IoU) is applied to
sample the positive and negative candidates for training. However, we observe
that the selected positive anchors cannot always ensure accurate detections
after regression, while some negative samples can achieve accurate
localization. It indicates that the quality assessment of anchors through IoU
is not appropriate, and this further lead to inconsistency between
classification confidence and localization accuracy. In this paper, we propose
a dynamic anchor learning (DAL) method, which utilizes the newly defined
matching degree to comprehensively evaluate the localization potential of the
anchors and carry out a more efficient label assignment process. In this way,
the detector can dynamically select high-quality anchors to achieve accurate
object detection, and the divergence between classification and regression will
be alleviated. With the newly introduced DAL, we achieve superior detection
performance for arbitrary-oriented objects with only a few horizontal preset
anchors. Experimental results on three remote sensing datasets HRSC2016, DOTA,
UCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method
achieves substantial improvement compared with the baseline model. Besides, our
approach is also universal for object detection using horizontal bound box. The
code and models are available at https://github.com/ming71/DAL.
</p>
<a href="http://arxiv.org/abs/2012.04150" target="_blank">arXiv:2012.04150</a> [<a href="http://arxiv.org/pdf/2012.04150" target="_blank">pdf</a>]

<h2>Learning Portrait Style Representations. (arXiv:2012.04153v1 [cs.CV])</h2>
<h3>Sadat Shaik, Bernadette Bucher, Nephele Agrafiotis, Stephen Phillips, Kostas Daniilidis, William Schmenner</h3>
<p>Style analysis of artwork in computer vision predominantly focuses on
achieving results in target image generation through optimizing understanding
of low level style characteristics such as brush strokes. However,
fundamentally different techniques are required to computationally understand
and control qualities of art which incorporate higher level style
characteristics. We study style representations learned by neural network
architectures incorporating these higher level characteristics. We find
variation in learned style features from incorporating triplets annotated by
art historians as supervision for style similarity. Networks leveraging
statistical priors or pretrained on photo collections such as ImageNet can also
derive useful visual representations of artwork. We align the impact of these
expert human knowledge, statistical, and photo realism priors on style
representations with art historical research and use these representations to
perform zero-shot classification of artists. To facilitate this work, we also
present the first large-scale dataset of portraits prepared for computational
analysis.
</p>
<a href="http://arxiv.org/abs/2012.04153" target="_blank">arXiv:2012.04153</a> [<a href="http://arxiv.org/pdf/2012.04153" target="_blank">pdf</a>]

<h2>Data-driven learning of nonlocal models: from high-fidelity simulations to constitutive laws. (arXiv:2012.04157v1 [cs.LG])</h2>
<h3>Huaiqian You, Yue Yu, Stewart Silling, Marta D&#x27;Elia</h3>
<p>We show that machine learning can improve the accuracy of simulations of
stress waves in one-dimensional composite materials. We propose a data-driven
technique to learn nonlocal constitutive laws for stress wave propagation
models. The method is an optimization-based technique in which the nonlocal
kernel function is approximated via Bernstein polynomials. The kernel,
including both its functional form and parameters, is derived so that when used
in a nonlocal solver, it generates solutions that closely match high-fidelity
data. The optimal kernel therefore acts as a homogenized nonlocal continuum
model that accurately reproduces wave motion in a smaller-scale, more detailed
model that can include multiple materials. We apply this technique to wave
propagation within a heterogeneous bar with a periodic microstructure. Several
one-dimensional numerical tests illustrate the accuracy of our algorithm. The
optimal kernel is demonstrated to reproduce high-fidelity data for a composite
material in applications that are substantially different from the problems
used as training data.
</p>
<a href="http://arxiv.org/abs/2012.04157" target="_blank">arXiv:2012.04157</a> [<a href="http://arxiv.org/pdf/2012.04157" target="_blank">pdf</a>]

<h2>Stability and Identification of Random Asynchronous Linear Time-Invariant Systems. (arXiv:2012.04160v1 [cs.LG])</h2>
<h3>Sahin Lale, Oguzhan Teke, Babak Hassibi, Anima Anandkumar</h3>
<p>In many computational tasks and dynamical systems, asynchrony and
randomization are naturally present and have been considered as ways to
increase the speed and reduce the cost of computation while compromising the
accuracy and convergence rate. In this work, we show the additional benefits of
randomization and asynchrony on the stability of linear dynamical systems. We
introduce a natural model for random asynchronous linear time-invariant (LTI)
systems which generalizes the standard (synchronous) LTI systems. In this
model, each state variable is updated randomly and asynchronously with some
probability according to the underlying system dynamics. We examine how the
mean-square stability of random asynchronous LTI systems vary with respect to
randomization and asynchrony. Surprisingly, we show that the stability of
random asynchronous LTI systems does not imply or is not implied by the
stability of the synchronous variant of the system and an unstable synchronous
system can be stabilized via randomization and/or asynchrony. We further study
a special case of the introduced model, namely randomized LTI systems, where
each state element is updated randomly with some fixed but unknown probability.
We consider the problem of system identification of unknown randomized LTI
systems using the precise characterization of mean-square stability via
extended Lyapunov equation. For unknown randomized LTI systems, we propose a
systematic identification method to recover the underlying dynamics. Given a
single input/output trajectory, our method estimates the model parameters that
govern the system dynamics, the update probability of state variables, and the
noise covariance using the correlation matrices of collected data and the
extended Lyapunov equation. Finally, we empirically demonstrate that the
proposed method consistently recovers the underlying system dynamics with the
optimal rate.
</p>
<a href="http://arxiv.org/abs/2012.04160" target="_blank">arXiv:2012.04160</a> [<a href="http://arxiv.org/pdf/2012.04160" target="_blank">pdf</a>]

<h2>Learning Independent Instance Maps for Crowd Localization. (arXiv:2012.04164v1 [cs.CV])</h2>
<h3>Junyu Gao, Tao Han, Yuan Yuan, Qi Wang</h3>
<p>Accurately locating each head's position in the crowd scenes is a crucial
task in the field of crowd analysis. However, traditional density-based methods
only predict coarse prediction, and segmentation/detection-based methods cannot
handle extremely dense scenes and large-range scale-variations crowds. To this
end, we propose an end-to-end and straightforward framework for crowd
localization, named Independent Instance Map segmentation (IIM). Different from
density maps and boxes regression, each instance in IIM is non-overlapped. By
segmenting crowds into independent connected components, the positions and the
crowd counts (the centers and the number of components, respectively) are
obtained. Furthermore, to improve the segmentation quality for different
density regions, we present a differentiable Binarization Module (BM) to output
structured instance maps. BM brings two advantages into localization models: 1)
adaptively learn a threshold map for different images to detect each instance
more accurately; 2) directly train the model using loss on binary predictions
and labels. Extensive experiments verify the proposed method is effective and
outperforms the-state-of-the-art methods on the five popular crowd datasets.
Significantly, IIM improves F1-measure by 10.4\% on the NWPU-Crowd Localization
task. The source code and pre-trained models will be released at
\url{https://github.com/taohan10200/IIM}.
</p>
<a href="http://arxiv.org/abs/2012.04164" target="_blank">arXiv:2012.04164</a> [<a href="http://arxiv.org/pdf/2012.04164" target="_blank">pdf</a>]

<h2>Weakly-Supervised Cross-Domain Adaptation for Endoscopic Lesions Segmentation. (arXiv:2012.04170v1 [cs.CV])</h2>
<h3>Jiahua Dong, Yang Cong, Gan Sun, Yunsheng Yang, Xiaowei Xu, Zhengming Ding</h3>
<p>Weakly-supervised learning has attracted growing research attention on
medical lesions segmentation due to significant saving in pixel-level
annotation cost. However, 1) most existing methods require effective prior and
constraints to explore the intrinsic lesions characterization, which only
generates incorrect and rough prediction; 2) they neglect the underlying
semantic dependencies among weakly-labeled target enteroscopy diseases and
fully-annotated source gastroscope lesions, while forcefully utilizing
untransferable dependencies leads to the negative performance. To tackle above
issues, we propose a new weakly-supervised lesions transfer framework, which
can not only explore transferable domain-invariant knowledge across different
datasets, but also prevent the negative transfer of untransferable
representations. Specifically, a Wasserstein quantified transferability
framework is developed to highlight widerange transferable contextual
dependencies, while neglecting the irrelevant semantic characterizations.
Moreover, a novel selfsupervised pseudo label generator is designed to equally
provide confident pseudo pixel labels for both hard-to-transfer and
easyto-transfer target samples. It inhibits the enormous deviation of false
pseudo pixel labels under the self-supervision manner. Afterwards,
dynamically-searched feature centroids are aligned to narrow category-wise
distribution shift. Comprehensive theoretical analysis and experiments show the
superiority of our model on the endoscopic dataset and several public datasets.
</p>
<a href="http://arxiv.org/abs/2012.04170" target="_blank">arXiv:2012.04170</a> [<a href="http://arxiv.org/pdf/2012.04170" target="_blank">pdf</a>]

<h2>Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization. (arXiv:2012.04171v1 [cs.LG])</h2>
<h3>Joshua C. Chang, Patrick Fletcher, Jungmin Han, Ted L.Chang, Shashaank Vattikuti, Bart Desmet, Ayah Zirikly, Carson C. Chow</h3>
<p>Dimensionality reduction methods for count data are critical to a wide range
of applications in medical informatics and other fields where model
interpretability is paramount. For such data, hierarchical Poisson matrix
factorization (HPF) and other sparse probabilistic non-negative matrix
factorization (NMF) methods are considered to be interpretable generative
models. They consist of sparse transformations for decoding their learned
representations into predictions. However, sparsity in representation decoding
does not necessarily imply sparsity in the encoding of representations from the
original data features. HPF is often incorrectly interpreted in the literature
as if it possesses encoder sparsity. The distinction between decoder sparsity
and encoder sparsity is subtle but important. Due to the lack of encoder
sparsity, HPF does not possess the column-clustering property of classical NMF
-- the factor loading matrix does not sufficiently define how each factor is
formed from the original features. We address this deficiency by
self-consistently enforcing encoder sparsity, using a generalized additive
model (GAM), thereby allowing one to relate each representation coordinate to a
subset of the original data features. In doing so, the method also gains the
ability to perform feature selection. We demonstrate our method on simulated
data and give an example of how encoder sparsity is of practical use in a
concrete application of representing inpatient comorbidities in Medicare
patients.
</p>
<a href="http://arxiv.org/abs/2012.04171" target="_blank">arXiv:2012.04171</a> [<a href="http://arxiv.org/pdf/2012.04171" target="_blank">pdf</a>]

<h2>WSR: A WiFi Sensor for Collaborative Robotics. (arXiv:2012.04174v1 [cs.RO])</h2>
<h3>Ninad Jadhav, Weiying Wang, Diana Zhang, Oussama Khatib, Swarun Kumar, Stephanie Gil</h3>
<p>In this paper we derive a new capability for robots to measure relative
direction, or Angle-of-Arrival (AOA), to other robots operating in
non-line-of-sight and unmapped environments with occlusions, without requiring
external infrastructure. We do so by capturing all of the paths that a WiFi
signal traverses as it travels from a transmitting to a receiving robot, which
we term an AOA profile. The key intuition is to "emulate antenna arrays in the
air" as the robots move in 3D space, a method akin to Synthetic Aperture Radar
(SAR). The main contributions include development of i) a framework to
accommodate arbitrary 3D trajectories, as well as continuous mobility all
robots, while computing AOA profiles and ii) an accompanying analysis that
provides a lower bound on variance of AOA estimation as a function of robot
trajectory geometry based on the Cramer Rao Bound. This is a critical
distinction with previous work on SAR that restricts robot mobility to
prescribed motion patterns, does not generalize to 3D space, and/or requires
transmitting robots to be static during data acquisition periods. Our method
results in more accurate AOA profiles and thus better AOA estimation, and
formally characterizes this observation as the informativeness of the
trajectory; a computable quantity for which we derive a closed form. All
theoretical developments are substantiated by extensive simulation and hardware
experiments. We also show that our formulation can be used with an
off-the-shelf trajectory estimation sensor. Finally, we demonstrate the
performance of our system on a multi-robot dynamic rendezvous task.
</p>
<a href="http://arxiv.org/abs/2012.04174" target="_blank">arXiv:2012.04174</a> [<a href="http://arxiv.org/pdf/2012.04174" target="_blank">pdf</a>]

<h2>Multi-modal Visual Tracking: Review and Experimental Comparison. (arXiv:2012.04176v1 [cs.CV])</h2>
<h3>Pengyu Zhang, Dong Wang, Huchuan Lu</h3>
<p>Visual object tracking, as a fundamental task in computer vision, has drawn
much attention in recent years. To extend trackers to a wider range of
applications, researchers have introduced information from multiple modalities
to handle specific scenes, which is a promising research prospect with emerging
methods and benchmarks. To provide a thorough review of multi-modal track-ing,
we summarize the multi-modal tracking algorithms, especially visible-depth
(RGB-D) tracking and visible-thermal (RGB-T) tracking in a unified taxonomy
from different aspects. Second, we provide a detailed description of the
related benchmarks and challenges. Furthermore, we conduct extensive
experiments to analyze the effectiveness of trackers on five datasets: PTB,
VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, we discuss various future
directions from different perspectives, including model design and dataset
construction for further research.
</p>
<a href="http://arxiv.org/abs/2012.04176" target="_blank">arXiv:2012.04176</a> [<a href="http://arxiv.org/pdf/2012.04176" target="_blank">pdf</a>]

<h2>GraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs. (arXiv:2012.04187v1 [cs.LG])</h2>
<h3>Binghui Wang, Ang Li, Hai Li, Yiran Chen</h3>
<p>Graph-based semi-supervised node classification (GraphSSC) has wide
applications, ranging from networking and security to data mining and machine
learning, etc. However, existing centralized GraphSSC methods are impractical
to solve many real-world graph-based problems, as collecting the entire graph
and labeling a reasonable number of labels is time-consuming and costly, and
data privacy may be also violated. Federated learning (FL) is an emerging
learning paradigm that enables collaborative learning among multiple clients,
which can mitigate the issue of label scarcity and protect data privacy as
well. Therefore, performing GraphSSC under the FL setting is a promising
solution to solve real-world graph-based problems. However, existing FL methods
1) perform poorly when data across clients are non-IID, 2) cannot handle data
with new label domains, and 3) cannot leverage unlabeled data, while all these
issues naturally happen in real-world graph-based problems. To address the
above issues, we propose the first FL framework, namely GraphFL, for
semi-supervised node classification on graphs. Our framework is motivated by
meta-learning methods. Specifically, we propose two GraphFL methods to
respectively address the non-IID issue in graph data and handle the tasks with
new label domains. Furthermore, we design a self-training method to leverage
unlabeled graph data. We adopt representative graph neural networks as GraphSSC
methods and evaluate GraphFL on multiple graph datasets. Experimental results
demonstrate that GraphFL significantly outperforms the compared FL baseline and
GraphFL with self-training can obtain better performance.
</p>
<a href="http://arxiv.org/abs/2012.04187" target="_blank">arXiv:2012.04187</a> [<a href="http://arxiv.org/pdf/2012.04187" target="_blank">pdf</a>]

<h2>Robustness of Accuracy Metric and its Inspirations in Learning with Noisy Labels. (arXiv:2012.04193v1 [cs.LG])</h2>
<h3>Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, Pheng-Ann Heng</h3>
<p>For multi-class classification under class-conditional label noise, we prove
that the accuracy metric itself can be robust. We concretize this finding's
inspiration in two essential aspects: training and validation, with which we
address critical issues in learning with noisy labels. For training, we show
that maximizing training accuracy on sufficiently many noisy samples yields an
approximately optimal classifier. For validation, we prove that a noisy
validation set is reliable, addressing the critical demand of model selection
in scenarios like hyperparameter-tuning and early stopping. Previously, model
selection using noisy validation samples has not been theoretically justified.
We verify our theoretical results and additional claims with extensive
experiments. We show characterizations of models trained with noisy labels,
motivated by our theoretical results, and verify the utility of a noisy
validation set by showing the impressive performance of a framework termed
noisy best teacher and student (NTS). Our code is released.
</p>
<a href="http://arxiv.org/abs/2012.04193" target="_blank">arXiv:2012.04193</a> [<a href="http://arxiv.org/pdf/2012.04193" target="_blank">pdf</a>]

<h2>Neural fidelity warping for efficient robot morphology design. (arXiv:2012.04195v1 [cs.RO])</h2>
<h3>Sha Hu, Zeshi Yang, Greg Mori</h3>
<p>We consider the problem of optimizing a robot morphology to achieve the best
performance for a target task, under computational resource limitations. The
evaluation process for each morphological design involves learning a controller
for the design, which can consume substantial time and computational resources.
To address the challenge of expensive robot morphology evaluation, we present a
continuous multi-fidelity Bayesian Optimization framework that efficiently
utilizes computational resources via low-fidelity evaluations. We identify the
problem of non-stationarity over fidelity space. Our proposed fidelity warping
mechanism can learn representations of learning epochs and tasks to model
non-stationary covariances between continuous fidelity evaluations which prove
challenging for off-the-shelf stationary kernels. Various experiments
demonstrate that our method can utilize the low-fidelity evaluations to
efficiently search for the optimal robot morphology, outperforming
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.04195" target="_blank">arXiv:2012.04195</a> [<a href="http://arxiv.org/pdf/2012.04195" target="_blank">pdf</a>]

<h2>VAE-Info-cGAN: Generating Synthetic Images by Combining Pixel-level and Feature-level Geospatial Conditional Inputs. (arXiv:2012.04196v1 [cs.CV])</h2>
<h3>Xuerong Xiao, Swetava Ganguli, Vipul Pandey</h3>
<p>Training robust supervised deep learning models for many geospatial
applications of computer vision is difficult due to dearth of class-balanced
and diverse training data. Conversely, obtaining enough training data for many
applications is financially prohibitive or may be infeasible, especially when
the application involves modeling rare or extreme events. Synthetically
generating data (and labels) using a generative model that can sample from a
target distribution and exploit the multi-scale nature of images can be an
inexpensive solution to address scarcity of labeled data. Towards this goal, we
present a deep conditional generative model, called VAE-Info-cGAN, that
combines a Variational Autoencoder (VAE) with a conditional Information
Maximizing Generative Adversarial Network (InfoGAN), for synthesizing
semantically rich images simultaneously conditioned on a pixel-level condition
(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC
can only vary in the channel dimension from the synthesized image and is meant
to be a task-specific input. The FLC is modeled as an attribute vector in the
latent space of the generated image which controls the contributions of various
characteristic attributes germane to the target distribution. An interpretation
of the attribute vector to systematically generate synthetic images by varying
a chosen binary macroscopic feature is explored. Experiments on a GPS
trajectories dataset show that the proposed model can accurately generate
various forms of spatio-temporal aggregates across different geographic
locations while conditioned only on a raster representation of the road
network. The primary intended application of the VAE-Info-cGAN is synthetic
data (and label) generation for targeted data augmentation for computer
vision-based modeling of problems relevant to geospatial analysis and remote
sensing.
</p>
<a href="http://arxiv.org/abs/2012.04196" target="_blank">arXiv:2012.04196</a> [<a href="http://arxiv.org/pdf/2012.04196" target="_blank">pdf</a>]

<h2>Cost Sensitive Optimization of Deepfake Detector. (arXiv:2012.04199v1 [cs.CV])</h2>
<h3>Ivan Kukanov, Janne Karttunen, Hannu Sillanp&#xe4;&#xe4;, Ville Hautam&#xe4;ki</h3>
<p>Since the invention of cinema, the manipulated videos have existed. But
generating manipulated videos that can fool the viewer has been a
time-consuming endeavor. With the dramatic improvements in the deep generative
modeling, generating believable looking fake videos has become a reality. In
the present work, we concentrate on the so-called deepfake videos, where the
source face is swapped with the targets. We argue that deepfake detection task
should be viewed as a screening task, where the user, such as the video
streaming platform, will screen a large number of videos daily. It is clear
then that only a small fraction of the uploaded videos are deepfakes, so the
detection performance needs to be measured in a cost-sensitive way. Preferably,
the model parameters also need to be estimated in the same way. This is
precisely what we propose here.
</p>
<a href="http://arxiv.org/abs/2012.04199" target="_blank">arXiv:2012.04199</a> [<a href="http://arxiv.org/pdf/2012.04199" target="_blank">pdf</a>]

<h2>GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms. (arXiv:2012.04201v1 [cs.LG])</h2>
<h3>Jiwei Liu, Bojan Tunguz, Gilberto Titericz</h3>
<p>Black-box optimization is essential for tuning complex machine learning
algorithms which are easier to experiment with than to understand. In this
paper, we show that a simple ensemble of black-box optimization algorithms can
outperform any single one of them. However, searching for such an optimal
ensemble requires a large number of experiments. We propose a
Multi-GPU-optimized framework to accelerate a brute force search for the
optimal ensemble of black-box optimization algorithms by running many
experiments in parallel. The lightweight optimizations are performed by CPU
while expensive model training and evaluations are assigned to GPUs. The
multi-GPU solution achieves 10x speedup of the CPU implementation. With the
optimal ensemble found by GPU-accelerated exhaustive search, we won the 2nd
place of NeurIPS 2020 black-box optimization challenge.
</p>
<a href="http://arxiv.org/abs/2012.04201" target="_blank">arXiv:2012.04201</a> [<a href="http://arxiv.org/pdf/2012.04201" target="_blank">pdf</a>]

<h2>Efficient Estimation of Influence of a Training Instance. (arXiv:2012.04207v1 [cs.LG])</h2>
<h3>Sosuke Kobayashi, Sho Yokoi, Jun Suzuki, Kentaro Inui</h3>
<p>Understanding the influence of a training instance on a neural network model
leads to improving interpretability. However, it is difficult and inefficient
to evaluate the influence, which shows how a model's prediction would be
changed if a training instance were not used. In this paper, we propose an
efficient method for estimating the influence. Our method is inspired by
dropout, which zero-masks a sub-network and prevents the sub-network from
learning each training instance. By switching between dropout masks, we can use
sub-networks that learned or did not learn each training instance and estimate
its influence. Through experiments with BERT and VGGNet on classification
datasets, we demonstrate that the proposed method can capture training
influences, enhance the interpretability of error predictions, and cleanse the
training dataset for improving generalization.
</p>
<a href="http://arxiv.org/abs/2012.04207" target="_blank">arXiv:2012.04207</a> [<a href="http://arxiv.org/pdf/2012.04207" target="_blank">pdf</a>]

<h2>The Architectural Implications of Distributed Reinforcement Learning on CPU-GPU Systems. (arXiv:2012.04210v1 [cs.LG])</h2>
<h3>Ahmet Inci, Evgeny Bolotin, Yaosheng Fu, Gal Dalal, Shie Mannor, David Nellans, Diana Marculescu</h3>
<p>With deep reinforcement learning (RL) methods achieving results that exceed
human capabilities in games, robotics, and simulated environments, continued
scaling of RL training is crucial to its deployment in solving complex
real-world problems. However, improving the performance scalability and power
efficiency of RL training through understanding the architectural implications
of CPU-GPU systems remains an open problem. In this work we investigate and
improve the performance and power efficiency of distributed RL training on
CPU-GPU systems by approaching the problem not solely from the GPU
microarchitecture perspective but following a holistic system-level analysis
approach. We quantify the overall hardware utilization on a state-of-the-art
distributed RL training framework and empirically identify the bottlenecks
caused by GPU microarchitectural, algorithmic, and system-level design choices.
We show that the GPU microarchitecture itself is well-balanced for
state-of-the-art RL frameworks, but further investigation reveals that the
number of actors running the environment interactions and the amount of
hardware resources available to them are the primary performance and power
efficiency limiters. To this end, we introduce a new system design metric,
CPU/GPU ratio, and show how to find the optimal balance between CPU and GPU
resources when designing scalable and efficient CPU-GPU systems for RL
training.
</p>
<a href="http://arxiv.org/abs/2012.04210" target="_blank">arXiv:2012.04210</a> [<a href="http://arxiv.org/pdf/2012.04210" target="_blank">pdf</a>]

<h2>Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives. (arXiv:2012.04216v1 [cs.AI])</h2>
<h3>Angie Peng, Jeff Naecker, Ben Hutchinson, Andrew Smart, Nyalleng Moorosi</h3>
<p>How should we decide which fairness criteria or definitions to adopt in
machine learning systems? To answer this question, we must study the fairness
preferences of actual users of machine learning systems. Stringent parity
constraints on treatment or impact can come with trade-offs, and may not even
be preferred by the social groups in question (Zafar et al., 2017). Thus it
might be beneficial to elicit what the group's preferences are, rather than
rely on a priori defined mathematical fairness constraints. Simply asking for
self-reported rankings of users is challenging because research has shown that
there are often gaps between people's stated and actual preferences(Bernheim et
al., 2013).

This paper outlines a research program and experimental designs for
investigating these questions. Participants in the experiments are invited to
perform a set of tasks in exchange for a base payment--they are told upfront
that they may receive a bonus later on, and the bonus could depend on some
combination of output quantity and quality. The same group of workers then
votes on a bonus payment structure, to elicit preferences. The voting is
hypothetical (not tied to an outcome) for half the group and actual (tied to
the actual payment outcome) for the other half, so that we can understand the
relation between a group's actual preferences and hypothetical (stated)
preferences. Connections and lessons from fairness in machine learning are
explored.
</p>
<a href="http://arxiv.org/abs/2012.04216" target="_blank">arXiv:2012.04216</a> [<a href="http://arxiv.org/pdf/2012.04216" target="_blank">pdf</a>]

<h2>Evaluating Explainable Methods for Predictive Process Analytics: A Functionally-Grounded Approach. (arXiv:2012.04218v1 [cs.AI])</h2>
<h3>Mythreyi Velmurugan, Chun Ouyang, Catarina Moreira, Renuka Sindhgatta</h3>
<p>Predictive process analytics focuses on predicting the future states of
running instances of a business process. While advanced machine learning
techniques have been used to increase accuracy of predictions, the resulting
predictive models lack transparency. Current explainable machine learning
methods, such as LIME and SHAP, can be used to interpret black box models.
However, it is unclear how fit for purpose these methods are in explaining
process predictive models. In this paper, we draw on evaluation measures used
in the field of explainable AI and propose functionally-grounded evaluation
metrics for assessing explainable methods in predictive process analytics. We
apply the proposed metrics to evaluate the performance of LIME and SHAP in
interpreting process predictive models built on XGBoost, which has been shown
to be relatively accurate in process predictions. We conduct the evaluation
using three open source, real-world event logs and analyse the evaluation
results to derive insights. The research contributes to understanding the
trustworthiness of explainable methods for predictive process analytics as a
fundamental and key step towards human user-oriented evaluation.
</p>
<a href="http://arxiv.org/abs/2012.04218" target="_blank">arXiv:2012.04218</a> [<a href="http://arxiv.org/pdf/2012.04218" target="_blank">pdf</a>]

<h2>Federated Multi-Task Learning for Competing Constraints. (arXiv:2012.04221v1 [cs.LG])</h2>
<h3>Tian Li, Shengyuan Hu, Ahmad Beirami, Virginia Smith</h3>
<p>In addition to accuracy, fairness and robustness are two critical concerns
for federated learning systems. In this work, we first identify that robustness
to adversarial training-time attacks and fairness, measured as the uniformity
of performance across devices, are competing constraints in statistically
heterogeneous networks. To address these constraints, we propose employing a
simple, general multi-task learning objective, and analyze the ability of the
objective to achieve a favorable tradeoff between fairness and robustness. We
develop a scalable solver for the objective and show that multi-task learning
can enable more accurate, robust, and fair models relative to state-of-the-art
baselines across a suite of federated datasets.
</p>
<a href="http://arxiv.org/abs/2012.04221" target="_blank">arXiv:2012.04221</a> [<a href="http://arxiv.org/pdf/2012.04221" target="_blank">pdf</a>]

<h2>Scale Aware Adaptation for Land-Cover Classification in Remote Sensing Imagery. (arXiv:2012.04222v1 [cs.CV])</h2>
<h3>Xueqing Deng, Yi Zhu, Yuxin Tian, Shawn Newsam</h3>
<p>Land-cover classification using remote sensing imagery is an important Earth
observation task. Recently, land cover classification has benefited from the
development of fully connected neural networks for semantic segmentation. The
benchmark datasets available for training deep segmentation models in remote
sensing imagery tend to be small, however, often consisting of only a handful
of images from a single location with a single scale. This limits the models'
ability to generalize to other datasets. Domain adaptation has been proposed to
improve the models' generalization but we find these approaches are not
effective for dealing with the scale variation commonly found between remote
sensing image collections. We therefore propose a scale aware adversarial
learning framework to perform joint cross-location and cross-scale land-cover
classification. The framework has a dual discriminator architecture with a
standard feature discriminator as well as a novel scale discriminator. We also
introduce a scale attention module which produces scale-enhanced features.
Experimental results show that the proposed framework outperforms
state-of-the-art domain adaptation methods by a large margin.
</p>
<a href="http://arxiv.org/abs/2012.04222" target="_blank">arXiv:2012.04222</a> [<a href="http://arxiv.org/pdf/2012.04222" target="_blank">pdf</a>]

<h2>KNN-enhanced Deep Learning Against Noisy Labels. (arXiv:2012.04224v1 [cs.CV])</h2>
<h3>Shuyu Kong, You Li, Jia Wang, Amin Rezaei, Hai Zhou</h3>
<p>Supervised learning on Deep Neural Networks (DNNs) is data hungry. Optimizing
performance of DNN in the presence of noisy labels has become of paramount
importance since collecting a large dataset will usually bring in noisy labels.
Inspired by the robustness of K-Nearest Neighbors (KNN) against data noise, in
this work, we propose to apply deep KNN for label cleanup. Our approach
leverages DNNs for feature extraction and KNN for ground-truth label inference.
We iteratively train the neural network and update labels to simultaneously
proceed towards higher label recovery rate and better classification
performance. Experiment results show that under the same setting, our approach
outperforms existing label correction methods and achieves better accuracy on
multiple datasets, e.g.,76.78% on Clothing1M dataset.
</p>
<a href="http://arxiv.org/abs/2012.04224" target="_blank">arXiv:2012.04224</a> [<a href="http://arxiv.org/pdf/2012.04224" target="_blank">pdf</a>]

<h2>Active Learning: Problem Settings and Recent Developments. (arXiv:2012.04225v1 [cs.LG])</h2>
<h3>Hideitsu Hino</h3>
<p>In supervised learning, acquiring labeled training data for a predictive
model can be very costly, but acquiring a large amount of unlabeled data is
often quite easy. Active learning is a method of obtaining predictive models
with high precision at a limited cost through the adaptive selection of samples
for labeling. This paper explains the basic problem settings of active learning
and recent research trends. In particular, research on learning acquisition
functions to select samples from the data for labeling, theoretical work on
active learning algorithms, and stopping criteria for sequential data
acquisition are highlighted. Application examples for material development and
measurement are introduced.
</p>
<a href="http://arxiv.org/abs/2012.04225" target="_blank">arXiv:2012.04225</a> [<a href="http://arxiv.org/pdf/2012.04225" target="_blank">pdf</a>]

<h2>A Unifying Framework for Formal Theories of Novelty:Framework, Examples and Discussion. (arXiv:2012.04226v1 [cs.AI])</h2>
<h3>T. E. Boult, P. A. Grabowicz, D. S. Prijatelj, R. Stern, L. Holder, J. Alspector, M. Jafarzadeh, T. Ahmad, A. R. Dhamija, C.Li, S. Cruz, A. Shrivastava, C. Vondrick, W. J. Scheirer</h3>
<p>Managing inputs that are novel, unknown, or out-of-distribution is critical
as an agent moves from the lab to the open world. Novelty-related problems
include being tolerant to novel perturbations of the normal input, detecting
when the input includes novel items, and adapting to novel inputs. While
significant research has been undertaken in these areas, a noticeable gap
exists in the lack of a formalized definition of novelty that transcends
problem domains. As a team of researchers spanning multiple research groups and
different domains, we have seen, first hand, the difficulties that arise from
ill-specified novelty problems, as well as inconsistent definitions and
terminology. Therefore, we present the first unified framework for formal
theories of novelty and use the framework to formally define a family of
novelty types. Our framework can be applied across a wide range of domains,
from symbolic AI to reinforcement learning, and beyond to open world image
recognition. Thus, it can be used to help kick-start new research efforts and
accelerate ongoing work on these important novelty-related problems. This
extended version of our AAAI 2021 paper included more details and examples in
multiple domains.
</p>
<a href="http://arxiv.org/abs/2012.04226" target="_blank">arXiv:2012.04226</a> [<a href="http://arxiv.org/pdf/2012.04226" target="_blank">pdf</a>]

<h2>Accelerating Continuous Normalizing Flow with Trajectory Polynomial Regularization. (arXiv:2012.04228v1 [cs.LG])</h2>
<h3>Han-Hsien Huang, Mi-Yen Yeh</h3>
<p>In this paper, we propose an approach to effectively accelerating the
computation of continuous normalizing flow (CNF), which has been proven to be a
powerful tool for the tasks such as variational inference and density
estimation. The training time cost of CNF can be extremely high because the
required number of function evaluations (NFE) for solving corresponding
ordinary differential equations (ODE) is very large. We think that the high NFE
results from large truncation errors of solving ODEs. To address the problem,
we propose to add a regularization. The regularization penalizes the difference
between the trajectory of the ODE and its fitted polynomial regression. The
trajectory of ODE will approximate a polynomial function, and thus the
truncation error will be smaller. Furthermore, we provide two proofs and claim
that the additional regularization does not harm training quality. Experimental
results show that our proposed method can result in 42.3% to 71.3% reduction of
NFE on the task of density estimation, and 19.3% to 32.1% reduction of NFE on
variational auto-encoder, while the testing losses are not affected at all.
</p>
<a href="http://arxiv.org/abs/2012.04228" target="_blank">arXiv:2012.04228</a> [<a href="http://arxiv.org/pdf/2012.04228" target="_blank">pdf</a>]

<h2>Molecule Optimization via Fragment-based Generative Models. (arXiv:2012.04231v1 [cs.LG])</h2>
<h3>Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, Xia Ning</h3>
<p>In drug discovery, molecule optimization is an important step in order to
modify drug candidates into better ones in terms of desired drug properties.
With the recent advance of Artificial Intelligence, this traditionally in vitro
process has been increasingly facilitated by in silico approaches. We present
an innovative in silico approach to computationally optimizing molecules and
formulate the problem as to generate optimized molecular graphs via deep
generative models. Our generative models follow the key idea of fragment-based
drug design, and optimize molecules by modifying their small fragments. Our
models learn how to identify the to-be-optimized fragments and how to modify
such fragments by learning from the difference of molecules that have good and
bad properties. In optimizing a new molecule, our models apply the learned
signals to decode optimized fragments at the predicted location of the
fragments. We also construct multiple such models into a pipeline such that
each of the models in the pipeline is able to optimize one fragment, and thus
the entire pipeline is able to modify multiple fragments of molecule if needed.
We compare our models with other state-of-the-art methods on benchmark datasets
and demonstrate that our methods significantly outperform others with more than
80% property improvement under moderate molecular similarity constraints, and
more than 10% property improvement under high molecular similarity constraints.
</p>
<a href="http://arxiv.org/abs/2012.04231" target="_blank">arXiv:2012.04231</a> [<a href="http://arxiv.org/pdf/2012.04231" target="_blank">pdf</a>]

<h2>Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework. (arXiv:2012.04240v1 [cs.LG])</h2>
<h3>Sung-En Chang, Yanyu Li, Mengshu Sun, Runbin Shi, Hayden K.-H. So, Xuehai Qian, Yanzhi Wang, Xue Lin</h3>
<p>Deep Neural Networks (DNNs) have achieved extraordinary performance in
various application domains. To support diverse DNN models, efficient
implementations of DNN inference on edge-computing platforms, e.g., ASICs,
FPGAs, and embedded systems, are extensively investigated. Due to the huge
model size and computation amount, model compression is a critical step to
deploy DNN models on edge devices. This paper focuses on weight quantization, a
hardware-friendly model compression approach that is complementary to weight
pruning. Unlike existing methods that use the same quantization scheme for all
weights, we propose the first solution that applies different quantization
schemes for different rows of the weight matrix. It is motivated by (1) the
distribution of the weights in the different rows are not the same; and (2) the
potential of achieving better utilization of heterogeneous FPGA hardware
resources. To achieve that, we first propose a hardware-friendly quantization
scheme named sum-of-power-of-2 (SP2) suitable for Gaussian-like weight
distribution, in which the multiplication arithmetic can be replaced with logic
shifter and adder, thereby enabling highly efficient implementations with the
FPGA LUT resources. In contrast, the existing fixed-point quantization is
suitable for Uniform-like weight distribution and can be implemented
efficiently by DSP. Then to fully explore the resources, we propose an
FPGA-centric mixed scheme quantization (MSQ) with an ensemble of the proposed
SP2 and the fixed-point schemes. Combining the two schemes can maintain, or
even increase accuracy due to better matching with weight distributions.
</p>
<a href="http://arxiv.org/abs/2012.04240" target="_blank">arXiv:2012.04240</a> [<a href="http://arxiv.org/pdf/2012.04240" target="_blank">pdf</a>]

<h2>Texture Transform Attention for Realistic Image Inpainting. (arXiv:2012.04242v1 [cs.CV])</h2>
<h3>Yejin Kim, Manri Cheon, Junwoo Lee</h3>
<p>Over the last few years, the performance of inpainting to fill missing
regions has shown significant improvements by using deep neural networks. Most
of inpainting work create a visually plausible structure and texture, however,
due to them often generating a blurry result, final outcomes appear unrealistic
and make feel heterogeneity. In order to solve this problem, the existing
methods have used a patch based solution with deep neural network, however,
these methods also cannot transfer the texture properly. Motivated by these
observation, we propose a patch based method. Texture Transform Attention
network(TTA-Net) that better produces the missing region inpainting with fine
details. The task is a single refinement network and takes the form of U-Net
architecture that transfers fine texture features of encoder to coarse semantic
features of decoder through skip-connection. Texture Transform Attention is
used to create a new reassembled texture map using fine textures and coarse
semantics that can efficiently transfer texture information as a result. To
stabilize training process, we use a VGG feature layer of ground truth and
patch discriminator. We evaluate our model end-to-end with the publicly
available datasets CelebA-HQ and Places2 and demonstrate that images of higher
quality can be obtained to the existing state-of-the-art approaches.
</p>
<a href="http://arxiv.org/abs/2012.04242" target="_blank">arXiv:2012.04242</a> [<a href="http://arxiv.org/pdf/2012.04242" target="_blank">pdf</a>]

<h2>Out-Of-Distribution Detection With Subspace Techniques And Probabilistic Modeling Of Features. (arXiv:2012.04250v1 [cs.LG])</h2>
<h3>Ibrahima Ndiour, Nilesh Ahuja, Omesh Tickoo</h3>
<p>This paper presents a principled approach for detecting out-of-distribution
(OOD) samples in deep neural networks (DNN). Modeling probability distributions
on deep features has recently emerged as an effective, yet computationally
cheap method to detect OOD samples in DNN. However, the features produced by a
DNN at any given layer do not fully occupy the corresponding high-dimensional
feature space. We apply linear statistical dimensionality reduction techniques
and nonlinear manifold-learning techniques on the high-dimensional features in
order to capture the true subspace spanned by the features. We hypothesize that
such lower-dimensional feature embeddings can mitigate the curse of
dimensionality, and enhance any feature-based method for more efficient and
effective performance. In the context of uncertainty estimation and OOD, we
show that the log-likelihood score obtained from the distributions learnt on
this lower-dimensional subspace is more discriminative for OOD detection. We
also show that the feature reconstruction error, which is the $L_2$-norm of the
difference between the original feature and the pre-image of its embedding, is
highly effective for OOD detection and in some cases superior to the
log-likelihood scores. The benefits of our approach are demonstrated on image
features by detecting OOD images, using popular DNN architectures on commonly
used image datasets such as CIFAR10, CIFAR100, and SVHN.
</p>
<a href="http://arxiv.org/abs/2012.04250" target="_blank">arXiv:2012.04250</a> [<a href="http://arxiv.org/pdf/2012.04250" target="_blank">pdf</a>]

<h2>Variational Interaction Information Maximization for Cross-domain Disentanglement. (arXiv:2012.04251v1 [cs.CV])</h2>
<h3>HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, Kee-Eung Kim</h3>
<p>Cross-domain disentanglement is the problem of learning representations
partitioned into domain-invariant and domain-specific representations, which is
a key to successful domain transfer or measuring semantic distance between two
domains. Grounded in information theory, we cast the simultaneous learning of
domain-invariant and domain-specific representations as a joint objective of
multiple information constraints, which does not require adversarial training
or gradient reversal layers. We derive a tractable bound of the objective and
propose a generative model named Interaction Information Auto-Encoder (IIAE).
Our approach reveals insights on the desirable representation for cross-domain
disentanglement and its connection to Variational Auto-Encoder (VAE). We
demonstrate the validity of our model in the image-to-image translation and the
cross-domain retrieval tasks. We further show that our model achieves the
state-of-the-art performance in the zero-shot sketch based image retrieval
task, even without external knowledge. Our implementation is publicly available
at: https://github.com/gr8joo/IIAE
</p>
<a href="http://arxiv.org/abs/2012.04251" target="_blank">arXiv:2012.04251</a> [<a href="http://arxiv.org/pdf/2012.04251" target="_blank">pdf</a>]

<h2>Data Instance Prior for Transfer Learning in GANs. (arXiv:2012.04256v1 [cs.CV])</h2>
<h3>Puneet Mangla, Nupur Kumari, Mayank Singh, Vineeth N Balasubramanian, Balaji Krishnamurthy</h3>
<p>Recent advances in generative adversarial networks (GANs) have shown
remarkable progress in generating high-quality images. However, this gain in
performance depends on the availability of a large amount of training data. In
limited data regimes, training typically diverges, and therefore the generated
samples are of low quality and lack diversity. Previous works have addressed
training in low data setting by leveraging transfer learning and data
augmentation techniques. We propose a novel transfer learning method for GANs
in the limited data domain by leveraging informative data prior derived from
self-supervised/supervised pre-trained networks trained on a diverse source
domain. We perform experiments on several standard vision datasets using
various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the
proposed method effectively transfers knowledge to domains with few target
images, outperforming existing state-of-the-art techniques in terms of image
quality and diversity. We also show the utility of data instance prior in
large-scale unconditional image generation and image editing tasks.
</p>
<a href="http://arxiv.org/abs/2012.04256" target="_blank">arXiv:2012.04256</a> [<a href="http://arxiv.org/pdf/2012.04256" target="_blank">pdf</a>]

<h2>Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v1 [cs.CV])</h2>
<h3>Shao-Yuan Lo, Jeya Maria Jose Valanarasu, Vishal M. Patel</h3>
<p>Adversarial robustness of deep neural networks is an extensively studied
problem in the literature and various methods have been proposed to defend
against adversarial images. However, only a handful of defense methods have
been developed for defending against attacked videos. In this paper, we propose
a novel Over-and-Under complete restoration network for Defending against
adversarial videos (OUDefend). Most restoration networks adopt an
encoder-decoder architecture that first shrinks spatial dimension then expands
it back. This approach learns undercomplete representations, which have large
receptive fields to collect global information but overlooks local details. On
the other hand, overcomplete representations have opposite properties. Hence,
OUDefend is designed to balance local and global features by learning those two
representations. We attach OUDefend to target video recognition models as a
feature restoration block and train the entire network end-to-end. Experimental
results show that the defenses focusing on images may be ineffective to videos,
while OUDefend enhances robustness against different types of adversarial
videos, ranging from additive attacks, multiplicative attacks to physically
realizable attacks.
</p>
<a href="http://arxiv.org/abs/2012.04262" target="_blank">arXiv:2012.04262</a> [<a href="http://arxiv.org/pdf/2012.04262" target="_blank">pdf</a>]

<h2>Active Visual Localization in Partially Calibrated Environments. (arXiv:2012.04263v1 [cs.CV])</h2>
<h3>Yingda Yin, Qingnan Fan, Fei Xia, Qihang Fang, Siyan Dong, Leonidas Guibas, Baoquan Chen</h3>
<p>Humans can robustly localize themselves without a map after they get lost
following prominent visual cues or landmarks. In this work, we aim at endowing
autonomous agents the same ability. Such ability is important in robotics
applications yet very challenging when an agent is exposed to partially
calibrated environments, where camera images with accurate 6 Degree-of-Freedom
pose labels only cover part of the scene. To address the above challenge, we
explore using Reinforcement Learning to search for a policy to generate
intelligent motions so as to actively localize the agent given visual
information in partially calibrated environments. Our core contribution is to
formulate the active visual localization problem as a Partially Observable
Markov Decision Process and propose an algorithmic framework based on Deep
Reinforcement Learning to solve it. We further propose an indoor scene dataset
ACR-6, which consists of both synthetic and real data and simulates challenging
scenarios for active visual localization. We benchmark our algorithm against
handcrafted baselines for localization and demonstrate that our approach
significantly outperforms them on localization success rate.
</p>
<a href="http://arxiv.org/abs/2012.04263" target="_blank">arXiv:2012.04263</a> [<a href="http://arxiv.org/pdf/2012.04263" target="_blank">pdf</a>]

<h2>Learning to Generate Content-Aware Dynamic Detectors. (arXiv:2012.04265v1 [cs.CV])</h2>
<h3>Junyi Feng, Jiashen Hua, Baisheng Lai, Jianqiang Huang, Xi Li, Xian-sheng Hua</h3>
<p>Model efficiency is crucial for object detection. Mostprevious works rely on
either hand-crafted design or auto-search methods to obtain a static
architecture, regardless ofthe difference of inputs. In this paper, we
introduce a newperspective of designing efficient detectors, which is
automatically generating sample-adaptive model architectureon the fly. The
proposed method is named content-aware dynamic detectors (CADDet). It first
applies a multi-scale densely connected network with dynamic routing as the
supernet. Furthermore, we introduce a course-to-fine strat-egy tailored for
object detection to guide the learning of dynamic routing, which contains two
metrics: 1) dynamic global budget constraint assigns data-dependent
expectedbudgets for individual samples; 2) local path similarity regularization
aims to generate more diverse routing paths. With these, our method achieves
higher computational efficiency while maintaining good performance. To the best
of our knowledge, our CADDet is the first work to introduce dynamic routing
mechanism in object detection. Experiments on MS-COCO dataset demonstrate that
CADDet achieves 1.8 higher mAP with 10% fewer FLOPs compared with vanilla
routing strategy. Compared with the models based upon similar building blocks,
CADDet achieves a 42% FLOPs reduction with a competitive mAP.
</p>
<a href="http://arxiv.org/abs/2012.04265" target="_blank">arXiv:2012.04265</a> [<a href="http://arxiv.org/pdf/2012.04265" target="_blank">pdf</a>]

<h2>UnrealPerson: An Adaptive Pipeline towards Costless Person Re-identification. (arXiv:2012.04268v1 [cs.CV])</h2>
<h3>Tianyu Zhang, Lingxi Xie, Longhui Wei, Zijie Zhuang, Yongfei Zhang, Bo Li, Qi Tian</h3>
<p>The main difficulty of person re-identification (ReID) lies in collecting
annotated data and transferring the model across different domains. This paper
presents UnrealPerson, a novel pipeline that makes full use of unreal image
data to decrease the costs in both the training and deployment stages. Its
fundamental part is a system that can generate synthesized images of
high-quality and from controllable distributions. Instance-level annotation
goes with the synthesized data and is almost free. We point out some details in
image synthesis that largely impact the data quality. With 3,000 IDs and
120,000 instances, our method achieves a 38.5% rank-1 accuracy when being
directly transferred to MSMT17. It almost doubles the former record using
synthesized data and even surpasses previous direct transfer records using real
data. This offers a good basis for unsupervised domain adaption, where our
pre-trained model is easily plugged into the state-of-the-art algorithms
towards higher accuracy. In addition, the data distribution can be flexibly
adjusted to fit some corner ReID scenarios, which widens the application of our
pipeline. We will publish our data synthesis toolkit and synthesized data in
https://github.com/FlyHighest/UnrealPerson.
</p>
<a href="http://arxiv.org/abs/2012.04268" target="_blank">arXiv:2012.04268</a> [<a href="http://arxiv.org/pdf/2012.04268" target="_blank">pdf</a>]

<h2>Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain Adaptation using Structurally Regularized Deep Clustering. (arXiv:2012.04280v1 [cs.CV])</h2>
<h3>Hui Tang, Xiatian Zhu, Ke Chen, Kui Jia, C. L. Philip Chen</h3>
<p>Unsupervised domain adaptation (UDA) is to learn classification models that
make predictions for unlabeled data on a target domain, given labeled data on a
source domain whose distribution diverges from the target one. Mainstream UDA
methods strive to learn domain-aligned features such that classifiers trained
on the source features can be readily applied to the target ones. Although
impressive results have been achieved, these methods have a potential risk of
damaging the intrinsic data structures of target discrimination, raising an
issue of generalization particularly for UDA tasks in an inductive setting. To
address this issue, we are motivated by a UDA assumption of structural
similarity across domains, and propose to directly uncover the intrinsic target
discrimination via constrained clustering, where we constrain the clustering
solutions using structural source regularization that hinges on the very same
assumption. Technically, we propose a hybrid model of Structurally Regularized
Deep Clustering, which integrates the regularized discriminative clustering of
target data with a generative one, and we thus term our method as SRDC++. Our
hybrid model is based on a deep clustering framework that minimizes the
Kullback-Leibler divergence between the distribution of network prediction and
an auxiliary one, where we impose structural regularization by learning
domain-shared classifier and cluster centroids. By enriching the structural
similarity assumption, we are able to extend SRDC++ for a pixel-level UDA task
of semantic segmentation. We conduct extensive experiments on seven UDA
benchmarks of image classification and semantic segmentation. With no explicit
feature alignment, our proposed SRDC++ outperforms all the existing methods
under both the inductive and transductive settings. We make our implementation
codes publicly available at https://github.com/huitangtang/SRDCPP.
</p>
<a href="http://arxiv.org/abs/2012.04280" target="_blank">arXiv:2012.04280</a> [<a href="http://arxiv.org/pdf/2012.04280" target="_blank">pdf</a>]

<h2>A Quality Diversity Approach to Automatically Generating Human-Robot Interaction Scenarios in Shared Autonomy. (arXiv:2012.04283v1 [cs.RO])</h2>
<h3>Matthew Fontaine, Stefanos Nikolaidis</h3>
<p>As interactions between humans and robots grow in scale and complexity, the
human-robot interaction (HRI) research community needs new computational
methods to automatically evaluate the performance of novel algorithms and
applications. Strong evaluation methods should reduce researcher bias and
explore the diverse scenarios of interaction between humans and robots. We
propose quality diversity (QD) algorithms as a method for simultaneously
exploring both environments and human actions to discover diverse failure
scenarios. We focus on the shared autonomy domain, where the robot attempts to
infer the goal of a human operator. We evaluate our approach by automatically
generating scenarios for two published algorithms in this domain: shared
autonomy via hindsight optimization and linear policy blending. Some of the
generated scenarios confirm previous theoretical findings, while others are
surprising and bring about a new understanding of state-of-the-art
implementations. Our experiments show that QD outperforms Monte-Carlo
simulation and optimization based methods in effectively searching the scenario
space, highlighting its promise for automatic evaluation in the shared autonomy
domain.
</p>
<a href="http://arxiv.org/abs/2012.04283" target="_blank">arXiv:2012.04283</a> [<a href="http://arxiv.org/pdf/2012.04283" target="_blank">pdf</a>]

<h2>Optimal Survival Trees. (arXiv:2012.04284v1 [cs.LG])</h2>
<h3>Dimitris Bertsimas, Jack Dunn, Emma Gibson, Agni Orfanoudaki</h3>
<p>Tree-based models are increasingly popular due to their ability to identify
complex relationships that are beyond the scope of parametric models. Survival
tree methods adapt these models to allow for the analysis of censored outcomes,
which often appear in medical data. We present a new Optimal Survival Trees
algorithm that leverages mixed-integer optimization (MIO) and local search
techniques to generate globally optimized survival tree models. We demonstrate
that the OST algorithm improves on the accuracy of existing survival tree
methods, particularly in large datasets.
</p>
<a href="http://arxiv.org/abs/2012.04284" target="_blank">arXiv:2012.04284</a> [<a href="http://arxiv.org/pdf/2012.04284" target="_blank">pdf</a>]

<h2>CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v1 [cs.AI])</h2>
<h3>Tayfun Ates, Muhammed Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, Deniz Yuret</h3>
<p>Recent advances in Artificial Intelligence and deep learning have revived the
interest in studying the gap between the reasoning capabilities of humans and
machines. In this ongoing work, we introduce CRAFT, a new visual question
answering dataset that requires causal reasoning about physical forces and
object interactions. It contains 38K video and question pairs that are
generated from 3K videos from 10 different virtual environments, containing
different number of objects in motion that interact with each other. Two
question categories from CRAFT include previously studied descriptive and
counterfactual questions. Besides, inspired by the theory of force dynamics
from the field of human cognitive psychology, we introduce new question
categories that involve understanding the intentions of objects through the
notions of cause, enable, and prevent. Our preliminary results demonstrate that
even though these tasks are very intuitive for humans, the implemented
baselines could not cope with the underlying challenges.
</p>
<a href="http://arxiv.org/abs/2012.04293" target="_blank">arXiv:2012.04293</a> [<a href="http://arxiv.org/pdf/2012.04293" target="_blank">pdf</a>]

<h2>Context-Aware Graph Convolution Network for Target Re-identification. (arXiv:2012.04298v1 [cs.CV])</h2>
<h3>Deyi Ji, Haoran Wang, Hanzhe Hu, Weihao Gan, Wei Wu, Junjie Yan</h3>
<p>Most existing re-identification methods focus on learning ro-bust and
discriminative features with deep convolution net-works. However, many of them
consider content similarityseparately and fail to utilize the context
information of thequery and gallery sets, e.g. probe-gallery and
gallery-galleryrelations, thus hard samples may not be well solved due tothe
limited or even misleading information. In this paper,we present a novel
Context-Aware Graph Convolution Net-work (CAGCN), where the probe-gallery
relations are en-coded into the graph nodes and the graph edge connectionsare
well controlled by the gallery-gallery relations. In thisway, hard samples can
be addressed with the context infor-mation flows among other easy samples
during the graph rea-soning. Specifically, we adopt an effective hard gallery
sam-pler to obtain high recall for positive samples while keeping areasonable
graph size, which can also weaken the imbalancedproblem in training process
with low computation complex-ity. Experiments show that the proposed method
achievesstate-of-the-art performance on both person and vehicle
re-identification datasets in a plug and play fashion with limitedoverhead.
</p>
<a href="http://arxiv.org/abs/2012.04298" target="_blank">arXiv:2012.04298</a> [<a href="http://arxiv.org/pdf/2012.04298" target="_blank">pdf</a>]

<h2>Perceptual Robust Hashing for Color Images with Canonical Correlation Analysis. (arXiv:2012.04312v1 [cs.CV])</h2>
<h3>Xinran Li, Chuan Qin, Zhenxing Qian, Heng Yao, Xinpeng Zhang</h3>
<p>In this paper, a novel perceptual image hashing scheme for color images is
proposed based on ring-ribbon quadtree and color vector angle. First, original
image is subjected to normalization and Gaussian low-pass filtering to produce
a secondary image, which is divided into a series of ring-ribbons with
different radii and the same number of pixels. Then, both textural and color
features are extracted locally and globally. Quadtree decomposition (QD) is
applied on luminance values of the ring-ribbons to extract local textural
features, and the gray level co-occurrence matrix (GLCM) is used to extract
global textural features. Local color features of significant corner points on
outer boundaries of ring-ribbons are extracted through color vector angles
(CVA), and color low-order moments (CLMs) is utilized to extract global color
features. Finally, two types of feature vectors are fused via canonical
correlation analysis (CCA) to prodcue the final hash after scrambling. Compared
with direct concatenation, the CCA feature fusion method improves
classification performance, which better reflects overall correlation between
two sets of feature vectors. Receiver operating characteristic (ROC) curve
shows that our scheme has satisfactory performances with respect to robustness,
discrimination and security, which can be effectively used in copy detection
and content authentication.
</p>
<a href="http://arxiv.org/abs/2012.04312" target="_blank">arXiv:2012.04312</a> [<a href="http://arxiv.org/pdf/2012.04312" target="_blank">pdf</a>]

<h2>Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning. (arXiv:2012.04324v1 [cs.CV])</h2>
<h3>Riccardo Volpi, Diane Larlus, Gr&#xe9;gory Rogez</h3>
<p>Most standard learning approaches lead to fragile models which are prone to
drift when sequentially trained on samples of a different nature - the
well-known "catastrophic forgetting" issue. In particular, when a model
consecutively learns from different visual domains, it tends to forget the past
ones in favor of the most recent. In this context, we show that one way to
learn models that are inherently more robust against forgetting is domain
randomization - for vision tasks, randomizing the current domain's distribution
with heavy image manipulations. Building on this result, we devise a
meta-learning strategy where a regularizer explicitly penalizes any loss
associated with transferring the model from the current domain to different
"auxiliary" meta-domains, while also easing adaptation to them. Such
meta-domains, are also generated through randomized image manipulations. We
empirically demonstrate in a variety of experiments - spanning from
classification to semantic segmentation - that our approach results in models
that are less prone to catastrophic forgetting when transferred to new domains.
</p>
<a href="http://arxiv.org/abs/2012.04324" target="_blank">arXiv:2012.04324</a> [<a href="http://arxiv.org/pdf/2012.04324" target="_blank">pdf</a>]

<h2>StacMR: Scene-Text Aware Cross-Modal Retrieval. (arXiv:2012.04329v1 [cs.CV])</h2>
<h3>Andr&#xe9;s Mafla, Rafael Sampaio de Rezende, Llu&#xed;s G&#xf3;mez, Diane Larlus, Dimosthenis Karatzas</h3>
<p>Recent models for cross-modal retrieval have benefited from an increasingly
rich understanding of visual scenes, afforded by scene graphs and object
interactions to mention a few. This has resulted in an improved matching
between the visual representation of an image and the textual representation of
its caption. Yet, current visual representations overlook a key aspect: the
text appearing in images, which may contain crucial information for retrieval.
In this paper, we first propose a new dataset that allows exploration of
cross-modal retrieval where images contain scene-text instances. Then, armed
with this dataset, we describe several approaches which leverage scene text,
including a better scene-text aware cross-modal retrieval method which uses
specialized representations for text from the captions and text from the visual
scene, and reconcile them in a common embedding space. Extensive experiments
confirm that cross-modal retrieval approaches benefit from scene text and
highlight interesting research questions worth exploring further. Dataset and
code are available at this http URL
</p>
<a href="http://arxiv.org/abs/2012.04329" target="_blank">arXiv:2012.04329</a> [<a href="http://arxiv.org/pdf/2012.04329" target="_blank">pdf</a>]

<h2>Two-Phase Learning for Overcoming Noisy Labels. (arXiv:2012.04337v1 [cs.LG])</h2>
<h3>Hwanjun Song, Minseok Kim, Dongmin Park, Jae-Gil Lee</h3>
<p>To counter the challenge associated with noise labels, the learning strategy
of deep neural networks must be differentiated over the learning period during
the training process. Therefore, we propose a novel two-phase learning method,
MORPH, which automatically transitions its learning phase at the point when the
network begins to rapidly memorize false-labeled samples. In the first phase,
MORPH starts to update the network for all the training samples before the
transition point. Without any supervision, the learning phase is converted to
the next phase on the basis of the estimated best transition point.
Subsequently, MORPH resumes the training of the network only for a maximal safe
set, which maintains the collection of almost certainly true-labeled samples at
each epoch. Owing to its two-phase learning, MORPH realizes noise-free training
for any type of label noise for practical use. Moreover, extensive experiments
using six datasets verify that MORPH significantly outperforms five
state-of-the art methods in terms of test error and training time.
</p>
<a href="http://arxiv.org/abs/2012.04337" target="_blank">arXiv:2012.04337</a> [<a href="http://arxiv.org/pdf/2012.04337" target="_blank">pdf</a>]

<h2>An Empirical Study of Explainable AI Techniques on Deep Learning Models For Time Series Tasks. (arXiv:2012.04344v1 [cs.LG])</h2>
<h3>Udo Schlegel, Daniela Oelke, Daniel A. Keim, Mennatallah El-Assady</h3>
<p>Decision explanations of machine learning black-box models are often
generated by applying Explainable AI (XAI) techniques. However, many proposed
XAI methods produce unverified outputs. Evaluation and verification are usually
achieved with a visual interpretation by humans on individual images or text.
In this preregistration, we propose an empirical study and benchmark framework
to apply attribution methods for neural networks developed for images and text
data on time series. We present a methodology to automatically evaluate and
rank attribution techniques on time series using perturbation methods to
identify reliable approaches.
</p>
<a href="http://arxiv.org/abs/2012.04344" target="_blank">arXiv:2012.04344</a> [<a href="http://arxiv.org/pdf/2012.04344" target="_blank">pdf</a>]

<h2>k-Factorization Subspace Clustering. (arXiv:2012.04345v1 [cs.LG])</h2>
<h3>Jicong Fan</h3>
<p>Subspace clustering (SC) aims to cluster data lying in a union of
low-dimensional subspaces. Usually, SC learns an affinity matrix and then
performs spectral clustering. Both steps suffer from high time and space
complexity, which leads to difficulty in clustering large datasets. This paper
presents a method called k-Factorization Subspace Clustering (k-FSC) for
large-scale subspace clustering. K-FSC directly factorizes the data into k
groups via pursuing structured sparsity in the matrix factorization model.
Thus, k-FSC avoids learning affinity matrix and performing eigenvalue
decomposition, and hence has low time and space complexity on large datasets.
An efficient algorithm is proposed to solve the optimization of k-FSC. In
addition, k-FSC is able to handle noise, outliers, and missing data and
applicable to arbitrarily large datasets and streaming data. Extensive
experiments show that k-FSC outperforms state-of-the-art subspace clustering
methods.
</p>
<a href="http://arxiv.org/abs/2012.04345" target="_blank">arXiv:2012.04345</a> [<a href="http://arxiv.org/pdf/2012.04345" target="_blank">pdf</a>]

<h2>MANGO: A Mask Attention Guided One-Stage Scene Text Spotter. (arXiv:2012.04350v1 [cs.CV])</h2>
<h3>Liang Qiao, Ying Chen, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, Fei Wu</h3>
<p>Recently end-to-end scene text spotting has become a popular research topic
due to its advantages of global optimization and high maintainability in real
applications. Most methods attempt to develop various region of interest (RoI)
operations to concatenate the detection part and the sequence recognition part
into a two-stage text spotting framework. However, in such framework, the
recognition part is highly sensitive to the detected results (\emph{e.g.}, the
compactness of text contours). To address this problem, in this paper, we
propose a novel Mask AttentioN Guided One-stage text spotting framework named
MANGO, in which character sequences can be directly recognized without RoI
operation. Concretely, a position-aware mask attention module is developed to
generate attention weights on each text instance and its characters. It allows
different text instances in an image to be allocated on different feature map
channels which are further grouped as a batch of instance features. Finally, a
lightweight sequence decoder is applied to generate the character sequences. It
is worth noting that MANGO inherently adapts to arbitrary-shaped text spotting
and can be trained end-to-end with only coarse position information
(\emph{e.g.}, rectangular bounding box) and text annotations. Experimental
results show that the proposed method achieves competitive and even new
state-of-the-art performance on both regular and irregular text spotting
benchmarks, i.e., ICDAR 2013, ICDAR 2015, Total-Text, and SCUT-CTW1500.
</p>
<a href="http://arxiv.org/abs/2012.04350" target="_blank">arXiv:2012.04350</a> [<a href="http://arxiv.org/pdf/2012.04350" target="_blank">pdf</a>]

<h2>Data Dependent Randomized Smoothing. (arXiv:2012.04351v1 [cs.LG])</h2>
<h3>Motasem Alfarra, Adel Bibi, Philip H. S. Torr, Bernard Ghanem</h3>
<p>Randomized smoothing is a recent technique that achieves state-of-art
performance in training certifiably robust deep neural networks. While the
smoothing family of distributions is often connected to the choice of the norm
used for certification, the parameters of the distributions are always set as
global hyper parameters independent of the input data on which a network is
certified. In this work, we revisit Gaussian randomized smoothing where we show
that the variance of the Gaussian distribution can be optimized at each input
so as to maximize the certification radius for the construction of the smoothed
classifier. This new approach is generic, parameter-free, and easy to
implement. In fact, we show that our data dependent framework can be seamlessly
incorporated into 3 randomized smoothing approaches, leading to consistent
improved certified accuracy. When this framework is used in the training
routine of these approaches followed by a data dependent certification, we get
9% and 6% improvement over the certified accuracy of the strongest baseline for
a radius of 0.5 on CIFAR10 and ImageNet, respectively.
</p>
<a href="http://arxiv.org/abs/2012.04351" target="_blank">arXiv:2012.04351</a> [<a href="http://arxiv.org/pdf/2012.04351" target="_blank">pdf</a>]

<h2>Reinforcement Based Learning on Classification Task Could Yield Better Generalization and Adversarial Accuracy. (arXiv:2012.04353v1 [cs.LG])</h2>
<h3>Shashi Kant Gupta</h3>
<p>Deep Learning has become interestingly popular in computer vision, mostly
attaining near or above human-level performance in various vision tasks. But
recent work has also demonstrated that these deep neural networks are very
vulnerable to adversarial examples (adversarial examples - inputs to a model
which are naturally similar to original data but fools the model in classifying
it into a wrong class). Humans are very robust against such perturbations; one
possible reason could be that humans do not learn to classify based on an error
between "target label" and "predicted label" but possibly due to reinforcements
that they receive on their predictions. In this work, we proposed a novel
method to train deep learning models on an image classification task. We used a
reward-based optimization function, similar to the vanilla policy gradient
method used in reinforcement learning, to train our model instead of
conventional cross-entropy loss. An empirical evaluation on the cifar10 dataset
showed that our method learns a more robust classifier than the same model
architecture trained using cross-entropy loss function (on adversarial
training). At the same time, our method shows a better generalization with the
difference in test accuracy and train accuracy $&lt; 2\%$ for most of the time
compared to the cross-entropy one, whose difference most of the time remains $&gt;
2\%$.
</p>
<a href="http://arxiv.org/abs/2012.04353" target="_blank">arXiv:2012.04353</a> [<a href="http://arxiv.org/pdf/2012.04353" target="_blank">pdf</a>]

<h2>3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection. (arXiv:2012.04355v1 [cs.CV])</h2>
<h3>He Wang, Yezhen Cong, Or Litany, Yue Gao, Leonidas J. Guibas</h3>
<p>3D object detection is an important yet demanding task that heavily relies on
difficult to obtain 3D annotations. To reduce the required amount of
supervision, we propose 3DIoUMatch, a novel method for semi-supervised 3D
object detection. We adopt VoteNet, a popular point cloud-based object
detector, as our backbone and leverage a teacher-student mutual learning
framework to propagate information from the labeled to the unlabeled train set
in the form of pseudo-labels. However, due to the high task complexity, we
observe that the pseudo-labels suffer from significant noise and are thus not
directly usable. To that end, we introduce a confidence-based filtering
mechanism. The key to our approach is a novel differentiable 3D IoU estimation
module. This module is used for filtering poorly localized proposals as well as
for IoU-guided bounding box deduplication. At inference time, this module is
further utilized to improve localization through test-time optimization. Our
method consistently improves state-of-the-art methods on both ScanNet and
SUN-RGBD benchmarks by significant margins. For example, when training using
only 10\% labeled data on ScanNet, 3DIoUMatch achieves 7.7 absolute improvement
on mAP@0.25 and 8.5 absolute improvement on mAP@0.5 upon the prior art.
</p>
<a href="http://arxiv.org/abs/2012.04355" target="_blank">arXiv:2012.04355</a> [<a href="http://arxiv.org/pdf/2012.04355" target="_blank">pdf</a>]

<h2>DE-RRD: A Knowledge Distillation Framework for Recommender System. (arXiv:2012.04357v1 [cs.LG])</h2>
<h3>SeongKu Kang, Junyoung Hwang, Wonbin Kweon, Hwanjo Yu</h3>
<p>Recent recommender systems have started to employ knowledge distillation,
which is a model compression technique distilling knowledge from a cumbersome
model (teacher) to a compact model (student), to reduce inference latency while
maintaining performance. The state-of-the-art methods have only focused on
making the student model accurately imitate the predictions of the teacher
model. They have a limitation in that the prediction results incompletely
reveal the teacher's knowledge. In this paper, we propose a novel knowledge
distillation framework for recommender system, called DE-RRD, which enables the
student model to learn from the latent knowledge encoded in the teacher model
as well as from the teacher's predictions. Concretely, DE-RRD consists of two
methods: 1) Distillation Experts (DE) that directly transfers the latent
knowledge from the teacher model. DE exploits "experts" and a novel expert
selection strategy for effectively distilling the vast teacher's knowledge to
the student with limited capacity. 2) Relaxed Ranking Distillation (RRD) that
transfers the knowledge revealed from the teacher's prediction with
consideration of the relaxed ranking orders among items. Our extensive
experiments show that DE-RRD outperforms the state-of-the-art competitors and
achieves comparable or even better performance to that of the teacher model
with faster inference time.
</p>
<a href="http://arxiv.org/abs/2012.04357" target="_blank">arXiv:2012.04357</a> [<a href="http://arxiv.org/pdf/2012.04357" target="_blank">pdf</a>]

<h2>Efficient Automatic CASH via Rising Bandits. (arXiv:2012.04371v1 [cs.LG])</h2>
<h3>Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, Bin Cui</h3>
<p>The Combined Algorithm Selection and Hyperparameter optimization (CASH) is
one of the most fundamental problems in Automatic Machine Learning (AutoML).
The existing Bayesian optimization (BO) based solutions turn the CASH problem
into a Hyperparameter Optimization (HPO) problem by combining the
hyperparameters of all machine learning (ML) algorithms, and use BO methods to
solve it. As a result, these methods suffer from the low-efficiency problem due
to the huge hyperparameter space in CASH. To alleviate this issue, we propose
the alternating optimization framework, where the HPO problem for each ML
algorithm and the algorithm selection problem are optimized alternately. In
this framework, the BO methods are used to solve the HPO problem for each ML
algorithm separately, incorporating a much smaller hyperparameter space for BO
methods. Furthermore, we introduce Rising Bandits, a CASH-oriented Multi-Armed
Bandits (MAB) variant, to model the algorithm selection in CASH. This framework
can take the advantages of both BO in solving the HPO problem with a relatively
small hyperparameter space and the MABs in accelerating the algorithm
selection. Moreover, we further develop an efficient online algorithm to solve
the Rising Bandits with provably theoretical guarantees. The extensive
experiments on 30 OpenML datasets demonstrate the superiority of the proposed
approach over the competitive baselines.
</p>
<a href="http://arxiv.org/abs/2012.04371" target="_blank">arXiv:2012.04371</a> [<a href="http://arxiv.org/pdf/2012.04371" target="_blank">pdf</a>]

<h2>MAP-Elites enables Powerful Stepping Stones and Diversity for Modular Robotics. (arXiv:2012.04375v1 [cs.RO])</h2>
<h3>J&#xf8;rgen Nordmoen, Frank Veenstra, Kai Olav Ellefsen, Kyrre Glette</h3>
<p>In modular robotics, modules can be reconfigured to change the morphology of
the robot, making it able to adapt for specific tasks. However, optimizing both
the body and control is a difficult challenge due to the intricate relationship
between fine-tuning control and morphological changes that can invalidate such
optimizations. To solve this challenge we compare three different Evolutionary
Algorithms on their capacity to optimize morphologies in modular robotics. We
compare two objective-based search algorithms, with MAP-Elites. To understand
the benefit of diversity we transition the evolved populations into two
difficult environments to see if diversity can have an impact on solving
complex environments. In addition, we analyse the genealogical ancestry to shed
light on the notion of stepping stones as key to enable high performance. The
results show that MAP-Elites is capable of evolving the highest performing
solutions in addition to generating the largest morphological diversity. For
the transition between environments the results show that MAP-Elites is better
at regaining performance by promoting morphological diversity. With the
analysis of genealogical ancestry we show that MAP-Elites produces more diverse
and higher performing stepping stones than the other objective-based search
algorithms. Transitioning the populations to more difficult environments show
the utility of morphological diversity, while the analysis of stepping stones
show a strong correlation between diversity of ancestry and maximum performance
on the locomotion task. The paper shows the advantage of promoting diversity
for solving a locomotion task in different environments for modular robotics.
By showing that the quality and diversity of stepping stones in Evolutionary
Algorithms is an important factor for overall performance we have opened up a
new area of analysis and results.
</p>
<a href="http://arxiv.org/abs/2012.04375" target="_blank">arXiv:2012.04375</a> [<a href="http://arxiv.org/pdf/2012.04375" target="_blank">pdf</a>]

<h2>Learning Structured Declarative Rule Sets -- A Challenge for Deep Discrete Learning. (arXiv:2012.04377v1 [cs.LG])</h2>
<h3>Johannes F&#xfc;rnkranz, Eyke H&#xfc;llermeier, Eneldo Loza Menc&#xed;a, Michael Rapp</h3>
<p>Arguably the key reason for the success of deep neural networks is their
ability to autonomously form non-linear combinations of the input features,
which can be used in subsequent layers of the network. The analogon to this
capability in inductive rule learning is to learn a structured rule base, where
the inputs are combined to learn new auxiliary concepts, which can then be used
as inputs by subsequent rules. Yet, research on rule learning algorithms that
have such capabilities is still in their infancy, which is - we would argue -
one of the key impediments to substantial progress in this field. In this
position paper, we want to draw attention to this unsolved problem, with a
particular focus on previous work in predicate invention and multi-label rule
learning
</p>
<a href="http://arxiv.org/abs/2012.04377" target="_blank">arXiv:2012.04377</a> [<a href="http://arxiv.org/pdf/2012.04377" target="_blank">pdf</a>]

<h2>Forecasting the Olympic medal distribution during a pandemic: a socio-economic machine learning model. (arXiv:2012.04378v1 [cs.LG])</h2>
<h3>Christoph Schlembach, Sascha L. Schmidt, Dominik Schreyer, Linus Wunderlich</h3>
<p>Forecasting the number of Olympic medals for each nation is highly relevant
for different stakeholders: Ex ante, sports betting companies can determine the
odds while sponsors and media companies can allocate their resources to
promising teams. Ex post, sports politicians and managers can benchmark the
performance of their teams and evaluate the drivers of success. To
significantly increase the Olympic medal forecasting accuracy, we apply machine
learning, more specifically a two-staged Random Forest, thus outperforming more
traditional na\"ive forecast for three previous Olympics held between 2008 and
2016 for the first time. Regarding the Tokyo 2020 Games in 2021, our model
suggests that the United States will lead the Olympic medal table, winning 120
medals, followed by China (87) and Great Britain (74). Intriguingly, we predict
that the current COVID-19 pandemic will not significantly alter the medal count
as all countries suffer from the pandemic to some extent (data inherent) and
limited historical data points on comparable diseases (model inherent).
</p>
<a href="http://arxiv.org/abs/2012.04378" target="_blank">arXiv:2012.04378</a> [<a href="http://arxiv.org/pdf/2012.04378" target="_blank">pdf</a>]

<h2>Using Feature Alignment can Improve Clean Average Precision and Adversarial Robustness in Object Detection. (arXiv:2012.04382v1 [cs.CV])</h2>
<h3>Weipeng Xu, Hongcheng Huang</h3>
<p>The 2D object detection in clean images has been a well studied topic, but
its vulnerability against adversarial attacks is still worrying. Existing work
has improved the robustness of object detector by adversarial training, but at
the same time, the average precision (AP) on clean images drops significantly.
In this paper, we improve object detection algorithm by guiding the output of
intermediate feature layer. On the basis of adversarial training, we propose
two feature alignment methods, namely Knowledge-Distilled Feature Alignment
(KDFA) and Self-Supervised Feature Alignment (SSFA). The detector's clean AP
and robustness can be improved by aligning the features of the middle layer of
the network. We conduct extensive experiments on PASCAL VOC and MS-COCO
datasets to verify the effectiveness of our proposed approach. The code of our
experiments is available at https://github.com/grispeut/Feature-Alignment.git.
</p>
<a href="http://arxiv.org/abs/2012.04382" target="_blank">arXiv:2012.04382</a> [<a href="http://arxiv.org/pdf/2012.04382" target="_blank">pdf</a>]

<h2>Structure-Consistent Weakly Supervised Salient Object Detection with Local Saliency Coherence. (arXiv:2012.04404v1 [cs.CV])</h2>
<h3>Siyue Yu, Bingfeng Zhang, Jimin Xiao, Eng Gee Lim</h3>
<p>Sparse labels have been attracting much attention in recent years. However,
the performance gap between weakly supervised and fully supervised salient
object detection methods is huge, and most previous weakly supervised works
adopt complex training methods with many bells and whistles. In this work, we
propose a one-round end-to-end training approach for weakly supervised salient
object detection via scribble annotations without pre/post-processing
operations or extra supervision data. Since scribble labels fail to offer
detailed salient regions, we propose a local coherence loss to propagate the
labels to unlabeled regions based on image features and pixel distance, so as
to predict integral salient regions with complete object structures. We design
a saliency structure consistency loss as self-supervision to ensure consistent
saliency maps are predicted with different scales of the same image as input,
which could be viewed as a regularization technique to enhance the model
generalization ability. Additionally, we design an aggregation module (AGGM) to
better integrate high-level features, low-level features and global context
information for the decoder to aggregate various information. Extensive
experiments show that our method achieves a new state-of-the-art performance on
six benchmarks (e.g. for the ECSSD dataset: F_\beta = 0.8995, E_\xi = 0.9079
and MAE = 0.0489$), with an average gain of 4.60\% for F-measure, 2.05\% for
E-measure and 1.88\% for MAE over the previous best method on this task. Source
code is available at this http URL
</p>
<a href="http://arxiv.org/abs/2012.04404" target="_blank">arXiv:2012.04404</a> [<a href="http://arxiv.org/pdf/2012.04404" target="_blank">pdf</a>]

<h2>NavRep: Unsupervised Representations for Reinforcement Learning of Robot Navigation in Dynamic Human Environments. (arXiv:2012.04406v1 [cs.RO])</h2>
<h3>Daniel Dugas, Juan Nieto, Roland Siegwart, Jen Jen Chung</h3>
<p>Robot navigation is a task where reinforcement learning approaches are still
unable to compete with traditional path planning. State-of-the-art methods
differ in small ways, and do not all provide reproducible, openly available
implementations. This makes comparing methods a challenge. Recent research has
shown that unsupervised learning methods can scale impressively, and be
leveraged to solve difficult problems. In this work, we design ways in which
unsupervised learning can be used to assist reinforcement learning for robot
navigation. We train two end-to-end, and 18 unsupervised-learning-based
architectures, and compare them, along with existing approaches, in unseen test
cases. We demonstrate our approach working on a real life robot. Our results
show that unsupervised learning methods are competitive with end-to-end
methods. We also highlight the importance of various components such as input
representation, predictive unsupervised learning, and latent features. We make
all our models publicly available, as well as training and testing
environments, and tools. This release also includes OpenAI-gym-compatible
environments designed to emulate the training conditions described by other
papers, with as much fidelity as possible. Our hope is that this helps in
bringing together the field of RL for robot navigation, and allows meaningful
comparisons across state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2012.04406" target="_blank">arXiv:2012.04406</a> [<a href="http://arxiv.org/pdf/2012.04406" target="_blank">pdf</a>]

<h2>Active machine learning for spatio-temporal predictions using feature embedding. (arXiv:2012.04407v1 [cs.LG])</h2>
<h3>Arsam Aryandoust, Stefan Pfenninger</h3>
<p>Active learning (AL) could contribute to solving critical environmental
problems through improved spatio-temporal predictions. Yet such predictions
involve high-dimensional feature spaces with mixed data types and missing data,
which existing methods have difficulties dealing with. Here, we propose a novel
batch AL method that fills this gap. We encode and cluster features of
candidate data points, and query the best data based on the distance of
embedded features to their cluster centers. We introduce a new metric of
informativeness that we call embedding entropy and a general class of neural
networks that we call embedding networks for using it. Empirical tests on
forecasting electricity demand show a simultaneous reduction in prediction
error by up to 63-88% and data usage by up to 50-69% compared to passive
learning (PL) benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.04407" target="_blank">arXiv:2012.04407</a> [<a href="http://arxiv.org/pdf/2012.04407" target="_blank">pdf</a>]

<h2>Multiple Hypothesis Semantic Mapping for Robust Data Association. (arXiv:2012.04423v1 [cs.RO])</h2>
<h3>Lukas Bernreiter, Abel Gawel, Hannes Sommer, Juan Nieto, Roland Siegwart, Cesar Cadena</h3>
<p>In this paper, we present a semantic mapping approach with multiple
hypothesis tracking for data association. As semantic information has the
potential to overcome ambiguity in measurements and place recognition, it forms
an eminent modality for autonomous systems. This is particularly evident in
urban scenarios with several similar looking surroundings. Nevertheless, it
requires the handling of a non-Gaussian and discrete random variable coming
from object detectors. Previous methods facilitate semantic information for
global localization and data association to reduce the instance ambiguity
between the landmarks. However, many of these approaches do not deal with the
creation of complete globally consistent representations of the environment and
typically do not scale well. We utilize multiple hypothesis trees to derive a
probabilistic data association for semantic measurements by means of position,
instance and class to create a semantic representation. We propose an optimized
mapping method and make use of a pose graph to derive a novel semantic SLAM
solution. Furthermore, we show that semantic covisibility graphs allow for a
precise place recognition in urban environments. We verify our approach using
real-world outdoor dataset and demonstrate an average drift reduction of 33 %
w.r.t. the raw odometry source. Moreover, our approach produces 55 % less
hypotheses on average than a regular multiple hypotheses approach.
</p>
<a href="http://arxiv.org/abs/2012.04423" target="_blank">arXiv:2012.04423</a> [<a href="http://arxiv.org/pdf/2012.04423" target="_blank">pdf</a>]

<h2>On Irrelevant Literals in Pseudo-Boolean Constraint Learning. (arXiv:2012.04424v1 [cs.AI])</h2>
<h3>Danel Le Berre, Pierre Marquis, Stefan Mengel, Romain Wallon</h3>
<p>Learning pseudo-Boolean (PB) constraints in PB solvers exploiting cutting
planes based inference is not as well understood as clause learning in
conflict-driven clause learning solvers. In this paper, we show that PB
constraints derived using cutting planes may contain \emph{irrelevant
literals}, i.e., literals whose assigned values (whatever they are) never
change the truth value of the constraint. Such literals may lead to infer
constraints that are weaker than they should be, impacting the size of the
proof built by the solver, and thus also affecting its performance. This
suggests that current implementations of PB solvers based on cutting planes
should be reconsidered to prevent the generation of irrelevant literals.
Indeed, detecting and removing irrelevant literals is too expensive in practice
to be considered as an option (the associated problem is NP-hard.
</p>
<a href="http://arxiv.org/abs/2012.04424" target="_blank">arXiv:2012.04424</a> [<a href="http://arxiv.org/pdf/2012.04424" target="_blank">pdf</a>]

<h2>A General Computational Framework to Measure the Expressiveness of Complex Networks Using a Tighter Upper Bound of Linear Regions. (arXiv:2012.04428v1 [cs.LG])</h2>
<h3>Yutong Xie, Gaoxiang Chen, Quanzheng Li</h3>
<p>The expressiveness of deep neural network (DNN) is a perspective to
understandthe surprising performance of DNN. The number of linear regions, i.e.
pieces thata piece-wise-linear function represented by a DNN, is generally used
to measurethe expressiveness. And the upper bound of regions number partitioned
by a rec-tifier network, instead of the number itself, is a more practical
measurement ofexpressiveness of a rectifier DNN. In this work, we propose a new
and tighter up-per bound of regions number. Inspired by the proof of this upper
bound and theframework of matrix computation in Hinz &amp; Van de Geer (2019), we
propose ageneral computational approach to compute a tight upper bound of
regions numberfor theoretically any network structures (e.g. DNN with all kind
of skip connec-tions and residual structures). Our experiments show our upper
bound is tighterthan existing ones, and explain why skip connections and
residual structures canimprove network performance.
</p>
<a href="http://arxiv.org/abs/2012.04428" target="_blank">arXiv:2012.04428</a> [<a href="http://arxiv.org/pdf/2012.04428" target="_blank">pdf</a>]

<h2>RC-SSFL: Towards Robust and Communication-efficient Semi-supervised Federated Learning System. (arXiv:2012.04432v1 [cs.LG])</h2>
<h3>Yi Liu, Xingliang Yuan, Ruihui Zhao, Yifeng Zheng, Yefeng Zheng</h3>
<p>Federated Learning (FL) is an emerging decentralized artificial intelligence
paradigm, which promises to train a shared global model in high-quality while
protecting user data privacy. However, the current systems rely heavily on a
strong assumption: all clients have a wealth of ground truth labeled data,
which may not be always feasible in the real life. In this paper, we present a
practical Robust, and Communication-efficient Semi-supervised FL (RC-SSFL)
system design that can enable the clients to jointly learn a high-quality model
that is comparable to typical FL's performance. In this setting, we assume that
the client has only unlabeled data and the server has a limited amount of
labeled data. Besides, we consider malicious clients can launch poisoning
attacks to harm the performance of the global model. To solve this issue,
RC-SSFL employs a minimax optimization-based client selection strategy to
select the clients who hold high-quality updates and uses geometric median
aggregation to robustly aggregate model updates. Furthermore, RC-SSFL
implements a novel symmetric quantization method to greatly improve
communication efficiency. Extensive case studies on two real-world datasets
demonstrate that RC-SSFL can maintain the performance comparable to typical FL
in the presence of poisoning attacks and reduce communication overhead by $2
\times \sim 4 \times $.
</p>
<a href="http://arxiv.org/abs/2012.04432" target="_blank">arXiv:2012.04432</a> [<a href="http://arxiv.org/pdf/2012.04432" target="_blank">pdf</a>]

<h2>Towards Communication-efficient and Attack-Resistant Federated Edge Learning for Industrial Internet of Things. (arXiv:2012.04436v1 [cs.LG])</h2>
<h3>Yi Liu, Ruihui Zhao, Jiawen Kang, Abdulsalam Yassine, Dusit Niyato, Jialiang Peng</h3>
<p>Federated Edge Learning (FEL) allows edge nodes to train a global deep
learning model collaboratively for edge computing in the Industrial Internet of
Things (IIoT), which significantly promotes the development of Industrial 4.0.
However, FEL faces two critical challenges: communication overhead and data
privacy. FEL suffers from expensive communication overhead when training
large-scale multi-node models. Furthermore, due to the vulnerability of FEL to
gradient leakage and label-flipping attacks, the training process of the global
model is easily compromised by adversaries. To address these challenges, we
propose a communication-efficient and privacy-enhanced asynchronous FEL
framework for edge computing in IIoT. First, we introduce an asynchronous model
update scheme to reduce the computation time that edge nodes wait for global
model aggregation. Second, we propose an asynchronous local differential
privacy mechanism, which improves communication efficiency and mitigates
gradient leakage attacks by adding well-designed noise to the gradients of edge
nodes. Third, we design a cloud-side malicious node detection mechanism to
detect malicious nodes by testing the local model quality. Such a mechanism can
avoid malicious nodes participating in training to mitigate label-flipping
attacks. Extensive experimental studies on two real-world datasets demonstrate
that the proposed framework can not only improve communication efficiency but
also mitigate malicious attacks while its accuracy is comparable to traditional
FEL frameworks.
</p>
<a href="http://arxiv.org/abs/2012.04436" target="_blank">arXiv:2012.04436</a> [<a href="http://arxiv.org/pdf/2012.04436" target="_blank">pdf</a>]

<h2>SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine Reconstruction with Self-Projection Optimization. (arXiv:2012.04439v1 [cs.CV])</h2>
<h3>Xinhai Liu, Xinchen Liu, Zhizhong Han, Yu-Shen Liu</h3>
<p>The task of point cloud upsampling aims to acquire dense and uniform point
sets from sparse and irregular point sets. Although significant progress has
been made with deep learning models, they require ground-truth dense point sets
as the supervision information, which can only trained on synthetic paired
training data and are not suitable for training under real-scanned sparse data.
However, it is expensive and tedious to obtain large scale paired sparse-dense
point sets for training from real scanned sparse data. To address this problem,
we propose a self-supervised point cloud upsampling network, named SPU-Net, to
capture the inherent upsampling patterns of points lying on the underlying
object surface. Specifically, we propose a coarse-to-fine reconstruction
framework, which contains two main components: point feature extraction and
point feature expansion, respectively. In the point feature extraction, we
integrate self-attention module with graph convolution network (GCN) to
simultaneously capture context information inside and among local regions. In
the point feature expansion, we introduce a hierarchically learnable folding
strategy to generate the upsampled point sets with learnable 2D grids.
Moreover, to further optimize the noisy points in the generated point sets, we
propose a novel self-projection optimization associated with uniform and
reconstruction terms, as a joint loss, to facilitate the self-supervised point
cloud upsampling. We conduct various experiments on both synthetic and
real-scanned datasets, and the results demonstrate that we achieve comparable
performance to the state-of-the-art supervised methods.
</p>
<a href="http://arxiv.org/abs/2012.04439" target="_blank">arXiv:2012.04439</a> [<a href="http://arxiv.org/pdf/2012.04439" target="_blank">pdf</a>]

<h2>URoboSim -- An Episodic Simulation Framework for Prospective Reasoning in Robotic Agents. (arXiv:2012.04442v1 [cs.AI])</h2>
<h3>Michael Neumann, Sebastian Koralewski, Michael Beetz</h3>
<p>Anticipating what might happen as a result of an action is an essential
ability humans have in order to perform tasks effectively. On the other hand,
robots capabilities in this regard are quite lacking. While machine learning is
used to increase the ability of prospection it is still limiting for novel
situations. A possibility to improve the prospection ability of robots is
through simulation of imagined motions and the physical results of these
actions. Therefore, we present URoboSim, a robot simulator that allows robots
to perform tasks as mental simulation before performing this task in reality.
We show the capabilities of URoboSim in form of mental simulations, generating
data for machine learning and the usage as belief state for a real robot.
</p>
<a href="http://arxiv.org/abs/2012.04442" target="_blank">arXiv:2012.04442</a> [<a href="http://arxiv.org/pdf/2012.04442" target="_blank">pdf</a>]

<h2>Split: Inferring Unobserved Event Probabilities for Disentangling Brand-Customer Interactions. (arXiv:2012.04445v1 [cs.LG])</h2>
<h3>Ayush Chauhan, Aditya Anand, Shaddy Garg, Sunny Dhamnani, Shiv Kumar Saini</h3>
<p>Often, data contains only composite events composed of multiple events, some
observed and some unobserved. For example, search ad click is observed by a
brand, whereas which customers were shown a search ad - an actionable variable
- is often not observed. In such cases, inference is not possible on unobserved
event. This occurs when a marketing action is taken over earned and paid
digital channels. Similar setting arises in numerous datasets where multiple
actors interact. One approach is to use the composite event as a proxy for the
unobserved event of interest. However, this leads to invalid inference. This
paper takes a direct approach whereby an event of interest is identified based
on information on the composite event and aggregate data on composite events
(e.g. total number of search ads shown). This work contributes to the
literature by proving identification of the unobserved events' probabilities up
to a scalar factor under mild condition. We propose an approach to identify the
scalar factor by using aggregate data that is usually available from earned and
paid channels. The factor is identified by adding a loss term to the usual
cross-entropy loss. We validate the approach on three synthetic datasets. In
addition, the approach is validated on a real marketing problem where some
observed events are hidden from the algorithm for validation. The proposed
modification to the cross-entropy loss function improves the average
performance by 46%.
</p>
<a href="http://arxiv.org/abs/2012.04445" target="_blank">arXiv:2012.04445</a> [<a href="http://arxiv.org/pdf/2012.04445" target="_blank">pdf</a>]

<h2>Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization. (arXiv:2012.04456v1 [cs.LG])</h2>
<h3>Yingfan Wang, Haiyang Huang, Cynthia Rudin, Yaron Shaposhnik</h3>
<p>Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMAP have
demonstrated impressive visualization performance on many real world datasets.
One tension that has always faced these methods is the trade-off between
preservation of global structure and preservation of local structure: these
methods can either handle one or the other, but not both. In this work, our
main goal is to understand what aspects of DR methods are important for
preserving both local and global structure: it is difficult to design a better
method without a true understanding of the choices we make in our algorithms
and their empirical impact on the lower-dimensional embeddings they produce.
Towards the goal of local structure preservation, we provide several useful
design principles for DR loss functions based on our new understanding of the
mechanisms behind successful DR methods. Towards the goal of global structure
preservation, our analysis illuminates that the choice of which components to
preserve is important. We leverage these insights to design a new algorithm for
DR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP),
which preserves both local and global structure. Our work provides several
unexpected insights into what design choices both to make and avoid when
constructing DR algorithms.
</p>
<a href="http://arxiv.org/abs/2012.04456" target="_blank">arXiv:2012.04456</a> [<a href="http://arxiv.org/pdf/2012.04456" target="_blank">pdf</a>]

<h2>Combining reinforcement learning with lin-kernighan-helsgaun algorithm for the traveling salesman problem. (arXiv:2012.04461v1 [cs.AI])</h2>
<h3>Jiongzhi Zheng, Kun He, Jianrong Zhou, Yan Jin, Chu-min Li</h3>
<p>We address the Traveling Salesman Problem (TSP), a famous NP-hard
combinatorial optimization problem. And we propose a variable strategy
reinforced approach, denoted as VSR-LKH, which combines three reinforcement
learning methods (Q-learning, Sarsa and Monte Carlo) with the well-known TSP
algorithm Lin-Kernighan-Helsgaun (LKH). VSR-LKH replaces the inflexible
traversal operation in LKH, and lets the program learn to make choice at each
search step by reinforcement learning. Experimental results on 111 TSP
benchmarks from the TSPLIB with up to 85,900 cities demonstrate the excellent
performance of the proposed method.
</p>
<a href="http://arxiv.org/abs/2012.04461" target="_blank">arXiv:2012.04461</a> [<a href="http://arxiv.org/pdf/2012.04461" target="_blank">pdf</a>]

<h2>Multi-Objective Interpolation Training for Robustness to Label Noise. (arXiv:2012.04462v1 [cs.CV])</h2>
<h3>Diego Ortego, Eric Arazo, Paul Albert, Noel E. O&#x27;Connor, Kevin McGuinness</h3>
<p>Deep neural networks trained with standard cross-entropy loss memorize noisy
labels, which degrades their performance. Most research to mitigate this
memorization proposes new robust classification loss functions. Conversely, we
explore the behavior of supervised contrastive learning under label noise to
understand how it can improve image classification in these scenarios. In
particular, we propose a Multi-Objective Interpolation Training (MOIT) approach
that jointly exploits contrastive learning and classification. We show that
standard contrastive learning degrades in the presence of label noise and
propose an interpolation training strategy to mitigate this behavior. We
further propose a novel label noise detection method that exploits the robust
feature representations learned via contrastive learning to estimate per-sample
soft-labels whose disagreements with the original labels accurately identify
noisy samples. This detection allows treating noisy samples as unlabeled and
training a classifier in a semi-supervised manner. We further propose MOIT+, a
refinement of MOIT by fine-tuning on detected clean samples. Hyperparameter and
ablation studies verify the key components of our method. Experiments on
synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves
state-of-the-art results. Code is available at https://git.io/JI40X.
</p>
<a href="http://arxiv.org/abs/2012.04462" target="_blank">arXiv:2012.04462</a> [<a href="http://arxiv.org/pdf/2012.04462" target="_blank">pdf</a>]

<h2>Rotation-Invariant Autoencoders for Signals on Spheres. (arXiv:2012.04474v1 [cs.CV])</h2>
<h3>Suhas Lohit, Shubhendu Trivedi</h3>
<p>Omnidirectional images and spherical representations of $3D$ shapes cannot be
processed with conventional 2D convolutional neural networks (CNNs) as the
unwrapping leads to large distortion. Using fast implementations of spherical
and $SO(3)$ convolutions, researchers have recently developed deep learning
methods better suited for classifying spherical images. These newly proposed
convolutional layers naturally extend the notion of convolution to functions on
the unit sphere $S^2$ and the group of rotations $SO(3)$ and these layers are
equivariant to 3D rotations. In this paper, we consider the problem of
unsupervised learning of rotation-invariant representations for spherical
images. In particular, we carefully design an autoencoder architecture
consisting of $S^2$ and $SO(3)$ convolutional layers. As 3D rotations are often
a nuisance factor, the latent space is constrained to be exactly invariant to
these input transformations. As the rotation information is discarded in the
latent space, we craft a novel rotation-invariant loss function for training
the network. Extensive experiments on multiple datasets demonstrate the
usefulness of the learned representations on clustering, retrieval and
classification applications.
</p>
<a href="http://arxiv.org/abs/2012.04474" target="_blank">arXiv:2012.04474</a> [<a href="http://arxiv.org/pdf/2012.04474" target="_blank">pdf</a>]

<h2>Analyzing Finite Neural Networks: Can We Trust Neural Tangent Kernel Theory?. (arXiv:2012.04477v1 [cs.LG])</h2>
<h3>Mariia Seleznova, Gitta Kutyniok</h3>
<p>Neural Tangent Kernel (NTK) theory is widely used to study the dynamics of
infinitely-wide deep neural networks (DNNs) under gradient descent. But do the
results for infinitely-wide networks give us hints about the behaviour of real
finite-width ones? In this paper we study empirically when NTK theory is valid
in practice for fully-connected ReLu and sigmoid networks. We find out that
whether a network is in the NTK regime depends on the hyperparameters of random
initialization and network's depth. In particular, NTK theory does not explain
behaviour of sufficiently deep networks initialized so that their gradients
explode: the kernel is random at initialization and changes significantly
during training, contrary to NTK theory. On the other hand, in case of
vanishing gradients DNNs are in the NTK regime but become untrainable rapidly
with depth. We also describe a framework to study generalization properties of
DNNs by means of NTK theory and discuss its limits.
</p>
<a href="http://arxiv.org/abs/2012.04477" target="_blank">arXiv:2012.04477</a> [<a href="http://arxiv.org/pdf/2012.04477" target="_blank">pdf</a>]

<h2>Emotive Response to a Hybrid-Face Robot and Translation to Consumer Social Robots. (arXiv:2012.04511v1 [cs.RO])</h2>
<h3>Maitreyee Wairagkar, Maria R Lima, Daniel Bazo, Richard Craig, Hugo Weissbart, Appolinaire C Etoundi, Tobias Reichenbach, Prashant Iyenger, Sneh Vaswani, Christopher James, Payam Barnaghi, Chris Melhuish, Ravi Vaidyanathan</h3>
<p>We introduce the conceptual formulation, design, fabrication, control and
commercial translation with IoT connection of a hybrid-face social robot and
validation of human emotional response to its affective interactions. The
hybrid-face robot integrates a 3D printed faceplate and a digital display to
simplify conveyance of complex facial movements while providing the impression
of three-dimensional depth for natural interaction. We map the space of
potential emotions of the robot to specific facial feature parameters and
characterise the recognisability of the humanoid hybrid-face robot's archetypal
facial expressions. We introduce pupil dilation as an additional degree of
freedom for conveyance of emotive states. Human interaction experiments
demonstrate the ability to effectively convey emotion from the hybrid-robot
face to human observers by mapping their neurophysiological
electroencephalography (EEG) response to perceived emotional information and
through interviews. Results show main hybrid-face robotic expressions can be
discriminated with recognition rates above 80% and invoke human emotive
response similar to that of actual human faces as measured by the face-specific
N170 event-related potentials in EEG. The hybrid-face robot concept has been
modified, implemented, and released in the commercial IoT robotic platform Miko
(My Companion), an affective robot with facial and conversational features
currently in use for human-robot interaction in children by Emotix Inc. We
demonstrate that human EEG responses to Miko emotions are comparative to
neurophysiological responses for actual human facial recognition. Finally,
interviews show above 90% expression recognition rates in our commercial robot.
We conclude that simplified hybrid-face abstraction conveys emotions
effectively and enhances human-robot interaction.
</p>
<a href="http://arxiv.org/abs/2012.04511" target="_blank">arXiv:2012.04511</a> [<a href="http://arxiv.org/pdf/2012.04511" target="_blank">pdf</a>]

<h2>SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic Navigation. (arXiv:2012.04512v1 [cs.CV])</h2>
<h3>Yiqing Liang, Boyuan Chen, Shuran Song</h3>
<p>This paper focuses on visual semantic navigation, the task of producing
actions for an active agent to navigate to a specified target object category
in an unknown environment. To complete this task, the algorithm should
simultaneously locate and navigate to an instance of the category. In
comparison to the traditional point goal navigation, this task requires the
agent to have a stronger contextual prior of indoor environments. We introduce
SSCNav, an algorithm that explicitly models scene priors using a
confidence-aware semantic scene completion module to complete the scene and
guide the agent's navigation planning. Given a partial observation of the
environment, SSCNav first infers a complete scene representation with semantic
labels for the unobserved scene together with a confidence map associated with
its own prediction. Then, a policy network infers the action from the scene
completion result and confidence map. Our experiments demonstrate that the
proposed scene completion module improves the efficiency of the downstream
navigation policies. https://youtu.be/tfBbdGS72zg
</p>
<a href="http://arxiv.org/abs/2012.04512" target="_blank">arXiv:2012.04512</a> [<a href="http://arxiv.org/pdf/2012.04512" target="_blank">pdf</a>]

<h2>Human Motion Tracking by Registering an Articulated Surface to 3-D Points and Normals. (arXiv:2012.04514v1 [cs.CV])</h2>
<h3>Radu Horaud, Matti Niskanen, Guillaume Dewaele, Edmond Boyer</h3>
<p>We address the problem of human motion tracking by registering a surface to
3-D data. We propose a method that iteratively computes two things: Maximum
likelihood estimates for both the kinematic and free-motion parameters of a
kinematic human-body representation, as well as probabilities that the data are
assigned either to a body part, or to an outlier cluster. We introduce a new
metric between observed points and normals on one side, and a parameterized
surface on the other side, the latter being defined as a blending over a set of
ellipsoids. We claim that this metric is well suited when one deals with either
visual-hull or visual-shape observations. We illustrate the method by tracking
human motions using sparse visual-shape data (3-D surface points and normals)
gathered from imperfect silhouettes.
</p>
<a href="http://arxiv.org/abs/2012.04514" target="_blank">arXiv:2012.04514</a> [<a href="http://arxiv.org/pdf/2012.04514" target="_blank">pdf</a>]

<h2>Digital Gimbal: End-to-end Deep Image Stabilization with Learnable Exposure Times. (arXiv:2012.04515v1 [cs.CV])</h2>
<h3>Omer Dahary, Matan Jacoby, Alex M. Bronstein</h3>
<p>Mechanical image stabilization using actuated gimbals enables capturing
long-exposure shots without suffering from blur due to camera motion. These
devices, however, are often physically cumbersome and expensive, limiting their
widespread use. In this work, we propose to digitally emulate a mechanically
stabilized system from the input of a fast unstabilized camera. To exploit the
trade-off between motion blur at long exposures and low SNR at short exposures,
we train a CNN that estimates a sharp high-SNR image by aggregating a burst of
noisy short-exposure frames, related by unknown motion. We further suggest
learning the burst's exposure times in an end-to-end manner, thus balancing the
noise and blur across the frames. We demonstrate this method's advantage over
the traditional approach of deblurring a single image or denoising a
fixed-exposure burst.
</p>
<a href="http://arxiv.org/abs/2012.04515" target="_blank">arXiv:2012.04515</a> [<a href="http://arxiv.org/pdf/2012.04515" target="_blank">pdf</a>]

<h2>GMM-Based Generative Adversarial Encoder Learning. (arXiv:2012.04525v1 [cs.LG])</h2>
<h3>Yuri Feigin, Hedva Spitzer, Raja Giryes</h3>
<p>While GAN is a powerful model for generating images, its inability to infer a
latent space directly limits its use in applications requiring an encoder. Our
paper presents a simple architectural setup that combines the generative
capabilities of GAN with an encoder. We accomplish this by combining the
encoder with the discriminator using shared weights, then training them
simultaneously using a new loss term. We model the output of the encoder latent
space via a GMM, which leads to both good clustering using this latent space
and improved image generation by the GAN. Our framework is generic and can be
easily plugged into any GAN strategy. In particular, we demonstrate it both
with Vanilla GAN and Wasserstein GAN, where in both it leads to an improvement
in the generated images in terms of both the IS and FID scores. Moreover, we
show that our encoder learns a meaningful representation as its clustering
results are competitive with the current GAN-based state-of-the-art in
clustering.
</p>
<a href="http://arxiv.org/abs/2012.04525" target="_blank">arXiv:2012.04525</a> [<a href="http://arxiv.org/pdf/2012.04525" target="_blank">pdf</a>]

<h2>Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting. (arXiv:2012.04529v1 [cs.CV])</h2>
<h3>Lingbo Liu, Jiaqi Chen, Hefeng Wu, Guanbin Li, Chenglong Li, Liang Lin</h3>
<p>Crowd counting is a fundamental yet challenging problem, which desires rich
information to generate pixel-wise crowd density maps. However, most previous
methods only utilized the limited information of RGB images and may fail to
discover the potential pedestrians in unconstrained environments. In this work,
we find that incorporating optical and thermal information can greatly help to
recognize pedestrians. To promote future researches in this field, we introduce
a large-scale RGBT Crowd Counting (RGBT-CC) benchmark, which contains 2,030
pairs of RGB-thermal images with 138,389 annotated people. Furthermore, to
facilitate the multimodal crowd counting, we propose a cross-modal
collaborative representation learning framework, which consists of multiple
modality-specific branches, a modality-shared branch, and an Information
Aggregation-Distribution Module (IADM) to fully capture the complementary
information of different modalities. Specifically, our IADM incorporates two
collaborative information transfer components to dynamically enhance the
modality-shared and modality-specific representations with a dual information
propagation mechanism. Extensive experiments conducted on the RGBT-CC benchmark
demonstrate the effectiveness of our framework for RGBT crowd counting.
Moreover, the proposed approach is universal for multimodal crowd counting and
is also capable to achieve superior performance on the ShanghaiTechRGBD
dataset.
</p>
<a href="http://arxiv.org/abs/2012.04529" target="_blank">arXiv:2012.04529</a> [<a href="http://arxiv.org/pdf/2012.04529" target="_blank">pdf</a>]

<h2>Discovering key topics from short, real-world medical inquiries via natural language processing and unsupervised learning. (arXiv:2012.04545v1 [cs.LG])</h2>
<h3>Angelo Ziletti, Christoph Berns, Oliver Treichel, Thomas Weber, Jennifer Liang, Stephanie Kammerath, Marion Schwaerzler, Jagatheswari Virayah, David Ruau, Xin Ma, Andreas Mattern</h3>
<p>Millions of unsolicited medical inquiries are received by pharmaceutical
companies every year. It has been hypothesized that these inquiries represent a
treasure trove of information, potentially giving insight into matters
regarding medicinal products and the associated medical treatments. However,
due to the large volume and specialized nature of the inquiries, it is
difficult to perform timely, recurrent, and comprehensive analyses. Here, we
propose a machine learning approach based on natural language processing and
unsupervised learning to automatically discover key topics in real-world
medical inquiries from customers. This approach does not require ontologies nor
annotations. The discovered topics are meaningful and medically relevant, as
judged by medical information specialists, thus demonstrating that unsolicited
medical inquiries are a source of valuable customer insights. Our work paves
the way for the machine-learning-driven analysis of medical inquiries in the
pharmaceutical industry, which ultimately aims at improving patient care.
</p>
<a href="http://arxiv.org/abs/2012.04545" target="_blank">arXiv:2012.04545</a> [<a href="http://arxiv.org/pdf/2012.04545" target="_blank">pdf</a>]

<h2>In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. (arXiv:2012.04550v1 [cs.LG])</h2>
<h3>Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, Percy Liang</h3>
<p>Consider a prediction setting where a few inputs (e.g., satellite images) are
expensively annotated with the prediction targets (e.g., crop types), and many
inputs are cheaply annotated with auxiliary information (e.g., climate
information). How should we best leverage this auxiliary information for the
prediction task? Empirically across three image and time-series datasets, and
theoretically in a multi-task linear regression setting, we show that (i) using
auxiliary information as input features improves in-distribution error but can
hurt out-of-distribution (OOD) error; while (ii) using auxiliary information as
outputs of auxiliary tasks to pre-train a model improves OOD error. To get the
best of both worlds, we introduce In-N-Out, which first trains a model with
auxiliary inputs and uses it to pseudolabel all the in-distribution inputs,
then pre-trains a model on OOD auxiliary outputs and fine-tunes this model with
the pseudolabels (self-training). We show both theoretically and empirically
that In-N-Out outperforms auxiliary inputs or outputs alone on both
in-distribution and OOD error.
</p>
<a href="http://arxiv.org/abs/2012.04550" target="_blank">arXiv:2012.04550</a> [<a href="http://arxiv.org/pdf/2012.04550" target="_blank">pdf</a>]

<h2>CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing. (arXiv:2012.04551v1 [cs.CV])</h2>
<h3>Ajinkya Kadu, Tristan van Leeuwen, K. Joost Batenburg</h3>
<p>We introduce single-shot X-ray tomography that aims to estimate the target
image from a single cone-beam projection measurement. This linear inverse
problem is extremely under-determined since the measurements are far fewer than
the number of unknowns. Moreover, it is more challenging than conventional
tomography where a sufficiently large number of projection angles forms the
measurements, allowing for a simple inversion process. However, single-shot
tomography becomes less severe if the target image is only composed of known
shapes. Hence, the shape prior transforms a linear ill-posed image estimation
problem to a non-linear problem of estimating the roto-translations of the
shapes. In this paper, we circumvent the non-linearity by using a dictionary of
possible roto-translations of the shapes. We propose a convex program CoShaRP
to recover the dictionary-coefficients successfully. CoShaRP relies on
simplex-type constraint and can be solved quickly using a primal-dual
algorithm. The numerical experiments show that CoShaRP recovers shapes stably
from moderately noisy measurements.
</p>
<a href="http://arxiv.org/abs/2012.04551" target="_blank">arXiv:2012.04551</a> [<a href="http://arxiv.org/pdf/2012.04551" target="_blank">pdf</a>]

<h2>Bayesian Image Reconstruction using Deep Generative Models. (arXiv:2012.04567v1 [cs.CV])</h2>
<h3>Razvan V Marinescu, Daniel Moyer, Polina Golland</h3>
<p>Machine learning models are commonly trained end-to-end and in a supervised
setting, using paired (input, output) data. Classical examples include recent
super-resolution methods that train on pairs of (low-resolution,
high-resolution) images. However, these end-to-end approaches require
re-training every time there is a distribution shift in the inputs (e.g., night
images vs daylight) or relevant latent variables (e.g., camera blur or hand
motion). In this work, we leverage state-of-the-art (SOTA) generative models
(here StyleGAN2) for building powerful image priors, which enable application
of Bayes' theorem for many downstream reconstruction tasks. Our method, called
Bayesian Reconstruction through Generative Models (BRGM), uses a single
pre-trained generator model to solve different image restoration tasks, i.e.,
super-resolution and in-painting, by combining it with different forward
corruption models. We demonstrate BRGM on three large, yet diverse, datasets
that enable us to build powerful priors: (i) 60,000 images from the Flick Faces
High Quality dataset \cite{karras2019style} (ii) 240,000 chest X-rays from
MIMIC III and (iii) a combined collection of 5 brain MRI datasets with 7,329
scans. Across all three datasets and without any dataset-specific
hyperparameter tuning, our approach yields state-of-the-art performance on
super-resolution, particularly at low-resolution levels, as well as inpainting,
compared to state-of-the-art methods that are specific to each reconstruction
task. We will make our code and pre-trained models available online.
</p>
<a href="http://arxiv.org/abs/2012.04567" target="_blank">arXiv:2012.04567</a> [<a href="http://arxiv.org/pdf/2012.04567" target="_blank">pdf</a>]

<h2>Estimation of the Mean Function of Functional Data via Deep Neural Networks. (arXiv:2012.04573v1 [stat.ML])</h2>
<h3>Shuoyang Wang, Guanqun Cao, Zuofeng Shang</h3>
<p>In this work, we propose a deep neural network method to perform
nonparametric regression for functional data. The proposed estimators are based
on sparsely connected deep neural networks with ReLU activation function. By
properly choosing network architecture, our estimator achieves the optimal
nonparametric convergence rate in empirical norm. Under certain circumstances
such as trigonometric polynomial kernel and a sufficiently large sampling
frequency, the convergence rate is even faster than root-$n$ rate. Through
Monte Carlo simulation studies we examine the finite-sample performance of the
proposed method. Finally, the proposed method is applied to analyze positron
emission tomography images of patients with Alzheimer disease obtained from the
Alzheimer Disease Neuroimaging Initiative database.
</p>
<a href="http://arxiv.org/abs/2012.04573" target="_blank">arXiv:2012.04573</a> [<a href="http://arxiv.org/pdf/2012.04573" target="_blank">pdf</a>]

<h2>Convergence Rates for Multi-classs Logistic Regression Near Minimum. (arXiv:2012.04576v1 [cs.LG])</h2>
<h3>Dwight Nwaigwe, Marek Rychlik</h3>
<p>Training a neural network is typically done via variations of gradient
descent. If a minimum of the loss function exists and gradient descent is used
as the training method, we provide an expression that relates learning rate to
the rate of convergence to the minimum. We also discuss existence of a minimum.
</p>
<a href="http://arxiv.org/abs/2012.04576" target="_blank">arXiv:2012.04576</a> [<a href="http://arxiv.org/pdf/2012.04576" target="_blank">pdf</a>]

<h2>Synthetic Data: Opening the data floodgates to enable faster, more directed development of machine learning methods. (arXiv:2012.04580v1 [cs.LG])</h2>
<h3>James Jordon, Alan Wilson, Mihaela van der Schaar</h3>
<p>Many ground-breaking advancements in machine learning can be attributed to
the availability of a large volume of rich data. Unfortunately, many
large-scale datasets are highly sensitive, such as healthcare data, and are not
widely available to the machine learning community. Generating synthetic data
with privacy guarantees provides one such solution, allowing meaningful
research to be carried out "at scale" - by allowing the entirety of the machine
learning community to potentially accelerate progress within a given field. In
this article, we provide a high-level view of synthetic data: what it means,
how we might evaluate it and how we might use it.
</p>
<a href="http://arxiv.org/abs/2012.04580" target="_blank">arXiv:2012.04580</a> [<a href="http://arxiv.org/pdf/2012.04580" target="_blank">pdf</a>]

<h2>MERANet: Facial Micro-Expression Recognition using 3D Residual Attention Network. (arXiv:2012.04581v1 [cs.CV])</h2>
<h3>Viswanatha Reddy Gajjala, Sai Prasanna Teja Reddy, Snehasis Mukherjee, Shiv Ram Dubey</h3>
<p>We propose a facial micro-expression recognition model using 3D residual
attention network called MERANet. The proposed model takes advantage of
spatial-temporal attention and channel attention together, to learn deeper
fine-grained subtle features for classification of emotions. The proposed model
also encompasses both spatial and temporal information simultaneously using the
3D kernels and residual connections. Moreover, the channel features and
spatio-temporal features are re-calibrated using the channel and
spatio-temporal attentions, respectively in each residual module. The
experiments are conducted on benchmark facial micro-expression datasets. A
superior performance is observed as compared to the state-of-the-art for facial
micro-expression recognition.
</p>
<a href="http://arxiv.org/abs/2012.04581" target="_blank">arXiv:2012.04581</a> [<a href="http://arxiv.org/pdf/2012.04581" target="_blank">pdf</a>]

<h2>Social Media Unrest Prediction during the {COVID}-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises. (arXiv:2012.04586v1 [stat.ML])</h2>
<h3>Dirk Johann&#xdf;en, Chris Biemann</h3>
<p>The COVID-19 pandemic has caused international social tension and unrest.
Besides the crisis itself, there are growing signs of rising conflict potential
of societies around the world. Indicators of global mood changes are hard to
detect and direct questionnaires suffer from social desirability biases.
However, so-called implicit methods can reveal humans intrinsic desires from
e.g. social media texts. We present psychologically validated social unrest
predictors and replicate scalable and automated predictions, setting a new
state of the art on a recent German shared task dataset. We employ this model
to investigate a change of language towards social unrest during the COVID-19
pandemic by comparing established psychological predictors on samples of tweets
from spring 2019 with spring 2020. The results show a significant increase of
the conflict indicating psychometrics. With this work, we demonstrate the
applicability of automated NLP-based approaches to quantitative psychological
research.
</p>
<a href="http://arxiv.org/abs/2012.04586" target="_blank">arXiv:2012.04586</a> [<a href="http://arxiv.org/pdf/2012.04586" target="_blank">pdf</a>]

<h2>Models, Pixels, and Rewards: Evaluating Design Trade-offs in Visual Model-Based Reinforcement Learning. (arXiv:2012.04603v1 [cs.LG])</h2>
<h3>Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn, Sergey Levine, Dumitru Erhan</h3>
<p>Model-based reinforcement learning (MBRL) methods have shown strong sample
efficiency and performance across a variety of tasks, including when faced with
high-dimensional visual observations. These methods learn to predict the
environment dynamics and expected reward from interaction and use this
predictive model to plan and perform the task. However, MBRL methods vary in
their fundamental design choices, and there is no strong consensus in the
literature on how these design decisions affect performance. In this paper, we
study a number of design decisions for the predictive model in visual MBRL
algorithms, focusing specifically on methods that use a predictive model for
planning. We find that a range of design decisions that are often considered
crucial, such as the use of latent spaces, have little effect on task
performance. A big exception to this finding is that predicting future
observations (i.e., images) leads to significant task performance improvement
compared to only predicting rewards. We also empirically find that image
prediction accuracy, somewhat surprisingly, correlates more strongly with
downstream task performance than reward prediction accuracy. We show how this
phenomenon is related to exploration and how some of the lower-scoring models
on standard benchmarks (that require exploration) will perform the same as the
best-performing models when trained on the same training data. Simultaneously,
in the absence of exploration, models that fit the data better usually perform
better on the downstream task as well, but surprisingly, these are often not
the same models that perform the best when learning and exploring from scratch.
These findings suggest that performance and exploration place important and
potentially contradictory requirements on the model.
</p>
<a href="http://arxiv.org/abs/2012.04603" target="_blank">arXiv:2012.04603</a> [<a href="http://arxiv.org/pdf/2012.04603" target="_blank">pdf</a>]

<h2>Minimax Regret Optimisation for Robust Planning in Uncertain Markov Decision Processes. (arXiv:2012.04626v1 [cs.AI])</h2>
<h3>Marc Rigter, Bruno Lacerda, Nick Hawes</h3>
<p>The parameters for a Markov Decision Process (MDP) often cannot be specified
exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets
which the parameters belong to. Minimax regret has been proposed as an
objective for planning in UMDPs to find robust policies which are not overly
conservative. In this work, we focus on planning for Stochastic Shortest Path
(SSP) UMDPs with uncertain cost and transition functions. We introduce a
Bellman equation to compute the regret for a policy. We propose a dynamic
programming algorithm that utilises the regret Bellman equation, and show that
it optimises minimax regret exactly for UMDPs with independent uncertainties.
For coupled uncertainties, we extend our approach to use options to enable a
trade off between computation and solution quality. We evaluate our approach on
both synthetic and real-world domains, showing that it significantly
outperforms existing baselines.
</p>
<a href="http://arxiv.org/abs/2012.04626" target="_blank">arXiv:2012.04626</a> [<a href="http://arxiv.org/pdf/2012.04626" target="_blank">pdf</a>]

<h2>CASTing Your Model: Learning to Localize Improves Self-Supervised Representations. (arXiv:2012.04630v1 [cs.CV])</h2>
<h3>Ramprasaath R. Selvaraju, Karan Desai, Justin Johnson, Nikhil Naik</h3>
<p>Recent advances in self-supervised learning (SSL) have largely closed the gap
with supervised ImageNet pretraining. Despite their success these methods have
been primarily applied to unlabeled ImageNet images, and show marginal gains
when trained on larger sets of uncurated images. We hypothesize that current
SSL methods perform best on iconic images, and struggle on complex scene images
with many objects. Analyzing contrastive SSL methods shows that they have poor
visual grounding and receive poor supervisory signal when trained on scene
images. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome
these limitations. CAST uses unsupervised saliency maps to intelligently sample
crops, and to provide grounding supervision via a Grad-CAM attention loss.
Experiments on COCO show that CAST significantly improves the features learned
by SSL methods on scene images, and further experiments show that CAST-trained
models are more robust to changes in backgrounds.
</p>
<a href="http://arxiv.org/abs/2012.04630" target="_blank">arXiv:2012.04630</a> [<a href="http://arxiv.org/pdf/2012.04630" target="_blank">pdf</a>]

<h2>Mutual Information Decay Curves and Hyper-Parameter Grid Search Design for Recurrent Neural Architectures. (arXiv:2012.04632v1 [cs.LG])</h2>
<h3>Abhijit Mahalunkar, John D. Kelleher</h3>
<p>We present an approach to design the grid searches for hyper-parameter
optimization for recurrent neural architectures. The basis for this approach is
the use of mutual information to analyze long distance dependencies (LDDs)
within a dataset. We also report a set of experiments that demonstrate how
using this approach, we obtain state-of-the-art results for DilatedRNNs across
a range of benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.04632" target="_blank">arXiv:2012.04632</a> [<a href="http://arxiv.org/pdf/2012.04632" target="_blank">pdf</a>]

<h2>Accurate 3D Object Detection using Energy-Based Models. (arXiv:2012.04634v1 [cs.CV])</h2>
<h3>Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Sch&#xf6;n</h3>
<p>Accurate 3D object detection (3DOD) is crucial for safe navigation of complex
environments by autonomous robots. Regressing accurate 3D bounding boxes in
cluttered environments based on sparse LiDAR data is however a highly
challenging problem. We address this task by exploring recent advances in
conditional energy-based models (EBMs) for probabilistic regression. While
methods employing EBMs for regression have demonstrated impressive performance
on 2D object detection in images, these techniques are not directly applicable
to 3D bounding boxes. In this work, we therefore design a differentiable
pooling operator for 3D bounding boxes, serving as the core module of our EBM
network. We further integrate this general approach into the state-of-the-art
3D object detector SA-SSD. On the KITTI dataset, our proposed approach
consistently outperforms the SA-SSD baseline across all 3DOD metrics,
demonstrating the potential of EBM-based regression for highly accurate 3DOD.
Code is available at https://github.com/fregu856/ebms_3dod.
</p>
<a href="http://arxiv.org/abs/2012.04634" target="_blank">arXiv:2012.04634</a> [<a href="http://arxiv.org/pdf/2012.04634" target="_blank">pdf</a>]

<h2>TAP: Text-Aware Pre-training for Text-VQA and Text-Caption. (arXiv:2012.04638v1 [cs.CV])</h2>
<h3>Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo</h3>
<p>In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and
Text-Caption tasks. These two tasks aim at reading and understanding scene text
in images for question answering and image caption generation, respectively. In
contrast to the conventional vision-language pre-training that fails to capture
scene text and its relationship with the visual and text modalities, TAP
explicitly incorporates scene text (generated from OCR engines) in
pre-training. With three pre-training tasks, including masked language modeling
(MLM), image-text (contrastive) matching (ITM), and relative (spatial) position
prediction (RPP), TAP effectively helps the model learn a better aligned
representation among the three modalities: text word, visual object, and scene
text. Due to this aligned representation learning, even pre-trained on the same
downstream task dataset, TAP already boosts the absolute accuracy on the
TextVQA dataset by +5.4%, compared with a non-TAP baseline. To further improve
the performance, we build a large-scale dataset based on the Conceptual Caption
dataset, named OCR-CC, which contains 1.4 million scene text-related image-text
pairs. Pre-trained on this OCR-CC dataset, our approach outperforms the state
of the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA,
+8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.
</p>
<a href="http://arxiv.org/abs/2012.04638" target="_blank">arXiv:2012.04638</a> [<a href="http://arxiv.org/pdf/2012.04638" target="_blank">pdf</a>]

<h2>Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos. (arXiv:2012.04641v1 [cs.CV])</h2>
<h3>Kevis-Kokitsi Maninis, Stefan Popov, Matthias Nie&#xdf;ner, Vittorio Ferrari</h3>
<p>We address the task of aligning CAD models to a video sequence of a complex
scene containing multiple objects. Our method is able to process arbitrary
videos and fully automatically recover the 9 DoF pose for each object appearing
in it, thus aligning them in a common 3D coordinate frame. The core idea of our
method is to integrate neural network predictions from individual frames with a
temporally global, multi-view constraint optimization formulation. This
integration process resolves the scale and depth ambiguities in the per-frame
predictions, and generally improves the estimate of all pose parameters. By
leveraging multi-view constraints, our method also resolves occlusions and
handles objects that are out of view in individual frames, thus reconstructing
all objects into a single globally consistent CAD representation of the scene.
In comparison to the state-of-the-art single-frame method Mask2CAD that we
build on, we achieve substantial improvements on Scan2CAD (from 11.6% to 30.2%
class average accuracy).
</p>
<a href="http://arxiv.org/abs/2012.04641" target="_blank">arXiv:2012.04641</a> [<a href="http://arxiv.org/pdf/2012.04641" target="_blank">pdf</a>]

<h2>The Lottery Ticket Hypothesis for Object Recognition. (arXiv:2012.04643v1 [cs.CV])</h2>
<h3>Sharath Girish, Shishira R. Maiya, Kamal Gupta, Hao Chen, Larry Davis, Abhinav Shrivastava</h3>
<p>Recognition tasks, such as object recognition and keypoint estimation, have
seen widespread adoption in recent years. Most state-of-the-art methods for
these tasks use deep networks that are computationally expensive and have huge
memory footprints. This makes it exceedingly difficult to deploy these systems
on low power embedded devices. Hence, the importance of decreasing the storage
requirements and the amount of computation in such models is paramount. The
recently proposed Lottery Ticket Hypothesis (LTH) states that deep neural
networks trained on large datasets contain smaller subnetworks that achieve on
par performance as the dense networks. In this work, we perform the first
empirical study investigating LTH for model pruning in the context of object
detection, instance segmentation, and keypoint estimation. Our studies reveal
that lottery tickets obtained from ImageNet pretraining do not transfer well to
the downstream tasks. We provide guidance on how to find lottery tickets with
up to 80% overall sparsity on different sub-tasks without incurring any drop in
the performance. Finally, we analyse the behavior of trained tickets with
respect to various task attributes such as object size, frequency, and
difficulty of detection.
</p>
<a href="http://arxiv.org/abs/2012.04643" target="_blank">arXiv:2012.04643</a> [<a href="http://arxiv.org/pdf/2012.04643" target="_blank">pdf</a>]

<h2>Semantic Image Synthesis via Efficient Class-Adaptive Normalization. (arXiv:2012.04644v1 [cs.CV])</h2>
<h3>Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming He, Lu Yuan, Gang Hua, Nenghai Yu</h3>
<p>Spatially-adaptive normalization (SPADE) is remarkably successful recently in
conditional semantic image synthesis, which modulates the normalized activation
with spatially-varying transformations learned from semantic layouts, to
prevent the semantic information from being washed away. Despite its impressive
performance, a more thorough understanding of the advantages inside the box is
still highly demanded to help reduce the significant computation and parameter
overhead introduced by this novel structure. In this paper, from a
return-on-investment point of view, we conduct an in-depth analysis of the
effectiveness of this spatially-adaptive normalization and observe that its
modulation parameters benefit more from semantic-awareness rather than
spatial-adaptiveness, especially for high-resolution input masks. Inspired by
this observation, we propose class-adaptive normalization (CLADE), a
lightweight but equally-effective variant that is only adaptive to semantic
class. In order to further improve spatial-adaptiveness, we introduce
intra-class positional map encoding calculated from semantic layouts to
modulate the normalization parameters of CLADE and propose a truly
spatially-adaptive variant of CLADE, namely CLADE-ICPE. %Benefiting from this
design, CLADE greatly reduces the computation cost while being able to preserve
the semantic information in the generation. Through extensive experiments on
multiple challenging datasets, we demonstrate that the proposed CLADE can be
generalized to different SPADE-based methods while achieving comparable
generation quality compared to SPADE, but it is much more efficient with fewer
extra parameters and lower computational cost. The code is available at
https://github.com/tzt101/CLADE.git
</p>
<a href="http://arxiv.org/abs/2012.04644" target="_blank">arXiv:2012.04644</a> [<a href="http://arxiv.org/pdf/2012.04644" target="_blank">pdf</a>]

<h2>Logical Induction. (arXiv:1609.03543v5 [cs.AI] UPDATED)</h2>
<h3>Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, Jessica Taylor</h3>
<p>We present a computable algorithm that assigns probabilities to every logical
statement in a given formal language, and refines those probabilities over
time. For instance, if the language is Peano arithmetic, it assigns
probabilities to all arithmetical statements, including claims about the twin
prime conjecture, the outputs of long-running computations, and its own
probabilities. We show that our algorithm, an instance of what we call a
logical inductor, satisfies a number of intuitive desiderata, including: (1) it
learns to predict patterns of truth and falsehood in logical statements, often
long before having the resources to evaluate the statements, so long as the
patterns can be written down in polynomial time; (2) it learns to use
appropriate statistical summaries to predict sequences of statements whose
truth values appear pseudorandom; and (3) it learns to have accurate beliefs
about its own current beliefs, in a manner that avoids the standard paradoxes
of self-reference. For example, if a given computer program only ever produces
outputs in a certain range, a logical inductor learns this fact in a timely
manner; and if late digits in the decimal expansion of $\pi$ are difficult to
predict, then a logical inductor learns to assign $\approx 10\%$ probability to
"the $n$th digit of $\pi$ is a 7" for large $n$. Logical inductors also learn
to trust their future beliefs more than their current beliefs, and their
beliefs are coherent in the limit (whenever $\phi \implies \psi$,
$\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical
inductors strictly dominate the universal semimeasure in the limit.

These properties and many others all follow from a single logical induction
criterion, which is motivated by a series of stock trading analogies. Roughly
speaking, each logical sentence $\phi$ is associated with a stock that is worth
\$1 per share if [...]
</p>
<a href="http://arxiv.org/abs/1609.03543" target="_blank">arXiv:1609.03543</a> [<a href="http://arxiv.org/pdf/1609.03543" target="_blank">pdf</a>]

<h2>How To Solve Moral Conundrums with Computability Theory. (arXiv:1805.08347v2 [cs.AI] UPDATED)</h2>
<h3>Min Baek</h3>
<p>Various moral conundrums plague population ethics: the Non-Identity Problem,
the Procreation Asymmetry, the Repugnant Conclusion, and more. I argue that the
aforementioned moral conundrums have a structure neatly accounted for, and
solved by, some ideas in computability theory. I introduce a mathematical model
based on computability theory and show how previous arguments pertaining to
these conundrums fit into the model. This paper proceeds as follows. First, I
do a very brief survey of the history of computability theory in moral
philosophy. Second, I follow various papers, and show how their arguments fit
into, or don't fit into, our model. Third, I discuss the implications of our
model to the question why the human race should or should not continue to
exist. Finally, I show that our model may be interpreted according to a
Confucian-Taoist moral principle.
</p>
<a href="http://arxiv.org/abs/1805.08347" target="_blank">arXiv:1805.08347</a> [<a href="http://arxiv.org/pdf/1805.08347" target="_blank">pdf</a>]

<h2>Memory-like Map Decay for Autonomous Vehicles based on Grid Maps. (arXiv:1810.02355v3 [cs.AI] UPDATED)</h2>
<h3>Thomas Teixeira, Filipe Mutz, Karin Satie Komati, Lucas Veronese, Vinicius B. Cardoso, Claudine Badue, Thiago Oliveira-Santos, Alberto F. De Souza</h3>
<p>In this work, we present a novel strategy for correcting imperfections in
occupancy grid maps called map decay. The objective of map decay is to correct
invalid occupancy probabilities of map cells that are unobservable by sensors.
The strategy was inspired by an analogy between the memory architecture
believed to exist in the human brain and the maps maintained by an autonomous
vehicle. It consists in merging sensory information obtained during runtime
(online) with a priori data from a high-precision map constructed offline. In
map decay, cells observed by sensors are updated using traditional occupancy
grid mapping techniques and unobserved cells are adjusted so that their
occupancy probabilities tend to the values found in the offline map. This
strategy is grounded in the idea that the most precise information available
about an unobservable cell is the value found in the high-precision offline
map. Map decay was successfully tested and is still in use in the IARA
autonomous vehicle from Universidade Federal do Esp\'irito Santo.
</p>
<a href="http://arxiv.org/abs/1810.02355" target="_blank">arXiv:1810.02355</a> [<a href="http://arxiv.org/pdf/1810.02355" target="_blank">pdf</a>]

<h2>Understanding Recurrent Neural Architectures by Analyzing and Synthesizing Long Distance Dependencies in Benchmark Sequential Datasets. (arXiv:1810.02966v4 [cs.LG] UPDATED)</h2>
<h3>Abhijit Mahalunkar, John D. Kelleher</h3>
<p>In order to build efficient deep recurrent neural architectures, it is
essential to analyze the complexityof long distance dependencies (LDDs) of the
dataset being modeled. In this paper, we presentdetailed analysis of the
dependency decay curve exhibited by various datasets. The datasets sampledfrom
a similar process (e.g. natural language, sequential MNIST, Strictlyk-Piecewise
languages,etc) display variations in the properties of the dependency decay
curve. Our analysis reveal thefactors resulting in these variations; such as
(i) number of unique symbols in a dataset, (ii) size ofthe dataset, (iii)
number of interacting symbols within a given LDD, and (iv) the distance
betweenthe interacting symbols. We test these factors by generating synthesized
datasets of the Strictlyk-Piecewise languages. Another advantage of these
synthesized datasets is that they enable targetedtesting of deep recurrent
neural architectures in terms of their ability to model LDDs with
differentcharacteristics. We also demonstrate that analysing dependency decay
curves can inform the selectionof optimal hyper-parameters for SOTA deep
recurrent neural architectures. This analysis can directlycontribute to the
development of more accurate and efficient sequential models.
</p>
<a href="http://arxiv.org/abs/1810.02966" target="_blank">arXiv:1810.02966</a> [<a href="http://arxiv.org/pdf/1810.02966" target="_blank">pdf</a>]

<h2>Learning Choice Functions: Concepts and Architectures. (arXiv:1901.10860v3 [cs.LG] UPDATED)</h2>
<h3>Karlson Pfannschmidt, Pritha Gupta, Eyke H&#xfc;llermeier</h3>
<p>We study the problem of learning choice functions, which play an important
role in various domains of application, most notably in the field of economics.
Formally, a choice function is a mapping from sets to sets: Given a set of
choice alternatives as input, a choice function identifies a subset of most
preferred elements. Learning choice functions from suitable training data comes
with a number of challenges. For example, the sets provided as input and the
subsets produced as output can be of any size. Moreover, since the order in
which alternatives are presented is irrelevant, a choice function should be
symmetric. Perhaps most importantly, choice functions are naturally
context-dependent, in the sense that the preference in favor of an alternative
may depend on what other options are available. We formalize the problem of
learning choice functions and present two general approaches based on two
representations of context-dependent utility functions. Both approaches are
instantiated by means of appropriate neural network architectures, and their
performance is demonstrated on suitable benchmark tasks.
</p>
<a href="http://arxiv.org/abs/1901.10860" target="_blank">arXiv:1901.10860</a> [<a href="http://arxiv.org/pdf/1901.10860" target="_blank">pdf</a>]

<h2>Efficient Change-Point Detection for Tackling Piecewise-Stationary Bandits. (arXiv:1902.01575v2 [stat.ML] UPDATED)</h2>
<h3>Lilian Besson (IRISA), Emilie Kaufmann (CNRS, CRIStAL, Scool), Odalric-Ambrym Maillard (Scool), Julien Seznec (Scool)</h3>
<p>We introduce GLR-klUCB, a novel algorithm for the piecewise iid
non-stationary bandit problem with bounded rewards. This algorithm combines an
efficient bandit algorithm, kl-UCB, with an efficient, parameter-free,
changepoint detector, the Bernoulli Generalized Likelihood Ratio Test, for
which we provide new theoretical guarantees of independent interest. Unlike
previous non-stationary bandit algorithms using a change-point detector,
GLR-klUCB does not need to be calibrated based on prior knowledge on the arms'
means. We prove that this algorithm can attain a $O(\sqrt{TA
\Upsilon_T\log(T)})$ regret in $T$ rounds on some "easy" instances, where A is
the number of arms and $\Upsilon_T$ the number of change-points, without prior
knowledge of $\Upsilon_T$. In contrast with recently proposed algorithms that
are agnostic to $\Upsilon_T$, we perform a numerical study showing that
GLR-klUCB is also very efficient in practice, beyond easy instances.
</p>
<a href="http://arxiv.org/abs/1902.01575" target="_blank">arXiv:1902.01575</a> [<a href="http://arxiv.org/pdf/1902.01575" target="_blank">pdf</a>]

<h2>Machine Learning Based Analysis of Finnish World War II Photographers. (arXiv:1904.09811v4 [cs.CV] UPDATED)</h2>
<h3>Kateryna Chumachenko, Anssi M&#xe4;nnist&#xf6;, Alexandros Iosifidis, Jenni Raitoharju</h3>
<p>In this paper, we demonstrate the benefits of using state-of-the-art machine
learning methods in the analysis of historical photo archives. Specifically, we
analyze prominent Finnish World War II photographers, who have captured high
numbers of photographs in the publicly available Finnish Wartime Photograph
Archive, which contains 160,000 photographs from Finnish Winter, Continuation,
and Lapland Wars captures in 1939-1945. We were able to find some special
characteristics for different photographers in terms of their typical photo
content and framing (e.g., close-ups vs. overall shots, number of people).
Furthermore, we managed to train a neural network that can successfully
recognize the photographer from some of the photos, which shows that such
photos are indeed characteristic for certain photographers. We further analyzed
the similarities and differences between the photographers using the features
extracted from the photographer classifier network. We make our annotations and
analysis pipeline publicly available, in an effort to introduce this new
research problem to the machine learning and computer vision communities and
facilitate future research in historical and societal studies over the photo
archives.
</p>
<a href="http://arxiv.org/abs/1904.09811" target="_blank">arXiv:1904.09811</a> [<a href="http://arxiv.org/pdf/1904.09811" target="_blank">pdf</a>]

<h2>Budget-Aware Adapters for Multi-Domain Learning. (arXiv:1905.06242v3 [cs.CV] UPDATED)</h2>
<h3>Rodrigo Berriel, St&#xe9;phane Lathuili&#xe8;re, Moin Nabi, Tassilo Klein, Thiago Oliveira-Santos, Nicu Sebe, Elisa Ricci</h3>
<p>Multi-Domain Learning (MDL) refers to the problem of learning a set of models
derived from a common deep architecture, each one specialized to perform a task
in a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL
with a particular interest in obtaining domain-specific models with an
adjustable budget in terms of the number of network parameters and
computational complexity. Our intuition is that, as in real applications the
number of domains and tasks can be very large, an effective MDL approach should
not only focus on accuracy but also on having as few parameters as possible. To
implement this idea we derive specialized deep models for each domain by
adapting a pre-trained architecture but, differently from other methods, we
propose a novel strategy to automatically adjust the computational complexity
of the network. To this aim, we introduce Budget-Aware Adapters that select the
most relevant feature channels to better handle data from a novel domain. Some
constraints on the number of active switches are imposed in order to obtain a
network respecting the desired complexity budget. Experimentally, we show that
our approach leads to recognition accuracy competitive with state-of-the-art
approaches but with much lighter networks both in terms of storage and
computation.
</p>
<a href="http://arxiv.org/abs/1905.06242" target="_blank">arXiv:1905.06242</a> [<a href="http://arxiv.org/pdf/1905.06242" target="_blank">pdf</a>]

<h2>R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object. (arXiv:1908.05612v6 [cs.CV] UPDATED)</h2>
<h3>Xue Yang, Junchi Yan, Ziming Feng, Tao He</h3>
<p>Rotation detection is a challenging task due to the difficulties of locating
the multi-angle objects and separating them effectively from the background.
Though considerable progress has been made, for practical settings, there still
exist challenges for rotating objects with large aspect ratio, dense
distribution and category extremely imbalance. In this paper, we propose an
end-to-end refined single-stage rotation detector for fast and accurate object
detection by using a progressive regression approach from coarse to fine
granularity. Considering the shortcoming of feature misalignment in existing
refined single-stage detector, we design a feature refinement module to improve
detection performance by getting more accurate features. The key idea of
feature refinement module is to re-encode the position information of the
current refined bounding box to the corresponding feature points through
pixel-wise feature interpolation to realize feature reconstruction and
alignment. For more accurate rotation estimation, an approximate SkewIoU loss
is proposed to solve the problem that the calculation of SkewIoU is not
derivable. Experiments on three popular remote sensing public datasets DOTA,
HRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the
effectiveness of our approach. Tensorflow and Pytorch version codes are
available at https://github.com/Thinklab-SJTU/R3Det_Tensorflow and
https://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection, and R3Det is also
integrated in our open source rotation detection benchmark:
https://github.com/yangxue0827/RotationDetection.
</p>
<a href="http://arxiv.org/abs/1908.05612" target="_blank">arXiv:1908.05612</a> [<a href="http://arxiv.org/pdf/1908.05612" target="_blank">pdf</a>]

<h2>Partially Observable Markov Decision Process Modelling for Assessing Hierarchies. (arXiv:1908.07031v7 [stat.ML] UPDATED)</h2>
<h3>Weipeng Huang, Guangyuan Piao, Raul Moreno, Neil J. Hurley</h3>
<p>Hierarchical clustering has been shown to be valuable in many scenarios.
Despite its usefulness to many situations, there is no agreed methodology on
how to properly evaluate the hierarchies produced from different techniques,
particularly in the case where ground-truth labels are unavailable. This
motivates us to propose a framework for assessing the quality of hierarchical
clustering allocations which covers the case of no ground-truth information.
This measurement is useful, e.g., to assess the hierarchical structures used by
online retailer websites to display their product catalogues. Our framework is
one of the few attempts for the hierarchy evaluation from a decision-theoretic
perspective. We model the process as a bot searching stochastically for items
in the hierarchy and establish a measure representing the degree to which the
hierarchy supports this search. We employ Partially Observable Markov Decision
Processes (POMDP) to model the uncertainty, the decision making, and the
cognitive return for searchers in such a scenario.
</p>
<a href="http://arxiv.org/abs/1908.07031" target="_blank">arXiv:1908.07031</a> [<a href="http://arxiv.org/pdf/1908.07031" target="_blank">pdf</a>]

<h2>Neural Image Compression and Explanation. (arXiv:1908.08988v2 [cs.CV] UPDATED)</h2>
<h3>Xiang Li, Shihao Ji</h3>
<p>Explaining the prediction of deep neural networks (DNNs) and semantic image
compression are two active research areas of deep learning with a numerous of
applications in decision-critical systems, such as surveillance cameras, drones
and self-driving cars, where interpretable decision is critical and
storage/network bandwidth is limited. In this paper, we propose a novel
end-to-end Neural Image Compression and Explanation (NICE) framework that
learns to (1) explain the predictions of convolutional neural networks (CNNs),
and (2) subsequently compress the input images for efficient storage or
transmission. Specifically, NICE generates a sparse mask over an input image by
attaching a stochastic binary gate to each pixel of the image, whose parameters
are learned through the interaction with the CNN classifier to be explained.
The generated mask is able to capture the saliency of each pixel measured by
its influence to the final prediction of CNN; it can also be used to produce a
mixed-resolution image, where important pixels maintain their original high
resolution and insignificant background pixels are subsampled to a low
resolution. The produced images achieve a high compression rate (e.g., about
0.6x of original image file size), while retaining a similar classification
accuracy. Extensive experiments across multiple image classification benchmarks
demonstrate the superior performance of NICE compared to the state-of-the-art
methods in terms of explanation quality and semantic image compression rate.
Our code is available at: https://github.com/lxuniverse/NICE.
</p>
<a href="http://arxiv.org/abs/1908.08988" target="_blank">arXiv:1908.08988</a> [<a href="http://arxiv.org/pdf/1908.08988" target="_blank">pdf</a>]

<h2>BSP-Net: Generating Compact Meshes via Binary Space Partitioning. (arXiv:1911.06971v6 [cs.CV] UPDATED)</h2>
<h3>Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang</h3>
<p>Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only
played a minor role in the deep learning revolution. Leading methods for
learning generative models of shapes rely on implicit functions, and generate
meshes only after expensive iso-surfacing routines. To overcome these
challenges, we are inspired by a classical spatial data structure from computer
graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core
ingredient of BSP is an operation for recursive subdivision of space to obtain
convex sets. By exploiting this property, we devise BSP-Net, a network that
learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net
is unsupervised since no convex shape decompositions are needed for training.
The network is trained to reconstruct a shape using a set of convexes obtained
from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can
be easily extracted to form a polygon mesh, without any need for iso-surfacing.
The generated meshes are compact (i.e., low-poly) and well suited to represent
sharp geometry; they are guaranteed to be watertight and can be easily
parameterized. We also show that the reconstruction quality by BSP-Net is
competitive with state-of-the-art methods while using much fewer primitives.
Code is available at https://github.com/czq142857/BSP-NET-original.
</p>
<a href="http://arxiv.org/abs/1911.06971" target="_blank">arXiv:1911.06971</a> [<a href="http://arxiv.org/pdf/1911.06971" target="_blank">pdf</a>]

<h2>Implicit Regularization and Convergence for Weight Normalization. (arXiv:1911.07956v4 [cs.LG] UPDATED)</h2>
<h3>Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya Gunasekar, Rachel Ward, Qiang Liu</h3>
<p>Normalization methods such as batch [Ioffe and Szegedy, 2015], weight
[Salimansand Kingma, 2016], instance [Ulyanov et al., 2016], and layer
normalization [Baet al., 2016] have been widely used in modern machine
learning. Here, we study the weight normalization (WN) method [Salimans and
Kingma, 2016] and a variant called reparametrized projected gradient descent
(rPGD) for overparametrized least-squares regression. WN and rPGD reparametrize
the weights with a scale g and a unit vector w and thus the objective function
becomes non-convex. We show that this non-convex formulation has beneficial
regularization effects compared to gradient descent on the original objective.
These methods adaptively regularize the weights and converge close to the
minimum l2 norm solution, even for initializations far from zero. For certain
stepsizes of g and w , we show that they can converge close to the minimum norm
solution. This is different from the behavior of gradient descent, which
converges to the minimum norm solution only when started at a point in the
range space of the feature matrix, and is thus more sensitive to
initialization.
</p>
<a href="http://arxiv.org/abs/1911.07956" target="_blank">arXiv:1911.07956</a> [<a href="http://arxiv.org/pdf/1911.07956" target="_blank">pdf</a>]

<h2>Low Rank Approximation for Smoothing Spline via Eigensystem Truncation. (arXiv:1911.10434v2 [stat.ML] UPDATED)</h2>
<h3>Danqing Xu, Yuedong Wang</h3>
<p>Smoothing splines provide a powerful and flexible means for nonparametric
estimation and inference. With a cubic time complexity, fitting smoothing
spline models to large data is computationally prohibitive. In this paper, we
use the theoretical optimal eigenspace to derive a low rank approximation of
the smoothing spline estimates. We develop a method to approximate the
eigensystem when it is unknown and derive error bounds for the approximate
estimates. The proposed methods are easy to implement with existing software.
Extensive simulations show that the new methods are accurate, fast, and
compares favorably against existing methods.
</p>
<a href="http://arxiv.org/abs/1911.10434" target="_blank">arXiv:1911.10434</a> [<a href="http://arxiv.org/pdf/1911.10434" target="_blank">pdf</a>]

<h2>Regularization Shortcomings for Continual Learning. (arXiv:1912.03049v3 [cs.LG] UPDATED)</h2>
<h3>Timoth&#xe9;e Lesort, Andrei Stoian, David Filliat</h3>
<p>In most machine learning algorithms, training data are assumed independent
and identically distributed (iid). Otherwise, the algorithms' performances are
challenged. A famous phenomenon with non-iid data distribution is known as
\say{catastrophic forgetting}. Algorithms dealing with it are gathered in the
\textit{Continual Learning} research field. In this article, we study the
\textit{regularization} based approaches to continual learning. We show that
those approaches can not learn to discriminate classes from different tasks in
an elemental continual benchmark: class-incremental setting. We make
theoretical reasoning to prove this shortcoming and illustrate it with examples
and experiments.
</p>
<a href="http://arxiv.org/abs/1912.03049" target="_blank">arXiv:1912.03049</a> [<a href="http://arxiv.org/pdf/1912.03049" target="_blank">pdf</a>]

<h2>Multi-Robot Collision Avoidance under Uncertainty with Probabilistic Safety Barrier Certificates. (arXiv:1912.09957v3 [cs.RO] UPDATED)</h2>
<h3>Wenhao Luo, Wen Sun, Ashish Kapoor</h3>
<p>Safety in terms of collision avoidance for multi-robot systems is a difficult
challenge under uncertainty, non-determinism and lack of complete information.
This paper aims to propose a collision avoidance method that accounts for both
measurement uncertainty and motion uncertainty. In particular, we propose
Probabilistic Safety Barrier Certificates (PrSBC) using Control Barrier
Functions to define the space of admissible control actions that are
probabilistically safe with formally provable theoretical guarantee. By
formulating the chance constrained safety set into deterministic control
constraints with PrSBC, the method entails minimally modifying an existing
controller to determine an alternative safe controller via quadratic
programming constrained to PrSBC constraints. The key advantage of the approach
is that no assumptions about the form of uncertainty are required other than
finite support, also enabling worst-case guarantees. We demonstrate
effectiveness of the approach through experiments on realistic simulation
environments.
</p>
<a href="http://arxiv.org/abs/1912.09957" target="_blank">arXiv:1912.09957</a> [<a href="http://arxiv.org/pdf/1912.09957" target="_blank">pdf</a>]

<h2>Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN. (arXiv:2001.11761v3 [cs.CV] UPDATED)</h2>
<h3>Milad Mozafari, Leila Reddy, Rufin VanRullen</h3>
<p>Decoding and reconstructing images from brain imaging data is a research area
of high interest. Recent progress in deep generative neural networks has
introduced new opportunities to tackle this problem. Here, we employ a recently
proposed large-scale bi-directional generative adversarial network, called
BigBiGAN, to decode and reconstruct natural scenes from fMRI patterns. BigBiGAN
converts images into a 120-dimensional latent space which encodes class and
attribute information together, and can also reconstruct images based on their
latent vectors. We computed a linear mapping between fMRI data, acquired over
images from 150 different categories of ImageNet, and their corresponding
BigBiGAN latent vectors. Then, we applied this mapping to the fMRI activity
patterns obtained from 50 new test images from 50 unseen categories in order to
retrieve their latent vectors, and reconstruct the corresponding images.
Pairwise image decoding from the predicted latent vectors was highly accurate
(84%). Moreover, qualitative and quantitative assessments revealed that the
resulting image reconstructions were visually plausible, successfully captured
many attributes of the original images, and had high perceptual similarity with
the original content. This method establishes a new state-of-the-art for
fMRI-based natural image reconstruction, and can be flexibly updated to take
into account any future improvements in generative models of natural scene
images.
</p>
<a href="http://arxiv.org/abs/2001.11761" target="_blank">arXiv:2001.11761</a> [<a href="http://arxiv.org/pdf/2001.11761" target="_blank">pdf</a>]

<h2>Semi-supervised learning with an open augmenting unknown class for cost-effective training and reliable classifications. (arXiv:2002.01368v3 [stat.ML] UPDATED)</h2>
<h3>Emile R. Engelbrecht, Johan A. du Preez</h3>
<p>The ability to (a) train off partially labelled datasets and (b) ensure
resulting networks separate data outside the domain of interest hugely expands
the practical and cost-effective applicability of neural network classifiers.
We design a classifier based off generative adversarial networks (GANs) that
trains off a practical and cost-saving semi-supervised criteria which,
specifically, allows novel classes within the unlabelled training set.
Furthermore, we ensure the resulting classifier is capable of absolute novel
class detection, be these from the semi-supervised unlabelled training set or a
so-called open set. Results are both state-of-the-art and a first of its kind.
We argue this technique greatly decreases training cost in respect to labelling
while greatly improving the reliability of classifications.
</p>
<a href="http://arxiv.org/abs/2002.01368" target="_blank">arXiv:2002.01368</a> [<a href="http://arxiv.org/pdf/2002.01368" target="_blank">pdf</a>]

<h2>Concept Whitening for Interpretable Image Recognition. (arXiv:2002.01650v5 [cs.LG] UPDATED)</h2>
<h3>Zhi Chen, Yijie Bei, Cynthia Rudin</h3>
<p>What does a neural network encode about a concept as we traverse through the
layers? Interpretability in machine learning is undoubtedly important, but the
calculations of neural networks are very challenging to understand. Attempts to
see inside their hidden layers can either be misleading, unusable, or rely on
the latent space to possess properties that it may not have. In this work,
rather than attempting to analyze a neural network posthoc, we introduce a
mechanism, called concept whitening (CW), to alter a given layer of the network
to allow us to better understand the computation leading up to that layer. When
a concept whitening module is added to a CNN, the axes of the latent space are
aligned with known concepts of interest. By experiment, we show that CW can
provide us a much clearer understanding for how the network gradually learns
concepts over layers. CW is an alternative to a batch normalization layer in
that it normalizes, and also decorrelates (whitens) the latent space. CW can be
used in any layer of the network without hurting predictive performance.
</p>
<a href="http://arxiv.org/abs/2002.01650" target="_blank">arXiv:2002.01650</a> [<a href="http://arxiv.org/pdf/2002.01650" target="_blank">pdf</a>]

<h2>Minimum adjusted Rand index for two clusterings of a given size. (arXiv:2002.03677v2 [stat.ML] UPDATED)</h2>
<h3>Jos&#xe9; E. Chac&#xf3;n, Ana I. Rastrojo</h3>
<p>In an unpublished presentation, Steinley reported that the minimum adjusted
Rand index for the comparison of two clusterings of size $r$ is $-1/r$.
However, in a subsequent paper Chac\'on noted that this apparent bound can be
lowered. Here, it is shown that the lower bound proposed by Chac\'on is indeed
the minimum possible one. The result is even more general, since it is valid
for two clusterings of possibly different sizes.
</p>
<a href="http://arxiv.org/abs/2002.03677" target="_blank">arXiv:2002.03677</a> [<a href="http://arxiv.org/pdf/2002.03677" target="_blank">pdf</a>]

<h2>Task-Aware Variational Adversarial Active Learning. (arXiv:2002.04709v2 [cs.LG] UPDATED)</h2>
<h3>Kwanyoung Kim, Dongwon Park, Kwang In Kim, Se Young Chun</h3>
<p>Often, labeling large amount of data is challenging due to high labeling cost
limiting the application domain of deep learning techniques. Active learning
(AL) tackles this by querying the most informative samples to be annotated
among unlabeled pool. Two promising directions for AL that have been recently
explored are task-agnostic approach to select data points that are far from the
current labeled pool and task-aware approach that relies on the perspective of
task model. Unfortunately, the former does not exploit structures from tasks
and the latter does not seem to well-utilize overall data distribution. Here,
we propose task-aware variational adversarial AL (TA-VAAL) that modifies
task-agnostic VAAL, that considered data distribution of both label and
unlabeled pools, by relaxing task learning loss prediction to ranking loss
prediction and by using ranking conditional generative adversarial network to
embed normalized ranking loss information on VAAL. Our proposed TA-VAAL
outperforms state-of-the-arts on various benchmark datasets for classifications
with balanced / imbalanced labels as well as semantic segmentation and its
task-aware and task-agnostic AL properties were confirmed with our in-depth
analyses.
</p>
<a href="http://arxiv.org/abs/2002.04709" target="_blank">arXiv:2002.04709</a> [<a href="http://arxiv.org/pdf/2002.04709" target="_blank">pdf</a>]

<h2>Resolving Spurious Correlations in Causal Models of Environments via Interventions. (arXiv:2002.05217v2 [cs.LG] UPDATED)</h2>
<h3>Sergei Volodin, Nevan Wichers, Jeremy Nixon</h3>
<p>Causal models bring many benefits to decision-making systems (or agents) by
making them interpretable, sample-efficient, and robust to changes in the input
distribution. However, spurious correlations can lead to wrong causal models
and predictions. We consider the problem of inferring a causal model of a
reinforcement learning environment and we propose a method to deal with
spurious correlations. Specifically, our method designs a reward function that
incentivizes an agent to do an intervention to find errors in the causal model.
The data obtained from doing the intervention is used to improve the causal
model. We propose several intervention design methods and compare them. The
experimental results in a grid-world environment show that our approach leads
to better causal models compared to baselines: learning the model on data from
a random policy or a policy trained on the environment's reward. The main
contribution consists of methods to design interventions to resolve spurious
correlations.
</p>
<a href="http://arxiv.org/abs/2002.05217" target="_blank">arXiv:2002.05217</a> [<a href="http://arxiv.org/pdf/2002.05217" target="_blank">pdf</a>]

<h2>Rethinking Bias-Variance Trade-off for Generalization of Neural Networks. (arXiv:2002.11328v3 [cs.LG] UPDATED)</h2>
<h3>Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, Yi Ma</h3>
<p>The classical bias-variance trade-off predicts that bias decreases and
variance increase with model complexity, leading to a U-shaped risk curve.
Recent work calls this into question for neural networks and other
over-parameterized models, for which it is often observed that larger models
generalize better. We provide a simple explanation for this by measuring the
bias and variance of neural networks: while the bias is monotonically
decreasing as in the classical theory, the variance is unimodal or bell-shaped:
it increases then decreases with the width of the network. We vary the network
architecture, loss function, and choice of dataset and confirm that variance
unimodality occurs robustly for all models we considered. The risk curve is the
sum of the bias and variance curves and displays different qualitative shapes
depending on the relative scale of bias and variance, with the double descent
curve observed in recent literature as a special case. We corroborate these
empirical results with a theoretical analysis of two-layer linear networks with
random first layer. Finally, evaluation on out-of-distribution data shows that
most of the drop in accuracy comes from increased bias while variance increases
by a relatively small amount. Moreover, we find that deeper models decrease
bias and increase variance for both in-distribution and out-of-distribution
data.
</p>
<a href="http://arxiv.org/abs/2002.11328" target="_blank">arXiv:2002.11328</a> [<a href="http://arxiv.org/pdf/2002.11328" target="_blank">pdf</a>]

<h2>FLIC: Fast Lidar Image Clustering. (arXiv:2003.00575v2 [cs.CV] UPDATED)</h2>
<h3>Frederik Hasecke, Lukas Hahn, Anton Kummert</h3>
<p>Lidar sensors are widely used in various applications, ranging from
scientific fields over industrial use to integration in consumer products. With
an ever growing number of different driver assistance systems, they have been
introduced to automotive series production in recent years and are considered
an important building block for the practical realisation of autonomous
driving. However, due to the potentially large amount of Lidar points per scan,
tailored algorithms are required to identify objects (e.g. pedestrians or
vehicles) with high precision in a very short time. In this work, we propose an
algorithmic approach for real-time instance segmentation of Lidar sensor data.
We show how our method leverages the properties of the Euclidean distance to
retain three-dimensional measurement information, while being narrowed down to
a two-dimensional representation for fast computation. We further introduce
what we call "skip connections", to make our approach robust against
over-segmentation and improve assignment in cases of partial occlusion. Through
detailed evaluation on public data and comparison with established methods, we
show how these aspects enable state-of-the-art performance and runtime on a
single CPU core.
</p>
<a href="http://arxiv.org/abs/2003.00575" target="_blank">arXiv:2003.00575</a> [<a href="http://arxiv.org/pdf/2003.00575" target="_blank">pdf</a>]

<h2>Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control. (arXiv:2003.06709v4 [cs.LG] UPDATED)</h2>
<h3>Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B&#xf6;hmer, Shimon Whiteson</h3>
<p>Centralised training with decentralised execution (CTDE) is an important
learning paradigm in multi-agent reinforcement learning (MARL). To make
progress in CTDE, we introduce Multi-Agent MuJoCo (MAMuJoCo), a novel benchmark
suite that, unlike StarCraft Multi-Agent Challenge (SMAC), the predominant
benchmark environment, applies to continuous robotic control tasks. To
demonstrate the utility of MAMuJoCo, we present a range of benchmark results on
this new suite, including comparing the state-of-the-art actor-critic method
MADDPG against two novel variants of existing methods. These new methods
outperform MADDPG on a number of MAMuJoCo tasks. In addition, we show that, in
these continuous cooperative MAMuJoCo tasks, value factorisation plays a
greater role in performance than the underlying algorithmic choices. This
motivates the necessity of extending the study of value factorisations from
$Q$-learning to actor-critic algorithms.
</p>
<a href="http://arxiv.org/abs/2003.06709" target="_blank">arXiv:2003.06709</a> [<a href="http://arxiv.org/pdf/2003.06709" target="_blank">pdf</a>]

<h2>Which visual questions are difficult to answer? Analysis with Entropy of Answer Distributions. (arXiv:2004.05595v2 [cs.CV] UPDATED)</h2>
<h3>Kento Terao, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda, Shun&#x27;ichi Satoh</h3>
<p>We propose a novel approach to identify the difficulty of visual questions
for Visual Question Answering (VQA) without direct supervision or annotations
to the difficulty. Prior works have considered the diversity of ground-truth
answers of human annotators. In contrast, we analyze the difficulty of visual
questions based on the behavior of multiple different VQA models. We propose to
cluster the entropy values of the predicted answer distributions obtained by
three different models: a baseline method that takes as input images and
questions, and two variants that take as input images only and questions only.
We use a simple k-means to cluster the visual questions of the VQA v2
validation set. Then we use state-of-the-art methods to determine the accuracy
and the entropy of the answer distributions for each cluster. A benefit of the
proposed method is that no annotation of the difficulty is required, because
the accuracy of each cluster reflects the difficulty of visual questions that
belong to it. Our approach can identify clusters of difficult visual questions
that are not answered correctly by state-of-the-art methods. Detailed analysis
on the VQA v2 dataset reveals that 1) all methods show poor performances on the
most difficult cluster (about 10% accuracy), 2) as the cluster difficulty
increases, the answers predicted by the different methods begin to differ, and
3) the values of cluster entropy are highly correlated with the cluster
accuracy. We show that our approach has the advantage of being able to assess
the difficulty of visual questions without ground-truth (i.e. the test set of
VQA v2) by assigning them to one of the clusters. We expect that this can
stimulate the development of novel directions of research and new algorithms.
Clustering results are available online at https://github.com/tttamaki/vqd .
</p>
<a href="http://arxiv.org/abs/2004.05595" target="_blank">arXiv:2004.05595</a> [<a href="http://arxiv.org/pdf/2004.05595" target="_blank">pdf</a>]

<h2>Multi-Resolution POMDP Planning for Multi-Object Search in 3D. (arXiv:2005.02878v3 [cs.RO] UPDATED)</h2>
<h3>Kaiyu Zheng, Yoonchang Sung, George Konidaris, Stefanie Tellex</h3>
<p>Robots operating in household environments must find objects on shelves,
under tables, and in cupboards. Previous work often formulates the object
search problem as a POMDP Partially Observable Markov Decision Process), yet
constrain the search space in 2D to reduce computational complexity, although
objects exist in a rich 3D environment. We present a POMDP formulation for
multi-object search in a 3D region with a frustum-shaped field-of-view and an
efficient multi-resolution planning algorithm to solve this POMDP. To achieve
efficient planning, our algorithm uses a new octree-based representation that
captures beliefs at different resolution levels, enabling the agent to induce
abstract POMDPs with dramatically smaller state and observation spaces. Our
evaluation in a simulated 3D domain shows that our approach achieves
significantly higher reward ($\geq$ 51% in the largest instance) and finds more
objects compared to baselines without a resolution hierarchy, as the search
space becomes larger, and as the sensor uncertainty increases. We show that our
approach enables a mobile robot to automatically find objects placed at
different heights in two 10m$^2\times$2m regions by moving its base and
actuating its torso.
</p>
<a href="http://arxiv.org/abs/2005.02878" target="_blank">arXiv:2005.02878</a> [<a href="http://arxiv.org/pdf/2005.02878" target="_blank">pdf</a>]

<h2>What makes for good views for contrastive learning. (arXiv:2005.10243v2 [cs.CV] UPDATED)</h2>
<h3>Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola</h3>
<p>Contrastive learning between multiple views of the data has recently achieved
state of the art performance in the field of self-supervised representation
learning. Despite its success, the influence of different view choices has been
less studied. In this paper, we use empirical analysis to better understand the
importance of view selection, and argue that we should reduce the mutual
information (MI) between views while keeping task-relevant information intact.
To verify this hypothesis, we devise unsupervised and semi-supervised
frameworks that learn effective views by aiming to reduce their MI. We also
consider data augmentation as a way to reduce MI, and show that increasing data
augmentation indeed leads to decreasing MI and improves downstream
classification accuracy. As a by-product, we also achieve a new
state-of-the-art accuracy on unsupervised pre-training for ImageNet
classification ($73\%$ top-1 linear readoff with a ResNet-50). In addition,
transferring our models to PASCAL VOC object detection and COCO instance
segmentation consistently outperforms supervised pre-training.
Code:this http URL
</p>
<a href="http://arxiv.org/abs/2005.10243" target="_blank">arXiv:2005.10243</a> [<a href="http://arxiv.org/pdf/2005.10243" target="_blank">pdf</a>]

<h2>Longitudinal Deep Kernel Gaussian Process Regression. (arXiv:2005.11770v4 [stat.ML] UPDATED)</h2>
<h3>Junjie Liang, Yanting Wu, Dongkuan Xu, Vasant Honavar</h3>
<p>Gaussian processes offer an attractive framework for predictive modeling from
longitudinal data, i.e., irregularly sampled, sparse observations from a set of
individuals over time. However, such methods have two key shortcomings: (i)
They rely on ad hoc heuristics or expensive trial and error to choose the
effective kernels, and (ii) They fail to handle multilevel correlation
structure in the data. We introduce Longitudinal deep kernel Gaussian process
regression (L-DKGPR), which to the best of our knowledge, is the only method to
overcome these limitations by fully automating the discovery of complex
multilevel correlation structure from longitudinal data. Specifically, L-DKGPR
eliminates the need for ad hoc heuristics or trial and error using a novel
adaptation of deep kernel learning that combines the expressive power of deep
neural networks with the flexibility of non-parametric kernel methods. L-DKGPR
effectively learns the multilevel correlation with a novel addictive kernel
that simultaneously accommodates both time-varying and the time-invariant
effects. We derive an efficient algorithm to train L-DKGPR using latent space
inducing points and variational inference. Results of extensive experiments on
several benchmark data sets demonstrate that L-DKGPR significantly outperforms
the state-of-the-art longitudinal data analysis (LDA) methods.
</p>
<a href="http://arxiv.org/abs/2005.11770" target="_blank">arXiv:2005.11770</a> [<a href="http://arxiv.org/pdf/2005.11770" target="_blank">pdf</a>]

<h2>HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass Lens. (arXiv:2005.14446v3 [cs.CV] UPDATED)</h2>
<h3>Zhaohui Yang, Yunhe Wang, Xinghao Chen, Jianyuan Guo, Wei Zhang, Chao Xu, Chunjing Xu, Dacheng Tao, Chang Xu</h3>
<p>Neural Architecture Search (NAS) refers to automatically design the
architecture. We propose an hourglass-inspired approach (HourNAS) for this
problem that is motivated by the fact that the effects of the architecture
often proceed from the vital few blocks. Acting like the narrow neck of an
hourglass, vital blocks in the guaranteed path from the input to the output of
a deep neural network restrict the information flow and influence the network
accuracy. The other blocks occupy the major volume of the network and determine
the overall network complexity, corresponding to the bulbs of an hourglass. To
achieve an extremely fast NAS while preserving the high accuracy, we propose to
identify the vital blocks and make them the priority in the architecture
search. The search space of those non-vital blocks is further shrunk to only
cover the candidates that are affordable under the computational resource
constraints. Experimental results on the ImageNet show that only using 3 hours
(0.1 days) with one GPU, our HourNAS can search an architecture that achieves a
77.0% Top-1 accuracy, which outperforms the state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2005.14446" target="_blank">arXiv:2005.14446</a> [<a href="http://arxiv.org/pdf/2005.14446" target="_blank">pdf</a>]

<h2>Wasserstein Distance guided Adversarial Imitation Learning with Reward Shape Exploration. (arXiv:2006.03503v2 [cs.LG] UPDATED)</h2>
<h3>Ming Zhang, Yawei Wang, Xiaoteng Ma, Li Xia, Jun Yang, Zhiheng Li, Xiu Li</h3>
<p>The generative adversarial imitation learning (GAIL) has provided an
adversarial learning framework for imitating expert policy from demonstrations
in high-dimensional continuous tasks. However, almost all GAIL and its
extensions only design a kind of reward function of logarithmic form in the
adversarial training strategy with the Jensen-Shannon (JS) divergence for all
complex environments. The fixed logarithmic type of reward function may be
difficult to solve all complex tasks, and the vanishing gradients problem
caused by the JS divergence will harm the adversarial learning process. In this
paper, we propose a new algorithm named Wasserstein Distance guided Adversarial
Imitation Learning (WDAIL) for promoting the performance of imitation learning
(IL). There are three improvements in our method: (a) introducing the
Wasserstein distance to obtain more appropriate measure in the adversarial
training process, (b) using proximal policy optimization (PPO) in the
reinforcement learning stage which is much simpler to implement and makes the
algorithm more efficient, and (c) exploring different reward function shapes to
suit different tasks for improving the performance. The experiment results show
that the learning procedure remains remarkably stable, and achieves significant
performance in the complex continuous control tasks of MuJoCo.
</p>
<a href="http://arxiv.org/abs/2006.03503" target="_blank">arXiv:2006.03503</a> [<a href="http://arxiv.org/pdf/2006.03503" target="_blank">pdf</a>]

<h2>Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?. (arXiv:2006.05121v2 [cs.CV] UPDATED)</h2>
<h3>Corentin Kervadec (LIRIS), Grigory Antipov, Moez Baccouche (imagine), Christian Wolf (imagine)</h3>
<p>Visual Question Answering (VQA) models are notorious for their tendency to
rely on dataset biases. The large and unbalanced diversity of questions and
concepts involved in VQA and the lack of high standard annotated data tend to
prevent models from learning to `reason', leading them to perform `educated
guesses' instead, relying on specific training set statistics, which is not
helpful for generalizing to real world scenarios. In this paper, we claim that
the standard evaluation metric, which consists in measuring the overall
in-domain accuracy is misleading. Questions and concepts being unequally
distributed, it tends to favor models which exploit subtle training set
statistics. Alternatively, naively evaluating generalization by introducing
artificial distribution shift between train and test splits is also not
completely satisfying. First, the shifts do not reflect real words tendencies,
resulting in unsuitable models; second, since the shifts are artificially
handcrafted, trained models are specifically designed for this particular
setting, and paradoxically do not generalize to other configurations. We
propose the GQA-OOD benchmark designed to overcome these concerns: we measure
and compare accuracy over, both, rare and frequent question-answer pairs and
argue that the former is better suited to the evaluation of reasoning
abilities, which we experimentally validate with models trained to more or less
exploit biases. In a large-scale study involving 7 VQA models and 3 bias
reduction techniques, we also experimentally demonstrate that these models fail
to address questions involving infrequent concepts and provide recommendations
for future directions of research.
</p>
<a href="http://arxiv.org/abs/2006.05121" target="_blank">arXiv:2006.05121</a> [<a href="http://arxiv.org/pdf/2006.05121" target="_blank">pdf</a>]

<h2>Exploration by Maximizing R\'enyi Entropy for Zero-Shot Meta RL. (arXiv:2006.06193v2 [cs.LG] UPDATED)</h2>
<h3>Chuheng Zhang, Yuanying Cai, Longbo Huang, Jian Li</h3>
<p>Exploring the transition dynamics is essential to the success of
reinforcement learning (RL) algorithms. To face the challenges of exploration,
we consider a zero-shot meta RL framework that completely separates exploration
from exploitation and is suitable for the meta RL setting where there are many
reward functions of interest. In the exploration phase, the agent learns an
exploratory policy by interacting with a reward-free environment and collects a
dataset of transitions by executing the policy. In the planning phase, the
agent computes a good policy for any reward function based on the dataset
without further interacting with the environment. This framework brings new
challenges for exploration algorithms. In the exploration phase, we propose to
maximize the R\'enyi entropy over the state-action space and justify this
objective theoretically. We further deduce a policy gradient formulation for
this objective and design a practical exploration algorithm that can deal with
complex environments based on PPO. In the planning phase, we use a batch RL
algorithm, batch constrained deep Q-learning (BCQ), to solve for good policies
given arbitrary reward functions. Empirically, we show that our exploration
algorithm is effective and sample efficient, and results in superior policies
for arbitrary reward functions in the planning phase.
</p>
<a href="http://arxiv.org/abs/2006.06193" target="_blank">arXiv:2006.06193</a> [<a href="http://arxiv.org/pdf/2006.06193" target="_blank">pdf</a>]

<h2>Achieving robustness in classification using optimal transport with hinge regularization. (arXiv:2006.06520v2 [cs.LG] UPDATED)</h2>
<h3>Mathieu Serrurier, Franck Mamalet, Alberto Gonz&#xe1;lez-Sanz, Thibaut Boissin, Jean-Michel Loubes, Eustasio del Barrio</h3>
<p>Adversarial examples have pointed out Deep Neural Networks vulnerability to
small local noise. It has been shown that constraining their Lipschitz constant
should enhance robustness, but make them harder to learn with classical loss
functions. We propose a new framework for binary classification, based on
optimal transport, which integrates this Lipschitz constraint as a theoretical
requirement. We propose to learn 1-Lipschitz networks using a new loss that is
an hinge regularized version of the Kantorovich-Rubinstein dual formulation for
the Wasserstein distance estimation. This loss function has a direct
interpretation in terms of adversarial robustness together with certifiable
robustness bound. We also prove that this hinge regularized version is still
the dual formulation of an optimal transportation problem, and has a solution.
We also establish several geometrical properties of this optimal solution, and
extend the approach to multi-class problems. Experiments show that the proposed
approach provides the expected guarantees in terms of robustness without any
significant accuracy drop. The adversarial examples, on the proposed models,
visibly and meaningfully change the input providing an explanation for the
classification.
</p>
<a href="http://arxiv.org/abs/2006.06520" target="_blank">arXiv:2006.06520</a> [<a href="http://arxiv.org/pdf/2006.06520" target="_blank">pdf</a>]

<h2>Self-training Avoids Using Spurious Features Under Domain Shift. (arXiv:2006.10032v3 [cs.LG] UPDATED)</h2>
<h3>Yining Chen, Colin Wei, Ananya Kumar, Tengyu Ma</h3>
<p>In unsupervised domain adaptation, existing theory focuses on situations
where the source and target domains are close. In practice, conditional entropy
minimization and pseudo-labeling work even when the domain shifts are much
larger than those analyzed by existing theory. We identify and analyze one
particular setting where the domain shift can be large, but these algorithms
provably work: certain spurious features correlate with the label in the source
domain but are independent of the label in the target. Our analysis considers
linear classification where the spurious features are Gaussian and the
non-spurious features are a mixture of log-concave distributions. For this
setting, we prove that entropy minimization on unlabeled target data will avoid
using the spurious feature if initialized with a decently accurate source
classifier, even though the objective is non-convex and contains multiple bad
local minima using the spurious features. We verify our theory for spurious
domain shift tasks on semi-synthetic Celeb-A and MNIST datasets. Our results
suggest that practitioners collect and self-train on large, diverse datasets to
reduce biases in classifiers even if labeling is impractical.
</p>
<a href="http://arxiv.org/abs/2006.10032" target="_blank">arXiv:2006.10032</a> [<a href="http://arxiv.org/pdf/2006.10032" target="_blank">pdf</a>]

<h2>ByGARS: Byzantine SGD with Arbitrary Number of Attackers. (arXiv:2006.13421v2 [cs.LG] UPDATED)</h2>
<h3>Jayanth Regatti, Hao Chen, Abhishek Gupta</h3>
<p>We propose two novel stochastic gradient descent algorithms, ByGARS and
ByGARS++, for distributed machine learning in the presence of any number of
Byzantine adversaries. In these algorithms, reputation scores of workers are
computed using an auxiliary dataset at the server. This reputation score is
then used for aggregating the gradients for stochastic gradient descent. The
computational complexity of ByGARS++ is the same as the usual distributed
stochastic gradient descent method with only an additional inner product
computation in every iteration. We show that using these reputation scores for
gradient aggregation is robust to any number of multiplicative noise Byzantine
adversaries and use two-timescale stochastic approximation theory to prove
convergence for strongly convex loss functions. We demonstrate the
effectiveness of the algorithms for non-convex learning problems using MNIST
and CIFAR-10 datasets against almost all state-of-the-art Byzantine attacks. We
also show that the proposed algorithms are robust to multiple different types
of attacks at the same time.
</p>
<a href="http://arxiv.org/abs/2006.13421" target="_blank">arXiv:2006.13421</a> [<a href="http://arxiv.org/pdf/2006.13421" target="_blank">pdf</a>]

<h2>Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes. (arXiv:2007.00487v3 [cs.LG] UPDATED)</h2>
<h3>Timoth&#xe9;e Lesort</h3>
<p>Humans learn all their life long. They accumulate knowledge from a sequence
of learning experiences and remember the essential concepts without forgetting
what they have learned previously. Artificial neural networks struggle to learn
similarly. They often rely on data rigorously preprocessed to learn solutions
to specific problems such as classification or regression. In particular, they
forget their past learning experiences if trained on new ones. Therefore,
artificial neural networks are often inept to deal with real-life settings such
as an autonomous-robot that has to learn on-line to adapt to new situations and
overcome new problems without forgetting its past learning-experiences.
Continual learning (CL) is a branch of machine learning addressing this type of
problem. Continual algorithms are designed to accumulate and improve knowledge
in a curriculum of learning-experiences without forgetting. In this thesis, we
propose to explore continual algorithms with replay processes. Replay processes
gather together rehearsal methods and generative replay methods. Generative
Replay consists of regenerating past learning experiences with a generative
model to remember them. Rehearsal consists of saving a core-set of samples from
past learning experiences to rehearse them later. The replay processes make
possible a compromise between optimizing the current learning objective and the
past ones enabling learning without forgetting in sequences of tasks settings.
We show that they are very promising methods for continual learning. Notably,
they enable the re-evaluation of past data with new knowledge and the
confrontation of data from different learning-experiences. We demonstrate their
ability to learn continually through unsupervised learning, supervised learning
and reinforcement learning tasks.
</p>
<a href="http://arxiv.org/abs/2007.00487" target="_blank">arXiv:2007.00487</a> [<a href="http://arxiv.org/pdf/2007.00487" target="_blank">pdf</a>]

<h2>Odyssey: Creation, Analysis and Detection of Trojan Models. (arXiv:2007.08142v2 [cs.CV] UPDATED)</h2>
<h3>Marzieh Edraki, Nazmul Karim, Nazanin Rahnavard, Ajmal Mian, Mubarak Shah</h3>
<p>Along with the success of deep neural network (DNN) models, rise the threats
to the integrity of these models. A recent threat is the Trojan attack where an
attacker interferes with the training pipeline by inserting triggers into some
of the training samples and trains the model to act maliciously only for
samples that contain the trigger. Since the knowledge of triggers is privy to
the attacker, detection of Trojan networks is challenging. Existing Trojan
detectors make strong assumptions about the types of triggers and attacks. We
propose a detector that is based on the analysis of the intrinsic DNN
properties; that are affected due to the Trojaning process. For a comprehensive
analysis, we develop Odysseus, the most diverse dataset to date with over 3,000
clean and Trojan models. Odysseus covers a large spectrum of attacks; generated
by leveraging the versatility in trigger designs and source to target class
mappings. Our analysis results show that Trojan attacks affect the classifier
margin and shape of decision boundary around the manifold of clean data.
Exploiting these two factors, we propose an efficient Trojan detector that
operates without any knowledge of the attack and significantly outperforms
existing methods. Through a comprehensive set of experiments we demonstrate the
efficacy of the detector on cross model architectures, unseen Triggers and
regularized models.
</p>
<a href="http://arxiv.org/abs/2007.08142" target="_blank">arXiv:2007.08142</a> [<a href="http://arxiv.org/pdf/2007.08142" target="_blank">pdf</a>]

<h2>Do Adversarially Robust ImageNet Models Transfer Better?. (arXiv:2007.08489v2 [cs.CV] UPDATED)</h2>
<h3>Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander Madry</h3>
<p>Transfer learning is a widely-used paradigm in deep learning, where models
pre-trained on standard datasets can be efficiently adapted to downstream
tasks. Typically, better pre-trained models yield better transfer results,
suggesting that initial accuracy is a key aspect of transfer learning
performance. In this work, we identify another such aspect: we find that
adversarially robust models, while less accurate, often perform better than
their standard-trained counterparts when used for transfer learning.
Specifically, we focus on adversarially robust ImageNet classifiers, and show
that they yield improved accuracy on a standard suite of downstream
classification tasks. Further analysis uncovers more differences between robust
and standard models in the context of transfer learning. Our results are
consistent with (and in fact, add to) recent hypotheses stating that robustness
leads to improved feature representations. Our code and models are available at
https://github.com/Microsoft/robust-models-transfer .
</p>
<a href="http://arxiv.org/abs/2007.08489" target="_blank">arXiv:2007.08489</a> [<a href="http://arxiv.org/pdf/2007.08489" target="_blank">pdf</a>]

<h2>Value-Decomposition Multi-Agent Actor-Critics. (arXiv:2007.12306v3 [cs.AI] UPDATED)</h2>
<h3>Jianyu Su, Stephen Adams, Peter A. Beling</h3>
<p>The exploitation of extra state information has been an active research area
in multi-agent reinforcement learning (MARL). QMIX represents the joint
action-value using a non-negative function approximator and achieves the best
performance, by far, on multi-agent benchmarks, StarCraft II micromanagement
tasks. However, our experiments show that, in some cases, QMIX is incompatible
with A2C, a training paradigm that promotes algorithm training efficiency. To
obtain a reasonable trade-off between training efficiency and algorithm
performance, we extend value-decomposition to actor-critics that are compatible
with A2C and propose a novel actor-critic framework, value-decomposition
actor-critics (VDACs). We evaluate VDACs on the testbed of StarCraft II
micromanagement tasks and demonstrate that the proposed framework improves
median performance over other actor-critic methods. Furthermore, we use a set
of ablation experiments to identify the key factors that contribute to the
performance of VDACs.
</p>
<a href="http://arxiv.org/abs/2007.12306" target="_blank">arXiv:2007.12306</a> [<a href="http://arxiv.org/pdf/2007.12306" target="_blank">pdf</a>]

<h2>Cautious Active Clustering. (arXiv:2008.01245v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Cloninger, Hrushikesh Mhaskar</h3>
<p>We consider the problem of classification of points sampled from an unknown
probability measure on a Euclidean space. We study the question of querying the
class label at a very small number of judiciously chosen points so as to be
able to attach the appropriate class label to every point in the set. Our
approach is to consider the unknown probability measure as a convex combination
of the conditional probabilities for each class. Our technique involves the use
of a highly localized kernel constructed from Hermite polynomials, in order to
create a hierarchical estimate of the supports of the constituent probability
measures. We do not need to make any assumptions on the nature of any of the
probability measures nor know in advance the number of classes involved. We
give theoretical guarantees measured by the $F$-score for our classification
scheme. Examples include classification in hyper-spectral images and MNIST
classification.
</p>
<a href="http://arxiv.org/abs/2008.01245" target="_blank">arXiv:2008.01245</a> [<a href="http://arxiv.org/pdf/2008.01245" target="_blank">pdf</a>]

<h2>Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via Geometry-Aware Adversarial Learning. (arXiv:2008.05058v2 [cs.CV] UPDATED)</h2>
<h3>Borna Be&#x161;i&#x107;, Abhinav Valada</h3>
<p>Dynamic objects have a significant impact on the robot's perception of the
environment which degrades the performance of essential tasks such as
localization and mapping. In this work, we address this problem by synthesizing
plausible color, texture and geometry in regions occluded by dynamic objects.
We propose the novel geometry-aware DynaFill architecture that follows a
coarse-to-fine topology and incorporates our gated recurrent feedback mechanism
to adaptively fuse information from previous timesteps. We optimize our
architecture using adversarial training to synthesize fine realistic textures
which enables it to hallucinate color and depth structure in occluded regions
online in a spatially and temporally coherent manner, without relying on future
frame information. Casting our inpainting problem as an image-to-image
translation task, our model also corrects regions correlated with the presence
of dynamic objects in the scene, such as shadows or reflections. We introduce a
large-scale hyperrealistic dataset with RGB-D images, semantic segmentation
labels, camera poses as well as groundtruth RGB-D information of occluded
regions. Extensive quantitative and qualitative evaluations show that our
approach achieves state-of-the-art performance, even in challenging weather
conditions. Furthermore, we present results for retrieval-based visual
localization with the synthesized images that demonstrate the utility of our
approach.
</p>
<a href="http://arxiv.org/abs/2008.05058" target="_blank">arXiv:2008.05058</a> [<a href="http://arxiv.org/pdf/2008.05058" target="_blank">pdf</a>]

<h2>A soft robot that adapts to environments through shape change. (arXiv:2008.06397v3 [cs.RO] UPDATED)</h2>
<h3>Dylan S. Shah (1), Joshua P. Powers (2), Liana G. Tilton (1), Sam Kriegman (2), Josh Bongard (2), Rebecca Kramer-Bottiglio (1) ((1) Yale University, (2) University of Vermont)</h3>
<p>Many organisms, including various species of spiders and caterpillars, change
their shape to switch gaits and adapt to different environments. Recent
technological advances, ranging from stretchable circuits to highly deformable
soft robots, have begun to make shape changing robots a possibility. However,
it is currently unclear how and when shape change should occur, and what
capabilities could be gained, leading to a wide range of unsolved design and
control problems. To begin addressing these questions, here we simulate,
design, and build a soft robot that utilizes shape change to achieve locomotion
over both a flat and inclined surface. Modeling this robot in simulation, we
explore its capabilities in two environments and demonstrate the automated
discovery of environment-specific shapes and gaits that successfully transfer
to the physical hardware. We found that the shape-changing robot traverses
these environments better than an equivalent but non-morphing robot, in
simulation and reality.
</p>
<a href="http://arxiv.org/abs/2008.06397" target="_blank">arXiv:2008.06397</a> [<a href="http://arxiv.org/pdf/2008.06397" target="_blank">pdf</a>]

<h2>DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term Representations. (arXiv:2009.06308v2 [cs.CV] UPDATED)</h2>
<h3>Ruben Tolosana, Paula Delgado-Santos, Andres Perez-Uribe, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales</h3>
<p>This study proposes DeepWriteSYN, a novel on-line handwriting synthesis
approach via deep short-term representations. It comprises two modules: i) an
optional and interchangeable temporal segmentation, which divides the
handwriting into short-time segments consisting of individual or multiple
concatenated strokes; and ii) the on-line synthesis of those short-time
handwriting segments, which is based on a sequence-to-sequence Variational
Autoencoder (VAE). The main advantages of the proposed approach are that the
synthesis is carried out in short-time segments (that can run from a character
fraction to full characters) and that the VAE can be trained on a configurable
handwriting dataset. These two properties give a lot of flexibility to our
synthesiser, e.g., as shown in our experiments, DeepWriteSYN can generate
realistic handwriting variations of a given handwritten structure corresponding
to the natural variation within a given population or a given subject. These
two cases are developed experimentally for individual digits and handwriting
signatures, respectively, achieving in both cases remarkable results.

Also, we provide experimental results for the task of on-line signature
verification showing the high potential of DeepWriteSYN to improve
significantly one-shot learning scenarios. To the best of our knowledge, this
is the first synthesis approach capable of generating realistic on-line
handwriting in the short term (including handwritten signatures) via deep
learning. This can be very useful as a module toward long-term realistic
handwriting generation either completely synthetic or as natural variation of
given handwriting samples.
</p>
<a href="http://arxiv.org/abs/2009.06308" target="_blank">arXiv:2009.06308</a> [<a href="http://arxiv.org/pdf/2009.06308" target="_blank">pdf</a>]

<h2>Adaptive Label Smoothing. (arXiv:2009.06432v2 [cs.CV] UPDATED)</h2>
<h3>Ujwal Krothapalli, A. Lynn Abbott</h3>
<p>This paper concerns the use of objectness measures to improve the calibration
performance of Convolutional Neural Networks (CNNs). CNNs have proven to be
very good classifiers and generally localize objects well; however, the loss
functions typically used to train classification CNNs do not penalize inability
to localize an object, nor do they take into account an object's relative size
in the given image. During training on ImageNet-1K almost all approaches use
random crops on the images and this transformation sometimes provides the CNN
with background only samples. This causes the classifiers to depend on context.
Context dependence is harmful for safety-critical applications. We present a
novel approach to classification that combines the ideas of objectness and
label smoothing during training. Unlike previous methods, we compute a
smoothing factor that is \emph{adaptive} based on relative object size within
an image. This causes our approach to produce confidences that are grounded in
the size of the object being classified instead of relying on context to make
the correct predictions. We present extensive results using ImageNet to
demonstrate that CNNs trained using adaptive label smoothing are much less
likely to be overconfident in their predictions. We show qualitative results
using class activation maps and quantitative results using classification and
transfer learning tasks. Our approach is able to produce an order of magnitude
reduction in confidence when predicting on context only images when compared to
baselines. Using transfer learning, we gain 2.1mAP on MS COCO compared to the
hard label approach.
</p>
<a href="http://arxiv.org/abs/2009.06432" target="_blank">arXiv:2009.06432</a> [<a href="http://arxiv.org/pdf/2009.06432" target="_blank">pdf</a>]

<h2>Modeling human visual search: A combined Bayesian searcher and saliency map approach for eye movement guidance in natural scenes. (arXiv:2009.08373v2 [cs.AI] UPDATED)</h2>
<h3>M. Sclar, G. Bujia, S. Vita, G. Solovey, J. E. Kamienkowski</h3>
<p>Finding objects is essential for almost any daily-life visual task. Saliency
models have been useful to predict fixation locations in natural images, but
are static, i.e., they provide no information about the time-sequence of
fixations. Nowadays, one of the biggest challenges in the field is to go beyond
saliency maps to predict a sequence of fixations related to a visual task, such
as searching for a given target. Bayesian observer models have been proposed
for this task, as they represent visual search as an active sampling process.
Nevertheless, they were mostly evaluated on artificial images, and how they
adapt to natural images remains largely unexplored.

Here, we propose a unified Bayesian model for visual search guided by
saliency maps as prior information. We validated our model with a visual search
experiment in natural scenes recording eye movements. We show that, although
state-of-the-art saliency models perform well in predicting the first two
fixations in a visual search task, their performance degrades to chance
afterward. This suggests that saliency maps alone are good to model bottom-up
first impressions, but are not enough to explain the scanpaths when top-down
task information is critical. Thus, we propose to use them as priors of
Bayesian searchers. This approach leads to a behavior very similar to humans
for the whole scanpath, both in the percentage of target found as a function of
the fixation rank and the scanpath similarity, reproducing the entire sequence
of eye movements.
</p>
<a href="http://arxiv.org/abs/2009.08373" target="_blank">arXiv:2009.08373</a> [<a href="http://arxiv.org/pdf/2009.08373" target="_blank">pdf</a>]

<h2>Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't. (arXiv:2009.10713v3 [cs.LG] UPDATED)</h2>
<h3>Weinan E, Chao Ma, Stephan Wojtowytsch, Lei Wu</h3>
<p>The purpose of this article is to review the achievements made in the last
few years towards the understanding of the reasons behind the success and
subtleties of neural network-based machine learning. In the tradition of good
old applied mathematics, we will not only give attention to rigorous
mathematical results, but also the insight we have gained from careful
numerical experiments as well as the analysis of simplified models. Along the
way, we also list the open problems which we believe to be the most important
topics for further study. This is not a complete overview over this quickly
moving field, but we hope to provide a perspective which may be helpful
especially to new researchers in the area.
</p>
<a href="http://arxiv.org/abs/2009.10713" target="_blank">arXiv:2009.10713</a> [<a href="http://arxiv.org/pdf/2009.10713" target="_blank">pdf</a>]

<h2>Adaptive confidence thresholding for monocular depth estimation. (arXiv:2009.12840v2 [cs.CV] UPDATED)</h2>
<h3>Hyesong Choi, Hunsang Lee, Sunkyung Kim, Sunok Kim, Seungryong Kim, Kwanghoon Sohn, Dongbo Min</h3>
<p>Self-supervised monocular depth estimation has become an appealing solution
to the lack of ground truth labels, but its reconstruction loss often produces
over-smoothed results across object boundaries and is incapable of handling
occlusion explicitly. In this paper, we propose a new approach to leverage
pseudo ground truth depth maps of stereo images generated from pretrained
stereo matching methods. The confidence map of the pseudo ground truth depth
map is estimated to mitigate performance degeneration by inaccurate pseudo
depth maps. To cope with the prediction error of the confidence map itself, we
also leverage the threshold network that learns the threshold dynamically
conditioned on the pseudo depth maps. The pseudo depth labels filtered out by
the thresholded confidence map are used to supervise the monocular depth
network. Furthermore, we propose the probabilistic framework that refines the
monocular depth map with the help of its uncertainty map through the
pixel-adaptive convolution (PAC) layer. Experimental results demonstrate
superior performance to state-of-the-art monocular depth estimation methods.
Lastly, we exhibit that the proposed threshold learning can also be used to
improve the performance of existing confidence estimation approaches.
</p>
<a href="http://arxiv.org/abs/2009.12840" target="_blank">arXiv:2009.12840</a> [<a href="http://arxiv.org/pdf/2009.12840" target="_blank">pdf</a>]

<h2>Why Adversarial Interaction Creates Non-Homogeneous Patterns: A Pseudo-Reaction-Diffusion Model for Turing Instability. (arXiv:2010.00521v2 [cs.LG] UPDATED)</h2>
<h3>Litu Rout</h3>
<p>Long after Turing's seminal Reaction-Diffusion (RD) model, the elegance of
his fundamental equations alleviated much of the skepticism surrounding pattern
formation. Though Turing model is a simplification and an idealization, it is
one of the best-known theoretical models to explain patterns as a reminiscent
of those observed in nature. Over the years, concerted efforts have been made
to align theoretical models to explain patterns in real systems. The apparent
difficulty in identifying the specific dynamics of the RD system makes the
problem particularly challenging. Interestingly, we observe Turing-like
patterns in a system of neurons with adversarial interaction. In this study, we
establish the involvement of Turing instability to create such patterns. By
theoretical and empirical studies, we present a pseudo-reaction-diffusion model
to explain the mechanism that may underlie these phenomena. While supervised
learning attains homogeneous equilibrium, this paper suggests that the
introduction of an adversary helps break this homogeneity to create
non-homogeneous patterns at equilibrium. Further, we prove that randomly
initialized gradient descent with over-parameterization can converge
exponentially fast to an $\epsilon$-stationary point even under adversarial
interaction. In addition, different from sole supervision, we show that the
solutions obtained under adversarial interaction are not limited to a tiny
subspace around initialization.
</p>
<a href="http://arxiv.org/abs/2010.00521" target="_blank">arXiv:2010.00521</a> [<a href="http://arxiv.org/pdf/2010.00521" target="_blank">pdf</a>]

<h2>Exploring the Interchangeability of CNN Embedding Spaces. (arXiv:2010.02323v3 [cs.CV] UPDATED)</h2>
<h3>David McNeely-White, Benjamin Sattelberg, Nathaniel Blanchard, Ross Beveridge</h3>
<p>CNN feature spaces can be linearly mapped and consequently are often
interchangeable. This equivalence holds across variations in architectures,
training datasets, and network tasks. Specifically, we mapped between 10
image-classification CNNs and between 4 facial-recognition CNNs. When image
embeddings generated by one CNN are transformed into embeddings corresponding
to the feature space of a second CNN trained on the same task, their respective
image classification or face verification performance is largely preserved. For
CNNs trained to the same classes and sharing a common backend-logit (soft-max)
architecture, a linear-mapping may always be calculated directly from the
backend layer weights. However, the case of a closed-set analysis with perfect
knowledge of classifiers is limiting. Therefore, empirical methods of
estimating mappings are presented for both the closed-set image classification
task and the open-set task of face recognition. The results presented expose
the essentially interchangeable nature of CNNs embeddings for two important and
common recognition tasks. The implications are far-reaching, suggesting an
underlying commonality between representations learned by networks designed and
trained for a common task. One practical implication is that face embeddings
from some commonly used CNNs can be compared using these mappings.
</p>
<a href="http://arxiv.org/abs/2010.02323" target="_blank">arXiv:2010.02323</a> [<a href="http://arxiv.org/pdf/2010.02323" target="_blank">pdf</a>]

<h2>Age and Gender Prediction From Face Images Using Attentional Convolutional Network. (arXiv:2010.03791v2 [cs.CV] UPDATED)</h2>
<h3>Amirali Abdolrashidi, Mehdi Minaei, Elham Azimi, Shervin Minaee</h3>
<p>Automatic prediction of age and gender from face images has drawn a lot of
attention recently, due it is wide applications in various facial analysis
problems. However, due to the large intra-class variation of face images (such
as variation in lighting, pose, scale, occlusion), the existing models are
still behind the desired accuracy level, which is necessary for the use of
these models in real-world applications. In this work, we propose a deep
learning framework, based on the ensemble of attentional and residual
convolutional networks, to predict gender and age group of facial images with
high accuracy rate. Using attention mechanism enables our model to focus on the
important and informative parts of the face, which can help it to make a more
accurate prediction. We train our model in a multi-task learning fashion, and
augment the feature embedding of the age classifier, with the predicted gender,
and show that doing so can further increase the accuracy of age prediction. Our
model is trained on a popular face age and gender dataset, and achieved
promising results. Through visualization of the attention maps of the train
model, we show that our model has learned to become sensitive to the right
regions of the face.
</p>
<a href="http://arxiv.org/abs/2010.03791" target="_blank">arXiv:2010.03791</a> [<a href="http://arxiv.org/pdf/2010.03791" target="_blank">pdf</a>]

<h2>Smooth Variational Graph Embeddings for Efficient Neural Architecture Search. (arXiv:2010.04683v2 [cs.LG] UPDATED)</h2>
<h3>Jovita Lukasik, David Friede, Arber Zela, Heiner Stuckenschmidt, Frank Hutter, Margret Keuper</h3>
<p>In this paper, we propose an approach to neural architecture search (NAS)
based on graph embeddings. NAS has been addressed previously using discrete,
sampling based methods, which are computationally expensive as well as
differentiable approaches, which come at lower costs but enforce stronger
constraints on the search space. The proposed approach leverages advantages
from both sides by building a smooth variational neural architecture embedding
space in which we evaluate a structural subset of architectures at training
time using the predicted performance while it allows to extrapolate from this
subspace at inference time. We evaluate the proposed approach in the context of
two common search spaces, the graph structure defined by the ENAS approach and
the NAS-Bench-101 search space, and improve over the state of the art in both.
We provide our implementation at \url{https://github.com/automl/SVGe}.
</p>
<a href="http://arxiv.org/abs/2010.04683" target="_blank">arXiv:2010.04683</a> [<a href="http://arxiv.org/pdf/2010.04683" target="_blank">pdf</a>]

<h2>Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification. (arXiv:2010.05785v2 [cs.CV] UPDATED)</h2>
<h3>Oren Nuriel, Sagie Benaim, Lior Wolf</h3>
<p>Recent work has shown that convolutional neural network classifiers overly
rely on texture at the expense of shape cues. We make a similar but different
distinction between shape and local image cues, on the one hand, and global
image statistics, on the other. Our method, called Permuted Adaptive Instance
Normalization (pAdaIN), reduces the representation of global statistics in the
hidden layers of image classifiers. pAdaIN samples a random permutation $\pi$
that rearranges the samples in a given batch. Adaptive Instance Normalization
(AdaIN) is then applied between the activations of each (non-permuted) sample
$i$ and the corresponding activations of the sample $\pi(i)$, thus swapping
statistics between the samples of the batch. Since the global image statistics
are distorted, this swapping procedure causes the network to rely on cues, such
as shape or texture. By choosing the random permutation with probability $p$
and the identity permutation otherwise, one can control the effect's strength.

With the correct choice of $p$, fixed apriori for all experiments and
selected without considering the test data, our method consistently outperforms
baselines in multiple settings. In image classification, our method improves on
both CIFAR100 and ImageNet using multiple architectures. In the setting of
robustness, our method improves on both ImageNet-C and Cifar-100-C for multiple
architectures. In the setting of domain adaptation and domain generalization,
our method achieves state of the art results on the transfer learning task from
GTAV to Cityscapes and on the PACS benchmark.
</p>
<a href="http://arxiv.org/abs/2010.05785" target="_blank">arXiv:2010.05785</a> [<a href="http://arxiv.org/pdf/2010.05785" target="_blank">pdf</a>]

<h2>MOTChallenge: A Benchmark for Single-Camera Multiple Target Tracking. (arXiv:2010.07548v2 [cs.CV] UPDATED)</h2>
<h3>Patrick Dendorfer, Aljo&#x161;a O&#x161;ep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, Laura Leal-Taix&#xe9;</h3>
<p>Standardized benchmarks have been crucial in pushing the performance of
computer vision algorithms, especially since the advent of deep learning.
Although leaderboards should not be over-claimed, they often provide the most
objective measure of performance and are therefore important guides for
research. We present MOTChallenge, a benchmark for single-camera Multiple
Object Tracking (MOT) launched in late 2014, to collect existing and new data,
and create a framework for the standardized evaluation of multiple object
tracking methods. The benchmark is focused on multiple people tracking, since
pedestrians are by far the most studied object in the tracking community, with
applications ranging from robot navigation to self-driving cars. This paper
collects the first three releases of the benchmark: (i) MOT15, along with
numerous state-of-the-art results that were submitted in the last years, (ii)
MOT16, which contains new challenging videos, and (iii) MOT17, that extends
MOT16 sequences with more precise labels and evaluates tracking performance on
three different object detectors. The second and third release not only offers
a significant increase in the number of labeled boxes but also provide labels
for multiple object classes beside pedestrians, as well as the level of
visibility for every single object of interest. We finally provide a
categorization of state-of-the-art trackers and a broad error analysis. This
will help newcomers understand the related work and research trends in the MOT
community, and hopefully shed some light on potential future research
directions.
</p>
<a href="http://arxiv.org/abs/2010.07548" target="_blank">arXiv:2010.07548</a> [<a href="http://arxiv.org/pdf/2010.07548" target="_blank">pdf</a>]

<h2>An explainable deep vision system for animal classification and detection in trail-camera images with automatic post-deployment retraining. (arXiv:2010.11472v3 [cs.CV] UPDATED)</h2>
<h3>Golnaz Moallem (1), Don D. Pathirage (1), Joel Reznick (1), James Gallagher (2), Hamed Sari-Sarraf (1) ((1) Applied Vision Lab Texas Tech University (2) Texas Parks and Wildlife Department)</h3>
<p>This paper introduces an automated vision system for animal detection in
trail-camera images taken from a field under the administration of the Texas
Parks and Wildlife Department. As traditional wildlife counting techniques are
intrusive and labor intensive to conduct, trail-camera imaging is a
comparatively non-intrusive method for capturing wildlife activity. However,
given the large volume of images produced from trail-cameras, manual analysis
of the images remains time-consuming and inefficient. We implemented a
two-stage deep convolutional neural network pipeline to find animal-containing
images in the first stage and then process these images to detect birds in the
second stage. The animal classification system classifies animal images with
overall 93% sensitivity and 96% specificity. The bird detection system achieves
better than 93% sensitivity, 92% specificity, and 68% average
Intersection-over-Union rate. The entire pipeline processes an image in less
than 0.5 seconds as opposed to an average 30 seconds for a human labeler. We
also addressed post-deployment issues related to data drift for the animal
classification system as image features vary with seasonal changes. This system
utilizes an automatic retraining algorithm to detect data drift and update the
system. We introduce a novel technique for detecting drifted images and
triggering the retraining procedure. Two statistical experiments are also
presented to explain the prediction behavior of the animal classification
system. These experiments investigate the cues that steers the system towards a
particular decision. Statistical hypothesis testing demonstrates that the
presence of an animal in the input image significantly contributes to the
system's decisions.
</p>
<a href="http://arxiv.org/abs/2010.11472" target="_blank">arXiv:2010.11472</a> [<a href="http://arxiv.org/pdf/2010.11472" target="_blank">pdf</a>]

<h2>Continual Learning in Low-rank Orthogonal Subspaces. (arXiv:2010.11635v2 [cs.LG] UPDATED)</h2>
<h3>Arslan Chaudhry, Naeemullah Khan, Puneet K. Dokania, Philip H. S. Torr</h3>
<p>In continual learning (CL), a learner is faced with a sequence of tasks,
arriving one after the other, and the goal is to remember all the tasks once
the continual learning experience is finished. The prior art in CL uses
episodic memory, parameter regularization or extensible network structures to
reduce interference among tasks, but in the end, all the approaches learn
different tasks in a joint vector space. We believe this invariably leads to
interference among different tasks. We propose to learn tasks in different
(low-rank) vector subspaces that are kept orthogonal to each other in order to
minimize interference. Further, to keep the gradients of different tasks coming
from these subspaces orthogonal to each other, we learn isometric mappings by
posing network training as an optimization problem over the Stiefel manifold.
To the best of our understanding, we report, for the first time, strong results
over experience-replay baseline with and without memory on standard
classification benchmarks in continual learning. The code is made publicly
available.
</p>
<a href="http://arxiv.org/abs/2010.11635" target="_blank">arXiv:2010.11635</a> [<a href="http://arxiv.org/pdf/2010.11635" target="_blank">pdf</a>]

<h2>Graph-based Reinforcement Learning for Active Learning in Real Time: An Application in Modeling River Networks. (arXiv:2010.14000v2 [cs.LG] UPDATED)</h2>
<h3>Xiaowei Jia, Beiyu Lin, Jacob Zwart, Jeffrey Sadler, Alison Appling, Samantha Oliver, Jordan Read</h3>
<p>Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.
</p>
<a href="http://arxiv.org/abs/2010.14000" target="_blank">arXiv:2010.14000</a> [<a href="http://arxiv.org/pdf/2010.14000" target="_blank">pdf</a>]

<h2>$\mu$NAS: Constrained Neural Architecture Search for Microcontrollers. (arXiv:2010.14246v3 [cs.LG] UPDATED)</h2>
<h3>Edgar Liberis, &#x141;ukasz Dudziak, Nicholas D. Lane</h3>
<p>IoT devices are powered by microcontroller units (MCUs) which are extremely
resource-scarce: a typical MCU may have an underpowered processor and around 64
KB of memory and persistent storage, which is orders of magnitude fewer
computational resources than is typically required for deep learning. Designing
neural networks for such a platform requires an intricate balance between
keeping high predictive performance (accuracy) while achieving low memory and
storage usage and inference latency. This is extremely challenging to achieve
manually, so in this work, we build a neural architecture search (NAS) system,
called $\mu$NAS, to automate the design of such small-yet-powerful MCU-level
networks. $\mu$NAS explicitly targets the three primary aspects of resource
scarcity of MCUs: the size of RAM, persistent storage and processor speed.
$\mu$NAS represents a significant advance in resource-efficient models,
especially for "mid-tier" MCUs with memory requirements ranging from 0.5 KB to
64 KB. We show that on a variety of image classification datasets $\mu$NAS is
able to (a) improve top-1 classification accuracy by up to 4.8%, or (b) reduce
memory footprint by 4--13x, or (c) reduce the number of multiply-accumulate
operations by at least 2x, compared to existing MCU specialist literature and
resource-efficient models.
</p>
<a href="http://arxiv.org/abs/2010.14246" target="_blank">arXiv:2010.14246</a> [<a href="http://arxiv.org/pdf/2010.14246" target="_blank">pdf</a>]

<h2>One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. (arXiv:2010.14484v2 [cs.LG] UPDATED)</h2>
<h3>Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn</h3>
<p>While reinforcement learning algorithms can learn effective policies for
complex tasks, these policies are often brittle to even minor task variations,
especially when variations are not explicitly provided during training. One
natural approach to this problem is to train agents with manually specified
variation in the training task or environment. However, this may be infeasible
in practical situations, either because making perturbations is not possible,
or because it is unclear how to choose suitable perturbation strategies without
sacrificing performance. The key insight of this work is that learning diverse
behaviors for accomplishing a task can directly lead to behavior that
generalizes to varying environments, without needing to perform explicit
perturbations during training. By identifying multiple solutions for the task
in a single environment during training, our approach can generalize to new
situations by abandoning solutions that are no longer effective and adopting
those that are. We theoretically characterize a robustness set of environments
that arises from our algorithm and empirically find that our diversity-driven
approach can extrapolate to various changes in the environment and task.
</p>
<a href="http://arxiv.org/abs/2010.14484" target="_blank">arXiv:2010.14484</a> [<a href="http://arxiv.org/pdf/2010.14484" target="_blank">pdf</a>]

<h2>Meta-Learning with Adaptive Hyperparameters. (arXiv:2011.00209v2 [cs.LG] UPDATED)</h2>
<h3>Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon Kim, Kyoung Mu Lee</h3>
<p>Despite its popularity, several recent works question the effectiveness of
MAML when test tasks are different from training tasks, thus suggesting various
task-conditioned methodology to improve the initialization. Instead of
searching for better task-aware initialization, we focus on a complementary
factor in MAML framework, inner-loop optimization (or fast adaptation).
Consequently, we propose a new weight update rule that greatly enhances the
fast adaptation process. Specifically, we introduce a small meta-network that
can adaptively generate per-step hyperparameters: learning rate and weight
decay coefficients. The experimental results validate that the Adaptive
Learning of hyperparameters for Fast Adaptation (ALFA) is the equally important
ingredient that was often neglected in the recent few-shot learning approaches.
Surprisingly, fast adaptation from random initialization with ALFA can already
outperform MAML.
</p>
<a href="http://arxiv.org/abs/2011.00209" target="_blank">arXiv:2011.00209</a> [<a href="http://arxiv.org/pdf/2011.00209" target="_blank">pdf</a>]

<h2>Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter. (arXiv:2011.00574v2 [cs.CV] UPDATED)</h2>
<h3>Omid Taheri, Hassan Salarieh, Aria Alasty</h3>
<p>Human motion capture is frequently used to study rehabilitation and clinical
problems, as well as to provide realistic animation for the entertainment
industry. IMU-based systems, as well as Marker-based motion tracking systems,
are the most popular methods to track movement due to their low cost of
implementation and lightweight. This paper proposes a quaternion-based Extended
Kalman filter approach to recover the human leg segments motions with a set of
IMU sensors data fused with camera-marker system data. In this paper, an
Extended Kalman Filter approach is developed to fuse the data of two IMUs and
one RGB camera for human leg motion tracking. Based on the complementary
properties of the inertial sensors and camera-marker system, in the introduced
new measurement model, the orientation data of the upper leg and the lower leg
is updated through three measurement equations. The positioning of the human
body is made possible by the tracked position of the pelvis joint by the camera
marker system. A mathematical model has been utilized to estimate joints' depth
in 2D images. The efficiency of the proposed algorithm is evaluated by an
optical motion tracker system.
</p>
<a href="http://arxiv.org/abs/2011.00574" target="_blank">arXiv:2011.00574</a> [<a href="http://arxiv.org/pdf/2011.00574" target="_blank">pdf</a>]

<h2>Multi-View Adaptive Fusion Network for 3D Object Detection. (arXiv:2011.00652v2 [cs.CV] UPDATED)</h2>
<h3>Guojun Wang, Bin Tian, Yachen Zhang, Long Chen, Dongpu Cao, Jian Wu</h3>
<p>3D object detection based on LiDAR-camera fusion is becoming an emerging
research theme for autonomous driving. However, it has been surprisingly
difficult to effectively fuse both modalities without information loss and
interference. To solve this issue, we propose a single-stage multi-view fusion
framework that takes LiDAR bird's-eye view, LiDAR range view and camera view
images as inputs for 3D object detection. To effectively fuse multi-view
features, we propose an attentive pointwise fusion (APF) module to estimate the
importance of the three sources with attention mechanisms that can achieve
adaptive fusion of multi-view features in a pointwise manner. Furthermore, an
attentive pointwise weighting (APW) module is designed to help the network
learn structure information and point feature importance with two extra tasks,
namely, foreground classification and center regression, and the predicted
foreground probability is used to reweight the point features. We design an
end-to-end learnable network named MVAF-Net to integrate these two components.
Our evaluations conducted on the KITTI 3D object detection datasets demonstrate
that the proposed APF and APW modules offer significant performance gains.
Moreover, the proposed MVAF-Net achieves the best performance among all
single-stage fusion methods and outperforms most two-stage fusion methods,
achieving the best trade-off between speed and accuracy on the KITTI benchmark.
</p>
<a href="http://arxiv.org/abs/2011.00652" target="_blank">arXiv:2011.00652</a> [<a href="http://arxiv.org/pdf/2011.00652" target="_blank">pdf</a>]

<h2>Identifying and interpreting tuning dimensions in deep networks. (arXiv:2011.03043v2 [cs.LG] UPDATED)</h2>
<h3>Nolan S. Dey, J. Eric Taylor, Bryan P. Tripp, Alexander Wong, Graham W. Taylor</h3>
<p>In neuroscience, a tuning dimension is a stimulus attribute that accounts for
much of the activation variance of a group of neurons. These are commonly used
to decipher the responses of such groups. While researchers have attempted to
manually identify an analogue to these tuning dimensions in deep neural
networks, we are unaware of an automatic way to discover them. This work
contributes an unsupervised framework for identifying and interpreting "tuning
dimensions" in deep networks. Our method correctly identifies the tuning
dimensions of a synthetic Gabor filter bank and tuning dimensions of the first
two layers of InceptionV1 trained on ImageNet.
</p>
<a href="http://arxiv.org/abs/2011.03043" target="_blank">arXiv:2011.03043</a> [<a href="http://arxiv.org/pdf/2011.03043" target="_blank">pdf</a>]

<h2>Channel Pruning via Multi-Criteria based on Weight Dependency. (arXiv:2011.03240v3 [cs.CV] UPDATED)</h2>
<h3>Yangchun Yan, Chao Li, Rongzuo Guo, Kang Yang, Yongjun Xu</h3>
<p>Channel pruning has demonstrated its effectiveness in compressing ConvNets.
In many prior arts, the importance of an output feature map is only determined
by its associated filter. However, these methods ignore a small part of weights
in the next layer which disappears as the feature map is removed. They ignore
the dependency of the weights. In addition, many pruning methods use only one
criterion for evaluation, and find a sweet-spot of pruning structure and
accuracy in a trial-and-error fashion, which can be time-consuming. To address
the above issues, we proposed a channel pruning algorithm via multi-criteria
based on weight dependency, CPMC, which can compress a variety of models
efficiently. We design the importance of the feature map in three aspects,
including its associated weight value, computational cost, and parameter
quantity. Use the phenomenon of weight dependency, We get the importance by
assessing its associated filter and the corresponding partial weights of the
next layer. Then we use global normalization to achieve cross-layer comparison.
Our method can compress various CNN models, including VGGNet, ResNet, and
DenseNet, on various image classification datasets. Extensive experiments have
shown CPMC outperforms the others significantly.
</p>
<a href="http://arxiv.org/abs/2011.03240" target="_blank">arXiv:2011.03240</a> [<a href="http://arxiv.org/pdf/2011.03240" target="_blank">pdf</a>]

<h2>Unsupervised Learning of Dense Visual Representations. (arXiv:2011.05499v2 [cs.CV] UPDATED)</h2>
<h3>Pedro O. Pinheiro, Amjad Almahairi, Ryan Y. Benmalek, Florian Golemo, Aaron Courville</h3>
<p>Contrastive self-supervised learning has emerged as a promising approach to
unsupervised visual representation learning. In general, these methods learn
global (image-level) representations that are invariant to different views
(i.e., compositions of data augmentation) of the same image. However, many
visual understanding tasks require dense (pixel-level) representations. In this
paper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised
learning of dense representations. VADeR learns pixelwise representations by
forcing local features to remain constant over different viewing conditions.
Specifically, this is achieved through pixel-level contrastive learning:
matching features (that is, features that describes the same location of the
scene on different views) should be close in an embedding space, while
non-matching features should be apart. VADeR provides a natural representation
for dense prediction tasks and transfers well to downstream tasks. Our method
outperforms ImageNet supervised pretraining (and strong unsupervised baselines)
in multiple dense prediction tasks.
</p>
<a href="http://arxiv.org/abs/2011.05499" target="_blank">arXiv:2011.05499</a> [<a href="http://arxiv.org/pdf/2011.05499" target="_blank">pdf</a>]

<h2>Structured Attention Graphs for Understanding Deep Image Classifications. (arXiv:2011.06733v2 [cs.CV] UPDATED)</h2>
<h3>Vivswan Shitole, Li Fuxin, Minsuk Kahng, Prasad Tadepalli, Alan Fern</h3>
<p>Attention maps are a popular way of explaining the decisions of convolutional
networks for image classification. Typically, for each image of interest, a
single attention map is produced, which assigns weights to pixels based on
their importance to the classification. A single attention map, however,
provides an incomplete understanding since there are often many other maps that
explain a classification equally well. In this paper, we introduce structured
attention graphs (SAGs), which compactly represent sets of attention maps for
an image by capturing how different combinations of image regions impact a
classifier's confidence. We propose an approach to compute SAGs and a
visualization for SAGs so that deeper insight can be gained into a classifier's
decisions. We conduct a user study comparing the use of SAGs to traditional
attention maps for answering counterfactual questions about image
classifications. Our results show that the users are more correct when
answering comparative counterfactual questions based on SAGs compared to the
baselines.
</p>
<a href="http://arxiv.org/abs/2011.06733" target="_blank">arXiv:2011.06733</a> [<a href="http://arxiv.org/pdf/2011.06733" target="_blank">pdf</a>]

<h2>NightVision: Generating Nighttime Satellite Imagery from Infra-Red Observations. (arXiv:2011.07017v2 [cs.CV] UPDATED)</h2>
<h3>Paula Harder, William Jones, Redouane Lguensat, Shahine Bouabid, James Fulton, D&#xe1;nell Quesada-Chac&#xf3;n, Aris Marcolongo, Sofija Stefanovi&#x107;, Yuhan Rao, Peter Manshausen, Duncan Watson-Parris</h3>
<p>The recent explosion in applications of machine learning to satellite imagery
often rely on visible images and therefore suffer from a lack of data during
the night. The gap can be filled by employing available infra-red observations
to generate visible images. This work presents how deep learning can be applied
successfully to create those images by using U-Net based architectures. The
proposed methods show promising results, achieving a structural similarity
index (SSIM) up to 86\% on an independent test set and providing visually
convincing output images, generated from infra-red observations.
</p>
<a href="http://arxiv.org/abs/2011.07017" target="_blank">arXiv:2011.07017</a> [<a href="http://arxiv.org/pdf/2011.07017" target="_blank">pdf</a>]

<h2>BanglaWriting: A multi-purpose offline Bangla handwriting dataset. (arXiv:2011.07499v2 [cs.CV] UPDATED)</h2>
<h3>M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir</h3>
<p>This article presents a Bangla handwriting dataset named BanglaWriting that
contains single-page handwritings of 260 individuals of different personalities
and ages. Each page includes bounding-boxes that bounds each word, along with
the unicode representation of the writing. This dataset contains 21,234 words
and 32,787 characters in total. Moreover, this dataset includes 5,470 unique
words of Bangla vocabulary. Apart from the usual words, the dataset comprises
261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of
the bounding-boxes and word labels are manually-generated. The dataset can be
used for complex optical character/word recognition, writer identification,
handwritten word segmentation, and word generation. Furthermore, this dataset
is suitable for extracting age-based and gender-based variation of handwriting.
</p>
<a href="http://arxiv.org/abs/2011.07499" target="_blank">arXiv:2011.07499</a> [<a href="http://arxiv.org/pdf/2011.07499" target="_blank">pdf</a>]

<h2>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v4 [cs.LG] UPDATED)</h2>
<h3>Hao-Zhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, Wei Chen</h3>
<p>Conventional unsupervised multi-source domain adaptation (UMDA) methods
assume all source domains can be accessed directly. This neglects the
privacy-preserving policy, that is, all the data and computations must be kept
decentralized. There exists three problems in this scenario: (1) Minimizing the
domain distance requires the pairwise calculation of the data from source and
target domains, which is not accessible. (2) The communication cost and privacy
security limit the application of UMDA methods (e.g., the domain adversarial
training). (3) Since users have no authority to check the data quality, the
irrelevant or malicious source domains are more likely to appear, which causes
negative transfer. In this study, we propose a privacy-preserving UMDA paradigm
named Knowledge Distillation based Decentralized Domain Adaptation (KD3A),
which performs domain adaptation through the knowledge distillation on models
from different source domains. KD3A solves the above problems with three
components: (1) A multi-source knowledge distillation method named Knowledge
Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting
strategy named Consensus Focus to identify both the malicious and irrelevant
domains. (3) A decentralized optimization strategy for domain distance named
BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is
robust to the negative transfer and brings a 100x reduction of communication
cost compared with other decentralized UMDA methods. Moreover, our KD3A
significantly outperforms state-of-the-art UMDA approaches.
</p>
<a href="http://arxiv.org/abs/2011.09757" target="_blank">arXiv:2011.09757</a> [<a href="http://arxiv.org/pdf/2011.09757" target="_blank">pdf</a>]

<h2>SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations. (arXiv:2011.10684v4 [cs.LG] UPDATED)</h2>
<h3>Hao-Zhe Feng, Kezhi Kong, Minghao Chen, Tianye Zhang, Minfeng Zhu, Wei Chen</h3>
<p>Semi-supervised variational autoencoders (VAEs) have obtained strong results,
but have also encountered the challenge that good ELBO values do not always
imply accurate inference results. In this paper, we investigate and propose two
causes of this problem: (1) The ELBO objective cannot utilize the label
information directly. (2) A bottleneck value exists and continuing to optimize
ELBO after this value will not improve inference accuracy. On the basis of the
experiment results, we propose SHOT-VAE to address these problems without
introducing additional prior knowledge. The SHOT-VAE offers two contributions:
(1) A new ELBO approximation named smooth-ELBO that integrates the label
predictive loss into ELBO. (2) An approximation based on optimal interpolation
that breaks the ELBO value bottleneck by reducing the margin between ELBO and
the data likelihood. The SHOT-VAE achieves good performance with a 25.30% error
rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on
CIFAR-10 with 4k labels.
</p>
<a href="http://arxiv.org/abs/2011.10684" target="_blank">arXiv:2011.10684</a> [<a href="http://arxiv.org/pdf/2011.10684" target="_blank">pdf</a>]

<h2>Improving Augmentation and Evaluation Schemes for Semantic Image Synthesis. (arXiv:2011.12636v2 [cs.CV] UPDATED)</h2>
<h3>Prateek Katiyar, Anna Khoreva</h3>
<p>Despite data augmentation being a de facto technique for boosting the
performance of deep neural networks, little attention has been paid to
developing augmentation strategies for generative adversarial networks (GANs).
To this end, we introduce a novel augmentation scheme designed specifically for
GAN-based semantic image synthesis models. We propose to randomly warp object
shapes in the semantic label maps used as an input to the generator. The local
shape discrepancies between the warped and non-warped label maps and images
enable the GAN to learn better the structural and geometric details of the
scene and thus to improve the quality of generated images. While benchmarking
the augmented GAN models against their vanilla counterparts, we discover that
the quantification metrics reported in the previous semantic image synthesis
studies are strongly biased towards specific semantic classes as they are
derived via an external pre-trained segmentation network. We therefore propose
to improve the established semantic image synthesis evaluation scheme by
analyzing separately the performance of generated images on the biased and
unbiased classes for the given segmentation network. Finally, we show strong
quantitative and qualitative improvements obtained with our augmentation
scheme, on both class splits, using state-of-the-art semantic image synthesis
models across three different datasets. On average across COCO-Stuff, ADE20K
and Cityscapes datasets, the augmented models outperform their vanilla
counterparts by ~3 mIoU and ~10 FID points.
</p>
<a href="http://arxiv.org/abs/2011.12636" target="_blank">arXiv:2011.12636</a> [<a href="http://arxiv.org/pdf/2011.12636" target="_blank">pdf</a>]

<h2>Self-EMD: Self-Supervised Object Detection without ImageNet. (arXiv:2011.13677v2 [cs.CV] UPDATED)</h2>
<h3>Songtao Liu, Zeming Li, Jian Sun</h3>
<p>In this paper, we propose a novel self-supervised representation learning
method, Self-EMD, for object detection. Our method directly trained on
unlabeled non-iconic image dataset like COCO, instead of commonly used
iconic-object image dataset like ImageNet. We keep the convolutional feature
maps as the image embedding to preserve spatial structures and adopt Earth
Mover's Distance (EMD) to compute the similarity between two embeddings. Our
Faster R-CNN (ResNet50-FPN) baseline achieves 39.8% mAP on COCO, which is on
par with the state of the art self-supervised methods pre-trained on ImageNet.
More importantly, it can be further improved to 40.4% mAP with more unlabeled
images, showing its great potential for leveraging more easily obtained
unlabeled data. Code will be made available.
</p>
<a href="http://arxiv.org/abs/2011.13677" target="_blank">arXiv:2011.13677</a> [<a href="http://arxiv.org/pdf/2011.13677" target="_blank">pdf</a>]

<h2>Distilled Thompson Sampling: Practical and Efficient Thompson Sampling via Imitation Learning. (arXiv:2011.14266v2 [cs.LG] UPDATED)</h2>
<h3>Hongseok Namkoong, Samuel Daulton, Eytan Bakshy</h3>
<p>Thompson sampling (TS) has emerged as a robust technique for contextual
bandit problems. However, TS requires posterior inference and optimization for
action generation, prohibiting its use in many internet applications where
latency and ease of deployment are of concern. We propose a novel
imitation-learning-based algorithm that distills a TS policy into an explicit
policy representation by performing posterior inference and optimization
offline. The explicit policy representation enables fast online decision-making
and easy deployment in mobile and server-based environments. Our algorithm
iteratively performs offline batch updates to the TS policy and learns a new
imitation policy. Since we update the TS policy with observations collected
under the imitation policy, our algorithm emulates an off-policy version of TS.
Our imitation algorithm guarantees Bayes regret comparable to TS, up to the sum
of single-step imitation errors. We show these imitation errors can be made
arbitrarily small when unlabeled contexts are cheaply available, which is the
case for most large-scale internet applications. Empirically, we show that our
imitation policy achieves comparable regret to TS, while reducing decision-time
latency by over an order of magnitude.
</p>
<a href="http://arxiv.org/abs/2011.14266" target="_blank">arXiv:2011.14266</a> [<a href="http://arxiv.org/pdf/2011.14266" target="_blank">pdf</a>]

<h2>Sparse Semi-Supervised Action Recognition with Active Learning. (arXiv:2012.01740v2 [cs.CV] UPDATED)</h2>
<h3>Jingyuan Li, Eli Shlizerman</h3>
<p>Current state-of-the-art methods for skeleton-based action recognition are
supervised and rely on labels. The reliance is limiting the performance due to
the challenges involved in annotation and mislabeled data. Unsupervised methods
have been introduced, however, they organize sequences into clusters and still
require labels to associate clusters with actions. In this paper, we propose a
novel approach for skeleton-based action recognition, called SESAR, that
connects these approaches. SESAR leverages the information from both unlabeled
data and a handful of sequences actively selected for labeling, combining
unsupervised training with sparsely supervised guidance. SESAR is composed of
two main components, where the first component learns a latent representation
for unlabeled action sequences through an Encoder-Decoder RNN which
reconstructs the sequences, and the second component performs active learning
to select sequences to be labeled based on cluster and classification
uncertainty. When the two components are simultaneously trained on
skeleton-based action sequences, they correspond to a robust system for action
recognition with only a handful of labeled samples. We evaluate our system on
common datasets with multiple sequences and actions, such as NW UCLA, NTU RGB+D
60, and UWA3D. Our results outperform standalone skeleton-based supervised,
unsupervised with cluster identification, and active-learning methods for
action recognition when applied to sparse labeled samples, as low as 1% of the
data.
</p>
<a href="http://arxiv.org/abs/2012.01740" target="_blank">arXiv:2012.01740</a> [<a href="http://arxiv.org/pdf/2012.01740" target="_blank">pdf</a>]

<h2>Proceedings of NeurIPS 2020 Workshop on Artificial Intelligence for Humanitarian Assistance and Disaster Response. (arXiv:2012.02108v2 [cs.AI] UPDATED)</h2>
<h3>Ritwik Gupta, Eric T. Heim, Edoardo Nemni</h3>
<p>These are the "proceedings" of the 2nd AI + HADR workshop which was held
virtually on December 12, 2020 as part of the Neural Information Processing
Systems conference. These are non-archival and merely serve as a way to collate
all the papers accepted to the workshop.
</p>
<a href="http://arxiv.org/abs/2012.02108" target="_blank">arXiv:2012.02108</a> [<a href="http://arxiv.org/pdf/2012.02108" target="_blank">pdf</a>]

<h2>Joint Estimation of Image Representations and their Lie Invariants. (arXiv:2012.02903v2 [cs.AI] UPDATED)</h2>
<h3>Christine Allen-Blanchette, Kostas Daniilidis</h3>
<p>Images encode both the state of the world and its content. The former is
useful for tasks such as planning and control, and the latter for
classification. The automatic extraction of this information is challenging
because of the high-dimensionality and entangled encoding inherent to the image
representation. This article introduces two theoretical approaches aimed at the
resolution of these challenges. The approaches allow for the interpolation and
extrapolation of images from an image sequence by joint estimation of the image
representation and the generators of the sequence dynamics. In the first
approach, the image representations are learned using probabilistic PCA
\cite{tipping1999probabilistic}. The linear-Gaussian conditional distributions
allow for a closed form analytical description of the latent distributions but
assumes the underlying image manifold is a linear subspace. In the second
approach, the image representations are learned using probabilistic nonlinear
PCA which relieves the linear manifold assumption at the cost of requiring a
variational approximation of the latent distributions. In both approaches, the
underlying dynamics of the image sequence are modelled explicitly to
disentangle them from the image representations. The dynamics themselves are
modelled with Lie group structure which enforces the desirable properties of
smoothness and composability of inter-image transformations.
</p>
<a href="http://arxiv.org/abs/2012.02903" target="_blank">arXiv:2012.02903</a> [<a href="http://arxiv.org/pdf/2012.02903" target="_blank">pdf</a>]

<h2>iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v2 [cs.AI] UPDATED)</h2>
<h3>Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart&#xed;n-Mart&#xed;n, Linxi Fan, Guanzhi Wang, Shyamal Buch, Claudia D&#x27;Arpino, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Li Fei-Fei, Silvio Savarese</h3>
<p>We present iGibson, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains fifteen fully interactive home-sized scenes populated with
rigid and articulated objects. The scenes are replicas of 3D scanned real-world
homes, aligning the distribution of objects and layout to that of the real
world. iGibson integrates several key features to facilitate the study of
interactive tasks: i) generation of high-quality visual virtual sensor signals
(RGB, depth, segmentation, LiDAR, flow, among others), ii) domain randomization
to change the materials of the objects (both visual texture and dynamics)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of simple human demonstrated behaviors. iGibson is
open-sourced with comprehensive examples and documentation. For more
information, visit our project website: this http URL
</p>
<a href="http://arxiv.org/abs/2012.02924" target="_blank">arXiv:2012.02924</a> [<a href="http://arxiv.org/pdf/2012.02924" target="_blank">pdf</a>]

<h2>Depth estimation from 4D light field videos. (arXiv:2012.03021v2 [cs.CV] UPDATED)</h2>
<h3>Takahiro Kinoshita, Satoshi Ono</h3>
<p>Depth (disparity) estimation from 4D Light Field (LF) images has been a
research topic for the last couple of years. Most studies have focused on depth
estimation from static 4D LF images while not considering temporal information,
i.e., LF videos. This paper proposes an end-to-end neural network architecture
for depth estimation from 4D LF videos. This study also constructs a
medium-scale synthetic 4D LF video dataset that can be used for training deep
learning-based methods. Experimental results using synthetic and real-world 4D
LF videos show that temporal information contributes to the improvement of
depth estimation accuracy in noisy regions. Dataset and code is available at:
https://mediaeng-lfv.github.io/LFV_Disparity_Estimation
</p>
<a href="http://arxiv.org/abs/2012.03021" target="_blank">arXiv:2012.03021</a> [<a href="http://arxiv.org/pdf/2012.03021" target="_blank">pdf</a>]

<h2>PAC-Learning for Strategic Classification. (arXiv:2012.03310v2 [cs.LG] UPDATED)</h2>
<h3>Ravi Sundaram, Anil Vullikanti, Haifeng Xu, Fan Yao</h3>
<p>Machine learning (ML) algorithms may be susceptible to being gamed by
individuals with knowledge of the algorithm (a.k.a. Goodhart's law). Such
concerns have motivated a surge of recent work on strategic classification
where each data point is a self-interested agent and may strategically
manipulate his features to induce a more desirable classification outcome for
himself. Previous works assume agents have homogeneous preferences and all
equally prefer the positive label. This paper generalizes strategic
classification to settings where different data points may have different
preferences over the classification outcomes. Besides a richer model, this
generalization allows us to include evasion attacks in adversarial ML also as a
special case of our model where positive [resp. negative] data points prefer
the negative [resp. positive] label, and thus for the first time allows
strategic and adversarial learning to be studied under the same framework.

We introduce the strategic VC-dimension (SVC), which captures the
PAC-learnability of a hypothesis class in our general strategic setup. SVC
generalizes the notion of adversarial VC-dimension (AVC) introduced recently by
Cullina et al. arXiv:1806.01471. We then instantiate our framework for arguably
the most basic hypothesis class, i.e., linear classifiers. We fully
characterize the statistical learnability of linear classifiers by pinning down
its SVC and the computational tractability by pinning down the complexity of
the empirical risk minimization problem. Our bound of SVC for linear
classifiers also strictly generalizes the AVC bound for linear classifiers in
arXiv:1806.01471. Finally, we briefly study the power of randomization in our
strategic classification setup. We show that randomization may strictly
increase the accuracy in general, but will not help in the special case of
adversarial classification under evasion attacks.
</p>
<a href="http://arxiv.org/abs/2012.03310" target="_blank">arXiv:2012.03310</a> [<a href="http://arxiv.org/pdf/2012.03310" target="_blank">pdf</a>]

<h2>Mapping Network States Using Connectivity Queries. (arXiv:2012.03413v2 [cs.LG] UPDATED)</h2>
<h3>Alexander Rodr&#xed;guez, Bijaya Adhikari, Andr&#xe9;s D. Gonz&#xe1;lez, Charles Nicholson, Anil Vullikanti, B. Aditya Prakash</h3>
<p>Can we infer all the failed components of an infrastructure network, given a
sample of reachable nodes from supply nodes? One of the most critical
post-disruption processes after a natural disaster is to quickly determine the
damage or failure states of critical infrastructure components. However, this
is non-trivial, considering that often only a fraction of components may be
accessible or observable after a disruptive event. Past work has looked into
inferring failed components given point probes, i.e. with a direct sample of
failed components. In contrast, we study the harder problem of inferring failed
components given partial information of some `serviceable' reachable nodes and
a small sample of point probes, being the first often more practical to obtain.
We formulate this novel problem using the Minimum Description Length (MDL)
principle, and then present a greedy algorithm that minimizes MDL cost
effectively. We evaluate our algorithm on domain-expert simulations of real
networks in the aftermath of an earthquake. Our algorithm successfully identify
failed components, especially the critical ones affecting the overall system
performance.
</p>
<a href="http://arxiv.org/abs/2012.03413" target="_blank">arXiv:2012.03413</a> [<a href="http://arxiv.org/pdf/2012.03413" target="_blank">pdf</a>]

<h2>Confidence-aware Non-repetitive Multimodal Transformers for TextCaps. (arXiv:2012.03662v2 [cs.CV] UPDATED)</h2>
<h3>Zhaokai Wang, Renda Bao, Qi Wu, Si Liu</h3>
<p>When describing an image, reading text in the visual scene is crucial to
understand the key information. Recent work explores the TextCaps task, i.e.
image captioning with reading Optical Character Recognition (OCR) tokens, which
requires models to read text and cover them in generated captions. Existing
approaches fail to generate accurate descriptions because of their (1) poor
reading ability; (2) inability to choose the crucial words among all extracted
OCR tokens; (3) repetition of words in predicted captions. To this end, we
propose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to
tackle the above challenges. Our CNMT consists of a reading, a reasoning and a
generation modules, in which Reading Module employs better OCR systems to
enhance text reading ability and a confidence embedding to select the most
noteworthy tokens. To address the issue of word redundancy in captions, our
Generation Module includes a repetition mask to avoid predicting repeated word
in captions. Our model outperforms state-of-the-art models on TextCaps dataset,
improving from 81.0 to 93.0 in CIDEr. Our source code is publicly available.
</p>
<a href="http://arxiv.org/abs/2012.03662" target="_blank">arXiv:2012.03662</a> [<a href="http://arxiv.org/pdf/2012.03662" target="_blank">pdf</a>]

<h2>A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to Regularization. (arXiv:2012.03801v2 [cs.LG] UPDATED)</h2>
<h3>Adepu Ravi Sankar, Yash Khasbage, Rahul Vigneswaran, Vineeth N Balasubramanian</h3>
<p>Loss landscape analysis is extremely useful for a deeper understanding of the
generalization ability of deep neural network models. In this work, we propose
a layerwise loss landscape analysis where the loss surface at every layer is
studied independently and also on how each correlates to the overall loss
surface. We study the layerwise loss landscape by studying the eigenspectra of
the Hessian at each layer. In particular, our results show that the layerwise
Hessian geometry is largely similar to the entire Hessian. We also report an
interesting phenomenon where the Hessian eigenspectrum of middle layers of the
deep neural network are observed to most similar to the overall Hessian
eigenspectrum. We also show that the maximum eigenvalue and the trace of the
Hessian (both full network and layerwise) reduce as training of the network
progresses. We leverage on these observations to propose a new regularizer
based on the trace of the layerwise Hessian. Penalizing the trace of the
Hessian at every layer indirectly forces Stochastic Gradient Descent to
converge to flatter minima, which are shown to have better generalization
performance. In particular, we show that such a layerwise regularizer can be
leveraged to penalize the middlemost layers alone, which yields promising
results. Our empirical studies on well-known deep nets across datasets support
the claims of this work
</p>
<a href="http://arxiv.org/abs/2012.03801" target="_blank">arXiv:2012.03801</a> [<a href="http://arxiv.org/pdf/2012.03801" target="_blank">pdf</a>]

<h2>NeRD: Neural Reflectance Decomposition from Image Collections. (arXiv:2012.03918v2 [cs.CV] UPDATED)</h2>
<h3>Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P.A. Lensch</h3>
<p>Decomposing a scene into its shape, reflectance, and illumination is a
challenging but essential problem in computer vision and graphics. This problem
is inherently more challenging when the illumination is not a single light
source under laboratory conditions but is instead an unconstrained
environmental illumination. Though recent work has shown that implicit
representations can be used to model the radiance field of an object, these
techniques only enable view synthesis and not relighting. Additionally,
evaluating these radiance fields is resource and time-intensive. By decomposing
a scene into explicit representations, any rendering framework can be leveraged
to generate novel views under any illumination in real-time. NeRD is a method
that achieves this decomposition by introducing physically-based rendering to
neural radiance fields. Even challenging non-Lambertian reflectances, complex
geometry, and unknown illumination can be decomposed to high-quality models.
The datasets and code is available at the project page:
https://markboss.me/publication/2021-nerd/
</p>
<a href="http://arxiv.org/abs/2012.03918" target="_blank">arXiv:2012.03918</a> [<a href="http://arxiv.org/pdf/2012.03918" target="_blank">pdf</a>]

<h2>FSL-BM: Fuzzy Supervised Learning with Binary Meta-Feature for Classification. (arXiv:1709.09268v2 [cs.LG] CROSS LISTED)</h2>
<h3>Kamran Kowsari, Nima Bari, Roman Vichr, Farhad A. Goodarzi</h3>
<p>This paper introduces a novel real-time Fuzzy Supervised Learning with Binary
Meta-Feature (FSL-BM) for big data classification task. The study of real-time
algorithms addresses several major concerns, which are namely: accuracy, memory
consumption, and ability to stretch assumptions and time complexity. Attaining
a fast computational model providing fuzzy logic and supervised learning is one
of the main challenges in the machine learning. In this research paper, we
present FSL-BM algorithm as an efficient solution of supervised learning with
fuzzy logic processing using binary meta-feature representation using Hamming
Distance and Hash function to relax assumptions. While many studies focused on
reducing time complexity and increasing accuracy during the last decade, the
novel contribution of this proposed solution comes through integration of
Hamming Distance, Hash function, binary meta-features, binary classification to
provide real time supervised method. Hash Tables (HT) component gives a fast
access to existing indices; and therefore, the generation of new indices in a
constant time complexity, which supersedes existing fuzzy supervised algorithms
with better or comparable results. To summarize, the main contribution of this
technique for real-time Fuzzy Supervised Learning is to represent hypothesis
through binary input as meta-feature space and creating the Fuzzy Supervised
Hash table to train and validate model.
</p>
<a href="http://arxiv.org/abs/1709.09268" target="_blank">arXiv:1709.09268</a> [<a href="http://arxiv.org/pdf/1709.09268" target="_blank">pdf</a>]

