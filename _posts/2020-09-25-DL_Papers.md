---
title: Latest Deep Learning Papers
date: 2020-11-29 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (284 Articles)</h1>
<h2>Reinforcement Learning for Robust Missile Autopilot Design. (arXiv:2011.12956v1 [cs.LG])</h2>
<h3>Bernardo Cortez</h3>
<p>Designing missiles' autopilot controllers has been a complex task, given the
extensive flight envelope and the nonlinear flight dynamics. A solution that
can excel both in nominal performance and in robustness to uncertainties is
still to be found. While Control Theory often debouches into parameters'
scheduling procedures, Reinforcement Learning has presented interesting results
in ever more complex tasks, going from videogames to robotic tasks with
continuous action domains. However, it still lacks clearer insights on how to
find adequate reward functions and exploration strategies. To the best of our
knowledge, this work is pioneer in proposing Reinforcement Learning as a
framework for flight control. In fact, it aims at training a model-free agent
that can control the longitudinal flight of a missile, achieving optimal
performance and robustness to uncertainties. To that end, under TRPO's
methodology, the collected experience is augmented according to HER, stored in
a replay buffer and sampled according to its significance. Not only does this
work enhance the concept of prioritized experience replay into BPER, but it
also reformulates HER, activating them both only when the training progress
converges to suboptimal policies, in what is proposed as the SER methodology.
Besides, the Reward Engineering process is carefully detailed. The results show
that it is possible both to achieve the optimal performance and to improve the
agent's robustness to uncertainties (with low damage on nominal performance) by
further training it in non-nominal environments, therefore validating the
proposed approach and encouraging future research in this field.
</p>
<a href="http://arxiv.org/abs/2011.12956" target="_blank">arXiv:2011.12956</a> [<a href="http://arxiv.org/pdf/2011.12956" target="_blank">pdf</a>]

<h2>PS-DeVCEM: Pathology-sensitive deep learning model for video capsule endoscopy based on weakly labeled data. (arXiv:2011.12957v1 [cs.CV])</h2>
<h3>A. Mohammed, I. Farup, M. Pedersen, S. Yildirim, &#xd8; Hovde</h3>
<p>We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for
frame-level anomaly detection and multi-label classification of different colon
diseases in video capsule endoscopy (VCE) data. Our proposed model is capable
of coping with the key challenge of colon apparent heterogeneity caused by
several types of diseases. Our model is driven by attention-based deep multiple
instance learning and is trained end-to-end on weakly labeled data using video
labels instead of detailed frame-by-frame annotation. The spatial and temporal
features are obtained through ResNet50 and residual Long short-term memory
(residual LSTM) blocks, respectively. Additionally, the learned temporal
attention module provides the importance of each frame to the final label
prediction. Moreover, we developed a self-supervision method to maximize the
distance between classes of pathologies. We demonstrate through qualitative and
quantitative experiments that our proposed weakly supervised learning model
gives superior precision and F1-score reaching, 61.6% and 55.1%, as compared to
three state-of-the-art video analysis methods respectively. We also show our
model's ability to temporally localize frames with pathologies, without frame
annotation information during training. Furthermore, we collected and annotated
the first and largest VCE dataset with only video labels. The dataset contains
455 short video segments with 28,304 frames and 14 classes of colorectal
diseases and artifacts. Dataset and code supporting this publication will be
made available on our home page.
</p>
<a href="http://arxiv.org/abs/2011.12957" target="_blank">arXiv:2011.12957</a> [<a href="http://arxiv.org/pdf/2011.12957" target="_blank">pdf</a>]

<h2>Deep Convolutional Neural Networks: A survey of the foundations, selected improvements, and some current applications. (arXiv:2011.12960v1 [cs.CV])</h2>
<h3>Lars Lien Ankile, Morgan Feet Heggland, Kjartan Krange</h3>
<p>Within the world of machine learning there exists a wide range of different
methods with respective advantages and applications. This paper seeks to
present and discuss one such method, namely Convolutional Neural Networks
(CNNs). CNNs are deep neural networks that use a special linear operation
called convolution. This operation represents a key and distinctive element of
CNNs, and will therefore be the focus of this method paper. The discussion
starts with the theoretical foundations that underlie convolutions and CNNs.
Then, the discussion proceeds to discuss some improvements and augmentations
that can be made to adapt the method to estimate a wider set of function
classes. The paper mainly investigates two ways of improving the method: by
using locally connected layers, which can make the network less invariant to
translation, and tiled convolution, which allows for the learning of more
complex invariances than standard convolution. Furthermore, the use of the Fast
Fourier Transform can improve the computational efficiency of convolution.
Subsequently, this paper discusses two applications of convolution that have
proven to be very effective in practice. First, the YOLO architecture is a
state of the art neural network for image object classification, which
accurately predicts bounding boxes around objects in images. Second, tumor
detection in mammography may be performed using CNNs, accomplishing 7.2% higher
specificity than actual doctors with only .3% less sensitivity. Finally, the
invention of technology that outperforms humans in different fields also raises
certain ethical and regulatory questions that are briefly discussed.
</p>
<a href="http://arxiv.org/abs/2011.12960" target="_blank">arXiv:2011.12960</a> [<a href="http://arxiv.org/pdf/2011.12960" target="_blank">pdf</a>]

<h2>Grafit: Learning fine-grained image representations with coarse labels. (arXiv:2011.12982v1 [cs.CV])</h2>
<h3>Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, Herv&#xe9; J&#xe9;gou</h3>
<p>This paper tackles the problem of learning a finer representation than the
one provided by training labels. This enables fine-grained category retrieval
of images in a collection annotated with coarse labels only.

Our network is learned with a nearest-neighbor classifier objective, and an
instance loss inspired by self-supervised learning. By jointly leveraging the
coarse labels and the underlying fine-grained latent space, it significantly
improves the accuracy of category-level retrieval methods.

Our strategy outperforms all competing methods for retrieving or classifying
images at a finer granularity than that available at train time. It also
improves the accuracy for transfer learning tasks to fine-grained datasets,
thereby establishing the new state of the art on five public benchmarks, like
iNaturalist-2018.
</p>
<a href="http://arxiv.org/abs/2011.12982" target="_blank">arXiv:2011.12982</a> [<a href="http://arxiv.org/pdf/2011.12982" target="_blank">pdf</a>]

<h2>Sign language segmentation with temporal convolutional networks. (arXiv:2011.12986v1 [cs.CV])</h2>
<h3>Katrin Renz, Nicolaj C. Stache, Samuel Albanie, G&#xfc;l Varol</h3>
<p>The objective of this work is to determine the location of temporal
boundaries between signs in continuous sign language videos. Our approach
employs 3D convolutional neural network representations with iterative temporal
segment refinement to resolve ambiguities between sign boundary cues. We
demonstrate the effectiveness of our approach on the BSLCORPUS, PHOENIX14 and
BSL-1K datasets, showing considerable improvement over the prior state of the
art and the ability to generalise to new signers, languages and domains.
</p>
<a href="http://arxiv.org/abs/2011.12986" target="_blank">arXiv:2011.12986</a> [<a href="http://arxiv.org/pdf/2011.12986" target="_blank">pdf</a>]

<h2>Mixed Membership Graph Clustering via Systematic Edge Query. (arXiv:2011.12988v1 [cs.LG])</h2>
<h3>Shahana Ibrahim, Xiao Fu</h3>
<p>This work considers clustering nodes of a largely incomplete graph. Under the
problem setting, only a small amount of queries about the edges can be made,
but the entire graph is not observable. This problem finds applications in
large-scale data clustering using limited annotations, community detection
under restricted survey resources, and graph topology inference under
hidden/removed node interactions. Prior works treated this problem as a convex
optimization-based matrix completion task. However, this line of work is
designed for learning single cluster membership of nodes belonging to disjoint
clusters, yet mixed (multiple) cluster membership nodes and overlapping
clusters often arise in practice. Existing works also rely on the uniformly
random edge query pattern and nuclear norm-based optimization, which give rise
to a number of implementation and scalability challenges. This work aims at
learning mixed membership of the nodes of overlapping clusters using edge
queries. Our method offers membership learning guarantees under systematic
query patterns (as opposed to random ones). The query patterns can be
controlled and adjusted by the system designers to accommodate implementation
challenges---e.g., to avoid querying edges that are physically hard to acquire.
Our framework also features a lightweight and scalable algorithm. Real-data
experiments on crowdclustering and community detection are used to showcase the
effectiveness of our method.
</p>
<a href="http://arxiv.org/abs/2011.12988" target="_blank">arXiv:2011.12988</a> [<a href="http://arxiv.org/pdf/2011.12988" target="_blank">pdf</a>]

<h2>Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v1 [cs.LG])</h2>
<h3>Reena Elangovan, Shubham Jain, Anand Raghunathan</h3>
<p>Precision scaling has emerged as a popular technique to optimize the compute
and storage requirements of Deep Neural Networks (DNNs). Efforts toward
creating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum
precision required to achieve a given network-level accuracy varies
considerably across networks, and even across layers within a network,
requiring support for variable precision in DNN hardware. Previous proposals
such as bit-serial hardware incur high overheads, significantly diminishing the
benefits of lower precision. To efficiently support precision
re-configurability in DNN accelerators, we introduce an approximate computing
method wherein DNN computations are performed block-wise (a block is a group of
bits) and re-configurability is supported at the granularity of blocks. Results
of block-wise computations are composed in an approximate manner to enable
efficient re-configurability. We design a DNN accelerator that embodies
approximate blocked computation and propose a method to determine a suitable
approximation configuration for a given DNN. By varying the approximation
configurations across DNNs, we achieve 1.11x-1.34x and 1.29x-1.6x improvement
in system energy and performance respectively, over an 8-bit fixed-point (FxP8)
baseline, with negligible loss in classification accuracy. Further, by varying
the approximation configurations across layers and data-structures within DNNs,
we achieve 1.14x-1.67x and 1.31x-1.93x improvement in system energy and
performance respectively, with negligible accuracy loss.
</p>
<a href="http://arxiv.org/abs/2011.13000" target="_blank">arXiv:2011.13000</a> [<a href="http://arxiv.org/pdf/2011.13000" target="_blank">pdf</a>]

<h2>PREDATOR: Registration of 3D Point Clouds with Low Overlap. (arXiv:2011.13005v1 [cs.CV])</h2>
<h3>Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas Wieser, Konrad Schindler</h3>
<p>We introduce PREDATOR, a model for pairwise point-cloud registration with
deep attention to the overlap region. Different from previous work, our model
is specifically designed to handle (also) point-cloud pairs with low overlap.
Its key novelty is an overlap-attention block for early information exchange
between the latent encodings of the two point clouds. In this way the
subsequent decoding of the latent representations into per-point features is
conditioned on the respective other point cloud, and thus can predict which
points are not only salient, but also lie in the overlap region between the two
point clouds. The ability to focus on points that are relevant for matching
greatly improves performance: PREDATOR raises the rate of successful
registrations by more than 20% in the low-overlap scenario, and also sets a new
state of the art for the 3DMatch benchmark with 89% registration recall. Source
code and pre-trained models will be available at
https://github.com/ShengyuH/OverlapPredator.
</p>
<a href="http://arxiv.org/abs/2011.13005" target="_blank">arXiv:2011.13005</a> [<a href="http://arxiv.org/pdf/2011.13005" target="_blank">pdf</a>]

<h2>A Simulated Annealing Algorithm for Joint Stratification and Sample Allocation Designs. (arXiv:2011.13006v1 [cs.AI])</h2>
<h3>Mervyn O&#x27;Luing, Steven Prestwich, S. Armagan Tarim</h3>
<p>This study combined simulated annealing with delta evaluation to solve the
joint stratification and sample allocation problem. In this problem, atomic
strata are partitioned into mutually exclusive and collectively exhaustive
strata. Each stratification is a solution, the quality of which is measured by
its cost. The Bell number of possible solutions is enormous for even a moderate
number of atomic strata and an additional layer of complexity is added with the
evaluation time of each solution. Many larger scale combinatorial optimisation
problems cannot be solved to optimality because the search for an optimum
solution requires a prohibitive amount of computation time; a number of local
search heuristic algorithms have been designed for this problem but these can
become trapped in local minima preventing any further improvements. We add to
the existing suite of local search algorithms with a simulated annealing
algorithm that allows for an escape from local minima and uses delta evaluation
to exploit the similarity between consecutive solutions and thereby reduce the
evaluation time.
</p>
<a href="http://arxiv.org/abs/2011.13006" target="_blank">arXiv:2011.13006</a> [<a href="http://arxiv.org/pdf/2011.13006" target="_blank">pdf</a>]

<h2>Advancing diagnostic performance and clinical usability of neural networks via adversarial training and dual batch normalization. (arXiv:2011.13011v1 [cs.LG])</h2>
<h3>Tianyu Han, Sven Nebelung, Federico Pedersoli, Markus Zimmermann, Maximilian Schulze-Hagen, Michael Ho, Christoph Haarburger, Fabian Kiessling, Christiane Kuhl, Volkmar Schulz, Daniel Truhn</h3>
<p>Unmasking the decision-making process of machine learning models is essential
for implementing diagnostic support systems in clinical practice. Here, we
demonstrate that adversarially trained models can significantly enhance the
usability of pathology detection as compared to their standard counterparts. We
let six experienced radiologists rate the interpretability of saliency maps in
datasets of X-rays, computed tomography, and magnetic resonance imaging scans.
Significant improvements were found for our adversarial models, which could be
further improved by the application of dual batch normalization. Contrary to
previous research on adversarially trained models, we found that the accuracy
of such models was equal to standard models when sufficiently large datasets
and dual batch norm training were used. To ensure transferability, we
additionally validated our results on an external test set of 22,433 X-rays.
These findings elucidate that different paths for adversarial and real images
are needed during training to achieve state of the art results with superior
clinical interpretability.
</p>
<a href="http://arxiv.org/abs/2011.13011" target="_blank">arXiv:2011.13011</a> [<a href="http://arxiv.org/pdf/2011.13011" target="_blank">pdf</a>]

<h2>Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation. (arXiv:2011.13026v1 [cs.CV])</h2>
<h3>Davis Wertheimer, Omid Poursaeed, Bharath Hariharan</h3>
<p>We aim to build image generation models that generalize to new domains from
few examples. To this end, we first investigate the generalization properties
of classic image generators, and discover that autoencoders generalize
extremely well to new domains, even when trained on highly constrained data. We
leverage this insight to produce a robust, unsupervised few-shot image
generation algorithm, and introduce a novel training procedure based on
recovering an image from data augmentations. Our Augmentation-Interpolative
AutoEncoders synthesize realistic images of novel objects from only a few
reference images, and outperform both prior interpolative models and supervised
few-shot image generators. Our procedure is simple and lightweight, generalizes
broadly, and requires no category labels or other supervision during training.
</p>
<a href="http://arxiv.org/abs/2011.13026" target="_blank">arXiv:2011.13026</a> [<a href="http://arxiv.org/pdf/2011.13026" target="_blank">pdf</a>]

<h2>Accommodating Picky Customers: Regret Bound and Exploration Complexity for Multi-Objective Reinforcement Learning. (arXiv:2011.13034v1 [cs.LG])</h2>
<h3>Jingfeng Wu, Vladimir Braverman, Lin F. Yang</h3>
<p>In this paper we consider multi-objective reinforcement learning where the
objectives are balanced using preferences. In practice, the preferences are
often given in an adversarial manner, e.g., customers can be picky in many
applications. We formalize this problem as an episodic learning problem on a
Markov decision process, where transitions are unknown and a reward function is
the inner product of a preference vector with pre-specified multi-objective
reward functions. In the online setting, the agent receives a (adversarial)
preference every episode and proposes policies to interact with the
environment. We provide a model-based algorithm that achieves a regret bound
$\widetilde{\mathcal{O}}\left({\sqrt{\min\{d,S\}\cdot H^3 SAK}}\right)$, where
$d$ is the number of objectives, $S$ is the number of states, $A$ is the number
of actions, $H$ is the length of the horizon, and $K$ is the number of
episodes. Furthermore, we consider preference-free exploration, i.e., the agent
first interacts with the environment without specifying any preference and then
is able to accommodate arbitrary preference vectors up to $\epsilon$ error. Our
proposed algorithm is provably efficient with a nearly optimal sample
complexity $\widetilde{\mathcal{O}}\left({\frac{\min\{d,S\}\cdot H^4
SA}{\epsilon^2}}\right)$.
</p>
<a href="http://arxiv.org/abs/2011.13034" target="_blank">arXiv:2011.13034</a> [<a href="http://arxiv.org/pdf/2011.13034" target="_blank">pdf</a>]

<h2>RetroGNN: Approximating Retrosynthesis by Graph Neural Networks for De Novo Drug Design. (arXiv:2011.13042v1 [cs.LG])</h2>
<h3>Cheng-Hao Liu, Maksym Korablyov, Stanis&#x142;aw Jastrz&#x119;bski, Pawe&#x142; W&#x142;odarczyk-Pruszy&#x144;ski, Yoshua Bengio, Marwin H. S. Segler</h3>
<p>De novo molecule generation often results in chemically unfeasible molecules.
A natural idea to mitigate this problem is to bias the search process towards
more easily synthesizable molecules using a proxy for synthetic accessibility.
However, using currently available proxies still results in highly unrealistic
compounds. We investigate the feasibility of training deep graph neural
networks to approximate the outputs of a retrosynthesis planning software, and
their use to bias the search process. We evaluate our method on a benchmark
involving searching for drug-like molecules with antibiotic properties.
Compared to enumerating over five million existing molecules from the ZINC
database, our approach finds molecules predicted to be more likely to be
antibiotics while maintaining good drug-like properties and being easily
synthesizable. Importantly, our deep neural network can successfully filter out
hard to synthesize molecules while achieving a $10^5$ times speed-up over using
the retrosynthesis planning software.
</p>
<a href="http://arxiv.org/abs/2011.13042" target="_blank">arXiv:2011.13042</a> [<a href="http://arxiv.org/pdf/2011.13042" target="_blank">pdf</a>]

<h2>Learning to Infer Shape Programs Using Latent Execution Self Training. (arXiv:2011.13045v1 [cs.CV])</h2>
<h3>Homer Walke, R. Kenny Jones, Daniel Ritchie</h3>
<p>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, enabling shape editing, and more. Supervised learning is hard to
apply to this problem, as paired (program, shape) data rarely exists. Recent
approaches use supervised pre-training with randomly-generated programs and
then refine using self-supervised learning. But self-supervised learning either
requires that the program execution process be differentiable or relies on
reinforcement learning, which is unstable and slow to converge. In this paper,
we present a new approach for learning to infer shape programs, which we call
latent execution self training (LEST). As with recent prior work, LEST starts
by training on randomly-generated (program, shape) pairs. As its name implies,
it is based on the idea of self-training: running a model on unlabeled input
shapes, treating the predicted programs as ground truth latent labels, and
training again. Self-training is known to be susceptible to local minima. LEST
circumvents this problem by leveraging the fact that predicted latent programs
are executable: for a given shape $\mathbf{x}^* \in S^*$ and its predicted
program $\mathbf{z} \in P$, we execute $\mathbf{z}$ to obtain a shape
$\mathbf{x} \in S$ and train on $(\mathbf{z} \in P, \mathbf{x} \in S)$ pairs,
rather than $(\mathbf{z} \in P, \mathbf{x}^* \in S^*)$ pairs. Experiments show
that the distribution of executed shapes $S$ converges toward the distribution
of real shapes $S^*$. We establish connections between LEST and algorithms for
learning generative models, including variational Bayes, wake sleep, and
expectation maximization. For constructive solid geometry and assembly-based
modeling, LEST's inferred programs converge to high reconstruction accuracy
significantly faster than those of reinforcement learning.
</p>
<a href="http://arxiv.org/abs/2011.13045" target="_blank">arXiv:2011.13045</a> [<a href="http://arxiv.org/pdf/2011.13045" target="_blank">pdf</a>]

<h2>Can Temporal Information Help with Contrastive Self-Supervised Learning?. (arXiv:2011.13046v1 [cs.CV])</h2>
<h3>Yutong Bai, Haoqi Fan, Ishan Misra, Ganesh Venkatesh, Yongyi Lu, Yuyin Zhou, Qihang Yu, Vikas Chandra, Alan Yuille</h3>
<p>Leveraging temporal information has been regarded as essential for developing
video understanding models. However, how to properly incorporate temporal
information into the recent successful instance discrimination based
contrastive self-supervised learning (CSL) framework remains unclear. As an
intuitive solution, we find that directly applying temporal augmentations does
not help, or even impair video CSL in general. This counter-intuitive
observation motivates us to re-design existing video CSL frameworks, for better
integration of temporal knowledge.

To this end, we present Temporal-aware Contrastive self-supervised
learningTaCo, as a general paradigm to enhance video CSL. Specifically, TaCo
selects a set of temporal transformations not only as strong data augmentation
but also to constitute extra self-supervision for video understanding. By
jointly contrasting instances with enriched temporal transformations and
learning these transformations as self-supervised signals, TaCo can
significantly enhance unsupervised video representation learning. For instance,
TaCo demonstrates consistent improvement in downstream classification tasks
over a list of backbones and CSL approaches. Our best model achieves 85.1%
(UCF-101) and 51.6% (HMDB-51) top-1 accuracy, which is a 3% and 2.4% relative
improvement over the previous state-of-the-art.
</p>
<a href="http://arxiv.org/abs/2011.13046" target="_blank">arXiv:2011.13046</a> [<a href="http://arxiv.org/pdf/2011.13046" target="_blank">pdf</a>]

<h2>Squared $\ell_2$ Norm as Consistency Loss for Leveraging Augmented Data to Learn Robust and Invariant Representations. (arXiv:2011.13052v1 [cs.LG])</h2>
<h3>Haohan Wang, Zeyi Huang, Xindi Wu, Eric P. Xing</h3>
<p>Data augmentation is one of the most popular techniques for improving the
robustness of neural networks. In addition to directly training the model with
original samples and augmented samples, a torrent of methods regularizing the
distance between embeddings/representations of the original samples and their
augmented counterparts have been introduced. In this paper, we explore these
various regularization choices, seeking to provide a general understanding of
how we should regularize the embeddings. Our analysis suggests the ideal
choices of regularization correspond to various assumptions. With an invariance
test, we argue that regularization is important if the model is to be used in a
broader context than the accuracy-driven setting because non-regularized
approaches are limited in learning the concept of invariance, despite equally
high accuracy. Finally, we also show that the generic approach we identified
(squared $\ell_2$ norm regularized augmentation) outperforms several recent
methods, which are each specially designed for one task and significantly more
complicated than ours, over three different tasks.
</p>
<a href="http://arxiv.org/abs/2011.13052" target="_blank">arXiv:2011.13052</a> [<a href="http://arxiv.org/pdf/2011.13052" target="_blank">pdf</a>]

<h2>How to train your conditional GAN: An approach using geometrically structured latent manifolds. (arXiv:2011.13055v1 [cs.CV])</h2>
<h3>Sameera Ramasinghe, Moshiur Farazi, Salman Khan, Nick Barnes, Stepehn Gould</h3>
<p>Conditional generative modeling typically requires capturing one-to-many
mappings between the inputs and outputs. However, vanilla conditional GANs
(cGAN) tend to ignore the variations of the latent seeds which results in
mode-collapse. As a solution, recent works have moved towards comparatively
expensive models for generating diverse outputs in a conditional setting. In
this paper, we argue that the limited diversity of the vanilla cGANs is not due
to a lack of capacity, but a result of non-optimal training schemes. We tackle
this problem from a geometrical perspective and propose a novel training
mechanism that increases both the diversity and the visual quality of the
vanilla cGAN. The proposed solution does not demand architectural modifications
and paves the way for more efficient architectures that target conditional
generation in multi-modal spaces. We validate the efficacy of our model against
a diverse set of tasks and show that the proposed solution is generic and
effective across multiple datasets.
</p>
<a href="http://arxiv.org/abs/2011.13055" target="_blank">arXiv:2011.13055</a> [<a href="http://arxiv.org/pdf/2011.13055" target="_blank">pdf</a>]

<h2>Effective Sample Pair Generation for Ultrasound Video Contrastive Representation Learning. (arXiv:2011.13066v1 [cs.CV])</h2>
<h3>Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong, Yongfang Luo, Xiang Wan</h3>
<p>Most deep neural networks (DNNs) based ultrasound (US) medical image analysis
models use pretrained backbones (e.g., ImageNet) for better model
generalization. However, the domain gap between natural and medical images
causes an inevitable performance bottleneck when applying to US image analysis.
Our idea is to pretrain DNNs on US images directly to avoid this bottleneck.
Due to the lack of annotated large-scale datasets of US images, we first
construct a new large-scale US video-based image dataset named US-4, containing
over 23,000 high-resolution images from four US video sub-datasets, where two
sub-datasets are newly collected by our local experienced doctors. To make full
use of this dataset, we then innovatively propose an US semi-supervised
contrastive learning (USCL) method to effectively learn feature representations
of US images, with a new sample pair generation (SPG) scheme to tackle the
problem that US images extracted from videos have high similarities. Moreover,
the USCL treats contrastive loss as a consistent regularization, which boosts
the performance of pretrained backbones by combining the supervised loss in a
mutually reinforcing way. Extensive experiments on down-stream tasks'
fine-tuning show the superiority of our approach against ImageNet pretraining
and pretraining using previous state-of-the-art semi-supervised learning
approaches. In particular, our pretrained backbone gets fine-tuning accuracy of
over 94%, which is 9% higher than 85% of the ImageNet pretrained model on the
widely used POCUS dataset. The constructed US-4 dataset and source codes of
this work will be made public.
</p>
<a href="http://arxiv.org/abs/2011.13066" target="_blank">arXiv:2011.13066</a> [<a href="http://arxiv.org/pdf/2011.13066" target="_blank">pdf</a>]

<h2>Hawkes Processes Modeling, Inference and Control: An Overview. (arXiv:2011.13073v1 [cs.LG])</h2>
<h3>Rafael Lima</h3>
<p>Hawkes Processes are a type of point process which models self-excitement
among time events. It has been used in a myriad of applications, ranging from
finance and earthquakes to crime rates and social network activity
analysis.Recently, a surge of different tools and algorithms have showed their
way up to top-tier Machine Learning conferences. This work aims to give a broad
view of the recent advances on the Hawkes Processes modeling and inference to a
newcomer to the field.
</p>
<a href="http://arxiv.org/abs/2011.13073" target="_blank">arXiv:2011.13073</a> [<a href="http://arxiv.org/pdf/2011.13073" target="_blank">pdf</a>]

<h2>Omni-GAN: On the Secrets of cGANs and Beyond. (arXiv:2011.13074v1 [cs.CV])</h2>
<h3>Peng Zhou, Lingxi Xie, Bingbing Ni, Qi Tian</h3>
<p>It has been an important problem to design a proper discriminator for
conditional generative adversarial networks (cGANs). In this paper, we
investigate two popular choices, the projection-based and classification-based
discriminators, and reveal that both of them suffer some kind of drawbacks that
affect the learning ability of cGANs. Then, we present our solution that trains
a powerful discriminator and avoids over-fitting with regularization. In
addition, we unify multiple targets (class, domain, reality, etc.) into one
loss function to enable a wider range of applications. Our algorithm, named
\textbf{Omni-GAN}, achieves competitive performance on a few popular
benchmarks. More importantly, Omni-GAN enjoys both high generation quality and
low risks in mode collapse, offering new possibilities for optimizing
cGANs.Code is available at \url{https://github.com/PeterouZh/Omni-GAN-PyTorch}.
</p>
<a href="http://arxiv.org/abs/2011.13074" target="_blank">arXiv:2011.13074</a> [<a href="http://arxiv.org/pdf/2011.13074" target="_blank">pdf</a>]

<h2>Non-Rigid Puzzles. (arXiv:2011.13076v1 [cs.CV])</h2>
<h3>Or Litany, Emanuele Rodol&#xe0;, Alex Bronstein, Michael Bronstein, Daniel Cremers</h3>
<p>Shape correspondence is a fundamental problem in computer graphics and
vision, with applications in various problems including animation, texture
mapping, robotic vision, medical imaging, archaeology and many more. In
settings where the shapes are allowed to undergo non-rigid deformations and
only partial views are available, the problem becomes very challenging. To this
end, we present a non-rigid multi-part shape matching algorithm. We assume to
be given a reference shape and its multiple parts undergoing a non-rigid
deformation. Each of these query parts can be additionally contaminated by
clutter, may overlap with other parts, and there might be missing parts or
redundant ones. Our method simultaneously solves for the segmentation of the
reference model, and for a dense correspondence to (subsets of) the parts.
Experimental results on synthetic as well as real scans demonstrate the
effectiveness of our method in dealing with this challenging matching scenario.
</p>
<a href="http://arxiv.org/abs/2011.13076" target="_blank">arXiv:2011.13076</a> [<a href="http://arxiv.org/pdf/2011.13076" target="_blank">pdf</a>]

<h2>Functional Time Series Forecasting: Functional Singular Spectrum Analysis Approaches. (arXiv:2011.13077v1 [stat.ML])</h2>
<h3>Jordan Trinka, Hossein Haghbin, Mehdi Maadooliat</h3>
<p>In this paper, we propose two nonparametric methods used in the forecasting
of functional time-dependent data, namely functional singular spectrum analysis
recurrent forecasting and vector forecasting. Both algorithms utilize the
results of functional singular spectrum analysis and past observations in order
to predict future data points where recurrent forecasting predicts one function
at a time and the vector forecasting predicts functional vectors. We compare
our forecasting methods to a gold standard algorithm used in the prediction of
functional, time-dependent data by way of simulation and real data and we find
our techniques do better for periodic stochastic processes.
</p>
<a href="http://arxiv.org/abs/2011.13077" target="_blank">arXiv:2011.13077</a> [<a href="http://arxiv.org/pdf/2011.13077" target="_blank">pdf</a>]

<h2>Photoacoustic Reconstruction Using Sparsity in Curvelet Frame. (arXiv:2011.13080v1 [cs.CV])</h2>
<h3>Bolin Pan, Simon R. Arridge, Felix Lucka, Ben T. Cox, Nam Huynh, Paul C. Beard, Edward Z. Zhang, Marta M. Betcke</h3>
<p>We compare two approaches to photoacoustic image reconstruction from
compressed/subsampled photoacoustic data based on assumption of sparsity in the
Curvelet frame: DR, a two step approach based on the recovery of the complete
volume of the photoacoustic data from the subsampled data followed by the
acoustic inversion, and p0R, a one step approach where the photoacoustic image
(the initial pressure, p0) is directly recovered from the subsampled data. For
representation of the photoacoustic data, we propose a modification of the
Curvelet transform corresponding to the restriction to the range of the
photoacoustic forward operator. Both recovery problems are formulated in a
variational framework. As the Curvelet frame is heavily overdetermined, we use
reweighted l1 norm penalties to enhance the sparsity of the solution. The data
reconstruction problem DR is a standard compressed sensing recovery problem,
which we solve using an ADMM-type algorithm, SALSA. Subsequently, the initial
pressure is recovered using time reversal as implemented in the k-Wave Toolbox.
The p0 reconstruction problem, p0R, aims to recover the photoacoustic image
directly via FISTA, or ADMM when in addition including a non-negativity
constraint. We compare and discuss the relative merits of the two approaches
and illustrate them on 2D simulated and 3D real data.
</p>
<a href="http://arxiv.org/abs/2011.13080" target="_blank">arXiv:2011.13080</a> [<a href="http://arxiv.org/pdf/2011.13080" target="_blank">pdf</a>]

<h2>Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes. (arXiv:2011.13084v1 [cs.CV])</h2>
<h3>Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang</h3>
<p>We present a method to perform novel view and time synthesis of dynamic
scenes, requiring only a monocular video with known camera poses as input. To
do this, we introduce Neural Scene Flow Fields, a new representation that
models the dynamic scene as a time-variant continuous function of appearance,
geometry, and 3D scene motion. Our representation is optimized through a neural
network to fit the observed input views. We show that our representation can be
used for complex dynamic scenes, including thin structures, view-dependent
effects, and natural degrees of motion. We conduct a number of experiments that
demonstrate our approach significantly outperforms recent monocular view
synthesis methods, and show qualitative results of space-time view synthesis on
a variety of real-world videos.
</p>
<a href="http://arxiv.org/abs/2011.13084" target="_blank">arXiv:2011.13084</a> [<a href="http://arxiv.org/pdf/2011.13084" target="_blank">pdf</a>]

<h2>The Evolution of Concept-Acquisition based on Developmental Psychology. (arXiv:2011.13089v1 [cs.AI])</h2>
<h3>Hui Wei</h3>
<p>A conceptual system with rich connotation is key to improving the performance
of knowledge-based artificial intelligence systems. While a conceptual system,
which has abundant concepts and rich semantic relationships, and is
developable, evolvable, and adaptable to multi-task environments, its actual
construction is not only one of the major challenges of knowledge engineering,
but also the fundamental goal of research on knowledge and conceptualization.
Finding a new method to represent concepts and construct a conceptual system
will therefore greatly improve the performance of many intelligent systems.
Fortunately the core of human cognition is a system with relatively complete
concepts and a mechanism that ensures the establishment and development of the
system. The human conceptual system can not be achieved immediately, but rather
must develop gradually. Developmental psychology carefully observes the process
of concept acquisition in humans at the behavioral level, and along with
cognitive psychology has proposed some rough explanations of those
observations. However, due to the lack of research in aspects such as
representation, systematic models, algorithm details and realization, many of
the results of developmental psychology have not been applied directly to the
building of artificial conceptual systems. For example, Karmiloff-Smith's
Representation Redescription (RR) supposition reflects a concept-acquisition
process that re-describes a lower level representation of a concept to a higher
one. This paper is inspired by this developmental psychology viewpoint. We use
an object-oriented approach to re-explain and materialize RR supposition from
the formal semantic perspective, because the OO paradigm is a natural way to
describe the outside world, and it also has strict grammar regulations.
</p>
<a href="http://arxiv.org/abs/2011.13089" target="_blank">arXiv:2011.13089</a> [<a href="http://arxiv.org/pdf/2011.13089" target="_blank">pdf</a>]

<h2>Predictive PER: Balancing Priority and Diversity towards Stable Deep Reinforcement Learning. (arXiv:2011.13093v1 [cs.LG])</h2>
<h3>Sanghwa Lee, Jaeyoung Lee, Ichiro Hasuo</h3>
<p>Prioritized experience replay (PER) samples important transitions, rather
than uniformly, to improve the performance of a deep reinforcement learning
agent. We claim that such prioritization has to be balanced with sample
diversity for making the DQN stabilized and preventing forgetting. Our proposed
improvement over PER, called Predictive PER (PPER), takes three countermeasures
(TDInit, TDClip, TDPred) to (i) eliminate priority outliers and explosions and
(ii) improve the sample diversity and distributions, weighted by priorities,
both leading to stabilizing the DQN. The most notable among the three is the
introduction of the second DNN called TDPred to generalize the in-distribution
priorities. Ablation study and full experiments with Atari games show that each
countermeasure by its own way and PPER contribute to successfully enhancing
stability and thus performance over PER.
</p>
<a href="http://arxiv.org/abs/2011.13093" target="_blank">arXiv:2011.13093</a> [<a href="http://arxiv.org/pdf/2011.13093" target="_blank">pdf</a>]

<h2>Combinatorial Bayesian Optimization with Random Mapping Functions to Convex Polytope. (arXiv:2011.13094v1 [stat.ML])</h2>
<h3>Jungtaek Kim, Minsu Cho, Seungjin Choi</h3>
<p>Bayesian optimization is a popular method for solving the problem of global
optimization of an expensive-to-evaluate black-box function. It relies on a
probabilistic surrogate model of the objective function, upon which an
acquisition function is built to determine where next to evaluate the objective
function. In general, Bayesian optimization with Gaussian process regression
operates on a continuous space. When input variables are categorical or
discrete, an extra care is needed. A common approach is to use one-hot encoded
or Boolean representation for categorical variables which might yield a {\em
combinatorial explosion} problem. In this paper we present a method for
Bayesian optimization in a combinatorial space, which can operate well in a
large combinatorial space. The main idea is to use a random mapping which
embeds the combinatorial space into a convex polytope in a continuous space, on
which all essential process is performed to determine a solution to the
black-box optimization in the combinatorial space. We describe our {\em
combinatorial Bayesian optimization} algorithm and present its regret analysis.
Numerical experiments demonstrate that our method outperforms existing methods.
</p>
<a href="http://arxiv.org/abs/2011.13094" target="_blank">arXiv:2011.13094</a> [<a href="http://arxiv.org/pdf/2011.13094" target="_blank">pdf</a>]

<h2>Automatic Detection of Cardiac Chambers Using an Attention-based YOLOv4 Framework from Four-chamber View of Fetal Echocardiography. (arXiv:2011.13096v1 [cs.CV])</h2>
<h3>Sibo Qiao, Shanchen Pang, Gang Luo, Silin Pan, Xun Wang, Min Wang, Xue Zhai, Taotao Chen</h3>
<p>Echocardiography is a powerful prenatal examination tool for early diagnosis
of fetal congenital heart diseases (CHDs). The four-chamber (FC) view is a
crucial and easily accessible ultrasound (US) image among echocardiography
images. Automatic analysis of FC views contributes significantly to the early
diagnosis of CHDs. The first step to automatically analyze fetal FC views is
locating the fetal four crucial chambers of heart in a US image. However, it is
a greatly challenging task due to several key factors, such as numerous
speckles in US images, the fetal cardiac chambers with small size and unfixed
positions, and category indistinction caused by the similarity of cardiac
chambers. These factors hinder the process of capturing robust and
discriminative features, hence destroying fetal cardiac anatomical chambers
precise localization. Therefore, we first propose a multistage residual hybrid
attention module (MRHAM) to improve the feature learning. Then, we present an
improved YOLOv4 detection model, namely MRHAM-YOLOv4-Slim. Specially, the
residual identity mapping is replaced with the MRHAM in the backbone of
MRHAM-YOLOv4-Slim, accurately locating the four important chambers in fetal FC
views. Extensive experiments demonstrate that our proposed method outperforms
current state-of-the-art, including the precision of 0.890, the recall of
0.920, the F1 score of 0.907 and the mAP of 0.913.
</p>
<a href="http://arxiv.org/abs/2011.13096" target="_blank">arXiv:2011.13096</a> [<a href="http://arxiv.org/pdf/2011.13096" target="_blank">pdf</a>]

<h2>An End-to-end Deep Reinforcement Learning Approach for the Long-term Short-term Planning on the Frenet Space. (arXiv:2011.13098v1 [cs.RO])</h2>
<h3>Majid Moghadam, Ali Alizadeh, Engin Tekin, Gabriel Hugh Elkaim</h3>
<p>Tactical decision making and strategic motion planning for autonomous highway
driving are challenging due to the complication of predicting other road users'
behaviors, diversity of environments, and complexity of the traffic
interactions. This paper presents a novel end-to-end continuous deep
reinforcement learning approach towards autonomous cars' decision-making and
motion planning. For the first time, we define both states and action spaces on
the Frenet space to make the driving behavior less variant to the road
curvatures than the surrounding actors' dynamics and traffic interactions. The
agent receives time-series data of past trajectories of the surrounding
vehicles and applies convolutional neural networks along the time channels to
extract features in the backbone. The algorithm generates continuous
spatiotemporal trajectories on the Frenet frame for the feedback controller to
track. Extensive high-fidelity highway simulations on CARLA show the
superiority of the presented approach compared with commonly used baselines and
discrete reinforcement learning on various traffic scenarios. Furthermore, the
proposed method's advantage is confirmed with a more comprehensive performance
evaluation against 1000 randomly generated test scenarios.
</p>
<a href="http://arxiv.org/abs/2011.13098" target="_blank">arXiv:2011.13098</a> [<a href="http://arxiv.org/pdf/2011.13098" target="_blank">pdf</a>]

<h2>An Autonomous Driving Framework for Long-term Decision-making and Short-term Trajectory Planning on Frenet Space. (arXiv:2011.13099v1 [cs.RO])</h2>
<h3>Majid Moghadam, Gabriel Hugh Elkaim</h3>
<p>In this paper, we present a hierarchical framework for decision-making and
planning on highway driving tasks. We utilized intelligent driving models (IDM
and MOBIL) to generate long-term decisions based on the traffic situation
flowing around the ego. The decisions both maximize ego performance while
respecting other vehicles' objectives. Short-term trajectory optimization is
performed on the Frenet space to make the calculations invariant to the road's
three-dimensional curvatures. A novel obstacle avoidance approach is introduced
on the Frenet frame for the moving obstacles. The optimization explores the
driving corridors to generate spatiotemporal polynomial trajectories to
navigate through the traffic safely and obey the BP commands. The framework
also introduces a heuristic supervisor that identifies unexpected situations
and recalculates each module in case of a potential emergency. Experiments in
CARLA simulation have shown the potential and the scalability of the framework
in implementing various driving styles that match human behavior.
</p>
<a href="http://arxiv.org/abs/2011.13099" target="_blank">arXiv:2011.13099</a> [<a href="http://arxiv.org/pdf/2011.13099" target="_blank">pdf</a>]

<h2>Regret Bounds for Adaptive Nonlinear Control. (arXiv:2011.13101v1 [cs.LG])</h2>
<h3>Nicholas M. Boffi, Stephen Tu, Jean-Jacques E. Slotine</h3>
<p>We study the problem of adaptively controlling a known discrete-time
nonlinear system subject to unmodeled disturbances. We prove the first
finite-time regret bounds for adaptive nonlinear control with matched
uncertainty in the stochastic setting, showing that the regret suffered by
certainty equivalence adaptive control, compared to an oracle controller with
perfect knowledge of the unmodeled disturbances, is upper bounded by
$\widetilde{O}(\sqrt{T})$ in expectation. Furthermore, we show that when the
input is subject to a $k$ timestep delay, the regret degrades to
$\widetilde{O}(k \sqrt{T})$. Our analysis draws connections between classical
stability notions in nonlinear control theory (Lyapunov stability and
contraction theory) and modern regret analysis from online convex optimization.
The use of stability theory allows us to analyze the challenging
infinite-horizon single trajectory setting.
</p>
<a href="http://arxiv.org/abs/2011.13101" target="_blank">arXiv:2011.13101</a> [<a href="http://arxiv.org/pdf/2011.13101" target="_blank">pdf</a>]

<h2>Motion Control for Mobile Robot Navigation Using Machine Learning: a Survey. (arXiv:2011.13112v1 [cs.RO])</h2>
<h3>Xuesu Xiao, Bo Liu, Garrett Warnell, Peter Stone</h3>
<p>Moving in complex environments is an essential capability of intelligent
mobile robots. Decades of research and engineering have been dedicated to
developing sophisticated navigation systems to move mobile robots from one
point to another. Despite their overall success, a recently emerging research
thrust is devoted to developing machine learning techniques to address the same
problem, based in large part on the success of deep learning. However, to date,
there has not been much direct comparison between the classical and emerging
paradigms to this problem. In this article, we survey recent works that apply
machine learning for motion control in mobile robot navigation, within the
context of classical navigation systems. The surveyed works are classified into
different categories, which delineate the relationship of the learning
approaches to classical methods. Based on this classification, we identify
common challenges and promising future directions.
</p>
<a href="http://arxiv.org/abs/2011.13112" target="_blank">arXiv:2011.13112</a> [<a href="http://arxiv.org/pdf/2011.13112" target="_blank">pdf</a>]

<h2>Polka Lines: Learning Structured Illumination and Reconstruction for Active Stereo. (arXiv:2011.13117v1 [cs.CV])</h2>
<h3>Seung-Hwan Baek, Felix Heide</h3>
<p>Active stereo cameras that recover depth from structured light captures have
become a cornerstone sensor modality for 3D scene reconstruction and
understanding tasks across application domains. Existing active stereo cameras
project a pseudo-random dot pattern on object surfaces to extract disparity
independently of object texture. Such hand-crafted patterns are designed in
isolation from the scene statistics, ambient illumination conditions, and the
reconstruction method. In this work, we propose the first method to jointly
learn structured illumination and reconstruction, parameterized by a
diffractive optical element and a neural network, in an end-to-end fashion. To
this end, we introduce a novel differentiable image formation model for active
stereo, relying on both wave and geometric optics, and a novel trinocular
reconstruction network. The jointly optimized pattern, which we dub "Polka
Lines," together with the reconstruction network, achieve state-of-the-art
active-stereo depth estimates across imaging conditions. We validate the
proposed method in simulation and on a hardware prototype, and show that our
method outperforms existing active stereo systems.
</p>
<a href="http://arxiv.org/abs/2011.13117" target="_blank">arXiv:2011.13117</a> [<a href="http://arxiv.org/pdf/2011.13117" target="_blank">pdf</a>]

<h2>Multi-view Depth Estimation using Epipolar Spatio-Temporal Network. (arXiv:2011.13118v1 [cs.CV])</h2>
<h3>Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, Wenping Wang</h3>
<p>We present a novel method for multi-view depth estimation from a single
video, which is a critical task in various applications, such as perception,
reconstruction and robot navigation. Although previous learning-based methods
have demonstrated compelling results, most works estimate depth maps of
individual video frames independently, without taking into consideration the
strong geometric and temporal coherence among the frames. Moreover, current
state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for
cost regularization and therefore require high computational cost, thus
limiting their deployment in real-world applications. Our method achieves
temporally coherent depth estimation results by using a novel Epipolar
Spatio-Temporal (EST) transformer to explicitly associate geometric and
temporal correlation with multiple estimated depth maps. Furthermore, to reduce
the computational cost, inspired by recent Mixture-of-Experts models, we design
a compact hybrid network consisting of a 2D context-aware network and a 3D
matching network which learn 2D context information and 3D disparity cues
separately. Extensive experiments demonstrate that our method achieves higher
accuracy in depth estimation and significant speedup than the SOTA methods.
</p>
<a href="http://arxiv.org/abs/2011.13118" target="_blank">arXiv:2011.13118</a> [<a href="http://arxiv.org/pdf/2011.13118" target="_blank">pdf</a>]

<h2>Evaluation of Out-of-Distribution Detection Performance of Self-Supervised Learning in a Controllable Environment. (arXiv:2011.13120v1 [cs.LG])</h2>
<h3>Jeonghoon Park, Kyungmin Jo, Daehoon Gwak, Jimin Hong, Jaegul Choo, Edward Choi</h3>
<p>We evaluate the out-of-distribution (OOD) detection performance of
self-supervised learning (SSL) techniques with a new evaluation framework.
Unlike the previous evaluation methods, the proposed framework adjusts the
distance of OOD samples from the in-distribution samples. We evaluate an
extensive combination of OOD detection algorithms on three different
implementations of the proposed framework using simulated samples, images, and
text. SSL methods consistently demonstrated the improved OOD detection
performance in all evaluation settings.
</p>
<a href="http://arxiv.org/abs/2011.13120" target="_blank">arXiv:2011.13120</a> [<a href="http://arxiv.org/pdf/2011.13120" target="_blank">pdf</a>]

<h2>Lifting 2D StyleGAN for 3D-Aware Face Generation. (arXiv:2011.13126v1 [cs.CV])</h2>
<h3>Yichun Shi, Divyansh Aggarwal, Anil K. Jain</h3>
<p>We propose a framework, called LiftedGAN, that disentangles and lifts a
pre-trained StyleGAN2 for 3D-aware face generation. Our model is "3D-aware" in
the sense that it is able to (1) disentangle the latent space of StyleGAN2 into
texture, shape, viewpoint, lighting and (2) generate 3D components for
rendering synthetic images. Unlike most previous methods, our method is
completely self-supervised, i.e. it neither requires any manual annotation nor
3DMM model for training. Instead, it learns to generate images as well as their
3D components by distilling the prior knowledge in StyleGAN2 with a
differentiable renderer. The proposed model is able to output both the 3D shape
and texture, allowing explicit pose and lighting control over generated images.
Qualitative and quantitative results show the superiority of our approach over
existing methods on 3D-controllable GANs in content controllability while
generating realistic high quality images.
</p>
<a href="http://arxiv.org/abs/2011.13126" target="_blank">arXiv:2011.13126</a> [<a href="http://arxiv.org/pdf/2011.13126" target="_blank">pdf</a>]

<h2>Generative Learning of Heterogeneous Tail Dependence. (arXiv:2011.13132v1 [cs.LG])</h2>
<h3>Xiangqian Sun, Xing Yan, Qi Wu</h3>
<p>We propose a multivariate generative model to capture the complex dependence
structure often encountered in business and financial data. Our model features
heterogeneous and asymmetric tail dependence between all pairs of individual
dimensions while also allowing heterogeneity and asymmetry in the tails of the
marginals. A significant merit of our model structure is that it is not prone
to error propagation in the parameter estimation process, hence very scalable,
as the dimensions of datasets grow large. However, the likelihood methods are
infeasible for parameter estimation in our case due to the lack of a
closed-form density function. Instead, we devise a novel moment learning
algorithm to learn the parameters. To demonstrate the effectiveness of the
model and its estimator, we test them on simulated as well as real-world
datasets. Results show that this framework gives better finite-sample
performance compared to the copula-based benchmarks as well as recent similar
models.
</p>
<a href="http://arxiv.org/abs/2011.13132" target="_blank">arXiv:2011.13132</a> [<a href="http://arxiv.org/pdf/2011.13132" target="_blank">pdf</a>]

<h2>A Fast Point Cloud Ground Segmentation Approach Based on Coarse-To-Fine Markov Random Field. (arXiv:2011.13140v1 [cs.CV])</h2>
<h3>Weixin Huang, Huawei Liang, Linglong Lin, Zhiling Wang, Shaobo Wang, Biao Yu, Runxin Niu</h3>
<p>Ground segmentation is an important preprocessing task for autonomous
vehicles (AVs) with 3D LiDARs. To solve the problem of existing ground
segmentation methods being very difficult to balance accuracy and computational
complexity, a fast point cloud ground segmentation approach based on a
coarse-to-fine Markov random field (MRF) method is proposed. The method uses an
improved elevation map for ground coarse segmentation, and then uses
spatiotemporal adjacent points to optimize the segmentation results. The
processed point cloud is classified into high-confidence obstacle points,
ground points, and unknown classification points to initialize an MRF model.
The graph cut method is then used to solve the model to achieve fine
segmentation. Experiments on datasets showed that our method improves on other
algorithms in terms of ground segmentation accuracy and is faster than other
graph-based algorithms, which require only a single core of an I7-3770 CPU to
process a frame of Velodyne HDL-64E data (in 39.77 ms, on average). Field tests
were also conducted to demonstrate the effectiveness of the proposed method.
</p>
<a href="http://arxiv.org/abs/2011.13140" target="_blank">arXiv:2011.13140</a> [<a href="http://arxiv.org/pdf/2011.13140" target="_blank">pdf</a>]

<h2>Dense Attention Fluid Network for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2011.13144v1 [cs.CV])</h2>
<h3>Qijian Zhang, Runmin Cong, Chongyi Li, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, Sam Kwong</h3>
<p>Despite the remarkable advances in visual saliency analysis for natural scene
images (NSIs), salient object detection (SOD) for optical remote sensing images
(RSIs) still remains an open and challenging problem. In this paper, we propose
an end-to-end Dense Attention Fluid Network (DAFNet) for SOD in optical RSIs. A
Global Context-aware Attention (GCA) module is proposed to adaptively capture
long-range semantic context relationships, and is further embedded in a Dense
Attention Fluid (DAF) structure that enables shallow attention cues flow into
deep layers to guide the generation of high-level feature attention maps.
Specifically, the GCA module is composed of two key components, where the
global feature aggregation module achieves mutual reinforcement of salient
feature embeddings from any two spatial locations, and the cascaded pyramid
attention module tackles the scale variation issue by building up a cascaded
pyramid framework to progressively refine the attention map in a coarse-to-fine
manner. In addition, we construct a new and challenging optical RSI dataset for
SOD that contains 2,000 images with pixel-wise saliency annotations, which is
currently the largest publicly available benchmark. Extensive experiments
demonstrate that our proposed DAFNet significantly outperforms the existing
state-of-the-art SOD competitors. https://github.com/rmcong/DAFNet_TIP20
</p>
<a href="http://arxiv.org/abs/2011.13144" target="_blank">arXiv:2011.13144</a> [<a href="http://arxiv.org/pdf/2011.13144" target="_blank">pdf</a>]

<h2>Learning to Assist Drone Landings. (arXiv:2011.13146v1 [cs.RO])</h2>
<h3>Kal Backman, Dana Kuli&#x107;, Hoam Chung</h3>
<p>Unmanned aerial vehicles (UAVs) are often used for navigating dangerous
terrains, however they are difficult to pilot. Due to complex input-output
mapping schemes, limited perception, the complex system dynamics and the need
to maintain a safe operation distance, novice pilots experience difficulties in
performing safe landings in obstacle filled environments. Previous work has
proposed autonomous landing methods however these approaches do not adapt to
the pilot's control inputs and require the pilot's goal to be known a priori.
In this work we propose a shared autonomy approach that assists novice pilots
to perform safe landings on one of several elevated platforms at a proficiency
equal to or greater than experienced pilots. Our approach consists of two
modules, a perceptual module and a policy module. The perceptual module
compresses high dimensionality RGB-D images into a latent vector trained with a
cross-modal variational auto-encoder. The policy module provides assistive
control inputs trained with the reinforcement algorithm TD3. We conduct a user
study (n=33) where participants land a simulated drone on a specified platform
out of five candidate platforms with and without the use of the assistant.
Despite the goal platform not being known to the assistant, participants of all
skill levels were able to outperform experienced participants while assisted in
the task.
</p>
<a href="http://arxiv.org/abs/2011.13146" target="_blank">arXiv:2011.13146</a> [<a href="http://arxiv.org/pdf/2011.13146" target="_blank">pdf</a>]

<h2>Better Knowledge Retention through Metric Learning. (arXiv:2011.13149v1 [cs.LG])</h2>
<h3>Ke Li, Shichong Peng, Kailas Vodrahalli, Jitendra Malik</h3>
<p>In continual learning, new categories may be introduced over time, and an
ideal learning system should perform well on both the original categories and
the new categories. While deep neural nets have achieved resounding success in
the classical supervised setting, they are known to forget about knowledge
acquired in prior episodes of learning if the examples encountered in the
current episode of learning are drastically different from those encountered in
prior episodes. In this paper, we propose a new method that can both leverage
the expressive power of deep neural nets and is resilient to forgetting when
new categories are introduced. We found the proposed method can reduce
forgetting by 2.3x to 6.9x on CIFAR-10 compared to existing methods and by 1.8x
to 2.7x on ImageNet compared to an oracle baseline.
</p>
<a href="http://arxiv.org/abs/2011.13149" target="_blank">arXiv:2011.13149</a> [<a href="http://arxiv.org/pdf/2011.13149" target="_blank">pdf</a>]

<h2>Continuous Conversion of CT Kernel using Switchable CycleGAN with AdaIN. (arXiv:2011.13150v1 [cs.CV])</h2>
<h3>Serin Yang, Eung Yeop Kim, Jong Chul Ye</h3>
<p>In X-ray computed tomography (CT) reconstruction, different filter kernels
are used for different structures being emphasized. Since the raw sinogram data
is usually removed after reconstruction, in case there are additional
requirements for reconstructed images with other types of kernels that were not
previously generated, the patient may need to be scanned again. Accordingly,
there exists increasing demand for post-hoc image domain conversion from one
kernel to another without sacrificing the image content. In this paper, we
propose a novel unsupervised kernel conversion method using cycle-consistent
generative adversarial network (cycleGAN) with adaptive instance normalization
(AdaIN). In contrast to the existing deep learning approaches for kernel
conversion, our method does not require paired dataset for training. In
addition, our network can not only translate the images between two different
kernels but also generate images on every interpolating path along an optimal
transport between the two kernel image domains, enabling synergestic
combination of the two filter kernels. Experimental results confirm the
advantages of the proposed algorithm.
</p>
<a href="http://arxiv.org/abs/2011.13150" target="_blank">arXiv:2011.13150</a> [<a href="http://arxiv.org/pdf/2011.13150" target="_blank">pdf</a>]

<h2>Transformation Driven Visual Reasoning. (arXiv:2011.13160v1 [cs.CV])</h2>
<h3>Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng</h3>
<p>This paper defines a new visual reasoning paradigm by introducing an
important factor, i.e., transformation. The motivation comes from the fact that
most existing visual reasoning tasks, such as CLEVR in VQA, are solely defined
to test how well the machine understands the concepts and relations within
static settings, like one image. We argue that this kind of state driven visual
reasoning approach has limitations in reflecting whether the machine has the
ability to infer the dynamics between different states, which has been shown as
important as state-level reasoning for human cognition in Piaget's theory. To
tackle this problem, we propose a novel transformation driven visual reasoning
task. Given both the initial and final states, the target is to infer the
corresponding single-step or multi-step transformation, represented as a
triplet (object, attribute, value) or a sequence of triplets, respectively.
Following this definition, a new dataset namely TRANCE is constructed on the
basis of CLEVR, including three levels of settings, i.e., Basic (single-step
transformation), Event (multi-step transformation), and View (multi-step
transformation with variant views). Experimental results show that the
state-of-the-art visual reasoning models perform well on Basic, but are still
far from human-level intelligence on Event and View. We believe the proposed
new paradigm will boost the development of machine visual reasoning. More
advanced methods and real data need to be investigated in this direction. Code
is available at: https://github.com/hughplay/TVR.
</p>
<a href="http://arxiv.org/abs/2011.13160" target="_blank">arXiv:2011.13160</a> [<a href="http://arxiv.org/pdf/2011.13160" target="_blank">pdf</a>]

<h2>Achievements and Challenges in Explaining Deep Learning based Computer-Aided Diagnosis Systems. (arXiv:2011.13169v1 [cs.AI])</h2>
<h3>Adriano Lucieri, Muhammad Naseer Bajwa, Andreas Dengel, Sheraz Ahmed</h3>
<p>Remarkable success of modern image-based AI methods and the resulting
interest in their applications in critical decision-making processes has led to
a surge in efforts to make such intelligent systems transparent and
explainable. The need for explainable AI does not stem only from ethical and
moral grounds but also from stricter legislation around the world mandating
clear and justifiable explanations of any decision taken or assisted by AI.
Especially in the medical context where Computer-Aided Diagnosis can have a
direct influence on the treatment and well-being of patients, transparency is
of utmost importance for safe transition from lab research to real world
clinical practice. This paper provides a comprehensive overview of current
state-of-the-art in explaining and interpreting Deep Learning based algorithms
in applications of medical research and diagnosis of diseases. We discuss early
achievements in development of explainable AI for validation of known disease
criteria, exploration of new potential biomarkers, as well as methods for the
subsequent correction of AI models. Various explanation methods like visual,
textual, post-hoc, ante-hoc, local and global have been thoroughly and
critically analyzed. Subsequently, we also highlight some of the remaining
challenges that stand in the way of practical applications of AI as a clinical
decision support tool and provide recommendations for the direction of future
research.
</p>
<a href="http://arxiv.org/abs/2011.13169" target="_blank">arXiv:2011.13169</a> [<a href="http://arxiv.org/pdf/2011.13169" target="_blank">pdf</a>]

<h2>Explainable Tensorized Neural Ordinary Differential Equations forArbitrary-step Time Series Prediction. (arXiv:2011.13174v1 [cs.LG])</h2>
<h3>Penglei Gao, Xi Yang, Rui Zhang, Kaizhu Huang</h3>
<p>We propose a continuous neural network architecture, termed Explainable
Tensorized Neural Ordinary Differential Equations (ETN-ODE), for multi-step
time series prediction at arbitrary time points. Unlike the existing
approaches, which mainly handle univariate time series for multi-step
prediction or multivariate time series for single-step prediction, ETN-ODE
could model multivariate time series for arbitrary-step prediction. In
addition, it enjoys a tandem attention, w.r.t. temporal attention and variable
attention, being able to provide explainable insights into the data.
Specifically, ETN-ODE combines an explainable Tensorized Gated Recurrent Unit
(Tensorized GRU or TGRU) with Ordinary Differential Equations (ODE). The
derivative of the latent states is parameterized with a neural network. This
continuous-time ODE network enables a multi-step prediction at arbitrary time
points. We quantitatively and qualitatively demonstrate the effectiveness and
the interpretability of ETN-ODE on five different multi-step prediction tasks
and one arbitrary-step prediction task. Extensive experiments show that ETN-ODE
can lead to accurate predictions at arbitrary time points while attaining best
performance against the baseline methods in standard multi-step time series
prediction.
</p>
<a href="http://arxiv.org/abs/2011.13174" target="_blank">arXiv:2011.13174</a> [<a href="http://arxiv.org/pdf/2011.13174" target="_blank">pdf</a>]

<h2>Regularization with Latent Space Virtual Adversarial Training. (arXiv:2011.13181v1 [cs.LG])</h2>
<h3>Genki Osada, Budrul Ahsan, Revoti Prasad Bora, Takashi Nishide</h3>
<p>Virtual Adversarial Training (VAT) has shown impressive results among
recently developed regularization methods called consistency regularization.
VAT utilizes adversarial samples, generated by injecting perturbation in the
input space, for training and thereby enhances the generalization ability of a
classifier. However, such adversarial samples can be generated only within a
very small area around the input data point, which limits the adversarial
effectiveness of such samples. To address this problem we propose LVAT (Latent
space VAT), which injects perturbation in the latent space instead of the input
space. LVAT can generate adversarial samples flexibly, resulting in more
adverse effects and thus more effective regularization. The latent space is
built by a generative model, and in this paper, we examine two different type
of models: variational auto-encoder and normalizing flow, specifically Glow. We
evaluated the performance of our method in both supervised and semi-supervised
learning scenarios for an image classification task using SVHN and CIFAR-10
datasets. In our evaluation, we found that our method outperforms VAT and other
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.13181" target="_blank">arXiv:2011.13181</a> [<a href="http://arxiv.org/pdf/2011.13181" target="_blank">pdf</a>]

<h2>TinaFace: Strong but Simple Baseline for Face Detection. (arXiv:2011.13183v1 [cs.CV])</h2>
<h3>Yanjia Zhu, Hongxiang Cai, Shuhan Zhang, Chenhao Wang, Yichao Xiong</h3>
<p>Face detection has received intensive attention in recent years. Many works
present lots of special methods for face detection from different perspectives
like model architecture, data augmentation, label assignment and etc., which
make the overall algorithm and system become more and more complex. In this
paper, we point out that \textbf{there is no gap between face detection and
generic object detection}. Then we provide a strong but simple baseline method
to deal with face detection named TinaFace. We use ResNet-50 \cite{he2016deep}
as backbone, and all modules and techniques in TinaFace are constructed on
existing modules, easily implemented and based on generic object detection. On
the hard test set of the most popular and challenging face detection benchmark
WIDER FACE \cite{yang2016wider}, with single-model and single-scale, our
TinaFace achieves 92.1\% average precision (AP), which exceeds most of the
recent face detectors with larger backbone. And after using test time
augmentation (TTA), our TinaFace outperforms the current state-of-the-art
method and achieves 92.4\% AP. The code will be available at
\url{https://github.com/Media-Smart/vedadet}.
</p>
<a href="http://arxiv.org/abs/2011.13183" target="_blank">arXiv:2011.13183</a> [<a href="http://arxiv.org/pdf/2011.13183" target="_blank">pdf</a>]

<h2>Neural Networks for Pulmonary Disease Diagnosis using Auditory and Demographic Information. (arXiv:2011.13194v1 [cs.LG])</h2>
<h3>Morteza Hosseini, Haoran Ren, Hasib-Al Rashid, Arnab Neelim Mazumder, Bharat Prakash, Tinoosh Mohsenin</h3>
<p>Pulmonary diseases impact millions of lives globally and annually. The recent
outbreak of the pandemic of the COVID-19, a novel pulmonary infection, has more
than ever brought the attention of the research community to the machine-aided
diagnosis of respiratory problems. This paper is thus an effort to exploit
machine learning for classification of respiratory problems and proposes a
framework that employs as much correlated information (auditory and demographic
information in this work) as a dataset provides to increase the sensitivity and
specificity of a diagnosing system. First, we use deep convolutional neural
networks (DCNNs) to process and classify a publicly released pulmonary auditory
dataset, and then we take advantage of the existing demographic information
within the dataset and show that the accuracy of the pulmonary classification
increases by 5% when trained on the auditory information in conjunction with
the demographic information. Since the demographic data can be extracted using
computer vision, we suggest using another parallel DCNN to estimate the
demographic information of the subject under test visioned by the processing
computer. Lastly, as a proposition to bring the healthcare system to users'
fingertips, we measure deployment characteristics of the auditory DCNN model
onto processing components of an NVIDIA TX2 development board.
</p>
<a href="http://arxiv.org/abs/2011.13194" target="_blank">arXiv:2011.13194</a> [<a href="http://arxiv.org/pdf/2011.13194" target="_blank">pdf</a>]

<h2>Polyhedral Friction Cone Estimator for Object Manipulation. (arXiv:2011.13199v1 [cs.RO])</h2>
<h3>Morteza Azad, Silvia Cruciani, Michael J. Mathew, Graham Deacon, Guillaume de Chambrier</h3>
<p>A polyhedral friction cone is a set of reaction wrenches that an object can
experience whilst in contact with its environment. This polyhedron is a
powerful tool to control an object's motion and interaction with the
environment. It can be derived analytically, upon knowledge of object and
environment geometries, contact point locations and friction coefficients. We
propose to estimate the polyhedral friction cone so that a priori knowledge of
these quantities is no longer required. Additionally, we introduce a solution
to transform the estimated friction cone to avoid re-estimation while the
object moves. We present an analysis of the estimated polyhedral friction cone
and demonstrate its application for manipulating an object in simulation and
with a real robot.
</p>
<a href="http://arxiv.org/abs/2011.13199" target="_blank">arXiv:2011.13199</a> [<a href="http://arxiv.org/pdf/2011.13199" target="_blank">pdf</a>]

<h2>t-EVA: Time-Efficient t-SNE Video Annotation. (arXiv:2011.13202v1 [cs.CV])</h2>
<h3>Soroosh Poorgholi, Osman Semih Kayhan, Jan C. van Gemert</h3>
<p>Video understanding has received more attention in the past few years due to
the availability of several large-scale video datasets. However, annotating
large-scale video datasets are cost-intensive. In this work, we propose a
time-efficient video annotation method using spatio-temporal feature similarity
and t-SNE dimensionality reduction to speed up the annotation process
massively. Placing the same actions from different videos near each other in
the two-dimensional space based on feature similarity helps the annotator to
group-label video clips. We evaluate our method on two subsets of the
ActivityNet (v1.3) and a subset of the Sports-1M dataset. We show that t-EVA
can outperform other video annotation tools while maintaining test accuracy on
video classification.
</p>
<a href="http://arxiv.org/abs/2011.13202" target="_blank">arXiv:2011.13202</a> [<a href="http://arxiv.org/pdf/2011.13202" target="_blank">pdf</a>]

<h2>Everyone Knows that Everyone Knows: Gossip Protocols for Super Experts. (arXiv:2011.13203v1 [cs.AI])</h2>
<h3>Hans van Ditmarsch, Malvin Gattinger, Rahim Ramezanian</h3>
<p>A gossip protocol is a procedure for sharing secrets in a network. The basic
action in a gossip protocol is a telephone call wherein the calling agents
exchange all the secrets they know. An agent who knows all secrets is an
expert. The usual termination condition is that all agents are experts.

Instead, we explore protocols wherein the termination condition is that all
agents know that all agents are experts. We call such agents super experts.
Additionally, we model that agents who are super experts do not make and do not
answer calls. Such agents are called engaged agents. We also model that such
gossip protocols are common knowledge among the agents. We investigate
conditions under which protocols terminate, both in the synchronous case, where
there is a global clock, and in the asynchronous case, where there is not. We
show that a commonly known protocol with engaged agents may terminate faster
than the same protocol without engaged agents.
</p>
<a href="http://arxiv.org/abs/2011.13203" target="_blank">arXiv:2011.13203</a> [<a href="http://arxiv.org/pdf/2011.13203" target="_blank">pdf</a>]

<h2>Handling Object Symmetries in CNN-based Pose Estimation. (arXiv:2011.13209v1 [cs.CV])</h2>
<h3>Jesse Richter-Klug, Udo Frese</h3>
<p>In this paper we investigate the problems that Convolutional Neural Networks
(CNN) based pose estimators have with symmetric objects. We find that the CNN's
output representation has to form a closed loop when continuously rotating by
one step of symmetry. Otherwise the CNN has to learn an uncontinuous function.
On a 1-DOF toy example we show that commonly used representations do not
fulfill this demand and analyze the problems caused thereby. In particular we
find, that the popular min-over-symmetries approach for creating a symmetry
aware loss tends not to work well with gradient based optimization, i.e. deep
learning. We propose a representation called "closed symmetry loop"' (csl) from
these insights, where the angle of relevant vectors is multiplied by the
symmetry order and then generalize it to 6-DOF. The representation extends our
previous algorithm including a method to disambiguate symmetric equivalents
during the final pose estimation. The algorithm handles continuous rotational
symmetry (i.e. a bottle) and discrete rotational symmetry (general boxes, boxes
with a square face, uniform prims, but no cubes). It is evaluated on the T-LESS
dataset.
</p>
<a href="http://arxiv.org/abs/2011.13209" target="_blank">arXiv:2011.13209</a> [<a href="http://arxiv.org/pdf/2011.13209" target="_blank">pdf</a>]

<h2>Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path Planning. (arXiv:2011.13219v1 [cs.RO])</h2>
<h3>Qingbiao Li, Weizhe Lin, Zhe Liu, Amanda Prorok</h3>
<p>The domains of transport and logistics are increasingly relying on autonomous
mobile robots for the handling and distribution of passengers or resources. At
large system scales, finding decentralized path planning and coordination
solutions is key to efficient system performance. Recently, Graph Neural
Networks (GNNs) have become popular due to their ability to learn communication
policies in decentralized multi-agent systems. Yet, vanilla GNNs rely on
simplistic message aggregation mechanisms that prevent agents from prioritizing
important information.

To tackle this challenge, in this paper, we extend our previous work that
utilizes GNNs in multi-agent path planning by incorporating a novel mechanism
to allow for message-dependent attention. Our Message-Aware Graph Attention
neTwork (MAGAT) is based on a key-query-like mechanism that determines the
relative importance of features in the messages received from various
neighboring robots. We show that MAGAT is able to achieve a performance close
to that of a coupled centralized expert algorithm. Further, ablation studies
and comparisons to several benchmark models show that our attention mechanism
is very effective across different robot densities and performs stably in
different constraints in communication bandwidth.

Experiments demonstrate that our model is able to generalize well in
previously unseen problem instances, and it achieves a 47% improvement over the
benchmark success rate, even in very large-scale instances that are 100x larger
than the training instances.
</p>
<a href="http://arxiv.org/abs/2011.13219" target="_blank">arXiv:2011.13219</a> [<a href="http://arxiv.org/pdf/2011.13219" target="_blank">pdf</a>]

<h2>Depth-Enhanced Feature Pyramid Network for Occlusion-Aware Verification of Buildings from Oblique Images. (arXiv:2011.13226v1 [cs.CV])</h2>
<h3>Qing Zhu, Shengzhi Huang, Han Hu, Haifeng Li, Min Chen, Ruofei Zhong</h3>
<p>Detecting the changes of buildings in urban environments is essential.
Existing methods that use only nadir images suffer from severe problems of
ambiguous features and occlusions between buildings and other regions.
Furthermore, buildings in urban environments vary significantly in scale, which
leads to performance issues when using single-scale features. To solve these
issues, this paper proposes a fused feature pyramid network, which utilizes
both color and depth data for the 3D verification of existing buildings 2D
footprints from oblique images. First, the color data of oblique images are
enriched with the depth information rendered from 3D mesh models. Second,
multiscale features are fused in the feature pyramid network to convolve both
the color and depth data. Finally, multi-view information from both the nadir
and oblique images is used in a robust voting procedure to label changes in
existing buildings. Experimental evaluations using both the ISPRS benchmark
datasets and Shenzhen datasets reveal that the proposed method outperforms the
ResNet and EfficientNet networks by 5\% and 2\%, respectively, in terms of
recall rate and precision. We demonstrate that the proposed method can
successfully detect all changed buildings; therefore, only those marked as
changed need to be manually checked during the pipeline updating procedure;
this significantly reduces the manual quality control requirements. Moreover,
ablation studies indicate that using depth data, feature pyramid modules, and
multi-view voting strategies can lead to clear and progressive improvements.
</p>
<a href="http://arxiv.org/abs/2011.13226" target="_blank">arXiv:2011.13226</a> [<a href="http://arxiv.org/pdf/2011.13226" target="_blank">pdf</a>]

<h2>MultiStar: Instance Segmentation of Overlapping Objects with Star-Convex Polygons. (arXiv:2011.13228v1 [cs.CV])</h2>
<h3>Florin C. Walter, Sebastian Damrich, Fred A. Hamprecht</h3>
<p>Instance segmentation of overlapping objects in biomedical images remains a
largely unsolved problem. We take up this challenge and present MultiStar, an
extension to the popular instance segmentation method StarDist. The key novelty
of our method is that we identify pixels at which objects overlap and use this
information to improve proposal sampling and to avoid suppressing proposals of
truly overlapping objects. This allows us to apply the ideas of StarDist to
images with overlapping objects, while incurring only a small overhead compared
to the established method. MultiStar shows promising results on two datasets
and has the advantage of using a simple and easy to train network architecture.
</p>
<a href="http://arxiv.org/abs/2011.13228" target="_blank">arXiv:2011.13228</a> [<a href="http://arxiv.org/pdf/2011.13228" target="_blank">pdf</a>]

<h2>Molecular representation learning with language models and domain-relevant auxiliary tasks. (arXiv:2011.13230v1 [cs.LG])</h2>
<h3>Benedek Fabian, Thomas Edlich, H&#xe9;l&#xe9;na Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, Mohamed Ahmed</h3>
<p>We apply a Transformer architecture, specifically BERT, to learn flexible and
high quality molecular representations for drug discovery problems. We study
the impact of using different combinations of self-supervised tasks for
pre-training, and present our results for the established Virtual Screening and
QSAR benchmarks. We show that: i) The selection of appropriate self-supervised
task(s) for pre-training has a significant impact on performance in subsequent
downstream tasks such as Virtual Screening. ii) Using auxiliary tasks with more
domain relevance for Chemistry, such as learning to predict calculated
molecular properties, increases the fidelity of our learnt representations.
iii) Finally, we show that molecular representations learnt by our model
`MolBert' improve upon the current state of the art on the benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.13230" target="_blank">arXiv:2011.13230</a> [<a href="http://arxiv.org/pdf/2011.13230" target="_blank">pdf</a>]

<h2>The Devil is in the Boundary: Exploiting Boundary Representation for Basis-based Instance Segmentation. (arXiv:2011.13241v1 [cs.CV])</h2>
<h3>Myungchul Kim, Sanghyun Woo, Dahun Kim, In So Kweon</h3>
<p>Pursuing a more coherent scene understanding towards real-time vision
applications, single-stage instance segmentation has recently gained
popularity, achieving a simpler and more efficient design than its two-stage
counterparts. Besides, its global mask representation often leads to superior
accuracy to the two-stage Mask R-CNN which has been dominant thus far. Despite
the promising advances in single-stage methods, finer delineation of instance
boundaries still remains unexcavated. Indeed, boundary information provides a
strong shape representation that can operate in synergy with the
fully-convolutional mask features of the single-stage segmenter. In this work,
we propose Boundary Basis based Instance Segmentation(B2Inst) to learn a global
boundary representation that can complement existing global-mask-based methods
that are often lacking high-frequency details. Besides, we devise a unified
quality measure of both mask and boundary and introduce a network block that
learns to score the per-instance predictions of itself. When applied to the
strongest baselines in single-stage instance segmentation, our B2Inst leads to
consistent improvements and accurately parse out the instance boundaries in a
scene. Regardless of being single-stage or two-stage frameworks, we outperform
the existing state-of-the-art methods on the COCO dataset with the same
ResNet-50 and ResNet-101 backbones.
</p>
<a href="http://arxiv.org/abs/2011.13241" target="_blank">arXiv:2011.13241</a> [<a href="http://arxiv.org/pdf/2011.13241" target="_blank">pdf</a>]

<h2>MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v1 [cs.CV])</h2>
<h3>Abdullah Hamdi, Silvio Giancola, Bing Li, Ali Thabet, Bernard Ghanem</h3>
<p>Multi-view projection methods have shown the capability to reach
state-of-the-art performance on 3D shape recognition. Most advances in
multi-view representation focus on pooling techniques that learn to aggregate
information from the different views, which tend to be heuristically set and
fixed for all shapes. To circumvent the lack of dynamism of current multi-view
methods, we propose to learn those viewpoints. In particular, we introduce a
Multi-View Transformation Network (MVTN) that regresses optimal viewpoints for
3D shape recognition. By leveraging advances in differentiable rendering, our
MVTN is trained end-to-end with any multi-view network and optimized for 3D
shape classification. We show that MVTN can be seamlessly integrated into
various multi-view approaches to exhibit clear performance gains in the tasks
of 3D shape classification and shape retrieval without any extra training
supervision. Furthermore, our MVTN improves multi-view networks to achieve
state-of-the-art performance in rotation robustness and in object shape
retrieval on ModelNet40.
</p>
<a href="http://arxiv.org/abs/2011.13244" target="_blank">arXiv:2011.13244</a> [<a href="http://arxiv.org/pdf/2011.13244" target="_blank">pdf</a>]

<h2>IFSS-Net: Interactive Few-Shot Siamese Network for Faster Muscles Segmentation and Propagation in 3-D Freehand Ultrasound. (arXiv:2011.13246v1 [cs.CV])</h2>
<h3>Dawood Al Chanti, Vanessa Gonzalez Duque, Marion Crouzier, Antoine Nordez, Lilian Lacourpaille, Diana Mateus</h3>
<p>We present an accurate, fast and efficient method for segmentation and muscle
mask propagation in 3D freehand ultrasound data, towards accurate volume
quantification. To this end, we propose a deep Siamese 3D Encoder-Decoder
network that captures the evolution of the muscle appearance and shape for
contiguous slices and uses it to propagate a reference mask annotated by a
clinical expert. To handle longer changes of the muscle shape over the entire
volume and to provide an accurate propagation, we devised a Bidirectional Long
Short Term Memory module. To train our model with a minimal amount of training
samples, we propose a strategy to combine learning from few annotated 2D
ultrasound slices with sequential pseudo-labeling of the unannotated slices. To
promote few-shot learning, we propose a decremental update of the objective
function to guide the model convergence in the absence of large amounts of
annotated data. Finally, to handle the class-imbalance between foreground and
background muscle pixels, we propose a parametric Tversky loss function that
learns to adaptively penalize false positives and false negatives. We validate
our approach for the segmentation, label propagation, and volume computation of
the three low-limb muscles on a dataset of 44 subjects. We achieve a dice score
coefficient of over $95~\%$ and a small fraction of error with
$1.6035~\pm~0.587$.
</p>
<a href="http://arxiv.org/abs/2011.13246" target="_blank">arXiv:2011.13246</a> [<a href="http://arxiv.org/pdf/2011.13246" target="_blank">pdf</a>]

<h2>Channel-wise Distillation for Semantic Segmentation. (arXiv:2011.13256v1 [cs.CV])</h2>
<h3>Changyong Shu, Yifan Liu, Jianfei Gao, Lin Xu, Chunhua Shen</h3>
<p>Knowledge distillation (KD) has been proven to be a simple and effective tool
for training compact models. Almost all KD variants for semantic segmentation
align the student and teacher networks' feature maps in the spatial domain,
typically by minimizing point-wise and/or pair-wise discrepancy. Observing that
in semantic segmentation, some layers' feature activations of each channel tend
to encode saliency of scene categories (analogue to class activation mapping),
we propose to align features channel-wise between the student and teacher
networks. To this end, we first transform the feature map of each channel into
a distribution using softmax normalization, and then minimize the
Kullback-Leibler (KL) divergence of the corresponding channels of the two
networks. By doing so, our method focuses on mimicking the soft distributions
of channels between networks. In particular, the KL divergence enables learning
to pay more attention to the most salient regions of the channel-wise maps,
presumably corresponding to the most useful signals for semantic segmentation.
Experiments demonstrate that our channel-wise distillation outperforms almost
all existing spatial distillation methods for semantic segmentation
considerably, and requires less computational cost during training. We
consistently achieve superior performance on three benchmarks with various
network structures. Code is available at: https://git.io/ChannelDis
</p>
<a href="http://arxiv.org/abs/2011.13256" target="_blank">arXiv:2011.13256</a> [<a href="http://arxiv.org/pdf/2011.13256" target="_blank">pdf</a>]

<h2>CYPUR-NN: Crop Yield Prediction Using Regression and Neural Networks. (arXiv:2011.13265v1 [cs.CV])</h2>
<h3>Sandesh Ramesh, Anirudh Hebbar, Varun Yadav, Thulasiram Gunta, A Balachandra</h3>
<p>Our recent study using historic data of paddy yield and associated conditions
include humidity, luminescence, and temperature. By incorporating regression
models and neural networks (NN), one can produce highly satisfactory
forecasting of paddy yield. Simulations indicate that our model can predict
paddy yield with high accuracy while concurrently detecting diseases that may
exist and are oblivious to the human eye. Crop Yield Prediction Using
Regression and Neural Networks (CYPUR-NN) is developed here as a system that
will facilitate agriculturists and farmers to predict yield from a picture or
by entering values via a web interface. CYPUR-NN has been tested on stock
images and the experimental results are promising.
</p>
<a href="http://arxiv.org/abs/2011.13265" target="_blank">arXiv:2011.13265</a> [<a href="http://arxiv.org/pdf/2011.13265" target="_blank">pdf</a>]

<h2>Group-Skeleton-Based Human Action Recognition in Complex Events. (arXiv:2011.13273v1 [cs.CV])</h2>
<h3>Tingtian Li, Zixun Sun, Xiao Chen</h3>
<p>Human action recognition as an important application of computer vision has
been studied for decades. Among various approaches, skeleton-based methods
recently attract increasing attention due to their robust and superior
performance. However, existing skeleton-based methods ignore the potential
action relationships between different persons, while the action of a person is
highly likely to be impacted by another person especially in complex events. In
this paper, we propose a novel group-skeleton-based human action recognition
method in complex events. This method first utilizes multi-scale
spatial-temporal graph convolutional networks (MS-G3Ds) to extract skeleton
features from multiple persons. In addition to the traditional key point
coordinates, we also input the key point speed values to the networks for
better performance. Then we use multilayer perceptrons (MLPs) to embed the
distance values between the reference person and other persons into the
extracted features. Lastly, all the features are fed into another MS-G3D for
feature fusion and classification. For avoiding class imbalance problems, the
networks are trained with a focal loss. The proposed algorithm is also our
solution for the Large-scale Human-centric Video Analysis in Complex Events
Challenge. Results on the HiEve dataset show that our method can give superior
performance compared to other state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.13273" target="_blank">arXiv:2011.13273</a> [<a href="http://arxiv.org/pdf/2011.13273" target="_blank">pdf</a>]

<h2>AMLSI: A Novel Accurate Action Model Learning Algorithm. (arXiv:2011.13277v1 [cs.AI])</h2>
<h3>Maxence Grand, Humbert Fiorino, Damien Pellier</h3>
<p>This paper presents new approach based on grammar induction called AMLSI
Action Model Learning with State machine Interactions. The AMLSI approach does
not require a training dataset of plan traces to work. AMLSI proceeds by trial
and error: it queries the system to learn with randomly generated action
sequences, and it observes the state transitions of the system, then AMLSI
returns a PDDL domain corresponding to the system. A key issue for domain
learning is the ability to plan with the learned domains. It often happens that
a small learning error leads to a domain that is unusable for planning. Unlike
other algorithms, we show that AMLSI is able to lift this lock by learning
domains from partial and noisy observations with sufficient accuracy to allow
planners to solve new problems.
</p>
<a href="http://arxiv.org/abs/2011.13277" target="_blank">arXiv:2011.13277</a> [<a href="http://arxiv.org/pdf/2011.13277" target="_blank">pdf</a>]

<h2>Totally and Partially Ordered Hierarchical Planners in PDDL4J Library. (arXiv:2011.13297v1 [cs.AI])</h2>
<h3>Damien Pellier, Humbert Fiorino</h3>
<p>In this paper, we outline the implementation of the TFD (Totally Ordered Fast
Downward) and the PFD (Partially ordered Fast Downward) hierarchical planners
that participated in the first HTN IPC competition in 2020. These two planners
are based on forward-chaining task decomposition coupled with a compact
grounding of actions, methods, tasks and HTN problems.
</p>
<a href="http://arxiv.org/abs/2011.13297" target="_blank">arXiv:2011.13297</a> [<a href="http://arxiv.org/pdf/2011.13297" target="_blank">pdf</a>]

<h2>Predictive Collision Management for Time and Risk Dependent Path Planning. (arXiv:2011.13305v1 [cs.LG])</h2>
<h3>Carsten Hahn, Sebastian Feld, Hannes Schroter</h3>
<p>Autonomous agents such as self-driving cars or parcel robots need to
recognize and avoid possible collisions with obstacles in order to move
successfully in their environment. Humans, however, have learned to predict
movements intuitively and to avoid obstacles in a forward-looking way. The task
of collision avoidance can be divided into a global and a local level.
Regarding the global level, we propose an approach called "Predictive Collision
Management Path Planning" (PCMP). At the local level, solutions for collision
avoidance are used that prevent an inevitable collision. Therefore, the aim of
PCMP is to avoid unnecessary local collision scenarios using predictive
collision management. PCMP is a graph-based algorithm with a focus on the time
dimension consisting of three parts: (1) movement prediction, (2) integration
of movement prediction into a time-dependent graph, and (3) time and
risk-dependent path planning. The algorithm combines the search for a shortest
path with the question: is the detour worth avoiding a possible collision
scenario? We evaluate the evasion behavior in different simulation scenarios
and the results show that a risk-sensitive agent can avoid 47.3% of the
collision scenarios while making a detour of 1.3%. A risk-averse agent avoids
up to 97.3% of the collision scenarios with a detour of 39.1%. Thus, an agent's
evasive behavior can be controlled actively and risk-dependent using PCMP.
</p>
<a href="http://arxiv.org/abs/2011.13305" target="_blank">arXiv:2011.13305</a> [<a href="http://arxiv.org/pdf/2011.13305" target="_blank">pdf</a>]

<h2>A Metric for Linear Symmetry-Based Disentanglement. (arXiv:2011.13306v1 [cs.LG])</h2>
<h3>Luis A. P&#xe9;rez Rey, Loek Tonnaer, Vlado Menkovski, Mike Holenderski, Jacobus W. Portegies</h3>
<p>The definition of Linear Symmetry-Based Disentanglement (LSBD) proposed by
(Higgins et al., 2018) outlines the properties that should characterize a
disentangled representation that captures the symmetries of data. However, it
is not clear how to measure the degree to which a data representation fulfills
these properties. We propose a metric for the evaluation of the level of LSBD
that a data representation achieves. We provide a practical method to evaluate
this metric and use it to evaluate the disentanglement of the data
representations obtained for three datasets with underlying $SO(2)$ symmetries.
</p>
<a href="http://arxiv.org/abs/2011.13306" target="_blank">arXiv:2011.13306</a> [<a href="http://arxiv.org/pdf/2011.13306" target="_blank">pdf</a>]

<h2>SelfText Beyond Polygon: Unconstrained Text Detection with Box Supervision and Dynamic Self-Training. (arXiv:2011.13307v1 [cs.CV])</h2>
<h3>Weijia Wu, Enze Xie, Ruimao Zhang, Wenhai Wang, Guan Pang, Zhen Li, Hong Zhou, Ping Luo</h3>
<p>Although a polygon is a more accurate representation than an upright bounding
box for text detection, the annotations of polygons are extremely expensive and
challenging. Unlike existing works that employ fully-supervised training with
polygon annotations, we propose a novel text detection system termed SelfText
Beyond Polygon (SBP) with Bounding Box Supervision (BBS) and Dynamic Self
Training (DST), where training a polygon-based text detector with only a
limited set of upright bounding box annotations. For BBS, we firstly utilize
the synthetic data with character-level annotations to train a Skeleton
Attention Segmentation Network (SASN). Then the box-level annotations are
adopted to guide the generation of high-quality polygon-liked pseudo labels,
which can be used to train any detectors. In this way, our method achieves the
same performance as text detectors trained with polygon annotations (i.e., both
are 85.0% F-score for PSENet on ICDAR2015 ). For DST, through dynamically
removing the false alarms, it is able to leverage limited labeled data as well
as massive unlabeled data to further outperform the expensive baseline. We hope
SBP can provide a new perspective for text detection to save huge labeling
costs.
</p>
<a href="http://arxiv.org/abs/2011.13307" target="_blank">arXiv:2011.13307</a> [<a href="http://arxiv.org/pdf/2011.13307" target="_blank">pdf</a>]

<h2>Polarization-driven Semantic Segmentation via Efficient Attention-bridged Fusion. (arXiv:2011.13313v1 [cs.CV])</h2>
<h3>Kaite Xiang, Kailun Yang, Kaiwei Wang</h3>
<p>Semantic Segmentation (SS) is promising for outdoor scene perception in
safety-critical applications like autonomous vehicles, assisted navigation and
so on. However, traditional SS is primarily based on RGB images, which limits
the reliability of SS in complex outdoor scenes, where RGB images lack
necessary information dimensions to fully perceive unconstrained environments.
As preliminary investigation, we examine SS in an unexpected obstacle detection
scenario, which demonstrates the necessity of multimodal fusion. Thereby, in
this work, we present EAFNet, an Efficient Attention-bridged Fusion Network to
exploit complementary information coming from different optical sensors.
Specifically, we incorporate polarization sensing to obtain supplementary
information, considering its optical characteristics for robust representation
of diverse materials. By using a single-shot polarization sensor, we build the
first RGB-P dataset which consists of 394 annotated pixel-aligned
RGB-Polarization images. A comprehensive variety of experiments shows the
effectiveness of EAFNet to fuse polarization and RGB information, as well as
the flexibility to be adapted to other sensor combination scenarios.
</p>
<a href="http://arxiv.org/abs/2011.13313" target="_blank">arXiv:2011.13313</a> [<a href="http://arxiv.org/pdf/2011.13313" target="_blank">pdf</a>]

<h2>Adaptive Multiplane Image Generation from a Single Internet Picture. (arXiv:2011.13317v1 [cs.CV])</h2>
<h3>Diogo C. Luvizon, Gustavo Sutter P. Carvalho, Andreza A. dos Santos, Jhonatas S. Conceicao, Jose L. Flores-Campana, Luis G. L. Decker, Marcos R. Souza, Helio Pedrini, Antonio Joia, Otavio A. B. Penatti</h3>
<p>In the last few years, several works have tackled the problem of novel view
synthesis from stereo images or even from a single picture. However, previous
methods are computationally expensive, specially for high-resolution images. In
this paper, we address the problem of generating a multiplane image (MPI) from
a single high-resolution picture. We present the adaptive-MPI representation,
which allows rendering novel views with low computational requirements. To this
end, we propose an adaptive slicing algorithm that produces an MPI with a
variable number of image planes. We present a new lightweight CNN for depth
estimation, which is learned by knowledge distillation from a larger network.
Occluded regions in the adaptive-MPI are inpainted also by a lightweight CNN.
We show that our method is capable of producing high-quality predictions with
one order of magnitude less parameters compared to previous approaches. The
robustness of our method is evidenced on challenging pictures from the
Internet.
</p>
<a href="http://arxiv.org/abs/2011.13317" target="_blank">arXiv:2011.13317</a> [<a href="http://arxiv.org/pdf/2011.13317" target="_blank">pdf</a>]

<h2>Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v1 [cs.CV])</h2>
<h3>Zhen Huang, Xu Shen, Xinmei Tian, Houqiang Li, Jianqiang Huang, Xian-Sheng Hua</h3>
<p>Skeleton-based human action recognition has attracted much attention with the
prevalence of accessible depth sensors. Recently, graph convolutional networks
(GCNs) have been widely used for this task due to their powerful capability to
model graph data. The topology of the adjacency graph is a key factor for
modeling the correlations of the input skeletons. Thus, previous methods mainly
focus on the design/learning of the graph topology. But once the topology is
learned, only a single-scale feature and one transformation exist in each layer
of the networks. Many insights, such as multi-scale information and multiple
sets of transformations, that have been proven to be very effective in
convolutional neural networks (CNNs), have not been investigated in GCNs. The
reason is that, due to the gap between graph-structured skeleton data and
conventional image/video data, it is very challenging to embed these insights
into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy
in GCNs for skeleton sequence processing. Specifically, we design a simple and
highly modularized graph convolutional network architecture for skeleton-based
action recognition. Our network is constructed by repeating a building block
that aggregates multi-granularity information from both the spatial and
temporal paths. Extensive experiments demonstrate that our network outperforms
state-of-the-art methods by a significant margin with only 1/5 of the
parameters and 1/10 of the FLOPs.
</p>
<a href="http://arxiv.org/abs/2011.13322" target="_blank">arXiv:2011.13322</a> [<a href="http://arxiv.org/pdf/2011.13322" target="_blank">pdf</a>]

<h2>DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution. (arXiv:2011.13328v1 [cs.CV])</h2>
<h3>Tong He, Chunhua Shen, Anton van den Hengel</h3>
<p>Previous top-performing approaches for point cloud instance segmentation
involve a bottom-up strategy, which often includes inefficient operations or
complex pipelines, such as grouping over-segmented components, introducing
additional steps for refining, or designing complicated loss functions. The
inevitable variation in the instance scales can lead bottom-up methods to
become particularly sensitive to hyper-parameter values. To this end, we
propose instead a dynamic, proposal-free, data-driven approach that generates
the appropriate convolution kernels to apply in response to the nature of the
instances. To make the kernels discriminative, we explore a large context by
gathering homogeneous points that share identical semantic categories and have
close votes for the geometric centroids. Instances are then decoded by several
simple convolutional layers. Due to the limited receptive field introduced by
the sparse convolution, a small light-weight transformer is also devised to
capture the long-range dependencies and high-level interactions among point
samples. The proposed method achieves promising results on both ScanetNetV2 and
S3DIS, and this performance is robust to the particular hyper-parameter values
chosen. It also improves inference speed by more than 25% over the current
state-of-the-art. Code is available at: https://git.io/DyCo3D
</p>
<a href="http://arxiv.org/abs/2011.13328" target="_blank">arXiv:2011.13328</a> [<a href="http://arxiv.org/pdf/2011.13328" target="_blank">pdf</a>]

<h2>Learning from Simulation, Racing in Reality. (arXiv:2011.13332v1 [cs.RO])</h2>
<h3>Eugenio Chisari, Alexander Liniger, Alisa Rupenyan, Luc Van Gool, John Lygeros</h3>
<p>We present a reinforcement learning-based solution to autonomously race on a
miniature race car platform. We show that a policy that is trained purely in
simulation using a relatively simple vehicle model, including model
randomization, can be successfully transferred to the real robotic setup. We
achieve this by using novel policy output regularization approach and a lifted
action space which enables smooth actions but still aggressive race car
driving. We show that this regularized policy does outperform the Soft Actor
Critic (SAC) baseline method, both in simulation and on the real car, but it is
still outperformed by a Model Predictive Controller (MPC) state of the art
method. The refinement of the policy with three hours of real-world interaction
data allows the reinforcement learning policy to achieve lap times similar to
the MPC controller while reducing track constraint violations by 50%.
</p>
<a href="http://arxiv.org/abs/2011.13332" target="_blank">arXiv:2011.13332</a> [<a href="http://arxiv.org/pdf/2011.13332" target="_blank">pdf</a>]

<h2>4D Human Body Capture from Egocentric Video via 3D Scene Grounding. (arXiv:2011.13341v1 [cs.CV])</h2>
<h3>Miao Liu, Dexin Yang, Yan Zhang, Zhaopeng Cui, James M. Rehg, Siyu Tang</h3>
<p>To understand human daily social interaction from egocentric perspective, we
introduce a novel task of reconstructing a time series of second-person 3D
human body meshes from monocular egocentric videos. The unique viewpoint and
rapid embodied camera motion of egocentric videos raise additional technical
barriers for human body capture. To address those challenges, we propose a
novel optimization-based approach that leverages 2D observations of the entire
video sequence and human-scene interaction constraint to estimate second-person
human poses, shapes and global motion that are grounded on the 3D environment
captured from the egocentric view. We conduct detailed ablation studies to
validate our design choice. Moreover, we compare our method with previous
state-of-the-art method on human motion capture from monocular video, and show
that our method estimates more accurate human-body poses and shapes under the
challenging egocentric setting. In addition, we demonstrate that our approach
produces more realistic human-scene interaction. Our project page is available
at: https://aptx4869lm.github.io/4DEgocentricBodyCapture/
</p>
<a href="http://arxiv.org/abs/2011.13341" target="_blank">arXiv:2011.13341</a> [<a href="http://arxiv.org/pdf/2011.13341" target="_blank">pdf</a>]

<h2>Unsupervised learning for economic risk evaluation in the context of Covid-19 pandemic. (arXiv:2011.13350v1 [cs.LG])</h2>
<h3>Santiago Cortes, Yullys M. Quintero</h3>
<p>Justifying draconian measures during the Covid-19 pandemic was difficult not
only because of the restriction of individual rights, but also because of its
economic impact. The objective of this work is to present a machine learning
approach to identify regions that should implement similar health policies. For
that end, we successfully developed a system that gives a notion of economic
impact given the prediction of new incidental cases through unsupervised
learning and time series forecasting. This system was built taking into account
computational restrictions and low maintenance requirements in order to improve
the system's resilience. Finally this system was deployed as part of a web
application for simulation and data analysis of COVID-19, in Colombia,
available at (https://covid19.dis.eafit.edu.co).
</p>
<a href="http://arxiv.org/abs/2011.13350" target="_blank">arXiv:2011.13350</a> [<a href="http://arxiv.org/pdf/2011.13350" target="_blank">pdf</a>]

<h2>Beyond Single Instance Multi-view Unsupervised Representation Learning. (arXiv:2011.13356v1 [cs.CV])</h2>
<h3>Xiangxiang Chu, Xiaohang Zhan, Xiaolin Wei</h3>
<p>Recent unsupervised contrastive representation learning follows a Single
Instance Multi-view (SIM) paradigm where positive pairs are usually constructed
with intra-image data augmentation. In this paper, we propose an effective
approach called Beyond Single Instance Multi-view (BSIM). Specifically, we
impose more accurate instance discrimination capability by measuring the joint
similarity between two randomly sampled instances and their mixture, namely
spurious-positive pairs. We believe that learning joint similarity helps to
improve the performance when encoded features are distributed more evenly in
the latent space. We apply it as an orthogonal improvement for unsupervised
contrastive representation learning, including current outstanding methods
SimCLR, MoCo, and BYOL. We evaluate our learned representations on many
downstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC
2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial
gains with a large margin almost on all these tasks compared with prior arts.
</p>
<a href="http://arxiv.org/abs/2011.13356" target="_blank">arXiv:2011.13356</a> [<a href="http://arxiv.org/pdf/2011.13356" target="_blank">pdf</a>]

<h2>ClusterFace: Joint Clustering and Classification for Set-Based Face Recognition. (arXiv:2011.13360v1 [cs.CV])</h2>
<h3>S. W. Arachchilage, E. Izquierdo</h3>
<p>Deep learning technology has enabled successful modeling of complex facial
features when high quality images are available. Nonetheless, accurate modeling
and recognition of human faces in real world scenarios `on the wild' or under
adverse conditions remains an open problem. When unconstrained faces are mapped
into deep features, variations such as illumination, pose, occlusion, etc., can
create inconsistencies in the resultant feature space. Hence, deriving
conclusions based on direct associations could lead to degraded performance.
This rises the requirement for a basic feature space analysis prior to face
recognition. This paper devises a joint clustering and classification scheme
which learns deep face associations in an easy-to-hard way. Our method is based
on hierarchical clustering where the early iterations tend to preserve high
reliability. The rationale of our method is that a reliable clustering result
can provide insights on the distribution of the feature space, that can guide
the classification that follows. Experimental evaluations on three tasks, face
verification, face identification and rank-order search, demonstrates better or
competitive performance compared to the state-of-the-art, on all three
experiments.
</p>
<a href="http://arxiv.org/abs/2011.13360" target="_blank">arXiv:2011.13360</a> [<a href="http://arxiv.org/pdf/2011.13360" target="_blank">pdf</a>]

<h2>SSDL: Self-Supervised Domain Learning for Improved Face Recognition. (arXiv:2011.13361v1 [cs.CV])</h2>
<h3>S. W. Arachchilage, E. Izquierdo</h3>
<p>Face recognition in unconstrained environments is challenging due to
variations in illumination, quality of sensing, motion blur and etc. An
individual's face appearance can vary drastically under different conditions
creating a gap between train (source) and varying test (target) data. The
domain gap could cause decreased performance levels in direct knowledge
transfer from source to target. Despite fine-tuning with domain specific data
could be an effective solution, collecting and annotating data for all domains
is extremely expensive. To this end, we propose a self-supervised domain
learning (SSDL) scheme that trains on triplets mined from unlabelled data. A
key factor in effective discriminative learning, is selecting informative
triplets. Building on most confident predictions, we follow an "easy-to-hard"
scheme of alternate triplet mining and self-learning. Comprehensive experiments
on four different benchmarks show that SSDL generalizes well on different
domains.
</p>
<a href="http://arxiv.org/abs/2011.13361" target="_blank">arXiv:2011.13361</a> [<a href="http://arxiv.org/pdf/2011.13361" target="_blank">pdf</a>]

<h2>SoccerNet-v2 : A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos. (arXiv:2011.13367v1 [cs.CV])</h2>
<h3>Adrien Deli&#xe8;ge, Anthony Cioppa, Silvio Giancola, Meisam J. Seikavandi, Jacob V. Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas B. Moeslund, Marc Van Droogenbroeck</h3>
<p>Understanding broadcast videos is a challenging task in computer vision, as
it requires generic reasoning capabilities to appreciate the content offered by
the video editing. In this work, we propose SoccerNet-v2, a novel large-scale
corpus of manual annotations for the SoccerNet video dataset, along with open
challenges to encourage more research in soccer understanding and broadcast
production. Specifically, we release around 300k annotations within SoccerNet's
500 untrimmed broadcast soccer videos. We extend current tasks in the realm of
soccer to include action spotting, camera shot segmentation with boundary
detection, and we define a novel replay grounding task. For each task, we
provide and discuss benchmark results, reproducible with our open-source
adapted implementations of the most relevant works in the field. SoccerNet-v2
is presented to the broader research community to help push computer vision
closer to automatic solutions for more general video understanding and
production purposes.
</p>
<a href="http://arxiv.org/abs/2011.13367" target="_blank">arXiv:2011.13367</a> [<a href="http://arxiv.org/pdf/2011.13367" target="_blank">pdf</a>]

<h2>Automated Blood Cell Counting from Non-invasive Capillaroscopy Videos with Bidirectional Temporal Deep Learning Tracking Algorithm. (arXiv:2011.13371v1 [cs.CV])</h2>
<h3>Luojie Huang, Gregory N. McKay, Nicholas J. Durr</h3>
<p>Oblique back-illumination capillaroscopy has recently been introduced as a
method for high-quality, non-invasive blood cell imaging in human capillaries.
To make this technique practical for clinical blood cell counting, solutions
for automatic processing of acquired videos are needed. Here, we take the first
step towards this goal, by introducing a deep learning multi-cell tracking
model, named CycleTrack, which achieves accurate blood cell counting from
capillaroscopic videos. CycleTrack combines two simple online tracking models,
SORT and CenterTrack, and is tailored to features of capillary blood cell flow.
Blood cells are tracked by displacement vectors in two opposing temporal
directions (forward- and backward-tracking) between consecutive frames. This
approach yields accurate tracking despite rapidly moving and deforming blood
cells. The proposed model outperforms other baseline trackers, achieving 65.57%
Multiple Object Tracking Accuracy and 73.95% ID F1 score on test videos.
Compared to manual blood cell counting, CycleTrack achieves 96.58 $\pm$ 2.43%
cell counting accuracy among 8 test videos with 1000 frames each compared to
93.45% and 77.02% accuracy for independent CenterTrack and SORT almost without
additional time expense. It takes 800s to track and count approximately 8000
blood cells from 9,600 frames captured in a typical one-minute video. Moreover,
the blood cell velocity measured by CycleTrack demonstrates a consistent,
pulsatile pattern within the physiological range of heart rate. Lastly, we
discuss future improvements for the CycleTrack framework, which would enable
clinical translation of the oblique back-illumination microscope towards a
real-time and non-invasive point-of-care blood cell counting and analyzing
technology.
</p>
<a href="http://arxiv.org/abs/2011.13371" target="_blank">arXiv:2011.13371</a> [<a href="http://arxiv.org/pdf/2011.13371" target="_blank">pdf</a>]

<h2>Understand Watchdogs: Discover How Game Bot Get Discovered. (arXiv:2011.13374v1 [cs.AI])</h2>
<h3>Eunji Park, Kyoung Ho Park, Huy Kang Kim</h3>
<p>The game industry has long been troubled by malicious activities utilizing
game bots. The game bots disturb other game players and destroy the
environmental system of the games. For these reasons, the game industry put
their best efforts to detect the game bots among players' characters using the
learning-based detections. However, one problem with the detection
methodologies is that they do not provide rational explanations about their
decisions. To resolve this problem, in this work, we investigate the
explainabilities of the game bot detection. We develop the XAI model using a
dataset from the Korean MMORPG, AION, which includes game logs of human players
and game bots. More than one classification model has been applied to the
dataset to be analyzed by applying interpretable models. This provides us
explanations about the game bots' behavior, and the truthfulness of the
explanations has been evaluated. Besides, interpretability contributes to
minimizing false detection, which imposes unfair restrictions on human players.
</p>
<a href="http://arxiv.org/abs/2011.13374" target="_blank">arXiv:2011.13374</a> [<a href="http://arxiv.org/pdf/2011.13374" target="_blank">pdf</a>]

<h2>Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect. (arXiv:2011.13375v1 [cs.CV])</h2>
<h3>Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee, Earlence Fernandes</h3>
<p>Physical adversarial examples for camera-based computer vision have so far
been achieved through visible artifacts -- a sticker on a Stop sign, colorful
borders around eyeglasses or a 3D printed object with a colorful texture. An
implicit assumption here is that the perturbations must be visible so that a
camera can sense them. By contrast, we contribute a procedure to generate, for
the first time, physical adversarial examples that are invisible to human eyes.
Rather than modifying the victim object with visible artifacts, we modify light
that illuminates the object. We demonstrate how an attacker can craft a
modulated light signal that adversarially illuminates a scene and causes
targeted misclassifications on a state-of-the-art ImageNet deep learning model.
Concretely, we exploit the radiometric rolling shutter effect in commodity
cameras to create precise striping patterns that appear on images. To human
eyes, it appears like the object is illuminated, but the camera creates an
image with stripes that will cause ML models to output the attacker-desired
classification. We conduct a range of simulation and physical experiments with
LEDs, demonstrating targeted attack rates up to 84%.
</p>
<a href="http://arxiv.org/abs/2011.13375" target="_blank">arXiv:2011.13375</a> [<a href="http://arxiv.org/pdf/2011.13375" target="_blank">pdf</a>]

<h2>How Well Do Self-Supervised Models Transfer?. (arXiv:2011.13377v1 [cs.CV])</h2>
<h3>Linus Ericsson, Henry Gouk, Timothy M. Hospedales</h3>
<p>Self-supervised visual representation learning has seen huge progress in
recent months. However, no large scale evaluation has compared the many
pre-trained models that are now available. In this paper, we evaluate the
transfer performance of 13 top self-supervised models on 25 downstream tasks,
including many-shot classification, few-shot classification, object detection
and dense prediction. We compare their performance to a supervised baseline and
conclude that on most datasets, the best self-supervised models outperform
supervision, confirming the recently observed trend in the literature. We find
ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot
recognition, but increasingly less so for few-shot, object detection and dense
prediction, as well as to unstructured data. There is no single self-supervised
method which dominates overall, but notably DeepCluster-v2 comes out on top in
recognition and SimCLR-v2 in detection and dense prediction. Our analysis of
feature properties suggests that top self-supervised learners struggle to
preserve colour information as well as supervised (likely due to use of
augmentation), but exhibit better calibration for recognition and suffer from
less attentive overfitting than supervised learners.
</p>
<a href="http://arxiv.org/abs/2011.13377" target="_blank">arXiv:2011.13377</a> [<a href="http://arxiv.org/pdf/2011.13377" target="_blank">pdf</a>]

<h2>Prediction in ungauged regions with sparse flow duration curves and input-selection ensemble modeling. (arXiv:2011.13380v1 [cs.LG])</h2>
<h3>Dapeng Feng, Kathryn Lawson, Chaopeng Shen</h3>
<p>While long short-term memory (LSTM) models have demonstrated stellar
performance with streamflow predictions, there are major risks in applying
these models in contiguous regions with no gauges, or predictions in ungauged
regions (PUR) problems. However, softer data such as the flow duration curve
(FDC) may be already available from nearby stations, or may become available.
Here we demonstrate that sparse FDC data can be migrated and assimilated by an
LSTM-based network, via an encoder. A stringent region-based holdout test
showed a median Kling-Gupta efficiency (KGE) of 0.62 for a US dataset,
substantially higher than previous state-of-the-art global-scale ungauged basin
tests. The baseline model without FDC was already competitive (median KGE
0.56), but integrating FDCs had substantial value. Because of the inaccurate
representation of inputs, the baseline models might sometimes produce
catastrophic results. However, model generalizability was further meaningfully
improved by compiling an ensemble based on models with different input
selections.
</p>
<a href="http://arxiv.org/abs/2011.13380" target="_blank">arXiv:2011.13380</a> [<a href="http://arxiv.org/pdf/2011.13380" target="_blank">pdf</a>]

<h2>Automatic coding of students' writing via Contrastive Representation Learning in the Wasserstein space. (arXiv:2011.13384v1 [cs.LG])</h2>
<h3>Ruijie Jiang, Julia Gouvea, David Hammer, Shuchin Aeron</h3>
<p>Qualitative analysis of verbal data is of central importance in the learning
sciences. It is labor-intensive and time-consuming, however, which limits the
amount of data researchers can include in studies. This work is a step towards
building a statistical machine learning (ML) method for achieving an automated
support for qualitative analyses of students' writing, here specifically in
score laboratory reports in introductory biology for sophistication of
argumentation and reasoning. We start with a set of lab reports from an
undergraduate biology course, scored by a four-level scheme that considers the
complexity of argument structure, the scope of evidence, and the care and
nuance of conclusions. Using this set of labeled data, we show that a popular
natural language modeling processing pipeline, namely vector representation of
words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model
for capturing language generation as a state-space model, is able to
quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK)
prediction score, when trained in via a novel contrastive learning set-up. We
show that the ML algorithm approached the inter-rater reliability of human
analysis. Ultimately, we conclude, that machine learning (ML) for natural
language processing (NLP) holds promise for assisting learning sciences
researchers in conducting qualitative studies at much larger scales than is
currently possible.
</p>
<a href="http://arxiv.org/abs/2011.13384" target="_blank">arXiv:2011.13384</a> [<a href="http://arxiv.org/pdf/2011.13384" target="_blank">pdf</a>]

<h2>3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer. (arXiv:2011.13388v1 [cs.CV])</h2>
<h3>Mattia Segu, Margarita Grinvald, Roland Siegwart, Federico Tombari</h3>
<p>Transferring the style from one image onto another is a popular and widely
studied task in computer vision. Yet, learning-based style transfer in the 3D
setting remains a largely unexplored problem. To our knowledge, we propose the
first learning-based generative approach for style transfer between 3D objects.
Our method allows to combine the content and style of a source and target 3D
model to generate a novel shape that resembles in style the target while
retaining the source content. The proposed framework can synthesize new 3D
shapes both in the form of point clouds and meshes. Furthermore, we extend our
technique to implicitly learn the underlying multimodal style distribution of
the individual category domains. By sampling style codes from the learned
distributions, we increase the variety of styles that our model can confer to a
given reference object. Experimental results validate the effectiveness of the
proposed 3D style transfer method on a number of benchmarks.
</p>
<a href="http://arxiv.org/abs/2011.13388" target="_blank">arXiv:2011.13388</a> [<a href="http://arxiv.org/pdf/2011.13388" target="_blank">pdf</a>]

<h2>Generalization in Reinforcement Learning by Soft Data Augmentation. (arXiv:2011.13389v1 [cs.LG])</h2>
<h3>Nicklas Hansen, Xiaolong Wang</h3>
<p>Extensive efforts have been made to improve the generalization ability of
Reinforcement Learning (RL) methods via domain randomization and data
augmentation. However, as more factors of variation are introduced during
training, the optimization process becomes increasingly more difficult, leading
to low sample efficiency and unstable training. Instead of learning policies
directly from augmented data, we propose SOft Data Augmentation (SODA), a
method that decouples augmentation from policy learning. Specifically, SODA
imposes a soft constraint on the encoder that aims to maximize the mutual
information between latent representations of augmented and non-augmented data,
while the RL optimization process uses strictly non-augmented data. Empirical
evaluations are performed on diverse tasks from DeepMind Control suite as well
as a robotic manipulation task, and we find SODA to significantly advance
sample efficiency, generalization, and stability in training over
state-of-the-art vision-based RL methods.
</p>
<a href="http://arxiv.org/abs/2011.13389" target="_blank">arXiv:2011.13389</a> [<a href="http://arxiv.org/pdf/2011.13389" target="_blank">pdf</a>]

<h2>Depth-Aware Action Recognition: Pose-Motion Encoding through Temporal Heatmaps. (arXiv:2011.13399v1 [cs.CV])</h2>
<h3>Mattia Segu, Federico Pirovano, Gianmario Fumagalli, Amedeo Fabris</h3>
<p>Most state-of-the-art methods for action recognition rely only on 2D spatial
features encoding appearance, motion or pose. However, 2D data lacks the depth
information, which is crucial for recognizing fine-grained actions. In this
paper, we propose a depth-aware volumetric descriptor that encodes pose and
motion information in a unified representation for action classification
in-the-wild. Our framework is robust to many challenges inherent to action
recognition, e.g. variation in viewpoint, scene, clothing and body shape. The
key component of our method is the Depth-Aware Pose Motion representation
(DA-PoTion), a new video descriptor that encodes the 3D movement of semantic
keypoints of the human body. Given a video, we produce human joint heatmaps for
each frame using a state-of-the-art 3D human pose regressor and we give each of
them a unique color code according to the relative time in the clip. Then, we
aggregate such 3D time-encoded heatmaps for all human joints to obtain a
fixed-size descriptor (DA-PoTion), which is suitable for classifying actions
using a shallow 3D convolutional neural network (CNN). The DA-PoTion alone
defines a new state-of-the-art on the Penn Action Dataset. Moreover, we
leverage the intrinsic complementarity of our pose motion descriptor with
appearance based approaches by combining it with Inflated 3D ConvNet (I3D) to
define a new state-of-the-art on the JHMDB Dataset.
</p>
<a href="http://arxiv.org/abs/2011.13399" target="_blank">arXiv:2011.13399</a> [<a href="http://arxiv.org/pdf/2011.13399" target="_blank">pdf</a>]

<h2>Learning from Lexical Perturbations for Consistent Visual Question Answering. (arXiv:2011.13406v1 [cs.CV])</h2>
<h3>Spencer Whitehead, Hui Wu, Yi Ren Fung, Heng Ji, Rogerio Feris, Kate Saenko</h3>
<p>Existing Visual Question Answering (VQA) models are often fragile and
sensitive to input variations. In this paper, we propose a novel approach to
address this issue based on modular networks, which creates two questions
related by linguistic perturbations and regularizes the visual reasoning
process between them to be consistent during training. We show that our
framework markedly improves consistency and generalization ability,
demonstrating the value of controlled linguistic perturbations as a useful and
currently underutilized training and regularization tool for VQA models. We
also present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and
augmentation pipeline to create controllable linguistic variations of VQA
questions. Our benchmark uniquely draws from large-scale linguistic resources,
avoiding human annotation effort while maintaining data quality compared to
generative approaches. We benchmark existing VQA models using VQA P2 and
provide robustness analysis on each type of linguistic variation.
</p>
<a href="http://arxiv.org/abs/2011.13406" target="_blank">arXiv:2011.13406</a> [<a href="http://arxiv.org/pdf/2011.13406" target="_blank">pdf</a>]

<h2>Generative Layout Modeling using Constraint Graphs. (arXiv:2011.13417v1 [cs.CV])</h2>
<h3>Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas Guibas, Peter Wonka</h3>
<p>We propose a new generative model for layout generation. We generate layouts
in three steps. First, we generate the layout elements as nodes in a layout
graph. Second, we compute constraints between layout elements as edges in the
layout graph. Third, we solve for the final layout using constrained
optimization. For the first two steps, we build on recent transformer
architectures. The layout optimization implements the constraints efficiently.
We show three practical contributions compared to the state of the art: our
work requires no user input, produces higher quality layouts, and enables many
novel capabilities for conditional layout generation.
</p>
<a href="http://arxiv.org/abs/2011.13417" target="_blank">arXiv:2011.13417</a> [<a href="http://arxiv.org/pdf/2011.13417" target="_blank">pdf</a>]

<h2>Outcome Indistinguishability. (arXiv:2011.13426v1 [cs.LG])</h2>
<h3>Cynthia Dwork, Michael P. Kim, Omer Reingold, Guy N. Rothblum, Gal Yona</h3>
<p>Prediction algorithms assign numbers to individuals that are popularly
understood as individual "probabilities" -- what is the probability of 5-year
survival after cancer diagnosis? -- and which increasingly form the basis for
life-altering decisions. Drawing on an understanding of computational
indistinguishability developed in complexity theory and cryptography, we
introduce Outcome Indistinguishability. Predictors that are Outcome
Indistinguishable yield a generative model for outcomes that cannot be
efficiently refuted on the basis of the real-life observations produced by
Nature. We investigate a hierarchy of Outcome Indistinguishability definitions,
whose stringency increases with the degree to which distinguishers may access
the predictor in question. Our findings reveal that Outcome
Indistinguishability behaves qualitatively differently than previously studied
notions of indistinguishability. First, we provide constructions at all levels
of the hierarchy. Then, leveraging recently-developed machinery for proving
average-case fine-grained hardness, we obtain lower bounds on the complexity of
the more stringent forms of Outcome Indistinguishability. This hardness result
provides the first scientific grounds for the political argument that, when
inspecting algorithmic risk prediction instruments, auditors should be granted
oracle access to the algorithm, not simply historical predictions.
</p>
<a href="http://arxiv.org/abs/2011.13426" target="_blank">arXiv:2011.13426</a> [<a href="http://arxiv.org/pdf/2011.13426" target="_blank">pdf</a>]

<h2>Multi-view Human Pose and Shape Estimation Using Learnable Volumetric Aggregation. (arXiv:2011.13427v1 [cs.CV])</h2>
<h3>Soyong Shin, Eni Halilaj</h3>
<p>Human pose and shape estimation from RGB images is a highly sought after
alternative to marker-based motion capture, which is laborious, requires
expensive equipment, and constrains capture to laboratory environments.
Monocular vision-based algorithms, however, still suffer from rotational
ambiguities and are not ready for translation in healthcare applications, where
high accuracy is paramount. While fusion of data from multiple viewpoints could
overcome these challenges, current algorithms require further improvement to
obtain clinically acceptable accuracies. In this paper, we propose a learnable
volumetric aggregation approach to reconstruct 3D human body pose and shape
from calibrated multi-view images. We use a parametric representation of the
human body, which makes our approach directly applicable to medical
applications. Compared to previous approaches, our framework shows higher
accuracy and greater promise for real-time prediction, given its cost
efficiency.
</p>
<a href="http://arxiv.org/abs/2011.13427" target="_blank">arXiv:2011.13427</a> [<a href="http://arxiv.org/pdf/2011.13427" target="_blank">pdf</a>]

<h2>Explaining Deep Learning Models for Structured Data using Layer-Wise Relevance Propagation. (arXiv:2011.13429v1 [cs.LG])</h2>
<h3>hsan Ullah, Andre Rios, Vaibhav Gala, Susan Mckeever</h3>
<p>Trust and credibility in machine learning models is bolstered by the ability
of a model to explain itsdecisions. While explainability of deep learning
models is a well-known challenge, a further chal-lenge is clarity of the
explanation itself, which must be interpreted by downstream users.
Layer-wiseRelevance Propagation (LRP), an established explainability technique
developed for deep models incomputer vision, provides intuitive human-readable
heat maps of input images. We present the novelapplication of LRP for the first
time with structured datasets using a deep neural network (1D-CNN),for Credit
Card Fraud detection and Telecom Customer Churn prediction datasets. We show
how LRPis more effective than traditional explainability concepts of Local
Interpretable Model-agnostic Ex-planations (LIME) and Shapley Additive
Explanations (SHAP) for explainability. This effectivenessis both local to a
sample level and holistic over the whole testing set. We also discuss the
significantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP
(108s), and thus its poten-tial for real time application scenarios. In
addition, our validation of LRP has highlighted features forenhancing model
performance, thus opening up a new area of research of using XAI as an
approachfor feature subset selection
</p>
<a href="http://arxiv.org/abs/2011.13429" target="_blank">arXiv:2011.13429</a> [<a href="http://arxiv.org/pdf/2011.13429" target="_blank">pdf</a>]

<h2>ShapeFlow: Dynamic Shape Interpreter for TensorFlow. (arXiv:2011.13452v1 [cs.LG])</h2>
<h3>Sahil Verma, Zhendong Su</h3>
<p>We present ShapeFlow, a dynamic abstract interpreter for TensorFlow which
quickly catches tensor shape incompatibility errors, one of the most common
bugs in deep learning code. ShapeFlow shares the same APIs as TensorFlow but
only captures and emits tensor shapes, its abstract domain. ShapeFlow
constructs a custom shape computational graph, similar to the computational
graph used by TensorFlow. ShapeFlow requires no code annotation or code
modification by the programmer, and therefore is convenient to use. We evaluate
ShapeFlow on 52 programs collected by prior empirical studies to show how fast
and accurately it can catch shape incompatibility errors compared to
TensorFlow. We use two baselines: a worst-case training dataset size and a more
realistic dataset size. ShapeFlow detects shape incompatibility errors highly
accurately -- with no false positives and a single false negative -- and highly
efficiently -- with an average speed-up of 499X and 24X for the first and
second baseline, respectively. We believe ShapeFlow is a practical tool that
benefits machine learning developers. We will open-source ShapeFlow on GitHub
to make it publicly available to both the developer and research communities.
</p>
<a href="http://arxiv.org/abs/2011.13452" target="_blank">arXiv:2011.13452</a> [<a href="http://arxiv.org/pdf/2011.13452" target="_blank">pdf</a>]

<h2>Score-Based Generative Modeling through Stochastic Differential Equations. (arXiv:2011.13456v1 [cs.LG])</h2>
<h3>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole</h3>
<p>Creating noise from data is easy; creating data from noise is generative
modeling. We present a stochastic differential equation (SDE) that smoothly
transforms a complex data distribution to a known prior distribution by slowly
injecting noise, and a corresponding reverse-time SDE that transforms the prior
distribution back into the data distribution by slowly removing the noise.
Crucially, the reverse-time SDE depends only on the time-dependent gradient
field (a.k.a., score) of the perturbed data distribution. By leveraging
advances in score-based generative modeling, we can accurately estimate these
scores with neural networks, and use numerical SDE solvers to generate samples.
We show that this framework encapsulates previous approaches in diffusion
probabilistic modeling and score-based generative modeling, and allows for new
sampling procedures. In particular, we introduce a predictor-corrector
framework to correct errors in the evolution of the discretized reverse-time
SDE. We also derive an equivalent neural ODE that samples from the same
distribution as the SDE, which enables exact likelihood computation, and
improved sampling efficiency. In addition, our framework enables conditional
generation with an unconditional model, as we demonstrate with experiments on
class-conditional generation, image inpainting, and colorization. Combined with
multiple architectural improvements, we achieve record-breaking performance for
unconditional image generation on CIFAR-10 with an Inception score of 9.89 and
FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high
fidelity generation of $1024 \times 1024$ images for the first time from a
score-based generative model.
</p>
<a href="http://arxiv.org/abs/2011.13456" target="_blank">arXiv:2011.13456</a> [<a href="http://arxiv.org/pdf/2011.13456" target="_blank">pdf</a>]

<h2>Meta-learning in natural and artificial intelligence. (arXiv:2011.13464v1 [cs.AI])</h2>
<h3>Jane X. Wang</h3>
<p>Meta-learning, or learning to learn, has gained renewed interest in recent
years within the artificial intelligence community. However, meta-learning is
incredibly prevalent within nature, has deep roots in cognitive science and
psychology, and is currently studied in various forms within neuroscience. The
aim of this review is to recast previous lines of research in the study of
biological intelligence within the lens of meta-learning, placing these works
into a common framework. More recent points of interaction between AI and
neuroscience will be discussed, as well as interesting new directions that
arise under this perspective.
</p>
<a href="http://arxiv.org/abs/2011.13464" target="_blank">arXiv:2011.13464</a> [<a href="http://arxiv.org/pdf/2011.13464" target="_blank">pdf</a>]

<h2>Exploring grid topology reconfiguration using a simple deep reinforcement learning approach. (arXiv:2011.13465v1 [cs.LG])</h2>
<h3>Medha Subramanian, Jan Viebahn, Simon Tindemans, Benjamin Donnot, Antoine Marot</h3>
<p>System operators are faced with increasingly volatile operating conditions.
In order to manage system reliability in a cost-effective manner, control room
operators are turning to computerised decision support tools based on AI and
machine learning. Specifically, Reinforcement Learning (RL) is a promising
technique to train agents that suggest grid control actions to operators. In
this paper, a simple baseline approach is presented using RL to represent an
artificial control room operator that can operate a IEEE 14-bus test case for a
duration of 1 week. This agent takes topological switching actions to control
power flows on the grid, and is trained on only a single well-chosen scenario.
The behaviour of this agent is tested on different time-series of generation
and demand, demonstrating its ability to operate the grid successfully in 965
out of 1000 scenarios. The type and variability of topologies suggested by the
agent are analysed across the test scenarios, demonstrating efficient and
diverse agent behaviour.
</p>
<a href="http://arxiv.org/abs/2011.13465" target="_blank">arXiv:2011.13465</a> [<a href="http://arxiv.org/pdf/2011.13465" target="_blank">pdf</a>]

<h2>Episodic Self-Imitation Learning with Hindsight. (arXiv:2011.13467v1 [cs.AI])</h2>
<h3>Tianhong Dai, Hengyan Liu, Anil Anthony Bharath</h3>
<p>Episodic self-imitation learning, a novel self-imitation algorithm with a
trajectory selection module and an adaptive loss function, is proposed to speed
up reinforcement learning. Compared to the original self-imitation learning
algorithm, which samples good state-action pairs from the experience replay
buffer, our agent leverages entire episodes with hindsight to aid
self-imitation learning. A selection module is introduced to filter
uninformative samples from each episode of the update. The proposed method
overcomes the limitations of the standard self-imitation learning algorithm, a
transitions-based method which performs poorly in handling continuous control
environments with sparse rewards. From the experiments, episodic self-imitation
learning is shown to perform better than baseline on-policy algorithms,
achieving comparable performance to state-of-the-art off-policy algorithms in
several simulated robot control tasks. The trajectory selection module is shown
to prevent the agent learning undesirable hindsight experiences. With the
capability of solving sparse reward problems in continuous control settings,
episodic self-imitation learning has the potential to be applied to real-world
problems that have continuous action spaces, such as robot guidance and
manipulation.
</p>
<a href="http://arxiv.org/abs/2011.13467" target="_blank">arXiv:2011.13467</a> [<a href="http://arxiv.org/pdf/2011.13467" target="_blank">pdf</a>]

<h2>Fine-Grained Re-Identification. (arXiv:2011.13475v1 [cs.CV])</h2>
<h3>Priyank Pathak</h3>
<p>Research into the task of re-identification (ReID) is picking up momentum in
computer vision for its many use cases and zero-shot learning nature. This
paper proposes a computationally efficient fine-grained ReID model, FGReID,
which is among the first models to unify image and video ReID while keeping the
number of training parameters minimal. FGReID takes advantage of video-based
pre-training and spatial feature attention to improve performance on both video
and image ReID tasks. FGReID achieves state-of-the-art (SOTA) on MARS,
iLIDS-VID, and PRID-2011 video person ReID benchmarks. Eliminating temporal
pooling yields an image ReID model that surpasses SOTA on CUHK01 and Market1501
image person ReID benchmarks. The FGReID achieves near SOTA performance on the
vehicle ReID dataset VeRi as well, demonstrating its ability to generalize.
Additionally we do an ablation study analyzing the key features influencing
model performance on ReID tasks. Finally, we discuss the moral dilemmas related
to ReID tasks, including the potential for misuse. Code for this work is
publicly available at https:
//github.com/ppriyank/Fine-grained-ReIdentification.
</p>
<a href="http://arxiv.org/abs/2011.13475" target="_blank">arXiv:2011.13475</a> [<a href="http://arxiv.org/pdf/2011.13475" target="_blank">pdf</a>]

<h2>Bidirectional Modeling and Analysis of Brain Aging with Normalizing Flows. (arXiv:2011.13484v1 [cs.CV])</h2>
<h3>Matthias Wilms, Jordan J. Bannister, Pauline Mouches, M. Ethan MacDonald, Deepthi Rajashekar, S&#xf6;nke Langner, Nils D. Forkert</h3>
<p>Brain aging is a widely studied longitudinal process throughout which the
brain undergoes considerable morphological changes and various machine learning
approaches have been proposed to analyze it. Within this context, brain age
prediction from structural MR images and age-specific brain morphology template
generation are two problems that have attracted much attention. While most
approaches tackle these tasks independently, we assume that they are inverse
directions of the same functional bidirectional relationship between a brain's
morphology and an age variable. In this paper, we propose to model this
relationship with a single conditional normalizing flow, which unifies brain
age prediction and age-conditioned generative modeling in a novel way. In an
initial evaluation of this idea, we show that our normalizing flow brain aging
model can accurately predict brain age while also being able to generate
age-specific brain morphology templates that realistically represent the
typical aging trend in a healthy population. This work is a step towards
unified modeling of functional relationships between 3D brain morphology and
clinical variables of interest with powerful normalizing flows.
</p>
<a href="http://arxiv.org/abs/2011.13484" target="_blank">arXiv:2011.13484</a> [<a href="http://arxiv.org/pdf/2011.13484" target="_blank">pdf</a>]

<h2>Interactive Machine Learning of Musical Gesture. (arXiv:2011.13487v1 [cs.LG])</h2>
<h3>Federico Ghelli Visi, Atau Tanaka</h3>
<p>This chapter presents an overview of Interactive Machine Learning (IML)
techniques applied to the analysis and design of musical gestures. We go
through the main challenges and needs related to capturing, analysing, and
applying IML techniques to human bodily gestures with the purpose of performing
with sound synthesis systems. We discuss how different algorithms may be used
to accomplish different tasks, including interacting with complex synthesis
techniques and exploring interaction possibilities by means of Reinforcement
Learning (RL) in an interaction paradigm we developed called Assisted
Interactive Machine Learning (AIML). We conclude the chapter with a description
of how some of these techniques were employed by the authors for the
development of four musical pieces, thus outlining the implications that IML
have for musical practice.
</p>
<a href="http://arxiv.org/abs/2011.13487" target="_blank">arXiv:2011.13487</a> [<a href="http://arxiv.org/pdf/2011.13487" target="_blank">pdf</a>]

<h2>Fast IR Drop Estimation with Machine Learning. (arXiv:2011.13491v1 [cs.LG])</h2>
<h3>Zhiyao Xie, Hai Li, Xiaoqing Xu, Jiang Hu, Yiran Chen</h3>
<p>IR drop constraint is a fundamental requirement enforced in almost all chip
designs. However, its evaluation takes a long time, and mitigation techniques
for fixing violations may require numerous iterations. As such, fast and
accurate IR drop prediction becomes critical for reducing design turnaround
time. Recently, machine learning (ML) techniques have been actively studied for
fast IR drop estimation due to their promise and success in many fields. These
studies target at various design stages with different emphasis, and
accordingly, different ML algorithms are adopted and customized. This paper
provides a review to the latest progress in ML-based IR drop estimation
techniques. It also serves as a vehicle for discussing some general challenges
faced by ML applications in electronics design automation (EDA), and
demonstrating how to integrate ML models with conventional techniques for the
better efficiency of EDA tools.
</p>
<a href="http://arxiv.org/abs/2011.13491" target="_blank">arXiv:2011.13491</a> [<a href="http://arxiv.org/pdf/2011.13491" target="_blank">pdf</a>]

<h2>Spectral Analysis and Stability of Deep Neural Dynamics. (arXiv:2011.13492v1 [cs.LG])</h2>
<h3>Jan Drgona, Elliott Skomski, Soumya Vasisht, Aaron Tuor, Draguna Vrabie</h3>
<p>Our modern history of deep learning follows the arc of famous emergent
disciplines in engineering (e.g. aero- and fluid dynamics) when theory lagged
behind successful practical applications. Viewing neural networks from a
dynamical systems perspective, in this work, we propose a novel
characterization of deep neural networks as pointwise affine maps, making them
accessible to a broader range of analysis methods to help close the gap between
theory and practice. We begin by showing the equivalence of neural networks
with parameter-varying affine maps parameterized by the state (feature) vector.
As the paper's main results, we provide necessary and sufficient conditions for
the global stability of generic deep feedforward neural networks. Further, we
identify links between the spectral properties of layer-wise weight
parametrizations, different activation functions, and their effect on the
overall network's eigenvalue spectra. We analyze a range of neural networks
with varying weight initializations, activation functions, bias terms, and
depths. Our view of neural networks as affine parameter varying maps allows us
to "crack open the black box" of global neural network dynamical behavior
through visualization of stationary points, regions of attraction, state-space
partitioning, eigenvalue spectra, and stability properties. Our analysis covers
neural networks both as an end-to-end function and component-wise without
simplifying assumptions or approximations. The methods we develop here provide
tools to establish relationships between global neural dynamical properties and
their constituent components which can aid in the principled design of neural
networks for dynamics modeling and optimal control.
</p>
<a href="http://arxiv.org/abs/2011.13492" target="_blank">arXiv:2011.13492</a> [<a href="http://arxiv.org/pdf/2011.13492" target="_blank">pdf</a>]

<h2>FIST: A Feature-Importance Sampling and Tree-Based Method for Automatic Design Flow Parameter Tuning. (arXiv:2011.13493v1 [cs.LG])</h2>
<h3>Zhiyao Xie, Guan-Qi Fang, Yu-Hung Huang, Haoxing Ren, Yanqing Zhang, Brucek Khailany, Shao-Yun Fang, Jiang Hu, Yiran Chen, Erick Carvajal Barboza</h3>
<p>Design flow parameters are of utmost importance to chip design quality and
require a painfully long time to evaluate their effects. In reality, flow
parameter tuning is usually performed manually based on designers' experience
in an ad hoc manner. In this work, we introduce a machine learning-based
automatic parameter tuning methodology that aims to find the best design
quality with a limited number of trials. Instead of merely plugging in machine
learning engines, we develop clustering and approximate sampling techniques for
improving tuning efficiency. The feature extraction in this method can reuse
knowledge from prior designs. Furthermore, we leverage a state-of-the-art
XGBoost model and propose a novel dynamic tree technique to overcome
overfitting. Experimental results on benchmark circuits show that our approach
achieves 25% improvement in design quality or 37% reduction in sampling cost
compared to random forest method, which is the kernel of a highly cited
previous work. Our approach is further validated on two industrial designs. By
sampling less than 0.02% of possible parameter sets, it reduces area by 1.83%
and 1.43% compared to the best solutions hand-tuned by experienced designers.
</p>
<a href="http://arxiv.org/abs/2011.13493" target="_blank">arXiv:2011.13493</a> [<a href="http://arxiv.org/pdf/2011.13493" target="_blank">pdf</a>]

<h2>PowerNet: Transferable Dynamic IR Drop Estimation via Maximum Convolutional Neural Network. (arXiv:2011.13494v1 [cs.LG])</h2>
<h3>Zhiyao Xie, Haoxing Ren, Brucek Khailany, Ye Sheng, Santosh Santosh, Jiang Hu, Yiran Chen</h3>
<p>IR drop is a fundamental constraint required by almost all chip designs.
However, its evaluation usually takes a long time that hinders mitigation
techniques for fixing its violations. In this work, we develop a fast dynamic
IR drop estimation technique, named PowerNet, based on a convolutional neural
network (CNN). It can handle both vector-based and vectorless IR analyses.
Moreover, the proposed CNN model is general and transferable to different
designs. This is in contrast to most existing machine learning (ML) approaches,
where a model is applicable only to a specific design. Experimental results
show that PowerNet outperforms the latest ML method by 9% in accuracy for the
challenging case of vectorless IR drop and achieves a 30 times speedup compared
to an accurate IR drop commercial tool. Further, a mitigation tool guided by
PowerNet reduces IR drop hotspots by 26% and 31% on two industrial designs,
respectively, with very limited modification on their power grids.
</p>
<a href="http://arxiv.org/abs/2011.13494" target="_blank">arXiv:2011.13494</a> [<a href="http://arxiv.org/pdf/2011.13494" target="_blank">pdf</a>]

<h2>Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces. (arXiv:2011.13495v1 [cs.CV])</h2>
<h3>Baorui Ma, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker</h3>
<p>Reconstructing continuous surfaces from 3D point clouds is a fundamental
operation in 3D geometry processing. Several recent state-of-the-art methods
address this problem using neural networks to learn signed distance functions
(SDFs). In this paper, we introduce Neural-Pull, a new approach that is simple
and leads to high quality SDFs. Specifically, we train a neural network to pull
query 3D locations to their closest neighbors on the surface using the
predicted signed distance values and the gradient at the query locations, both
of which are computed by the network itself. The pulling operation moves each
query location with a stride given by the distance predicted by the network.
Based on the sign of the distance, this may move the query location along or
against the direction of the gradient of the SDF. This is a differentiable
operation that allows us to update the signed distance value and the gradient
simultaneously during training. Our outperforming results under widely used
benchmarks demonstrate that we can learn SDFs more accurately and flexibly for
surface reconstruction and single image reconstruction than the
state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2011.13495" target="_blank">arXiv:2011.13495</a> [<a href="http://arxiv.org/pdf/2011.13495" target="_blank">pdf</a>]

<h2>Tractable loss function and color image generation of multinary restricted Boltzmann machine. (arXiv:2011.13509v1 [cs.CV])</h2>
<h3>Juno Hwang, Wonseok Hwang, Junghyo Jo</h3>
<p>The restricted Boltzmann machine (RBM) is a representative generative model
based on the concept of statistical mechanics. In spite of the strong merit of
interpretability, unavailability of backpropagation makes it less competitive
than other generative models. Here we derive differentiable loss functions for
both binary and multinary RBMs. Then we demonstrate their learnability and
performance by generating colored face images.
</p>
<a href="http://arxiv.org/abs/2011.13509" target="_blank">arXiv:2011.13509</a> [<a href="http://arxiv.org/pdf/2011.13509" target="_blank">pdf</a>]

<h2>Physics-Informed Neural Network for Modelling the Thermochemical Curing Process of Composite-Tool Systems During Manufacture. (arXiv:2011.13511v1 [cs.LG])</h2>
<h3>Sina Amini Niaki, Ehsan Haghighat, Xinglong Li, Trevor Campbell, Reza Vaziri</h3>
<p>We present a Physics-Informed Neural Network (PINN) to simulate the
thermochemical evolution of a composite material on a tool undergoing cure in
an autoclave. In particular, we solve the governing coupled system of
differential equations -- including conductive heat transfer and resin cure
kinetics -- by optimizing the parameters of a deep neural network using a
physics-based loss function. To account for the vastly different behaviour of
thermal conduction and resin cure, we design a PINN consisting of two
disconnected subnetworks, and develop a sequential training algorithm that
mitigates instability present in traditional training methods. Further, we
incorporate explicit discontinuities into the DNN at the composite-tool
interface and enforce known physical behaviour directly in the loss function to
improve the solution near the interface. Finally, we train the PINN with a
technique that automatically adapts the weights on the loss terms corresponding
to PDE, boundary, interface, and initial conditions. The performance of the
proposed PINN is demonstrated in multiple scenarios with different material
thicknesses and thermal boundary conditions.
</p>
<a href="http://arxiv.org/abs/2011.13511" target="_blank">arXiv:2011.13511</a> [<a href="http://arxiv.org/pdf/2011.13511" target="_blank">pdf</a>]

<h2>Human-in-the-loop Cueing Strategy for Gait Rehabilitation. (arXiv:2011.13516v1 [cs.RO])</h2>
<h3>Tina LY Wu, Anna Murphy, Chao Chen, Dana Kulic</h3>
<p>External feedback in the form of visual, auditory and tactile cues has been
used to assist patients to overcome mobility challenges. However, these cues
can become less effective over time. There is limited research on adapting cues
to account for inter and intra-personal variations in cue responsiveness. We
propose a cue-provision framework that consists of a gait performance
monitoring algorithm and an adaptive cueing strategy to improve gait
performance. The proposed approach learns a model of the person's response to
cues using Gaussian Process regression. The model is then used within an
on-line optimization algorithm to generate cues to improve gait performance. We
conduct a study with healthy participants to evaluate the ability of the
adaptive cueing strategy to influence human gait, and compare its effectiveness
to two other cueing approaches: the standard fixed cue approach and a
proportional cue approach. The results show that adaptive cueing is more
effective in changing the person's gait state once the response model is
learned compared to the other methods.
</p>
<a href="http://arxiv.org/abs/2011.13516" target="_blank">arXiv:2011.13516</a> [<a href="http://arxiv.org/pdf/2011.13516" target="_blank">pdf</a>]

<h2>Efficient Information Diffusion in Time-Varying Graphs through Deep Reinforcement Learning. (arXiv:2011.13518v1 [cs.LG])</h2>
<h3>Matheus R. F. Mendon&#xe7;a, Andr&#xe9; M. S. Barreto, Artur Ziviani</h3>
<p>Network seeding for efficient information diffusion over time-varying
graphs~(TVGs) is a challenging task with many real-world applications. There
are several ways to model this spatio-temporal influence maximization problem,
but the ultimate goal is to determine the best moment for a node to start the
diffusion process. In this context, we propose Spatio-Temporal Influence
Maximization~(STIM), a model trained with Reinforcement Learning and Graph
Embedding over a set of artificial TVGs that is capable of learning the
temporal behavior and connectivity pattern of each node, allowing it to predict
the best moment to start a diffusion through the TVG. We also develop a special
set of artificial TVGs used for training that simulate a stochastic diffusion
process in TVGs, showing that the STIM network can learn an efficient policy
even over a non-deterministic environment. STIM is also evaluated with a
real-world TVG, where it also manages to efficiently propagate information
through the nodes. Finally, we also show that the STIM model has a time
complexity of $O(|E|)$. STIM, therefore, presents a novel approach for
efficient information diffusion in TVGs, being highly versatile, where one can
change the goal of the model by simply changing the adopted reward function.
</p>
<a href="http://arxiv.org/abs/2011.13518" target="_blank">arXiv:2011.13518</a> [<a href="http://arxiv.org/pdf/2011.13518" target="_blank">pdf</a>]

<h2>Net2: A Graph Attention Network Method Customized for Pre-Placement Net Length Estimation. (arXiv:2011.13522v1 [cs.LG])</h2>
<h3>Zhiyao Xie, Rongjian Liang, Xiaoqing Xu, Jiang Hu, Yixiao Duan, Yiran Chen</h3>
<p>Net length is a key proxy metric for optimizing timing and power across
various stages of a standard digital design flow. However, the bulk of net
length information is not available until cell placement, and hence it is a
significant challenge to explicitly consider net length optimization in design
stages prior to placement, such as logic synthesis. This work addresses this
challenge by proposing a graph attention network method with customization,
called Net2, to estimate individual net length before cell placement. Its
accuracy-oriented version Net2a achieves about 15% better accuracy than several
previous works in identifying both long nets and long critical paths. Its fast
version Net2f is more than 1000 times faster than placement while still
outperforms previous works and other neural network techniques in terms of
various accuracy metrics.
</p>
<a href="http://arxiv.org/abs/2011.13522" target="_blank">arXiv:2011.13522</a> [<a href="http://arxiv.org/pdf/2011.13522" target="_blank">pdf</a>]

<h2>Robust Attacks on Deep Learning Face Recognition in the Physical World. (arXiv:2011.13526v1 [cs.CV])</h2>
<h3>Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, Xiaojiang Du</h3>
<p>Deep neural networks (DNNs) have been increasingly used in face recognition
(FR) systems. Recent studies, however, show that DNNs are vulnerable to
adversarial examples, which can potentially mislead the FR systems using DNNs
in the physical world. Existing attacks on these systems either generate
perturbations working merely in the digital world, or rely on customized
equipments to generate perturbations and are not robust in varying physical
environments. In this paper, we propose FaceAdv, a physical-world attack that
crafts adversarial stickers to deceive FR systems. It mainly consists of a
sticker generator and a transformer, where the former can craft several
stickers with different shapes and the latter transformer aims to digitally
attach stickers to human faces and provide feedbacks to the generator to
improve the effectiveness of stickers. We conduct extensive experiments to
evaluate the effectiveness of FaceAdv on attacking 3 typical FR systems (i.e.,
ArcFace, CosFace and FaceNet). The results show that compared with a
state-of-the-art attack, FaceAdv can significantly improve success rate of both
dodging and impersonating attacks. We also conduct comprehensive evaluations to
demonstrate the robustness of FaceAdv.
</p>
<a href="http://arxiv.org/abs/2011.13526" target="_blank">arXiv:2011.13526</a> [<a href="http://arxiv.org/pdf/2011.13526" target="_blank">pdf</a>]

<h2>The NEOLIX Open Dataset for AutonomousDriving. (arXiv:2011.13528v1 [cs.CV])</h2>
<h3>Lichao Wang, Lanxin Lei, Hongli Song, Weibao Wang</h3>
<p>With the gradual maturity of 5G technology,autonomous driving technology has
attracted moreand more attention among the research commu-nity. Autonomous
driving vehicles rely on the co-operation of artificial intelligence, visual
comput-ing, radar, monitoring equipment and GPS, whichenables computers to
operate motor vehicles auto-matically and safely without human
interference.However, the large-scale dataset for training andsystem evaluation
is still a hot potato in the devel-opment of robust perception models. In this
paper,we present the NEOLIX dataset and its applica-tions in the autonomous
driving area. Our datasetincludes about 30,000 frames with point cloud la-bels,
and more than 600k 3D bounding boxes withannotations. The data collection
covers multipleregions, and various driving conditions, includingday, night,
dawn, dusk and sunny day. In orderto label this complete dataset, we developed
vari-ous tools and algorithms specified for each task tospeed up the labelling
process. It is expected thatour dataset and related algorithms can support
andmotivate researchers for the further developmentof autonomous driving in the
field of computer vi-sion.
</p>
<a href="http://arxiv.org/abs/2011.13528" target="_blank">arXiv:2011.13528</a> [<a href="http://arxiv.org/pdf/2011.13528" target="_blank">pdf</a>]

<h2>They are Not Completely Useless: Towards Recycling Transferable Unlabeled Data for Class-Mismatched Semi-Supervised Learning. (arXiv:2011.13529v1 [cs.LG])</h2>
<h3>Huang Zhuo, Tai Ying, Wang Chengjie, Yang Jian, Gong Chen</h3>
<p>Semi-Supervised Learning (SSL) with mismatched classes deals with the problem
that the classes-of-interests in the limited labeled data is only a subset of
the classes in massive unlabeled data. As a result, the classes only possessed
by the unlabeled data may mislead the classifier training and thus hindering
the realistic landing of various SSL methods. To solve this problem, existing
methods usually divide unlabeled data to in-distribution (ID) data and
out-of-distribution (OOD) data, and directly discard or weaken the OOD data to
avoid their adverse impact. In other words, they treat OOD data as completely
useless and thus the potential valuable information for classification
contained by them is totally ignored. To remedy this defect, this paper
proposes a "Transferable OOD data Recycling" (TOOR) method which properly
utilizes ID data as well as the "recyclable" OOD data to enrich the information
for conducting class-mismatched SSL. Specifically, TOOR firstly attributes all
unlabeled data to ID data or OOD data, among which the ID data are directly
used for training. Then we treat the OOD data that have a close relationship
with ID data and labeled data as recyclable, and employ adversarial domain
adaptation to project them to the space of ID data and labeled data. In other
words, the recyclability of an OOD datum is evaluated by its transferability,
and the recyclable OOD data are transferred so that they are compatible with
the distribution of known classes-of-interests. Consequently, our TOOR method
extracts more information from unlabeled data than existing approaches, so it
can achieve the improved performance which is demonstrated by the experiments
on typical benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.13529" target="_blank">arXiv:2011.13529</a> [<a href="http://arxiv.org/pdf/2011.13529" target="_blank">pdf</a>]

<h2>Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness. (arXiv:2011.13538v1 [cs.LG])</h2>
<h3>Yilun Jin, Lixin Fan, Kam Woh Ng, Ce Ju, Qiang Yang</h3>
<p>Deep neural networks (DNNs) are known to be prone to adversarial attacks, for
which many remedies are proposed. While adversarial training (AT) is regarded
as the most robust defense, it suffers from poor performance both on clean
examples and under other types of attacks, e.g. attacks with larger
perturbations. Meanwhile, regularizers that encourage uncertain outputs, such
as entropy maximization (EntM) and label smoothing (LS) can maintain accuracy
on clean examples and improve performance under weak attacks, yet their ability
to defend against strong attacks is still in doubt. In this paper, we revisit
uncertainty promotion regularizers, including EntM and LS, in the field of
adversarial learning. We show that EntM and LS alone provide robustness only
under small perturbations. Contrarily, we show that uncertainty promotion
regularizers complement AT in a principled manner, consistently improving
performance on both clean examples and under various attacks, especially
attacks with large perturbations. We further analyze how uncertainty promotion
regularizers enhance the performance of AT from the perspective of Jacobian
matrices $\nabla_X f(X;\theta)$, and find out that EntM effectively shrinks the
norm of Jacobian matrices and hence promotes robustness.
</p>
<a href="http://arxiv.org/abs/2011.13538" target="_blank">arXiv:2011.13538</a> [<a href="http://arxiv.org/pdf/2011.13538" target="_blank">pdf</a>]

<h2>Patch-VQ: 'Patching Up' the Video Quality Problem. (arXiv:2011.13544v1 [cs.CV])</h2>
<h3>Zhenqiang Ying (1), Maniratnam Mandal (1), Deepti Ghadiyaram (2), Alan Bovik (1) ((1) University of Texas at Austin, (2) Facebook AI)</h3>
<p>No-reference (NR) perceptual video quality assessment (VQA) is a complex,
unsolved, and important problem to social and streaming media applications.
Efficient and accurate video quality predictors are needed to monitor and guide
the processing of billions of shared, often imperfect, user-generated content
(UGC). Unfortunately, current NR models are limited in their prediction
capabilities on real-world, "in-the-wild" UGC video data. To advance progress
on this problem, we created the largest (by far) subjective video quality
dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time
localized video patches ('v-patches'), and 5.5M human perceptual quality
annotations. Using this, we created two unique NR-VQA models: (a) a
local-to-global region-based NR VQA architecture (called PVQ) that learns to
predict global video quality and achieves state-of-the-art performance on 3 UGC
datasets, and (b) a first-of-a-kind space-time video quality mapping engine
(called PVQ Mapper) that helps localize and visualize perceptual distortions in
space and time. We will make the new database and prediction models available
immediately following the review process.
</p>
<a href="http://arxiv.org/abs/2011.13544" target="_blank">arXiv:2011.13544</a> [<a href="http://arxiv.org/pdf/2011.13544" target="_blank">pdf</a>]

<h2>Self-Supervised Time Series Representation Learning by Inter-Intra Relational Reasoning. (arXiv:2011.13548v1 [cs.LG])</h2>
<h3>Haoyi Fan, Fengbin Zhang, Yue Gao</h3>
<p>Self-supervised learning achieves superior performance in many domains by
extracting useful representations from the unlabeled data. However, most of
traditional self-supervised methods mainly focus on exploring the inter-sample
structure while less efforts have been concentrated on the underlying
intra-temporal structure, which is important for time series data. In this
paper, we present SelfTime: a general self-supervised time series
representation learning framework, by exploring the inter-sample relation and
intra-temporal relation of time series to learn the underlying structure
feature on the unlabeled time series. Specifically, we first generate the
inter-sample relation by sampling positive and negative samples of a given
anchor sample, and intra-temporal relation by sampling time pieces from this
anchor. Then, based on the sampled relation, a shared feature extraction
backbone combined with two separate relation reasoning heads are employed to
quantify the relationships of the sample pairs for inter-sample relation
reasoning, and the relationships of the time piece pairs for intra-temporal
relation reasoning, respectively. Finally, the useful representations of time
series are extracted from the backbone under the supervision of relation
reasoning heads. Experimental results on multiple real-world time series
datasets for time series classification task demonstrate the effectiveness of
the proposed method. Code and data are publicly available at
https://haoyfan.github.io/.
</p>
<a href="http://arxiv.org/abs/2011.13548" target="_blank">arXiv:2011.13548</a> [<a href="http://arxiv.org/pdf/2011.13548" target="_blank">pdf</a>]

<h2>Tight Hardness Results for Training Depth-2 ReLU Networks. (arXiv:2011.13550v1 [cs.LG])</h2>
<h3>Surbhi Goel, Adam Klivans, Pasin Manurangsi, Daniel Reichman</h3>
<p>We prove several hardness results for training depth-2 neural networks with
the ReLU activation function; these networks are simply weighted sums (that may
include negative coefficients) of ReLUs. Our goal is to output a depth-2 neural
network that minimizes the square loss with respect to a given training set. We
prove that this problem is NP-hard already for a network with a single ReLU. We
also prove NP-hardness for outputting a weighted sum of $k$ ReLUs minimizing
the squared error (for $k&gt;1$) even in the realizable setting (i.e., when the
labels are consistent with an unknown depth-2 ReLU network). We are also able
to obtain lower bounds on the running time in terms of the desired additive
error $\epsilon$. To obtain our lower bounds, we use the Gap Exponential Time
Hypothesis (Gap-ETH) as well as a new hypothesis regarding the hardness of
approximating the well known Densest $\kappa$-Subgraph problem in
subexponential time (these hypotheses are used separately in proving different
lower bounds). For example, we prove that under reasonable hardness
assumptions, any proper learning algorithm for finding the best fitting ReLU
must run in time exponential in $1/\epsilon^2$. Together with a previous work
regarding improperly learning a ReLU (Goel et al., COLT'17), this implies the
first separation between proper and improper algorithms for learning a ReLU. We
also study the problem of properly learning a depth-2 network of ReLUs with
bounded weights giving new (worst-case) upper bounds on the running time needed
to learn such networks both in the realizable and agnostic settings. Our upper
bounds on the running time essentially matches our lower bounds in terms of the
dependency on $\epsilon$.
</p>
<a href="http://arxiv.org/abs/2011.13550" target="_blank">arXiv:2011.13550</a> [<a href="http://arxiv.org/pdf/2011.13550" target="_blank">pdf</a>]

<h2>Association: Remind Your GAN not to Forget. (arXiv:2011.13553v1 [cs.CV])</h2>
<h3>Yi Gu, Yuting Gao, Ruoxin Chen, Feiyang Cai, Jie Li, Chentao Wu</h3>
<p>Neural networks are susceptible to catastrophic forgetting. They fail to
preserve previously acquired knowledge when adapting to new tasks. Inspired by
human associative memory system, we propose a brain-like approach that imitates
the associative learning process to achieve continual learning. We design a
heuristics mechanism to potentiatively stimulates the model, which guides the
model to recall the historical episodes based on the current circumstance and
obtained association experience. Besides, a distillation measure is added to
depressively alter the efficacy of synaptic transmission, which dampens the
feature reconstruction learning for new task. The framework is mediated by
potentiation and depression stimulation that play opposing roles in directing
synaptic and behavioral plasticity. It requires no access to the original data
and is more similar to human cognitive process. Experiments demonstrate the
effectiveness of our method in alleviating catastrophic forgetting on continual
image reconstruction problems.
</p>
<a href="http://arxiv.org/abs/2011.13553" target="_blank">arXiv:2011.13553</a> [<a href="http://arxiv.org/pdf/2011.13553" target="_blank">pdf</a>]

<h2>SocialGuard: An Adversarial Example Based Privacy-Preserving Technique for Social Images. (arXiv:2011.13560v1 [cs.CV])</h2>
<h3>Mingfu Xue, Shichang Sun, Zhiyu Wu, Can He, Jian Wang, Weiqiang Liu</h3>
<p>The popularity of various social platforms has prompted more people to share
their routine photos online. However, undesirable privacy leakages occur due to
such online photo sharing behaviors. Advanced deep neural network (DNN) based
object detectors can easily steal users' personal information exposed in shared
photos. In this paper, we propose a novel adversarial example based
privacy-preserving technique for social images against object detectors based
privacy stealing. Specifically, we develop an Object Disappearance Algorithm to
craft two kinds of adversarial social images. One can hide all objects in the
social images from being detected by an object detector, and the other can make
the customized sensitive objects be incorrectly classified by the object
detector. The Object Disappearance Algorithm constructs perturbation on a clean
social image. After being injected with the perturbation, the social image can
easily fool the object detector, while its visual quality will not be degraded.
We use two metrics, privacy-preserving success rate and privacy leakage rate,
to evaluate the effectiveness of the proposed method. Experimental results show
that, the proposed method can effectively protect the privacy of social images.
The privacy-preserving success rates of the proposed method on MS-COCO and
PASCAL VOC 2007 datasets are high up to 96.1% and 99.3%, respectively, and the
privacy leakage rates on these two datasets are as low as 0.57% and 0.07%,
respectively. In addition, compared with existing image processing methods (low
brightness, noise, blur, mosaic and JPEG compression), the proposed method can
achieve much better performance in privacy protection and image visual quality
maintenance.
</p>
<a href="http://arxiv.org/abs/2011.13560" target="_blank">arXiv:2011.13560</a> [<a href="http://arxiv.org/pdf/2011.13560" target="_blank">pdf</a>]

<h2>Analyzing Unaligned Multimodal Sequence via Graph Convolution and Graph Pooling Fusion. (arXiv:2011.13572v1 [cs.AI])</h2>
<h3>Sijie Mai, Songlong Xing, Jiaxuan He, Ying Zeng, Haifeng Hu</h3>
<p>In this paper, we study the task of multimodal sequence analysis which aims
to draw inferences from visual, language and acoustic sequences. A majority of
existing works generally focus on aligned fusion, mostly at word level, of the
three modalities to accomplish this task, which is impractical in real-world
scenarios. To overcome this issue, we seek to address the task of multimodal
sequence analysis on unaligned modality sequences which is still relatively
underexplored and also more challenging. Recurrent neural network (RNN) and its
variants are widely used in multimodal sequence analysis, but they are
susceptible to the issues of gradient vanishing/explosion and high time
complexity due to its recurrent nature. Therefore, we propose a novel model,
termed Multimodal Graph, to investigate the effectiveness of graph neural
networks (GNN) on modeling multimodal sequential data. The graph-based
structure enables parallel computation in time dimension and can learn longer
temporal dependency in long unaligned sequences. Specifically, our Multimodal
Graph is hierarchically structured to cater to two stages, i.e., intra- and
inter-modal dynamics learning. For the first stage, a graph convolutional
network is employed for each modality to learn intra-modal dynamics. In the
second stage, given that the multimodal sequences are unaligned, the commonly
considered word-level fusion does not pertain. To this end, we devise a graph
pooling fusion network to automatically learn the associations between various
nodes from different modalities. Additionally, we define multiple ways to
construct the adjacency matrix for sequential data. Experimental results
suggest that our graph-based model reaches state-of-the-art performance on two
benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2011.13572" target="_blank">arXiv:2011.13572</a> [<a href="http://arxiv.org/pdf/2011.13572" target="_blank">pdf</a>]

<h2>A survey of benchmarking frameworks for reinforcement learning. (arXiv:2011.13577v1 [cs.LG])</h2>
<h3>Belinda Stapelberg, Katherine M. Malan</h3>
<p>Reinforcement learning has recently experienced increased prominence in the
machine learning community. There are many approaches to solving reinforcement
learning problems with new techniques developed constantly. When solving
problems using reinforcement learning, there are various difficult challenges
to overcome. To ensure progress in the field, benchmarks are important for
testing new algorithms and comparing with other approaches. The reproducibility
of results for fair comparison is therefore vital in ensuring that improvements
are accurately judged. This paper provides an overview of different
contributions to reinforcement learning benchmarking and discusses how they can
assist researchers to address the challenges facing reinforcement learning. The
contributions discussed are the most used and recent in the literature. The
paper discusses the contributions in terms of implementation, tasks and
provided algorithm implementations with benchmarks. The survey aims to bring
attention to the wide range of reinforcement learning benchmarking tasks
available and to encourage research to take place in a standardised manner.
Additionally, this survey acts as an overview for researchers not familiar with
the different tasks that can be used to develop and test new reinforcement
learning algorithms.
</p>
<a href="http://arxiv.org/abs/2011.13577" target="_blank">arXiv:2011.13577</a> [<a href="http://arxiv.org/pdf/2011.13577" target="_blank">pdf</a>]

<h2>A Sheaf and Topology Approach to Generating Local Branch Numbers in Digital Images. (arXiv:2011.13580v1 [cs.CV])</h2>
<h3>Chuan-Shen Hu, Yu-Min Chung</h3>
<p>This paper concerns a theoretical approach that combines topological data
analysis (TDA) and sheaf theory. Topological data analysis, a rising field in
mathematics and computer science, concerns the shape of the data and has been
proven effective in many scientific disciplines. Sheaf theory, a mathematics
subject in algebraic geometry, provides a framework for describing the local
consistency in geometric objects. Persistent homology (PH) is one of the main
driving forces in TDA, and the idea is to track changes of geometric objects at
different scales. The persistence diagram (PD) summarizes the information of PH
in the form of a multi-set. While PD provides useful information about the
underlying objects, it lacks fine relations about the local consistency of
specific pairs of generators in PD, such as the merging relation between two
connected components in the PH. The sheaf structure provides a novel point of
view for describing the merging relation of local objects in PH. It is the goal
of this paper to establish a theoretic framework that utilizes the sheaf theory
to uncover finer information from the PH. We also show that the proposed theory
can be applied to identify the branch numbers of local objects in digital
images.
</p>
<a href="http://arxiv.org/abs/2011.13580" target="_blank">arXiv:2011.13580</a> [<a href="http://arxiv.org/pdf/2011.13580" target="_blank">pdf</a>]

<h2>Improving Layer-wise Adaptive Rate Methods using Trust Ratio Clipping. (arXiv:2011.13584v1 [cs.LG])</h2>
<h3>Jeffrey Fong, Siwei Chen, Kaiqi Chen</h3>
<p>Training neural networks with large batch is of fundamental significance to
deep learning. Large batch training remarkably reduces the amount of training
time but has difficulties in maintaining accuracy. Recent works have put
forward optimization methods such as LARS and LAMB to tackle this issue through
adaptive layer-wise optimization using trust ratios. Though prevailing, such
methods are observed to still suffer from unstable and extreme trust ratios
which degrades performance. In this paper, we propose a new variant of LAMB,
called LAMBC, which employs trust ratio clipping to stabilize its magnitude and
prevent extreme values. We conducted experiments on image classification tasks
such as ImageNet and CIFAR-10 and our empirical results demonstrate promising
improvements across different batch sizes.
</p>
<a href="http://arxiv.org/abs/2011.13584" target="_blank">arXiv:2011.13584</a> [<a href="http://arxiv.org/pdf/2011.13584" target="_blank">pdf</a>]

<h2>Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles. (arXiv:2011.13588v1 [cs.CV])</h2>
<h3>Yafu Tian, Alexander Carballo, Ruifeng Li, Kazuya Takeda</h3>
<p>Rich semantic information extraction plays a vital role on next-generation
intelligent vehicles. Currently there is great amount of research focusing on
fundamental applications such as 6D pose detection, road scene semantic
segmentation, etc. And this provides us a great opportunity to think about how
shall these data be organized and exploited.

In this paper we propose road scene graph,a special scene-graph for
intelligent vehicles. Different to classical data representation, this graph
provides not only object proposals but also their pair-wise relationships. By
organizing them in a topological graph, these data are explainable,
fully-connected, and could be easily processed by GCNs (Graph Convolutional
Networks). Here we apply scene graph on roads using our Road Scene Graph
dataset, including the basic graph prediction model. This work also includes
experimental evaluations using the proposed model.
</p>
<a href="http://arxiv.org/abs/2011.13588" target="_blank">arXiv:2011.13588</a> [<a href="http://arxiv.org/pdf/2011.13588" target="_blank">pdf</a>]

<h2>Multi-objective Neural Architecture Search with Almost No Training. (arXiv:2011.13591v1 [cs.CV])</h2>
<h3>Shengran Hu, Ran Cheng, Cheng He, Zhichao Lu</h3>
<p>In the recent past, neural architecture search (NAS) has attracted increasing
attention from both academia and industries. Despite the steady stream of
impressive empirical results, most existing NAS algorithms are computationally
prohibitive to execute due to the costly iterations of stochastic gradient
descent (SGD) training. In this work, we propose an effective alternative,
dubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of
network architectures. By just training the last linear classification layer,
RWE reduces the computational cost of evaluating an architecture from hours to
seconds. When integrated within an evolutionary multi-objective algorithm, RWE
obtains a set of efficient architectures with state-of-the-art performance on
CIFAR-10 with less than two hours' searching on a single GPU card. Ablation
studies on rank-order correlations and transfer learning experiments to
ImageNet have further validated the effectiveness of RWE.
</p>
<a href="http://arxiv.org/abs/2011.13591" target="_blank">arXiv:2011.13591</a> [<a href="http://arxiv.org/pdf/2011.13591" target="_blank">pdf</a>]

<h2>A Mixed Integer Linear Program For Human And Material Resources Optimization In Emergency Department. (arXiv:2011.13596v1 [cs.RO])</h2>
<h3>Ibtissem Chouba (UTT), Lionel Amodeo (UTT), Farouk Yalaoui (UTT), Taha Arbaoui (UTT), David Laplanche</h3>
<p>The discrepancy between patient demand and the emergency departments (ED)
capacity, that mainly depends on human resources and on beds available for
patients, often lead to ED's overcrowding and to the increase in waiting time.
In this paper, we focus on the optimization of the human (medical and
paramedical staff) and material resources (beds) in the ED of the hospital
center of Troyes, France (CHT). We seek to minimize the total number of waiting
patients from their arrival to their discharge. We propose a mixed integer
linear program solved by a sample average approximation (SAA) approach. The
program has been tested on a set of real data gathered from the ED information
system. Numerical results show that the optimization of human and material
resources leads to a decrease of total number of waiting patients.
</p>
<a href="http://arxiv.org/abs/2011.13596" target="_blank">arXiv:2011.13596</a> [<a href="http://arxiv.org/pdf/2011.13596" target="_blank">pdf</a>]

<h2>Distributed Variational Bayesian Algorithms Over Sensor Networks. (arXiv:2011.13600v1 [stat.ML])</h2>
<h3>Junhao Hua, Chunguang Li</h3>
<p>Distributed inference/estimation in Bayesian framework in the context of
sensor networks has recently received much attention due to its broad
applicability. The variational Bayesian (VB) algorithm is a technique for
approximating intractable integrals arising in Bayesian inference. In this
paper, we propose two novel distributed VB algorithms for general Bayesian
inference problem, which can be applied to a very general class of
conjugate-exponential models. In the first approach, the global natural
parameters at each node are optimized using a stochastic natural gradient that
utilizes the Riemannian geometry of the approximation space, followed by an
information diffusion step for cooperation with the neighbors. In the second
method, a constrained optimization formulation for distributed estimation is
established in natural parameter space and solved by alternating direction
method of multipliers (ADMM). An application of the distributed
inference/estimation of a Bayesian Gaussian mixture model is then presented, to
evaluate the effectiveness of the proposed algorithms. Simulations on both
synthetic and real datasets demonstrate that the proposed algorithms have
excellent performance, which are almost as good as the corresponding
centralized VB algorithm relying on all data available in a fusion center.
</p>
<a href="http://arxiv.org/abs/2011.13600" target="_blank">arXiv:2011.13600</a> [<a href="http://arxiv.org/pdf/2011.13600" target="_blank">pdf</a>]

<h2>PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers. (arXiv:2011.13607v1 [cs.CV])</h2>
<h3>Frank Yu, Mathieu Salzmann, Pascal Fua, Helge Rhodin</h3>
<p>Local processing is an essential feature of CNNs and other neural network
architectures - it is one of the reasons why they work so well on images where
relevant information is, to a large extent, local. However, perspective effects
stemming from the projection in a conventional camera vary for different global
positions in the image. We introduce Perspective Crop Layers (PCLs) - a form of
perspective crop of the region of interest based on the camera geometry - and
show that accounting for the perspective consistently improves the accuracy of
state-of-the-art 3D pose reconstruction methods. PCLs are modular neural
network layers, which, when inserted into existing CNN and MLP architectures,
deterministically remove the location-dependent perspective effects while
leaving end-to-end training and the number of parameters of the underlying
neural network unchanged. We demonstrate that PCL leads to improved 3D human
pose reconstruction accuracy for CNN architectures that use cropping
operations, such as spatial transformer networks (STN), and, somewhat
surprisingly, MLPs used for 2D-to-3D keypoint lifting. Our conclusion is that
it is important to utilize camera calibration information when available, for
classical and deep-learning-based computer vision alike. PCL offers an easy way
to improve the accuracy of existing 3D reconstruction networks by making them
geometry-aware.
</p>
<a href="http://arxiv.org/abs/2011.13607" target="_blank">arXiv:2011.13607</a> [<a href="http://arxiv.org/pdf/2011.13607" target="_blank">pdf</a>]

<h2>Eigenvalue-corrected Natural Gradient Based on a New Approximation. (arXiv:2011.13609v1 [cs.LG])</h2>
<h3>Kai-Xin Gao, Xiao-Lei Liu, Zheng-Hai Huang, Min Wang, Shuangling Wang, Zidong Wang, Dachuan Xu, Fan Yu</h3>
<p>Using second-order optimization methods for training deep neural networks
(DNNs) has attracted many researchers. A recently proposed method,
Eigenvalue-corrected Kronecker Factorization (EKFAC) (George et al., 2018),
proposes an interpretation of viewing natural gradient update as a diagonal
method, and corrects the inaccurate re-scaling factor in the Kronecker-factored
eigenbasis. Gao et al. (2020) considers a new approximation to the natural
gradient, which approximates the Fisher information matrix (FIM) to a constant
multiplied by the Kronecker product of two matrices and keeps the trace equal
before and after the approximation. In this work, we combine the ideas of these
two methods and propose Trace-restricted Eigenvalue-corrected Kronecker
Factorization (TEKFAC). The proposed method not only corrects the inexact
re-scaling factor under the Kronecker-factored eigenbasis, but also considers
the new approximation method and the effective damping technique proposed in
Gao et al. (2020). We also discuss the differences and relationships among the
Kronecker-factored approximations. Empirically, our method outperforms SGD with
momentum, Adam, EKFAC and TKFAC on several DNNs.
</p>
<a href="http://arxiv.org/abs/2011.13609" target="_blank">arXiv:2011.13609</a> [<a href="http://arxiv.org/pdf/2011.13609" target="_blank">pdf</a>]

<h2>Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. (arXiv:2011.13611v1 [cs.CV])</h2>
<h3>Mu Cai, Hong Zhang, Huijuan Huang, Qichuan Geng, Gao Huang</h3>
<p>Image-to-image translation aims at translating a particular style of an image
to another. The synthesized images can be more photo-realistic and
identity-preserving by decomposing the image into content and style in a
disentangled manner. While existing models focus on designing specialized
network architecture to separate the two components, this paper investigates
how to explicitly constrain the content and style statistics of images. We
achieve this goal by transforming the input image into high frequency and low
frequency information, which correspond to the content and style, respectively.
We regulate the frequency distribution from two aspects: a) a spatial level
restriction to locally restrict the frequency distribution of images; b) a
spectral level regulation to enhance the global consistency among images. On
multiple datasets we show that the proposed approach consistently leads to
significant improvements on top of various state-of-the-art image translation
models.
</p>
<a href="http://arxiv.org/abs/2011.13611" target="_blank">arXiv:2011.13611</a> [<a href="http://arxiv.org/pdf/2011.13611" target="_blank">pdf</a>]

<h2>Multi-task MR Imaging with Iterative Teacher Forcing and Re-weighted Deep Learning. (arXiv:2011.13614v1 [cs.CV])</h2>
<h3>Kehan Qi, Yu Gong, Xinfeng Liu, Xin Liu, Hairong Zheng, Shanshan Wang</h3>
<p>Noises, artifacts, and loss of information caused by the magnetic resonance
(MR) reconstruction may compromise the final performance of the downstream
applications. In this paper, we develop a re-weighted multi-task deep learning
method to learn prior knowledge from the existing big dataset and then utilize
them to assist simultaneous MR reconstruction and segmentation from the
under-sampled k-space data. The multi-task deep learning framework is equipped
with two network sub-modules, which are integrated and trained by our designed
iterative teacher forcing scheme (ITFS) under the dynamic re-weighted loss
constraint (DRLC). The ITFS is designed to avoid error accumulation by
injecting the fully-sampled data into the training process. The DRLC is
proposed to dynamically balance the contributions from the reconstruction and
segmentation sub-modules so as to co-prompt the multi-task accuracy. The
proposed method has been evaluated on two open datasets and one in vivo
in-house dataset and compared to six state-of-the-art methods. Results show
that the proposed method possesses encouraging capabilities for simultaneous
and accurate MR reconstruction and segmentation.
</p>
<a href="http://arxiv.org/abs/2011.13614" target="_blank">arXiv:2011.13614</a> [<a href="http://arxiv.org/pdf/2011.13614" target="_blank">pdf</a>]

<h2>Manipulating Medical Image Translation with Manifold Disentanglement. (arXiv:2011.13615v1 [cs.CV])</h2>
<h3>Siyu Liu, Jason A. Dowling, Craig Engstrom, Peter B. Greer, Stuart Crozier, Shekhar S. Chandra</h3>
<p>Medical image translation (e.g. CT to MR) is a challenging task as it
requires I) faithful translation of domain-invariant features (e.g. shape
information of anatomical structures) and II) realistic synthesis of
target-domain features (e.g. tissue appearance in MR). In this work, we propose
Manifold Disentanglement Generative Adversarial Network (MDGAN), a novel image
translation framework that explicitly models these two types of features. It
employs a fully convolutional generator to model domain-invariant features, and
it uses style codes to separately model target-domain features as a manifold.
This design aims to explicitly disentangle domain-invariant features and
domain-specific features while gaining individual control of both. The image
translation process is formulated as a stylisation task, where the input is
"stylised" (translated) into diverse target-domain images based on style codes
sampled from the learnt manifold. We test MDGAN for multi-modal medical image
translation, where we create two domain-specific manifold clusters on the
manifold to translate segmentation maps into pseudo-CT and pseudo-MR images,
respectively. We show that by traversing a path across the MR manifold cluster,
the target output can be manipulated while still retaining the shape
information from the input.
</p>
<a href="http://arxiv.org/abs/2011.13615" target="_blank">arXiv:2011.13615</a> [<a href="http://arxiv.org/pdf/2011.13615" target="_blank">pdf</a>]

<h2>Temporal-Channel Transformer for 3D Lidar-Based Video Object Detection in Autonomous Driving. (arXiv:2011.13628v1 [cs.CV])</h2>
<h3>Zhenxun Yuan, Xiao Song, Lei Bai, Wengang Zhou, Zhe Wang, Wanli Ouyang</h3>
<p>The strong demand of autonomous driving in the industry has lead to strong
interest in 3D object detection and resulted in many excellent 3D object
detection algorithms. However, the vast majority of algorithms only model
single-frame data, ignoring the temporal information of the sequence of data.
In this work, we propose a new transformer, called Temporal-Channel
Transformer, to model the spatial-temporal domain and channel domain
relationships for video object detecting from Lidar data. As a special design
of this transformer, the information encoded in the encoder is different from
that in the decoder, i.e. the encoder encodes temporal-channel information of
multiple frames while the decoder decodes the spatial-channel information for
the current frame in a voxel-wise manner. Specifically, the temporal-channel
encoder of the transformer is designed to encode the information of different
channels and frames by utilizing the correlation among features from different
channels and frames. On the other hand, the spatial decoder of the transformer
will decode the information for each location of the current frame. Before
conducting the object detection with detection head, the gate mechanism is
deployed for re-calibrating the features of current frame, which filters out
the object irrelevant information by repetitively refine the representation of
target frame along with the up-sampling process. Experimental results show that
we achieve the state-of-the-art performance in grid voxel-based 3D object
detection on the nuScenes benchmark.
</p>
<a href="http://arxiv.org/abs/2011.13628" target="_blank">arXiv:2011.13628</a> [<a href="http://arxiv.org/pdf/2011.13628" target="_blank">pdf</a>]

<h2>Randomized Transferable Machine. (arXiv:2011.13629v1 [cs.LG])</h2>
<h3>Pengfei Wei, Tze Yun Leong</h3>
<p>Feature-based transfer is one of the most effective methodologies for
transfer learning. Existing studies usually assume that the learned new feature
representation is truly \emph{domain-invariant}, and thus directly train a
transfer model $\mathcal{M}$ on source domain. In this paper, we consider a
more realistic scenario where the new feature representation is suboptimal and
small divergence still exists across domains. We propose a new learning
strategy with a transfer model called Randomized Transferable Machine (RTM).
More specifically, we work on source data with the new feature representation
learned from existing feature-based transfer methods. The key idea is to
enlarge source training data populations by randomly corrupting source data
using some noises, and then train a transfer model $\widetilde{\mathcal{M}}$
that performs well on all the corrupted source data populations. In principle,
the more corruptions are made, the higher the probability of the target data
can be covered by the constructed source populations, and thus better transfer
performance can be achieved by $\widetilde{\mathcal{M}}$. An ideal case is with
infinite corruptions, which however is infeasible in reality. We develop a
marginalized solution with linear regression model and dropout noise. With a
marginalization trick, we can train an RTM that is equivalently to training
using infinite source noisy populations without truly conducting any
corruption. More importantly, such an RTM has a closed-form solution, which
enables very fast and efficient training. Extensive experiments on various
real-world transfer tasks show that RTM is a promising transfer model.
</p>
<a href="http://arxiv.org/abs/2011.13629" target="_blank">arXiv:2011.13629</a> [<a href="http://arxiv.org/pdf/2011.13629" target="_blank">pdf</a>]

<h2>Deep Reinforcement Learning for Wireless Scheduling with Multiclass Services. (arXiv:2011.13634v1 [cs.LG])</h2>
<h3>Apostolos Avranas (EURECOM), Marios Kountouris (EURECOM), Philippe Ciblat (T&#xe9;l&#xe9;com Paris)</h3>
<p>In this paper, we investigate the problem of scheduling and resource
allocation over a time varying set of clients with heterogeneous demands.In
this context, a service provider has to schedule traffic destined to users with
different classes of requirements and to allocate bandwidth resources over time
as a means to efficiently satisfy service demands within a limited time
horizon. This is a highly intricate problem, in particular in wireless
communication systems, and solutions may involve tools stemming from diverse
fields, including combinatorics and constrained optimization. Although recent
work has successfully proposed solutions based on Deep Reinforcement Learning
(DRL), the challenging setting of heterogeneous user traffic and demands has
not been addressed. We propose a deep deterministic policy gradient algorithm
that combines state-of-the-art techniques, namely Distributional RL and Deep
Sets, to train a model for heterogeneous traffic scheduling. We test on diverse
scenarios with different time dependence dynamics, users' requirements, and
resources available, demonstrating consistent results using both synthetic and
real data. We evaluate the algorithm on a wireless communication setting using
both synthetic and real data and show significant gains in terms of Quality of
Service (QoS) defined by the classes, against state-of-the-art conventional
algorithms from combinatorics, optimization and scheduling metric(e.g.
Knapsack, Integer Linear Programming, Frank-Wolfe, Exponential Rule).
</p>
<a href="http://arxiv.org/abs/2011.13634" target="_blank">arXiv:2011.13634</a> [<a href="http://arxiv.org/pdf/2011.13634" target="_blank">pdf</a>]

<h2>Combination of interval-valued belief structures based on belief entropy. (arXiv:2011.13636v1 [cs.AI])</h2>
<h3>Miao Qin, Yongchuan Tang</h3>
<p>This paper investigates the issues of combination and normalization of
interval-valued belief structures within the framework of Dempster-Shafer
theory of evidence. Existing approaches are reviewed and thoroughly analyzed.
The advantages and drawbacks of previous approach are presented. A new
optimality approach based on uncertainty measure is developed, where the
problem of combining interval-valued belief structures degenerates into
combining basic probability assignments. Numerical examples are provided to
illustrate the rationality of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2011.13636" target="_blank">arXiv:2011.13636</a> [<a href="http://arxiv.org/pdf/2011.13636" target="_blank">pdf</a>]

<h2>Descriptor-Free Multi-View Region Matching for Instance-Wise 3D Reconstruction. (arXiv:2011.13649v1 [cs.CV])</h2>
<h3>Takuma Doi, Fumio Okura, Toshiki Nagahara, Yasuyuki Matsushita, Yasushi Yagi</h3>
<p>This paper proposes a multi-view extension of instance segmentation without
relying on texture or shape descriptor matching. Multi-view instance
segmentation becomes challenging for scenes with repetitive textures and
shapes, e.g., plant leaves, due to the difficulty of multi-view matching using
texture or shape descriptors. To this end, we propose a multi-view region
matching method based on epipolar geometry, which does not rely on any feature
descriptors. We further show that the epipolar region matching can be easily
integrated into instance segmentation and effective for instance-wise 3D
reconstruction. Experiments demonstrate the improved accuracy of multi-view
instance matching and the 3D reconstruction compared to the baseline methods.
</p>
<a href="http://arxiv.org/abs/2011.13649" target="_blank">arXiv:2011.13649</a> [<a href="http://arxiv.org/pdf/2011.13649" target="_blank">pdf</a>]

<h2>Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence. (arXiv:2011.13650v1 [cs.CV])</h2>
<h3>Yu Deng, Jiaolong Yang, Xin Tong</h3>
<p>We propose a novel Deformed Implicit Field (DIF) representation for modeling
3D shapes of a category and generating dense correspondences among shapes. With
DIF, a 3D shape is represented by a template implicit field shared across the
category, together with a 3D deformation field and a correction field dedicated
for each shape instance. Shape correspondences can be easily established using
their deformation fields. Our neural network, dubbed DIF-Net, jointly learns a
shape latent space and these fields for 3D objects belonging to a category
without using any correspondence or part label. The learned DIF-Net can also
provides reliable correspondence uncertainty measurement reflecting shape
structure discrepancy. Experiments show that DIF-Net not only produces
high-fidelity 3D shapes but also builds high-quality dense correspondences
across different shapes. We also demonstrate several applications such as
texture transfer and shape editing, where our method achieves compelling
results that cannot be achieved by previous methods.
</p>
<a href="http://arxiv.org/abs/2011.13650" target="_blank">arXiv:2011.13650</a> [<a href="http://arxiv.org/pdf/2011.13650" target="_blank">pdf</a>]

<h2>Towards real-time object recognition and pose estimation in point clouds. (arXiv:2011.13669v1 [cs.CV])</h2>
<h3>Marlon Marcon, Olga Regina Pereira Bellon, Luciano Silva</h3>
<p>Object recognition and 6DoF pose estimation are quite challenging tasks in
computer vision applications. Despite efficiency in such tasks, standard
methods deliver far from real-time processing rates. This paper presents a
novel pipeline to estimate a fine 6DoF pose of objects, applied to realistic
scenarios in real-time. We split our proposal into three main parts. Firstly, a
Color feature classification leverages the use of pre-trained CNN color
features trained on the ImageNet for object detection. A Feature-based
registration module conducts a coarse pose estimation, and finally, a
Fine-adjustment step performs an ICP-based dense registration. Our proposal
achieves, in the best case, an accuracy performance of almost 83\% on the RGB-D
Scenes dataset. Regarding processing time, the object detection task is done at
a frame processing rate up to 90 FPS, and the pose estimation at almost 14 FPS
in a full execution strategy. We discuss that due to the proposal's modularity,
we could let the full execution occurs only when necessary and perform a
scheduled execution that unlocks real-time processing, even for multitask
situations.
</p>
<a href="http://arxiv.org/abs/2011.13669" target="_blank">arXiv:2011.13669</a> [<a href="http://arxiv.org/pdf/2011.13669" target="_blank">pdf</a>]

<h2>Deinterlacing Network for Early Interlaced Videos. (arXiv:2011.13675v1 [cs.CV])</h2>
<h3>Yang Zhao, Wei Jia, Ronggang Wang, Xiaoping Liu, Xuesong Gao, Weiqiang Chen, Wen Gao</h3>
<p>With the rapid development of image restoration techniques, high-definition
reconstruction of early videos has achieved impressive results. However, there
are few studies about the interlacing artifacts that often appear in early
videos and significantly affect visual perception. Traditional deinterlacing
approaches are mainly focused on early interlacing scanning systems and thus
cannot handle the complex and complicated artifacts in real-world early
interlaced videos. Hence, this paper proposes a specific deinterlacing network
(DIN), which is motivated by the traditional deinterlacing strategy. The
proposed DIN consists of two stages, i.e., a cooperative vertical interpolation
stage for split fields, and a merging stage that is applied to perceive
movements and remove ghost artifacts. Experimental results demonstrate that the
proposed method can effectively remove complex artifacts in early interlaced
videos.
</p>
<a href="http://arxiv.org/abs/2011.13675" target="_blank">arXiv:2011.13675</a> [<a href="http://arxiv.org/pdf/2011.13675" target="_blank">pdf</a>]

<h2>Self-EMD: Self-Supervised Object Detection without ImageNet. (arXiv:2011.13677v1 [cs.CV])</h2>
<h3>Songtao Liu, Zeming Li, Jian Sun</h3>
<p>In this paper, we propose a novel self-supervised representation learning
method, Self-EMD, for object detection. Our method directly trained on
unlabeled non-iconic image dataset like COCO, instead of commonly used
iconic-object image dataset like ImageNet. We keep the convolutional feature
maps as the image embedding to preserve spatial structures and adopt Earth
Mover's Distance (EMD) to compute the similarity between two embeddings. Our
Faster R-CNN (ResNet50-FPN) baseline achieves 39.8% mAP on COCO, which is on
par with the state of the art self-supervised methods pre-trained on ImageNet.
More importantly, it can be further improved to 40.4% mAP with more unlabeled
images, showing its great potential for leveraging more easily obtained
unlabeled data. Code will be made available.
</p>
<a href="http://arxiv.org/abs/2011.13677" target="_blank">arXiv:2011.13677</a> [<a href="http://arxiv.org/pdf/2011.13677" target="_blank">pdf</a>]

<h2>Point and Ask: Incorporating Pointing into Visual Question Answering. (arXiv:2011.13681v1 [cs.CV])</h2>
<h3>Arjun Mani, Will Hinthorn, Nobline Yoo, Olga Russakovsky</h3>
<p>Visual Question Answering (VQA) has become one of the key benchmarks of
visual recognition progress. Multiple VQA extensions have been explored to
better simulate real-world settings: different question formulations, changing
training and test distributions, conversational consistency in dialogues, and
explanation-based answering. In this work, we further expand this space by
considering visual questions that include a spatial point of reference.
Pointing is a nearly universal gesture among humans, and real-world VQA is
likely to involve a gesture towards the target region.

Concretely, we (1) introduce and motivate point-input questions as an
extension of VQA, (2) define three novel classes of questions within this
space, and (3) for each class, introduce both a benchmark dataset and a series
of baseline models to handle its unique challenges. There are two key
distinctions from prior work. First, we explicitly design the benchmarks to
require the point input, i.e., we ensure that the visual question cannot be
answered accurately without the spatial reference. Second, we explicitly
explore the more realistic point spatial input rather than the standard but
unnatural bounding box input. Through our exploration we uncover and address
several visual recognition challenges, including the ability to infer human
intent, reason both locally and globally about the image, and effectively
combine visual, language and spatial inputs. Code is available at:
https://github.com/princetonvisualai/pointingqa .
</p>
<a href="http://arxiv.org/abs/2011.13681" target="_blank">arXiv:2011.13681</a> [<a href="http://arxiv.org/pdf/2011.13681" target="_blank">pdf</a>]

<h2>MEBOW: Monocular Estimation of Body Orientation In the Wild. (arXiv:2011.13688v1 [cs.CV])</h2>
<h3>Chenyan Wu, Yukun Chen, Jiajia Luo, Che-Chun Su, Anuja Dawane, Bikramjot Hanzra, Zhuo Deng, Bilan Liu, James Wang, Cheng-Hao Kuo</h3>
<p>Body orientation estimation provides crucial visual cues in many
applications, including robotics and autonomous driving. It is particularly
desirable when 3-D pose estimation is difficult to infer due to poor image
resolution, occlusion or indistinguishable body parts. We present COCO-MEBOW
(Monocular Estimation of Body Orientation in the Wild), a new large-scale
dataset for orientation estimation from a single in-the-wild image. The
body-orientation labels for around 130K human bodies within 55K images from the
COCO dataset have been collected using an efficient and high-precision
annotation pipeline. We also validated the benefits of the dataset. First, we
show that our dataset can substantially improve the performance and the
robustness of a human body orientation estimation model, the development of
which was previously limited by the scale and diversity of the available
training data. Additionally, we present a novel triple-source solution for 3-D
human pose estimation, where 3-D pose labels, 2-D pose labels, and our
body-orientation labels are all used in joint training. Our model significantly
outperforms state-of-the-art dual-source solutions for monocular 3-D human pose
estimation, where training only uses 3-D pose labels and 2-D pose labels. This
substantiates an important advantage of MEBOW for 3-D human pose estimation,
which is particularly appealing because the per-instance labeling cost for body
orientations is far less than that for 3-D poses. The work demonstrates high
potential of MEBOW in addressing real-world challenges involving understanding
human behaviors. Further information of this work is available at
https://chenyanwu.github.io/MEBOW/.
</p>
<a href="http://arxiv.org/abs/2011.13688" target="_blank">arXiv:2011.13688</a> [<a href="http://arxiv.org/pdf/2011.13688" target="_blank">pdf</a>]

<h2>Automated acquisition of structured, semantic models of manipulation activities from human VR demonstration. (arXiv:2011.13689v1 [cs.AI])</h2>
<h3>Andrei Haidu, Michael Beetz</h3>
<p>In this paper we present a system capable of collecting and annotating, human
performed, robot understandable, everyday activities from virtual environments.
The human movements are mapped in the simulated world using off-the-shelf
virtual reality devices with full body, and eye tracking capabilities. All the
interactions in the virtual world are physically simulated, thus movements and
their effects are closely relatable to the real world. During the activity
execution, a subsymbolic data logger is recording the environment and the human
gaze on a per-frame basis, enabling offline scene reproduction and replays.
Coupled with the physics engine, online monitors (symbolic data loggers) are
parsing (using various grammars) and recording events, actions, and their
effects in the simulated world.
</p>
<a href="http://arxiv.org/abs/2011.13689" target="_blank">arXiv:2011.13689</a> [<a href="http://arxiv.org/pdf/2011.13689" target="_blank">pdf</a>]

<h2>Robust and Natural Physical Adversarial Examples for Object Detectors. (arXiv:2011.13692v1 [cs.CV])</h2>
<h3>Mingfu Xue, Chengxiang Yuan, Can He, Jian Wang, Weiqiang Liu</h3>
<p>Recently, many studies show that deep neural networks (DNNs) are susceptible
to adversarial examples. However, in order to convince that adversarial
examples are real threats in real physical world, it is necessary to study and
evaluate the adversarial examples in real-world scenarios. In this paper, we
propose a robust and natural physical adversarial example attack method
targeting object detectors under real-world conditions, which is more
challenging than targeting image classifiers. The generated adversarial
examples are robust to various physical constraints and visually look similar
to the original images, thus these adversarial examples are natural to humans
and will not cause any suspicions. First, to ensure the robustness of the
adversarial examples in real-world conditions, the proposed method exploits
different image transformation functions (Distance, Angle, Illumination,
Printing and Photographing), to simulate various physical changes during the
iterative optimization of the adversarial examples generation. Second, to
construct natural adversarial examples, the proposed method uses an adaptive
mask to constrain the area and intensities of added perturbations, and utilizes
the real-world perturbation score (RPS) to make the perturbations be similar to
those real noises in physical world. Compared with existing studies, our
generated adversarial examples can achieve a high success rate with less
conspicuous perturbations. Experimental results demonstrate that, the generated
adversarial examples are robust under various indoor and outdoor physical
conditions. Finally, the proposed physical adversarial attack method is
universal and can work in black-box scenarios. The generated adversarial
examples generalize well between different models.
</p>
<a href="http://arxiv.org/abs/2011.13692" target="_blank">arXiv:2011.13692</a> [<a href="http://arxiv.org/pdf/2011.13692" target="_blank">pdf</a>]

<h2>Lightweight U-Net for High-Resolution Breast Imaging. (arXiv:2011.13698v1 [cs.CV])</h2>
<h3>Mickael Tardy, Diana Mateus</h3>
<p>We study the fully convolutional neural networks in the context of malignancy
detection for breast cancer screening. We work on a supervised segmentation
task looking for an acceptable compromise between the precision of the network
and the computational complexity.
</p>
<a href="http://arxiv.org/abs/2011.13698" target="_blank">arXiv:2011.13698</a> [<a href="http://arxiv.org/pdf/2011.13698" target="_blank">pdf</a>]

<h2>Direct Evolutionary Optimization of Variational Autoencoders With Binary Latents. (arXiv:2011.13704v1 [stat.ML])</h2>
<h3>Enrico Guiraud, Jakob Drefs, J&#xf6;rg L&#xfc;cke</h3>
<p>Discrete latent variables are considered important for real world data, which
has motivated research on Variational Autoencoders (VAEs) with discrete
latents. However, standard VAE-training is not possible in this case, which has
motivated different strategies to manipulate discrete distributions in order to
train discrete VAEs similarly to conventional ones. Here we ask if it is also
possible to keep the discrete nature of the latents fully intact by applying a
direct discrete optimization for the encoding model. The approach is
consequently strongly diverting from standard VAE-training by sidestepping
sampling approximation, reparameterization trick and amortization. Discrete
optimization is realized in a variational setting using truncated posteriors in
conjunction with evolutionary algorithms. For VAEs with binary latents, we (A)
show how such a discrete variational method ties into gradient ascent for
network weights, and (B) how the decoder is used to select latent states for
training. Conventional amortized training is more efficient and applicable to
large neural networks. However, using smaller networks, we here find direct
discrete optimization to be efficiently scalable to hundreds of latents. More
importantly, we find the effectiveness of direct optimization to be highly
competitive in `zero-shot' learning. In contrast to large supervised networks,
the here investigated VAEs can, e.g., denoise a single image without previous
training on clean data and/or training on large image datasets. More generally,
the studied approach shows that training of VAEs is indeed possible without
sampling-based approximation and reparameterization, which may be interesting
for the analysis of VAE-training in general. For `zero-shot' settings a direct
optimization, furthermore, makes VAEs competitive where they have previously
been outperformed by non-generative approaches.
</p>
<a href="http://arxiv.org/abs/2011.13704" target="_blank">arXiv:2011.13704</a> [<a href="http://arxiv.org/pdf/2011.13704" target="_blank">pdf</a>]

<h2>3D Invisible Cloak. (arXiv:2011.13705v1 [cs.CV])</h2>
<h3>Mingfu Xue, Can He, Zhiyu Wu, Jian Wang, Zhe Liu, Weiqiang Liu</h3>
<p>In this paper, we propose a novel physical stealth attack against the person
detectors in real world. The proposed method generates an adversarial patch,
and prints it on real clothes to make a three dimensional (3D) invisible cloak.
Anyone wearing the cloak can evade the detection of person detectors and
achieve stealth. We consider the impacts of those 3D physical constraints
(i.e., radian, wrinkle, occlusion, angle, etc.) on person stealth attacks, and
propose 3D transformations to generate 3D invisible cloak. We launch the person
stealth attacks in 3D physical space instead of 2D plane by printing the
adversarial patches on real clothes under challenging and complex 3D physical
scenarios. The conventional and 3D transformations are performed on the patch
during its optimization process. Further, we study how to generate the optimal
3D invisible cloak. Specifically, we explore how to choose input images with
specific shapes and colors to generate the optimal 3D invisible cloak. Besides,
after successfully making the object detector misjudge the person as other
objects, we explore how to make a person completely disappeared, i.e., the
person will not be detected as any objects. Finally, we present a systematic
evaluation framework to methodically evaluate the performance of the proposed
attack in digital domain and physical world. Experimental results in various
indoor and outdoor physical scenarios show that, the proposed person stealth
attack method is robust and effective even under those complex and challenging
physical conditions, such as the cloak is wrinkled, obscured, curved, and from
different angles. The attack success rate in digital domain (Inria data set) is
86.56%, while the static and dynamic stealth attack performance in physical
world is 100% and 77%, respectively, which are significantly better than
existing works.
</p>
<a href="http://arxiv.org/abs/2011.13705" target="_blank">arXiv:2011.13705</a> [<a href="http://arxiv.org/pdf/2011.13705" target="_blank">pdf</a>]

<h2>ROS Based Visual Programming Tool for Mobile Robot Education and Applications. (arXiv:2011.13706v1 [cs.RO])</h2>
<h3>Mustafa Karaca, Ugur Yayan</h3>
<p>Visual programming languages (VPLs) provide coding without typing texts. VPL
makes coding easy to programmers with automatically adding usually used some
code structure. Beginners in coding have generally two main challenges;
transforming ideas into logical expressions and syntax errors. Syntax errors
are impossible with VPLs because of there is no forgotten parentheses and
semicolons. VPLs provide to focus on algorithm for programmers. VPL is a new
trend for educational robotic environments. In this study, Robot Operating
System (ROS) compatible web based visual programming system has been developed
for evarobot. ROS provides libraries and tools to help software developers
create robot applications. It provides hardware abstraction, device drivers,
libraries, visualizers, message-passing, package management, and more. Blockly
has been used as VPL for the study and to generate / use blocks (commucation,
sensing etc.). Some applications were generated like teleoperation, SLAM and
wander etc. In this system, communication between server and client is
supported by rosbridge package. Web page connected to ROS which runs on server
using roslibjs library. Rosbridge provides a JSON API to ROS functionality for
non-ROS programs.
</p>
<a href="http://arxiv.org/abs/2011.13706" target="_blank">arXiv:2011.13706</a> [<a href="http://arxiv.org/pdf/2011.13706" target="_blank">pdf</a>]

<h2>Detection of Malaria Vector Breeding Habitats using Topographic Models. (arXiv:2011.13714v1 [cs.LG])</h2>
<h3>Aishwarya Jadhav</h3>
<p>Treatment of stagnant water bodies that act as a breeding site for malarial
vectors is a fundamental step in most malaria elimination campaigns. However,
identification of such water bodies over large areas is expensive,
labour-intensive and time-consuming and hence, challenging in countries with
limited resources. Practical models that can efficiently locate water bodies
can target the limited resources by greatly reducing the area that needs to be
scanned by the field workers. To this end, we propose a practical topographic
model based on easily available, global, high-resolution DEM data to predict
locations of potential vector-breeding water sites. We surveyed the Obuasi
region of Ghana to assess the impact of various topographic features on
different types of water bodies and uncover the features that significantly
influence the formation of aquatic habitats. We further evaluate the
effectiveness of multiple models. Our best model significantly outperforms
earlier attempts that employ topographic variables for detection of small water
sites, even the ones that utilize additional satellite imagery data and
demonstrates robustness across different settings.
</p>
<a href="http://arxiv.org/abs/2011.13714" target="_blank">arXiv:2011.13714</a> [<a href="http://arxiv.org/pdf/2011.13714" target="_blank">pdf</a>]

<h2>A Study on the Uncertainty of Convolutional Layers in Deep Neural Networks. (arXiv:2011.13719v1 [cs.LG])</h2>
<h3>Haojing Shen, Sihong Chen, Ran Wang</h3>
<p>This paper shows a Min-Max property existing in the connection weights of the
convolutional layers in a neural network structure, i.e., the LeNet.
Specifically, the Min-Max property means that, during the back
propagation-based training for LeNet, the weights of the convolutional layers
will become far away from their centers of intervals, i.e., decreasing to their
minimum or increasing to their maximum. From the perspective of uncertainty, we
demonstrate that the Min-Max property corresponds to minimizing the fuzziness
of the model parameters through a simplified formulation of convolution. It is
experimentally confirmed that the model with the Min-Max property has a
stronger adversarial robustness, thus this property can be incorporated into
the design of loss function. This paper points out a changing tendency of
uncertainty in the convolutional layers of LeNet structure, and gives some
insights to the interpretability of convolution.
</p>
<a href="http://arxiv.org/abs/2011.13719" target="_blank">arXiv:2011.13719</a> [<a href="http://arxiv.org/pdf/2011.13719" target="_blank">pdf</a>]

<h2>Lower Bounds for Approximate Knowledge Compilation. (arXiv:2011.13721v1 [cs.AI])</h2>
<h3>Alexis de Colnet, Stefan Mengel</h3>
<p>Knowledge compilation studies the trade-off between succinctness and
efficiency of different representation languages. For many languages, there are
known strong lower bounds on the representation size, but recent work shows
that, for some languages, one can bypass these bounds using approximate
compilation. The idea is to compile an approximation of the knowledge for which
the number of errors can be controlled. We focus on circuits in deterministic
decomposable negation normal form (d-DNNF), a compilation language suitable in
contexts such as probabilistic reasoning, as it supports efficient model
counting and probabilistic inference. Moreover, there are known size lower
bounds for d-DNNF which by relaxing to approximation one might be able to
avoid. In this paper we formalize two notions of approximation: weak
approximation which has been studied before in the decision diagram literature
and strong approximation which has been used in recent algorithmic results. We
then show lower bounds for approximation by d-DNNF, complementing the positive
results from the literature.
</p>
<a href="http://arxiv.org/abs/2011.13721" target="_blank">arXiv:2011.13721</a> [<a href="http://arxiv.org/pdf/2011.13721" target="_blank">pdf</a>]

<h2>A study of traits that affect learnability in GANs. (arXiv:2011.13728v1 [cs.LG])</h2>
<h3>Niladri Shekhar Dutt, Sunil Patel</h3>
<p>Generative Adversarial Networks GANs are algorithmic architectures that use
two neural networks, pitting one against the opposite so as to come up with
new, synthetic instances of data that can pass for real data. Training a GAN is
a challenging problem which requires us to apply advanced techniques like
hyperparameter tuning, architecture engineering etc. Many different losses,
regularization and normalization schemes, network architectures have been
proposed to solve this challenging problem for different types of datasets. It
becomes necessary to understand the experimental observations and deduce a
simple theory for it. In this paper, we perform empirical experiments using
parameterized synthetic datasets to probe what traits affect learnability.
</p>
<a href="http://arxiv.org/abs/2011.13728" target="_blank">arXiv:2011.13728</a> [<a href="http://arxiv.org/pdf/2011.13728" target="_blank">pdf</a>]

<h2>TStarBot-X: An Open-Sourced and Comprehensive Study for Efficient League Training in StarCraft II Full Game. (arXiv:2011.13729v1 [cs.AI])</h2>
<h3>Lei Han, Jiechao Xiong, Peng Sun, Xinghai Sun, Meng Fang, Qingwei Guo, Qiaobo Chen, Tengfei Shi, Hongsheng Yu, Zhengyou Zhang</h3>
<p>StarCraft, one of the most difficult esport games with long-standing history
of professional tournaments, has attracted generations of players and fans, and
also, intense attentions in artificial intelligence research. Recently,
Google's DeepMind announced AlphaStar, a grandmaster level AI in StarCraft II.
In this paper, we introduce a new AI agent, named TStarBot-X, that is trained
under limited computation resources and can play competitively with expert
human players. TStarBot-X takes advantage of important techniques introduced in
AlphaStar, and also benefits from substantial innovations including new league
training methods, novel multi-agent roles, rule-guided policy search,
lightweight neural network architecture, and importance sampling in imitation
learning, etc. We show that with limited computation resources, a faithful
reimplementation of AlphaStar can not succeed and the proposed techniques are
necessary to ensure TStarBot-X's competitive performance. We reveal all
technical details that are complementary to those mentioned in AlphaStar,
showing the most sensitive parts in league training, reinforcement learning and
imitation learning that affect the performance of the agents. Most importantly,
this is an open-sourced study that all codes and resources (including the
trained model parameters) are publicly accessible via
https://github.com/tencent-ailab/tleague_projpage We expect this study could be
beneficial for both academic and industrial future research in solving complex
problems like StarCraft, and also, might provide a sparring partner for all
StarCraft II players and other AI agents.
</p>
<a href="http://arxiv.org/abs/2011.13729" target="_blank">arXiv:2011.13729</a> [<a href="http://arxiv.org/pdf/2011.13729" target="_blank">pdf</a>]

<h2>Rethinking Generalization in American Sign Language Prediction for Edge Devices with Extremely Low Memory Footprint. (arXiv:2011.13741v1 [cs.LG])</h2>
<h3>Aditya Jyoti Paul, Puranjay Mohan, Stuti Sehgal</h3>
<p>Due to the boom in technical compute in the last few years, the world has
seen massive advances in artificially intelligent systems solving diverse
real-world problems. But a major roadblock in the ubiquitous acceptance of
these models is their enormous computational complexity and memory footprint.
Hence efficient architectures and training techniques are required for
deployment on extremely low resource inference endpoints. This paper proposes
an architecture for detection of alphabets in American Sign Language on an ARM
Cortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging
parameter quantization is a common technique that might cause varying drops in
test accuracy. This paper proposes using interpolation as augmentation amongst
other techniques as an efficient method of reducing this drop, which also helps
the model generalize well to previously unseen noisy data. The proposed model
is about 185 KB post-quantization and inference speed is 20 frames per second.
</p>
<a href="http://arxiv.org/abs/2011.13741" target="_blank">arXiv:2011.13741</a> [<a href="http://arxiv.org/pdf/2011.13741" target="_blank">pdf</a>]

<h2>Reinforcement Learning-based Joint Path and Energy Optimization of Cellular-Connected Unmanned Aerial Vehicles. (arXiv:2011.13744v1 [cs.LG])</h2>
<h3>Arash Hooshmand</h3>
<p>Unmanned Aerial Vehicles (UAVs) have attracted considerable research interest
recently. Especially when it comes to the realm of Internet of Things, the UAVs
with Internet connectivity are one of the main demands. Furthermore, the energy
constraint i.e. battery limit is a bottle-neck of the UAVs that can limit their
applications. We try to address and solve the energy problem. Therefore, a path
planning method for a cellular-connected UAV is proposed that will enable the
UAV to plan its path in an area much larger than its battery range by getting
recharged in certain positions equipped with power stations (PSs). In addition
to the energy constraint, there are also no-fly zones; for example, due to Air
to Air (A2A) and Air to Ground (A2G) interference or for lack of necessary
connectivity that impose extra constraints in the trajectory optimization of
the UAV. No-fly zones determine the infeasible areas that should be avoided. We
have used a reinforcement learning (RL) hierarchically to extend typical
short-range path planners to consider battery recharge and solve the problem of
UAVs in long missions. The problem is simulated for the UAV that flies over a
large area, and Q-learning algorithm could enable the UAV to find the optimal
path and recharge policy.
</p>
<a href="http://arxiv.org/abs/2011.13744" target="_blank">arXiv:2011.13744</a> [<a href="http://arxiv.org/pdf/2011.13744" target="_blank">pdf</a>]

<h2>Robust Autonomous Landing of UAV in Non-Cooperative Environments based on Dynamic Time Camera-LiDAR Fusion. (arXiv:2011.13761v1 [cs.RO])</h2>
<h3>Lyujie Chen, Xiaming Yuan, Yao Xiao, Yiding Zhang, Jihong Zhu</h3>
<p>Selecting safe landing sites in non-cooperative environments is a key step
towards the full autonomy of UAVs. However, the existing methods have the
common problems of poor generalization ability and robustness. Their
performance in unknown environments is significantly degraded and the error
cannot be self-detected and corrected. In this paper, we construct a UAV system
equipped with low-cost LiDAR and binocular cameras to realize autonomous
landing in non-cooperative environments by detecting the flat and safe ground
area. Taking advantage of the non-repetitive scanning and high FOV coverage
characteristics of LiDAR, we come up with a dynamic time depth completion
algorithm. In conjunction with the proposed self-evaluation method of the depth
map, our model can dynamically select the LiDAR accumulation time at the
inference phase to ensure an accurate prediction result. Based on the depth
map, the high-level terrain information such as slope, roughness, and the size
of the safe area are derived. We have conducted extensive autonomous landing
experiments in a variety of familiar or completely unknown environments,
verifying that our model can adaptively balance the accuracy and speed, and the
UAV can robustly select a safe landing site.
</p>
<a href="http://arxiv.org/abs/2011.13761" target="_blank">arXiv:2011.13761</a> [<a href="http://arxiv.org/pdf/2011.13761" target="_blank">pdf</a>]

<h2>Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank. (arXiv:2011.13772v1 [cs.LG])</h2>
<h3>Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, Holger Rauhut</h3>
<p>We provide an explicit analysis of the dynamics of vanilla gradient descent
for deep matrix factorization in a setting where the minimizer of the loss
function is unique. We show that the recovery rate of ground-truth eigenvectors
is proportional to the magnitude of the corresponding eigenvalues and that the
differences among the rates are amplified as the depth of the factorization
increases. For exactly characterized time intervals, the effective rank of
gradient descent iterates is provably close to the effective rank of a low-rank
projection of the ground-truth matrix, such that early stopping of gradient
descent produces regularized solutions that may be used for denoising, for
instance. In particular, apart from few initial steps of the iterations, the
effective rank of our matrix is monotonically increasing, suggesting that
"matrix factorization implicitly enforces gradient descent to take a route in
which the effective rank is monotone". Since empirical observations in more
general scenarios such as matrix sensing show a similar phenomenon, we believe
that our theoretical results shed some light on the still mysterious "implicit
bias" of gradient descent in deep learning.
</p>
<a href="http://arxiv.org/abs/2011.13772" target="_blank">arXiv:2011.13772</a> [<a href="http://arxiv.org/pdf/2011.13772" target="_blank">pdf</a>]

<h2>Image Generators with Conditionally-Independent Pixel Synthesis. (arXiv:2011.13775v1 [cs.CV])</h2>
<h3>Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, Denis Korzhenkov</h3>
<p>Existing image generator networks rely heavily on spatial convolutions and,
optionally, self-attention blocks in order to gradually synthesize images in a
coarse-to-fine manner. Here, we present a new architecture for image
generators, where the color value at each pixel is computed independently given
the value of a random latent vector and the coordinate of that pixel. No
spatial convolutions or similar operations that propagate information across
pixels are involved during the synthesis. We analyze the modeling capabilities
of such generators when trained in an adversarial fashion, and observe the new
generators to achieve similar generation quality to state-of-the-art
convolutional generators. We also investigate several interesting properties
unique to the new architecture.
</p>
<a href="http://arxiv.org/abs/2011.13775" target="_blank">arXiv:2011.13775</a> [<a href="http://arxiv.org/pdf/2011.13775" target="_blank">pdf</a>]

<h2>Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification. (arXiv:2011.13776v1 [cs.CV])</h2>
<h3>Hao Chen, Benoit Lagadec, Francois Bremond</h3>
<p>The objective of unsupervised person re-identification (Re-ID) is to learn
discriminative features without labor-intensive identity annotations.
State-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled
images in the target domain and learn from these noisy pseudo labels. Recently
introduced Mean Teacher Model is a promising way to mitigate the label noise.
However, during the training, self-ensembled teacher-student networks quickly
converge to a consensus which leads to a local minimum. We explore the
possibility of using an asymmetric structure inside neural network to address
this problem. First, asymmetric branches are proposed to extract features in
different manners, which enhances the feature diversity in appearance
signatures. Then, our proposed cross-branch supervision allows one branch to
get supervision from the other branch, which transfers distinct knowledge and
enhances the weight diversity between teacher and student networks. Extensive
experiments show that our proposed method can significantly surpass the
performance of previous work on both unsupervised domain adaptation and fully
unsupervised Re-ID tasks.
</p>
<a href="http://arxiv.org/abs/2011.13776" target="_blank">arXiv:2011.13776</a> [<a href="http://arxiv.org/pdf/2011.13776" target="_blank">pdf</a>]

<h2>Context-Conditioning as Cognitive Control: Guiding Meta-learning with Task Information. (arXiv:2011.13782v1 [cs.AI])</h2>
<h3>Rachit Dubey, Erin Grant, Michael Luo, Karthik Narasimhan, Thomas Griffiths</h3>
<p>Cognitive control, the ability of a system to adapt to the demands of a task,
is an integral part of cognition. A widely accepted fact about cognitive
control is that it is context-sensitive: Adults and children alike infer
information about a task's demands from contextual cues and use these
inferences to learn from ambiguous cues. However, the precise way in which
people use contextual cues to guide adaptation to a new task remains poorly
understood. This work connects the context-sensitive nature of cognitive
control to a method for meta-learning with context-conditioned adaptation. We
begin by identifying an essential difference between human learning and current
approaches to meta-learning: In contrast to humans, existing meta-learning
algorithms do not make use of task-specific contextual cues but instead rely
exclusively on online feedback in the form of task-specific labels or rewards.
To remedy this, we introduce a framework for using contextual information about
a task to guide the initialization of task-specific models before adaptation to
online feedback. We show how context-conditioned meta-learning can capture
human behavior in a cognitive task and how it can be scaled to improve the
speed of learning in various settings, including few-shot classification and
low-sample reinforcement learning. Our work demonstrates that guiding
meta-learning with task information can capture complex, human-like behavior,
thereby deepening our understanding of cognitive control.
</p>
<a href="http://arxiv.org/abs/2011.13782" target="_blank">arXiv:2011.13782</a> [<a href="http://arxiv.org/pdf/2011.13782" target="_blank">pdf</a>]

<h2>Spherical Interpolated Convolutional Network with Distance-Feature Density for 3D Semantic Segmentation of Point Clouds. (arXiv:2011.13784v1 [cs.CV])</h2>
<h3>Guangming Wang, Yehui Yang, Huixin Zhang, Zhe Liu, Hesheng Wang</h3>
<p>The semantic segmentation of point clouds is an important part of the
environment perception for robots. However, it is difficult to directly adopt
the traditional 3D convolution kernel to extract features from raw 3D point
clouds because of the unstructured property of point clouds. In this paper, a
spherical interpolated convolution operator is proposed to replace the
traditional grid-shaped 3D convolution operator. This newly proposed feature
extraction operator improves the accuracy of the network and reduces the
parameters of the network. In addition, this paper analyzes the defect of point
cloud interpolation methods based on the distance as the interpolation weight
and proposes the self-learned distance-feature density by combining the
distance and the feature correlation. The proposed method makes the feature
extraction of spherical interpolated convolution network more rational and
effective. The effectiveness of the proposed network is demonstrated on the 3D
semantic segmentation task of point clouds. Experiments show that the proposed
method achieves good performance on the ScanNet dataset and Paris-Lille-3D
dataset.
</p>
<a href="http://arxiv.org/abs/2011.13784" target="_blank">arXiv:2011.13784</a> [<a href="http://arxiv.org/pdf/2011.13784" target="_blank">pdf</a>]

<h2>Navigating the GAN Parameter Space for Semantic Image Editing. (arXiv:2011.13786v1 [cs.LG])</h2>
<h3>Anton Cherepkov, Andrey Voynov, Artem Babenko</h3>
<p>Generative Adversarial Networks (GANs) are currently an indispensable tool
for visual editing, being a standard component of image-to-image translation
and image restoration pipelines. Furthermore, GANs are especially useful for
controllable generation since their latent spaces contain a wide range of
interpretable directions, well suited for semantic editing operations. By
gradually changing latent codes along these directions, one can produce
impressive visual effects, unattainable without GANs.

In this paper, we significantly expand the range of visual effects achievable
with the state-of-the-art models, like StyleGAN2. In contrast to existing
works, which mostly operate by latent codes, we discover interpretable
directions in the space of the generator parameters. By several simple methods,
we explore this space and demonstrate that it also contains a plethora of
interpretable directions, which are an excellent source of non-trivial semantic
manipulations. The discovered manipulations cannot be achieved by transforming
the latent codes and can be used to edit both synthetic and real images. We
release our code and models and hope they will serve as a handy tool for
further efforts on GAN-based image editing.
</p>
<a href="http://arxiv.org/abs/2011.13786" target="_blank">arXiv:2011.13786</a> [<a href="http://arxiv.org/pdf/2011.13786" target="_blank">pdf</a>]

<h2>CASTELO: Clustered Atom Subtypes aidEd Lead Optimization -- a combined machine learning and molecular modeling method. (arXiv:2011.13788v1 [cs.LG])</h2>
<h3>Leili Zhang, Giacomo Domeniconi, Chih-Chieh Yang, Seung-gu Kang, Ruhong Zhou, Guojing Cong</h3>
<p>Drug discovery is a multi-stage process that comprises two costly major
steps: pre-clinical research and clinical trials. Among its stages, lead
optimization easily consumes more than half of the pre-clinical budget. We
propose a combined machine learning and molecular modeling approach that
automates lead optimization workflow \textit{in silico}. The initial data
collection is achieved with physics-based molecular dynamics (MD) simulation.
Contact matrices are calculated as the preliminary features extracted from the
simulations. To take advantage of the temporal information from the
simulations, we enhanced contact matrices data with temporal dynamism
representation, which are then modeled with unsupervised convolutional
variational autoencoder (CVAE). Finally, conventional clustering method and
CVAE-based clustering method are compared with metrics to rank the submolecular
structures and propose potential candidates for lead optimization. With no need
for extensive structure-activity relationship database, our method provides new
hints for drug modification hotspots which can be used to improve drug
efficacy. Our workflow can potentially reduce the lead optimization turnaround
time from months/years to days compared with the conventional labor-intensive
process and thus can potentially become a valuable tool for medical
researchers.
</p>
<a href="http://arxiv.org/abs/2011.13788" target="_blank">arXiv:2011.13788</a> [<a href="http://arxiv.org/pdf/2011.13788" target="_blank">pdf</a>]

<h2>A Hybrid Biped Stabilizer System Based on Analytical Control and Learning of Symmetrical Residual Physics. (arXiv:2011.13798v1 [cs.RO])</h2>
<h3>Mohammadreza Kasaei, Miguel Abreu, Nuno Lau, Artur Pereira, Luis Paulo Reis</h3>
<p>Although humanoid robots are made to resemble humans, their stability is not
yet comparable to ours. When facing external disturbances, humans efficiently
and unconsciously combine a set of strategies to regain stability. This work
deals with the problem of developing a robust hybrid stabilizer system for
biped robots. The Linear Inverted Pendulum (LIP) and Divergent Component of
Motion (DCM) concepts are used to formulate the biped locomotion and
stabilization as an analytical control framework. On top of that, a neural
network with symmetric partial data augmentation learns residuals to adjust the
joint's position, and thus improving the robot's stability when facing external
perturbations. The performance of the proposed framework was evaluated across a
set of challenging simulation scenarios. The results show a considerable
improvement over the baseline in recovering from large external forces.
Moreover, the produced behaviors are human-like and robust to considerably
noisy environments.
</p>
<a href="http://arxiv.org/abs/2011.13798" target="_blank">arXiv:2011.13798</a> [<a href="http://arxiv.org/pdf/2011.13798" target="_blank">pdf</a>]

<h2>Leveraging Regular Fundus Images for Training UWF Fundus Diagnosis Models via Adversarial Learning and Pseudo-Labeling. (arXiv:2011.13816v1 [cs.CV])</h2>
<h3>Lie Ju, Xin Wang, Xin Zhao, Paul Bonnington, Tom Drummond, Zongyuan Ge</h3>
<p>Recently, ultra-widefield (UWF) 200-degree fundus imaging by Optos cameras
has gradually been introduced because of its broader insights for detecting
more information on the fundus than regular 30-degree - 60-degree fundus
cameras. Compared with UWF fundus images, regular fundus images contain a large
amount of high-quality and well-annotated data. Due to the domain gap, models
trained by regular fundus images to recognize UWF fundus images perform poorly.
Hence, given that annotating medical data is labor intensive and time
consuming, in this paper, we explore how to leverage regular fundus images to
improve the limited UWF fundus data and annotations for more efficient
training. We propose the use of a modified cycle generative adversarial network
(CycleGAN) model to bridge the gap between regular and UWF fundus and generate
additional UWF fundus images for training. A consistency regularization term is
proposed in the loss of the GAN to improve and regulate the quality of the
generated data. Our method does not require that images from the two domains be
paired or even that the semantic labels be the same, which provides great
convenience for data collection. Furthermore, we show that our method is robust
to noise and errors introduced by the generated unlabeled data with the
pseudo-labeling technique. We evaluated the effectiveness of our methods on
several common fundus diseases and tasks, such as diabetic retinopathy (DR)
classification, lesion detection and tessellated fundus segmentation. The
experimental results demonstrate that our proposed method simultaneously
achieves superior generalizability of the learned representations and
performance improvements in multiple tasks.
</p>
<a href="http://arxiv.org/abs/2011.13816" target="_blank">arXiv:2011.13816</a> [<a href="http://arxiv.org/pdf/2011.13816" target="_blank">pdf</a>]

<h2>Generalized Pose-and-Scale Estimation using 4-Point Congruence Constraints. (arXiv:2011.13817v1 [cs.CV])</h2>
<h3>Victor Fragoso, Sudipta Sinha</h3>
<p>We present gP4Pc, a new method for computing the absolute pose of a
generalized camera with unknown internal scale from four corresponding 3D
point-and-ray pairs. Unlike most pose-and-scale methods, gP4Pc is based on
constraints arising from the congruence of shapes defined by two sets of four
points related by an unknown similarity transformation. By choosing a novel
parametrization for the problem, we derive a system of four quadratic equations
in four scalar variables. The variables represent the distances of 3D points
along the rays from the camera centers. After solving this system via Groebner
basis-based automatic polynomial solvers, we compute the similarity
transformation using an efficient 3D point-point alignment method. We also
propose a specialized variant of our solver for the case of coplanar points,
which is computationally very efficient and about 3x faster than the fastest
existing solver. Our experiments on real and synthetic datasets, demonstrate
that gP4Pc is among the fastest methods in terms of total running time when
used within a RANSAC framework, while achieving competitive numerical
stability, accuracy, and robustness to noise.
</p>
<a href="http://arxiv.org/abs/2011.13817" target="_blank">arXiv:2011.13817</a> [<a href="http://arxiv.org/pdf/2011.13817" target="_blank">pdf</a>]

<h2>Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers. (arXiv:2011.13824v1 [cs.AI])</h2>
<h3>Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, Cho-Jui Hsieh</h3>
<p>Formal verification of neural networks (NNs) is a challenging and important
problem. Existing efficient complete solvers typically require the
branch-and-bound (BaB) process, which splits the problem domain into
sub-domains and solves each sub-domain using faster but weaker incomplete
verifiers, such as Linear Programming (LP) on linearly relaxed sub-domains. In
this paper, we propose to use the backward mode linear relaxation based
perturbation analysis (LiRPA) to replace LP during the BaB process, which can
be efficiently implemented on the typical machine learning accelerators such as
GPUs and TPUs. However, unlike LP, LiRPA when applied naively can produce much
weaker bounds and even cannot check certain conflicts of sub-domains during
splitting, making the entire procedure incomplete after BaB. To address these
challenges, we apply a fast gradient based bound tightening procedure combined
with batch splits and the design of minimal usage of LP bound procedure,
enabling us to effectively use LiRPA on the accelerator hardware for the
challenging complete NN verification problem and significantly outperform
LP-based approaches. On a single GPU, we demonstrate an order of magnitude
speedup compared to existing LP-based approaches.
</p>
<a href="http://arxiv.org/abs/2011.13824" target="_blank">arXiv:2011.13824</a> [<a href="http://arxiv.org/pdf/2011.13824" target="_blank">pdf</a>]

<h2>Deep orthogonal linear networks are shallow. (arXiv:2011.13831v1 [stat.ML])</h2>
<h3>Pierre Ablin</h3>
<p>We consider the problem of training a deep orthogonal linear network, which
consists of a product of orthogonal matrices, with no non-linearity in-between.
We show that training the weights with Riemannian gradient descent is
equivalent to training the whole factorization by gradient descent. This means
that there is no effect of overparametrization and implicit bias at all in this
setting: training such a deep, overparametrized, network is perfectly
equivalent to training a one-layer shallow network.
</p>
<a href="http://arxiv.org/abs/2011.13831" target="_blank">arXiv:2011.13831</a> [<a href="http://arxiv.org/pdf/2011.13831" target="_blank">pdf</a>]

<h2>SFTrack++: A Fast Learnable Spectral Segmentation Approach for Space-Time Consistent Tracking. (arXiv:2011.13843v1 [cs.CV])</h2>
<h3>Elena Burceanu</h3>
<p>We propose an object tracking method, SFTrack++, that smoothly learns to
preserve the tracked object consistency over space and time dimensions by
taking a spectral clustering approach over the graph of pixels from the video,
using a fast 3D filtering formulation for finding the principal eigenvector of
this graph's adjacency matrix. To better capture complex aspects of the tracked
object, we enrich our formulation to multi-channel inputs, which permit
different points of view for the same input. The channel inputs could be, like
in our experiments, the output of multiple tracking methods or other feature
maps. After extracting and combining those feature maps, instead of relying
only on hidden layers representations to predict a good tracking bounding box,
we explicitly learn an intermediate, more refined one, namely the segmentation
map of the tracked object. This prevents the rough common bounding box approach
to introduce noise and distractors in the learning process. We test our method,
SFTrack++, on seven tracking benchmarks: VOT2018, LaSOT, TrackingNet, GOT10k,
NFS, OTB-100, and UAV123.
</p>
<a href="http://arxiv.org/abs/2011.13843" target="_blank">arXiv:2011.13843</a> [<a href="http://arxiv.org/pdf/2011.13843" target="_blank">pdf</a>]

<h2>Autonomous learning of multiple, context-dependent tasks. (arXiv:2011.13847v1 [cs.RO])</h2>
<h3>Vieri Giuliano Santucci, Davide Montella, Bruno Castro da Silva, Gianluca Baldassarre</h3>
<p>When facing the problem of autonomously learning multiple tasks with
reinforcement learning systems, researchers typically focus on solutions where
just one parametrised policy per task is sufficient to solve them. However, in
complex environments presenting different contexts, the same task might need a
set of different skills to be solved. These situations pose two challenges: (a)
to recognise the different contexts that need different policies; (b) quickly
learn the policies to accomplish the same tasks in the new discovered contexts.
These two challenges are even harder if faced within an open-ended learning
framework where an agent has to autonomously discover the goals that it might
accomplish in a given environment, and also to learn the motor skills to
accomplish them. We propose a novel open-ended learning robot architecture,
C-GRAIL, that solves the two challenges in an integrated fashion. In
particular, the architecture is able to detect new relevant contests, and
ignore irrelevant ones, on the basis of the decrease of the expected
performance for a given goal. Moreover, the architecture can quickly learn the
policies for the new contexts by exploiting transfer learning importing
knowledge from already acquired policies. The architecture is tested in a
simulated robotic environment involving a robot that autonomously learns to
reach relevant target objects in the presence of multiple obstacles generating
several different obstacles. The proposed architecture outperforms other models
not using the proposed autonomous context-discovery and transfer-learning
mechanisms.
</p>
<a href="http://arxiv.org/abs/2011.13847" target="_blank">arXiv:2011.13847</a> [<a href="http://arxiv.org/pdf/2011.13847" target="_blank">pdf</a>]

<h2>Robust Detection of Non-overlapping Ellipses from Points with Applications to Circular Target Extraction in Images and Cylinder Detection in Point Clouds. (arXiv:2011.13849v1 [cs.CV])</h2>
<h3>Reza Maalek, Derek Lichti</h3>
<p>This manuscript provides a collection of new methods for the automated
detection of non-overlapping ellipses from edge points. The new methods include
a robust Monte Carlo-based approach for detecting points following elliptical
patterns; process to detect non-overlapping ellipses from edge points; and
procedure to detect cylinders from three-dimensional point clouds. The proposed
methods were compared with established state-of-the-art methods, using
simulated and real-world datasets, through the design of four sets of original
experiments. It was found that the proposed robust ellipse detection was
superior to the popular least median of squares method in both simulated and
real-world datasets. The proposed process for detecting non-overlapping
ellipses outperformed two established edge chaining/following methods, proposed
by Fornaciari and Patraucean, in images. The proposed cylinder extraction
method identified all detectable mechanical pipes in real-world point clouds.
The results show promise for the application of the proposed methods for
automatic extraction of circular targets from images and mechanical pipes from
point clouds.
</p>
<a href="http://arxiv.org/abs/2011.13849" target="_blank">arXiv:2011.13849</a> [<a href="http://arxiv.org/pdf/2011.13849" target="_blank">pdf</a>]

<h2>Real-time Active Vision for a Humanoid Soccer Robot Using Deep Reinforcement Learning. (arXiv:2011.13851v1 [cs.RO])</h2>
<h3>Soheil Khatibi, Meisam Teimouri, Mahdi Rezaei</h3>
<p>In this paper, we present an active vision method using a deep reinforcement
learning approach for a humanoid soccer-playing robot. The proposed method
adaptively optimises the viewpoint of the robot to acquire the most useful
landmarks for self-localisation while keeping the ball into its viewpoint.
Active vision is critical for humanoid decision-maker robots with a limited
field of view. To deal with an active vision problem, several probabilistic
entropy-based approaches have previously been proposed which are highly
dependent on the accuracy of the self-localisation model. However, in this
research, we formulate the problem as an episodic reinforcement learning
problem and employ a Deep Q-learning method to solve it. The proposed network
only requires the raw images of the camera to move the robot's head toward the
best viewpoint. The model shows a very competitive rate of 80% success rate in
achieving the best viewpoint. We implemented the proposed method on a humanoid
robot simulated in Webots simulator. Our evaluations and experimental results
show that the proposed method outperforms the entropy-based methods in the
RoboCup context, in cases with high self-localisation errors.
</p>
<a href="http://arxiv.org/abs/2011.13851" target="_blank">arXiv:2011.13851</a> [<a href="http://arxiv.org/pdf/2011.13851" target="_blank">pdf</a>]

<h2>Field of Junctions. (arXiv:2011.13866v1 [cs.CV])</h2>
<h3>Dor Verbin, Todd Zickler</h3>
<p>We introduce a bottom-up model for jointly finding many boundary elements in
an image, including edges, curves, corners and junctions. The model explains
boundary shape in each small patch using a junction with M angles and a
freely-moving vertex. Images are analyzed by solving a non-convex optimization
problem using purposefully-designed algorithms, cooperatively finding M+2
junction values at every pixel. The resulting field of junctions is
simultaneously an edge detector, a corner/junction detector, and a
boundary-aware smoothing of regional appearance. We demonstrate how it behaves
at different scales, and for both single-channel and multi-channel input.
Notably, we find it has unprecedented resilience to noise: It succeeds at high
noise levels where previous methods for segmentation and for edge, corner and
junction detection fail.
</p>
<a href="http://arxiv.org/abs/2011.13866" target="_blank">arXiv:2011.13866</a> [<a href="http://arxiv.org/pdf/2011.13866" target="_blank">pdf</a>]

<h2>An open-ended learning architecture to face the REAL 2020 simulated robot competition. (arXiv:2011.13880v1 [cs.RO])</h2>
<h3>Emilio Cartoni (1), Davide Montella (1), Jochen Triesch (1), Gianluca Baldassarre (1) ((1) Institute of Cognitive Sciences and Technologies, (2) Frankfurt Institute for Advanced Studies)</h3>
<p>Open-ended learning is a core research field of machine learning and robotics
aiming to build learning machines and robots able to autonomously acquire
knowledge and skills and to reuse them to solve novel tasks. The multiple
challenges posed by open-ended learning have been operationalized in the
robotic competition REAL 2020. This requires a simulated camera-arm-gripper
robot to (a) autonomously learn to interact with objects during an intrinsic
phase where it can learn how to move objects and then (b) during an extrinsic
phase, to re-use the acquired knowledge to accomplish externally given goals
requiring the robot to move objects to specific locations unknown during the
intrinsic phase. Here we present a 'baseline architecture' for solving the
challenge, provided as baseline model for REAL 2020. Few models have all the
functionalities needed to solve the REAL 2020 benchmark and none has been
tested with it yet. The architecture we propose is formed by three components:
(1) Abstractor: abstracting sensory input to learn relevant control variables
from images; (2) Explorer: generating experience to learn goals and actions;
(3) Planner: formulating and executing action plans to accomplish the
externally provided goals. The architecture represents the first model to solve
the simpler REAL 2020 'Round 1' allowing the use of a simple parameterised push
action. On Round 2, the architecture was used with a more general action
(sequence of joints positions) achieving again higher than chance level
performance. The baseline software is well documented and available for
download and use at https://github.com/AIcrowd/REAL2020_starter_kit.
</p>
<a href="http://arxiv.org/abs/2011.13880" target="_blank">arXiv:2011.13880</a> [<a href="http://arxiv.org/pdf/2011.13880" target="_blank">pdf</a>]

<h2>Offline Learning from Demonstrations and Unlabeled Experience. (arXiv:2011.13885v1 [cs.LG])</h2>
<h3>Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf Aytar, Misha Denil, Nando de Freitas, Scott Reed</h3>
<p>Behavior cloning (BC) is often practical for robot learning because it allows
a policy to be trained offline without rewards, by supervised learning on
expert demonstrations. However, BC does not effectively leverage what we will
refer to as unlabeled experience: data of mixed and unknown quality without
reward annotations. This unlabeled data can be generated by a variety of
sources such as human teleoperation, scripted policies and other agents on the
same robot. Towards data-driven offline robot learning that can use this
unlabeled experience, we introduce Offline Reinforced Imitation Learning
(ORIL). ORIL first learns a reward function by contrasting observations from
demonstrator and unlabeled trajectories, then annotates all data with the
learned reward, and finally trains an agent via offline reinforcement learning.
Across a diverse set of continuous control and simulated robotic manipulation
tasks, we show that ORIL consistently outperforms comparable BC agents by
effectively leveraging unlabeled experience.
</p>
<a href="http://arxiv.org/abs/2011.13885" target="_blank">arXiv:2011.13885</a> [<a href="http://arxiv.org/pdf/2011.13885" target="_blank">pdf</a>]

<h2>Convolutional Neural Networks Towards Arduino Navigation of Indoor Environments. (arXiv:2011.13893v1 [cs.RO])</h2>
<h3>Michael Muratov, Sachkiran Kaur, Michael Szpakowicz</h3>
<p>In this paper we propose a number of tested ways in which a low-budget demo
car could be made to navigate an indoor environment. Canny Edge Detection,
Supervised Floor Detection and Imitation Learning were used separately and are
contrasted in their effectiveness. The equipment used in this paper
approximated an autonomous robot configured to work with a mobile device for
image processing. This paper does not provide definitive solutions and simply
illustrates the approaches taken to successfully achieve autonomous navigation
of indoor environments. The successes and failures of all approaches were
recorded and elaborated on to give the reader an insight into the construction
of such an autonomous robot.
</p>
<a href="http://arxiv.org/abs/2011.13893" target="_blank">arXiv:2011.13893</a> [<a href="http://arxiv.org/pdf/2011.13893" target="_blank">pdf</a>]

<h2>Efficient Scene Compression for Visual-based Localization. (arXiv:2011.13894v1 [cs.CV])</h2>
<h3>Marcela Mera-Trujillo, Benjamin Smith, Victor Fragoso</h3>
<p>Estimating the pose of a camera with respect to a 3D reconstruction or scene
representation is a crucial step for many mixed reality and robotics
applications. Given the vast amount of available data nowadays, many
applications constrain storage and/or bandwidth to work efficiently. To satisfy
these constraints, many applications compress a scene representation by
reducing its number of 3D points. While state-of-the-art methods use
$K$-cover-based algorithms to compress a scene, they are slow and hard to tune.
To enhance speed and facilitate parameter tuning, this work introduces a novel
approach that compresses a scene representation by means of a constrained
quadratic program (QP). Because this QP resembles a one-class support vector
machine, we derive a variant of the sequential minimal optimization to solve
it. Our approach uses the points corresponding to the support vectors as the
subset of points to represent a scene. We also present an efficient
initialization method that allows our method to converge quickly. Our
experiments on publicly available datasets show that our approach compresses a
scene representation quickly while delivering accurate pose estimates.
</p>
<a href="http://arxiv.org/abs/2011.13894" target="_blank">arXiv:2011.13894</a> [<a href="http://arxiv.org/pdf/2011.13894" target="_blank">pdf</a>]

<h2>Skill Transfer via Partially Amortized Hierarchical Planning. (arXiv:2011.13897v1 [cs.LG])</h2>
<h3>Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, Florian Shkurti</h3>
<p>To quickly solve new tasks in complex environments, intelligent agents need
to build up reusable knowledge. For example, a learned world model captures
knowledge about the environment that applies to new tasks. Similarly, skills
capture general behaviors that can apply to new tasks. In this paper, we
investigate how these two approaches can be integrated into a single
reinforcement learning agent. Specifically, we leverage the idea of partial
amortization for fast adaptation at test time. For this, actions are produced
by a policy that is learned over time while the skills it conditions on are
chosen using online planning. We demonstrate the benefits of our design
decisions across a suite of challenging locomotion tasks and demonstrate
improved sample efficiency in single tasks as well as in transfer from one task
to another, as compared to competitive baselines. Videos are available at:
https://sites.google.com/view/partial-amortization-hierarchy/home
</p>
<a href="http://arxiv.org/abs/2011.13897" target="_blank">arXiv:2011.13897</a> [<a href="http://arxiv.org/pdf/2011.13897" target="_blank">pdf</a>]

<h2>Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v1 [cs.CV])</h2>
<h3>Mahmut Yurt, Muzaffer &#xd6;zbey, Salman Ul Hassan Dar, Berk T&#x131;naz, Kader Karl&#x131; O&#x11f;uz, Tolga &#xc7;ukur</h3>
<p>Magnetic resonance imaging (MRI) offers the flexibility to image a given
anatomic volume under a multitude of tissue contrasts. Yet, scan time
considerations put stringent limits on the quality and diversity of MRI data.
The gold-standard approach to alleviate this limitation is to recover
high-quality images from data undersampled across various dimensions such as
the Fourier domain or contrast sets. A central divide among recovery methods is
whether the anatomy is processed per volume or per cross-section. Volumetric
models offer enhanced capture of global contextual information, but they can
suffer from suboptimal learning due to elevated model complexity.
Cross-sectional models with lower complexity offer improved learning behavior,
yet they ignore contextual information across the longitudinal dimension of the
volume. Here, we introduce a novel data-efficient progressively volumetrized
generative model (ProvoGAN) that decomposes complex volumetric image recovery
tasks into a series of simpler cross-sectional tasks across individual
rectilinear dimensions. ProvoGAN effectively captures global context and
recovers fine-structural details across all dimensions, while maintaining low
model complexity and data-efficiency advantages of cross-sectional models.
Comprehensive demonstrations on mainstream MRI reconstruction and synthesis
tasks show that ProvoGAN yields superior performance to state-of-the-art
volumetric and cross-sectional models.
</p>
<a href="http://arxiv.org/abs/2011.13913" target="_blank">arXiv:2011.13913</a> [<a href="http://arxiv.org/pdf/2011.13913" target="_blank">pdf</a>]

<h2>Machine learning for risk analysis of Urinary Tract Infection in people with dementia. (arXiv:2011.13916v1 [cs.LG])</h2>
<h3>Honglin Li, Payam Barnaghi, Severin Skillman, David Sharp, Ramin Nilforooshan, Helen Rostill</h3>
<p>The Urinary Tract Infections (UTIs) are one of the top reasons for unplanned
hospital admissions in people with dementia, and if detected early, they can be
timely treated. However, the standard UTI diagnosis tests, e.g. urine tests,
will be only taken if the patients are clinically suspected of having UTIs.
This causes a delay in diagnosis and treatment of the conditions and in some
cases like people with dementia, the symptoms can be difficult to observe.
Delay in detection and treatment of dementia is one of the key reasons for
unplanned hospital admissions in people with dementia. To address these issues,
we have developed a technology-assisted monitoring system, which is a Class 1
medical device. The system uses off-the-shelf and low-cost in-home sensory
devices to monitor environmental and physiological data of people with dementia
within their own homes. We have designed a machine learning model to use the
data and provide risk analysis for UTIs. We use a semi-supervised learning
model which leverage the environmental data, i.e. the data collected from the
motion sensors, smart plugs and network-connected body temperature monitoring
devices in the home, to detect patterns that can show the risk of UTIs. Since
the data is noisy and partially labelled, we combine the neural networks and
probabilistic neural networks to train an auto-encoder, which is to extract the
general representation of the data. We will demonstrate our smart home
management by videos/online, and show how our model can pick up the UTI related
patterns.
</p>
<a href="http://arxiv.org/abs/2011.13916" target="_blank">arXiv:2011.13916</a> [<a href="http://arxiv.org/pdf/2011.13916" target="_blank">pdf</a>]

<h2>Task Programming: Learning Data Efficient Behavior Representations. (arXiv:2011.13917v1 [cs.CV])</h2>
<h3>Jennifer J. Sun, Ann Kennedy, Eric Zhan, Yisong Yue, Pietro Perona</h3>
<p>Specialized domain knowledge is often necessary to accurately annotate
training sets for in-depth analysis, but can be burdensome and time-consuming
to acquire from domain experts. This issue arises prominently in automated
behavior analysis, in which agent movements or actions of interest are detected
from video tracking data. To reduce annotation effort, we present TREBA: a
method to learn annotation-sample efficient trajectory embedding for behavior
analysis, based on multi-task self-supervised learning. The tasks in our method
can be efficiently engineered by domain experts through a process we call "task
programming", which uses programs to explicitly encode structured knowledge
from domain experts. Total domain expert effort can be reduced by exchanging
data annotation time for the construction of a small number of programmed
tasks. We evaluate this trade-off using data from behavioral neuroscience, in
which specialized domain knowledge is used to identify behaviors. We present
experimental results in three datasets across two domains: mice and fruit
flies. Using embeddings from TREBA, we reduce annotation burden by up to a
factor of 10 without compromising accuracy compared to state-of-the-art
features. Our results thus suggest that task programming can be an effective
way to reduce annotation effort for domain experts.
</p>
<a href="http://arxiv.org/abs/2011.13917" target="_blank">arXiv:2011.13917</a> [<a href="http://arxiv.org/pdf/2011.13917" target="_blank">pdf</a>]

<h2>Unsupervised part representation by Flow Capsules. (arXiv:2011.13920v1 [cs.CV])</h2>
<h3>Sara Sabour, Andrea Tagliasacchi, Soroosh Yazdani, Geoffrey E. Hinton, David J. Fleet</h3>
<p>Capsule networks are designed to parse an image into a hierarchy of objects,
parts and relations. While promising, they remain limited by an inability to
learn effective low level part descriptions. To address this issue we propose a
novel self-supervised method for learning part descriptors of an image. During
training, we exploit motion as a powerful perceptual cue for part definition,
using an expressive decoder for part generation and layered image formation
with occlusion. Experiments demonstrate robust part discovery in the presence
of multiple objects, cluttered backgrounds, and significant occlusion. The
resulting part descriptors, a.k.a. part capsules, are decoded into shape masks,
filling in occluded pixels, along with relative depth on single images. We also
report unsupervised object classification using our capsule parts in a stacked
capsule autoencoder.
</p>
<a href="http://arxiv.org/abs/2011.13920" target="_blank">arXiv:2011.13920</a> [<a href="http://arxiv.org/pdf/2011.13920" target="_blank">pdf</a>]

<h2>Learning to Optimize with Hidden Constraints. (arXiv:1805.09293v3 [cs.LG] UPDATED)</h2>
<h3>Aaron Babier, Timothy C. Y. Chan, Adam Diamant, Rafid Mahmood</h3>
<p>The topic of learning to solve optimization problems has received interest
from both the operations research and machine learning communities where each
considers contrasting approaches: conditional stochastic optimization
frameworks solved using provably optimal structured models versus deep learning
models that leverage large data sets to yield empirically effective decision
estimators. In this work, we combine the best of both worlds to solve the
problem of learning to generate decisions to instances of continuous
optimization problems where the feasible set varies with contextual features.
We propose a novel framework for training a generative model to estimate
optimal decisions by combining interior point methods and adversarial learning
which we further embed within an active learning algorithm. Decisions generated
by our model satisfy in-sample and out-of-sample optimality guarantees.
Finally, we investigate case studies in portfolio optimization and personalized
treatment design, demonstrating that our approach yields significant advantages
over predict-then-optimize and supervised deep learning techniques,
respectively.
</p>
<a href="http://arxiv.org/abs/1805.09293" target="_blank">arXiv:1805.09293</a> [<a href="http://arxiv.org/pdf/1805.09293" target="_blank">pdf</a>]

<h2>Learning Embeddings of Directed Networks with Text-Associated Nodes---with Applications in Software Package Dependency Networks. (arXiv:1809.02270v5 [cs.LG] UPDATED)</h2>
<h3>Kexuan Sun, Shudan Zhong, Hong Xu</h3>
<p>A network embedding consists of a vector representation for each node in the
network. Its usefulness has been shown in many real-world application domains,
such as social networks and web networks. Directed networks with text
associated with each node, such as software package dependency networks, are
commonplace. However, to the best of our knowledge, their embeddings have
hitherto not been specifically studied. In this paper, we propose PCTADW-1 and
PCTADW-2, two algorithms based on neural networks that learn embeddings of
directed networks with text associated with each node. We create two new
node-labeled such networks: The package dependency networks in two popular
GNU/Linux distributions, Debian and Fedora. We experimentally demonstrate that
the embeddings produced by our algorithms resulted in node classification with
better quality than those of various baselines on these two networks. We
observe that there exist systematic presence of analogies (similar to those in
word embeddings) in the network embeddings of software package dependency
networks. To the best of our knowledge, this is the first time that such
systematic presence of analogies is observed in network and document
embeddings. We further demonstrate that these network embeddings can be novelly
used for better understanding software attributes, such as the development
process and user interface of software, etc.
</p>
<a href="http://arxiv.org/abs/1809.02270" target="_blank">arXiv:1809.02270</a> [<a href="http://arxiv.org/pdf/1809.02270" target="_blank">pdf</a>]

<h2>Adapting Auxiliary Losses Using Gradient Similarity. (arXiv:1812.02224v2 [stat.ML] UPDATED)</h2>
<h3>Yunshu Du, Wojciech M. Czarnecki, Siddhant M. Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, Balaji Lakshminarayanan</h3>
<p>One approach to deal with the statistical inefficiency of neural networks is
to rely on auxiliary losses that help to build useful representations. However,
it is not always trivial to know if an auxiliary task will be helpful for the
main task and when it could start hurting. We propose to use the cosine
similarity between gradients of tasks as an adaptive weight to detect when an
auxiliary loss is helpful to the main loss. We show that our approach is
guaranteed to converge to critical points of the main task and demonstrate the
practical usefulness of the proposed algorithm in a few domains: multi-task
supervised learning on subsets of ImageNet, reinforcement learning on
gridworld, and reinforcement learning on Atari games.
</p>
<a href="http://arxiv.org/abs/1812.02224" target="_blank">arXiv:1812.02224</a> [<a href="http://arxiv.org/pdf/1812.02224" target="_blank">pdf</a>]

<h2>Provable Low Rank Phase Retrieval. (arXiv:1902.04972v5 [cs.LG] UPDATED)</h2>
<h3>Seyedehsara Nayer, Praneeth Narayanamurthy, Namrata Vaswani</h3>
<p>We study the Low Rank Phase Retrieval (LRPR) problem defined as follows:
recover an $n \times q$ matrix $X^*$ of rank $r$ from a different and
independent set of $m$ phaseless (magnitude-only) linear projections of each of
its columns. To be precise, we need to recover $X^*$ from $y_k := |A_k{}'
x^*_k|, k=1,2,\dots, q$ when the measurement matrices $A_k$ are mutually
independent. Here $y_k$ is an $m$ length vector, $A_k$ is an $n \times m$
matrix, and $'$ denotes matrix transpose. The question is when can we solve
LRPR with $m \ll n$? A reliable solution can enable fast and low-cost phaseless
dynamic imaging, e.g., Fourier ptychographic imaging of live biological
specimens. In this work, we develop the first provably correct approach for
solving this LRPR problem. Our proposed algorithm, Alternating Minimization for
Low-Rank Phase Retrieval (AltMinLowRaP), is an AltMin based solution and hence
is also provably fast (converges geometrically). Our guarantee shows that
AltMinLowRaP solves LRPR to $\epsilon$ accuracy, with high probability, as long
as $m q \ge C n r^4 \log(1/\epsilon)$, the matrices $A_k$ contain i.i.d.
standard Gaussian entries, and the right singular vectors of $X^*$ satisfy the
incoherence assumption from matrix completion literature. Here $C$ is a
numerical constant that only depends on the condition number of $X^*$ and on
its incoherence parameter. Its time complexity is only $ C mq nr
\log^2(1/\epsilon)$. Since even the linear (with phase) version of the above
problem is not fully solved, the above result is also the first complete
solution and guarantee for the linear case. Finally, we also develop a simple
extension of our results for the dynamic LRPR setting.
</p>
<a href="http://arxiv.org/abs/1902.04972" target="_blank">arXiv:1902.04972</a> [<a href="http://arxiv.org/pdf/1902.04972" target="_blank">pdf</a>]

<h2>Pair-Matching: Links Prediction with Adaptive Queries. (arXiv:1905.07342v2 [stat.ML] UPDATED)</h2>
<h3>Christophe Giraud, Yann Issartel, Luc Leh&#xe9;ricy, Matthieu Lerasle</h3>
<p>The pair-matching problem appears in many applications where one wants to
discover good matches between pairs of entities or individuals. Formally, the
set of individuals is represented by the nodes of a graph where the edges,
unobserved at first, represent the good matches. The algorithm queries pairs of
nodes and observes the presence/absence of edges. Its goal is to discover as
many edges as possible with a fixed budget of queries. Pair-matching is a
particular instance of multi-armed bandit problem in which the arms are pairs
of individuals and the rewards are edges linking these pairs. This bandit
problem is non-standard though, as each arm can only be played once.

Given this last constraint, sublinear regret can be expected only if the
graph presents some underlying structure. This paper shows that sublinear
regret is achievable in the case where the graph is generated according to a
Stochastic Block Model (SBM) with two communities. Optimal regret bounds are
computed for this pair-matching problem. They exhibit a phase transition
related to the Kesten-Stigum threshold for community detection in SBM. The
pair-matching problem is considered in the case where each node is constrained
to be sampled less than a given amount of times. We show how optimal regret
rates depend on this constraint. The paper is concluded by a conjecture
regarding the optimal regret when the number of communities is larger than 2.
Contrary to the two communities case, we argue that a statistical-computational
gap would appear in this problem.
</p>
<a href="http://arxiv.org/abs/1905.07342" target="_blank">arXiv:1905.07342</a> [<a href="http://arxiv.org/pdf/1905.07342" target="_blank">pdf</a>]

<h2>Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. (arXiv:1905.12794v3 [cs.CV] UPDATED)</h2>
<h3>Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, Rogerio Feris</h3>
<p>Conversational interfaces for the detail-oriented retail fashion domain are
more natural, expressive, and user friendly than classical keyword-based search
interfaces. In this paper, we introduce the Fashion IQ dataset to support and
advance research on interactive fashion image retrieval. Fashion IQ is the
first fashion dataset to provide human-generated captions that distinguish
similar pairs of garment images together with side-information consisting of
real-world product descriptions and derived visual attribute labels for these
images. We provide a detailed analysis of the characteristics of the Fashion IQ
data, and present a transformer-based user simulator and interactive image
retriever that can seamlessly integrate visual attributes with image features,
user feedback, and dialog history, leading to improved performance over the
state of the art in dialog-based image retrieval. We believe that our dataset
will encourage further work on developing more natural and real-world
applicable conversational shopping assistants.
</p>
<a href="http://arxiv.org/abs/1905.12794" target="_blank">arXiv:1905.12794</a> [<a href="http://arxiv.org/pdf/1905.12794" target="_blank">pdf</a>]

<h2>Learning Curves for Deep Neural Networks: A Gaussian Field Theory Perspective. (arXiv:1906.05301v4 [cs.LG] UPDATED)</h2>
<h3>Omry Cohen, Or Malka, Zohar Ringel</h3>
<p>In the past decade, deep neural networks (DNNs) came to the fore as the
leading machine learning algorithms for a variety of tasks. Their raise was
founded on market needs and engineering craftsmanship, the latter based more on
trial and error than on theory. While still far behind the application
forefront, the theoretical study of DNNs has recently made important
advancements in analyzing the highly over-parameterized regime where some exact
results have been obtained. Leveraging these ideas and adopting a more
physics-like approach, here we construct a versatile field-theory formalism for
supervised deep learning, involving renormalization group, Feynman diagrams and
replicas. In particular we show that our approach leads to highly accurate
predictions of learning curves of truly deep DNNs trained on polynomial
regression tasks and that these predictions can be used for efficient
hyper-parameter optimization. In addition, they explain how DNNs generalize
well despite being highly over-parameterized, this due to an entropic bias to
simple functions which, for the case of fully-connected DNNs with data sampled
on the hypersphere, are low order polynomials in the input vector. Being a
complex interacting system of artificial neurons, we believe that such tools
and methodologies borrowed from condensed matter physics would prove essential
for obtaining an accurate quantitative understanding of deep learning.
</p>
<a href="http://arxiv.org/abs/1906.05301" target="_blank">arXiv:1906.05301</a> [<a href="http://arxiv.org/pdf/1906.05301" target="_blank">pdf</a>]

<h2>Sparse regular variation. (arXiv:1907.00686v4 [stat.ML] UPDATED)</h2>
<h3>Meyer Nicolas (LPSM (UMR\_8001)), Olivier Wintenberger (LPSM (UMR\_8001))</h3>
<p>Regular variation provides a convenient theoretical framework to study large
events. In the multivariate setting, the dependence structure of the positive
extremes is characterized by a measure - the spectral measure - defined on the
positive orthant of the unit sphere. This measure gathers information on the
localization of extreme events and has often a sparse support since severe
events do not simultaneously occur in all directions. However, it is defined
through weak convergence which does not provide a natural way to capture this
sparsity structure.In this paper, we introduce the notion of sparse regular
variation which allows to better learn the dependence structure of extreme
events. This concept is based on the Euclidean projection onto the simplex for
which efficient algorithms are known. We prove that under mild assumptions
sparse regular variation and regular variation are two equivalent notions and
we establish several results for sparsely regularly varying random vectors.
Finally, we illustrate on numerical examples how this new concept allows one to
detect extremal directions.
</p>
<a href="http://arxiv.org/abs/1907.00686" target="_blank">arXiv:1907.00686</a> [<a href="http://arxiv.org/pdf/1907.00686" target="_blank">pdf</a>]

<h2>Entropic Out-of-Distribution Detection. (arXiv:1908.05569v10 [cs.LG] UPDATED)</h2>
<h3>David Mac&#xea;do, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I. Oliveira, Teresa Ludermir</h3>
<p>In this paper, we argue that the unsatisfactory out-of-distribution (OOD)
detection performance of neural networks is mainly due to the SoftMax loss
anisotropy and propensity to produce low entropy probability distributions in
disagreement with the principle of maximum entropy. Current out-of-distribution
(OOD) detection approaches usually do not directly fix the SoftMax loss
drawbacks but rather build techniques to circumvent it. Unfortunately, those
methods usually produce undesired side effects (e.g., additional
hyperparameters, slower inferences, and collecting extra data). In the opposite
direction, we propose replacing SoftMax loss with a novel loss function that
does not suffer from the mentioned weaknesses. The proposed IsoMax loss is
isotropic (exclusively distance-based) and provides high entropy posterior
probability distributions. Replacing the SoftMax loss by IsoMax loss requires
no model or training changes. Additionally, the models trained with IsoMax loss
produce as fast and energy-efficient inferences as those trained using SoftMax
loss. Further, no classification accuracy drop is observed. The proposed method
does not rely on outlier/background data, hyperparameter tuning, temperature
calibration, feature extraction, metric learning, adversarial training,
ensemble procedures, or generative models. Our experiments showed that IsoMax
loss works as a seamless SoftMax loss drop-in replacement that significantly
improves neural networks OOD detection performance. Therefore, it may be used
as a baseline solution to be combined with current or future OOD detection
techniques to achieve even higher results.
</p>
<a href="http://arxiv.org/abs/1908.05569" target="_blank">arXiv:1908.05569</a> [<a href="http://arxiv.org/pdf/1908.05569" target="_blank">pdf</a>]

<h2>Development of Fast Refinement Detectors on AI Edge Platforms. (arXiv:1909.10798v2 [cs.CV] UPDATED)</h2>
<h3>Min-Kook Choi, Heechul Jung</h3>
<p>With the improvements in the object detection networks, several variations of
object detection networks have been achieved impressive performance. However,
the performance evaluation of most models has focused on detection accuracy,
and performance verification is mostly based on high-end GPU hardware. In this
paper, we propose real-time object detectors that guarantee balanced
performance for real-time systems on embedded platforms. The proposed model
utilizes the basic head structure of the RefineDet model, which is a variant of
the single-shot object detector (SSD). In order to ensure real-time
performance, CNN models with relatively shallow layers or fewer parameters have
been used as the backbone structure. In addition to the basic VGGNet and ResNet
structures, various backbone structures such as MobileNet, Xception, ResNeXt,
Inception-SENet, and SE-ResNeXt have been used for this purpose. Successful
training of object detection networks was achieved through an appropriate
combination of intermediate layers. The accuracy of the proposed detector was
estimated by the evaluation of the MS-COCO 2017 object detection dataset and
the inference speed on the NVIDIA Drive PX2 and Jetson Xavier boards were
tested to verify real-time performance in the embedded systems. The experiments
show that the proposed models ensure balanced performance in terms of accuracy
and inference speed in the embedded system environments. In addition, unlike
the high-end GPUs, the use of embedded GPUs involves several additional
concerns for efficient inference, which have been identified in this work. The
codes and models are publicly available on the web (link).
</p>
<a href="http://arxiv.org/abs/1909.10798" target="_blank">arXiv:1909.10798</a> [<a href="http://arxiv.org/pdf/1909.10798" target="_blank">pdf</a>]

<h2>Adaptive Class Weight based Dual Focal Loss for Improved Semantic Segmentation. (arXiv:1909.11932v3 [cs.CV] UPDATED)</h2>
<h3>Md Sazzad Hossain, Andrew P Paplinski, John M Betts</h3>
<p>In this paper, we propose a Dual Focal Loss (DFL) function, as a replacement
for the standard cross entropy (CE) function to achieve a better treatment of
the unbalanced classes in a dataset. Our DFL method is an improvement on the
recently reported Focal Loss (FL) cross-entropy function, which proposes a
scaling method that puts more weight on the examples that are difficult to
classify over those that are easy. However, the scaling parameter of FL is
empirically set, which is problem-dependent. In addition, like other CE
variants, FL only focuses on the loss of true classes. Therefore, no loss
feedback is gained from the false classes. Although focusing only on true
examples increases probability on true classes and correspondingly reduces
probability on false classes due to the nature of the softmax function, it does
not achieve the best convergence due to avoidance of the loss on false classes.
Our DFL method improves on the simple FL in two ways. Firstly, it takes the
idea of FL to focus more on difficult examples than the easy ones, but
evaluates loss on both true and negative classes with equal importance.
Secondly, the scaling parameter of DFL has been made learnable so that it can
tune itself by backpropagation rather than being dependent on manual tuning. In
this way, our proposed DFL method offers an auto-tunable loss function that can
reduce the class imbalance effect as well as put more focus on both true
difficult examples and negative easy examples.
</p>
<a href="http://arxiv.org/abs/1909.11932" target="_blank">arXiv:1909.11932</a> [<a href="http://arxiv.org/pdf/1909.11932" target="_blank">pdf</a>]

<h2>Predicting passenger origin-destination in online taxi-hailing systems. (arXiv:1910.08145v2 [cs.LG] UPDATED)</h2>
<h3>Pouria Golshanrad, Hamid Mahini, Behnam Bahrak</h3>
<p>Because of transportation planning, traffic management, and dispatch
optimization importance, passenger origin-destination prediction has become one
of the most important requirements for intelligent transportation systems
management. In this paper, we propose a model to predict the next specified
time window travels' origin and destination. To extract meaningful travel
flows, we use K-means clustering in four-dimensional space with maximum cluster
size limitation for origin and destination zones. Because of the large number
of clusters, we use non-negative matrix factorization to decrease the number of
travel clusters. Also, we use a stacked recurrent neural network model to
predict travel count in each cluster. Comparing our results with other existing
models show that our proposed model has 5-7% lower mean absolute percentage
error (MAPE) for 1-hour time windows, and 14% lower MAPE for 30-minute time
windows.
</p>
<a href="http://arxiv.org/abs/1910.08145" target="_blank">arXiv:1910.08145</a> [<a href="http://arxiv.org/pdf/1910.08145" target="_blank">pdf</a>]

<h2>An Unbiased Risk Estimator for Learning with Augmented Classes. (arXiv:1910.09388v2 [cs.LG] UPDATED)</h2>
<h3>Yu-Jie Zhang, Peng Zhao, Zhi-Hua Zhou</h3>
<p>This paper studies the problem of learning with augmented classes (LAC),
where augmented classes unobserved in the training data might emerge in the
testing phase. Previous studies generally attempt to discover augmented classes
by exploiting geometric properties, achieving inspiring empirical performance
yet lacking theoretical understandings particularly on the generalization
ability. In this paper we show that, by using unlabeled training data to
approximate the potential distribution of augmented classes, an unbiased risk
estimator of the testing distribution can be established for the LAC problem
under mild assumptions, which paves a way to develop a sound approach with
theoretical guarantees. Moreover, the proposed approach can adapt to complex
changing environments where augmented classes may appear and the prior of known
classes may change simultaneously. Extensive experiments confirm the
effectiveness of our proposed approach.
</p>
<a href="http://arxiv.org/abs/1910.09388" target="_blank">arXiv:1910.09388</a> [<a href="http://arxiv.org/pdf/1910.09388" target="_blank">pdf</a>]

<h2>Qualitative Numeric Planning: Reductions and Complexity. (arXiv:1912.04816v2 [cs.AI] UPDATED)</h2>
<h3>Blai Bonet, Hector Geffner</h3>
<p>Qualitative numerical planning is classical planning extended with
non-negative real variables that can be increased or decreased "qualitatively",
i.e., by positive indeterminate amounts. While deterministic planning with
numerical variables is undecidable in general, qualitative numerical planning
is decidable and provides a convenient abstract model for generalized planning.
The solutions to qualitative numerical problems (QNPs) were shown to correspond
to the strong cyclic solutions of an associated fully observable
non-deterministic (FOND) problem that terminate. This leads to a
generate-and-test algorithm for solving QNPs where solutions to a FOND problem
are generated one by one and tested for termination. The computational
shortcomings of this approach for solving QNPs, however, are that it is not
simple to amend FOND planners to generate all solutions, and that the number of
solutions to check can be doubly exponential in the number of variables. In
this work we address these limitations while providing additional insights on
QNPs. More precisely, we introduce two polynomial-time reductions, one from
QNPs to FOND problems and the other from FOND problems to QNPs both of which do
not involve termination tests. A result of these reductions is that QNPs are
shown to have the same expressive power and the same complexity as FOND
problems.
</p>
<a href="http://arxiv.org/abs/1912.04816" target="_blank">arXiv:1912.04816</a> [<a href="http://arxiv.org/pdf/1912.04816" target="_blank">pdf</a>]

<h2>Human Correspondence Consensus for 3D Object Semantic Understanding. (arXiv:1912.12577v2 [cs.CV] UPDATED)</h2>
<h3>Yujing Lou, Yang You, Chengkun Li, Zhoujun Cheng, Liangwei Li, Lizhuang Ma, Weiming Wang, Cewu Lu</h3>
<p>Semantic understanding of 3D objects is crucial in many applications such as
object manipulation. However, it is hard to give a universal definition of
point-level semantics that everyone would agree on. We observe that people have
a consensus on semantic correspondences between two areas from different
objects, but are less certain about the exact semantic meaning of each area.
Therefore, we argue that by providing human labeled correspondences between
different objects from the same category instead of explicit semantic labels,
one can recover rich semantic information of an object. In this paper, we
introduce a new dataset named CorresPondenceNet. Based on this dataset, we are
able to learn dense semantic embeddings with a novel geodesic consistency loss.
Accordingly, several state-of-the-art networks are evaluated on this
correspondence benchmark. We further show that CorresPondenceNet could not only
boost fine-grained understanding of heterogeneous objects but also cross-object
registration and partial object matching.
</p>
<a href="http://arxiv.org/abs/1912.12577" target="_blank">arXiv:1912.12577</a> [<a href="http://arxiv.org/pdf/1912.12577" target="_blank">pdf</a>]

<h2>GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values. (arXiv:2001.11113v7 [cs.LG] UPDATED)</h2>
<h3>Shangtong Zhang, Bo Liu, Shimon Whiteson</h3>
<p>We present GradientDICE for estimating the density ratio between the state
distribution of the target policy and the sampling distribution in off-policy
reinforcement learning. GradientDICE fixes several problems of GenDICE (Zhang
et al., 2020), the state-of-the-art for estimating such density ratios. Namely,
the optimization problem in GenDICE is not a convex-concave saddle-point
problem once nonlinearity in optimization variable parameterization is
introduced to ensure positivity, so any primal-dual algorithm is not guaranteed
to converge or find the desired solution. However, such nonlinearity is
essential to ensure the consistency of GenDICE even with a tabular
representation. This is a fundamental contradiction, resulting from GenDICE's
original formulation of the optimization problem. In GradientDICE, we optimize
a different objective from GenDICE by using the Perron-Frobenius theorem and
eliminating GenDICE's use of divergence. Consequently, nonlinearity in
parameterization is not necessary for GradientDICE, which is provably
convergent under linear function approximation.
</p>
<a href="http://arxiv.org/abs/2001.11113" target="_blank">arXiv:2001.11113</a> [<a href="http://arxiv.org/pdf/2001.11113" target="_blank">pdf</a>]

<h2>Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v3 [cs.LG] UPDATED)</h2>
<h3>Roi Pony, Itay Naeh, Shie Mannor</h3>
<p>Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that is practically unnoticeable by human observers and
is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.
</p>
<a href="http://arxiv.org/abs/2002.05123" target="_blank">arXiv:2002.05123</a> [<a href="http://arxiv.org/pdf/2002.05123" target="_blank">pdf</a>]

<h2>Set2Graph: Learning Graphs From Sets. (arXiv:2002.08772v3 [cs.LG] UPDATED)</h2>
<h3>Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron, Yaron Lipman</h3>
<p>Many problems in machine learning can be cast as learning functions from sets
to graphs, or more generally to hypergraphs; in short, Set2Graph functions.
Examples include clustering, learning vertex and edge features on graphs, and
learning features on triplets in a collection. A natural approach for building
Set2Graph models is to characterize all linear equivariant set-to-hypergraph
layers and stack them with non-linear activations. This poses two challenges:
(i) the expressive power of these networks is not well understood; and (ii)
these models would suffer from high, often intractable computational and memory
complexity, as their dimension grows exponentially. This paper advocates a
family of neural network models for learning Set2Graph functions that is both
practical and of maximal expressive power (universal), that is, can approximate
arbitrary continuous Set2Graph functions over compact sets. Testing these
models on different machine learning tasks, mainly an application to particle
physics, we find them favorable to existing baselines.
</p>
<a href="http://arxiv.org/abs/2002.08772" target="_blank">arXiv:2002.08772</a> [<a href="http://arxiv.org/pdf/2002.08772" target="_blank">pdf</a>]

<h2>LeafGAN: An Effective Data Augmentation Method for Practical Plant Disease Diagnosis. (arXiv:2002.10100v2 [cs.CV] UPDATED)</h2>
<h3>Quan Huu Cap, Hiroyuki Uga, Satoshi Kagiwada, Hitoshi Iyatomi</h3>
<p>Many applications for the automated diagnosis of plant disease have been
developed based on the success of deep learning techniques. However, these
applications often suffer from overfitting, and the diagnostic performance is
drastically decreased when used on test datasets from new environments. In this
paper, we propose LeafGAN, a novel image-to-image translation system with own
attention mechanism. LeafGAN generates a wide variety of diseased images via
transformation from healthy images, as a data augmentation tool for improving
the performance of plant disease diagnosis. Thanks to its own attention
mechanism, our model can transform only relevant areas from images with a
variety of backgrounds, thus enriching the versatility of the training images.
Experiments with five-class cucumber disease classification show that data
augmentation with vanilla CycleGAN cannot help to improve the generalization,
i.e., disease diagnostic performance increased by only 0.7% from the baseline.
In contrast, LeafGAN boosted the diagnostic performance by 7.4%. We also
visually confirmed the generated images by our LeafGAN were much better quality
and more convincing than those generated by vanilla CycleGAN. The code is
available publicly at: https://github.com/IyatomiLab/LeafGAN.
</p>
<a href="http://arxiv.org/abs/2002.10100" target="_blank">arXiv:2002.10100</a> [<a href="http://arxiv.org/pdf/2002.10100" target="_blank">pdf</a>]

<h2>LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud Semantic Segmentation. (arXiv:2003.01174v2 [cs.CV] UPDATED)</h2>
<h3>Peng Jiang, Srikanth Saripalli</h3>
<p>We present a boundary-aware domain adaptation model for LiDAR scan full-scene
semantic segmentation (LiDARNet). Our model can extract both the domain private
features and the domain shared features with a two branch structure. We
embedded Gated-SCNN into the segmentor component of LiDARNet to learn boundary
information while learning to predict full-scene semantic segmentation labels.
Moreover, we further reduce the domain gap by inducing the model to learn a
mapping between two domains using the domain shared and private features.
Besides, we introduce a new dataset (SemanticUSL (The access address of
SemanticUSL and code: this http URL)) for domain
adaptation for LiDAR point cloud semantic segmentation. The dataset has the
same data format and ontology as SemanticKITTI. We conducted experiments on
real-world datasets SemanticKITTI, SemanticPOSS, and SemanticUSL, which have
differences in channel distributions, reflectivity distributions, diversity of
scenes, and sensors setup. Using our approach, we can get a single
projection-based LiDAR full-scene semantic segmentation model working on both
domains. Our model can keep almost the same performance on the source domain
after adaptation and get an 8%-22% mIoU performance increase in the target
domain.
</p>
<a href="http://arxiv.org/abs/2003.01174" target="_blank">arXiv:2003.01174</a> [<a href="http://arxiv.org/pdf/2003.01174" target="_blank">pdf</a>]

<h2>Semi-Supervised StyleGAN for Disentanglement Learning. (arXiv:2003.03461v3 [cs.CV] UPDATED)</h2>
<h3>Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath, Anjul Patney, Ankit B. Patel, Anima Anandkumar</h3>
<p>Disentanglement learning is crucial for obtaining disentangled
representations and controllable generation. Current disentanglement methods
face several inherent limitations: difficulty with high-resolution images,
primarily focusing on learning disentangled representations, and
non-identifiability due to the unsupervised setting. To alleviate these
limitations, we design new architectures and loss functions based on StyleGAN
(Karras et al., 2019), for semi-supervised high-resolution disentanglement
learning. We create two complex high-resolution synthetic datasets for
systematic testing. We investigate the impact of limited supervision and find
that using only 0.25%~2.5% of labeled data is sufficient for good
disentanglement on both synthetic and real datasets. We propose new metrics to
quantify generator controllability, and observe there may exist a crucial
trade-off between disentangled representation learning and controllable
generation. We also consider semantic fine-grained image editing to achieve
better generalization to unseen images.
</p>
<a href="http://arxiv.org/abs/2003.03461" target="_blank">arXiv:2003.03461</a> [<a href="http://arxiv.org/pdf/2003.03461" target="_blank">pdf</a>]

<h2>Vox2Vox: 3D-GAN for Brain Tumour Segmentation. (arXiv:2003.13653v3 [cs.CV] UPDATED)</h2>
<h3>Marco Domenico Cirillo, David Abramian, Anders Eklund</h3>
<p>Gliomas are the most common primary brain malignancies, with different
degrees of aggressiveness, variable prognosis and various heterogeneous
histological sub-regions, i.e., peritumoral edema, necrotic core, enhancing and
non-enhancing tumour core. Although brain tumours can easily be detected using
multi-modal MRI, accurate tumor segmentation is a challenging task. Hence,
using the data provided by the BraTS Challenge 2020, we propose a 3D
volume-to-volume Generative Adversarial Network for segmentation of brain
tumours. The model, called Vox2Vox, generates realistic segmentation outputs
from multi-channel 3D MR images, segmenting the whole, core and enhancing tumor
with mean values of 87.20%, 81.14%, and 78.67% as dice scores and 6.44mm,
24.36mm, and 18.95mm for Hausdorff distance 95 percentile for the BraTS testing
set after ensembling 10 Vox2Vox models obtained with a 10-fold
cross-validation.
</p>
<a href="http://arxiv.org/abs/2003.13653" target="_blank">arXiv:2003.13653</a> [<a href="http://arxiv.org/pdf/2003.13653" target="_blank">pdf</a>]

<h2>Coarse-to-Fine Gaze Redirection with Numerical and Pictorial Guidance. (arXiv:2004.03064v4 [cs.CV] UPDATED)</h2>
<h3>Jingjing Chen, Jichao Zhang, Enver Sangineto, Jiayuan Fan, Tao Chen, Nicu Sebe</h3>
<p>Gaze redirection aims at manipulating the gaze of a given face image with
respect to a desired direction (i.e., a reference angle) and it can be applied
to many real life scenarios, such as video-conferencing or taking group photos.
However, previous work on this topic mainly suffers of two limitations: (1)
Low-quality image generation and (2) Low redirection precision. In this paper,
we propose to alleviate these problems by means of a novel gaze redirection
framework which exploits both a numerical and a pictorial direction guidance,
jointly with a coarse-to-fine learning strategy. Specifically, the coarse
branch learns the spatial transformation which warps input image according to
desired gaze. On the other hand, the fine-grained branch consists of a
generator network with conditional residual image learning and a multi-task
discriminator. This second branch reduces the gap between the previously warped
image and the ground-truth image and recovers finer texture details. Moreover,
we propose a numerical and pictorial guidance module~(NPG) which uses a
pictorial gazemap description and numerical angles as an extra guide to further
improve the precision of gaze redirection. Extensive experiments on a benchmark
dataset show that the proposed method outperforms the state-of-the-art
approaches in terms of both image quality and redirection precision. The code
is available at https://github.com/jingjingchen777/CFGR
</p>
<a href="http://arxiv.org/abs/2004.03064" target="_blank">arXiv:2004.03064</a> [<a href="http://arxiv.org/pdf/2004.03064" target="_blank">pdf</a>]

<h2>AdaStereo: A Simple and Efficient Approach for Adaptive Stereo Matching. (arXiv:2004.04627v2 [cs.CV] UPDATED)</h2>
<h3>Xiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang, Jianping Shi</h3>
<p>Recently records on stereo matching benchmarks are constantly broken by
end-to-end disparity networks, however the domain adaptation ability of these
deep models is quite poor. Addressing such problem, we present a novel
domain-adaptive pipeline called AdaStereo that aims to align multi-level
representations for deep stereo matching networks. Compared to previous methods
for adaptive stereo matching, our AdaStereo realizes a more standard, complete
and effective domain adaptation pipeline. Firstly, we propose a non-adversarial
progressive color transfer algorithm for input image-level alignment. Secondly,
we design an efficient parameter-free cost normalization layer for internal
feature-level alignment. Lastly, a highly-related auxiliary task,
self-supervised occlusion-aware reconstruction is presented to narrow the gaps
in output space. Without whistles and bells, our AdaStereo models achieve
state-of-the-art cross-domain performance on multiple stereo matching
benchmarks, including KITTI, Middlebury, ETH3D and DrivingStereo, even
outperform state-of-the-art disparity networks finetuned with target-domain
ground-truths.
</p>
<a href="http://arxiv.org/abs/2004.04627" target="_blank">arXiv:2004.04627</a> [<a href="http://arxiv.org/pdf/2004.04627" target="_blank">pdf</a>]

<h2>A Mean Field Games model for finite mixtures of Bernoulli and Categorical distributions. (arXiv:2004.08119v2 [stat.ML] UPDATED)</h2>
<h3>Laura Aquilanti, Simone Cacace, Fabio Camilli, Raul De Maio</h3>
<p>Finite mixture models are an important tool in the statistical analysis of
data, for example in data clustering. The optimal parameters of a mixture model
are usually computed by maximizing the log-likelihood functional via the
Expectation-Maximization algorithm. We propose an alternative approach based on
the theory of Mean Field Games, a class of differential games with an infinite
number of agents. We show that the solution of a finite state space
multi-population Mean Field Games system characterizes the critical points of
the log-likelihood functional for a Bernoulli mixture. The approach is then
generalized to mixture models of categorical distributions. Hence, the Mean
Field Games approach provides a method to compute the parameters of the mixture
model, and we show its application to some standard examples in cluster
analysis.
</p>
<a href="http://arxiv.org/abs/2004.08119" target="_blank">arXiv:2004.08119</a> [<a href="http://arxiv.org/pdf/2004.08119" target="_blank">pdf</a>]

<h2>HDD-Net: Hybrid Detector Descriptor with Mutual Interactive Learning. (arXiv:2005.05777v2 [cs.CV] UPDATED)</h2>
<h3>Axel Barroso-Laguna, Yannick Verdie, Benjamin Busam, Krystian Mikolajczyk</h3>
<p>Local feature extraction remains an active research area due to the advances
in fields such as SLAM, 3D reconstructions, or AR applications. The success in
these applications relies on the performance of the feature detector and
descriptor. While the detector-descriptor interaction of most methods is based
on unifying in single network detections and descriptors, we propose a method
that treats both extractions independently and focuses on their interaction in
the learning process rather than by parameter sharing. We formulate the
classical hard-mining triplet loss as a new detector optimisation term to
refine candidate positions based on the descriptor map. We propose a dense
descriptor that uses a multi-scale approach and a hybrid combination of
hand-crafted and learned features to obtain rotation and scale robustness by
design. We evaluate our method extensively on different benchmarks and show
improvements over the state of the art in terms of image matching on HPatches
and 3D reconstruction quality while keeping on par on camera localisation
tasks.
</p>
<a href="http://arxiv.org/abs/2005.05777" target="_blank">arXiv:2005.05777</a> [<a href="http://arxiv.org/pdf/2005.05777" target="_blank">pdf</a>]

<h2>A Randomized Algorithm to Reduce the Support of Discrete Measures. (arXiv:2006.01757v2 [cs.LG] UPDATED)</h2>
<h3>Francesco Cosentino, Harald Oberhauser, Alessandro Abate</h3>
<p>Given a discrete probability measure supported on $N$ atoms and a set of $n$
real-valued functions, there exists a probability measure that is supported on
a subset of $n+1$ of the original $N$ atoms and has the same mean when
integrated against each of the $n$ functions. If $ N \gg n$ this results in a
huge reduction of complexity. We give a simple geometric characterization of
barycenters via negative cones and derive a randomized algorithm that computes
this new measure by "greedy geometric sampling". We then study its properties,
and benchmark it on synthetic and real-world data to show that it can be very
beneficial in the $N\gg n$ regime. A Python implementation is available at
\url{https://github.com/FraCose/Recombination_Random_Algos}.
</p>
<a href="http://arxiv.org/abs/2006.01757" target="_blank">arXiv:2006.01757</a> [<a href="http://arxiv.org/pdf/2006.01757" target="_blank">pdf</a>]

<h2>Higher-Order Explanations of Graph Neural Networks via Relevant Walks. (arXiv:2006.03589v3 [cs.LG] UPDATED)</h2>
<h3>Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T. Sch&#xfc;tt, Klaus-Robert M&#xfc;ller, Gr&#xe9;goire Montavon</h3>
<p>Graph Neural Networks (GNNs) are a popular approach for predicting graph
structured data. As GNNs tightly entangle the input graph into the neural
network structure, common explainable AI approaches are not applicable. To a
large extent, GNNs have remained black-boxes for the user so far. In this
paper, we show that GNNs can in fact be naturally explained using higher-order
expansions, i.e. by identifying groups of edges that jointly contribute to the
prediction. Practically, we find that such explanations can be extracted using
a nested attribution scheme, where existing techniques such as layer-wise
relevance propagation (LRP) can be applied at each step. The output is a
collection of walks into the input graph that are relevant for the prediction.
Our novel explanation method, which we denote by GNN-LRP, is applicable to a
broad range of graph neural networks and lets us extract practically relevant
insights on sentiment analysis of text data, structure-property relationships
in quantum chemistry, and image classification.
</p>
<a href="http://arxiv.org/abs/2006.03589" target="_blank">arXiv:2006.03589</a> [<a href="http://arxiv.org/pdf/2006.03589" target="_blank">pdf</a>]

<h2>Automatic Code Summarization via Multi-dimensional Semantic Fusing in GNN. (arXiv:2006.05405v4 [cs.LG] UPDATED)</h2>
<h3>Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, Yang Liu</h3>
<p>Source code summarization aims to generate natural language summaries from
structured code snippets for better understanding code functionalities. Recent
works attempt to encode programs into graphs for learning program semantics and
yield promising results. However, these methods only use simple code
representations (e.g., AST), which limits the capability of learning the rich
semantics for complex programs. Furthermore, these models primarily rely on
graph-based message passing, which only captures local neighborhood relations.
To this end, in this paper, we combine diverse representations of the source
code (i.e., AST, CFG and PDG) into a joint code property graph. To better learn
semantics from the joint graph, we propose a retrieval-augmented mechanism to
augment source code semantics with external knowledge. Furthermore, we propose
a novel attention-based dynamic graph to capture global interactions among
nodes in the static graph and followed a hybrid message passing GNN to
incorporate both static and dynamic graph. To evaluate our proposed approach,
we release a new challenging benchmark, crawled from 200+ diversified
large-scale open-source C projects. Our method achieves the state-of-the-art
performance, improving existing methods by 1.66, 2.38 and 2.22 in terms of
BLEU-4, ROUGE-L and METEOR metrics.
</p>
<a href="http://arxiv.org/abs/2006.05405" target="_blank">arXiv:2006.05405</a> [<a href="http://arxiv.org/pdf/2006.05405" target="_blank">pdf</a>]

<h2>On Uniform Convergence and Low-Norm Interpolation Learning. (arXiv:2006.05942v2 [stat.ML] UPDATED)</h2>
<h3>Lijia Zhou, D.J. Sutherland, Nathan Srebro</h3>
<p>We consider an underdetermined noisy linear regression model where the
minimum-norm interpolating predictor is known to be consistent, and ask: can
uniform convergence in a norm ball, or at least (following Nagarajan and
Kolter) the subset of a norm ball that the algorithm selects on a typical input
set, explain this success? We show that uniformly bounding the difference
between empirical and population errors cannot show any learning in the norm
ball, and cannot show consistency for any set, even one depending on the exact
algorithm and distribution. But we argue we can explain the consistency of the
minimal-norm interpolator with a slightly weaker, yet standard, notion: uniform
convergence of zero-error predictors in a norm ball. We use this to bound the
generalization error of low- (but not minimal-) norm interpolating predictors.
</p>
<a href="http://arxiv.org/abs/2006.05942" target="_blank">arXiv:2006.05942</a> [<a href="http://arxiv.org/pdf/2006.05942" target="_blank">pdf</a>]

<h2>Spherical Motion Dynamics: Learning Dynamics of Neural Network with Normalization, Weight Decay, and SGD. (arXiv:2006.08419v4 [stat.ML] UPDATED)</h2>
<h3>Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, Jian Sun</h3>
<p>In this work, we comprehensively reveal the learning dynamics of neural
network with normalization, weight decay (WD), and SGD (with momentum), named
as Spherical Motion Dynamics (SMD). Most related works study SMD by focusing on
"effective learning rate" in "equilibrium" condition, where weight norm remains
unchanged. However, their discussions on why equilibrium condition can be
reached in SMD is either absent or less convincing. Our work investigates SMD
by directly exploring the cause of equilibrium condition. Specifically, 1) we
introduce the assumptions that can lead to equilibrium condition in SMD, and
prove that weight norm can converge at linear rate with given assumptions; 2)
we propose "angular update" as a substitute for effective learning rate to
measure the evolving of neural network in SMD, and prove angular update can
also converge to its theoretical value at linear rate; 3) we verify our
assumptions and theoretical results on various computer vision tasks including
ImageNet and MSCOCO with standard settings. Experiment results show our
theoretical findings agree well with empirical observations.
</p>
<a href="http://arxiv.org/abs/2006.08419" target="_blank">arXiv:2006.08419</a> [<a href="http://arxiv.org/pdf/2006.08419" target="_blank">pdf</a>]

<h2>Prior knowledge distillation based on financial time series. (arXiv:2006.09247v5 [cs.LG] UPDATED)</h2>
<h3>Jie Fang, Jianwu Lin</h3>
<p>One of the major characteristics of financial time series is that they
contain a large amount of non-stationary noise, which is challenging for deep
neural networks. People normally use various features to address this problem.
However, the performance of these features depends on the choice of
hyper-parameters. In this paper, we propose to use neural networks to represent
these indicators and train a large network constructed of smaller networks as
feature layers to fine-tune the prior knowledge represented by the indicators.
During back propagation, prior knowledge is transferred from human logic to
machine logic via gradient descent. Prior knowledge is the deep belief of
neural network and teaches the network to not be affected by non-stationary
noise. Moreover, co-distillation is applied to distill the structure into a
much smaller size to reduce redundant features and the risk of overfitting. In
addition, the decisions of the smaller networks in terms of gradient descent
are more robust and cautious than those of large networks. In numerical
experiments, we find that our algorithm is faster and more accurate than
traditional methods on real financial datasets. We also conduct experiments to
verify and comprehend the method.
</p>
<a href="http://arxiv.org/abs/2006.09247" target="_blank">arXiv:2006.09247</a> [<a href="http://arxiv.org/pdf/2006.09247" target="_blank">pdf</a>]

<h2>FrostNet: Towards Quantization-Aware Network Architecture Search. (arXiv:2006.09679v3 [cs.LG] UPDATED)</h2>
<h3>Taehoon Kim, YoungJoon Yoo, Jihoon Yang</h3>
<p>INT8 quantization has become one of the standard techniques for deploying
convolutional neural networks (CNNs) on edge devices to reduce the memory and
computational resource usages. By analyzing quantized performances of existing
mobile-target network architectures, we can raise an issue regarding the
importance of network architecture for optimal INT8 quantization. In this
paper, we present a new network architecture search (NAS) procedure to find a
network that guarantees both full-precision (FLOAT32) and quantized (INT8)
performances. We first propose critical but straightforward optimization method
which enables quantization-aware training (QAT) : floating-point statistic
assisting (StatAssist) and stochastic gradient boosting (GradBoost). By
integrating the gradient-based NAS with StatAssist and GradBoost, we discovered
a quantization-efficient network building block, Frost bottleneck. Furthermore,
we used Frost bottleneck as the building block for hardware-aware NAS to obtain
quantization-efficient networks, FrostNets, which show improved quantization
performances compared to other mobile-target networks while maintaining
competitive FLOAT32 performance. Our FrostNets achieve higher recognition
accuracy than existing CNNs with comparable latency when quantized, due to
higher latency reduction rate (average 65%).
</p>
<a href="http://arxiv.org/abs/2006.09679" target="_blank">arXiv:2006.09679</a> [<a href="http://arxiv.org/pdf/2006.09679" target="_blank">pdf</a>]

<h2>Kernel methods through the roof: handling billions of points efficiently. (arXiv:2006.10350v2 [cs.LG] UPDATED)</h2>
<h3>Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, Alessandro Rudi</h3>
<p>Kernel methods provide an elegant and principled approach to nonparametric
learning, but so far could hardly be used in large scale problems, since
na\"ive implementations scale poorly with data size. Recent advances have shown
the benefits of a number of algorithmic ideas, for example combining
optimization, numerical linear algebra and random projections. Here, we push
these efforts further to develop and test a solver that takes full advantage of
GPU hardware. Towards this end, we designed a preconditioned gradient solver
for kernel methods exploiting both GPU acceleration and parallelization with
multiple GPUs, implementing out-of-core variants of common linear algebra
operations to guarantee optimal hardware utilization. Further, we optimize the
numerical precision of different operations and maximize efficiency of
matrix-vector multiplications. As a result we can experimentally show dramatic
speedups on datasets with billions of points, while still guaranteeing state of
the art performance. Additionally, we make our software available as an easy to
use library.
</p>
<a href="http://arxiv.org/abs/2006.10350" target="_blank">arXiv:2006.10350</a> [<a href="http://arxiv.org/pdf/2006.10350" target="_blank">pdf</a>]

<h2>Exact posterior distributions of wide Bayesian neural networks. (arXiv:2006.10541v2 [stat.ML] UPDATED)</h2>
<h3>Jiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein</h3>
<p>Recent work has shown that the prior over functions induced by a deep
Bayesian neural network (BNN) behaves as a Gaussian process (GP) as the width
of all layers becomes large. However, many BNN applications are concerned with
the BNN function space posterior. While some empirical evidence of the
posterior convergence was provided in the original works of Neal (1996) and
Matthews et al. (2018), it is limited to small datasets or architectures due to
the notorious difficulty of obtaining and verifying exactness of BNN posterior
approximations. We provide the missing theoretical proof that the exact BNN
posterior converges (weakly) to the one induced by the GP limit of the prior.
For empirical validation, we show how to generate exact samples from a finite
BNN on a small dataset via rejection sampling.
</p>
<a href="http://arxiv.org/abs/2006.10541" target="_blank">arXiv:2006.10541</a> [<a href="http://arxiv.org/pdf/2006.10541" target="_blank">pdf</a>]

<h2>Adaptive Learning Rates with Maximum Variation Averaging. (arXiv:2006.11918v3 [cs.LG] UPDATED)</h2>
<h3>Chen Zhu, Yu Cheng, Zhe Gan, Furong Huang, Jingjing Liu, Tom Goldstein</h3>
<p>Adaptive gradient methods such as RMSProp and Adam use exponential moving
estimate of the squared gradient to compute coordinate-wise adaptive step
sizes, achieving better convergence than SGD in face of noisy objectives.
However, Adam can have undesirable convergence behavior due to unstable or
extreme adaptive learning rates. Methods such as AMSGrad and AdaBound have been
proposed to stabilize the adaptive learning rates of Adam in the later stage of
training, but they do not outperform Adam in some practical tasks such as
training Transformers. In this paper, we propose an adaptive learning rate
principle, in which the running mean of squared gradient is replaced by a
weighted mean, with weights chosen to maximize the estimated variance of each
coordinate. This gives a worst-case estimate for the local gradient variance,
taking smaller steps when large curvatures or noisy gradients are present,
which leads to more desirable convergence behavior than Adam. We prove the
proposed algorithm converges under mild assumptions for nonconvex stochastic
optimization problems, and demonstrate the improved efficacy of our adaptive
averaging approach on image classification, machine translation and natural
language understanding tasks. Moreover, our method overcomes the
non-convergence issue of Adam in BERT pretraining at large batch sizes, while
achieving better test performance than LAMB in the same setting. The code is
available at https://github.com/zhuchen03/MaxVA.
</p>
<a href="http://arxiv.org/abs/2006.11918" target="_blank">arXiv:2006.11918</a> [<a href="http://arxiv.org/pdf/2006.11918" target="_blank">pdf</a>]

<h2>Graph Prototypical Networks for Few-shot Learning on Attributed Networks. (arXiv:2006.12739v3 [cs.LG] UPDATED)</h2>
<h3>Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, Huan Liu</h3>
<p>Attributed networks nowadays are ubiquitous in a myriad of high-impact
applications, such as social network analysis, financial fraud detection, and
drug discovery. As a central analytical task on attributed networks, node
classification has received much attention in the research community. In
real-world attributed networks, a large portion of node classes only contain
limited labeled instances, rendering a long-tail node class distribution.
Existing node classification algorithms are unequipped to handle the
\textit{few-shot} node classes. As a remedy, few-shot learning has attracted a
surge of attention in the research community. Yet, few-shot node classification
remains a challenging problem as we need to address the following questions:
(i) How to extract meta-knowledge from an attributed network for few-shot node
classification? (ii) How to identify the informativeness of each labeled
instance for building a robust and effective model? To answer these questions,
in this paper, we propose a graph meta-learning framework -- Graph Prototypical
Networks (GPN). By constructing a pool of semi-supervised node classification
tasks to mimic the real test environment, GPN is able to perform
\textit{meta-learning} on an attributed network and derive a highly
generalizable model for handling the target classification task. Extensive
experiments demonstrate the superior capability of GPN in few-shot node
classification.
</p>
<a href="http://arxiv.org/abs/2006.12739" target="_blank">arXiv:2006.12739</a> [<a href="http://arxiv.org/pdf/2006.12739" target="_blank">pdf</a>]

<h2>Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks. (arXiv:2006.13782v2 [cs.CV] UPDATED)</h2>
<h3>Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin</h3>
<p>We present Neural Splines, a technique for 3D surface reconstruction that is
based on random feature kernels arising from infinitely-wide shallow ReLU
networks. Our method achieves state-of-the-art results, outperforming recent
neural network-based techniques and widely used Poisson Surface Reconstruction
(which, as we demonstrate, can also be viewed as a type of kernel method).
Because our approach is based on a simple kernel formulation, it is easy to
analyze and can be accelerated by general techniques designed for kernel-based
learning. We provide explicit analytical expressions for our kernel and argue
that our formulation can be seen as a generalization of cubic spline
interpolation to higher dimensions. In particular, the RKHS norm associated
with Neural Splines biases toward smooth interpolants.
</p>
<a href="http://arxiv.org/abs/2006.13782" target="_blank">arXiv:2006.13782</a> [<a href="http://arxiv.org/pdf/2006.13782" target="_blank">pdf</a>]

<h2>Can 3D Adversarial Logos Cloak Humans?. (arXiv:2006.14655v2 [cs.LG] UPDATED)</h2>
<h3>Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit Bajaj, Zhangyang Wang</h3>
<p>With the trend of adversarial attacks, researchers attempt to fool trained
object detectors in 2D scenes. Among many of them, an intriguing new form of
attack with potential real-world usage is to append adversarial patches (e.g.
logos) to images. Nevertheless, much less have we known about adversarial
attacks from 3D rendering views, which is essential for the attack to be
persistently strong in the physical world. This paper presents a new 3D
adversarial logo attack: we construct an arbitrary shape logo from a 2D texture
image and map this image into a 3D adversarial logo via a texture mapping
called logo transformation. The resulting 3D adversarial logo is then viewed as
an adversarial texture enabling easy manipulation of its shape and position.
This greatly extends the versatility of adversarial training for computer
graphics synthesized imagery. Contrary to the traditional adversarial patch,
this new form of attack is mapped into the 3D object world and back-propagates
to the 2D image domain through differentiable rendering. In addition, and
unlike existing adversarial patches, our new 3D adversarial logo is shown to
fool state-of-the-art deep object detectors robustly under model rotations,
leading to one step further for realistic attacks in the physical world. Our
codes are available at https://github.com/TAMU-VITA/3D_Adversarial_Logo.
</p>
<a href="http://arxiv.org/abs/2006.14655" target="_blank">arXiv:2006.14655</a> [<a href="http://arxiv.org/pdf/2006.14655" target="_blank">pdf</a>]

<h2>Psychological and Neural Evidence for Reinforcement Learning: A Survey. (arXiv:2007.01099v2 [cs.LG] UPDATED)</h2>
<h3>Ajay Subramanian, Sharad Chitlangia, Veeky Baths</h3>
<p>Reinforcement learning methods have been recently been very successful in
complex sequential tasks like playing Atari games, Go and Poker. Through
minimal input from humans, these algorithms can learn to perform complex tasks
from scratch, just through rewards obtained through interaction with their
environment. While there certainly has been considerable independent innovation
in the area to produce such results, many core ideas in RL are inspired by
animal learning, psychology and neuroscience phenomena. Moreover, these
algorithms are now helping advance neuroscience and cognitive science research
by serving as a computational model for many characteristic features of brain
functioning. In this paper, we review a number of key findings in neuroscience
and human behavior that provide evidence for the involvement of reinforcement
learning in human learning. Finally, we discuss how these findings have
provided inspiration for new RL algorithms.
</p>
<a href="http://arxiv.org/abs/2007.01099" target="_blank">arXiv:2007.01099</a> [<a href="http://arxiv.org/pdf/2007.01099" target="_blank">pdf</a>]

<h2>Joint Frequency and Image Space Learning for Fourier Imaging. (arXiv:2007.01441v2 [cs.CV] UPDATED)</h2>
<h3>Nalini M. Singh, Juan Eugenio Iglesias, Elfar Adalsteinsson, Adrian V. Dalca, Polina Golland</h3>
<p>We demonstrate that neural network layers that explicitly combine frequency
and image feature representations are a versatile building block for analysis
of imaging data acquired in the frequency space. Our work is motivated by the
challenges arising in MRI acquisition where the signal is a corrupted Fourier
transform of the desired image. The joint learning schemes proposed and
analyzed in this paper enable both correction of artifacts native to the
frequency space and manipulation of image space representations to reconstruct
coherent image structures. This is in contrast to most current deep learning
approaches for image reconstruction that apply learned data manipulations
solely in the frequency space or solely in the image space. We demonstrate the
advantages of joint convolutional learning on three diverse tasks: image
reconstruction from undersampled acquisitions, motion correction, and image
denoising in brain and knee MRI. We further demonstrate advantages of the joint
learning approaches across training schemes using a wide variety of loss
functions. Unlike purely image based and purely frequency based architectures,
the joint models produce consistently high quality output images across all
tasks and datasets. Joint image and frequency space feature representations
promise to significantly improve modeling and reconstruction of images acquired
in the frequency space. Our code is available at
https://github.com/nalinimsingh/interlacer.
</p>
<a href="http://arxiv.org/abs/2007.01441" target="_blank">arXiv:2007.01441</a> [<a href="http://arxiv.org/pdf/2007.01441" target="_blank">pdf</a>]

<h2>Rethinking Bottleneck Structure for Efficient Mobile Network Design. (arXiv:2007.02269v4 [cs.CV] UPDATED)</h2>
<h3>Zhou Daquan, Qibin Hou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan</h3>
<p>The inverted residual block is dominating architecture design for mobile
networks recently. It changes the classic residual bottleneck by introducing
two design rules: learning inverted residuals and using linear bottlenecks. In
this paper, we rethink the necessity of such design changes and find it may
bring risks of information loss and gradient confusion. We thus propose to flip
the structure and present a novel bottleneck design, called the sandglass
block, that performs identity mapping and spatial transformation at higher
dimensions and thus alleviates information loss and gradient confusion
effectively. Extensive experiments demonstrate that, different from the common
belief, such bottleneck structure is more beneficial than the inverted ones for
mobile networks. In ImageNet classification, by simply replacing the inverted
residual block with our sandglass block without increasing parameters and
computation, the classification accuracy can be improved by more than 1.7% over
MobileNetV2. On Pascal VOC 2007 test set, we observe that there is also 0.9%
mAP improvement in object detection. We further verify the effectiveness of the
sandglass block by adding it into the search space of neural architecture
search method DARTS. With 25% parameter reduction, the classification accuracy
is improved by 0.13% over previous DARTS models. Code can be found at:
https://github.com/zhoudaquan/rethinking_bottleneck_design.
</p>
<a href="http://arxiv.org/abs/2007.02269" target="_blank">arXiv:2007.02269</a> [<a href="http://arxiv.org/pdf/2007.02269" target="_blank">pdf</a>]

<h2>Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition. (arXiv:2008.03791v2 [cs.CV] UPDATED)</h2>
<h3>Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang</h3>
<p>Current methods for skeleton-based human action recognition usually work with
complete skeletons. However, in real scenarios, it is inevitable to capture
incomplete or noisy skeletons, which could significantly deteriorate the
performance of current methods when some informative joints are occluded or
disturbed. To improve the robustness of action recognition models, a
multi-stream graph convolutional network (GCN) is proposed to explore
sufficient discriminative features spreading over all skeleton joints, so that
the distributed redundant representation reduces the sensitivity of the action
models to non-standard skeletons. Concretely, the backbone GCN is extended by a
series of ordered streams which is responsible for learning discriminative
features from the joints less activated by preceding streams. Here, the
activation degrees of skeleton joints of each GCN stream are measured by the
class activation maps (CAM), and only the information from the unactivated
joints will be passed to the next stream, by which rich features over all
active joints are obtained. Thus, the proposed method is termed richly
activated GCN (RA-GCN). Compared to the state-of-the-art (SOTA) methods, the
RA-GCN achieves comparable performance on the standard NTU RGB+D 60 and 120
datasets. More crucially, on the synthetic occlusion and jittering datasets,
the performance deterioration due to the occluded and disturbed joints can be
significantly alleviated by utilizing the proposed RA-GCN.
</p>
<a href="http://arxiv.org/abs/2008.03791" target="_blank">arXiv:2008.03791</a> [<a href="http://arxiv.org/pdf/2008.03791" target="_blank">pdf</a>]

<h2>Grasping Field: Learning Implicit Representations for Human Grasps. (arXiv:2008.04451v3 [cs.CV] UPDATED)</h2>
<h3>Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang</h3>
<p>Robotic grasping of house-hold objects has made remarkable progress in recent
years. Yet, human grasps are still difficult to synthesize realistically. There
are several key reasons: (1) the human hand has many degrees of freedom (more
than robotic manipulators); (2) the synthesized hand should conform to the
surface of the object; and (3) it should interact with the object in a
semantically and physically plausible manner. To make progress in this
direction, we draw inspiration from the recent progress on learning-based
implicit representations for 3D object reconstruction. Specifically, we propose
an expressive representation for human grasp modelling that is efficient and
easy to integrate with deep neural networks. Our insight is that every point in
a three-dimensional space can be characterized by the signed distances to the
surface of the hand and the object, respectively. Consequently, the hand, the
object, and the contact area can be represented by implicit surfaces in a
common space, in which the proximity between the hand and the object can be
modelled explicitly. We name this 3D to 2D mapping as Grasping Field,
parameterize it with a deep neural network, and learn it from data. We
demonstrate that the proposed grasping field is an effective and expressive
representation for human grasp generation. Specifically, our generative model
is able to synthesize high-quality human grasps, given only on a 3D object
point cloud. The extensive experiments demonstrate that our generative model
compares favorably with a strong baseline and approaches the level of natural
human grasps. Our method improves the physical plausibility of the hand-object
contact reconstruction and achieves comparable performance for 3D hand
reconstruction compared to state-of-the-art methods.
</p>
<a href="http://arxiv.org/abs/2008.04451" target="_blank">arXiv:2008.04451</a> [<a href="http://arxiv.org/pdf/2008.04451" target="_blank">pdf</a>]

<h2>How to Put Users in Control of their Data in Federated Top-N Recommendation with Learning to Rank. (arXiv:2008.07192v2 [cs.LG] UPDATED)</h2>
<h3>Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara, Fedelucio Narducci</h3>
<p>Recommendation services are extensively adopted in several user-centered
applications as a tool to alleviate the information overload problem and help
users in orienteering in a vast space of possible choices. In such scenarios,
data ownership is a crucial concern since users may not be willing to share
their sensitive preferences (e.g., visited locations, read books, bought items)
with a central server. Unfortunately, data harvesting and collection is at the
basis of modern, state-of-the-art approaches to recommendation. To address this
issue, we present Federated Pair-wise Learning (FPL), an architecture in which
users collaborate in training a central factorization model while controlling
the amount of sensitive data leaving their devices. The proposed approach
implements pair-wise learning-to-rank optimization by following the Federated
Learning principles, conceived originally to mitigate the privacy risks of
traditional machine learning.
</p>
<a href="http://arxiv.org/abs/2008.07192" target="_blank">arXiv:2008.07192</a> [<a href="http://arxiv.org/pdf/2008.07192" target="_blank">pdf</a>]

<h2>Localizing Anomalies from Weakly-Labeled Videos. (arXiv:2008.08944v2 [cs.CV] UPDATED)</h2>
<h3>Hui Lv, Chuanwei Zhou, Chunyan Xu, Zhen Cui, Jian Yang</h3>
<p>Video anomaly detection under video-level labels is currently a challenging
task. Previous works have made progresses on discriminating whether a video
sequencecontains anomalies. However, most of them fail to accurately localize
the anomalous events within videos in the temporal domain. In this paper, we
propose a Weakly Supervised Anomaly Localization (WSAL) method focusing on
temporally localizing anomalous segments within anomalous videos. Inspired by
the appearance difference in anomalous videos, the evolution of adjacent
temporal segments is evaluated for the localization of anomalous segments. To
this end, a high-order context encoding model is proposed to not only extract
semantic representations but also measure the dynamic variations so that the
temporal context could be effectively utilized. In addition, in order to fully
utilize the spatial context information, the immediate semantics are directly
derived from the segment representations. The dynamic variations as well as the
immediate semantics, are efficiently aggregated to obtain the final anomaly
scores. An enhancement strategy is further proposed to deal with noise
interference and the absence of localization guidance in anomaly detection.
Moreover, to facilitate the diversity requirement for anomaly detection
benchmarks, we also collect a new traffic anomaly (TAD) dataset which specifies
in the traffic conditions, differing greatly from the current popular anomaly
detection evaluation benchmarks.Extensive experiments are conducted to verify
the effectiveness of different components, and our proposed method achieves new
state-of-the-art performance on the UCF-Crime and TAD datasets.
</p>
<a href="http://arxiv.org/abs/2008.08944" target="_blank">arXiv:2008.08944</a> [<a href="http://arxiv.org/pdf/2008.08944" target="_blank">pdf</a>]

<h2>HALO: Learning to Prune Neural Networks with Shrinkage. (arXiv:2008.10183v2 [cs.LG] UPDATED)</h2>
<h3>Skyler Seto, Martin T. Wells, Wenyu Zhang</h3>
<p>Deep neural networks achieve state-of-the-art performance in a variety of
tasks by extracting a rich set of features from unstructured data, however this
performance is closely tied to model size. Modern techniques for inducing
sparsity and reducing model size are (1) network pruning, (2) training with a
sparsity inducing penalty, and (3) training a binary mask jointly with the
weights of the network. We study different sparsity inducing penalties from the
perspective of Bayesian hierarchical models and present a novel penalty called
Hierarchical Adaptive Lasso (HALO) which learns to adaptively sparsify weights
of a given network via trainable parameters. When used to train
over-parametrized networks, our penalty yields small subnetworks with high
accuracy without fine-tuning. Empirically, on image recognition tasks, we find
that HALO is able to learn highly sparse network (only 5% of the parameters)
with significant gains in performance over state-of-the-art magnitude pruning
methods at the same level of sparsity. Code is available at
https://github.com/skyler120/sparsity-halo.
</p>
<a href="http://arxiv.org/abs/2008.10183" target="_blank">arXiv:2008.10183</a> [<a href="http://arxiv.org/pdf/2008.10183" target="_blank">pdf</a>]

<h2>A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research. (arXiv:2008.10244v2 [cs.CV] UPDATED)</h2>
<h3>Marc Aubreville, Christof A. Bertram, Taryn A. Donovan, Christian Marzahl, Andreas Maier, Robert Klopfleisch</h3>
<p>Canine mammary carcinoma (CMC) has been used as a model to investigate the
pathogenesis of human breast cancer and the same grading scheme is commonly
used to assess tumor malignancy in both. One key component of this grading
scheme is the density of mitotic figures (MF). Current publicly available
datasets on human breast cancer only provide annotations for small subsets of
whole slide images (WSIs). We present a novel dataset of 21 WSIs of CMC
completely annotated for MF. For this, a pathologist screened all WSIs for
potential MF and structures with a similar appearance. A second expert blindly
assigned labels, and for non-matching labels, a third expert assigned the final
labels. Additionally, we used machine learning to identify previously
undetected MF. Finally, we performed representation learning and
two-dimensional projection to further increase the consistency of the
annotations. Our dataset consists of 13,907 MF and 36,379 hard negatives. We
achieved a mean F1-score of 0.791 on the test set and of up to 0.696 on a human
breast cancer dataset.
</p>
<a href="http://arxiv.org/abs/2008.10244" target="_blank">arXiv:2008.10244</a> [<a href="http://arxiv.org/pdf/2008.10244" target="_blank">pdf</a>]

<h2>Deep Probabilistic Feature-metric Tracking. (arXiv:2008.13504v2 [cs.CV] UPDATED)</h2>
<h3>Binbin Xu, Andrew J. Davison, Stefan Leutenegger</h3>
<p>Dense image alignment from RGB-D images remains a critical issue for
real-world applications, especially under challenging lighting conditions and
in a wide baseline setting. In this paper, we propose a new framework to learn
a pixel-wise deep feature map and a deep feature-metric uncertainty map
predicted by a Convolutional Neural Network (CNN), which together formulate a
deep probabilistic feature-metric residual of the two-view constraint that can
be minimised using Gauss-Newton in a coarse-to-fine optimisation framework.
Furthermore, our network predicts a deep initial pose for faster and more
reliable convergence. The optimisation steps are differentiable and unrolled to
train in an end-to-end fashion. Due to its probabilistic essence, our approach
can easily couple with other residuals, where we show a combination with ICP.
Experimental results demonstrate state-of-the-art performances on the TUM RGB-D
dataset and the 3D rigid object tracking dataset. We further demonstrate our
method's robustness and convergence qualitatively.
</p>
<a href="http://arxiv.org/abs/2008.13504" target="_blank">arXiv:2008.13504</a> [<a href="http://arxiv.org/pdf/2008.13504" target="_blank">pdf</a>]

<h2>Adherent Mist and Raindrop Removal from a Single Image Using Attentive Convolutional Network. (arXiv:2009.01466v2 [cs.CV] UPDATED)</h2>
<h3>Da He, Xiaoyu Shang, Jiajia Luo</h3>
<p>Temperature difference-induced mist adhered to the glass, such as windshield,
camera lens, is often inhomogeneous and obscure, easily obstructing the vision
and severely degrading the image. Together with adherent raindrops, they bring
considerable challenges to various vision systems but without enough attention.
Recent methods for other similar problems typically use hand-crafted priors to
generate spatial attention maps. In this work, we newly present a problem of
image degradation caused by adherent mist and raindrops. An attentive
convolutional network is adopted to visually remove the adherent mist and
raindrop from a single image. A baseline architecture with general channel-wise
attention, spatial attention, and multilevel feature fusion is used.
Considering the variations and regional characteristics of adherent mist and
raindrops, we apply interpolation-based pyramid-attention blocks to perceive
spatial information at different scales. Experiments show that the proposed
method can improve severely degraded images' visibility, both qualitatively and
quantitatively. More applied experiments show that this underrated practical
problem is critical to high-level vision scenes. Our method also achieves
state-of-the-art performance on conventional dehazing and pure de-raindrop
problems, in addition to our task of handling adherent mist and raindrops.
</p>
<a href="http://arxiv.org/abs/2009.01466" target="_blank">arXiv:2009.01466</a> [<a href="http://arxiv.org/pdf/2009.01466" target="_blank">pdf</a>]

<h2>SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition. (arXiv:2009.06138v2 [cs.CV] UPDATED)</h2>
<h3>Liangzhi Li, Bowen Wang, Manisha Verma, Yuta Nakashima, Ryo Kawasaki, Hajime Nagahara</h3>
<p>Explainable artificial intelligence has been gaining attention in the past
few years. However, most existing methods are based on gradients or
intermediate features, which are not directly involved in the decision-making
process of the classifier. In this paper, we propose a slot attention-based
classifier called SCOUTER for transparent yet accurate classification. Two
major differences from other attention-based methods include: (a) SCOUTER's
explanation is involved in the final confidence for each category, offering
more intuitive interpretation, and (b) all the categories have their
corresponding positive or negative explanation, which tells "why the image is
of a certain category" or "why the image is not of a certain category." We
design a new loss tailored for SCOUTER that controls the model's behavior to
switch between positive and negative explanations, as well as the size of
explanatory regions. Experimental results show that SCOUTER can give better
visual explanations while keeping good accuracy on small and medium-sized
datasets.
</p>
<a href="http://arxiv.org/abs/2009.06138" target="_blank">arXiv:2009.06138</a> [<a href="http://arxiv.org/pdf/2009.06138" target="_blank">pdf</a>]

<h2>Collaborative Attention Mechanism for Multi-View Action Recognition. (arXiv:2009.06599v2 [cs.CV] UPDATED)</h2>
<h3>Yue Bai, Zhiqiang Tao, Lichen Wang, Sheng Li, Yu Yin, Yun Fu</h3>
<p>Multi-view action recognition (MVAR) leverages complementary temporal
information from different views to improve the learning performance. Obtaining
informative view-specific representation plays an essential role in MVAR.
Attention has been widely adopted as an effective strategy for discovering
discriminative cues underlying temporal data. However, most existing MVAR
methods only utilize attention to extract representation for each view
individually, ignoring the potential to dig latent patterns based on
mutual-support information in attention space. To this end, we propose a
collaborative attention mechanism (CAM) for solving the MVAR problem in this
paper. The proposed CAM detects the attention differences among multi-view, and
adaptively integrates frame-level information to benefit each other.
Specifically, we extend the long short-term memory (LSTM) to a Mutual-Aid RNN
(MAR) to achieve the multi-view collaboration process. CAM takes advantages of
view-specific attention pattern to guide another view and discover potential
information which is hard to be explored by itself. It paves a novel way to
leverage attention information and enhances the multi-view representation
learning. Extensive experiments on four action datasets illustrate the proposed
CAM achieves better results for each view and also boosts multi-view
performance.
</p>
<a href="http://arxiv.org/abs/2009.06599" target="_blank">arXiv:2009.06599</a> [<a href="http://arxiv.org/pdf/2009.06599" target="_blank">pdf</a>]

<h2>Domain-invariant Similarity Activation Map Metric Learning for Retrieval-based Long-term Visual Localization. (arXiv:2009.07719v3 [cs.CV] UPDATED)</h2>
<h3>Hanjiang Hu, Hesheng Wang, Zhe Liu, Weidong Chen</h3>
<p>Visual localization is a crucial component in the application of mobile robot
and autonomous driving. Image retrieval is an efficient and effective technique
in image-based localization methods. Due to the drastic variability of
environmental conditions, e.g. illumination, seasonal and weather changes,
retrieval-based visual localization is severely affected and becomes a
challenging problem. In this work, a general architecture is first formulated
probabilistically to extract domain-invariant feature through multi-domain
image translation. And then a novel gradient-weighted similarity activation
mapping loss (Grad-SAM) is incorporated for finer localization with high
accuracy. We also propose a new adaptive triplet loss to boost the metric
learning of the embedding in a self-supervised manner. The final coarse-to-fine
image retrieval pipeline is implemented as the sequential combination of models
without and with Grad-SAM loss. Extensive experiments have been conducted to
validate the effectiveness of the proposed approach on the CMU-Seasons dataset.
The strong generalization ability of our approach is verified on RobotCar
dataset using models pre-trained on urban part of CMU-Seasons dataset. Our
performance is on par with or even outperforms the state-of-the-art image-based
localization baselines in medium or high precision, especially under the
challenging environments with illumination variance, vegetation and night-time
images. Moreover, real-site experiments have been conducted to validate the
efficiency and effectiveness of the coarse-to-fine strategy for localization.
</p>
<a href="http://arxiv.org/abs/2009.07719" target="_blank">arXiv:2009.07719</a> [<a href="http://arxiv.org/pdf/2009.07719" target="_blank">pdf</a>]

<h2>Lucid Dreaming for Experience Replay: Refreshing Past States with the Current Policy. (arXiv:2009.13736v2 [cs.LG] UPDATED)</h2>
<h3>Yunshu Du, Garrett Warnell, Assefaw Gebremedhin, Peter Stone, Matthew E. Taylor</h3>
<p>Experience replay (ER) improves the data efficiency of off-policy
reinforcement learning (RL) algorithms by allowing an agent to store and reuse
its past experiences in a replay buffer. While many techniques have been
proposed to enhance ER by biasing how experiences are sampled from the buffer,
thus far they have not considered strategies for refreshing experiences inside
the buffer. In this work, we introduce Lucid Dreaming for Experience Replay
(LiDER), a conceptually new framework that allows replay experiences to be
refreshed by leveraging the agent's current policy. LiDER consists of three
steps: First, LiDER moves an agent back to a past state. Second, from that
state, LiDER then lets the agent execute a sequence of actions by following its
current policy---as if the agent were "dreaming" about the past and can try out
different behaviors to encounter new experiences in the dream. Third, LiDER
stores and reuses the new experience if it turned out better than what the
agent previously experienced, i.e., to refresh its memories. LiDER is designed
to be easily incorporated into off-policy, multi-worker RL algorithms that use
ER; we present in this work a case study of applying LiDER to an actor-critic
based algorithm. Results show LiDER consistently improves performance over the
baseline in six Atari 2600 games. Our open-source implementation of LiDER and
the data used to generate all plots in this work are available at
github.com/duyunshu/lucid-dreaming-for-exp-replay.
</p>
<a href="http://arxiv.org/abs/2009.13736" target="_blank">arXiv:2009.13736</a> [<a href="http://arxiv.org/pdf/2009.13736" target="_blank">pdf</a>]

<h2>Deep Learning-based Pipeline for Module Power Prediction from EL Measurements. (arXiv:2009.14712v2 [cs.CV] UPDATED)</h2>
<h3>Mathis Hoffmann, Claudia Buerhop-Lutz, Luca Reeb, Tobias Pickel, Thilo Winkler, Bernd Doll, Tobias W&#xfc;rfl, Ian Marius Peters, Christoph Brabec, Andreas Maier, Vincent Christlein</h3>
<p>Automated inspection plays an important role in monitoring large-scale
photovoltaic power plants. Commonly, electroluminescense measurements are used
to identify various types of defects on solar modules but have not been used to
determine the power of a module. However, knowledge of the power at maximum
power point is important as well, since drops in the power of a single module
can affect the performance of an entire string. By now, this is commonly
determined by measurements that require to discontact or even dismount the
module, rendering a regular inspection of individual modules infeasible. In
this work, we bridge the gap between electroluminescense measurements and the
power determination of a module. We compile a large dataset of 719
electroluminescense measurementsof modules at various stages of degradation,
especially cell cracks and fractures, and the corresponding power at maximum
power point. Here,we focus on inactive regions and cracks as the predominant
type of defect. We set up a baseline regression model to predict the power from
electroluminescense measurements with a mean absolute error of 9.0+/-3.7$W_P$
(4.0+/-8.4%). Then, we show that deep-learning can be used to train a model
that performs significantly better (7.3+/-2.7$W_P$ or 3.2+/-6.5%) and propose a
variant of class activation maps to obtain the per cell power loss, as
predicted by the model. With this work, we aim to open a new research topic.
Therefore, we publicly release the dataset, the code and trained models to
empower other researchers to compare against our results. Finally, we present a
thorough evaluation of certain boundary conditions like the dataset size and an
automated preprocessing pipeline for on-site measurements showing multiple
modules at once.
</p>
<a href="http://arxiv.org/abs/2009.14712" target="_blank">arXiv:2009.14712</a> [<a href="http://arxiv.org/pdf/2009.14712" target="_blank">pdf</a>]

<h2>Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization. (arXiv:2010.01112v2 [cs.LG] UPDATED)</h2>
<h3>Lanqing Li, Rui Yang, Dijun Luo</h3>
<p>We study the offline meta-reinforcement learning (OMRL) problem, a paradigm
which enables reinforcement learning (RL) algorithms to quickly adapt to unseen
tasks without any interactions with the environments, making RL truly practical
in many real-world applications. This problem is still not fully understood,
for which two major challenges need to be addressed. First, offline RL usually
suffers from bootstrapping errors of out-of-distribution state-actions which
leads to divergence of value functions. Second, meta-RL requires efficient and
robust task inference learned jointly with control policy. In this work, we
enforce behavior regularization on learned policy as a general approach to
offline RL, combined with a deterministic context encoder for efficient task
inference. We propose a novel negative-power distance metric on bounded context
embedding space, whose gradients propagation is detached from the Bellman
backup. We provide analysis and insight showing that some simple design choices
can yield substantial improvements over recent approaches involving meta-RL and
distance metric learning. To the best of our knowledge, our method is the first
model-free and end-to-end OMRL algorithm, which is computationally efficient
and demonstrated to outperform prior algorithms on several meta-RL benchmarks.
</p>
<a href="http://arxiv.org/abs/2010.01112" target="_blank">arXiv:2010.01112</a> [<a href="http://arxiv.org/pdf/2010.01112" target="_blank">pdf</a>]

<h2>Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v5 [cs.AI] UPDATED)</h2>
<h3>Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, Xiaodan Zhu</h3>
<p>Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.
</p>
<a href="http://arxiv.org/abs/2010.03084" target="_blank">arXiv:2010.03084</a> [<a href="http://arxiv.org/pdf/2010.03084" target="_blank">pdf</a>]

<h2>Balancing Constraints and Rewards with Meta-Gradient D4PG. (arXiv:2010.06324v2 [cs.LG] UPDATED)</h2>
<h3>Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, Timothy Mann</h3>
<p>Deploying Reinforcement Learning (RL) agents to solve real-world applications
often requires satisfying complex system constraints. Often the constraint
thresholds are incorrectly set due to the complex nature of a system or the
inability to verify the thresholds offline (e.g, no simulator or reasonable
offline evaluation procedure exists). This results in solutions where a task
cannot be solved without violating the constraints. However, in many real-world
cases, constraint violations are undesirable yet they are not catastrophic,
motivating the need for soft-constrained RL approaches. We present a
soft-constrained RL approach that utilizes meta-gradients to find a good
trade-off between expected return and minimizing constraint violations. We
demonstrate the effectiveness of this approach by showing that it consistently
outperforms the baselines across four different MuJoCo domains.
</p>
<a href="http://arxiv.org/abs/2010.06324" target="_blank">arXiv:2010.06324</a> [<a href="http://arxiv.org/pdf/2010.06324" target="_blank">pdf</a>]

<h2>Inductive Bias of Gradient Descent for Exponentially Weight Normalized Smooth Homogeneous Neural Nets. (arXiv:2010.12909v2 [cs.LG] UPDATED)</h2>
<h3>Depen Morwani, Harish G. Ramaswamy</h3>
<p>We analyze the inductive bias of gradient descent for weight normalized
smooth homogeneous neural nets, when trained on exponential or cross-entropy
loss. Our analysis focuses on exponential weight normalization (EWN), which
encourages weight updates along the radial direction. This paper shows that the
gradient flow path with EWN is equivalent to gradient flow on standard networks
with an adaptive learning rate, and hence causes the weights to be updated in a
way that prefers asymptotic relative sparsity. These results can be extended to
hold for gradient descent via an appropriate adaptive learning rate. The
asymptotic convergence rate of the loss in this setting is given by
$\Theta(\frac{1}{t(\log t)^2})$, and is independent of the depth of the
network. We contrast these results with the inductive bias of standard weight
normalization (SWN) and unnormalized architectures, and demonstrate their
implications on synthetic data sets.Experimental results on simple data sets
and architectures support our claim on sparse EWN solutions, even with SGD.
This demonstrates its potential applications in learning prunable neural
networks.
</p>
<a href="http://arxiv.org/abs/2010.12909" target="_blank">arXiv:2010.12909</a> [<a href="http://arxiv.org/pdf/2010.12909" target="_blank">pdf</a>]

<h2>System Identification via Meta-Learning in Linear Time-Varying Environments. (arXiv:2010.14664v2 [cs.LG] UPDATED)</h2>
<h3>Sen Lin, Hang Wang, Junshan Zhang</h3>
<p>System identification is a fundamental problem in reinforcement learning,
control theory and signal processing, and the non-asymptotic analysis of the
corresponding sample complexity is challenging and elusive, even for linear
time-varying (LTV) systems. To tackle this challenge, we develop an episodic
block model for the LTV system where the model parameters remain constant
within each block but change from block to block. Based on the observation that
the model parameters across different blocks are related, we treat each
episodic block as a learning task and then run meta-learning over many blocks
for system identification, using two steps, namely offline meta-learning and
online adaptation. We carry out a comprehensive non-asymptotic analysis of the
performance of meta-learning based system identification. To deal with the
technical challenges rooted in the sample correlation and small sample sizes in
each block, we devise a new two-scale martingale small-ball approach for
offline meta-learning, for arbitrary model correlation structure across blocks.
We then quantify the finite time error of online adaptation by leveraging
recent advances in linear stochastic approximation with correlated samples.
</p>
<a href="http://arxiv.org/abs/2010.14664" target="_blank">arXiv:2010.14664</a> [<a href="http://arxiv.org/pdf/2010.14664" target="_blank">pdf</a>]

<h2>Dynamically Feasible Deep Reinforcement Learning Policy for Robot Navigation in Dense Mobile Crowds. (arXiv:2010.14838v3 [cs.RO] UPDATED)</h2>
<h3>Utsav Patel, Nithish Kumar, Adarsh Jagan Sathyamoorthy, Dinesh Manocha</h3>
<p>We present a novel Deep Reinforcement Learning (DRL) based policy to compute
dynamically feasible and spatially aware velocities for a robot navigating
among mobile obstacles. Our approach combines the benefits of the Dynamic
Window Approach (DWA) in terms of satisfying the robot's dynamics constraints
with state-of-the-art DRL-based navigation methods that can handle moving
obstacles and pedestrians well. Our formulation achieves these goals by
embedding the environmental obstacles' motions in a novel low-dimensional
observation space. It also uses a novel reward function to positively reinforce
velocities that move the robot away from the obstacle's heading direction
leading to significantly lower number of collisions. We evaluate our method in
realistic 3-D simulated environments and on a real differential drive robot in
challenging dense indoor scenarios with several walking pedestrians. We compare
our method with state-of-the-art collision avoidance methods and observe
significant improvements in terms of success rate (up to 33\% increase), number
of dynamics constraint violations (up to 61\% decrease), and smoothness. We
also conduct ablation studies to highlight the advantages of our observation
space formulation, and reward structure.
</p>
<a href="http://arxiv.org/abs/2010.14838" target="_blank">arXiv:2010.14838</a> [<a href="http://arxiv.org/pdf/2010.14838" target="_blank">pdf</a>]

<h2>PAL : Pretext-based Active Learning. (arXiv:2010.15947v2 [cs.CV] UPDATED)</h2>
<h3>Shubhang Bhatnagar, Sachin Goyal, Darshan Tank, Amit Sethi</h3>
<p>The goal of active learning algorithms is to judiciously select subsets of
unlabeled samples to be labeled by an oracle, in order to reduce the time and
cost associated with supervised learning. Previously, active learning
techniques for deep neural networks have used the same network for the task at
hand (e.g., classification) as well as sample selection, which can be
conflicting goals. To address this issue, we use a separate sample scoring
network to capture the relevant information about the distribution of the
labeled samples, and use it to assess the novelty of unlabeled samples.
Specifically, we propose to efficiently train the scoring network using a
self-supervised learning (pretext) task on the labeled samples. To make the
scoring network more robust, we added to it another head, which is trained
using the supervised (task) objective itself. The scoring network was paired
with a scoring function that allows an appropriate trade-off between the two
heads. We also ensure that the selected samples are diverse by selectively
fine-tuning the scoring network in sub-rounds of each query round. The
resulting scheme performs competitively with the state-of-the-art on benchmark
datasets. More importantly, in realistic scenarios when some labels are
erroneous and new classes are introduced on the fly, the performance of the
proposed method remains strong.
</p>
<a href="http://arxiv.org/abs/2010.15947" target="_blank">arXiv:2010.15947</a> [<a href="http://arxiv.org/pdf/2010.15947" target="_blank">pdf</a>]

<h2>FusiformNet: Extracting Discriminative Facial Features on Different Levels. (arXiv:2011.00577v3 [cs.CV] UPDATED)</h2>
<h3>Kyo Takano</h3>
<p>Over the last several years, research on facial recognition based on Deep
Neural Network has evolved with approaches like task-specific loss functions,
image normalization and augmentation, network architectures, etc. However,
there have been few approaches with attention to how human faces differ from
person to person. Premising that inter-personal differences are found both
generally and locally on the human face, I propose FusiformNet, a novel
framework for feature extraction that leverages the nature of discriminative
facial features. Tested on Image-Unrestricted setting of Labeled Faces in the
Wild benchmark, this method achieved a state-of-the-art accuracy of 96.67%
without labeled outside data, image augmentation, normalization, or special
loss functions. Likewise, the method also performed on a par with previous
state-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its
ability to extract both general and local facial features, the utility of
FusiformNet may not be limited to facial recognition but also extend to other
DNN-based tasks.
</p>
<a href="http://arxiv.org/abs/2011.00577" target="_blank">arXiv:2011.00577</a> [<a href="http://arxiv.org/pdf/2011.00577" target="_blank">pdf</a>]

<h2>Adversarial training for predictive tasks: theoretical analysis and limitations in the deterministic case. (arXiv:2011.00835v2 [stat.ML] UPDATED)</h2>
<h3>Thibault Lesieur, J&#xe9;r&#xe9;mie Messud, Issa Hammoud, Hanyuan Peng, C&#xe9;line Lacombe, Paulien Jeunesse</h3>
<p>To train a deep neural network to mimic the outcomes of processing sequences,
a version of Conditional Generalized Adversarial Network (CGAN) can be used. It
has been observed by others that CGAN can help to improve the results even for
deterministic sequences, where only one output is associated with the
processing of a given input. Surprisingly, our CGAN-based tests on
deterministic geophysical processing sequences did not produce a real
improvement compared to the use of an $L_p$ loss; we here propose a first
theoretical explanation why. Our analysis goes from the non-deterministic case
to the deterministic one. It led us to develop an adversarial way to train a
content loss that gave better results on our data.
</p>
<a href="http://arxiv.org/abs/2011.00835" target="_blank">arXiv:2011.00835</a> [<a href="http://arxiv.org/pdf/2011.00835" target="_blank">pdf</a>]

<h2>VEGA: Towards an End-to-End Configurable AutoML Pipeline. (arXiv:2011.01507v4 [cs.CV] UPDATED)</h2>
<h3>Bochao Wang, Hang Xu, Jiajin Zhang, Chen Chen, Xiaozhi Fang, Yixing Xu, Ning Kang, Lanqing Hong, Chenhan Jiang, Xinyue Cai, Jiawei Li, Fengwei Zhou, Yong Li, Zhicheng Liu, Xinghao Chen, Kai Han, Han Shu, Dehua Song, Yunhe Wang, Wei Zhang, Chunjing Xu, Zhenguo Li, Wenzhi Liu, Tong Zhang</h3>
<p>Automated Machine Learning (AutoML) is an important industrial solution for
automatic discovery and deployment of the machine learning models. However,
designing an integrated AutoML system faces four great challenges of
configurability, scalability, integrability, and platform diversity. In this
work, we present VEGA, an efficient and comprehensive AutoML framework that is
compatible and optimized for multiple hardware platforms. a) The VEGA pipeline
integrates various modules of AutoML, including Neural Architecture Search
(NAS), Hyperparameter Optimization (HPO), Auto Data Augmentation, Model
Compression, and Fully Train. b) To support a variety of search algorithms and
tasks, we design a novel fine-grained search space and its description language
to enable easy adaptation to different search algorithms and tasks. c) We
abstract the common components of deep learning frameworks into a unified
interface. VEGA can be executed with multiple back-ends and hardwares.
Extensive benchmark experiments on multiple tasks demonstrate that VEGA can
improve the existing AutoML algorithms and discover new high-performance models
against SOTA methods, e.g. the searched DNet model zoo for Ascend 10x faster
than EfficientNet-B5 and 9.2x faster than RegNetX-32GF on ImageNet. VEGA is
open-sourced at https://github.com/huawei-noah/vega.
</p>
<a href="http://arxiv.org/abs/2011.01507" target="_blank">arXiv:2011.01507</a> [<a href="http://arxiv.org/pdf/2011.01507" target="_blank">pdf</a>]

<h2>Generating Unobserved Alternatives. (arXiv:2011.01926v3 [cs.LG] UPDATED)</h2>
<h3>Shichong Peng, Ke Li</h3>
<p>We consider problems where multiple predictions can be considered correct,
but only one of them is given as supervision. This setting differs from both
the regression and class-conditional generative modelling settings: in the
former, there is a unique observed output for each input, which is provided as
supervision; in the latter, there are many observed outputs for each input, and
many are provided as supervision. Applying either regression methods and
conditional generative models to the present setting often results in a model
that can only make a single prediction for each input. We explore several
problems that have this property and develop an approach that can generate
multiple high-quality predictions given the same input. As a result, it can be
used to generate high-quality outputs that are different from the observed
output.
</p>
<a href="http://arxiv.org/abs/2011.01926" target="_blank">arXiv:2011.01926</a> [<a href="http://arxiv.org/pdf/2011.01926" target="_blank">pdf</a>]

<h2>Center-wise Local Image Mixture For Contrastive Representation Learning. (arXiv:2011.02697v2 [cs.CV] UPDATED)</h2>
<h3>Hao Li, Xiaopeng Zhang, Ruoyu Sun, Hongkai Xiong, Qi Tian</h3>
<p>Recent advances in unsupervised representation learning have experienced
remarkable progress, especially with the achievements of contrastive learning,
which regards each image as well its augmentations as a separate class, while
does not consider the semantic similarity among images. This paper proposes a
new kind of data augmentation, named Center-wise Local Image Mixture, to expand
the neighborhood space of an image. CLIM encourages both local similarity and
global aggregation while pulling similar images. This is achieved by searching
local similar samples of an image, and only selecting images that are closer to
the corresponding cluster center, which we denote as center-wise local
selection. As a result, similar representations are progressively approaching
the clusters, while do not break the local similarity. Furthermore, image
mixture is used as a smoothing regularization to avoid overconfidence on the
selected samples. Besides, we introduce multi-resolution augmentation, which
enables the representation to be scale invariant. Integrating the two
augmentations produces better feature representation on several unsupervised
benchmarks. Notably, we reach 75.5% top-1 accuracy with linear evaluation over
ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels, as
well as consistently outperforming supervised pretraining on several downstream
transfer tasks.
</p>
<a href="http://arxiv.org/abs/2011.02697" target="_blank">arXiv:2011.02697</a> [<a href="http://arxiv.org/pdf/2011.02697" target="_blank">pdf</a>]

<h2>Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery. (arXiv:2011.03247v4 [cs.CV] UPDATED)</h2>
<h3>Shiqi Tian, Zhuo Zheng, Ailong Ma, Yanfei Zhong</h3>
<p>With the acceleration of the urban expansion, urban change detection (UCD),
as a significant and effective approach, can provide the change information
with respect to geospatial objects for dynamical urban analysis. However,
existing datasets suffer from three bottlenecks: (1) lack of high spatial
resolution images; (2) lack of semantic annotation; (3) lack of long-range
multi-temporal images. In this paper, we propose a large scale benchmark
dataset, termed Hi-UCD. This dataset uses aerial images with a spatial
resolution of 0.1 m provided by the Estonia Land Board, including three-time
phases, and semantically annotated with nine classes of land cover to obtain
the direction of ground objects change. It can be used for detecting and
analyzing refined urban changes. We benchmark our dataset using some classic
methods in binary and multi-class change detection. Experimental results show
that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong
benchmark accelerating future research.
</p>
<a href="http://arxiv.org/abs/2011.03247" target="_blank">arXiv:2011.03247</a> [<a href="http://arxiv.org/pdf/2011.03247" target="_blank">pdf</a>]

<h2>Reactive motion planning with probabilistic safety guarantees. (arXiv:2011.03590v2 [cs.RO] UPDATED)</h2>
<h3>Yuxiao Chen, Ugo Rosolia, Chuchu Fan, Aaron D. Ames, Richard Murray</h3>
<p>Motion planning in environments with multiple agents is critical to many
important autonomous applications such as autonomous vehicles and assistive
robots. This paper considers the problem of motion planning, where the
controlled agent shares the environment with multiple uncontrolled agents.
First, a predictive model of the uncontrolled agents is trained to predict all
possible trajectories within a short horizon based on the scenario. The
prediction is then fed to a motion planning module based on model predictive
control. We proved generalization bound for the predictive model using three
different methods, post-bloating, support vector machine (SVM), and conformal
analysis, all capable of generating stochastic guarantees of the correctness of
the predictor. The proposed approach is demonstrated in simulation in a
scenario emulating autonomous highway driving.
</p>
<a href="http://arxiv.org/abs/2011.03590" target="_blank">arXiv:2011.03590</a> [<a href="http://arxiv.org/pdf/2011.03590" target="_blank">pdf</a>]

<h2>Curse of Small Sample Size in Forecasting of the Active Cases in COVID-19 Outbreak. (arXiv:2011.03628v2 [cs.LG] UPDATED)</h2>
<h3>Mert Nak&#x131;p, Onur &#xc7;opur, C&#xfc;neyt G&#xfc;zeli&#x15f;</h3>
<p>During the COVID-19 pandemic, a massive number of attempts on the predictions
of the number of cases and the other future trends of this pandemic have been
made. However, they fail to predict, in a reliable way, the medium and long
term evolution of fundamental features of COVID-19 outbreak within acceptable
accuracy. This paper gives an explanation for the failure of machine learning
models in this particular forecasting problem. The paper shows that simple
linear regression models provide high prediction accuracy values reliably but
only for a 2-weeks period and that relatively complex machine learning models,
which have the potential of learning long term predictions with low errors,
cannot achieve to obtain good predictions with possessing a high generalization
ability. It is suggested in the paper that the lack of a sufficient number of
samples is the source of low prediction performance of the forecasting models.
The reliability of the forecasting results about the active cases is measured
in terms of the cross-validation prediction errors, which are used as
expectations for the generalization errors of the forecasters. To exploit the
information, which is of most relevant with the active cases, we perform
feature selection over a variety of variables. We apply different feature
selection methods, namely the Pairwise Correlation, Recursive Feature
Selection, and feature selection by using the Lasso regression and compare them
to each other and also with the models not employing any feature selection.
Furthermore, we compare Linear Regression, Multi-Layer Perceptron, and
Long-Short Term Memory models each of which is used for prediction active cases
together with the mentioned feature selection methods. Our results show that
the accurate forecasting of the active cases with high generalization ability
is possible up to 3 days only because of the small sample size of COVID-19
data.
</p>
<a href="http://arxiv.org/abs/2011.03628" target="_blank">arXiv:2011.03628</a> [<a href="http://arxiv.org/pdf/2011.03628" target="_blank">pdf</a>]

<h2>Quantifying and Learning Disentangled Representations with Limited Supervision. (arXiv:2011.06070v2 [cs.LG] UPDATED)</h2>
<h3>Loek Tonnaer, Luis A. P&#xe9;rez Rey, Vlado Menkovski, Mike Holenderski, Jacobus W. Portegies</h3>
<p>Learning low-dimensional representations that disentangle the underlying
factors of variation in data has been posited as an important step towards
interpretable machine learning with good generalization. To address the fact
that there is no consensus on what disentanglement entails, Higgins et al.
(2018) propose a formal definition for Linear Symmetry-Based Disentanglement,
or LSBD, arguing that underlying real-world transformations give exploitable
structure to data.

Although several works focus on learning LSBD representations, such methods
require supervision on the underlying transformations for the entire dataset,
and cannot deal with unlabeled data. Moreover, none of these works provide a
metric to quantify LSBD.

We propose a metric to quantify LSBD representations that is easy to compute
under certain well-defined assumptions. Furthermore, we present a method that
can leverage unlabeled data, such that LSBD representations can be learned with
limited supervision on transformations. Using our LSBD metric, our results show
that limited supervision is indeed sufficient to learn LSBD representations.
</p>
<a href="http://arxiv.org/abs/2011.06070" target="_blank">arXiv:2011.06070</a> [<a href="http://arxiv.org/pdf/2011.06070" target="_blank">pdf</a>]

<h2>Towards Optimal Problem Dependent Generalization Error Bounds in Statistical Learning Theory. (arXiv:2011.06186v2 [stat.ML] UPDATED)</h2>
<h3>Yunbei Xu, Assaf Zeevi</h3>
<p>We study problem-dependent rates, i.e., generalization errors that scale
near-optimally with the variance, the effective loss, or the gradient norms
evaluated at the "best hypothesis." We introduce a principled framework dubbed
"uniform localized convergence," and characterize sharp problem-dependent rates
for central statistical learning problems. From a methodological viewpoint, our
framework resolves several fundamental limitations of existing uniform
convergence and localization analysis approaches. It also provides improvements
and some level of unification in the study of localized complexities, one-sided
uniform inequalities, and sample-based iterative algorithms. In the so-called
"slow rate" regime, we provides the first (moment-penalized) estimator that
achieves the optimal variance-dependent rate for general "rich" classes; we
also establish improved loss-dependent rate for standard empirical risk
minimization. In the "fast rate" regime, we establish finite-sample
problem-dependent bounds that are comparable to precise asymptotics. In
addition, we show that efficient algorithms like gradient descent and
first-order Expectation-Maximization can achieve optimal generalization error
in several representative problems across the areas of non-convex learning,
stochastic optimization, and learning with missing data.
</p>
<a href="http://arxiv.org/abs/2011.06186" target="_blank">arXiv:2011.06186</a> [<a href="http://arxiv.org/pdf/2011.06186" target="_blank">pdf</a>]

<h2>DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer. (arXiv:2011.07589v2 [cs.CV] UPDATED)</h2>
<h3>Ajay Kumar Tanwani</h3>
<p>Generating large-scale synthetic data in simulation is a feasible alternative
to collecting/labelling real data for training vision-based deep learning
models, albeit the modelling inaccuracies do not generalize to the physical
world. In this paper, we present a domain-invariant representation learning
(DIRL) algorithm to adapt deep models to the physical environment with a small
amount of real data. Existing approaches that only mitigate the covariate shift
by aligning the marginal distributions across the domains and assume the
conditional distributions to be domain-invariant can lead to ambiguous transfer
in real scenarios. We propose to jointly align the marginal (input domains) and
the conditional (output labels) distributions to mitigate the covariate and the
conditional shift across the domains with adversarial learning, and combine it
with a triplet distribution loss to make the conditional distributions disjoint
in the shared feature space. Experiments on digit domains yield
state-of-the-art performance on challenging benchmarks, while sim-to-real
transfer of object recognition for vision-based decluttering with a mobile
robot improves from 26.8 % to 91.0 %, resulting in 86.5 % grasping accuracy of
a wide variety of objects. Code and supplementary details are available at
https://sites.google.com/view/dirl
</p>
<a href="http://arxiv.org/abs/2011.07589" target="_blank">arXiv:2011.07589</a> [<a href="http://arxiv.org/pdf/2011.07589" target="_blank">pdf</a>]

<h2>Domain Adaptive Knowledge Distillation for Driving Scene Semantic Segmentation. (arXiv:2011.08007v2 [cs.CV] UPDATED)</h2>
<h3>Divya Kothandaraman, Athira Nambiar, Anurag Mittal</h3>
<p>Practical autonomous driving systems face two crucial challenges: memory
constraints and domain gap issues. In this paper, we present a novel approach
to learn domain adaptive knowledge in models with limited memory, thus
bestowing the model with the ability to deal with these issues in a
comprehensive manner. We term this as "Domain Adaptive Knowledge Distillation"
and address the same in the context of unsupervised domain-adaptive semantic
segmentation by proposing a multi-level distillation strategy to effectively
distil knowledge at different levels. Further, we introduce a novel cross
entropy loss that leverages pseudo labels from the teacher. These pseudo
teacher labels play a multifaceted role towards: (i) knowledge distillation
from the teacher network to the student network &amp; (ii) serving as a proxy for
the ground truth for target domain images, where the problem is completely
unsupervised. We introduce four paradigms for distilling domain adaptive
knowledge and carry out extensive experiments and ablation studies on
real-to-real as well as synthetic-to-real scenarios. Our experiments
demonstrate the profound success of our proposed method.
</p>
<a href="http://arxiv.org/abs/2011.08007" target="_blank">arXiv:2011.08007</a> [<a href="http://arxiv.org/pdf/2011.08007" target="_blank">pdf</a>]

<h2>Towards Map-Based Validation of Semantic Segmentation Masks. (arXiv:2011.08008v2 [cs.CV] UPDATED)</h2>
<h3>Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Christian Bauckhage</h3>
<p>Artificial intelligence for autonomous driving must meet strict requirements
on safety and robustness. We propose to validate machine learning models for
self-driving vehicles not only with given ground truth labels, but also with
additional a-priori knowledge. In particular, we suggest to validate the
drivable area in semantic segmentation masks using given street map data. We
present first results, which indicate that prediction errors can be uncovered
by map-based validation.
</p>
<a href="http://arxiv.org/abs/2011.08008" target="_blank">arXiv:2011.08008</a> [<a href="http://arxiv.org/pdf/2011.08008" target="_blank">pdf</a>]

<h2>Combining GANs and AutoEncoders for Efficient Anomaly Detection. (arXiv:2011.08102v2 [cs.CV] UPDATED)</h2>
<h3>Fabio Carrara (1), Giuseppe Amato (1), Luca Brombin, Fabrizio Falchi (1), Claudio Gennaro (1) ((1) ISTI CNR, Pisa, Italy)</h3>
<p>In this work, we propose CBiGAN -- a novel method for anomaly detection in
images, where a consistency constraint is introduced as a regularization term
in both the encoder and decoder of a BiGAN. Our model exhibits fairly good
modeling power and reconstruction consistency capability. We evaluate the
proposed method on MVTec AD -- a real-world benchmark for unsupervised anomaly
detection on high-resolution images -- and compare against standard baselines
and state-of-the-art approaches. Experiments show that the proposed method
improves the performance of BiGAN formulations by a large margin and performs
comparably to expensive state-of-the-art iterative methods while reducing the
computational cost. We also observe that our model is particularly effective in
texture-type anomaly detection, as it sets a new state of the art in this
category. Our code is available at https://github.com/fabiocarrara/cbigan-ad/.
</p>
<a href="http://arxiv.org/abs/2011.08102" target="_blank">arXiv:2011.08102</a> [<a href="http://arxiv.org/pdf/2011.08102" target="_blank">pdf</a>]

<h2>Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video. (arXiv:2011.08627v2 [cs.CV] UPDATED)</h2>
<h3>Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee</h3>
<p>Despite the recent success of single image-based 3D human pose and shape
estimation methods, recovering temporally consistent and smooth 3D human motion
from a video is still challenging. Several video-based methods have been
proposed; however, they fail to resolve the single image-based methods'
temporal inconsistency issue due to a strong dependency on a static feature of
the current frame. In this regard, we present a temporally consistent mesh
recovery system (TCMR). It effectively focuses on the past and future frames'
temporal information without being dominated by the current static feature. Our
TCMR significantly outperforms previous video-based methods in temporal
consistency with better per-frame 3D pose and shape accuracy. We will release
the codes. Demo video:
https://www.youtube.com/watch?v=WB3nTnSQDII&amp;t=7s&amp;ab_channel=%EC%B5%9C%ED%99%8D%EC%84%9D
</p>
<a href="http://arxiv.org/abs/2011.08627" target="_blank">arXiv:2011.08627</a> [<a href="http://arxiv.org/pdf/2011.08627" target="_blank">pdf</a>]

<h2>Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video. (arXiv:2011.09846v4 [cs.CV] UPDATED)</h2>
<h3>Ben Saunders, Necati Cihan Camgoz, Richard Bowden</h3>
<p>To be truly understandable and accepted by Deaf communities, an automatic
Sign Language Production (SLP) system must generate a photo-realistic signer.
Prior approaches based on graphical avatars have proven unpopular, whereas
recent neural SLP works that produce skeleton pose sequences have been shown to
be not understandable to Deaf viewers.

In this paper, we propose SignGAN, the first SLP model to produce
photo-realistic continuous sign language videos directly from spoken language.
We employ a transformer architecture with a Mixture Density Network (MDN)
formulation to handle the translation from spoken language to skeletal pose. A
pose-conditioned human synthesis model is then introduced to generate a
photo-realistic sign language video from the skeletal pose sequence. This
allows the photo-realistic production of sign videos directly translated from
written text.

We further propose a novel keypoint-based loss function, which significantly
improves the quality of synthesized hand images, operating in the keypoint
space to avoid issues caused by motion blur. In addition, we introduce a method
for controllable video generation, enabling training on large, diverse sign
language datasets and providing the ability to control the signer appearance at
inference.

Using a dataset of eight different sign language interpreters extracted from
broadcast footage, we show that SignGAN significantly outperforms all baseline
methods for quantitative metrics and human perceptual studies.
</p>
<a href="http://arxiv.org/abs/2011.09846" target="_blank">arXiv:2011.09846</a> [<a href="http://arxiv.org/pdf/2011.09846" target="_blank">pdf</a>]

<h2>LAGNet: Logic-Aware Graph Network for Human Interaction Understanding. (arXiv:2011.10250v2 [cs.CV] UPDATED)</h2>
<h3>Zhenhua Wang, Jiajun Meng, Jin Zhou, Dongyan Guo, Guosheng Lin, Jianhua Zhang, Javen Qinfeng Shi, Shengyong Chen</h3>
<p>Compared with the progress made on human activity classification, much less
success has been achieved on human interaction understanding (HIU). Apart from
the latter task is much more challenging, the main cause is that recent
approaches learn human interactive relations via shallow graphical
representations, which is inadequate to model complicated human interactions.
In this paper, we propose a deep logic-aware graph network, which combines the
representative ability of graph attention and the rigorousness of logical
reasoning to facilitate human interaction understanding. Our network consists
of three components, a backbone CNN to extract image features, a graph network
to learn interactive relations among participants, and a logic-aware reasoning
module. Our key observation is that the first-order logic for HIU can be
embedded into higher-order energy functions, minimizing which delivers
logic-aware predictions. An efficient mean-field inference algorithm is
proposed, such that all modules of our network could be trained jointly in an
end-to-end way. Experimental results show that our approach achieves leading
performance on three existing benchmarks and a new challenging dataset crafted
by ourselves. Code is available at: https://git.io/LAGNet.
</p>
<a href="http://arxiv.org/abs/2011.10250" target="_blank">arXiv:2011.10250</a> [<a href="http://arxiv.org/pdf/2011.10250" target="_blank">pdf</a>]

<h2>PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v2 [cs.LG] UPDATED)</h2>
<h3>Neus Llop Torrent (1 and 2), Giorgio Visani (2 and 3), Enrico Bagli (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics and Engineering)</h3>
<p>The aim of this paper is to develop and test advanced analytical methods to
improve the prediction accuracy of Credit Risk Models, preserving at the same
time the model interpretability. In particular, the project focuses on applying
an explainable machine learning model to PSD2-related databases. The input data
were obtained solely from synthetic account transactions generated from a pool
of commercial banks from a pool of Italian commercial banks. Over the total
proven models, CatBoost has shown the highest performance. The algorithm
implementation produces a GINI of 0.45 after tuning the hyper-parameters
combined with their inherent class-weight resampling method. SHAP package is
used to provide a global and local interpretation of the model predictions to
formulate a human-comprehensive approach to understanding the decision-maker
algorithm. The 20 most important features are selected using the Shapley values
to present a full human-understandable model that reveals how the attributes of
an individual are related to its model prediction.
</p>
<a href="http://arxiv.org/abs/2011.10367" target="_blank">arXiv:2011.10367</a> [<a href="http://arxiv.org/pdf/2011.10367" target="_blank">pdf</a>]

<h2>GenderRobustness: Robustness of Gender Detection in Facial Recognition Systems with variation in Image Properties. (arXiv:2011.10472v2 [cs.CV] UPDATED)</h2>
<h3>Sharadha Srinivasan, Madan Musuvathi</h3>
<p>In recent times, there have been increasing accusations on artificial
intelligence systems and algorithms of computer vision of possessing implicit
biases. Even though these conversations are more prevalent now and systems are
improving by performing extensive testing and broadening their horizon, biases
still do exist. One such class of systems where bias is said to exist is facial
recognition systems, where bias has been observed on the basis of gender,
ethnicity, skin tone and other facial attributes. This is even more disturbing,
given the fact that these systems are used in practically every sector of the
industries today. From as critical as criminal identification to as simple as
getting your attendance registered, these systems have gained a huge market,
especially in recent years. That in itself is a good enough reason for
developers of these systems to ensure that the bias is kept to a bare minimum
or ideally non-existent, to avoid major issues like favoring a particular
gender, race, or class of people or rather making a class of people susceptible
to false accusations due to inability of these systems to correctly recognize
those people.
</p>
<a href="http://arxiv.org/abs/2011.10472" target="_blank">arXiv:2011.10472</a> [<a href="http://arxiv.org/pdf/2011.10472" target="_blank">pdf</a>]

<h2>A Population-based Hybrid Approach to Hyperparameter Optimization for Neural Networks. (arXiv:2011.11062v2 [cs.LG] UPDATED)</h2>
<h3>Marcello Serqueira, Pedro Gonz&#xe1;lez, Eduardo Bezerra</h3>
<p>In recent years, large amounts of data have been generated, and computer
power has kept growing. This scenario has led to a resurgence in the interest
in artificial neural networks. One of the main challenges in training effective
neural network models is finding the right combination of hyperparameters to be
used. Indeed, the choice of an adequate approach to search the hyperparameter
space directly influences the accuracy of the resulting neural network model.
Common approaches for hyperparameter optimization are Grid Search, Random
Search, and Bayesian Optimization. There are also population-based methods such
as CMA-ES. In this paper, we present HBRKGA, a new population-based approach
for hyperparameter optimization. HBRKGA is a hybrid approach that combines the
Biased Random Key Genetic Algorithm with a Random Walk technique to search the
hyperparameter space efficiently. Several computational experiments on eight
different datasets were performed to assess the effectiveness of the proposed
approach. Results showed that HBRKGA could find hyperparameter configurations
that outperformed (in terms of predictive quality) the baseline methods in six
out of eight datasets while showing a reasonable execution time.
</p>
<a href="http://arxiv.org/abs/2011.11062" target="_blank">arXiv:2011.11062</a> [<a href="http://arxiv.org/pdf/2011.11062" target="_blank">pdf</a>]

<h2>Imagination-enabled Robot Perception. (arXiv:2011.11397v3 [cs.RO] UPDATED)</h2>
<h3>Patrick Mania, Franklin Kenghagho Kenfack, Michael Neumann, Michael Beetz</h3>
<p>Many of today's robot perception systems aim at accomplishing perception
tasks that are too simplistic and too hard. They are too simplistic because
they do not require the perception systems to provide all the information
needed to accomplish manipulation tasks. Typically the perception results do
not include information about the part structure of objects, articulation
mechanisms and other attributes needed for adapting manipulation behavior. On
the other hand, the perception problems stated are also too hard because --
unlike humans -- the perception systems cannot leverage the expectations about
what they will see to their full potential. Therefore, we investigate a
variation of robot perception tasks suitable for robots accomplishing everyday
manipulation tasks, such as household robots or a robot in a retail store. In
such settings it is reasonable to assume that robots know most objects and have
detailed models of them.

We propose a perception system that maintains its beliefs about its
environment as a scene graph with physics simulation and visual rendering. When
detecting objects, the perception system retrieves the model of the object and
places it at the corresponding place in a VR-based environment model. The
physics simulation ensures that object detections that are physically not
possible are rejected and scenes can be rendered to generate expectations at
the image level. The result is a perception system that can provide useful
information for manipulation tasks.
</p>
<a href="http://arxiv.org/abs/2011.11397" target="_blank">arXiv:2011.11397</a> [<a href="http://arxiv.org/pdf/2011.11397" target="_blank">pdf</a>]

<h2>APAN: Asynchronous Propagate Attention Network for Real-time Temporal Graph Embedding. (arXiv:2011.11545v2 [cs.AI] UPDATED)</h2>
<h3>Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang, Xinwen Wang, Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, Zhenyu Guo</h3>
<p>Limited by the time complexity of querying k-hop neighbors in a graph
database, most graph algorithms cannot be deployed online and execute
millisecond-level inference. This problem dramatically limits the potential of
applying graph algorithms in certain areas, such as financial fraud detection.
Therefore, we propose Asynchronous Propagate Attention Network, an asynchronous
continuous time dynamic graph algorithm for real-time temporal graph embedding.
Traditional graph models usually execute two serial operations: first graph
computation and then model inference. We decouple model inference and graph
computation step so that the heavy graph query operations will not damage the
speed of model inference. Extensive experiments demonstrate that the proposed
method can achieve competitive performance and 8.7 times inference speed
improvement in the meantime.
</p>
<a href="http://arxiv.org/abs/2011.11545" target="_blank">arXiv:2011.11545</a> [<a href="http://arxiv.org/pdf/2011.11545" target="_blank">pdf</a>]

<h2>Differentially Private Learning Needs Better Features (or Much More Data). (arXiv:2011.11660v2 [cs.LG] UPDATED)</h2>
<h3>Florian Tram&#xe8;r, Dan Boneh</h3>
<p>We demonstrate that differentially private machine learning has not yet
reached its "AlexNet moment" on many canonical vision tasks: linear models
trained on handcrafted features significantly outperform end-to-end deep neural
networks for moderate privacy budgets. To exceed the performance of handcrafted
features, we show that private learning requires either much more private data,
or access to features learned on public data from a similar domain. Our work
introduces simple yet strong baselines for differentially private learning that
can inform the evaluation of future progress in this area.
</p>
<a href="http://arxiv.org/abs/2011.11660" target="_blank">arXiv:2011.11660</a> [<a href="http://arxiv.org/pdf/2011.11660" target="_blank">pdf</a>]

<h2>Path Design and Resource Management for NOMA enhanced Indoor Intelligent Robots. (arXiv:2011.11745v2 [cs.AI] UPDATED)</h2>
<h3>Ruikang Zhong, Xiao Liu, Yuanwei Liu, Yue Chen, Xianbin Wang</h3>
<p>A communication enabled indoor intelligent robots (IRs) service framework is
proposed, where non-orthogonal multiple access (NOMA) technique is adopted to
enable highly reliable communications. In cooperation with the ultramodern
indoor channel model recently proposed by the International Telecommunication
Union (ITU), the Lego modeling method is proposed, which can deterministically
describe the indoor layout and channel state in order to construct the radio
map. The investigated radio map is invoked as a virtual environment to train
the reinforcement learning agent, which can save training time and hardware
costs. Build on the proposed communication model, motions of IRs who need to
reach designated mission destinations and their corresponding down-link power
allocation policy are jointly optimized to maximize the mission efficiency and
communication reliability of IRs. In an effort to solve this optimization
problem, a novel reinforcement learning approach named deep transfer
deterministic policy gradient (DT-DPG) algorithm is proposed. Our simulation
results demonstrate that 1) With the aid of NOMA techniques, the communication
reliability of IRs is effectively improved; 2) The radio map is qualified to be
a virtual training environment, and its statistical channel state information
improves training efficiency by about 30%; 3) The proposed DT-DPG algorithm is
superior to the conventional deep deterministic policy gradient (DDPG)
algorithm in terms of optimization performance, training time, and anti-local
optimum ability.
</p>
<a href="http://arxiv.org/abs/2011.11745" target="_blank">arXiv:2011.11745</a> [<a href="http://arxiv.org/pdf/2011.11745" target="_blank">pdf</a>]

<h2>Dissecting Image Crops. (arXiv:2011.11831v3 [cs.CV] UPDATED)</h2>
<h3>Basile Van Hoorick, Carl Vondrick</h3>
<p>The elementary operation of cropping underpins nearly every computer vision
system, ranging from data augmentation and translation invariance to
computational photography and representation learning. This paper investigates
the subtle traces introduced by this operation. For example, despite
refinements to camera optics, lenses will leave behind certain clues, notably
chromatic aberration and vignetting. Photographers also leave behind other
clues relating to image aesthetics and scene composition. We study how to
detect these traces, and investigate the impact that cropping has on the image
distribution. While our aim is to dissect the fundamental impact of spatial
crops, there are also a number of practical implications to our work, such as
detecting image manipulations and equipping neural network researchers with a
better understanding of shortcut learning. Code is available at
https://github.com/basilevh/dissecting-image-crops.
</p>
<a href="http://arxiv.org/abs/2011.11831" target="_blank">arXiv:2011.11831</a> [<a href="http://arxiv.org/pdf/2011.11831" target="_blank">pdf</a>]

<h2>Learning Principle of Least Action with Reinforcement Learning. (arXiv:2011.11891v2 [cs.LG] UPDATED)</h2>
<h3>Zehao Jin, Joshua Yao-Yu Lin, Siao-Fong Li</h3>
<p>Nature provides a way to understand physics with reinforcement learning since
nature favors the economical way for an object to propagate. In the case of
classical mechanics, nature favors the object to move along the path according
to the integral of the Lagrangian, called the action $\mathcal{S}$. We consider
setting the reward/penalty as a function of $\mathcal{S}$, so the agent could
learn the physical trajectory of particles in various kinds of environments
with reinforcement learning. In this work, we verified the idea by using a
Q-Learning based algorithm on learning how light propagates in materials with
different refraction indices, and show that the agent could recover the
minimal-time path equivalent to the solution obtained by Snell's law or
Fermat's Principle. We also discuss the similarity of our reinforcement
learning approach to the path integral formalism.
</p>
<a href="http://arxiv.org/abs/2011.11891" target="_blank">arXiv:2011.11891</a> [<a href="http://arxiv.org/pdf/2011.11891" target="_blank">pdf</a>]

<h2>Efficient Initial Pose-graph Generation for Global SfM. (arXiv:2011.11986v2 [cs.CV] UPDATED)</h2>
<h3>Daniel Barath, Dmytro Mishkin, Ivan Eichhardt, Ilia Shipachev, Jiri Matas</h3>
<p>We propose ways to speed up the initial pose-graph generation for global
Structure-from-Motion algorithms. To avoid forming tentative point
correspondences by FLANN and geometric verification by RANSAC, which are the
most time-consuming steps of the pose-graph creation, we propose two new
methods - built on the fact that image pairs usually are matched consecutively.
Thus, candidate relative poses can be recovered from paths in the partly-built
pose-graph. We propose a heuristic for the A* traversal, considering global
similarity of images and the quality of the pose-graph edges. Given a relative
pose from a path, descriptor-based feature matching is made "light-weight" by
exploiting the known epipolar geometry. To speed up PROSAC-based sampling when
RANSAC is applied, we propose a third method to order the correspondences by
their inlier probabilities from previous estimations. The algorithms are tested
on 402130 image pairs from the 1DSfM dataset and they speed up the feature
matching 17 times and pose estimation 5 times.
</p>
<a href="http://arxiv.org/abs/2011.11986" target="_blank">arXiv:2011.11986</a> [<a href="http://arxiv.org/pdf/2011.11986" target="_blank">pdf</a>]

<h2>Effective and Sparse Count-Sketch via k-means clustering. (arXiv:2011.12046v2 [cs.LG] UPDATED)</h2>
<h3>Yuhan Wang, Zijian Lei, Liang Lan</h3>
<p>Count-sketch is a popular matrix sketching algorithm that can produce a
sketch of an input data matrix X in O(nnz(X))time where nnz(X) denotes the
number of non-zero entries in X. The sketched matrix will be much smaller than
X while preserving most of its properties. Therefore, count-sketch is widely
used for addressing high-dimensionality challenge in machine learning. However,
there are two main limitations of count-sketch: (1) The sketching matrix used
count-sketch is generated randomly which does not consider any intrinsic data
properties of X. This data-oblivious matrix sketching method could produce a
bad sketched matrix which will result in low accuracy for subsequent machine
learning tasks (e.g.classification); (2) For highly sparse input data,
count-sketch could produce a dense sketched data matrix. This dense sketch
matrix could make the subsequent machine learning tasks more computationally
expensive than on the original sparse data X. To address these two limitations,
we first show an interesting connection between count-sketch and k-means
clustering by analyzing the reconstruction error of the count-sketch method.
Based on our analysis, we propose to reduce the reconstruction error of
count-sketch by using k-means clustering algorithm to obtain the
low-dimensional sketched matrix. In addition, we propose to solve k-mean
clustering using gradient descent with -L1 ball projection to produce a sparse
sketched matrix. Our experimental results based on six real-life classification
datasets have demonstrated that our proposed method achieves higher accuracy
than the original count-sketch and other popular matrix sketching algorithms.
Our results also demonstrate that our method produces a sparser sketched data
matrix than other methods and therefore the prediction cost of our method will
be smaller than other matrix sketching methods.
</p>
<a href="http://arxiv.org/abs/2011.12046" target="_blank">arXiv:2011.12046</a> [<a href="http://arxiv.org/pdf/2011.12046" target="_blank">pdf</a>]

<h2>Computational efficient deep neural network with difference attention maps for facial action unit detection. (arXiv:2011.12082v2 [cs.CV] UPDATED)</h2>
<h3>Jing Chen, Chenhui Wang, Kejun Wang, Meichen Liu</h3>
<p>In this paper, we propose a computational efficient end-to-end training deep
neural network (CEDNN) model and spatial attention maps based on difference
images. Firstly, the difference image is generated by image processing. Then
five binary images of difference images are obtained using different
thresholds, which are used as spatial attention maps. We use group convolution
to reduce model complexity. Skip connection and $\text{1}\times \text{1}$
convolution are used to ensure good performance even if the network model is
not deep. As an input, spatial attention map can be selectively fed into the
input of each block. The feature maps tend to focus on the parts that are
related to the target task better. In addition, we only need to adjust the
parameters of classifier to train different numbers of AU. It can be easily
extended to varying datasets without increasing too much computation. A large
number of experimental results show that the proposed CEDNN is obviously better
than the traditional deep learning method on DISFA+ and CK+ datasets. After
adding spatial attention maps, the result is better than the most advanced AU
detection method. At the same time, the scale of the network is small, the
running speed is fast, and the requirement for experimental equipment is low.
</p>
<a href="http://arxiv.org/abs/2011.12082" target="_blank">arXiv:2011.12082</a> [<a href="http://arxiv.org/pdf/2011.12082" target="_blank">pdf</a>]

<h2>AI Discovering a Coordinate System of Chemical Elements: Dual Representation by Variational Autoencoders. (arXiv:2011.12090v2 [cs.LG] UPDATED)</h2>
<h3>Alex Glushkovsky</h3>
<p>The periodic table is a fundamental representation of chemical elements that
plays essential theoretical and practical roles. The research article discusses
the experiences of unsupervised training of neural networks to represent
elements on the 2D latent space based on their electron configurations while
forcing disentanglement. To emphasize chemical properties of the elements, the
original data of electron configurations has been realigned towards the
outermost valence orbitals. Recognizing seven shells and four subshells, the
input data has been arranged as (7x4) images. Latent space representation has
been performed using a convolutional beta variational autoencoder (beta-VAE).
Despite discrete and sparse input data, the beta-VAE disentangles elements of
different periods, blocks, groups, and types, while retaining the order along
atomic numbers. In addition, it isolates outliers on the latent space that
turned out to be known cases of Madelung's rule violations for lanthanide and
actinide elements. Considering the generative capabilities of beta-VAE and
discrete input data, the supervised machine learning has been set to find out
if there are insightful patterns distinguishing electron configurations between
real elements and decoded artificial ones. Also, the article addresses the
capability of dual representation by autoencoders. Conventionally, autoencoders
represent observations of input data on the latent space. However, by
transposing and duplicating original input data, it is possible to represent
variables on the latent space as well. The latest can lead to the discovery of
meaningful patterns among input variables. Applying that unsupervised learning
for transposed data of electron configurations, the order of input variables
that has been arranged by the encoder on the latent space has turned out to
exactly match the sequence of Madelung's rule.
</p>
<a href="http://arxiv.org/abs/2011.12090" target="_blank">arXiv:2011.12090</a> [<a href="http://arxiv.org/pdf/2011.12090" target="_blank">pdf</a>]

<h2>Improving Clinical Outcome Predictions Using Convolution over Medical Entities with Multimodal Learning. (arXiv:2011.12349v2 [cs.LG] UPDATED)</h2>
<h3>Batuhan Bardak, Mehmet Tan</h3>
<p>Early prediction of mortality and length of stay(LOS) of a patient is vital
for saving a patient's life and management of hospital resources. Availability
of electronic health records(EHR) makes a huge impact on the healthcare domain
and there has seen several works on predicting clinical problems. However, many
studies did not benefit from the clinical notes because of the sparse, and high
dimensional nature. In this work, we extract medical entities from clinical
notes and use them as additional features besides time-series features to
improve our predictions. We propose a convolution based multimodal
architecture, which not only learns effectively combining medical entities and
time-series ICU signals of patients, but also allows us to compare the effect
of different embedding techniques such as Word2vec, FastText on medical
entities. In the experiments, our proposed method robustly outperforms all
other baseline models including different multimodal architectures for all
clinical tasks. The code for the proposed method is available at
https://github.com/tanlab/ConvolutionMedicalNer.
</p>
<a href="http://arxiv.org/abs/2011.12349" target="_blank">arXiv:2011.12349</a> [<a href="http://arxiv.org/pdf/2011.12349" target="_blank">pdf</a>]

<h2>Batch Normalization Embeddings for Deep Domain Generalization. (arXiv:2011.12672v2 [cs.LG] UPDATED)</h2>
<h3>Mattia Segu, Alessio Tonioni, Federico Tombari</h3>
<p>Domain generalization aims at training machine learning models to perform
robustly across different and unseen domains. Several recent methods use
multiple datasets to train models to extract domain-invariant features, hoping
to generalize to unseen domains. Instead, first we explicitly train
domain-dependant representations by using ad-hoc batch normalization layers to
collect independent domain's statistics. Then, we propose to use these
statistics to map domains in a shared latent space, where membership to a
domain can be measured by means of a distance function. At test time, we
project samples from an unknown domain into the same space and infer properties
of their domain as a linear combination of the known ones. We apply the same
mapping strategy at training and test time, learning both a latent
representation and a powerful but lightweight ensemble model. We show a
significant increase in classification accuracy over current state-of-the-art
techniques on popular domain generalization benchmarks: PACS, Office-31 and
Office-Caltech.
</p>
<a href="http://arxiv.org/abs/2011.12672" target="_blank">arXiv:2011.12672</a> [<a href="http://arxiv.org/pdf/2011.12672" target="_blank">pdf</a>]

<h2>Towards Playing Full MOBA Games with Deep Reinforcement Learning. (arXiv:2011.12692v2 [cs.AI] UPDATED)</h2>
<h3>Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu</h3>
<p>MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand
challenges to AI systems such as multi-agent, enormous state-action space,
complex action control, etc. Developing AI for playing MOBA games has raised
much attention accordingly. However, existing work falls short in handling the
raw game complexity caused by the explosion of agent combinations, i.e.,
lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the
play to a pool of only 17 heroes. As a result, full MOBA games without
restrictions are far from being mastered by any existing AI system. In this
paper, we propose a MOBA AI learning paradigm that methodologically enables
playing full MOBA games with deep reinforcement learning. Specifically, we
develop a combination of novel and existing learning techniques, including
curriculum self-play learning, policy distillation, off-policy adaption,
multi-head value estimation, and Monte-Carlo tree-search, in training and
playing a large pool of heroes, meanwhile addressing the scalability issue
skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build
superhuman AI agents that can defeat top esports players. The superiority of
our AI is demonstrated by the first large-scale performance test of MOBA AI
agent in the literature.
</p>
<a href="http://arxiv.org/abs/2011.12692" target="_blank">arXiv:2011.12692</a> [<a href="http://arxiv.org/pdf/2011.12692" target="_blank">pdf</a>]

<h2>Multimodal Learning for Hateful Memes Detection. (arXiv:2011.12870v2 [cs.CV] UPDATED)</h2>
<h3>Yi Zhou, Zhenhao Chen</h3>
<p>Memes are multimedia documents containing images and phrases that usually
build a humorous meaning when combined. However, hateful memes are also spread
hatred within social networks. Automatically detecting the hateful memes would
help decrease their harmful societal impact. Unlike the conventional multimodal
tasks, where the visual and textual information is semantically aligned, the
challenge of hateful memes detection lies in its unique multimodal information.
The multimodal information in the memes are weakly aligned or even irrelevant,
which makes the model not only needs to understand the content in the memes but
also reasoning over the multiple modalities. In this paper, we focus on hateful
memes detection for multimodal memes and propose a novel method that
incorporates the image captioning process into the memes detection process. We
conducted extensive experiments on multimodal meme datasets and illustrated the
effectiveness of our approach. Our model also achieves promising results on the
Hateful memes detection challenge.
</p>
<a href="http://arxiv.org/abs/2011.12870" target="_blank">arXiv:2011.12870</a> [<a href="http://arxiv.org/pdf/2011.12870" target="_blank">pdf</a>]

<h2>Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption. (arXiv:2011.12902v2 [cs.CV] UPDATED)</h2>
<h3>Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, Cristian Canton Ferrer</h3>
<p>This work examines the vulnerability of multimodal (image + text) models to
adversarial threats similar to those discussed in previous literature on
unimodal (image- or text-only) models. We introduce realistic assumptions of
partial model knowledge and access, and discuss how these assumptions differ
from the standard "black-box"/"white-box" dichotomy common in current
literature on adversarial attacks. Working under various levels of these
"gray-box" assumptions, we develop new attack methodologies unique to
multimodal classification and evaluate them on the Hateful Memes Challenge
classification task. We find that attacking multiple modalities yields stronger
attacks than unimodal attacks alone (inducing errors in up to 73% of cases),
and that the unimodal image attacks on multimodal classifiers we explored were
stronger than character-based text augmentation attacks (inducing errors on
average in 45% and 30% of cases, respectively).
</p>
<a href="http://arxiv.org/abs/2011.12902" target="_blank">arXiv:2011.12902</a> [<a href="http://arxiv.org/pdf/2011.12902" target="_blank">pdf</a>]

<h2>Analyzing the Machine Learning Conference Review Process. (arXiv:2011.12919v2 [cs.LG] UPDATED)</h2>
<h3>David Tran, Alex Valtchanov, Keshav Ganapathy, Raymond Feng, Eric Slud, Micah Goldblum, Tom Goldstein</h3>
<p>Mainstream machine learning conferences have seen a dramatic increase in the
number of participants, along with a growing range of perspectives, in recent
years. Members of the machine learning community are likely to overhear
allegations ranging from randomness of acceptance decisions to institutional
bias. In this work, we critically analyze the review process through a
comprehensive study of papers submitted to ICLR between 2017 and 2020. We
quantify reproducibility/randomness in review scores and acceptance decisions,
and examine whether scores correlate with paper impact. Our findings suggest
strong institutional bias in accept/reject decisions, even after controlling
for paper quality. Furthermore, we find evidence for a gender gap, with female
authors receiving lower scores, lower acceptance rates, and fewer citations per
paper than their male counterparts. We conclude our work with recommendations
for future conference organizers.
</p>
<a href="http://arxiv.org/abs/2011.12919" target="_blank">arXiv:2011.12919</a> [<a href="http://arxiv.org/pdf/2011.12919" target="_blank">pdf</a>]

<h2>Deformable Neural Radiance Fields. (arXiv:2011.12948v2 [cs.CV] UPDATED)</h2>
<h3>Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla</h3>
<p>We present the first method capable of photorealistically reconstructing a
non-rigidly deforming scene using photos/videos captured casually from mobile
phones. Our approach -- D-NeRF -- augments neural radiance fields (NeRF) by
optimizing an additional continuous volumetric deformation field that warps
each observed point into a canonical 5D NeRF. We observe that these NeRF-like
deformation fields are prone to local minima, and propose a coarse-to-fine
optimization method for coordinate-based models that allows for more robust
optimization. By adapting principles from geometry processing and physical
simulation to NeRF-like models, we propose an elastic regularization of the
deformation field that further improves robustness. We show that D-NeRF can
turn casually captured selfie photos/videos into deformable NeRF models that
allow for photorealistic renderings of the subject from arbitrary viewpoints,
which we dub "nerfies." We evaluate our method by collecting data using a rig
with two mobile phones that take time-synchronized photos, yielding
train/validation images of the same pose at different viewpoints. We show that
our method faithfully reconstructs non-rigidly deforming scenes and reproduces
unseen views with high fidelity.
</p>
<a href="http://arxiv.org/abs/2011.12948" target="_blank">arXiv:2011.12948</a> [<a href="http://arxiv.org/pdf/2011.12948" target="_blank">pdf</a>]

<h2>Unsupervised Object Detection with LiDAR Clues. (arXiv:2011.12953v2 [cs.CV] UPDATED)</h2>
<h3>Hao Tian, Yuntao Chen, Jifeng Dai, Zhaoxiang Zhang, Xizhou Zhu</h3>
<p>Despite the importance of unsupervised object detection, to the best of our
knowledge, there is no previous work addressing this problem. One main issue,
widely known to the community, is that object boundaries derived only from 2D
image appearance are ambiguous and unreliable. To address this, we exploit
LiDAR clues to aid unsupervised object detection. By exploiting the 3D scene
structure, the issue of localization can be considerably mitigated. We further
identify another major issue, seldom noticed by the community, that the
long-tailed and open-ended (sub-)category distribution should be accommodated.
In this paper, we present the first practical method for unsupervised object
detection with the aid of LiDAR clues. In our approach, candidate object
segments based on 3D point clouds are firstly generated. Then, an iterative
segment labeling process is conducted to assign segment labels and to train a
segment labeling network, which is based on features from both 2D images and 3D
point clouds. The labeling process is carefully designed so as to mitigate the
issue of long-tailed and open-ended distribution. The final segment labels are
set as pseudo annotations for object detection network training. Extensive
experiments on the large-scale Waymo Open dataset suggest that the derived
unsupervised object detection method achieves reasonable accuracy compared with
that of strong supervision within the LiDAR visible range. Code shall be
released.
</p>
<a href="http://arxiv.org/abs/2011.12953" target="_blank">arXiv:2011.12953</a> [<a href="http://arxiv.org/pdf/2011.12953" target="_blank">pdf</a>]

<h2>Fast Object Segmentation Learning with Kernel-based Methods for Robotics. (arXiv:2011.12805v1 [cs.CV] CROSS LISTED)</h2>
<h3>Federico Ceola, Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco, Lorenzo Natale</h3>
<p>Object segmentation is a key component in the visual system of a robot that
performs tasks like grasping and object manipulation, especially in presence of
occlusions. Like many other Computer Vision tasks, the adoption of deep
architectures has made available algorithms that perform this task with
remarkable performance. However, adoption of such algorithms in robotics is
hampered by the fact that training requires large amount of computing time and
it cannot be performed on-line. In this work, we propose a novel architecture
for object segmentation, that overcomes this problem and provides comparable
performance in a fraction of the time required by the state-of-the-art methods.
Our approach is based on a pre-trained Mask R-CNN, in which various layers have
been replaced with a set of classifiers and regressors that are retrained for a
new task. We employ an efficient Kernel-based method that allows for fast
training on large scale problems. Our approach is validated on the YCB-Video
dataset which is widely adopted in the Computer Vision and Robotics community,
demonstrating that we can achieve and even surpass performance of the
state-of-the-art, with a significant reduction (${\sim}6\times$) of the
training time. The code will be released upon acceptance.
</p>
<a href="http://arxiv.org/abs/2011.12805" target="_blank">arXiv:2011.12805</a> [<a href="http://arxiv.org/pdf/2011.12805" target="_blank">pdf</a>]

