---
title: Latest Deep Learning Papers
date: 2020-12-06 21:00:01 +0800
featured-img: DL-logo
categories: [Deep_Learning]
tags: [Deep_Learning]
mathjax: Yes
---

<h1>Your arXiv Feed (181 Articles)</h1>
<h2>Methods of ranking for aggregated fuzzy numbers from interval-valued data. (arXiv:2012.02194v1 [cs.AI])</h2>
<h3>Justin Kane Gunn, Hadi Akbarzadeh Khorshidi, Uwe Aickelin</h3>
<p>This paper primarily presents two methods of ranking aggregated fuzzy numbers
from intervals using the Interval Agreement Approach (IAA). The two proposed
ranking methods within this study contain the combination and application of
previously proposed similarity measures, along with attributes novel to that of
aggregated fuzzy numbers from interval-valued data. The shortcomings of
previous measures, along with the improvements of the proposed methods, are
illustrated using both a synthetic and real-world application. The real-world
application regards the Technique for Order of Preference by Similarity to
Ideal Solution (TOPSIS) algorithm, modified to include both the previous and
newly proposed methods.
</p>
<a href="http://arxiv.org/abs/2012.02194" target="_blank">arXiv:2012.02194</a> [<a href="http://arxiv.org/pdf/2012.02194" target="_blank">pdf</a>]

<h2>Scan2Cap: Context-aware Dense Captioning in RGB-D Scans. (arXiv:2012.02206v1 [cs.CV])</h2>
<h3>Dave Zhenyu Chen, Ali Gholami, Matthias Nie&#xdf;ner, Angel X. Chang</h3>
<p>We introduce the task of dense captioning in 3D scans from commodity RGB-D
sensors. As input, we assume a point cloud of a 3D scene; the expected output
is the bounding boxes along with the descriptions for the underlying objects.
To address the 3D object detection and description problems, we propose
Scan2Cap, an end-to-end trained method, to detect objects in the input scene
and describe them in natural language. We use an attention mechanism that
generates descriptive tokens while referring to the related components in the
local context. To reflect object relations (i.e. relative spatial relations) in
the generated captions, we use a message passing graph module to facilitate
learning object relation features. Our method can effectively localize and
describe 3D objects in scenes from the ScanRefer dataset, outperforming 2D
baseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).
</p>
<a href="http://arxiv.org/abs/2012.02206" target="_blank">arXiv:2012.02206</a> [<a href="http://arxiv.org/pdf/2012.02206" target="_blank">pdf</a>]

<h2>Traffic Surveillance using Vehicle License Plate Detection and Recognition in Bangladesh. (arXiv:2012.02218v1 [cs.CV])</h2>
<h3>Md. Saif Hassan Onim, Muhaiminul Islam Akash, Mahmudul Haque, Raiyan Ibne Hafiz</h3>
<p>Computer vision coupled with Deep Learning (DL) techniques bring out a
substantial prospect in the field of traffic control, monitoring and law
enforcing activities. This paper presents a YOLOv4 object detection model in
which the Convolutional Neural Network (CNN) is trained and tuned for detecting
the license plate of the vehicles of Bangladesh and recognizing characters
using tesseract from the detected license plates. Here we also present a
Graphical User Interface (GUI) based on Tkinter, a python package. The license
plate detection model is trained with mean average precision (mAP) of 90.50%
and performed in a single TESLA T4 GPU with an average of 14 frames per second
(fps) on real time video footage.
</p>
<a href="http://arxiv.org/abs/2012.02218" target="_blank">arXiv:2012.02218</a> [<a href="http://arxiv.org/pdf/2012.02218" target="_blank">pdf</a>]

<h2>Personality-Driven Gaze Animation with Conditional Generative Adversarial Networks. (arXiv:2012.02224v1 [cs.CV])</h2>
<h3>Funda Durupinar</h3>
<p>We present a generative adversarial learning approach to synthesize gaze
behavior of a given personality. We train the model using an existing data set
that comprises eye-tracking data and personality traits of 42 participants
performing an everyday task. Given the values of Big-Five personality traits
(openness, conscientiousness, extroversion, agreeableness, and neuroticism),
our model generates time series data consisting of gaze target, blinking times,
and pupil dimensions. We use the generated data to synthesize the gaze motion
of virtual agents on a game engine.
</p>
<a href="http://arxiv.org/abs/2012.02224" target="_blank">arXiv:2012.02224</a> [<a href="http://arxiv.org/pdf/2012.02224" target="_blank">pdf</a>]

<h2>EVRNet: Efficient Video Restoration on Edge Devices. (arXiv:2012.02228v1 [cs.CV])</h2>
<h3>Sachin Mehta, Amit Kumar, Fitsum Reda, Varun Nasery, Vikram Mulukutla, Rakesh Ranjan, Vikas Chandra</h3>
<p>Video transmission applications (e.g., conferencing) are gaining momentum,
especially in times of global health pandemic. Video signals are transmitted
over lossy channels, resulting in low-quality received signals. To restore
videos on recipient edge devices in real-time, we introduce an efficient video
restoration network, EVRNet. EVRNet efficiently allocates parameters inside the
network using alignment, differential, and fusion modules. With extensive
experiments on video restoration tasks (deblocking, denoising, and
super-resolution), we demonstrate that EVRNet delivers competitive performance
to existing methods with significantly fewer parameters and MACs. For example,
EVRNet has 260 times fewer parameters and 958 times fewer MACs than enhanced
deformable convolution-based video restoration network (EDVR) for 4 times video
super-resolution while its SSIM score is 0.018 less than EDVR. We also
evaluated the performance of EVRNet under multiple distortions on unseen
dataset to demonstrate its ability in modeling variable-length sequences under
both camera and object motion.
</p>
<a href="http://arxiv.org/abs/2012.02228" target="_blank">arXiv:2012.02228</a> [<a href="http://arxiv.org/pdf/2012.02228" target="_blank">pdf</a>]

<h2>Graph Convolutional Neural Networks for Body Force Prediction. (arXiv:2012.02232v1 [cs.LG])</h2>
<h3>Francis Ogoke, Kazem Meidani, Amirreza Hashemi, Amir Barati Farimani</h3>
<p>Many scientific and engineering processes produce spatially unstructured
data. However, most data-driven models require a feature matrix that enforces
both a set number and order of features for each sample. They thus cannot be
easily constructed for an unstructured dataset. Therefore, a graph based
data-driven model to perform inference on fields defined on an unstructured
mesh, using a Graph Convolutional Neural Network (GCNN) is presented. The
ability of the method to predict global properties from spatially irregular
measurements with high accuracy is demonstrated by predicting the drag force
associated with laminar flow around airfoils from scattered velocity
measurements. The network can infer from field samples at different
resolutions, and is invariant to the order in which the measurements within
each sample are presented. The GCNN method, using inductive convolutional
layers and adaptive pooling, is able to predict this quantity with a validation
$R^{2}$ above 0.98, and a Normalized Mean Squared Error below 0.01, without
relying on spatial structure.
</p>
<a href="http://arxiv.org/abs/2012.02232" target="_blank">arXiv:2012.02232</a> [<a href="http://arxiv.org/pdf/2012.02232" target="_blank">pdf</a>]

<h2>Explaining Predictions of Deep Neural Classifier via Activation Analysis. (arXiv:2012.02248v1 [cs.AI])</h2>
<h3>Martin Stano, Wanda Benesova, Lukas Samuel Martak</h3>
<p>In many practical applications, deep neural networks have been typically
deployed to operate as a black box predictor. Despite the high amount of work
on interpretability and high demand on the reliability of these systems, they
typically still have to include a human actor in the loop, to validate the
decisions and handle unpredictable failures and unexpected corner cases. This
is true in particular for failure-critical application domains, such as medical
diagnosis. We present a novel approach to explain and support an interpretation
of the decision-making process to a human expert operating a deep learning
system based on Convolutional Neural Network (CNN). By modeling activation
statistics on selected layers of a trained CNN via Gaussian Mixture Models
(GMM), we develop a novel perceptual code in binary vector space that describes
how the input sample is processed by the CNN. By measuring distances between
pairs of samples in this perceptual encoding space, for any new input sample,
we can now retrieve a set of most perceptually similar and dissimilar samples
from an existing atlas of labeled samples, to support and clarify the decision
made by the CNN model. Possible uses of this approach include for example
Computer-Aided Diagnosis (CAD) systems working with medical imaging data, such
as Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) scans. We
demonstrate the viability of our method in the domain of medical imaging for
patient condition diagnosis, as the proposed decision explanation method via
similar ground truth domain examples (e.g. from existing diagnosis archives)
will be interpretable by the operating medical personnel. Our results indicate
that our method is capable of detecting distinct prediction strategies that
enable us to identify the most similar predictions from an existing atlas.
</p>
<a href="http://arxiv.org/abs/2012.02248" target="_blank">arXiv:2012.02248</a> [<a href="http://arxiv.org/pdf/2012.02248" target="_blank">pdf</a>]

<h2>Deep Learning for Road Traffic Forecasting: Does it Make a Difference?. (arXiv:2012.02260v1 [cs.LG])</h2>
<h3>Eric L. Manibardo, Ibai La&#xf1;a, Javier Del Ser</h3>
<p>Deep Learning methods have been proven to be flexible to model complex
phenomena. This has also been the case of Intelligent Transportation Systems
(ITS), in which several areas such as vehicular perception and traffic analysis
have widely embraced Deep Learning as a core modeling technology. Particularly
in short-term traffic forecasting, the capability of Deep Learning to deliver
good results has generated a prevalent inertia towards using Deep Learning
models, without examining in depth their benefits and downsides. This paper
focuses on critically analyzing the state of the art in what refers to the use
of Deep Learning for this particular ITS research area. To this end, we
elaborate on the findings distilled from a review of publications from recent
years, based on two taxonomic criteria. A posterior critical analysis is held
to formulate questions and trigger a necessary debate about the issues of Deep
Learning for traffic forecasting. The study is completed with a benchmark of
diverse short-term traffic forecasting methods over traffic datasets of
different nature, aimed to cover a wide spectrum of possible scenarios. Our
experimentation reveals that Deep Learning could not be the best modeling
technique for every case, which unveils some caveats unconsidered to date that
should be addressed by the community in prospective studies. These insights
reveal new challenges and research opportunities in road traffic forecasting,
which are enumerated and discussed thoroughly, with the intention of inspiring
and guiding future research efforts in this field.
</p>
<a href="http://arxiv.org/abs/2012.02260" target="_blank">arXiv:2012.02260</a> [<a href="http://arxiv.org/pdf/2012.02260" target="_blank">pdf</a>]

<h2>Domain Adaptation of Aerial Semantic Segmentation. (arXiv:2012.02264v1 [cs.CV])</h2>
<h3>Ying Chen, Xu Ouyang, Kaiyue Zhu, Gady Agam</h3>
<p>Semantic segmentation has achieved significant advances in recent years.
While deep neural networks perform semantic segmentation well, their success
rely on pixel level supervision which is expensive and time-consuming. Further,
training using data from one domain may not generalize well to data from a new
domain due to a domain gap between data distributions in the different domains.
This domain gap is particularly evident in aerial images where visual
appearance depends on the type of environment imaged, season, weather, and time
of day when the environment is imaged. Subsequently, this distribution gap
leads to severe accuracy loss when using a pretrained segmentation model to
analyze new data with different characteristics. In this paper, we propose a
novel unsupervised domain adaptation framework to address domain shift in the
context of aerial semantic image segmentation. To this end, we solve the
problem of domain shift by learn the soft label distribution difference between
the source and target domains. Further, we also apply entropy minimization on
the target domain to produce high-confident prediction rather than using
high-confident prediction by pseudo-labeling. We demonstrate the effectiveness
of our domain adaptation framework using the challenge image segmentation
dataset of ISPRS, and show improvement over state-of-the-art methods in terms
of various metrics.
</p>
<a href="http://arxiv.org/abs/2012.02264" target="_blank">arXiv:2012.02264</a> [<a href="http://arxiv.org/pdf/2012.02264" target="_blank">pdf</a>]

<h2>LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment. (arXiv:2012.02271v1 [cs.RO])</h2>
<h3>Florence Tsang, Tristan Walker, Ryan A. MacDonald, Armin Sadeghi, Stephen L. Smith</h3>
<p>Mobile robots are often tasked with repeatedly navigating through an
environment whose traversability changes over time. These changes may exhibit
some hidden structure, which can be learned. Many studies consider reactive
algorithms for online planning, however, these algorithms do not take advantage
of the past executions of the navigation task for future tasks. In this paper,
we formalize the problem of minimizing the total expected cost to perform
multiple start-to-goal navigation tasks on a roadmap by introducing the Learned
Reactive Planning Problem. We propose a method that captures information from
past executions to learn a motion policy to handle obstacles that the robot has
seen before. We propose the LAMP framework, which integrates the generated
motion policy with an existing navigation stack. Finally, an extensive set of
experiments in simulated and real-world environments show that the proposed
method outperforms the state-of-the-art algorithms by 10% to 40% in terms of
expected time to travel from start to goal. We also evaluate the robustness of
the proposed method in the presence of localization and mapping errors on a
real robot.
</p>
<a href="http://arxiv.org/abs/2012.02271" target="_blank">arXiv:2012.02271</a> [<a href="http://arxiv.org/pdf/2012.02271" target="_blank">pdf</a>]

<h2>Detecting Trojaned DNNs Using Counterfactual Attributions. (arXiv:2012.02275v1 [cs.LG])</h2>
<h3>Karan Sikka, Indranil Sur, Susmit Jha, Anirban Roy, Ajay Divakaran</h3>
<p>We target the problem of detecting Trojans or backdoors in DNNs. Such models
behave normally with typical inputs but produce specific incorrect predictions
for inputs poisoned with a Trojan trigger. Our approach is based on a novel
observation that the trigger behavior depends on a few ghost neurons that
activate on trigger pattern and exhibit abnormally higher relative attribution
for wrong decisions when activated. Further, these trigger neurons are also
active on normal inputs of the target class. Thus, we use counterfactual
attributions to localize these ghost neurons from clean inputs and then
incrementally excite them to observe changes in the model's accuracy. We use
this information for Trojan detection by using a deep set encoder that enables
invariance to the number of model classes, architecture, etc. Our approach is
implemented in the TrinityAI tool that exploits the synergies between
trustworthiness, resilience, and interpretability challenges in deep learning.
We evaluate our approach on benchmarks with high diversity in model
architectures, triggers, etc. We show consistent gains (+10%) over
state-of-the-art methods that rely on the susceptibility of the DNN to specific
adversarial attacks, which in turn requires strong assumptions on the nature of
the Trojan attack.
</p>
<a href="http://arxiv.org/abs/2012.02275" target="_blank">arXiv:2012.02275</a> [<a href="http://arxiv.org/pdf/2012.02275" target="_blank">pdf</a>]

<h2>A feedforward neural network for modelling of average pressure frequency response. (arXiv:2012.02276v1 [cs.LG])</h2>
<h3>Klas Pettersson, Andrey Karzhou, Irina Pettersson</h3>
<p>The Helmholtz equation has been used for modelling the sound pressure field
under a harmonic load. Computing harmonic sound pressure fields by means of
solving Helmholtz equation can quickly become unfeasible if one wants to study
many different geometries for ranges of frequencies. We propose a machine
learning approach, namely a feedforward dense neural network, for computing the
average sound pressure over a frequency range. The data is generated with
finite elements, by numerically computing the response of the average sound
pressure, by an eigenmode decomposition of the pressure. We analyze the
accuracy of the approximation and determine how much training data is needed in
order to reach a certain accuracy in the predictions of the average pressure
response.
</p>
<a href="http://arxiv.org/abs/2012.02276" target="_blank">arXiv:2012.02276</a> [<a href="http://arxiv.org/pdf/2012.02276" target="_blank">pdf</a>]

<h2>Optimal Policy Trees. (arXiv:2012.02279v1 [cs.LG])</h2>
<h3>Maxime Amram, Jack Dunn, Ying Daisy Zhuo</h3>
<p>We propose an approach for learning optimal tree-based prescription policies
directly from data, combining methods for counterfactual estimation from the
causal inference literature with recent advances in training globally-optimal
decision trees. The resulting method, Optimal Policy Trees, yields
interpretable prescription policies, is highly scalable, and handles both
discrete and continuous treatments. We conduct extensive experiments on both
synthetic and real-world datasets and demonstrate that these trees offer
best-in-class performance across a wide variety of problems.
</p>
<a href="http://arxiv.org/abs/2012.02279" target="_blank">arXiv:2012.02279</a> [<a href="http://arxiv.org/pdf/2012.02279" target="_blank">pdf</a>]

<h2>Creativity of Deep Learning: Conceptualization and Assessment. (arXiv:2012.02282v1 [cs.AI])</h2>
<h3>Johannes Schneider, Marcus Basalla</h3>
<p>While the potential of deep learning(DL) for automating simple tasks is
already well explored, recent research started investigating the use of deep
learning for creative design, both for complete artifact creation and
supporting humans in the creation process. In this paper, we use insights from
computational creativity to conceptualize and assess current applications of
generative deep learning in creative domains identified in a literature review.
We highlight parallels between current systems and different models of human
creativity as well as their shortcomings. While deep learning yields results of
high value, such as high quality images, their novelity is typically limited
due to multiple reasons such a being tied to a conceptual space defined by
training data and humans. Current DL methods also do not allow for changes in
the internal problem representation and they lack the capability to identify
connections across highly different domains, both of which are seen as major
drivers of human creativity.
</p>
<a href="http://arxiv.org/abs/2012.02282" target="_blank">arXiv:2012.02282</a> [<a href="http://arxiv.org/pdf/2012.02282" target="_blank">pdf</a>]

<h2>Generative Capacity of Probabilistic Protein Sequence Models. (arXiv:2012.02296v1 [cs.LG])</h2>
<h3>Francisco McGee, Quentin Novinger, Ronald M. Levy, Vincenzo Carnevale, Allan Haldane</h3>
<p>Variational autoencoders (VAEs) have recently gained popularity as generative
protein sequence models (GPSMs) to explore fitness landscapes and predict the
effect of mutations. Despite encouraging results, a quantitative
characterization of the VAE-generated probability distribution is still
lacking. In particular, it is currently unclear whether or not VAEs can
faithfully reproduce the complex multi-residue mutation patterns observed in
natural sequences arising due to epistasis. In other words, are frequently
observed subsequences assigned a correspondingly large probability by the VAE?
Using a set of sequence statistics we comparatively assess the accuracy, or
"generative capacity", of three GPSMs: a pairwise Potts Hamiltonian, a vanilla
VAE, and a site-independent model, using natural and synthetic datasets. We
show that the vanilla VAE's generative capacity lies between the pairwise Potts
and site-independent models. Importantly, our work measures GPSM generative
capacity in terms of higher-order sequence covariation and provides a new
framework for evaluating and interpreting GPSM accuracy that emphasizes the
role of epistasis.
</p>
<a href="http://arxiv.org/abs/2012.02296" target="_blank">arXiv:2012.02296</a> [<a href="http://arxiv.org/pdf/2012.02296" target="_blank">pdf</a>]

<h2>Planning Brachistochrone Hip Trajectory for a Toe-Foot Bipedal Robot going Downstairs. (arXiv:2012.02301v1 [cs.RO])</h2>
<h3>Gaurav Bhardwaj, Utkarsh A. Mishra, N. Sukavanam, R. Balasubramanian</h3>
<p>A novel efficient downstairs trajectory is proposed for a 9 link biped robot
model with toe-foot. Brachistochrone is the fastest descent trajectory for a
particle moving only under the influence of gravity. In most situations, while
climbing downstairs, human hip also follow brachistochrone trajectory for a
more responsive motion. Here, an adaptive trajectory planning algorithm is
developed so that biped robots of varying link lengths, masses can climb down
on varying staircase dimensions. We assume that the center of gravity (COG) of
the biped concerned lies on the hip. Zero Moment Point (ZMP) based COG
trajectory is considered and its stability is ensured. Cycloidal trajectory is
considered for ankle of the swing leg. Parameters of both cycloid and
brachistochrone depends on dimensions of staircase steps. Hence this paper can
be broadly divided into 4 steps 1) Developing ZMP based brachistochrone
trajectory for hip 2) Cycloidal trajectory planning for ankle by taking proper
collision constraints 3) Solving Inverse kinematics using unsupervised
artificial neural network (ANN) 4) Comparison between the proposed, a circular
arc and a virtual slope based hip trajectory. The proposed algorithms have been
implemented using MATLAB.
</p>
<a href="http://arxiv.org/abs/2012.02301" target="_blank">arXiv:2012.02301</a> [<a href="http://arxiv.org/pdf/2012.02301" target="_blank">pdf</a>]

<h2>Concept-based model explanations for Electronic Health Records. (arXiv:2012.02308v1 [cs.LG])</h2>
<h3>Sebastien Baur, Shaobo Hou, Eric Loreaux, Diana Mincu, Anne Mottram, Ivan Protsyuk, Nenad Tomasev, Martin G Seneviratne, Alan Karthikesanlingam, Jessica Schrouff</h3>
<p>Recurrent Neural Networks (RNNs) are often used for sequential modeling of
adverse outcomes in electronic health records (EHRs) due to their ability to
encode past clinical states. These deep, recurrent architectures have displayed
increased performance compared to other modeling approaches in a number of
tasks, fueling the interest in deploying deep models in clinical settings. One
of the key elements in ensuring safe model deployment and building user trust
is model explainability. Testing with Concept Activation Vectors (TCAV) has
recently been introduced as a way of providing human-understandable
explanations by comparing high-level concepts to the network's gradients. While
the technique has shown promising results in real-world imaging applications,
it has not been applied to structured temporal inputs. To enable an application
of TCAV to sequential predictions in the EHR, we propose an extension of the
method to time series data. We evaluate the proposed approach on an open EHR
benchmark from the intensive care unit, as well as synthetic data where we are
able to better isolate individual effects.
</p>
<a href="http://arxiv.org/abs/2012.02308" target="_blank">arXiv:2012.02308</a> [<a href="http://arxiv.org/pdf/2012.02308" target="_blank">pdf</a>]

<h2>BoxInst: High-Performance Instance Segmentation with Box Annotations. (arXiv:2012.02310v1 [cs.CV])</h2>
<h3>Zhi Tian, Chunhua Shen, Xinlong Wang, Hao Chen</h3>
<p>We present a high-performance method that can achieve mask-level instance
segmentation with only bounding-box annotations for training. While this
setting has been studied in the literature, here we show significantly stronger
performance with a simple design (e.g., dramatically improving previous best
reported mask AP of 21.1% in Hsu et al. (2019) to 31.6% on the COCO dataset).
Our core idea is to redesign the loss of learning masks in instance
segmentation, with no modification to the segmentation network itself. The new
loss functions can supervise the mask training without relying on mask
annotations. This is made possible with two loss terms, namely, 1) a surrogate
term that minimizes the discrepancy between the projections of the ground-truth
box and the predicted mask; 2) a pairwise loss that can exploit the prior that
proximal pixels with similar colors are very likely to have the same category
label. Experiments demonstrate that the redesigned mask loss can yield
surprisingly high-quality instance masks with only box annotations. For
example, without using any mask annotations, with a ResNet-101 backbone and 3x
training schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1%
of the fully supervised counterpart). Our excellent experiment results on COCO
and Pascal VOC indicate that our method dramatically narrows the performance
gap between weakly and fully supervised instance segmentation.

Code is available at: https://git.io/AdelaiDet
</p>
<a href="http://arxiv.org/abs/2012.02310" target="_blank">arXiv:2012.02310</a> [<a href="http://arxiv.org/pdf/2012.02310" target="_blank">pdf</a>]

<h2>ReMix: Calibrated Resampling for Class Imbalance in Deep learning. (arXiv:2012.02312v1 [cs.LG])</h2>
<h3>Colin Bellinger, Roberto Corizzo, Nathalie Japkowicz</h3>
<p>Class imbalance is a problem of significant importance in applied deep
learning where trained models are exploited for decision support and automated
decisions in critical areas such as health and medicine, transportation, and
finance. The challenge of learning deep models from imbalanced training data
remains high, and the state-of-the-art solutions are typically data dependent
and primarily focused on image data. Real-world imbalanced classification
problems, however, are much more diverse thus necessitating a general solution
that can be applied to tabular, image and text data. In this paper, we propose
ReMix, a training technique that leverages batch resampling, instance mixing
and soft-labels to enable the induction of robust deep models for imbalanced
learning. Our results show that dense nets and CNNs trained with ReMix
generally outperform the alternatives according to the g-mean and are better
calibrated according to the balanced Brier score.
</p>
<a href="http://arxiv.org/abs/2012.02312" target="_blank">arXiv:2012.02312</a> [<a href="http://arxiv.org/pdf/2012.02312" target="_blank">pdf</a>]

<h2>MLPerf Mobile Inference Benchmark: Why Mobile AI Benchmarking Is Hard and What to Do About It. (arXiv:2012.02328v1 [cs.LG])</h2>
<h3>Vijay Janapa Reddi, David Kanter, Peter Mattson, Jared Duke, Thai Nguyen, Ramesh Chukka, Kenneth Shiring, Koan-Sin Tan, Mark Charlebois, William Chou, Mostafa El-Khamy, Jungwook Hong, Michael Buch, Cindy Trinh, Thomas Atta-fosu, Fatih Cakir, Masoud Charkhabi, Xiaodong Chen, Jimmy Chiang, Dave Dexter, Woncheol Heo, Guenther Schmuelling, Maryam Shabani, Dylan Zika</h3>
<p>MLPerf Mobile is the first industry-standard open-source mobile benchmark
developed by industry members and academic researchers to allow
performance/accuracy evaluation of mobile devices with different AI chips and
software stacks. The benchmark draws from the expertise of leading mobile-SoC
vendors, ML-framework providers, and model producers. In this paper, we
motivate the drive to demystify mobile-AI performance and present MLPerf
Mobile's design considerations, architecture, and implementation. The benchmark
comprises a suite of models that operate under standard models, data sets,
quality metrics, and run rules. For the first iteration, we developed an app to
provide an "out-of-the-box" inference-performance benchmark for computer vision
and natural-language processing on mobile devices. MLPerf Mobile can serve as a
framework for integrating future models, for customizing quality-target
thresholds to evaluate system performance, for comparing software frameworks,
and for assessing heterogeneous-hardware capabilities for machine learning, all
fairly and faithfully with fully reproducible results.
</p>
<a href="http://arxiv.org/abs/2012.02328" target="_blank">arXiv:2012.02328</a> [<a href="http://arxiv.org/pdf/2012.02328" target="_blank">pdf</a>]

<h2>Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. (arXiv:2012.02334v1 [cs.LG])</h2>
<h3>Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty</h3>
<p>The last few years have witnessed an increased interest in incorporating
physics-informed inductive bias in deep learning frameworks. In particular, a
growing volume of literature has been exploring ways to enforce energy
conservation while using neural networks for learning dynamics from observed
time-series data. In this work, we present a comparative analysis of the
energy-conserving neural networks - for example, deep Lagrangian network,
Hamiltonian neural network, etc. - wherein the underlying physics is encoded in
their computation graph. We focus on ten neural network models and explain the
similarities and differences between the models. We compare their performance
in 4 different physical systems. Our result highlights that using a
high-dimensional coordinate system and then imposing restrictions via explicit
constraints can lead to higher accuracy in the learned dynamics. We also point
out the possibility of leveraging some of these energy-conserving models to
design energy-based controllers.
</p>
<a href="http://arxiv.org/abs/2012.02334" target="_blank">arXiv:2012.02334</a> [<a href="http://arxiv.org/pdf/2012.02334" target="_blank">pdf</a>]

<h2>Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking. (arXiv:2012.02337v1 [cs.CV])</h2>
<h3>Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatofighi, Mathieu Salzmann, Stephen Gould</h3>
<p>Despite the recent advances in multiple object tracking (MOT), achieved by
joint detection and tracking, dealing with long occlusions remains a challenge.
This is due to the fact that such techniques tend to ignore the long-term
motion information. In this paper, we introduce a probabilistic autoregressive
motion model to score tracklet proposals by directly measuring their
likelihood. This is achieved by training our model to learn the underlying
distribution of natural tracklets. As such, our model allows us not only to
assign new detections to existing tracklets, but also to inpaint a tracklet
when an object has been lost for a long time, e.g., due to occlusion, by
sampling tracklets so as to fill the gap caused by misdetections. Our
experiments demonstrate the superiority of our approach at tracking objects in
challenging sequences; it outperforms the state of the art in most standard MOT
metrics on multiple MOT benchmark datasets, including MOT16, MOT17, and MOT20.
</p>
<a href="http://arxiv.org/abs/2012.02337" target="_blank">arXiv:2012.02337</a> [<a href="http://arxiv.org/pdf/2012.02337" target="_blank">pdf</a>]

<h2>Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v1 [cs.CV])</h2>
<h3>Edwin G. Ng, Bo Pang, Piyush Sharma, Radu Soricut</h3>
<p>Image captioning models generally lack the capability to take into account
user interest, and usually default to global descriptions that try to balance
readability, informativeness, and information overload. On the other hand, VQA
models generally lack the ability to provide long descriptive answers, while
expecting the textual question to be quite precise. We present a method to
control the concepts that an image caption should focus on, using an additional
input called the guiding text that refers to either groundable or ungroundable
concepts in the image. Our model consists of a Transformer-based multimodal
encoder that uses the guiding text together with global and object-level image
features to derive early-fusion representations used to generate the guided
caption. While models trained on Visual Genome data have an in-domain advantage
of fitting well when guided with automatic object labels, we find that guided
captioning models trained on Conceptual Captions generalize better on
out-of-domain images and guiding texts. Our human-evaluation results indicate
that attempting in-the-wild guided image captioning requires access to large,
unrestricted-domain training datasets, and that increased style diversity (even
without increasing vocabulary size) is a key factor for improved performance.
</p>
<a href="http://arxiv.org/abs/2012.02339" target="_blank">arXiv:2012.02339</a> [<a href="http://arxiv.org/pdf/2012.02339" target="_blank">pdf</a>]

<h2>Decentralized Multi-target Tracking with Multiple Quadrotors using a PHD Filter. (arXiv:2012.02340v1 [cs.RO])</h2>
<h3>Aniket Shirsat, Spring Berman</h3>
<p>We consider a scenario in which a group of quadrotors is tasked at tracking
multiple stationary targets in an unknown, bounded environment. The quadrotors
search for targets along a spatial grid overlaid on the environment while
performing a random walk on this grid modeled by a discrete-time discrete-state
(DTDS) Markov chain. The quadrotors can transmit their estimates of the target
locations to other quadrotors that occupy their current location on the grid;
thus, their communication network is time-varying and not necessarily
connected. We model the search procedure as a renewal-reward process on the
underlying DTDS Markov chain. To accommodate changes in the set of targets
observed by each quadrotor as it explores the environment, along with
uncertainties in the quadrotors' measurements of the targets, we formulate the
tracking problem in terms of Random Finite Sets (RFS). The quadrotors use
RFS-based Probability Hypothesis Density (PHD) filters to estimate the number
of targets and their locations. We present a theoretical estimation framework,
based on the Gaussian Mixture formulation of the PHD filter, and preliminary
simulation results toward extending existing approaches for RFS-based
multi-target tracking to a decentralized multi-robot strategy for multi-target
tracking. We validate this approach with simulations of multi-target tracking
scenarios with different densities of robots and targets, and we evaluate the
average time required for the robots in each scenario to reach agreement on a
common set of targets.
</p>
<a href="http://arxiv.org/abs/2012.02340" target="_blank">arXiv:2012.02340</a> [<a href="http://arxiv.org/pdf/2012.02340" target="_blank">pdf</a>]

<h2>Divide and Learn: A Divide and Conquer Approach for Predict+Optimize. (arXiv:2012.02342v1 [cs.LG])</h2>
<h3>Ali Ugur Guler, Emir Demirovic, Jeffrey Chan, James Bailey, Christopher Leckie, Peter J. Stuckey</h3>
<p>The predict+optimize problem combines machine learning ofproblem coefficients
with a combinatorial optimization prob-lem that uses the predicted
coefficients. While this problemcan be solved in two separate stages, it is
better to directlyminimize the optimization loss. However, this requires
dif-ferentiating through a discrete, non-differentiable combina-torial
function. Most existing approaches use some form ofsurrogate gradient.
Demirovicet alshowed how to directlyexpress the loss of the optimization
problem in terms of thepredicted coefficients as a piece-wise linear function.
How-ever, their approach is restricted to optimization problemswith a dynamic
programming formulation. In this work wepropose a novel divide and conquer
algorithm to tackle op-timization problems without this restriction and predict
itscoefficients using the optimization loss. We also introduce agreedy version
of this approach, which achieves similar re-sults with less computation. We
compare our approach withother approaches to the predict+optimize problem and
showwe can successfully tackle some hard combinatorial problemsbetter than
other predict+optimize methods.
</p>
<a href="http://arxiv.org/abs/2012.02342" target="_blank">arXiv:2012.02342</a> [<a href="http://arxiv.org/pdf/2012.02342" target="_blank">pdf</a>]

<h2>ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v1 [cs.CV])</h2>
<h3>Takumi Kimura, Takashi Matsubara, Kuniaki Uehara</h3>
<p>A point cloud serves as a representation of the surface of a
three-dimensional shape. Deep generative models have been adapted to model
their variations typically by a map from a ball-like set of latent variables.
However, previous approaches have not paid much attention to the topological
structure of a point cloud; a continuous map cannot express the varying number
of holes and intersections. Moreover, a point cloud is often composed of
multiple subparts, and it is also hardly expressed. In this paper, we propose
ChartPointFlow, which is a flow-based generative model with multiple latent
labels. By maximizing the mutual information, a map conditioned by a label is
assigned to a continuous subset of a given point cloud, like a chart of a
manifold. This enables our proposed model to preserve the topological structure
with clear boundaries, while previous approaches tend to suffer from blurs and
to fail in generating holes. Experimental results demonstrate that
ChartPointFlow achieves the state-of-the-art performance in generation and
reconstruction among sampling-based point cloud generators.
</p>
<a href="http://arxiv.org/abs/2012.02346" target="_blank">arXiv:2012.02346</a> [<a href="http://arxiv.org/pdf/2012.02346" target="_blank">pdf</a>]

<h2>Self-Supervised VQA: Answering Visual Questions using Images and Captions. (arXiv:2012.02356v1 [cs.CV])</h2>
<h3>Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Chitta Baral</h3>
<p>Methodologies for training VQA models assume the availability of datasets
with human-annotated Image-Question-Answer(I-Q-A) triplets for training. This
has led to a heavy reliance and overfitting on datasets and a lack of
generalization to new types of questions and scenes. Moreover, these datasets
exhibit annotator subjectivity, biases, and errors, along with linguistic
priors, which percolate into VQA models trained on such samples. We study
whether models can be trained without any human-annotated Q-A pairs, but only
with images and associated text captions which are descriptive and less
subjective. We present a method to train models with procedurally generated Q-A
pairs from captions using techniques, such as templates and annotation
frameworks like QASRL. As most VQA models rely on dense and costly object
annotations extracted from object detectors, we propose spatial-pyramid image
patches as a simple but effective alternative to object bounding boxes, and
demonstrate that our method uses fewer human annotations. We benchmark on
VQA-v2, GQA, and on VQA-CP which contains a softer version of label shift. Our
methods surpass prior supervised methods on VQA-CP and are competitive with
methods without object features in fully supervised setting.
</p>
<a href="http://arxiv.org/abs/2012.02356" target="_blank">arXiv:2012.02356</a> [<a href="http://arxiv.org/pdf/2012.02356" target="_blank">pdf</a>]

<h2>Multimodal Privacy-preserving Mood Prediction from Mobile Data: A Preliminary Study. (arXiv:2012.02359v1 [cs.LG])</h2>
<h3>Terrance Liu, Paul Pu Liang, Michal Muszynski, Ryo Ishii, David Brent, Randy Auerbach, Nicholas Allen, Louis-Philippe Morency</h3>
<p>Mental health conditions remain under-diagnosed even in countries with common
access to advanced medical care. The ability to accurately and efficiently
predict mood from easily collectible data has several important implications
towards the early detection and intervention of mental health disorders. One
promising data source to help monitor human behavior is from daily smartphone
usage. However, care must be taken to summarize behaviors without identifying
the user through personal (e.g., personally identifiable information) or
protected attributes (e.g., race, gender). In this paper, we study behavioral
markers or daily mood using a recent dataset of mobile behaviors from high-risk
adolescent populations. Using computational models, we find that multimodal
modeling of both text and app usage features is highly predictive of daily mood
over each modality alone. Furthermore, we evaluate approaches that reliably
obfuscate user identity while remaining predictive of daily mood. By combining
multimodal representations with privacy-preserving learning, we are able to
push forward the performance-privacy frontier as compared to unimodal
approaches.
</p>
<a href="http://arxiv.org/abs/2012.02359" target="_blank">arXiv:2012.02359</a> [<a href="http://arxiv.org/pdf/2012.02359" target="_blank">pdf</a>]

<h2>Deep Learning for Medical Anomaly Detection -- A Survey. (arXiv:2012.02364v1 [cs.LG])</h2>
<h3>Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes</h3>
<p>Machine learning-based medical anomaly detection is an important problem that
has been extensively studied. Numerous approaches have been proposed across
various medical application domains and we observe several similarities across
these distinct applications. Despite this comparability, we observe a lack of
structured organisation of these diverse research applications such that their
advantages and limitations can be studied. The principal aim of this survey is
to provide a thorough theoretical analysis of popular deep learning techniques
in medical anomaly detection. In particular, we contribute a coherent and
systematic review of state-of-the-art techniques, comparing and contrasting
their architectural differences as well as training algorithms. Furthermore, we
provide a comprehensive overview of deep model interpretation strategies that
can be used to interpret model decisions. In addition, we outline the key
limitations of existing deep medical anomaly detection techniques and propose
key research directions for further investigation.
</p>
<a href="http://arxiv.org/abs/2012.02364" target="_blank">arXiv:2012.02364</a> [<a href="http://arxiv.org/pdf/2012.02364" target="_blank">pdf</a>]

<h2>DenserNet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation. (arXiv:2012.02366v1 [cs.CV])</h2>
<h3>Dongfang Liu, Yiming Cui, Liqi Yan, Christos Mousas, Baijian Yang, Yingjie Chen</h3>
<p>In this work, we introduce a Denser Feature Network (DenserNet) for visual
localization. Our work provides three principal contributions. First, we
develop a convolutional neural network (CNN) architecture which aggregates
feature maps at different semantic levels for image representations. Using
denser feature maps, our method can produce more keypoint features and increase
image retrieval accuracy. Second, our model is trained end-to-end without
pixel-level annotation other than positive and negative GPS-tagged image pairs.
We use a weakly supervised triplet ranking loss to learn discriminative
features and encourage keypoint feature repeatability for image representation.
Finally, our method is computationally efficient as our architecture has shared
features and parameters during computation. Our method can perform accurate
large-scale localization under challenging conditions while remaining the
computational constraint. Extensive experiment results indicate that our method
sets a new state-of-the-art on four challenging large-scale localization
benchmarks and three image retrieval benchmarks.
</p>
<a href="http://arxiv.org/abs/2012.02366" target="_blank">arXiv:2012.02366</a> [<a href="http://arxiv.org/pdf/2012.02366" target="_blank">pdf</a>]

<h2>Optical Wavelength Guided Self-Supervised Feature Learning For Galaxy Cluster Richness Estimate. (arXiv:2012.02368v1 [cs.CV])</h2>
<h3>Gongbo Liang, Yuanyuan Su, Sheng-Chieh Lin, Yu Zhang, Yuanyuan Zhang, Nathan Jacobs</h3>
<p>Most galaxies in the nearby Universe are gravitationally bound to a cluster
or group of galaxies. Their optical contents, such as optical richness, are
crucial for understanding the co-evolution of galaxies and large-scale
structures in modern astronomy and cosmology. The determination of optical
richness can be challenging. We propose a self-supervised approach for
estimating optical richness from multi-band optical images. The method uses the
data properties of the multi-band optical images for pre-training, which
enables learning feature representations from a large but unlabeled dataset. We
apply the proposed method to the Sloan Digital Sky Survey. The result shows our
estimate of optical richness lowers the mean absolute error and intrinsic
scatter by 11.84% and 20.78%, respectively, while reducing the need for labeled
training data by up to 60%. We believe the proposed method will benefit
astronomy and cosmology, where a large number of unlabeled multi-band images
are available, but acquiring image labels is costly.
</p>
<a href="http://arxiv.org/abs/2012.02368" target="_blank">arXiv:2012.02368</a> [<a href="http://arxiv.org/pdf/2012.02368" target="_blank">pdf</a>]

<h2>Scale-aware Insertion of Virtual Objects in Monocular Videos. (arXiv:2012.02371v1 [cs.CV])</h2>
<h3>Songhai Zhang, Xiangli Li, Yingtian Liu, Hongbo Fu</h3>
<p>In this paper, we propose a scale-aware method for inserting virtual objects
with proper sizes into monocular videos. To tackle the scale ambiguity problem
of geometry recovery from monocular videos, we estimate the global scale
objects in a video with a Bayesian approach incorporating the size priors of
objects, where the scene objects sizes should strictly conform to the same
global scale and the possibilities of global scales are maximized according to
the size distribution of object categories. To do so, we propose a dataset of
sizes of object categories: Metric-Tree, a hierarchical representation of sizes
of more than 900 object categories with the corresponding images. To handle the
incompleteness of objects recovered from videos, we propose a novel scale
estimation method that extracts plausible dimensions of objects for scale
optimization. Experiments have shown that our method for scale estimation
performs better than the state-of-the-art methods, and has considerable
validity and robustness for different video scenes. Metric-Tree has been made
available at: https://metric-tree.github.io
</p>
<a href="http://arxiv.org/abs/2012.02371" target="_blank">arXiv:2012.02371</a> [<a href="http://arxiv.org/pdf/2012.02371" target="_blank">pdf</a>]

<h2>CIT-GAN: Cyclic Image Translation Generative Adversarial Network With Application in Iris Presentation Attack Detection. (arXiv:2012.02374v1 [cs.CV])</h2>
<h3>Shivangi Yadav, Arun Ross</h3>
<p>In this work, we propose a novel Cyclic Image Translation Generative
Adversarial Network (CIT-GAN) for multi-domain style transfer. To facilitate
this, we introduce a Styling Network that has the capability to learn style
characteristics of each domain represented in the training dataset. The Styling
Network helps the generator to drive the translation of images from a source
domain to a reference domain and generate synthetic images with style
characteristics of the reference domain. The learned style characteristics for
each domain depend on both the style loss and domain classification loss. This
induces variability in style characteristics within each domain. The proposed
CIT-GAN is used in the context of iris presentation attack detection (PAD) to
generate synthetic presentation attack (PA) samples for classes that are
under-represented in the training set. Evaluation using current
state-of-the-art iris PAD methods demonstrates the efficacy of using such
synthetically generated PA samples for training PAD methods. Further, the
quality of the synthetically generated samples is evaluated using Frechet
Inception Distance (FID) score. Results show that the quality of synthetic
images generated by the proposed method is superior to that of other competing
methods, including StarGan.
</p>
<a href="http://arxiv.org/abs/2012.02374" target="_blank">arXiv:2012.02374</a> [<a href="http://arxiv.org/pdf/2012.02374" target="_blank">pdf</a>]

<h2>Generator Pyramid for High-Resolution Image Inpainting. (arXiv:2012.02381v1 [cs.CV])</h2>
<h3>Leilei Cao, Tong Yang, Yixu Wang, Bo Yan, Yandong Guo</h3>
<p>Inpainting high-resolution images with large holes challenges existing deep
learning based image inpainting methods. We present a novel framework --
PyramidFill for high-resolution image inpainting task, which explicitly
disentangles content completion and texture synthesis. PyramidFill attempts to
complete the content of unknown regions in a lower-resolution image, and
synthesis the textures of unknown regions in a higher-resolution image,
progressively. Thus, our model consists of a pyramid of fully convolutional
GANs, wherein the content GAN is responsible for completing contents in the
lowest-resolution masked image, and each texture GAN is responsible for
synthesizing textures in a higher-resolution image. Since completing contents
and synthesising textures demand different abilities from generators, we
customize different architectures for the content GAN and texture GAN.
Experiments on multiple datasets including CelebA-HQ, Places2 and a new natural
scenery dataset (NSHQ) with different resolutions demonstrate that PyramidFill
generates higher-quality inpainting results than the state-of-the-art methods.
To better assess high-resolution image inpainting methods, we will release
NSHQ, high-quality natural scenery images with high-resolution
1920$\times$1080.
</p>
<a href="http://arxiv.org/abs/2012.02381" target="_blank">arXiv:2012.02381</a> [<a href="http://arxiv.org/pdf/2012.02381" target="_blank">pdf</a>]

<h2>Self-supervised Learning of Pixel-wise Anatomical Embeddings in Radiological Images. (arXiv:2012.02383v1 [cs.CV])</h2>
<h3>Ke Yan, Jinzheng Cai, Dakai Jin, Shun Miao, Adam P. Harrison, Dazhou Guo, Youbao Tang, Jing Xiao, Jingjing Lu, Le Lu</h3>
<p>Radiological images such as computed tomography (CT) and X-rays render
anatomy with intrinsic structures. Being able to reliably locate the same
anatomical or semantic structure across varying images is a fundamental task in
medical image analysis. In principle it is possible to use landmark detection
or semantic segmentation for this task, but to work well these require large
numbers of labeled data for each anatomical structure and sub-structure of
interest. A more universal approach would discover the intrinsic structure from
unlabeled images. We introduce such an approach, called Self-supervised
Anatomical eMbedding (SAM). SAM generates semantic embeddings for each image
pixel that describes its anatomical location or body part. To produce such
embeddings, we propose a pixel-level contrastive learning framework. A
coarse-to-fine strategy ensures both global and local anatomical information
are encoded. Negative sample selection strategies are designed to enhance the
discriminability among different body parts. Using SAM, one can label any point
of interest on a template image, and then locate the same body part in other
images by simple nearest neighbor searching. We demonstrate the effectiveness
of SAM in multiple tasks with 2D and 3D image modalities. On a chest CT dataset
with 19 landmarks, SAM outperforms widely-used registration algorithms while
being 200 times faster. On two X-ray datasets, SAM, with only one labeled
template image, outperforms supervised methods trained on 50 labeled images. We
also apply SAM on whole-body follow-up lesion matching in CT and obtain an
accuracy of 91%.
</p>
<a href="http://arxiv.org/abs/2012.02383" target="_blank">arXiv:2012.02383</a> [<a href="http://arxiv.org/pdf/2012.02383" target="_blank">pdf</a>]

<h2>A Variant of Gradient Descent Algorithm Based on Gradient Averaging. (arXiv:2012.02387v1 [cs.LG])</h2>
<h3>Saugata Purkayastha, Sukannya Purkayastha</h3>
<p>In this work, we propose a new optimizer, Grad-Avg to optimize error
functions. We establish the convergence of the sequence of iterates of Grad-Avg
mathematically to a minimizer (under boundedness assumption). We apply Grad-Avg
along with some of the popular optimizers on regression as well as
classification tasks. In regression tasks, it is observed that the behaviour of
Grad-Avg is almost identical with Stochastic Gradient Descent (SGD). We present
a mathematical justification of this fact. In case of classification tasks, it
is observed that the performance of Grad-Avg can be enhanced by suitably
scaling the parameters. Experimental results demonstrate that Grad-Avg
converges faster than the other state-of-the-art optimizers for the
classification task on two benchmark datasets.
</p>
<a href="http://arxiv.org/abs/2012.02387" target="_blank">arXiv:2012.02387</a> [<a href="http://arxiv.org/pdf/2012.02387" target="_blank">pdf</a>]

<h2>P3-LOAM: PPP/LiDAR Loosely Coupled SLAM with Accurate Covariance Estimation and Robust RAIM in Urban Canyon Environment. (arXiv:2012.02399v1 [cs.RO])</h2>
<h3>Tao Li, Ling Pei, Yan Xiang, Qi Wu, Songpengcheng Xia, Lihao Tao, Wenxian Yu</h3>
<p>Light Detection and Ranging (LiDAR) based Simultaneous Localization and
Mapping (SLAM) has drawn increasing interests in autonomous driving. However,
LiDAR-SLAM suffers from accumulating errors which can be significantly
mitigated by Global Navigation Satellite System (GNSS). Precise Point
Positioning (PPP), an accurate GNSS operation mode independent of base
stations, gains more popularity in unmanned systems. Considering the features
of the two technologies, LiDAR-SLAM and PPP, this paper proposes a SLAM system,
namely P3-LOAM (PPP based LiDAR Odometry and Mapping) which couples LiDAR-SLAM
and PPP. For better integration, we derive LiDAR-SLAM positioning covariance by
using Singular Value Decomposition (SVD) Jacobian model, since SVD provides an
explicit analytic solution of Iterative Closest Point (ICP), which is a key
issue in LiDAR-SLAM. A novel method is then proposed to evaluate the estimated
LiDAR-SLAM covariance. In addition, to increase the reliability of GNSS in
urban canyon environment, we develop a LiDAR-SLAM assisted GNSS Receiver
Autonomous Integrity Monitoring (RAIM) algorithm. Finally, we validate
P$^3$-LOAM with UrbanNav, a challenging public dataset in urban canyon
environment. Comprehensive test results prove that P3-LOAM outperforms
benchmarks such as Single Point Positioning (SPP), PPP, LeGO-LOAM, SPP-LOAM,
and loosely coupled navigation system proposed by the publisher of UrbanNav in
terms of accuracy and availability.
</p>
<a href="http://arxiv.org/abs/2012.02399" target="_blank">arXiv:2012.02399</a> [<a href="http://arxiv.org/pdf/2012.02399" target="_blank">pdf</a>]

<h2>PeR-ViS: Person Retrieval in Video Surveillance using Semantic Description. (arXiv:2012.02408v1 [cs.CV])</h2>
<h3>Parshwa Shah, Arpit Garg, Vandit Gajjar</h3>
<p>A person is usually characterized by descriptors like age, gender, height,
cloth type, pattern, color, etc. Such descriptors are known as attributes
and/or soft-biometrics. They link the semantic gap between a person's
description and retrieval in video surveillance. Retrieving a specific person
with the query of semantic description has an important application in video
surveillance. Using computer vision to fully automate the person retrieval task
has been gathering interest within the research community. However, the
Current, trend mainly focuses on retrieving persons with image-based queries,
which have major limitations for practical usage. Instead of using an image
query, in this paper, we study the problem of person retrieval in video
surveillance with a semantic description. To solve this problem, we develop a
deep learning-based cascade filtering approach (PeR-ViS), which uses Mask R-CNN
[14] (person detection and instance segmentation) and DenseNet-161 [16]
(soft-biometric classification). On the standard person retrieval dataset of
SoftBioSearch [6], we achieve 0.566 Average IoU and 0.792 %w $IoU &gt; 0.4$,
surpassing the current state-of-the-art by a large margin. We hope our simple,
reproducible, and effective approach will help ease future research in the
domain of person retrieval in video surveillance. The source code and
pretrained weights available at https://parshwa1999.github.io/PeR-ViS/.
</p>
<a href="http://arxiv.org/abs/2012.02408" target="_blank">arXiv:2012.02408</a> [<a href="http://arxiv.org/pdf/2012.02408" target="_blank">pdf</a>]

<h2>When does gradient descent with logistic loss find interpolating two-layer networks?. (arXiv:2012.02409v1 [stat.ML])</h2>
<h3>Niladri S. Chatterji, Philip M. Long, Peter L. Bartlett</h3>
<p>We study the training of finite-width two-layer smoothed ReLU networks for
binary classification using the logistic loss. We show that gradient descent
drives the training loss to zero if the initial loss is small enough. When the
data satisfies certain cluster and separation conditions and the network is
wide enough, we show that one step of gradient descent reduces the loss
sufficiently that the first result applies. In contrast, all past analyses of
fixed-width networks that we know do not guarantee that the training loss goes
to zero.
</p>
<a href="http://arxiv.org/abs/2012.02409" target="_blank">arXiv:2012.02409</a> [<a href="http://arxiv.org/pdf/2012.02409" target="_blank">pdf</a>]

<h2>Universal Approximation Property of Neural Ordinary Differential Equations. (arXiv:2012.02414v1 [cs.LG])</h2>
<h3>Takeshi Teshima, Koichi Tojo, Masahiro Ikeda, Isao Ishikawa, Kenta Oono</h3>
<p>Neural ordinary differential equations (NODEs) is an invertible neural
network architecture promising for its free-form Jacobian and the availability
of a tractable Jacobian determinant estimator. Recently, the representation
power of NODEs has been partly uncovered: they form an $L^p$-universal
approximator for continuous maps under certain conditions. However, the
$L^p$-universality may fail to guarantee an approximation for the entire input
domain as it may still hold even if the approximator largely differs from the
target function on a small region of the input space. To further uncover the
potential of NODEs, we show their stronger approximation property, namely the
$\sup$-universality for approximating a large class of diffeomorphisms. It is
shown by leveraging a structure theorem of the diffeomorphism group, and the
result complements the existing literature by establishing a fairly large set
of mappings that NODEs can approximate with a stronger guarantee.
</p>
<a href="http://arxiv.org/abs/2012.02414" target="_blank">arXiv:2012.02414</a> [<a href="http://arxiv.org/pdf/2012.02414" target="_blank">pdf</a>]

<h2>Autonomous Navigation with Mobile Robots using Deep Learning and the Robot Operating System. (arXiv:2012.02417v1 [cs.RO])</h2>
<h3>Anh Nguyen, Quang D. Tran</h3>
<p>Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.
</p>
<a href="http://arxiv.org/abs/2012.02417" target="_blank">arXiv:2012.02417</a> [<a href="http://arxiv.org/pdf/2012.02417" target="_blank">pdf</a>]

<h2>Planning from Pixels using Inverse Dynamics Models. (arXiv:2012.02419v1 [cs.LG])</h2>
<h3>Keiran Paster, Sheila A. McIlraith, Jimmy Ba</h3>
<p>Learning task-agnostic dynamics models in high-dimensional observation spaces
can be challenging for model-based RL agents. We propose a novel way to learn
latent world models by learning to predict sequences of future actions
conditioned on task completion. These task-conditioned models adaptively focus
modeling capacity on task-relevant dynamics, while simultaneously serving as an
effective heuristic for planning with sparse rewards. We evaluate our method on
challenging visual goal completion tasks and show a substantial increase in
performance compared to prior model-free approaches.
</p>
<a href="http://arxiv.org/abs/2012.02419" target="_blank">arXiv:2012.02419</a> [<a href="http://arxiv.org/pdf/2012.02419" target="_blank">pdf</a>]

<h2>Constrained Risk-Averse Markov Decision Processes. (arXiv:2012.02423v1 [cs.AI])</h2>
<h3>Mohamadreza Ahmadi, Ugo Rosolia, Michel D. Ingham, Richard M. Murray, Aaron D. Ames</h3>
<p>We consider the problem of designing policies for Markov decision processes
(MDPs) with dynamic coherent risk objectives and constraints. We begin by
formulating the problem in a Lagrangian framework. Under the assumption that
the risk objectives and constraints can be represented by a Markov risk
transition mapping, we propose an optimization-based method to synthesize
Markovian policies that lower-bound the constrained risk-averse problem. We
demonstrate that the formulated optimization problems are in the form of
difference convex programs (DCPs) and can be solved by the disciplined
convex-concave programming (DCCP) framework. We show that these results
generalize linear programs for constrained MDPs with total discounted expected
costs and constraints. Finally, we illustrate the effectiveness of the proposed
method with numerical experiments on a rover navigation problem involving
conditional-value-at-risk (CVaR) and entropic-value-at-risk (EVaR) coherent
risk measures.
</p>
<a href="http://arxiv.org/abs/2012.02423" target="_blank">arXiv:2012.02423</a> [<a href="http://arxiv.org/pdf/2012.02423" target="_blank">pdf</a>]

<h2>Non-monotone risk functions for learning. (arXiv:2012.02424v1 [stat.ML])</h2>
<h3>Matthew J. Holland</h3>
<p>In this paper we consider generalized classes of potentially non-monotone
risk functions for use as evaluation metrics in learning tasks. The resulting
risks are in general non-convex and non-smooth, which makes both the
computational and inferential sides of the learning problem difficult. For
random losses belonging to any Banach space, we obtain sufficient conditions
for the risk functions to be weakly convex, and to admit unbiased stochastic
directional derivatives. We then use recent work on stochastic optimization of
weakly convex functionals to obtain non-asymptotic guarantees of
near-stationarity for Hilbert hypothesis classes, under assumptions that are
weak enough to capture a wide variety of feedback distributions, including
potentially heavy-tailed losses and gradients.
</p>
<a href="http://arxiv.org/abs/2012.02424" target="_blank">arXiv:2012.02424</a> [<a href="http://arxiv.org/pdf/2012.02424" target="_blank">pdf</a>]

<h2>Spatial-Temporal Alignment Network for Action Recognition and Detection. (arXiv:2012.02426v1 [cs.CV])</h2>
<h3>Junwei Liang, Liangliang Cao, Xuehan Xiong, Ting Yu, Alexander Hauptmann</h3>
<p>This paper studies how to introduce viewpoint-invariant feature
representations that can help action recognition and detection. Although we
have witnessed great progress of action recognition in the past decade, it
remains challenging yet interesting how to efficiently model the geometric
variations in large scale datasets. This paper proposes a novel
Spatial-Temporal Alignment Network (STAN) that aims to learn geometric
invariant representations for action recognition and action detection. The STAN
model is very light-weighted and generic, which could be plugged into existing
action recognition models like ResNet3D and the SlowFast with a very low extra
computational cost. We test our STAN model extensively on AVA, Kinetics-400,
AVA-Kinetics, Charades, and Charades-Ego datasets. The experimental results
show that the STAN model can consistently improve the state of the arts in both
action detection and action recognition tasks. We will release our data, models
and code.
</p>
<a href="http://arxiv.org/abs/2012.02426" target="_blank">arXiv:2012.02426</a> [<a href="http://arxiv.org/pdf/2012.02426" target="_blank">pdf</a>]

<h2>Proximal Policy Optimization Smoothed Algorithm. (arXiv:2012.02439v1 [cs.LG])</h2>
<h3>Wangshu Zhu, Andre Rosendo</h3>
<p>Proximal policy optimization (PPO) has yielded state-of-the-art results in
policy search, a subfield of reinforcement learning, with one of its key points
being the use of a surrogate objective function to restrict the step size at
each policy update. Although such restriction is helpful, the algorithm still
suffers from performance instability and optimization inefficiency from the
sudden flattening of the curve. To address this issue we present a PPO variant,
named Proximal Policy Optimization Smooth Algorithm (PPOS), and its critical
improvement is the use of a functional clipping method instead of a flat
clipping method. We compare our method with PPO and PPORB, which adopts a
rollback clipping method, and prove that our method can conduct more accurate
updates at each time step than other PPO methods. Moreover, we show that it
outperforms the latest PPO variants on both performance and stability in
challenging continuous control tasks.
</p>
<a href="http://arxiv.org/abs/2012.02439" target="_blank">arXiv:2012.02439</a> [<a href="http://arxiv.org/pdf/2012.02439" target="_blank">pdf</a>]

<h2>Mitigating Bias in Federated Learning. (arXiv:2012.02447v1 [cs.LG])</h2>
<h3>Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, Heiko Ludwig</h3>
<p>As methods to create discrimination-aware models develop, they focus on
centralized ML, leaving federated learning (FL) unexplored. FL is a rising
approach for collaborative ML, in which an aggregator orchestrates multiple
parties to train a global model without sharing their training data. In this
paper, we discuss causes of bias in FL and propose three pre-processing and
in-processing methods to mitigate bias, without compromising data privacy, a
key FL requirement. As data heterogeneity among parties is one of the
challenging characteristics of FL, we conduct experiments over several data
distributions to analyze their effects on model performance, fairness metrics,
and bias learning patterns. We conduct a comprehensive analysis of our proposed
techniques, the results demonstrating that these methods are effective even
when parties have skewed data distributions or as little as 20% of parties
employ the methods.
</p>
<a href="http://arxiv.org/abs/2012.02447" target="_blank">arXiv:2012.02447</a> [<a href="http://arxiv.org/pdf/2012.02447" target="_blank">pdf</a>]

<h2>Towards Natural Robustness Against Adversarial Examples. (arXiv:2012.02452v1 [cs.LG])</h2>
<h3>Haoyu Chu, Shikui Wei, Yao Zhao</h3>
<p>Recent studies have shown that deep neural networks are vulnerable to
adversarial examples, but most of the methods proposed to defense adversarial
examples cannot solve this problem fundamentally. In this paper, we
theoretically prove that there is an upper bound for neural networks with
identity mappings to constrain the error caused by adversarial noises. However,
in actual computations, this kind of neural network no longer holds any upper
bound and is therefore susceptible to adversarial examples. Following similar
procedures, we explain why adversarial examples can fool other deep neural
networks with skip connections. Furthermore, we demonstrate that a new family
of deep neural networks called Neural ODEs (Chen et al., 2018) holds a weaker
upper bound. This weaker upper bound prevents the amount of change in the
result from being too large. Thus, Neural ODEs have natural robustness against
adversarial examples. We evaluate the performance of Neural ODEs compared with
ResNet under three white-box adversarial attacks (FGSM, PGD, DI2-FGSM) and one
black-box adversarial attack (Boundary Attack). Finally, we show that the
natural robustness of Neural ODEs is even better than the robustness of neural
networks that are trained with adversarial training methods, such as TRADES and
YOPO.
</p>
<a href="http://arxiv.org/abs/2012.02452" target="_blank">arXiv:2012.02452</a> [<a href="http://arxiv.org/pdf/2012.02452" target="_blank">pdf</a>]

<h2>Optimising Design Verification Using Machine Learning: An Open Source Solution. (arXiv:2012.02453v1 [cs.LG])</h2>
<h3>B. Samhita Varambally, Naman Sehgal</h3>
<p>With the complexity of Integrated Circuits increasing, design verification
has become the most time consuming part of the ASIC design flow. Nearly 70% of
the SoC design cycle is consumed by verification. The most commonly used
approach to test all corner cases is through the use of Constrained Random
Verification. Random stimulus is given in order to hit all possible
combinations and test the design thoroughly. However, this approach often
requires significant human expertise to reach all corner cases. This paper
presents an alternative using Machine Learning to generate the input stimulus.
This will allow for faster thorough verification of the design with less human
intervention. Furthermore, it is proposed to use the open source verification
environment 'Cocotb'. Based on Python, it is simple, intuitive and has a vast
library of functions for machine learning applications. This makes it more
convenient to use than the bulkier approach using traditional Hardware
Verification Languages such as System Verilog or Specman E.
</p>
<a href="http://arxiv.org/abs/2012.02453" target="_blank">arXiv:2012.02453</a> [<a href="http://arxiv.org/pdf/2012.02453" target="_blank">pdf</a>]

<h2>Non-Asymptotic Analysis of Excess Risk via Empirical Risk Landscape. (arXiv:2012.02456v1 [cs.LG])</h2>
<h3>Mingyang Yi, Ruoyu Wang, Zhi-Ming Ma</h3>
<p>In this paper, we provide a unified analysis of the excess risk of the model
trained by some proper algorithms in both convex and non-convex regime. In
contrary to the existing results in the literature that depends on iteration
steps, our bounds to the excess risk do not diverge with the number of
iterations. This underscores that, at least for loss functions of certain
types, the excess risk on it can be guaranteed after a period of training. Our
technique relies on a non-asymptotic characterization of the empirical risk
landscape. To be rigorous, under the condition that the local minima of
population risk are non-degenerate, each local minimum of the smooth empirical
risk is guaranteed to generalize well. The conclusion is independent of the
convexity. Combining this with the classical optimization result, we derive
converged upper bounds to the excess risk in both convex and non-convex regime.
</p>
<a href="http://arxiv.org/abs/2012.02456" target="_blank">arXiv:2012.02456</a> [<a href="http://arxiv.org/pdf/2012.02456" target="_blank">pdf</a>]

<h2>A data-set of piercing needle through deformable objects for Deep Learning from Demonstrations. (arXiv:2012.02458v1 [cs.RO])</h2>
<h3>Hamidreza Hashempour, Kiyanoush Nazari, Fangxun Zhong, Amir Ghalamzan E.</h3>
<p>Many robotic tasks are still teleoperated since automating them is very time
consuming and expensive. Robot Learning from Demonstrations (RLfD) can reduce
programming time and cost. However, conventional RLfD approaches are not
directly applicable to many robotic tasks, e.g. robotic suturing with minimally
invasive robots, as they require a time-consuming process of designing features
from visual information. Deep Neural Networks (DNN) have emerged as useful
tools for creating complex models capturing the relationship between
high-dimensional observation space and low-level action/state space.
Nonetheless, such approaches require a dataset suitable for training
appropriate DNN models. This paper presents a dataset of inserting/piercing a
needle with two arms of da Vinci Research Kit in/through soft tissues. The
dataset consists of (1) 60 successful needle insertion trials with randomised
desired exit points recorded by 6 high-resolution calibrated cameras, (2) the
corresponding robot data, calibration parameters and (3) the commanded robot
control input where all the collected data are synchronised. The dataset is
designed for Deep-RLfD approaches. We also implemented several deep RLfD
architectures, including simple feed-forward CNNs and different Recurrent
Convolutional Networks (RCNs). Our study indicates RCNs improve the prediction
accuracy of the model despite that the baseline feed-forward CNNs successfully
learns the relationship between the visual information and the next step
control actions of the robot. The dataset, as well as our baseline
implementations of RLfD, are publicly available for bench-marking at
https://github.com/imanlab/d-lfd.
</p>
<a href="http://arxiv.org/abs/2012.02458" target="_blank">arXiv:2012.02458</a> [<a href="http://arxiv.org/pdf/2012.02458" target="_blank">pdf</a>]

<h2>Relational Pretrained Transformers towards Democratizing Data Preparation [Vision]. (arXiv:2012.02469v1 [cs.LG])</h2>
<h3>Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, Mourad Ouzzani</h3>
<p>Can AI help automate human-easy but computer-hard data preparation tasks (for
example, data cleaning, data integration, and information extraction), which
currently heavily involve data scientists, practitioners, and crowd workers? We
envision that human-easy data preparation for relational data can be automated.
To this end, we first identify the desiderata for computers to achieve
near-human intelligence for data preparation: computers need a deep-learning
architecture (or model) that can read and understand millions of tables;
computers require unsupervised learning to perform self-learning without
labeled data, and can gain knowledge from existing tasks and previous
experience; and computers desire few-shot learn-ing that can adjust to new
tasks with a few examples. Our proposal is called Relational Pretrained
Transformers (RPTs), a general frame-work for various data preparation tasks,
which typically consists of the following models/methods: (1) transformer, a
general and powerful deep-learning model, that can read tables/texts/images;(2)
masked language model for self-learning and collaborative train-ing for
transferring knowledge and experience; and (3) pattern-exploiting training that
better interprets a task from a few examples.We further present concrete RPT
architectures for three classical data preparation tasks, namely data cleaning,
entity resolution, and information extraction. We demonstrate RPTs with some
initial yet promising results. Last but not least, we identify activities that
will unleash a series of research opportunities to push forward the field of
data preparation.
</p>
<a href="http://arxiv.org/abs/2012.02469" target="_blank">arXiv:2012.02469</a> [<a href="http://arxiv.org/pdf/2012.02469" target="_blank">pdf</a>]

<h2>Photoacoustic Image Reconstruction Beyond Supervised to Compensate Limit-view and Remove Artifacts. (arXiv:2012.02472v1 [cs.CV])</h2>
<h3>Hengrong Lan, Changchun Yang, Feng Gao, Fei Gao</h3>
<p>Photoacoustic computed tomography (PACT) reconstructs the initial pressure
distribution from raw PA signals. Standard reconstruction always induces
artifacts using limited-view signals, which are influenced by limited angle
coverage of transducers, finite bandwidth, and uncertain heterogeneous
biological tissue. Recently, supervised deep learning has been used to overcome
limited-view problem that requires ground-truth. However, even full-view
sampling still induces artifacts that cannot be used to train the model. It
causes a dilemma that we could not acquire perfect ground-truth in practice. To
reduce the dependence on the quality of ground-truth, in this paper, for the
first time, we propose a beyond supervised reconstruction framework (BSR-Net)
based on deep learning to compensate the limited-view issue by feeding
limited-view position-wise data. A quarter position-wise data is fed into model
and outputs a group full-view data. Specifically, our method introduces a
residual structure, which generates beyond supervised reconstruction result,
whose artifacts are drastically reduced in the output compared to ground-truth.
Moreover, two novel losses are designed to restrain the artifacts. The
numerical and in-vivo results have demonstrated the performance of our method
to reconstruct the full-view image without artifacts.
</p>
<a href="http://arxiv.org/abs/2012.02472" target="_blank">arXiv:2012.02472</a> [<a href="http://arxiv.org/pdf/2012.02472" target="_blank">pdf</a>]

<h2>Offline Meta-level Model-based Reinforcement Learning Approach for Cold-Start Recommendation. (arXiv:2012.02476v1 [cs.LG])</h2>
<h3>Yanan Wang, Yong Ge, Li Li, Rui Chen, Tong Xu</h3>
<p>Reinforcement learning (RL) has shown great promise in optimizing long-term
user interest in recommender systems. However, existing RL-based recommendation
methods need a large number of interactions for each user to learn a robust
recommendation policy. The challenge becomes more critical when recommending to
new users who have a limited number of interactions. To that end, in this
paper, we address the cold-start challenge in the RL-based recommender systems
by proposing a meta-level model-based reinforcement learning approach for fast
user adaptation. In our approach, we learn to infer each user's preference with
a user context variable that enables recommendation systems to better adapt to
new users with few interactions. To improve adaptation efficiency, we learn to
recover the user policy and reward from only a few interactions via an inverse
reinforcement learning method to assist a meta-level recommendation agent.
Moreover, we model the interaction relationship between the user model and
recommendation agent from an information-theoretic perspective. Empirical
results show the effectiveness of the proposed method when adapting to new
users with only a single interaction sequence. We further provide a theoretical
analysis of the recommendation performance bound.
</p>
<a href="http://arxiv.org/abs/2012.02476" target="_blank">arXiv:2012.02476</a> [<a href="http://arxiv.org/pdf/2012.02476" target="_blank">pdf</a>]

<h2>Is It a Plausible Colour? UCapsNet for Image Colourisation. (arXiv:2012.02478v1 [cs.CV])</h2>
<h3>Rita Pucci, Christian Micheloni, Gian Luca Foresti, Niki Martinel</h3>
<p>Human beings can imagine the colours of a grayscale image with no particular
effort thanks to their ability of semantic feature extraction. Can an
autonomous system achieve that? Can it hallucinate plausible and vibrant
colours? This is the colourisation problem. Different from existing works
relying on convolutional neural network models pre-trained with supervision, we
cast such colourisation problem as a self-supervised learning task. We tackle
the problem with the introduction of a novel architecture based on Capsules
trained following the adversarial learning paradigm. Capsule networks are able
to extract a semantic representation of the entities in the image but loose
details about their spatial information, which is important for colourising a
grayscale image. Thus our UCapsNet structure comes with an encoding phase that
extracts entities through capsules and spatial details through convolutional
neural networks. A decoding phase merges the entity features with the spatial
features to hallucinate a plausible colour version of the input datum. Results
on the ImageNet benchmark show that our approach is able to generate more
vibrant and plausible colours than exiting solutions and achieves superior
performance than models pre-trained with supervision.
</p>
<a href="http://arxiv.org/abs/2012.02478" target="_blank">arXiv:2012.02478</a> [<a href="http://arxiv.org/pdf/2012.02478" target="_blank">pdf</a>]

<h2>A novel multi-classifier information fusion based on Dempster-Shafer theory: application to vibration-based fault detection. (arXiv:2012.02481v1 [cs.LG])</h2>
<h3>Vahid Yaghoubi, Liangliang Cheng, Wim Van Paepegem, Mathias Kersemans</h3>
<p>Achieving a high prediction rate is a crucial task in fault detection.
Although various classification procedures are available, none of them can give
high accuracy in all applications. Therefore, in this paper, a novel
multi-classifier fusion approach is developed to boost the performance of the
individual classifiers. This is acquired by using Dempster-Shafer theory (DST).
However, in cases with conflicting evidences, the DST may give
counter-intuitive results. In this regard, a preprocessing technique based on a
new metric is devised in order to measure and mitigate the conflict between the
evidences. To evaluate and validate the effectiveness of the proposed approach,
the method is applied to 15 benchmarks datasets from UCI and KEEL. Further, it
is applied for classifying polycrystalline Nickel alloy first-stage turbine
blades based on their broadband vibrational response. Through statistical
analysis with different levels of noise-to-signal ratio, and by comparing with
four state-of-the-art fusion techniques, it is shown that that the proposed
method improves the classification accuracy and outperforms the individual
classifiers.
</p>
<a href="http://arxiv.org/abs/2012.02481" target="_blank">arXiv:2012.02481</a> [<a href="http://arxiv.org/pdf/2012.02481" target="_blank">pdf</a>]

<h2>Unsupervised Adversarially-Robust Representation Learning on Graphs. (arXiv:2012.02486v1 [cs.LG])</h2>
<h3>Jiarong Xu, Junru Chen, Yang Yang, Yizhou Sun, Chunping Wang, Jiangang Lu</h3>
<p>Recent works have demonstrated that deep learning on graphs is vulnerable to
adversarial attacks, in that imperceptible perturbations on input data can lead
to dramatic performance deterioration. In this paper, we focus on the
underlying problem of learning robust representations on graphs via mutual
information. In contrast to previous works measure the task-specific robustness
based on the label space, we here take advantage of the representation space to
study a task-free robustness measure given the joint input space w.r.t graph
topology and node attributes. We formulate this problem as a constrained saddle
point optimization problem and solve it efficiently in a reduced search space.
Furthermore, we provably establish theoretical connections between our
task-free robustness measure and the robustness of downstream classifiers.
Extensive experiments demonstrate that our proposed method is able to enhance
robustness against adversarial attacks on graphs, yet even increases natural
accuracy.
</p>
<a href="http://arxiv.org/abs/2012.02486" target="_blank">arXiv:2012.02486</a> [<a href="http://arxiv.org/pdf/2012.02486" target="_blank">pdf</a>]

<h2>Compositionally Generalizable 3D Structure Prediction. (arXiv:2012.02493v1 [cs.CV])</h2>
<h3>Songfang Han, Jiayuan Gu, Kaichun Mo, Li Yi, Siyu Hu, Xuejin Chen, Hao Su</h3>
<p>Single-image 3D shape reconstruction is an important and long-standing
problem in computer vision. A plethora of existing works is constantly pushing
the state-of-the-art performance in the deep learning era. However, there
remains a much difficult and largely under-explored issue on how to generalize
the learned skills over novel unseen object categories that have very different
shape geometry distribution. In this paper, we bring in the concept of
compositional generalizability and propose a novel framework that factorizes
the 3D shape reconstruction problem into proper sub-problems, each of which is
tackled by a carefully designed neural sub-module with generalizability
guarantee. The intuition behind our formulation is that object parts (slates
and cylindrical parts), their relationships (adjacency, equal-length, and
parallelism) and shape substructures (T-junctions and a symmetric group of
parts) are mostly shared across object categories, even though the object
geometry may look very different (chairs and cabinets). Experiments on PartNet
show that we achieve superior performance than baseline methods, which
validates our problem factorization and network designs.
</p>
<a href="http://arxiv.org/abs/2012.02493" target="_blank">arXiv:2012.02493</a> [<a href="http://arxiv.org/pdf/2012.02493" target="_blank">pdf</a>]

<h2>How Many Annotators Do We Need? -- A Study on the Influence of Inter-Observer Variability on the Reliability of Automatic Mitotic Figure Assessment. (arXiv:2012.02495v1 [cs.CV])</h2>
<h3>Frauke Wilm, Christof A. Bertram, Christian Marzahl, Alexander Bartel, Taryn A. Donovan, Charles-Antoine Assenmacher, Kathrin Becker, Mark Bennett, Sarah Corner, Brieuc Cossic, Daniela Denk, Martina Dettwiler, Beatriz Garcia Gonzalez, Corinne Gurtner, Annika Lehmbecker, Sophie Merz, Stephanie Plog, Anja Schmidt, Rebecca C. Smedley, Marco Tecilla, Tuddow Thaiwong, Katharina Breininger, Matti Kiupel, Andreas Maier, Robert Klopfleisch, Marc Aubreville</h3>
<p>Density of mitotic figures in histologic sections is a prognostically
relevant characteristic for many tumours. Due to high inter-pathologist
variability, deep learning-based algorithms are a promising solution to improve
tumour prognostication. Pathologists are the gold standard for database
development, however, labelling errors may hamper development of accurate
algorithms. In the present work we evaluated the benefit of multi-expert
consensus (n = 3, 5, 7, 9, 17) on algorithmic performance. While training with
individual databases resulted in highly variable F$_1$ scores, performance was
notably increased and more consistent when using the consensus of three
annotators. Adding more annotators only resulted in minor improvements. We
conclude that databases by few pathologists with high label precision may be
the best compromise between high algorithmic performance and time investment.
</p>
<a href="http://arxiv.org/abs/2012.02495" target="_blank">arXiv:2012.02495</a> [<a href="http://arxiv.org/pdf/2012.02495" target="_blank">pdf</a>]

<h2>SAFFIRE: System for Autonomous Feature Filtering and Intelligent ROI Estimation. (arXiv:2012.02502v1 [cs.CV])</h2>
<h3>Marco Boschi, Luigi Di Stefano, Martino Alessandrini</h3>
<p>This work introduces a new framework, named SAFFIRE, to automatically extract
a dominant recurrent image pattern from a set of image samples. Such a pattern
shall be used to eliminate pose variations between samples, which is a common
requirement in many computer vision and machine learning tasks. The framework
is specialized here in the context of a machine vision system for automated
product inspection. Here, it is customary to ask the user for the
identification of an anchor pattern, to be used by the automated system to
normalize data before further processing. Yet, this is a very sensitive
operation which is intrinsically subjective and requires high expertise.
Hereto, SAFFIRE provides a unique and disruptive framework for unsupervised
identification of an optimal anchor pattern in a way which is fully transparent
to the user. SAFFIRE is thoroughly validated on several realistic case studies
for a machine vision inspection pipeline.
</p>
<a href="http://arxiv.org/abs/2012.02502" target="_blank">arXiv:2012.02502</a> [<a href="http://arxiv.org/pdf/2012.02502" target="_blank">pdf</a>]

<h2>Pose-Based Servo Control with Soft Tactile Sensing. (arXiv:2012.02504v1 [cs.RO])</h2>
<h3>Nathan F. Lepora, John Lloyd</h3>
<p>This paper describes a new way of controlling robots using soft tactile
sensors: pose-based tactile servo (PBTS) control. The basic idea is to embed a
tactile perception model for estimating the sensor pose within a servo control
loop that is applied to local object features such as edges and surfaces. PBTS
control is implemented with a soft curved optical tactile sensor (the BRL
TacTip) using a convolutional neural network trained to be insensitive to
shear. In consequence, robust and accurate controlled motion over various
complex 3D objects is attained. First, we review tactile servoing and its
relation to visual servoing, before formalising PBTS control. Then, we assess
PBTS over a range of regular and irregular objects. Finally, we reflect on the
relation to visual servo control and discuss how controlled soft touch gives a
route towards human-like dexterity in robots. A summary video is available here
https://youtu.be/12-DJeRcfn0
</p>
<a href="http://arxiv.org/abs/2012.02504" target="_blank">arXiv:2012.02504</a> [<a href="http://arxiv.org/pdf/2012.02504" target="_blank">pdf</a>]

<h2>On Detecting Data Pollution Attacks On Recommender Systems Using Sequential GANs. (arXiv:2012.02509v1 [cs.LG])</h2>
<h3>Behzad Shahrasbi, Venugopal Mani, Apoorv Reddy Arrabothu, Deepthi Sharma, Kannan Achan, Sushant Kumar</h3>
<p>Recommender systems are an essential part of any e-commerce platform.
Recommendations are typically generated by aggregating large amounts of user
data. A malicious actor may be motivated to sway the output of such recommender
systems by injecting malicious datapoints to leverage the system for financial
gain. In this work, we propose a semi-supervised attack detection algorithm to
identify the malicious datapoints. We do this by leveraging a portion of the
dataset that has a lower chance of being polluted to learn the distribution of
genuine datapoints. Our proposed approach modifies the Generative Adversarial
Network architecture to take into account the contextual information from user
activity. This allows the model to distinguish legitimate datapoints from the
injected ones.
</p>
<a href="http://arxiv.org/abs/2012.02509" target="_blank">arXiv:2012.02509</a> [<a href="http://arxiv.org/pdf/2012.02509" target="_blank">pdf</a>]

<h2>ID-Reveal: Identity-aware DeepFake Video Detection. (arXiv:2012.02512v1 [cs.CV])</h2>
<h3>Davide Cozzolino, Andreas R&#xf6;ssler, Justus Thies, Matthias Nie&#xdf;ner, Luisa Verdoliva</h3>
<p>State-of-the-art DeepFake forgery detectors are trained in a supervised
fashion to answer the question 'is this video real or fake?'. Given that their
training is typically method-specific, these approaches show poor
generalization across different types of facial manipulations, e.g., face
swapping or facial reenactment. In this work, we look at the problem from a
different perspective by focusing on the facial characteristics of a specific
identity; i.e., we want to answer the question 'Is this the person who is
claimed to be?'. To this end, we introduce ID-Reveal, a new approach that
learns temporal facial features, specific of how each person moves while
talking, by means of metric learning coupled with an adversarial training
strategy. Our method is independent of the specific type of manipulation since
it is trained only on real videos. Moreover, relying on high-level semantic
features, it is robust to widespread and disruptive forms of post-processing.
We performed a thorough experimental analysis on several publicly available
benchmarks, such as FaceForensics++, Google's DFD, and Celeb-DF. Compared to
state of the art, our method improves generalization and is more robust to
low-quality videos, that are usually spread over social networks. In
particular, we obtain an average improvement of more than 15% in terms of
accuracy for facial reenactment on high compressed videos.
</p>
<a href="http://arxiv.org/abs/2012.02512" target="_blank">arXiv:2012.02512</a> [<a href="http://arxiv.org/pdf/2012.02512" target="_blank">pdf</a>]

<h2>AuthNet: A Deep Learning based Authentication Mechanism using Temporal Facial Feature Movements. (arXiv:2012.02515v1 [cs.CV])</h2>
<h3>Mohit Raghavendra, Pravan Omprakash, B R Mukesh, Sowmya Kamath</h3>
<p>Biometric systems based on Machine learning and Deep learning are being
extensively used as authentication mechanisms in resource-constrained
environments like smartphones and other small computing devices. These
AI-powered facial recognition mechanisms have gained enormous popularity in
recent years due to their transparent, contact-less and non-invasive nature.
While they are effective to a large extent, there are ways to gain unauthorized
access using photographs, masks, glasses, etc. In this paper, we propose an
alternative authentication mechanism that uses both facial recognition and the
unique movements of that particular face while uttering a password, that is,
the temporal facial feature movements. The proposed model is not inhibited by
language barriers because a user can set a password in any language. When
evaluated on the standard MIRACL-VC1 dataset, the proposed model achieved an
accuracy of 98.1\%, underscoring its effectiveness as an effective and robust
system. The proposed method is also data efficient, since the model gave good
results even when trained with only 10 positive video samples. The competence
of the training of the network is also demonstrated by benchmarking the
proposed system against various compounded Facial recognition and Lip reading
models.
</p>
<a href="http://arxiv.org/abs/2012.02515" target="_blank">arXiv:2012.02515</a> [<a href="http://arxiv.org/pdf/2012.02515" target="_blank">pdf</a>]

<h2>A Note on Data Biases in Generative Models. (arXiv:2012.02516v1 [cs.CV])</h2>
<h3>Patrick Esser, Robin Rombach, Bj&#xf6;rn Ommer</h3>
<p>It is tempting to think that machines are less prone to unfairness and
prejudice. However, machine learning approaches compute their outputs based on
data. While biases can enter at any stage of the development pipeline, models
are particularly receptive to mirror biases of the datasets they are trained on
and therefore do not necessarily reflect truths about the world but, primarily,
truths about the data. To raise awareness about the relationship between modern
algorithms and the data that shape them, we use a conditional invertible neural
network to disentangle the dataset-specific information from the information
which is shared across different datasets. In this way, we can project the same
image onto different datasets, thereby revealing their inherent biases. We use
this methodology to (i) investigate the impact of dataset quality on the
performance of generative models, (ii) show how societal biases of datasets are
replicated by generative models, and (iii) present creative applications
through unpaired transfer between diverse datasets such as photographs, oil
portraits, and animes. Our code and an interactive demonstration are available
at https://github.com/CompVis/net2net .
</p>
<a href="http://arxiv.org/abs/2012.02516" target="_blank">arXiv:2012.02516</a> [<a href="http://arxiv.org/pdf/2012.02516" target="_blank">pdf</a>]

<h2>Kernel-convoluted Deep Neural Networks with Data Augmentation. (arXiv:2012.02521v1 [cs.LG])</h2>
<h3>Minjin Kim, Young-geun Kim, Dongha Kim, Yongdai Kim, Myunghee Cho Paik</h3>
<p>The Mixup method (Zhang et al. 2018), which uses linearly interpolated data,
has emerged as an effective data augmentation tool to improve generalization
performance and the robustness to adversarial examples. The motivation is to
curtail undesirable oscillations by its implicit model constraint to behave
linearly at in-between observed data points and promote smoothness. In this
work, we formally investigate this premise, propose a way to explicitly impose
smoothness constraints, and extend it to incorporate with implicit model
constraints. First, we derive a new function class composed of
kernel-convoluted models (KCM) where the smoothness constraint is directly
imposed by locally averaging the original functions with a kernel function.
Second, we propose to incorporate the Mixup method into KCM to expand the
domains of smoothness. In both cases of KCM and the KCM adapted with the Mixup,
we provide risk analysis, respectively, under some conditions for kernels. We
show that the upper bound of the excess risk is not slower than that of the
original function class. The upper bound of the KCM with the Mixup remains
dominated by that of the KCM if the perturbation of the Mixup vanishes faster
than \(O(n^{-1/2})\) where \(n\) is a sample size. Using CIFAR-10 and CIFAR-100
datasets, our experiments demonstrate that the KCM with the Mixup outperforms
the Mixup method in terms of generalization and robustness to adversarial
examples.
</p>
<a href="http://arxiv.org/abs/2012.02521" target="_blank">arXiv:2012.02521</a> [<a href="http://arxiv.org/pdf/2012.02521" target="_blank">pdf</a>]

<h2>Practical No-box Adversarial Attacks against DNNs. (arXiv:2012.02525v1 [cs.CV])</h2>
<h3>Qizhang Li, Yiwen Guo, Hao Chen</h3>
<p>The study of adversarial vulnerabilities of deep neural networks (DNNs) has
progressed rapidly. Existing attacks require either internal access (to the
architecture, parameters, or training set of the victim model) or external
access (to query the model). However, both the access may be infeasible or
expensive in many scenarios. We investigate no-box adversarial examples, where
the attacker can neither access the model information or the training set nor
query the model. Instead, the attacker can only gather a small number of
examples from the same problem domain as that of the victim model. Such a
stronger threat model greatly expands the applicability of adversarial attacks.
We propose three mechanisms for training with a very small dataset (on the
order of tens of examples) and find that prototypical reconstruction is the
most effective. Our experiments show that adversarial examples crafted on
prototypical auto-encoding models transfer well to a variety of image
classification and face verification models. On a commercial celebrity
recognition system held by clarifai.com, our approach significantly diminishes
the average prediction accuracy of the system to only 15.40%, which is on par
with the attack that transfers adversarial examples from a pre-trained Arcface
model.
</p>
<a href="http://arxiv.org/abs/2012.02525" target="_blank">arXiv:2012.02525</a> [<a href="http://arxiv.org/pdf/2012.02525" target="_blank">pdf</a>]

<h2>Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v1 [cs.LG])</h2>
<h3>Alex Hernandez-Garcia</h3>
<p>The renaissance of artificial neural networks was catalysed by the success of
classification models, tagged by the community with the broader term supervised
learning. The extraordinary results gave rise to a hype loaded with ambitious
promises and overstatements. Soon the community realised that the success owed
much to the availability of thousands of labelled examples. And supervised
learning went, for many, from glory to shame. Some criticised deep learning as
a whole and others proclaimed that the way forward had to be "alternatives" to
supervised learning: predictive, unsupervised, semi-supervised and, more
recently, self-supervised learning. However, these seem all brand names, rather
than actual categories of a theoretically grounded taxonomy. Moreover, the call
to banish supervised learning was motivated by the questionable claim that
humans learn with little or no supervision. Here, we review insights about
learning and supervision in nature, revisit the notion that learning is not
possible without supervision and argue that we will make better progress if we
just call it by its name.
</p>
<a href="http://arxiv.org/abs/2012.02526" target="_blank">arXiv:2012.02526</a> [<a href="http://arxiv.org/pdf/2012.02526" target="_blank">pdf</a>]

<h2>Demonstration-efficient Inverse Reinforcement Learning in Procedurally Generated Environments. (arXiv:2012.02527v1 [cs.LG])</h2>
<h3>Alessandro Sestini, Alexander Kuhnle, Andrew D. Bagdanov</h3>
<p>Deep Reinforcement Learning achieves very good results in domains where
reward functions can be manually engineered. At the same time, there is growing
interest within the community in using games based on Procedurally Content
Generation (PCG) as benchmark environments since this type of environment is
perfect for studying overfitting and generalization of agents under domain
shift. Inverse Reinforcement Learning (IRL) can instead extrapolate reward
functions from expert demonstrations, with good results even on
high-dimensional problems, however there are no examples of applying these
techniques to procedurally-generated environments. This is mostly due to the
number of demonstrations needed to find a good reward model. We propose a
technique based on Adversarial Inverse Reinforcement Learning which can
significantly decrease the need for expert demonstrations in PCG games. Through
the use of an environment with a limited set of initial seed levels, plus some
modifications to stabilize training, we show that our approach, DE-AIRL, is
demonstration-efficient and still able to extrapolate reward functions which
generalize to the fully procedural domain. We demonstrate the effectiveness of
our technique on two procedural environments, MiniGrid and DeepCrawl, for a
variety of tasks.
</p>
<a href="http://arxiv.org/abs/2012.02527" target="_blank">arXiv:2012.02527</a> [<a href="http://arxiv.org/pdf/2012.02527" target="_blank">pdf</a>]

<h2>Logic Synthesis Meets Machine Learning:Trading Exactness for Generalization. (arXiv:2012.02530v1 [cs.LG])</h2>
<h3>Shubham Rai, Walter Lau Neto, Yukio Miyasaka, Xinpei Zhang, Mingfei Yu, Qingyang Yi Masahiro Fujita, Guilherme B. Manske, Matheus F. Pontes, Leomar S. da Rosa Junior, Marilton S. de Aguiar, Paulo F. Butzen, Po-Chun Chien, Yu-Shan Huang, Hoa-Ren Wang, Jie-Hong R. Jiang, Jiaqi Gu, Zheng Zhao, Zixuan Jiang, David Z. Pan, Brunno A. de Abreu, Isac de Souza Campos, Augusto Berndt, Cristina Meinhardt, Jonata T. Carvalho, Mateus Grellert, Sergio Bampi, Aditya Lohana, Akash Kumar, Wei Zeng, Azadeh Davoodi, Rasit O. Topaloglu, Yuan Zhou, Jordan Dotzel, Yichi Zhang, Hanyu Wang, Zhiru Zhang, Valerio Tenace, Pierre-Emmanuel Gaillardon, Alan Mishchenko, Satrajit Chatterjee</h3>
<p>Logic synthesis is a fundamental step in hardware design whose goal is to
find structural representations of Boolean functions while minimizing delay and
area. If the function is completely-specified, the implementation accurately
represents the function. If the function is incompletely-specified, the
implementation has to be true only on the care set. While most of the
algorithms in logic synthesis rely on SAT and Boolean methods to exactly
implement the care set, we investigate learning in logic synthesis, attempting
to trade exactness for generalization. This work is directly related to machine
learning where the care set is the training set and the implementation is
expected to generalize on a validation set. We present learning
incompletely-specified functions based on the results of a competition
conducted at IWLS 2020. The goal of the competition was to implement 100
functions given by a set of care minterms for training, while testing the
implementation using a set of validation minterms sampled from the same
function. We make this benchmark suite available and offer a detailed
comparative analysis of the different approaches to learning
</p>
<a href="http://arxiv.org/abs/2012.02530" target="_blank">arXiv:2012.02530</a> [<a href="http://arxiv.org/pdf/2012.02530" target="_blank">pdf</a>]

<h2>DeepSym: Deep Symbol Generation and Rule Learning from Unsupervised Continuous Robot Interaction for Planning. (arXiv:2012.02532v1 [cs.RO])</h2>
<h3>Alper Ahmetoglu, M. Yunus Seker, Aysu Sayin, Serkan Bugur, Justus Piater, Erhan Oztop, Emre Ugur</h3>
<p>Autonomous discovery of discrete symbols and rules from continuous
interaction experience is a crucial building block of robot AI, but remains a
challenging problem. Solving it will overcome the limitations in scalability,
flexibility, and robustness of manually-designed symbols and rules, and will
constitute a substantial advance towards autonomous robots that can learn and
reason at abstract levels in open-ended environments. Towards this goal, we
propose a novel and general method that finds action-grounded, discrete object
and effect categories and builds probabilistic rules over them that can be used
in complex action planning. Our robot interacts with single and multiple
objects using a given action repertoire and observes the effects created in the
environment. In order to form action-grounded object, effect, and relational
categories, we employ a binarized bottleneck layer of a predictive, deep
encoder-decoder network that takes as input the image of the scene and the
action applied, and generates the resulting object displacements in the scene
(action effects) in pixel coordinates. The binary latent vector represents a
learned, action-driven categorization of objects. To distill the knowledge
represented by the neural network into rules useful for symbolic reasoning, we
train a decision tree to reproduce its decoder function. From its branches we
extract probabilistic rules and represent them in PPDDL, allowing off-the-shelf
planners to operate on the robot's sensorimotor experience. Our system is
verified in a physics-based 3d simulation environment where a robot arm-hand
system learned symbols that can be interpreted as 'rollable', 'insertable',
'larger-than' from its push and stack actions; and generated effective plans to
achieve goals such as building towers from given cubes, balls, and cups using
off-the-shelf probabilistic planners.
</p>
<a href="http://arxiv.org/abs/2012.02532" target="_blank">arXiv:2012.02532</a> [<a href="http://arxiv.org/pdf/2012.02532" target="_blank">pdf</a>]

<h2>F2Net: Learning to Focus on the Foreground for Unsupervised Video Object Segmentation. (arXiv:2012.02534v1 [cs.CV])</h2>
<h3>Daizong Liu, Dongdong Yu, Changhu Wang, Pan Zhou</h3>
<p>Although deep learning based methods have achieved great progress in
unsupervised video object segmentation, difficult scenarios (e.g., visual
similarity, occlusions, and appearance changing) are still not well-handled. To
alleviate these issues, we propose a novel Focus on Foreground Network (F2Net),
which delves into the intra-inter frame details for the foreground objects and
thus effectively improve the segmentation performance. Specifically, our
proposed network consists of three main parts: Siamese Encoder Module, Center
Guiding Appearance Diffusion Module, and Dynamic Information Fusion Module.
Firstly, we take a siamese encoder to extract the feature representations of
paired frames (reference frame and current frame). Then, a Center Guiding
Appearance Diffusion Module is designed to capture the inter-frame feature
(dense correspondences between reference frame and current frame), intra-frame
feature (dense correspondences in current frame), and original semantic feature
of current frame. Specifically, we establish a Center Prediction Branch to
predict the center location of the foreground object in current frame and
leverage the center point information as spatial guidance prior to enhance the
inter-frame and intra-frame feature extraction, and thus the feature
representation considerably focus on the foreground objects. Finally, we
propose a Dynamic Information Fusion Module to automatically select relatively
important features through three aforementioned different level features.
Extensive experiments on DAVIS2016, Youtube-object, and FBMS datasets show that
our proposed F2Net achieves the state-of-the-art performance with significant
improvement.
</p>
<a href="http://arxiv.org/abs/2012.02534" target="_blank">arXiv:2012.02534</a> [<a href="http://arxiv.org/pdf/2012.02534" target="_blank">pdf</a>]

<h2>Federated Learning with Heterogeneous Labels and Models for Mobile Activity Monitoring. (arXiv:2012.02539v1 [cs.LG])</h2>
<h3>Gautham Krishna Gudur, Satheesh K. Perepu</h3>
<p>Various health-care applications such as assisted living, fall detection,
etc., require modeling of user behavior through Human Activity Recognition
(HAR). Such applications demand characterization of insights from multiple
resource-constrained user devices using machine learning techniques for
effective personalized activity monitoring. On-device Federated Learning proves
to be an effective approach for distributed and collaborative machine learning.
However, there are a variety of challenges in addressing statistical (non-IID
data) and model heterogeneities across users. In addition, in this paper, we
explore a new challenge of interest -- to handle heterogeneities in labels
(activities) across users during federated learning. To this end, we propose a
framework for federated label-based aggregation, which leverages overlapping
information gain across activities using Model Distillation Update. We also
propose that federated transfer of model scores is sufficient rather than model
weight transfer from device to server. Empirical evaluation with the
Heterogeneity Human Activity Recognition (HHAR) dataset (with four activities
for effective elucidation of results) on Raspberry Pi 2 indicates an average
deterministic accuracy increase of at least ~11.01%, thus demonstrating the
on-device capabilities of our proposed framework.
</p>
<a href="http://arxiv.org/abs/2012.02539" target="_blank">arXiv:2012.02539</a> [<a href="http://arxiv.org/pdf/2012.02539" target="_blank">pdf</a>]

<h2>Crop Classification under Varying Cloud Cover with Neural Ordinary Differential Equations. (arXiv:2012.02542v1 [cs.CV])</h2>
<h3>Nando Metzger, Mehmet Ozgur Turkoglu, Stefano D&#x27;Aronco, Jan Dirk Wegner, Konrad Schindler</h3>
<p>Optical satellite sensors cannot see the Earth's surface through clouds.
Despite the periodic revisit cycle, image sequences acquired by Earth
observation satellites are therefore irregularly sampled in time.
State-of-the-art methods for crop classification (and other time series
analysis tasks) rely on techniques that implicitly assume regular temporal
spacing between observations, such as recurrent neural networks (RNNs). We
propose to use neural ordinary differential equations (NODEs) in combination
with RNNs to classify crop types in irregularly spaced image sequences. The
resulting ODE-RNN models consist of two steps: an update step, where a
recurrent unit assimilates new input data into the model's hidden state; and a
prediction step, in which NODE propagates the hidden state until the next
observation arrives. The prediction step is based on a continuous
representation of the latent dynamics, which has several advantages. At the
conceptual level, it is a more natural way to describe the mechanisms that
govern the phenological cycle. From a practical point of view, it makes it
possible to sample the system state at arbitrary points in time, such that one
can integrate observations whenever they are available, and extrapolate beyond
the last observation. Our experiments show that ODE-RNN indeed improves
classification accuracy over common baselines such as LSTM, GRU, and temporal
convolution. The gains are most prominent in the challenging scenario where
only few observations are available (i.e., frequent cloud cover). Moreover, we
show that the ability to extrapolate translates to better classification
performance early in the season, which is important for forecasting.
</p>
<a href="http://arxiv.org/abs/2012.02542" target="_blank">arXiv:2012.02542</a> [<a href="http://arxiv.org/pdf/2012.02542" target="_blank">pdf</a>]

<h2>Boosting offline handwritten text recognition in historical documents with few labeled lines. (arXiv:2012.02544v1 [cs.CV])</h2>
<h3>Jos&#xe9; Carlos Aradillas, Juan Jos&#xe9; Murillo-Fuentes, Pablo M. Olmos</h3>
<p>In this paper, we face the problem of offline handwritten text recognition
(HTR) in historical documents when few labeled samples are available and some
of them contain errors in the train set. Three main contributions are
developed. First we analyze how to perform transfer learning (TL) from a
massive database to a smaller historical database, analyzing which layers of
the model need a fine-tuning process. Second, we analyze methods to efficiently
combine TL and data augmentation (DA). Finally, an algorithm to mitigate the
effects of incorrect labelings in the training set is proposed. The methods are
analyzed over the ICFHR 2018 competition database, Washington and Parzival.
Combining all these techniques, we demonstrate a remarkable reduction of CER
(up to 6% in some cases) in the test set with little complexity overhead.
</p>
<a href="http://arxiv.org/abs/2012.02544" target="_blank">arXiv:2012.02544</a> [<a href="http://arxiv.org/pdf/2012.02544" target="_blank">pdf</a>]

<h2>Effect of the initial configuration of weights on the training and function of artificial neural networks. (arXiv:2012.02550v1 [cs.LG])</h2>
<h3>R. J. Jesus, M. L. Antunes, R. A. da Costa, S. N. Dorogovtsev, J. F. F. Mendes, R. L. Aguiar</h3>
<p>The function and performance of neural networks is largely determined by the
evolution of their weights and biases in the process of training, starting from
the initial configuration of these parameters to one of the local minima of the
loss function. We perform the quantitative statistical characterization of the
deviation of the weights of two-hidden-layer ReLU networks of various sizes
trained via Stochastic Gradient Descent (SGD) from their initial random
configuration. We compare the evolution of the distribution function of this
deviation with the evolution of the loss during training. We observed that
successful training via SGD leaves the network in the close neighborhood of the
initial configuration of its weights. For each initial weight of a link we
measured the distribution function of the deviation from this value after
training and found how the moments of this distribution and its peak depend on
the initial weight. We explored the evolution of these deviations during
training and observed an abrupt increase within the overfitting region. This
jump occurs simultaneously with a similarly abrupt increase recorded in the
evolution of the loss function. Our results suggest that SGD's ability to
efficiently find local minima is restricted to the vicinity of the random
initial configuration of weights.
</p>
<a href="http://arxiv.org/abs/2012.02550" target="_blank">arXiv:2012.02550</a> [<a href="http://arxiv.org/pdf/2012.02550" target="_blank">pdf</a>]

<h2>Deep Learning for Wrist Fracture Detection: Are We There Yet?. (arXiv:2012.02577v1 [cs.CV])</h2>
<h3>Abu Mohammed Raisuddin, Elias Vaattovaara, Mika Nevalainen, Marko Nikki, Elina J&#xe4;rvenp&#xe4;&#xe4;, Kaisa Makkonen, Pekka Pinola, Tuula Palsio, Arttu Niemensivu, Osmo Tervonen, Aleksei Tiulpin</h3>
<p>Wrist Fracture is the most common type of fracture with a high incidence
rate. Conventional radiography (i.e. X-ray imaging) is used for wrist fracture
detection routinely, but occasionally fracture delineation poses issues and an
additional confirmation by computed tomography (CT) is needed for diagnosis.
Recent advances in the field of Deep Learning (DL), a subfield of Artificial
Intelligence (AI), have shown that wrist fracture detection can be automated
using Convolutional Neural Networks. However, previous studies did not pay
close attention to the difficult cases which can only be confirmed via CT
imaging. In this study, we have developed and analyzed a state-of-the-art
DL-based pipeline for wrist (distal radius) fracture detection -- DeepWrist,
and evaluated it against one general population test set, and one challenging
test set comprising only cases requiring confirmation by CT. Our results reveal
that a typical state-of-the-art approach, such as DeepWrist, while having a
near-perfect performance on the general independent test set, has a
substantially lower performance on the challenging test set -- average
precision of 0.99 (0.99-0.99) vs 0.64 (0.46-0.83), respectively. Similarly, the
area under the ROC curve was of 0.99 (0.98-0.99) vs 0.84 (0.72-0.93),
respectively. Our findings highlight the importance of a meticulous analysis of
DL-based models before clinical use, and unearth the need for more challenging
settings for testing medical AI systems.
</p>
<a href="http://arxiv.org/abs/2012.02577" target="_blank">arXiv:2012.02577</a> [<a href="http://arxiv.org/pdf/2012.02577" target="_blank">pdf</a>]

<h2>A high performance approach to detecting small targets in long range low quality infrared videos. (arXiv:2012.02579v1 [cs.CV])</h2>
<h3>Chiman Kwan, Bence Budavari</h3>
<p>Since targets are small in long range infrared (IR) videos, it is challenging
to accurately detect targets in those videos. In this paper, we propose a high
performance approach to detecting small targets in long range and low quality
infrared videos. Our approach consists of a video resolution enhancement
module, a proven small target detector based on local intensity and gradient
(LIG), a connected component (CC) analysis module, and a track association
module to connect detections from multiple frames. Extensive experiments using
actual mid-wave infrared (MWIR) videos in ranges between 3500 m and 5000 m from
a benchmark dataset clearly demonstrated the efficacy of the proposed approach.
</p>
<a href="http://arxiv.org/abs/2012.02579" target="_blank">arXiv:2012.02579</a> [<a href="http://arxiv.org/pdf/2012.02579" target="_blank">pdf</a>]

<h2>Towards Good Practices of U-Net for Traffic Forecasting. (arXiv:2012.02598v1 [cs.CV])</h2>
<h3>Jingwei Xu, Jianjin Zhang, Zhiyu Yao, Yunbo Wang</h3>
<p>This technical report presents a solution for the 2020 Traffic4Cast
Challenge. We consider the traffic forecasting problem as a future frame
prediction task with relatively weak temporal dependencies (might be due to
stochastic urban traffic dynamics) and strong prior knowledge, \textit{i.e.},
the roadmaps of the cities. For these reasons, we use the U-Net as the backbone
model, and we propose a roadmap generation method to make the predicted traffic
flows more rational. Meanwhile, we use a fine-tuning strategy based on the
validation set to prevent overfitting, which effectively improves the
prediction results. At the end of this report, we further discuss several
approaches that we have considered or could be explored in future work: (1)
harnessing inherent data patterns, such as seasonality; (2) distilling and
transferring common knowledge between different cities. We also analyze the
validity of the evaluation metric.
</p>
<a href="http://arxiv.org/abs/2012.02598" target="_blank">arXiv:2012.02598</a> [<a href="http://arxiv.org/pdf/2012.02598" target="_blank">pdf</a>]

<h2>Prediction of Lane Number Using Results From Lane Detection. (arXiv:2012.02604v1 [cs.CV])</h2>
<h3>Panumate Chetprayoon, Fumihiko Takahashi, Yusuke Uchida</h3>
<p>The lane number that the vehicle is traveling in is a key factor in
intelligent vehicle fields. Many lane detection algorithms were proposed and if
we can perfectly detect the lanes, we can directly calculate the lane number
from the lane detection results. However, in fact, lane detection algorithms
sometimes underperform. Therefore, we propose a new approach for predicting the
lane number, where we combine the drive recorder image with the lane detection
results to predict the lane number. Experiments on our own dataset confirmed
that our approach delivered outstanding results without significantly
increasing computational cost.
</p>
<a href="http://arxiv.org/abs/2012.02604" target="_blank">arXiv:2012.02604</a> [<a href="http://arxiv.org/pdf/2012.02604" target="_blank">pdf</a>]

<h2>Application of deep learning to large scale riverine flow velocity estimation. (arXiv:2012.02620v1 [cs.LG])</h2>
<h3>Mojtaba Forghani, Yizhou Qian, Jonghyun Lee, Matthew W. Farthing, Tyler Hesser, Peter K. Kitanidis, Eric F. Darve</h3>
<p>Fast and reliable prediction of riverine flow velocities is important in many
applications, including flood risk management. The shallow water equations
(SWEs) are commonly used for prediction of the flow velocities. However,
accurate and fast prediction with standard SWE solvers is challenging in many
cases. Traditional approaches are computationally expensive and require
high-resolution riverbed profile measurement ( bathymetry) for accurate
predictions. As a result, they are a poor fit in situations where they need to
be evaluated repetitively due, for example, to varying boundary condition (BC),
or when the bathymetry is not known with certainty. In this work, we propose a
two-stage process that tackles these issues. First, using the principal
component geostatistical approach (PCGA) we estimate the probability density
function of the bathymetry from flow velocity measurements, and then we use
multiple machine learning algorithms to obtain a fast solver of the SWEs, given
augmented realizations from the posterior bathymetry distribution and the
prescribed range of BCs. The first step allows us to predict flow velocities
without direct measurement of the bathymetry. Furthermore, the augmentation of
the distribution in the second stage allows incorporation of the additional
bathymetry information into the flow velocity prediction for improved accuracy
and generalization, even if the bathymetry changes over time. Here, we use
three solvers, referred to as PCA-DNN (principal component analysis-deep neural
network), SE (supervised encoder), and SVE (supervised variational encoder),
and validate them on a reach of the Savannah river near Augusta, GA. Our
results show that the fast solvers are capable of predicting flow velocities
with good accuracy, at a computational cost that is significantly lower than
the cost of solving the full boundary value problem with traditional methods.
</p>
<a href="http://arxiv.org/abs/2012.02620" target="_blank">arXiv:2012.02620</a> [<a href="http://arxiv.org/pdf/2012.02620" target="_blank">pdf</a>]

<h2>Effective Label Propagation for Discriminative Semi-Supervised Domain Adaptation. (arXiv:2012.02621v1 [cs.CV])</h2>
<h3>Zhiyong Huang, Kekai Sheng, Weiming Dong, Xing Mei, Chongyang Ma, Feiyue Huang, Dengwen Zhou, Changsheng Xu</h3>
<p>Semi-supervised domain adaptation (SSDA) methods have demonstrated great
potential in large-scale image classification tasks when massive labeled data
are available in the source domain but very few labeled samples are provided in
the target domain. Existing solutions usually focus on feature alignment
between the two domains while paying little attention to the discrimination
capability of learned representations in the target domain. In this paper, we
present a novel and effective method, namely Effective Label Propagation (ELP),
to tackle this problem by using effective inter-domain and intra-domain
semantic information propagation. For inter-domain propagation, we propose a
new cycle discrepancy loss to encourage consistency of semantic information
between the two domains. For intra-domain propagation, we propose an effective
self-training strategy to mitigate the noises in pseudo-labeled target domain
data and improve the feature discriminability in the target domain. As a
general method, our ELP can be easily applied to various domain adaptation
approaches and can facilitate their feature discrimination in the target
domain. Experiments on Office-Home and DomainNet benchmarks show ELP
consistently improves the classification accuracy of mainstream SSDA methods by
2%~3%. Additionally, ELP also improves the performance of UDA methods as well
(81.5% vs 86.1%), based on UDA experiments on the VisDA-2017 benchmark. Our
source code and pre-trained models will be released soon.
</p>
<a href="http://arxiv.org/abs/2012.02621" target="_blank">arXiv:2012.02621</a> [<a href="http://arxiv.org/pdf/2012.02621" target="_blank">pdf</a>]

<h2>Advocating for Multiple Defense Strategies against Adversarial Examples. (arXiv:2012.02632v1 [cs.LG])</h2>
<h3>Alexandre Araujo, Laurent Meunier, Rafael Pinot, Benjamin Negrevergne</h3>
<p>It has been empirically observed that defense mechanisms designed to protect
neural networks against $\ell_\infty$ adversarial examples offer poor
performance against $\ell_2$ adversarial examples and vice versa. In this paper
we conduct a geometrical analysis that validates this observation. Then, we
provide a number of empirical insights to illustrate the effect of this
phenomenon in practice. Then, we review some of the existing defense mechanism
that attempts to defend against multiple attacks by mixing defense strategies.
Thanks to our numerical experiments, we discuss the relevance of this method
and state open questions for the adversarial examples community.
</p>
<a href="http://arxiv.org/abs/2012.02632" target="_blank">arXiv:2012.02632</a> [<a href="http://arxiv.org/pdf/2012.02632" target="_blank">pdf</a>]

<h2>Global Context Aware RCNN for Object Detection. (arXiv:2012.02637v1 [cs.CV])</h2>
<h3>Wenchao Zhang, Chong Fu, Haoyu Xie, Mai Zhu, Ming Tie, Junxin Chen</h3>
<p>RoIPool/RoIAlign is an indispensable process for the typical two-stage object
detection algorithm, it is used to rescale the object proposal cropped from the
feature pyramid to generate a fixed size feature map. However, these cropped
feature maps of local receptive fields will heavily lose global context
information. To tackle this problem, we propose a novel end-to-end trainable
framework, called Global Context Aware (GCA) RCNN, aiming at assisting the
neural network in strengthening the spatial correlation between the background
and the foreground by fusing global context information. The core component of
our GCA framework is a context aware mechanism, in which both global feature
pyramid and attention strategies are used for feature extraction and feature
refinement, respectively. Specifically, we leverage the dense connection to
improve the information flow of the global context at different stages in the
top-down process of FPN, and further use the attention mechanism to refine the
global context at each level in the feature pyramid. In the end, we also
present a lightweight version of our method, which only slightly increases
model complexity and computational burden. Experimental results on COCO
benchmark dataset demonstrate the significant advantages of our approach.
</p>
<a href="http://arxiv.org/abs/2012.02637" target="_blank">arXiv:2012.02637</a> [<a href="http://arxiv.org/pdf/2012.02637" target="_blank">pdf</a>]

<h2>Rethinking movie genre classification with fine grained semantic clustering. (arXiv:2012.02639v1 [cs.CV])</h2>
<h3>Edward Fish, Dr Andrew Gilbert, Jon Weinbren</h3>
<p>Movie genre classification is an active research area in machine learning.
However, due to the limited labels available, there can be large semantic
variations between movies within a single genre definition. We expand these
'coarse' genre labels by identifying 'fine-grained' semantic information within
the multi-modal content of movies. By leveraging pre-trained 'expert' networks,
we learn the influence of different combinations of modes for multi-label genre
classification. Using a contrastive loss, we continue to fine-tune this
'coarse' genre classification network to identify high-level intertextual
similarities between the movies across all genre labels. This leads to a more
'fine-grained' and detailed clustering, based on semantic similarities while
still retaining some genre information. Our approach is demonstrated on a newly
introduced multi-modal 37,866,450 frame, 8,800 movie trailer dataset,
MMX-Trailer-20, which includes pre-computed audio, location, motion, and image
embeddings.
</p>
<a href="http://arxiv.org/abs/2012.02639" target="_blank">arXiv:2012.02639</a> [<a href="http://arxiv.org/pdf/2012.02639" target="_blank">pdf</a>]

<h2>Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language. (arXiv:2012.02646v1 [cs.CV])</h2>
<h3>Songyang Zhang, Houwen Peng, Jianlong Fu, Yijuan Lu, Jiebo Luo</h3>
<p>We address the problem of retrieving a specific moment from an untrimmed
video by natural language. It is a challenging problem because a target moment
may take place in the context of other temporal moments in the untrimmed video.
Existing methods cannot tackle this challenge well since they do not fully
consider the temporal contexts between temporal moments. In this paper, we
model the temporal context between video moments by a set of predefined
two-dimensional maps under different temporal scales. For each map, one
dimension indicates the starting time of a moment and the other indicates the
duration. These 2D temporal maps can cover diverse video moments with different
lengths, while representing their adjacent contexts at different temporal
scales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal
Adjacent Network (MS-2D-TAN), a single-shot framework for moment localization.
It is capable of encoding the adjacent temporal contexts at each scale, while
learning discriminative features for matching video moments with referring
expressions. We evaluate the proposed MS-2D-TAN on three challenging
benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our
MS-2D-TAN outperforms the state of the art.
</p>
<a href="http://arxiv.org/abs/2012.02646" target="_blank">arXiv:2012.02646</a> [<a href="http://arxiv.org/pdf/2012.02646" target="_blank">pdf</a>]

<h2>Detecting 32 Pedestrian Attributes for Autonomous Vehicles. (arXiv:2012.02647v1 [cs.CV])</h2>
<h3>Taylor Mordan, Matthieu Cord, Patrick P&#xe9;rez, Alexandre Alahi</h3>
<p>Pedestrians are arguably one of the most safety-critical road users to
consider for autonomous vehicles in urban areas. In this paper, we address the
problem of jointly detecting pedestrians and recognizing 32 pedestrian
attributes. These encompass visual appearance and behavior, and also include
the forecasting of road crossing, which is a main safety concern. For this, we
introduce a Multi-Task Learning (MTL) model relying on a composite field
framework, which achieves both goals in an efficient way. Each field spatially
locates pedestrian instances and aggregates attribute predictions over them.
This formulation naturally leverages spatial context, making it well suited to
low resolution scenarios such as autonomous driving. By increasing the number
of attributes jointly learned, we highlight an issue related to the scales of
gradients, which arises in MTL with numerous tasks. We solve it by normalizing
the gradients coming from different objective functions when they join at the
fork in the network architecture during the backward pass, referred to as
fork-normalization. Experimental validation is performed on JAAD, a dataset
providing numerous attributes for pedestrian analysis from autonomous vehicles,
and shows competitive detection and attribute recognition results, as well as a
more stable MTL training.
</p>
<a href="http://arxiv.org/abs/2012.02647" target="_blank">arXiv:2012.02647</a> [<a href="http://arxiv.org/pdf/2012.02647" target="_blank">pdf</a>]

<h2>Understanding Attention: In Minds and Machines. (arXiv:2012.02659v1 [cs.AI])</h2>
<h3>Shriraj P. Sawant, Shruti Singh</h3>
<p>Attention is a complex and broad concept, studied across multiple disciplines
spanning artificial intelligence, cognitive science, psychology, neuroscience,
and related fields. Although many of the ideas regarding attention do not
significantly overlap among these fields, there is a common theme of adaptive
control of limited resources. In this work, we review the concept and variants
of attention in artificial neural networks (ANNs). We also discuss the origin
of attention from the neuroscience point of view parallel to that of ANNs.
Instead of having seemingly disconnected dialogues between varied disciplines,
we suggest grounding the ideas on common conceptual frameworks for a systematic
analysis of attention and towards possible unification of ideas in AI and
Neuroscience.
</p>
<a href="http://arxiv.org/abs/2012.02659" target="_blank">arXiv:2012.02659</a> [<a href="http://arxiv.org/pdf/2012.02659" target="_blank">pdf</a>]

<h2>Efficient semidefinite-programming-based inference for binary and multi-class MRFs. (arXiv:2012.02661v1 [cs.LG])</h2>
<h3>Chirag Pabbaraju, Po-Wei Wang, J. Zico Kolter</h3>
<p>Probabilistic inference in pairwise Markov Random Fields (MRFs), i.e.
computing the partition function or computing a MAP estimate of the variables,
is a foundational problem in probabilistic graphical models. Semidefinite
programming relaxations have long been a theoretically powerful tool for
analyzing properties of probabilistic inference, but have not been practical
owing to the high computational cost of typical solvers for solving the
resulting SDPs. In this paper, we propose an efficient method for computing the
partition function or MAP estimate in a pairwise MRF by instead exploiting a
recently proposed coordinate-descent-based fast semidefinite solver. We also
extend semidefinite relaxations from the typical binary MRF to the full
multi-class setting, and develop a compact semidefinite relaxation that can
again be solved efficiently using the solver. We show that the method
substantially outperforms (both in terms of solution quality and speed) the
existing state of the art in approximate inference, on benchmark problems drawn
from previous work. We also show that our approach can scale to large MRF
domains such as fully-connected pairwise CRF models used in computer vision.
</p>
<a href="http://arxiv.org/abs/2012.02661" target="_blank">arXiv:2012.02661</a> [<a href="http://arxiv.org/pdf/2012.02661" target="_blank">pdf</a>]

<h2>Learning in two-player games between transparent opponents. (arXiv:2012.02671v1 [cs.AI])</h2>
<h3>Adrian Hutter</h3>
<p>We consider a scenario in which two reinforcement learning agents repeatedly
play a matrix game against each other and update their parameters after each
round. The agents' decision-making is transparent to each other, which allows
each agent to predict how their opponent will play against them. To prevent an
infinite regress of both agents recursively predicting each other indefinitely,
each agent is required to give an opponent-independent response with some
probability at least epsilon. Transparency also allows each agent to anticipate
and shape the other agent's gradient step, i.e. to move to regions of parameter
space in which the opponent's gradient points in a direction favourable to
them. We study the resulting dynamics experimentally, using two algorithms from
previous literature (LOLA and SOS) for opponent-aware learning. We find that
the combination of mutually transparent decision-making and opponent-aware
learning robustly leads to mutual cooperation in a single-shot prisoner's
dilemma. In a game of chicken, in which both agents try to manoeuvre their
opponent towards their preferred equilibrium, converging to a mutually
beneficial outcome turns out to be much harder, and opponent-aware learning can
even lead to worst-case outcomes for both agents. This highlights the need to
develop opponent-aware learning algorithms that achieve acceptable outcomes in
social dilemmas involving an equilibrium selection problem.
</p>
<a href="http://arxiv.org/abs/2012.02671" target="_blank">arXiv:2012.02671</a> [<a href="http://arxiv.org/pdf/2012.02671" target="_blank">pdf</a>]

<h2>Accelerating Road Sign Ground Truth Construction with Knowledge Graph and Machine Learning. (arXiv:2012.02672v1 [cs.AI])</h2>
<h3>Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin</h3>
<p>Having a comprehensive, high-quality dataset of road sign annotation is
critical to the success of AI-based Road Sign Recognition (RSR) systems. In
practice, annotators often face difficulties in learning road sign systems of
different countries; hence, the tasks are often time-consuming and produce poor
results. We propose a novel approach using knowledge graphs and a machine
learning algorithm - variational prototyping-encoder (VPE) - to assist human
annotators in classifying road signs effectively. Annotators can query the Road
Sign Knowledge Graph using visual attributes and receive closest matching
candidates suggested by the VPE model. The VPE model uses the candidates from
the knowledge graph and a real sign image patch as inputs. We show that our
knowledge graph approach can reduce sign search space by 98.9%. Furthermore,
with VPE, our system can propose the correct single candidate for 75% of signs
in the tested datasets, eliminating the human search effort entirely in those
cases.
</p>
<a href="http://arxiv.org/abs/2012.02672" target="_blank">arXiv:2012.02672</a> [<a href="http://arxiv.org/pdf/2012.02672" target="_blank">pdf</a>]

<h2>Community detection using fast low-cardinality semidefinite programming. (arXiv:2012.02676v1 [cs.LG])</h2>
<h3>Po-Wei Wang, J. Zico Kolter</h3>
<p>Modularity maximization has been a fundamental tool for understanding the
community structure of a network, but the underlying optimization problem is
nonconvex and NP-hard to solve. State-of-the-art algorithms like the Louvain or
Leiden methods focus on different heuristics to help escape local optima, but
they still depend on a greedy step that moves node assignment locally and is
prone to getting trapped. In this paper, we propose a new class of
low-cardinality algorithm that generalizes the local update to maximize a
semidefinite relaxation derived from max-k-cut. This proposed algorithm is
scalable, empirically achieves the global semidefinite optimality for small
cases, and outperforms the state-of-the-art algorithms in real-world datasets
with little additional time cost. From the algorithmic perspective, it also
opens a new avenue for scaling-up semidefinite programming when the solutions
are sparse instead of low-rank.
</p>
<a href="http://arxiv.org/abs/2012.02676" target="_blank">arXiv:2012.02676</a> [<a href="http://arxiv.org/pdf/2012.02676" target="_blank">pdf</a>]

<h2>DPM: A Novel Training Method for Physics-Informed Neural Networks in Extrapolation. (arXiv:2012.02681v1 [cs.LG])</h2>
<h3>Jungeun Kim, Kookjin Lee, Dongeun Lee, Sheo Yon Jin, Noseong Park</h3>
<p>We present a method for learning dynamics of complex physical processes
described by time-dependent nonlinear partial differential equations (PDEs).
Our particular interest lies in extrapolating solutions in time beyond the
range of temporal domain used in training. Our choice for a baseline method is
physics-informed neural network (PINN) [Raissi et al., J. Comput. Phys.,
378:686--707, 2019] because the method parameterizes not only the solutions but
also the equations that describe the dynamics of physical processes. We
demonstrate that PINN performs poorly on extrapolation tasks in many benchmark
problems. To address this, we propose a novel method for better training PINN
and demonstrate that our newly enhanced PINNs can accurately extrapolate
solutions in time. Our method shows up to 72% smaller errors than existing
methods in terms of the standard L2-norm metric.
</p>
<a href="http://arxiv.org/abs/2012.02681" target="_blank">arXiv:2012.02681</a> [<a href="http://arxiv.org/pdf/2012.02681" target="_blank">pdf</a>]

<h2>Model-Agnostic Learning to Meta-Learn. (arXiv:2012.02684v1 [cs.LG])</h2>
<h3>Arnout Devos, Yatin Dandi</h3>
<p>In this paper, we propose a learning algorithm that enables a model to
quickly exploit commonalities among related tasks from an unseen task
distribution, before quickly adapting to specific tasks from that same
distribution. We investigate how learning with different task distributions can
first improve adaptability by meta-finetuning on related tasks before improving
goal task generalization with finetuning. Synthetic regression experiments
validate the intuition that learning to meta-learn improves adaptability and
consecutively generalization. The methodology, setup, and hypotheses in this
proposal were positively evaluated by peer review before conclusive experiments
were carried out.
</p>
<a href="http://arxiv.org/abs/2012.02684" target="_blank">arXiv:2012.02684</a> [<a href="http://arxiv.org/pdf/2012.02684" target="_blank">pdf</a>]

<h2>ESCAPED: Efficient Secure and Private Dot Product Framework for Kernel-based Machine Learning Algorithms with Applications in Healthcare. (arXiv:2012.02688v1 [cs.LG])</h2>
<h3>Ali Burak &#xdc;nal, Mete Akg&#xfc;n, Nico Pfeifer</h3>
<p>To train sophisticated machine learning models one usually needs many
training samples. Especially in healthcare settings these samples can be very
expensive, meaning that one institution alone usually does not have enough on
its own. Merging privacy-sensitive data from different sources is usually
restricted by data security and data protection measures. This can lead to
approaches that reduce data quality by putting noise onto the variables (e.g.,
in $\epsilon$-differential privacy) or omitting certain values (e.g., for
$k$-anonymity). Other measures based on cryptographic methods can lead to very
time-consuming computations, which is especially problematic for larger
multi-omics data. We address this problem by introducing ESCAPED, which stands
for Efficient SeCure And PrivatE Dot product framework, enabling the
computation of the dot product of vectors from multiple sources on a
third-party, which later trains kernel-based machine learning algorithms, while
neither sacrificing privacy nor adding noise. We evaluated our framework on
drug resistance prediction for HIV-infected people and multi-omics
dimensionality reduction and clustering problems in precision medicine. In
terms of execution time, our framework significantly outperforms the
best-fitting existing approaches without sacrificing the performance of the
algorithm. Even though we only show the benefit for kernel-based algorithms,
our framework can open up new research opportunities for further machine
learning models that require the dot product of vectors from multiple sources.
</p>
<a href="http://arxiv.org/abs/2012.02688" target="_blank">arXiv:2012.02688</a> [<a href="http://arxiv.org/pdf/2012.02688" target="_blank">pdf</a>]

<h2>Isometric Multi-Shape Matching. (arXiv:2012.02689v1 [cs.CV])</h2>
<h3>Maolin Gao, Zorah L&#xe4;hner, Johan Thunberg, Daniel Cremers, Florian Bernard</h3>
<p>Finding correspondences between shapes is a fundamental problem in computer
vision and graphics, which is relevant for many applications, including 3D
reconstruction, object tracking, and style transfer. The vast majority of
correspondence methods aim to find a solution between pairs of shapes, even if
multiple instances of the same class are available. While isometries are often
studied in shape correspondence problems, they have not been considered
explicitly in the multi-matching setting. This paper closes this gap by
proposing a novel optimisation formulation for isometric multi-shape matching.
We present a suitable optimisation algorithm for solving our formulation and
provide a convergence and complexity analysis. Our algorithm obtains
multi-matchings that are by construction provably cycle-consistent. We
demonstrate the superior performance of our method on various datasets and set
the new state-of-the-art in isometric multi-shape matching.
</p>
<a href="http://arxiv.org/abs/2012.02689" target="_blank">arXiv:2012.02689</a> [<a href="http://arxiv.org/pdf/2012.02689" target="_blank">pdf</a>]

<h2>Bayesian Active Learning for Wearable Stress and Affect Detection. (arXiv:2012.02702v1 [cs.LG])</h2>
<h3>Abhijith Ragav, Gautham Krishna Gudur</h3>
<p>In the recent past, psychological stress has been increasingly observed in
humans, and early detection is crucial to prevent health risks. Stress
detection using on-device deep learning algorithms has been on the rise owing
to advancements in pervasive computing. However, an important challenge that
needs to be addressed is handling unlabeled data in real-time via suitable
ground truthing techniques (like Active Learning), which should help establish
affective states (labels) while also selecting only the most informative data
points to query from an oracle. In this paper, we propose a framework with
capabilities to represent model uncertainties through approximations in
Bayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with
suitable acquisition functions for active learning. Empirical results on a
popular stress and affect detection dataset experimented on a Raspberry Pi 2
indicate that our proposed framework achieves a considerable efficiency boost
during inference, with a substantially low number of acquired pool points
during active learning across various acquisition functions. Variation Ratios
achieves an accuracy of 90.38% which is comparable to the maximum test accuracy
achieved while training on about 40% lesser data.
</p>
<a href="http://arxiv.org/abs/2012.02702" target="_blank">arXiv:2012.02702</a> [<a href="http://arxiv.org/pdf/2012.02702" target="_blank">pdf</a>]

<h2>Spatial Language Understanding for Object Search in Partially Observed Cityscale Environments. (arXiv:2012.02705v1 [cs.RO])</h2>
<h3>Kaiyu Zheng, Deniz Bayazit, Rebecca Mathew, Ellie Pavlick, Stefanie Tellex</h3>
<p>We present a system that enables robots to interpret spatial language as a
distribution over object locations for effective search in partially observable
cityscale environments. We introduce the spatial language observation space and
formulate a stochastic observation model under the framework of Partially
Observable Markov Decision Process (POMDP) which incorporates information
extracted from the spatial language into the robot's belief. To interpret
ambiguous, context-dependent prepositions (e.g.~front), we propose a
convolutional neural network model that learns to predict the language
provider's relative frame of reference (FoR) given environment context. We
demonstrate the generalizability of our FoR prediction model and object search
system through cross-validation over areas of five cities, each with a
40,000m$^2$ footprint. End-to-end experiments in simulation show that our
system achieves faster search and higher success rate compared to a
keyword-based baseline without spatial preposition understanding.
</p>
<a href="http://arxiv.org/abs/2012.02705" target="_blank">arXiv:2012.02705</a> [<a href="http://arxiv.org/pdf/2012.02705" target="_blank">pdf</a>]

<h2>Super-Selfish: Self-Supervised Learning onImages with PyTorch. (arXiv:2012.02706v1 [cs.CV])</h2>
<h3>Nicolas Wagner, Anirban Mukhopadhyay</h3>
<p>Super-Selfish is an easy to use PyTorch framework for image-based
self-supervised learning. Features can be learned with 13 algorithms that span
from simple classification to more complex state of theart contrastive pretext
tasks. The framework is easy to use and allowsfor pretraining any PyTorch
neural network with only two lines of code. Simultaneously, full flexibility is
maintained through modular design choices. The code can be found at
https://github.com/MECLabTUDA/Super_Selfish and installed using pip install
super-selfish.
</p>
<a href="http://arxiv.org/abs/2012.02706" target="_blank">arXiv:2012.02706</a> [<a href="http://arxiv.org/pdf/2012.02706" target="_blank">pdf</a>]

<h2>Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning. (arXiv:2012.02732v1 [cs.LG])</h2>
<h3>Woosuk Kwon, Gyeong-In Yu, Eunji Jeong, Byung-Gon Chun</h3>
<p>Deep learning (DL) frameworks take advantage of GPUs to improve the speed of
DL inference and training. Ideally, DL frameworks should be able to fully
utilize the computation power of GPUs such that the running time depends on the
amount of computation assigned to GPUs. Yet, we observe that in scheduling GPU
tasks, existing DL frameworks suffer from inefficiencies such as large
scheduling overhead and unnecessary serial execution. To this end, we propose
Nimble, a DL execution engine that runs GPU tasks in parallel with minimal
scheduling overhead. Nimble introduces a novel technique called ahead-of-time
(AoT) scheduling. Here, the scheduling procedure finishes before executing the
GPU kernel, thereby removing most of the scheduling overhead during run time.
Furthermore, Nimble automatically parallelizes the execution of GPU tasks by
exploiting multiple GPU streams in a single GPU. Evaluation on a variety of
neural networks shows that compared to PyTorch, Nimble speeds up inference and
training by up to 22.34$\times$ and 3.61$\times$, respectively. Moreover,
Nimble outperforms state-of-the-art inference systems, TensorRT and TVM, by up
to 2.81$\times$ and 1.70$\times$, respectively.
</p>
<a href="http://arxiv.org/abs/2012.02732" target="_blank">arXiv:2012.02732</a> [<a href="http://arxiv.org/pdf/2012.02732" target="_blank">pdf</a>]

<h2>Hierarchical Semantic Aggregation for Contrastive Representation Learning. (arXiv:2012.02733v1 [cs.CV])</h2>
<h3>Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai Xiong, Qi Tian</h3>
<p>Self-supervised learning based on instance discrimination has shown
remarkable progress. In particular, contrastive learning, which regards each
image as well as its augmentations as a separate class, and pushes all other
images away, has been proved effective for pretraining. However, contrasting
two images that are de facto similar in semantic space is hard for optimization
and not applicable for general representations. In this paper, we tackle the
representation inefficiency of contrastive learning and propose a hierarchical
training strategy to explicitly model the invariance to semantic similar images
in a bottom-up way. This is achieved by extending the contrastive loss to allow
for multiple positives per anchor, and explicitly pulling semantically similar
images/patches together at the earlier layers as well as the last embedding
space. In this way, we are able to learn feature representation that is more
discriminative throughout different layers, which we find is beneficial for
fast convergence. The hierarchical semantic aggregation strategy produces more
discriminative representation on several unsupervised benchmarks. Notably, on
ImageNet with ResNet-50 as backbone, we reach $76.4\%$ top-1 accuracy with
linear evaluation, and $75.1\%$ top-1 accuracy with only $10\%$ labels.
</p>
<a href="http://arxiv.org/abs/2012.02733" target="_blank">arXiv:2012.02733</a> [<a href="http://arxiv.org/pdf/2012.02733" target="_blank">pdf</a>]

<h2>SMPLy Benchmarking 3D Human Pose Estimation in the Wild. (arXiv:2012.02743v1 [cs.CV])</h2>
<h3>Vincent Leroy, Philippe Weinzaepfel, Romain Br&#xe9;gier, Hadrien Combaluzier, Gr&#xe9;gory Rogez</h3>
<p>Predicting 3D human pose from images has seen great recent improvements.
Novel approaches that can even predict both pose and shape from a single input
image have been introduced, often relying on a parametric model of the human
body such as SMPL. While qualitative results for such methods are often shown
for images captured in-the-wild, a proper benchmark in such conditions is still
missing, as it is cumbersome to obtain ground-truth 3D poses elsewhere than in
a motion capture room. This paper presents a pipeline to easily produce and
validate such a dataset with accurate ground-truth, with which we benchmark
recent 3D human pose estimation methods in-the-wild. We make use of the
recently introduced Mannequin Challenge dataset which contains in-the-wild
videos of people frozen in action like statues and leverage the fact that
people are static and the camera moving to accurately fit the SMPL model on the
sequences. A total of 24,428 frames with registered body models are then
selected from 567 scenes at almost no cost, using only online RGB videos. We
benchmark state-of-the-art SMPL-based human pose estimation methods on this
dataset. Our results highlight that challenges remain, in particular for
difficult poses or for scenes where the persons are partially truncated or
occluded.
</p>
<a href="http://arxiv.org/abs/2012.02743" target="_blank">arXiv:2012.02743</a> [<a href="http://arxiv.org/pdf/2012.02743" target="_blank">pdf</a>]

<h2>Challenging common interpretability assumptions in feature attribution explanations. (arXiv:2012.02748v1 [cs.LG])</h2>
<h3>Jonathan Dinu (1), Jeffrey Bigham (2), J. Zico Kolter (2) ((1) Unaffiliated, (2) Carnegie Mellon University)</h3>
<p>As machine learning and algorithmic decision making systems are increasingly
being leveraged in high-stakes human-in-the-loop settings, there is a pressing
need to understand the rationale of their predictions. Researchers have
responded to this need with explainable AI (XAI), but often proclaim
interpretability axiomatically without evaluation. When these systems are
evaluated, they are often tested through offline simulations with proxy metrics
of interpretability (such as model complexity). We empirically evaluate the
veracity of three common interpretability assumptions through a large scale
human-subjects experiment with a simple "placebo explanation" control. We find
that feature attribution explanations provide marginal utility in our task for
a human decision maker and in certain cases result in worse decisions due to
cognitive and contextual confounders. This result challenges the assumed
universal benefit of applying these methods and we hope this work will
underscore the importance of human evaluation in XAI research. Supplemental
materials -- including anonymized data from the experiment, code to replicate
the study, an interactive demo of the experiment, and the models used in the
analysis -- can be found at: https://doi.pizza/challenging-xai.
</p>
<a href="http://arxiv.org/abs/2012.02748" target="_blank">arXiv:2012.02748</a> [<a href="http://arxiv.org/pdf/2012.02748" target="_blank">pdf</a>]

<h2>An Empirical Method to Quantify the Peripheral Performance Degradation in Deep Networks. (arXiv:2012.02749v1 [cs.CV])</h2>
<h3>Calden Wloka, John K. Tsotsos</h3>
<p>When applying a convolutional kernel to an image, if the output is to remain
the same size as the input then some form of padding is required around the
image boundary, meaning that for each layer of convolution in a convolutional
neural network (CNN), a strip of pixels equal to the half-width of the kernel
size is produced with a non-veridical representation. Although most CNN kernels
are small to reduce the parameter load of a network, this non-veridical area
compounds with each convolutional layer. The tendency toward deeper and deeper
networks combined with stride-based down-sampling means that the propagation of
this region can end up covering a non-negligable portion of the image. Although
this issue with convolutions has been well acknowledged over the years, the
impact of this degraded peripheral representation on modern network behavior
has not been fully quantified. What are the limits of translation invariance?
Does image padding successfully mitigate the issue, or is performance affected
as an object moves between the image border and center? Using Mask R-CNN as an
experimental model, we design a dataset and methodology to quantify the spatial
dependency of network performance. Our dataset is constructed by inserting
objects into high resolution backgrounds, thereby allowing us to crop
sub-images which place target objects at specific locations relative to the
image border. By probing the behaviour of Mask R-CNN across a selection of
target locations, we see clear patterns of performance degredation near the
image boundary, and in particular in the image corners. Quantifying both the
extent and magnitude of this spatial anisotropy in network performance is
important for the deployment of deep networks into unconstrained and realistic
environments in which the location of objects or regions of interest are not
guaranteed to be well localized within a given image.
</p>
<a href="http://arxiv.org/abs/2012.02749" target="_blank">arXiv:2012.02749</a> [<a href="http://arxiv.org/pdf/2012.02749" target="_blank">pdf</a>]

<h2>Playing Text-Based Games with Common Sense. (arXiv:2012.02757v1 [cs.AI])</h2>
<h3>Sahith Dambekodi, Spencer Frazier, Prithviraj Ammanabrolu, Mark O. Riedl</h3>
<p>Text based games are simulations in which an agent interacts with the world
purely through natural language. They typically consist of a number of puzzles
interspersed with interactions with common everyday objects and locations. Deep
reinforcement learning agents can learn to solve these puzzles. However, the
everyday interactions with the environment, while trivial for human players,
present as additional puzzles to agents. We explore two techniques for
incorporating commonsense knowledge into agents. Inferring possibly hidden
aspects of the world state with either a commonsense inference model COMET, or
a language model BERT. Biasing an agents exploration according to common
patterns recognized by a language model. We test our technique in the 9to05
game, which is an extreme version of a text based game that requires numerous
interactions with common, everyday objects in common, everyday scenarios. We
conclude that agents that augment their beliefs about the world state with
commonsense inferences are more robust to observational errors and omissions of
common elements from text descriptions.
</p>
<a href="http://arxiv.org/abs/2012.02757" target="_blank">arXiv:2012.02757</a> [<a href="http://arxiv.org/pdf/2012.02757" target="_blank">pdf</a>]

<h2>Learning Equivariant Representations. (arXiv:2012.02771v1 [cs.CV])</h2>
<h3>Carlos Esteves</h3>
<p>State-of-the-art deep learning systems often require large amounts of data
and computation. For this reason, leveraging known or unknown structure of the
data is paramount. Convolutional neural networks (CNNs) are successful examples
of this principle, their defining characteristic being the shift-equivariance.
By sliding a filter over the input, when the input shifts, the response shifts
by the same amount, exploiting the structure of natural images where semantic
content is independent of absolute pixel positions. This property is essential
to the success of CNNs in audio, image and video recognition tasks. In this
thesis, we extend equivariance to other kinds of transformations, such as
rotation and scaling. We propose equivariant models for different
transformations defined by groups of symmetries. The main contributions are (i)
polar transformer networks, achieving equivariance to the group of similarities
on the plane, (ii) equivariant multi-view networks, achieving equivariance to
the group of symmetries of the icosahedron, (iii) spherical CNNs, achieving
equivariance to the continuous 3D rotation group, (iv) cross-domain image
embeddings, achieving equivariance to 3D rotations for 2D inputs, and (v)
spin-weighted spherical CNNs, generalizing the spherical CNNs and achieving
equivariance to 3D rotations for spherical vector fields. Applications include
image classification, 3D shape classification and retrieval, panoramic image
classification and segmentation, shape alignment and pose estimation. What
these models have in common is that they leverage symmetries in the data to
reduce sample and model complexity and improve generalization performance. The
advantages are more significant on (but not limited to) challenging tasks where
data is limited or input perturbations such as arbitrary rotations are present.
</p>
<a href="http://arxiv.org/abs/2012.02771" target="_blank">arXiv:2012.02771</a> [<a href="http://arxiv.org/pdf/2012.02771" target="_blank">pdf</a>]

<h2>Representation Based Complexity Measures for Predicting Generalization in Deep Learning. (arXiv:2012.02775v1 [cs.LG])</h2>
<h3>Parth Natekar, Manik Sharma</h3>
<p>Deep Neural Networks can generalize despite being significantly
overparametrized. Recent research has tried to examine this phenomenon from
various view points and to provide bounds on the generalization error or
measures predictive of the generalization gap based on these viewpoints, such
as norm-based, PAC-Bayes based, and margin-based analysis. In this work, we
provide an interpretation of generalization from the perspective of quality of
internal representations of deep neural networks, based on neuroscientific
theories of how the human visual system creates invariant and untangled object
representations. Instead of providing theoretical bounds, we demonstrate
practical complexity measures which can be computed ad-hoc to uncover
generalization behaviour in deep models. We also provide a detailed description
of our solution that won the NeurIPS competition on Predicting Generalization
in Deep Learning held at NeurIPS 2020. An implementation of our solution is
available at https://github.com/parthnatekar/pgdl.
</p>
<a href="http://arxiv.org/abs/2012.02775" target="_blank">arXiv:2012.02775</a> [<a href="http://arxiv.org/pdf/2012.02775" target="_blank">pdf</a>]

<h2>Learning to Fuse Asymmetric Feature Maps in Siamese Trackers. (arXiv:2012.02776v1 [cs.CV])</h2>
<h3>Wencheng Han, Xingping Dong, Fahad Shahbaz Khan, Ling Shao, Jianbing Shen</h3>
<p>In recent years, Siamese-based trackers have achieved promising performance
in visual tracking. Most recent Siamese-based trackers typically employ a
depth-wise cross-correlation (DW-XCorr) to obtain multi-channel correlation
information from the two feature maps (target and search region). However,
DW-XCorr has several limitations within Siamese-based tracking: it can easily
be fooled by distractors, has fewer activated channels, and provides weak
discrimination of object boundaries. Further, DW-XCorr is a handcrafted
parameter-free module and cannot fully benefit from offline learning on
large-scale data.

We propose a learnable module, called the asymmetric convolution (ACM), which
learns to better capture the semantic correlation information in offline
training on large-scale data. Different from DW-XCorr and its predecessor
(XCorr), which regard a single feature map as the convolution kernel, our ACM
decomposes the convolution operation on a concatenated feature map into two
mathematically equivalent operations, thereby avoiding the need for the feature
maps to be of the same size (width and height) during concatenation. Our ACM
can incorporate useful prior information, such as bounding-box size, with
standard visual features. Furthermore, ACM can easily be integrated into
existing Siamese trackers based on DW-XCorr or XCorr. To demonstrate its
generalization ability, we integrate ACM into three representative trackers:
SiamFC, SiamRPN++, and SiamBAN. Our experiments reveal the benefits of the
proposed ACM, which outperforms existing methods on six tracking benchmarks. On
the LaSOT test set, our ACM-based tracker obtains a significant improvement of
5.8% in terms of success (AUC), over the baseline.
</p>
<a href="http://arxiv.org/abs/2012.02776" target="_blank">arXiv:2012.02776</a> [<a href="http://arxiv.org/pdf/2012.02776" target="_blank">pdf</a>]

<h2>Few-shot Image Generation with Elastic Weight Consolidation. (arXiv:2012.02780v1 [cs.CV])</h2>
<h3>Yijun Li, Richard Zhang, Jingwan Lu, Eli Shechtman</h3>
<p>Few-shot image generation seeks to generate more data of a given domain, with
only few available training examples. As it is unreasonable to expect to fully
infer the distribution from just a few observations (e.g., emojis), we seek to
leverage a large, related source domain as pretraining (e.g., human faces).
Thus, we wish to preserve the diversity of the source domain, while adapting to
the appearance of the target. We adapt a pretrained model, without introducing
any additional parameters, to the few examples of the target domain. Crucially,
we regularize the changes of the weights during this adaptation, in order to
best preserve the information of the source dataset, while fitting the target.
We demonstrate the effectiveness of our algorithm by generating high-quality
results of different target domains, including those with extremely few
examples (e.g., &lt;10). We also analyze the performance of our method with
respect to some important factors, such as the number of examples and the
dissimilarity between the source and target domain.
</p>
<a href="http://arxiv.org/abs/2012.02780" target="_blank">arXiv:2012.02780</a> [<a href="http://arxiv.org/pdf/2012.02780" target="_blank">pdf</a>]

<h2>Batch Group Normalization. (arXiv:2012.02782v1 [cs.LG])</h2>
<h3>Xiao-Yun Zhou, Jiacheng Sun, Nanyang Ye, Xu Lan, Qijun Luo, Bo-Lin Lai, Pedro Esperanca, Guang-Zhong Yang, Zhenguo Li</h3>
<p>Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming to
train. Normalization is one of the effective solutions. Among previous
normalization methods, Batch Normalization (BN) performs well at medium and
large batch sizes and is with good generalizability to multiple vision tasks,
while its performance degrades significantly at small batch sizes. In this
paper, we find that BN saturates at extreme large batch sizes, i.e., 128 images
per worker, i.e., GPU, as well and propose that the degradation/saturation of
BN at small/extreme large batch sizes is caused by noisy/confused statistic
calculation. Hence without adding new trainable parameters, using
multiple-layer or multi-iteration information, or introducing extra
computation, Batch Group Normalization (BGN) is proposed to solve the
noisy/confused statistic calculation of BN at small/extreme large batch sizes
with introducing the channel, height and width dimension to compensate. The
group technique in Group Normalization (GN) is used and a hyper-parameter G is
used to control the number of feature instances used for statistic calculation,
hence to offer neither noisy nor confused statistic for different batch sizes.
We empirically demonstrate that BGN consistently outperforms BN, Instance
Normalization (IN), Layer Normalization (LN), GN, and Positional Normalization
(PN), across a wide spectrum of vision tasks, including image classification,
Neural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)
and Unsupervised Domain Adaptation (UDA), indicating its good performance,
robust stability to batch size and wide generalizability. For example, for
training ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1
accuracy of 66.512% while BGN achieves 76.096% with notable improvement.
</p>
<a href="http://arxiv.org/abs/2012.02782" target="_blank">arXiv:2012.02782</a> [<a href="http://arxiv.org/pdf/2012.02782" target="_blank">pdf</a>]

<h2>Unsupervised embedding of trajectories captures the latent structure of mobility. (arXiv:2012.02785v1 [cs.LG])</h2>
<h3>Dakota Murray, Jisung Yoon, Sadamori Kojaku, Rodrigo Costas, Woo-Sung Jung, Sta&#x161;a Milojevi&#x107;, Yong-Yeol Ahn</h3>
<p>Human mobility and migration drive major societal phenomena such as the
growth and evolution of cities, epidemics, economies, and innovation.
Historically, human mobility has been strongly constrained by physical
separation -- geographic distance. However, geographic distance is becoming
less relevant in the increasingly-globalized world in which physical barriers
are shrinking while linguistic, cultural, and historical relationships are
becoming more important. As understanding mobility is becoming critical for
contemporary society, finding frameworks that can capture this complexity is of
paramount importance. Here, using three distinct human trajectory datasets, we
demonstrate that a neural embedding model can encode nuanced relationships
between locations into a vector-space, providing an effective measure of
distance that reflects the multi-faceted structure of human mobility. Focusing
on the case of scientific mobility, we show that embeddings of scientific
organizations uncover cultural and linguistic relations, and even academic
prestige, at multiple levels of granularity. Furthermore, the embedding vectors
reveal universal relationships between organizational characteristics and their
place in the global landscape of scientific mobility. The ability to learn
scalable, dense, and meaningful representations of mobility directly from the
data can open up a new avenue of studying mobility across domains.
</p>
<a href="http://arxiv.org/abs/2012.02785" target="_blank">arXiv:2012.02785</a> [<a href="http://arxiv.org/pdf/2012.02785" target="_blank">pdf</a>]

<h2>Neural Dynamic Policies for End-to-End Sensorimotor Learning. (arXiv:2012.02788v1 [cs.LG])</h2>
<h3>Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, Deepak Pathak</h3>
<p>The current dominant paradigm in sensorimotor control, whether imitation or
reinforcement learning, is to train policies directly in raw action spaces such
as torque, joint angle, or end-effector position. This forces the agent to make
decisions individually at each timestep in training, and hence, limits the
scalability to continuous, high-dimensional, and long-horizon tasks. In
contrast, research in classical robotics has, for a long time, exploited
dynamical systems as a policy representation to learn robot behaviors via
demonstrations. These techniques, however, lack the flexibility and
generalizability provided by deep learning or reinforcement learning and have
remained under-explored in such settings. In this work, we begin to close this
gap and embed the structure of a dynamical system into deep neural
network-based policies by reparameterizing action spaces via second-order
differential equations. We propose Neural Dynamic Policies (NDPs) that make
predictions in trajectory distribution space as opposed to prior policy
learning methods where actions represent the raw control space. The embedded
structure allows end-to-end policy learning for both reinforcement and
imitation learning setups. We show that NDPs outperform the prior
state-of-the-art in terms of either efficiency or performance across several
robotic control tasks for both imitation and reinforcement learning setups.
Project video and code are available at
https://shikharbahl.github.io/neural-dynamic-policies/
</p>
<a href="http://arxiv.org/abs/2012.02788" target="_blank">arXiv:2012.02788</a> [<a href="http://arxiv.org/pdf/2012.02788" target="_blank">pdf</a>]

<h2>Deep Learning Estimation of Absorbed Dose for Nuclear Medicine Diagnostics. (arXiv:1805.09108v8 [stat.ML] UPDATED)</h2>
<h3>Luciano Melodia</h3>
<p>The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated
by convolving an image of a time-integrated activity distribution with a dose
voxel kernel (DVK) consisting of different types of tissues. This fast and
inacurate approximation is inappropriate for personalized dosimetry as it
neglects tissue heterogenity. The latter can be calculated using different
imaging techniques such as CT and SPECT combined with a time consuming
monte-carlo simulation. The aim of this study is, for the first time, an
estimation of DVKs from CT-derived density kernels (DK) via deep learning in
convolutional neural networks (CNNs). The proposed CNN achieved, on the test
set, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a
corresponding mean squared error (MSE) $= 1.24 \cdot 10^{-4}$. This
generalization ability shows that the trained CNN can indeed learn the
difficult transfer function from DK to DVK. Future work will evaluate DVKs
estimated by CNNs with full monte-carlo simulations of a whole body CT to
predict patient specific voxel dose maps.
</p>
<a href="http://arxiv.org/abs/1805.09108" target="_blank">arXiv:1805.09108</a> [<a href="http://arxiv.org/pdf/1805.09108" target="_blank">pdf</a>]

<h2>Adaptive Stress Testing: Finding Likely Failure Events with Reinforcement Learning. (arXiv:1811.02188v3 [cs.AI] UPDATED)</h2>
<h3>Ritchie Lee, Ole J. Mengshoel, Anshu Saksena, Ryan Gardner, Daniel Genin, Joshua Silbermann, Michael Owen, Mykel J. Kochenderfer</h3>
<p>Finding the most likely path to a set of failure states is important to the
analysis of safety-critical systems that operate over a sequence of time steps,
such as aircraft collision avoidance systems and autonomous cars. In many
applications such as autonomous driving, failures cannot be completely
eliminated due to the complex stochastic environment in which the system
operates. As a result, safety validation is not only concerned about whether a
failure can occur, but also discovering which failures are most likely to
occur. This article presents adaptive stress testing (AST), a framework for
finding the most likely path to a failure event in simulation. We consider a
general black box setting for partially observable and continuous-valued
systems operating in an environment with stochastic disturbances. We formulate
the problem as a Markov decision process and use reinforcement learning to
optimize it. The approach is simulation-based and does not require internal
knowledge of the system, making it suitable for black-box testing of large
systems. We present formulations for fully observable and partially observable
systems. In the latter case, we present a modified Monte Carlo tree search
algorithm that only requires access to the pseudorandom number generator of the
simulator to overcome partial observability. We also present an extension of
the framework, called differential adaptive stress testing (DAST), that can
find failures that occur in one system but not in another. This type of
differential analysis is useful in applications such as regression testing,
where we are concerned with finding areas of relative weakness compared to a
baseline. We demonstrate the effectiveness of the approach on an aircraft
collision avoidance application, where a prototype aircraft collision avoidance
system is stress tested to find the most likely scenarios of near mid-air
collision.
</p>
<a href="http://arxiv.org/abs/1811.02188" target="_blank">arXiv:1811.02188</a> [<a href="http://arxiv.org/pdf/1811.02188" target="_blank">pdf</a>]

<h2>Learning to Attend Relevant Regions in Videos from Eye Fixations. (arXiv:1811.08594v3 [cs.CV] UPDATED)</h2>
<h3>Thanh T. Nguyen, Dung Nguyen</h3>
<p>Attentively important regions in video frames account for a majority part of
the semantics in each frame. This information is helpful in many applications
not only for entertainment (such as auto generating commentary and tourist
guide) but also for robotic control which holds a larascope supported for
laparoscopic surgery. However, it is not always straightforward to define and
locate such semantic regions in videos. In this work, we attempt to address the
problem of attending relevant regions in videos by leveraging the eye fixations
labels with a RNN-based visual attention model. Our experimental results
suggest that this approach holds a good potential to learn to attend semantic
regions in videos while its performance also heavily relies on the quality of
eye fixations labels.
</p>
<a href="http://arxiv.org/abs/1811.08594" target="_blank">arXiv:1811.08594</a> [<a href="http://arxiv.org/pdf/1811.08594" target="_blank">pdf</a>]

<h2>Distributed Classification of Urban Congestion Using VANET. (arXiv:1904.12685v2 [cs.LG] UPDATED)</h2>
<h3>Al Mallah Ranwa, Farooq Bilal, Quintero Alejandro</h3>
<p>Vehicular Ad-hoc NETworks (VANET) can efficiently detect traffic congestion,
but detection is not enough because congestion can be further classified as
recurrent and non-recurrent congestion (NRC). In particular, NRC in an urban
network is mainly caused by incidents, workzones, special events and adverse
weather. We propose a framework for the real-time distributed classification of
congestion into its components on a heterogeneous urban road network using
VANET. We present models built on an understanding of the spatial and temporal
causality measures and trained on synthetic data extended from a real case
study of Cologne. Our performance evaluation shows a predictive accuracy of
87.63\% for the deterministic Classification Tree (CT), 88.83\% for the Naive
Bayesian classifier (NB), 89.51\% for Random Forest (RF) and 89.17\% for the
boosting technique. This framework can assist transportation agencies in
reducing urban congestion by developing effective congestion mitigation
strategies knowing the root causes of congestion.
</p>
<a href="http://arxiv.org/abs/1904.12685" target="_blank">arXiv:1904.12685</a> [<a href="http://arxiv.org/pdf/1904.12685" target="_blank">pdf</a>]

<h2>Variational Fair Clustering. (arXiv:1906.08207v5 [cs.LG] UPDATED)</h2>
<h3>Imtiaz Masud Ziko, Eric Granger, Jing Yuan, Ismail Ben Ayed</h3>
<p>We propose a general variational framework of fair clustering, which
integrates an original Kullback-Leibler (KL) fairness term with a large class
of clustering objectives, including prototype or graph based. Fundamentally
different from the existing combinatorial and spectral solutions, our
variational multi-term approach enables to control the trade-off levels between
the fairness and clustering objectives. We derive a general tight upper bound
based on a concave-convex decomposition of our fairness term, its
Lipschitz-gradient property and the Pinsker's inequality. Our tight upper bound
can be jointly optimized with various clustering objectives, while yielding a
scalable solution, with convergence guarantee. Interestingly, at each
iteration, it performs an independent update for each assignment variable.
Therefore, it can be easily distributed for large-scale datasets. This
scalability is important as it enables to explore different trade-off levels
between the fairness and clustering objectives. Unlike spectral relaxation, our
formulation does not require computing its eigenvalue decomposition. We report
comprehensive evaluations and comparisons with state-of-the-art methods over
various fair-clustering benchmarks, which show that our variational formulation
can yield highly competitive solutions in terms of fairness and clustering
objectives.
</p>
<a href="http://arxiv.org/abs/1906.08207" target="_blank">arXiv:1906.08207</a> [<a href="http://arxiv.org/pdf/1906.08207" target="_blank">pdf</a>]

<h2>MRI Reconstruction Using Deep Bayesian Estimation. (arXiv:1909.01127v2 [cs.CV] UPDATED)</h2>
<h3>GuanXiong Luo, Na Zhao, Wenhao Jiang, Peng Cao</h3>
<p>Purpose: To develop a deep learning-based Bayesian inference for MRI
reconstruction. Methods: We modeled the MRI reconstruction problem with Bayes's
theorem, following the recently proposed PixelCNN++ method. The image
reconstruction from incomplete k-space measurement was obtained by maximizing
the posterior possibility. A generative network was utilized as the image
prior, which was computationally tractable, and the k-space data fidelity was
enforced by using an equality constraint. The stochastic backpropagation was
utilized to calculate the descent gradient in the process of maximum a
posterior, and a projected subgradient method was used to impose the equality
constraint. In contrast to the other deep learning reconstruction methods, the
proposed one used the likelihood of prior as the training loss and the
objective function in reconstruction to improve the image quality. Results: The
proposed method showed an improved performance in preserving image details and
reducing aliasing artifacts, compared with GRAPPA, $\ell_1$-ESPRiT, and MODL, a
state-of-the-art deep learning reconstruction method. The proposed method
generally achieved more than 5 dB peak signal-to-noise ratio improvement for
compressed sensing and parallel imaging reconstructions compared with the other
methods. Conclusion: The Bayesian inference significantly improved the
reconstruction performance, compared with the conventional $\ell_1$-sparsity
prior in compressed sensing reconstruction tasks. More importantly, the
proposed reconstruction framework can be generalized for most MRI
reconstruction scenarios.
</p>
<a href="http://arxiv.org/abs/1909.01127" target="_blank">arXiv:1909.01127</a> [<a href="http://arxiv.org/pdf/1909.01127" target="_blank">pdf</a>]

<h2>A nonlocal feature-driven exemplar-based approach for image inpainting. (arXiv:1909.09301v2 [cs.CV] UPDATED)</h2>
<h3>Viktor Reshniak, Jeremy Trageser, Clayton G. Webster</h3>
<p>We present a nonlocal variational image completion technique which admits
simultaneous inpainting of multiple structures and textures in a unified
framework. The recovery of geometric structures is achieved by using general
convolution operators as a measure of behavior within an image. These are
combined with a nonlocal exemplar-based approach to exploit the self-similarity
of an image in the selected feature domains and to ensure the inpainting of
textures. We also introduce an anisotropic patch distance metric to allow for
better control of the feature selection within an image and present a nonlocal
energy functional based on this metric. Finally, we derive an optimization
algorithm for the proposed variational model and examine its validity
experimentally with various test images.
</p>
<a href="http://arxiv.org/abs/1909.09301" target="_blank">arXiv:1909.09301</a> [<a href="http://arxiv.org/pdf/1909.09301" target="_blank">pdf</a>]

<h2>Variance Reduced Stochastic Proximal Algorithm for AUC Maximization. (arXiv:1911.03548v2 [stat.ML] UPDATED)</h2>
<h3>Soham Dan, Dushyant Sahoo</h3>
<p>Stochastic Gradient Descent has been widely studied with classification
accuracy as a performance measure. However, these stochastic algorithms cannot
be directly used when non-decomposable pairwise performance measures are used
such as Area under the ROC curve (AUC) which is a common performance metric
when the classes are imbalanced. There have been several algorithms proposed
for optimizing AUC as a performance metric, and one of the recent being a
stochastic proximal gradient algorithm (SPAM). But the downside of the
stochastic methods is that they suffer from high variance leading to slower
convergence. To combat this issue, several variance reduced methods have been
proposed with faster convergence guarantees than vanilla stochastic gradient
descent. Again, these variance reduced methods are not directly applicable when
non-decomposable performance measures are used. In this paper, we develop a
Variance Reduced Stochastic Proximal algorithm for AUC Maximization
(\textsc{VRSPAM}) and perform a theoretical analysis as well as empirical
analysis to show that our algorithm converges faster than SPAM which is the
previous state-of-the-art for the AUC maximization problem.
</p>
<a href="http://arxiv.org/abs/1911.03548" target="_blank">arXiv:1911.03548</a> [<a href="http://arxiv.org/pdf/1911.03548" target="_blank">pdf</a>]

<h2>An Empirical Study on the Relation between Network Interpretability and Adversarial Robustness. (arXiv:1912.03430v6 [cs.LG] UPDATED)</h2>
<h3>Adam Noack, Isaac Ahern, Dejing Dou, Boyang Li</h3>
<p>Deep neural networks (DNNs) have had many successes, but they suffer from two
major issues: (1) a vulnerability to adversarial examples and (2) a tendency to
elude human interpretation. Interestingly, recent empirical and theoretical
evidence suggests these two seemingly disparate issues are actually connected.
In particular, robust models tend to provide more interpretable gradients than
non-robust models. However, whether this relationship works in the opposite
direction remains obscure. With this paper, we seek empirical answers to the
following question: can models acquire adversarial robustness when they are
trained to have interpretable gradients? We introduce a theoretically inspired
technique called Interpretation Regularization (IR), which encourages a model's
gradients to (1) match the direction of interpretable target salience maps and
(2) have small magnitude. To assess model performance and tease apart factors
that contribute to adversarial robustness, we conduct extensive experiments on
MNIST and CIFAR-10 with both $\ell_2$ and $\ell_\infty$ attacks. We demonstrate
that training the networks to have interpretable gradients improves their
robustness to adversarial perturbations. Applying the network interpretation
technique SmoothGrad yields additional performance gains, especially in
cross-norm attacks and under heavy perturbations. The results indicate that the
interpretability of the model gradients is a crucial factor for adversarial
robustness. Code for the experiments can be found at
https://github.com/a1noack/interp_regularization.
</p>
<a href="http://arxiv.org/abs/1912.03430" target="_blank">arXiv:1912.03430</a> [<a href="http://arxiv.org/pdf/1912.03430" target="_blank">pdf</a>]

<h2>An Online Learning Framework for Energy-Efficient Navigation of Electric Vehicles. (arXiv:2003.01416v2 [cs.LG] UPDATED)</h2>
<h3>Niklas &#xc5;kerblom, Yuxin Chen, Morteza Haghir Chehreghani</h3>
<p>Energy-efficient navigation constitutes an important challenge in electric
vehicles, due to their limited battery capacity. We employ a Bayesian approach
to model the energy consumption at road segments for efficient navigation. In
order to learn the model parameters, we develop an online learning framework
and investigate several exploration strategies such as Thompson Sampling and
Upper Confidence Bound. We then extend our online learning framework to
multi-agent setting, where multiple vehicles adaptively navigate and learn the
parameters of the energy model. We analyze Thompson Sampling and establish
rigorous regret bounds on its performance. Finally, we demonstrate the
performance of our methods via several real-world experiments on Luxembourg
SUMO Traffic dataset.
</p>
<a href="http://arxiv.org/abs/2003.01416" target="_blank">arXiv:2003.01416</a> [<a href="http://arxiv.org/pdf/2003.01416" target="_blank">pdf</a>]

<h2>VarMixup: Exploiting the Latent Space for Robust Training and Inference. (arXiv:2003.06566v2 [cs.LG] UPDATED)</h2>
<h3>Puneet Mangla, Vedant Singh, Shreyas Jayant Havaldar, Vineeth N Balasubramanian</h3>
<p>The vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has
led to the development of many defense approaches. Among them, Adversarial
Training (AT) is a popular and widely used approach for training adversarially
robust models. Mixup Training (MT), a recent popular training algorithm,
improves the generalization performance of models by introducing globally
linear behavior in between training examples. Although still in its early
phase, we observe a shift in trend of exploiting Mixup from perspectives of
generalisation to that of adversarial robustness. It has been shown that the
Mixup trained models improves the robustness of models but only passively. A
recent approach, Mixup Inference (MI), proposes an inference principle for
Mixup trained models to counter adversarial examples at inference time by
mixing the input with other random clean samples. In this work, we propose a
new approach - \textit{VarMixup (Variational Mixup)} - to better sample mixup
images by using the latent manifold underlying the data. Our experiments on
CIFAR-10, CIFAR-100, SVHN and Tiny-Imagenet demonstrate that \textit{VarMixup}
beats state-of-the-art AT techniques without training the model adversarially.
Additionally, we also conduct ablations that show that models trained on
\textit{VarMixup} samples are also robust to various input
corruptions/perturbations, have low calibration error and are transferable.
</p>
<a href="http://arxiv.org/abs/2003.06566" target="_blank">arXiv:2003.06566</a> [<a href="http://arxiv.org/pdf/2003.06566" target="_blank">pdf</a>]

<h2>clDice -- a Topology-Preserving Loss Function for Tubular Structure Segmentation. (arXiv:2003.07311v4 [cs.CV] UPDATED)</h2>
<h3>Suprosanna Shit, Johannes C. Paetzold, Anjany Sekuboyina, Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien P. W. Pluim, Ulrich Bauer, Bjoern H. Menze</h3>
<p>Accurate segmentation of tubular, network-like structures, such as vessels,
neurons, or roads, is relevant to many fields of research. For such structures,
the topology is their most important characteristic; particularly preserving
connectedness: in the case of vascular networks, missing a connected vessel
entirely alters the blood-flow dynamics. We introduce a novel similarity
measure termed centerlineDice (short clDice), which is calculated on the
intersection of the segmentation masks and their (morphological) skeleta. We
theoretically prove that clDice guarantees topology preservation up to homotopy
equivalence for binary 2D and 3D segmentation. Extending this, we propose a
computationally efficient, differentiable loss function (soft-clDice) for
training arbitrary neural segmentation networks. We benchmark the soft-clDice
loss on five public datasets, including vessels, roads and neurons (2D and 3D).
Training on soft-clDice leads to segmentation with more accurate connectivity
information, higher graph similarity, and better volumetric scores.
</p>
<a href="http://arxiv.org/abs/2003.07311" target="_blank">arXiv:2003.07311</a> [<a href="http://arxiv.org/pdf/2003.07311" target="_blank">pdf</a>]

<h2>Toward Fine-grained Facial Expression Manipulation. (arXiv:2004.03132v2 [cs.CV] UPDATED)</h2>
<h3>Jun Ling, Han Xue, Li Song, Shuhui Yang, Rong Xie, Xiao Gu</h3>
<p>Facial expression manipulation aims at editing facial expression with a given
condition. Previous methods edit an input image under the guidance of a
discrete emotion label or absolute condition (e.g., facial action units) to
possess the desired expression. However, these methods either suffer from
changing condition-irrelevant regions or are inefficient for fine-grained
editing. In this study, we take these two objectives into consideration and
propose a novel method. First, we replace continuous absolute condition with
relative condition, specifically, relative action units. With relative action
units, the generator learns to only transform regions of interest which are
specified by non-zero-valued relative AUs. Second, our generator is built on
U-Net but strengthened by Multi-Scale Feature Fusion (MSF) mechanism for
high-quality expression editing purposes. Extensive experiments on both
quantitative and qualitative evaluation demonstrate the improvements of our
proposed approach compared to the state-of-the-art expression editing methods.
Code is available at \url{https://github.com/junleen/Expression-manipulator}.
</p>
<a href="http://arxiv.org/abs/2004.03132" target="_blank">arXiv:2004.03132</a> [<a href="http://arxiv.org/pdf/2004.03132" target="_blank">pdf</a>]

<h2>Neural Analogical Matching. (arXiv:2004.03573v4 [cs.AI] UPDATED)</h2>
<h3>Maxwell Crouse, Constantine Nakos, Ibrahim Abdelaziz, Kenneth Forbus</h3>
<p>Analogy is core to human cognition. It allows us to solve problems based on
prior experience, it governs the way we conceptualize new information, and it
even influences our visual perception. The importance of analogy to humans has
made it an active area of research in the broader field of artificial
intelligence, resulting in data-efficient models that learn and reason in
human-like ways. While cognitive perspectives of analogy and deep learning have
generally been studied independently of one another, the integration of the two
lines of research is a promising step towards more robust and efficient
learning techniques. As part of a growing body of research on such an
integration, we introduce the Analogical Matching Network: a neural
architecture that learns to produce analogies between structured, symbolic
representations that are largely consistent with the principles of
Structure-Mapping Theory.
</p>
<a href="http://arxiv.org/abs/2004.03573" target="_blank">arXiv:2004.03573</a> [<a href="http://arxiv.org/pdf/2004.03573" target="_blank">pdf</a>]

<h2>Mixture Density Conditional Generative Adversarial Network Models (MD-CGAN). (arXiv:2004.03797v3 [cs.LG] UPDATED)</h2>
<h3>Jaleh Zand, Stephen Roberts</h3>
<p>Generative Adversarial Networks (GANs) have gained significant attention in
recent years, with impressive applications highlighted in computer vision in
particular. Compared to such examples, however, there have been more limited
applications of GANs to time series modelling, including forecasting. In this
work, we present the Mixture Density Conditional Generative Adversarial Model
(MD-CGAN), with a focus on time series forecasting. We show that our model is
capable of estimating a probabilistic posterior distribution over forecasts and
that, in comparison to a set of benchmark methods, the MD-CGAN model performs
well, particularly in situations where noise is a significant component of the
observed time series. Further, by using a Gaussian mixture model as the output
distribution, MD-CGAN offers posterior predictions that are non-Gaussian.
</p>
<a href="http://arxiv.org/abs/2004.03797" target="_blank">arXiv:2004.03797</a> [<a href="http://arxiv.org/pdf/2004.03797" target="_blank">pdf</a>]

<h2>EfficientPose: Scalable single-person pose estimation. (arXiv:2004.12186v2 [cs.CV] UPDATED)</h2>
<h3>Daniel Groos, Heri Ramampiaro, Espen A. F. Ihlen</h3>
<p>Single-person human pose estimation facilitates markerless movement analysis
in sports, as well as in clinical applications. Still, state-of-the-art models
for human pose estimation generally do not meet the requirements of real-life
applications. The proliferation of deep learning techniques has resulted in the
development of many advanced approaches. However, with the progresses in the
field, more complex and inefficient models have also been introduced, which
have caused tremendous increases in computational demands. To cope with these
complexity and inefficiency challenges, we propose a novel convolutional neural
network architecture, called EfficientPose, which exploits recently proposed
EfficientNets in order to deliver efficient and scalable single-person pose
estimation. EfficientPose is a family of models harnessing an effective
multi-scale feature extractor and computationally efficient detection blocks
using mobile inverted bottleneck convolutions, while at the same time ensuring
that the precision of the pose configurations is still improved. Due to its low
complexity and efficiency, EfficientPose enables real-world applications on
edge devices by limiting the memory footprint and computational cost. The
results from our experiments, using the challenging MPII single-person
benchmark, show that the proposed EfficientPose models substantially outperform
the widely-used OpenPose model both in terms of accuracy and computational
efficiency. In particular, our top-performing model achieves state-of-the-art
accuracy on single-person MPII, with low-complexity ConvNets.
</p>
<a href="http://arxiv.org/abs/2004.12186" target="_blank">arXiv:2004.12186</a> [<a href="http://arxiv.org/pdf/2004.12186" target="_blank">pdf</a>]

<h2>Generalization Bounds via Information Density and Conditional Information Density. (arXiv:2005.08044v4 [cs.LG] UPDATED)</h2>
<h3>Fredrik Hellstr&#xf6;m, Giuseppe Durisi</h3>
<p>We present a general approach, based on an exponential inequality, to derive
bounds on the generalization error of randomized learning algorithms. Using
this approach, we provide bounds on the average generalization error as well as
bounds on its tail probability, for both the PAC-Bayesian and single-draw
scenarios. Specifically, for the case of subgaussian loss functions, we obtain
novel bounds that depend on the information density between the training data
and the output hypothesis. When suitably weakened, these bounds recover many of
the information-theoretic available bounds in the literature. We also extend
the proposed exponential-inequality approach to the setting recently introduced
by Steinke and Zakynthinou (2020), where the learning algorithm depends on a
randomly selected subset of the available training data. For this setup, we
present bounds for bounded loss functions in terms of the conditional
information density between the output hypothesis and the random variable
determining the subset choice, given all training data. Through our approach,
we recover the average generalization bound presented by Steinke and
Zakynthinou (2020) and extend it to the PAC-Bayesian and single-draw scenarios.
For the single-draw scenario, we also obtain novel bounds in terms of the
conditional $\alpha$-mutual information and the conditional maximal leakage.
</p>
<a href="http://arxiv.org/abs/2005.08044" target="_blank">arXiv:2005.08044</a> [<a href="http://arxiv.org/pdf/2005.08044" target="_blank">pdf</a>]

<h2>mvlearn: Multiview Machine Learning in Python. (arXiv:2005.11890v3 [stat.ML] UPDATED)</h2>
<h3>Ronan Perry, Gavin Mischler, Richard Guo, Theodore Lee, Alexander Chang, Arman Koul, Cameron Franz, Hugo Richard, Iain Carmichael, Pierre Ablin, Alexandre Gramfort, Joshua T. Vogelstein</h3>
<p>As data are generated more and more from multiple disparate sources,
multiview data sets, where each sample has features in distinct views, have
ballooned in recent years. However, no comprehensive package exists that
enables non-specialists to use these methods easily. mvlearn is a Python
library which implements the leading multiview machine learning methods. Its
simple API closely follows that of scikit-learn for increased ease-of-use. The
package can be installed from Python Package Index (PyPI) and the conda package
manager and is released under the MIT open-source license. The documentation,
detailed examples, and all releases are available at
https://mvlearn.github.io/.
</p>
<a href="http://arxiv.org/abs/2005.11890" target="_blank">arXiv:2005.11890</a> [<a href="http://arxiv.org/pdf/2005.11890" target="_blank">pdf</a>]

<h2>Learning Long-Term Dependencies in Irregularly-Sampled Time Series. (arXiv:2006.04418v4 [cs.LG] UPDATED)</h2>
<h3>Mathias Lechner, Ramin Hasani</h3>
<p>Recurrent neural networks (RNNs) with continuous-time hidden states are a
natural fit for modeling irregularly-sampled time series. These models,
however, face difficulties when the input data possess long-term dependencies.
We prove that similar to standard RNNs, the underlying reason for this issue is
the vanishing or exploding of the gradient during training. This phenomenon is
expressed by the ordinary differential equation (ODE) representation of the
hidden state, regardless of the ODE solver's choice. We provide a solution by
designing a new algorithm based on the long short-term memory (LSTM) that
separates its memory from its time-continuous state. This way, we encode a
continuous-time dynamical flow within the RNN, allowing it to respond to inputs
arriving at arbitrary time-lags while ensuring a constant error propagation
through the memory path. We call these RNN models ODE-LSTMs. We experimentally
show that ODE-LSTMs outperform advanced RNN-based counterparts on non-uniformly
sampled data with long-term dependencies. All code and data is available at
https://github.com/mlech26l/ode-lstms.
</p>
<a href="http://arxiv.org/abs/2006.04418" target="_blank">arXiv:2006.04418</a> [<a href="http://arxiv.org/pdf/2006.04418" target="_blank">pdf</a>]

<h2>Efficient Contextual Bandits with Continuous Actions. (arXiv:2006.06040v2 [cs.LG] UPDATED)</h2>
<h3>Maryam Majzoubi, Chicheng Zhang, Rajan Chari, Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins</h3>
<p>We create a computationally tractable algorithm for contextual bandits with
continuous actions having unknown structure. Our reduction-style algorithm
composes with most supervised learning representations. We prove that it works
in a general sense and verify the new functionality with large-scale
experiments.
</p>
<a href="http://arxiv.org/abs/2006.06040" target="_blank">arXiv:2006.06040</a> [<a href="http://arxiv.org/pdf/2006.06040" target="_blank">pdf</a>]

<h2>Fast Robust Subspace Tracking via PCA in Sparse Data-Dependent Noise. (arXiv:2006.08030v3 [cs.LG] UPDATED)</h2>
<h3>Praneeth Narayanamurthy, Namrata Vaswani</h3>
<p>This work studies the robust subspace tracking (ST) problem. Robust ST can be
simply understood as a (slow) time-varying subspace extension of robust PCA. It
assumes that the true data lies in a low-dimensional subspace that is either
fixed or changes slowly with time. The goal is to track the changing subspaces
over time in the presence of additive sparse outliers and to do this quickly
(with a short delay). We introduce a "fast" mini-batch robust ST solution that
is provably correct under mild assumptions. Here "fast" means two things: (i)
the subspace changes can be detected and the subspaces can be tracked with
near-optimal delay, and (ii) the time complexity of doing this is the same as
that of simple (non-robust) PCA. Our main result assumes piecewise constant
subspaces (needed for identifiability), but we also provide a corollary for the
case when there is a little change at each time.

A second contribution is a novel non-asymptotic guarantee for PCA in linearly
data-dependent noise. An important setting where this is useful is for linearly
data dependent noise that is sparse with support that changes enough over time.
The analysis of the subspace update step of our proposed robust ST solution
uses this result.
</p>
<a href="http://arxiv.org/abs/2006.08030" target="_blank">arXiv:2006.08030</a> [<a href="http://arxiv.org/pdf/2006.08030" target="_blank">pdf</a>]

<h2>DisARM: An Antithetic Gradient Estimator for Binary Latent Variables. (arXiv:2006.10680v2 [cs.LG] UPDATED)</h2>
<h3>Zhe Dong, Andriy Mnih, George Tucker</h3>
<p>Training models with discrete latent variables is challenging due to the
difficulty of estimating the gradients accurately. Much of the recent progress
has been achieved by taking advantage of continuous relaxations of the system,
which are not always available or even possible. The Augment-REINFORCE-Merge
(ARM) estimator provides an alternative that, instead of relaxation, uses
continuous augmentation. Applying antithetic sampling over the augmenting
variables yields a relatively low-variance and unbiased estimator applicable to
any model with binary latent variables. However, while antithetic sampling
reduces variance, the augmentation process increases variance. We show that ARM
can be improved by analytically integrating out the randomness introduced by
the augmentation process, guaranteeing substantial variance reduction. Our
estimator, DisARM, is simple to implement and has the same computational cost
as ARM. We evaluate DisARM on several generative modeling benchmarks and show
that it consistently outperforms ARM and a strong independent sample baseline
in terms of both variance and log-likelihood. Furthermore, we propose a local
version of DisARM designed for optimizing the multi-sample variational bound,
and show that it outperforms VIMCO, the current state-of-the-art method.
</p>
<a href="http://arxiv.org/abs/2006.10680" target="_blank">arXiv:2006.10680</a> [<a href="http://arxiv.org/pdf/2006.10680" target="_blank">pdf</a>]

<h2>Efficient implementations of echo state network cross-validation. (arXiv:2006.11282v2 [cs.LG] UPDATED)</h2>
<h3>Mantas Luko&#x161;evi&#x10d;ius, Arnas Uselis</h3>
<p>Background/introduction: Cross-Validation (CV) is still uncommon in time
series modeling. Echo State Networks (ESNs), as a prime example of Reservoir
Computing (RC) models, are known for their fast and precise one-shot learning,
that often benefit from good hyper-parameter tuning. This makes them ideal to
change the status quo.

Methods: We discuss CV of time series for predicting a concrete time interval
of interest, suggest several schemes for cross-validating ESNs and introduce an
efficient algorithm for implementing them. This algorithm is presented as two
levels of optimizations of doing $k$-fold CV. Training an RC model typically
consists of two stages: (i) running the reservoir with the data and (ii)
computing the optimal readouts. The first level of our optimization addresses
the most computationally expensive part (i) and makes it remain constant
irrespective of $k$. It dramatically reduces reservoir computations in any type
of RC system and is enough if $k$ is small. The second level of optimization
also makes the (ii) part remain constant irrespective of large $k$, as long as
the dimension of the output is low. We discuss when the proposed validation
schemes for ESNs could be beneficial, three options for producing the final
model and empirically investigate them on six different real-world datasets, as
well as do empirical computation time experiments. We provide the code in an
online repository.

Results: Proposed CV schemes give better and more stable test performance in
all the six different real-world datasets, three task types. Empirical run
times confirm our complexity analysis.

Conclusions: In most situations $k$-fold CV of ESNs and many other RC models
can be done for virtually the same time and space complexity as a simple
single-split validation. This enables CV to become a standard practice in RC.
</p>
<a href="http://arxiv.org/abs/2006.11282" target="_blank">arXiv:2006.11282</a> [<a href="http://arxiv.org/pdf/2006.11282" target="_blank">pdf</a>]

<h2>BRUL\`E: Barycenter-Regularized Unsupervised Landmark Extraction. (arXiv:2006.11643v2 [cs.CV] UPDATED)</h2>
<h3>Iaroslav Bespalov, Nazar Buzun, Dmitry V. Dylov</h3>
<p>Unsupervised retrieval of image features is vital for many computer vision
tasks where the annotation is missing or scarce. In this work, we propose a new
unsupervised approach to detect the landmarks in images, and we validate it on
the popular task of human face key-points extraction. The method is based on
the idea of auto-encoding the wanted landmarks in the latent space while
discarding the non-essential information in the image (and effectively
preserving the interpretability). The interpretable latent space representation
is achieved with the aid of a new two-step regularization paradigm. The first
regularization step evaluates transport distance from a given set of landmarks
to the average value (the barycenter by Wasserstein distance). The second
regularization step controls deviations from the barycenter by applying random
geometric deformations synchronously to the initial image and to the encoded
landmarks. During decoding, we add style features generated from the noise and
reconstruct the initial image by the generative adversarial network (GAN) with
transposed convolutions modulated by this style. We demonstrate the
effectiveness of the approach both in unsupervised and in semi-supervised
training scenarios using 300-W, CelebA, and MAFL datasets. The proposed
regularization paradigm is shown to prevent overfitting, and the detection
quality is shown to improve beyond the supervised outcome.
</p>
<a href="http://arxiv.org/abs/2006.11643" target="_blank">arXiv:2006.11643</a> [<a href="http://arxiv.org/pdf/2006.11643" target="_blank">pdf</a>]

<h2>Generalisation Guarantees for Continual Learning with Orthogonal Gradient Descent. (arXiv:2006.11942v4 [stat.ML] UPDATED)</h2>
<h3>Mehdi Abbana Bennani, Thang Doan, Masashi Sugiyama</h3>
<p>In Continual Learning settings, deep neural networks are prone to
Catastrophic Forgetting. Orthogonal Gradient Descent was proposed to tackle the
challenge. However, no theoretical guarantees have been proven yet. We present
a theoretical framework to study Continual Learning algorithms in the Neural
Tangent Kernel regime. This framework comprises closed form expression of the
model through tasks and proxies for Transfer Learning, generalisation and tasks
similarity. In this framework, we prove that OGD is robust to Catastrophic
Forgetting then derive the first generalisation bound for SGD and OGD for
Continual Learning. Finally, we study the limits of this framework in practice
for OGD and highlight the importance of the Neural Tangent Kernel variation for
Continual Learning with OGD.
</p>
<a href="http://arxiv.org/abs/2006.11942" target="_blank">arXiv:2006.11942</a> [<a href="http://arxiv.org/pdf/2006.11942" target="_blank">pdf</a>]

<h2>Robust Linear Regression: Optimal Rates in Polynomial Time. (arXiv:2007.01394v4 [stat.ML] UPDATED)</h2>
<h3>Ainesh Bakshi, Adarsh Prasad</h3>
<p>We obtain robust and computationally efficient estimators for learning
several linear models that achieve statistically optimal convergence rate under
minimal distributional assumptions. Concretely, we assume our data is drawn
from a $k$-hypercontractive distribution and an $\epsilon$-fraction is
adversarially corrupted. We then describe an estimator that converges to the
optimal least-squares minimizer for the true distribution at a rate
proportional to $\epsilon^{2-2/k}$, when the noise is independent of the
covariates. We note that no such estimator was known prior to our work, even
with access to unbounded computation. The rate we achieve is
information-theoretically optimal and thus we resolve the main open question in
Klivans, Kothari and Meka [COLT'18].

Our key insight is to identify an analytic condition that serves as a
polynomial relaxation of independence of random variables. In particular, we
show that when the moments of the noise and covariates are
negatively-correlated, we obtain the same rate as independent noise. Further,
when the condition is not satisfied, we obtain a rate proportional to
$\epsilon^{2-4/k}$, and again match the information-theoretic lower bound. Our
central technical contribution is to algorithmically exploit independence of
random variables in the "sum-of-squares" framework by formulating it as the
aforementioned polynomial inequality.
</p>
<a href="http://arxiv.org/abs/2007.01394" target="_blank">arXiv:2007.01394</a> [<a href="http://arxiv.org/pdf/2007.01394" target="_blank">pdf</a>]

<h2>Counterfactual Data Augmentation using Locally Factored Dynamics. (arXiv:2007.02863v2 [cs.LG] UPDATED)</h2>
<h3>Silviu Pitis, Elliot Creager, Animesh Garg</h3>
<p>Many dynamic processes, including common scenarios in robotic control and
reinforcement learning (RL), involve a set of interacting subprocesses. Though
the subprocesses are not independent, their interactions are often sparse, and
the dynamics at any given time step can often be decomposed into locally
independent causal mechanisms. Such local causal structures can be leveraged to
improve the sample efficiency of sequence prediction and off-policy
reinforcement learning. We formalize this by introducing local causal models
(LCMs), which are induced from a global causal model by conditioning on a
subset of the state space. We propose an approach to inferring these structures
given an object-oriented state representation, as well as a novel algorithm for
Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an
experience replay to generate counterfactual experiences that are causally
valid in the global model. We find that CoDA significantly improves the
performance of RL agents in locally factored tasks, including the
batch-constrained and goal-conditioned settings.
</p>
<a href="http://arxiv.org/abs/2007.02863" target="_blank">arXiv:2007.02863</a> [<a href="http://arxiv.org/pdf/2007.02863" target="_blank">pdf</a>]

<h2>TripMD: Driving patterns investigation via Motif Analysis. (arXiv:2007.03727v2 [cs.AI] UPDATED)</h2>
<h3>Maria In&#xea; Silva, Roberto Henriques</h3>
<p>Processing driving data and investigating driving behavior has been receiving
an increasing interest in the last decades, with applications ranging from car
insurance pricing to policy making. A common strategy to analyze driving
behavior analysis is to study the maneuvers being performance by the driver. In
this paper, we propose TripMD, a system that extracts the most relevant driving
patterns from sensor recordings (such as acceleration) and provides a
visualization that allows for an easy investigation. Additionally, we test our
system using the UAH-DriveSet dataset, a publicly available naturalistic
driving dataset. We show that (1) our system can extract a rich number of
driving patterns from a single driver that are meaningful to understand driving
behaviors and (2) our system can be used to identify the driving behavior of an
unknown driver from a set of drivers whose behavior we know.
</p>
<a href="http://arxiv.org/abs/2007.03727" target="_blank">arXiv:2007.03727</a> [<a href="http://arxiv.org/pdf/2007.03727" target="_blank">pdf</a>]

<h2>Personalized Cross-Silo Federated Learning on Non-IID Data. (arXiv:2007.03797v2 [cs.LG] UPDATED)</h2>
<h3>Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, Yong Zhang</h3>
<p>Non-IID data present a tough challenge for federated learning. In this paper,
we explore a novel idea of facilitating pairwise collaborations between clients
with similar data. We propose FedAMP, a new method employing federated
attentive message passing to facilitate similar clients to collaborate more. We
establish the convergence of FedAMP for both convex and non-convex models, and
propose a heuristic method to further improve the performance of FedAMP when
clients adopt deep neural networks as personalized models. Our extensive
experiments on benchmark data sets demonstrate the superior performance of the
proposed methods.
</p>
<a href="http://arxiv.org/abs/2007.03797" target="_blank">arXiv:2007.03797</a> [<a href="http://arxiv.org/pdf/2007.03797" target="_blank">pdf</a>]

<h2>Collaborative Localization of Aerial and Ground Mobile Robots through Orthomosaic Map. (arXiv:2007.11233v3 [cs.RO] UPDATED)</h2>
<h3>Xuecheng Xu, Zexi Chen, Jiaxin Guo, Yue Wang, Yunkai Wang, Rong Xiong</h3>
<p>With the deepening of research on the SLAM system, the possibility of
cooperative SLAM with multi-robots has been proposed. This paper presents a map
matching and localization approach considering the cooperative SLAM of an
aerial-ground system. The proposed approach aims to help precisely matching the
map constructed by two independent systems that have large scale variance of
viewpoints of the same route and eventually enables the ground mobile robot to
localize itself in the global map given by the drone. It contains dense mapping
with Elevation Map and software "Metashape", map matching with a proposed
template matching algorithm, weighted normalized cross-correlation (WNCC) and
localization with particle filter. The approach enables map matching for
cooperative SLAM with the feasibility of multiple scene sensors, varies from
stereo cameras to lidars, and is insensitive to the synchronization of the two
systems. We demonstrate the accuracy, robustness, and the speed of the approach
under experiments of the Aero-Ground Dataset.
</p>
<a href="http://arxiv.org/abs/2007.11233" target="_blank">arXiv:2007.11233</a> [<a href="http://arxiv.org/pdf/2007.11233" target="_blank">pdf</a>]

<h2>Mask Point R-CNN. (arXiv:2008.00460v2 [cs.CV] UPDATED)</h2>
<h3>Wenchao Zhang, Chong Fu, Mai Zhu</h3>
<p>The attributes of object contours has great significance for instance
segmentation task. However, most of the current popular deep neural networks do
not pay much attention to the target edge information. Inspired by the human
annotation process when making instance segmentation datasets, in this paper,
we propose Mask Point RCNN aiming at promoting the neural networks attention to
the target edge information, which can heighten the information propagates
between multiple tasks by using different attributes features. Specifically, we
present an auxiliary task to Mask RCNN, including utilizing keypoint detection
technology to construct the target edge contour, and enhancing the sensitivity
of the network to the object edge through multi task learning and feature
fusion. These improvements are easy to implement and have a small amount of
additional computing overhead. By extensive evaluations on the Cityscapes
dataset, the results show that our approach outperforms vanilla Mask RCNN by
5.4% on the validation subset and 5.0% on the test subset.
</p>
<a href="http://arxiv.org/abs/2008.00460" target="_blank">arXiv:2008.00460</a> [<a href="http://arxiv.org/pdf/2008.00460" target="_blank">pdf</a>]

<h2>From Rain Generation to Rain Removal. (arXiv:2008.03580v2 [cs.CV] UPDATED)</h2>
<h3>Hong Wang, Zongsheng Yue, Qi Xie, Qian Zhao, Yefeng Zheng, Deyu Meng</h3>
<p>For the single image rain removal (SIRR) task, the performance of deep
learning (DL)-based methods is mainly affected by the designed deraining models
and training datasets. Most of current state-of-the-art focus on constructing
powerful deep models to obtain better deraining results. In this paper, to
further improve the deraining performance, we novelly attempt to handle the
SIRR task from the perspective of training datasets by exploring a more
efficient way to synthesize rainy images. Specifically, we build a full
Bayesian generative model for rainy image where the rain layer is parameterized
as a generator with the input as some latent variables representing the
physical structural rain factors, e.g., direction, scale, and thickness. To
solve this model, we employ the variational inference framework to approximate
the expected statistical distribution of rainy image in a data-driven manner.
With the learned generator, we can automatically and sufficiently generate
diverse and non-repetitive training pairs so as to efficiently enrich and
augment the existing benchmark datasets. User study qualitatively and
quantitatively evaluates the realism of generated rainy images. Comprehensive
experiments substantiate that the proposed model can faithfully extract the
complex rain distribution that not only helps significantly improve the
deraining performance of current deep single image derainers, but also largely
loosens the requirement of large training sample pre-collection for the SIRR
task.
</p>
<a href="http://arxiv.org/abs/2008.03580" target="_blank">arXiv:2008.03580</a> [<a href="http://arxiv.org/pdf/2008.03580" target="_blank">pdf</a>]

<h2>Parameter Sharing Exploration and Hetero-Center based Triplet Loss for Visible-Thermal Person Re-Identification. (arXiv:2008.06223v2 [cs.CV] UPDATED)</h2>
<h3>Haijun Liu, Xiaoheng Tan, Xichuan Zhou</h3>
<p>This paper focuses on the visible-thermal cross-modality person
re-identification (VT Re-ID) task, whose goal is to match person images between
the daytime visible modality and the nighttime thermal modality. The two-stream
network is usually adopted to address the cross-modality discrepancy, the most
challenging problem for VT Re-ID, by learning the multi-modality person
features. In this paper, we explore how many parameters of two-stream network
should share, which is still not well investigated in the existing literature.
By well splitting the ResNet50 model to construct the modality-specific feature
extracting network and modality-sharing feature embedding network, we
experimentally demonstrate the effect of parameters sharing of two-stream
network for VT Re-ID. Moreover, in the framework of part-level person feature
learning, we propose the hetero-center based triplet loss to relax the strict
constraint of traditional triplet loss through replacing the comparison of
anchor to all the other samples by anchor center to all the other centers. With
the extremely simple means, the proposed method can significantly improve the
VT Re-ID performance. The experimental results on two datasets show that our
proposed method distinctly outperforms the state-of-the-art methods by large
margins, especially on RegDB dataset achieving superior performance,
rank1/mAP/mINP 91.05%/83.28%/68.84%. It can be a new baseline for VT Re-ID,
with a simple but effective strategy.
</p>
<a href="http://arxiv.org/abs/2008.06223" target="_blank">arXiv:2008.06223</a> [<a href="http://arxiv.org/pdf/2008.06223" target="_blank">pdf</a>]

<h2>Memory-based Jitter: Improving Visual Recognition on Long-tailed Data with Diversity In Memory. (arXiv:2008.09809v3 [cs.CV] UPDATED)</h2>
<h3>Jialun Liu, Jingwei Zhang, Wenhui Li, Chi Zhang, Yifan Sun</h3>
<p>This paper considers deep visual recognition on long-tailed data. To be
general, we consider two applied scenarios, \ie, deep classification and deep
metric learning. Under the long-tailed data distribution, the majority classes
(\ie, tail classes) only occupy relatively few samples and are prone to lack of
within-class diversity. A radical solution is to augment the tail classes with
higher diversity. To this end, we introduce a simple and reliable method named
Memory-based Jitter (MBJ). We observe that during training, the deep model
constantly changes its parameters after every iteration, yielding the
phenomenon of \emph{weight jitters}. Consequentially, given a same image as the
input, two historical editions of the model generate two different features in
the deeply-embedded space, resulting in \emph{feature jitters}. Using a memory
bank, we collect these (model or feature) jitters across multiple training
iterations and get the so-called Memory-based Jitter. The accumulated jitters
enhance the within-class diversity for the tail classes and consequentially
improves long-tailed visual recognition. With slight modifications, MBJ is
applicable for two fundamental visual recognition tasks, \emph{i.e.}, deep
image classification and deep metric learning (on long-tailed data). Extensive
experiments on five long-tailed classification benchmarks and two deep metric
learning benchmarks demonstrate significant improvement. Moreover, the achieved
performance are on par with the state of the art on both tasks.
</p>
<a href="http://arxiv.org/abs/2008.09809" target="_blank">arXiv:2008.09809</a> [<a href="http://arxiv.org/pdf/2008.09809" target="_blank">pdf</a>]

<h2>Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition. (arXiv:2009.04724v2 [cs.CV] UPDATED)</h2>
<h3>Siteng Huang, Min Zhang, Yachen Kang, Donglin Wang</h3>
<p>The purpose of few-shot recognition is to recognize novel categories with a
limited number of labeled examples in each class. To encourage learning from a
supplementary view, recent approaches have introduced auxiliary semantic
modalities into effective metric-learning frameworks that aim to learn a
feature similarity between training samples (support set) and test samples
(query set). However, these approaches only augment the representations of
samples with available semantics while ignoring the query set, which loses the
potential for the improvement and may lead to a shift between the modalities
combination and the pure-visual representation. In this paper, we devise an
attributes-guided attention module (AGAM) to utilize human-annotated attributes
and learn more discriminative features. This plug-and-play module enables
visual contents and corresponding attributes to collectively focus on important
channels and regions for the support set. And the feature selection is also
achieved for query set with only visual information while the attributes are
not available. Therefore, representations from both sets are improved in a
fine-grained manner. Moreover, an attention alignment mechanism is proposed to
distill knowledge from the guidance of attributes to the pure-visual branch for
samples without attributes. Extensive experiments and analysis show that our
proposed module can significantly improve simple metric-based approaches to
achieve state-of-the-art performance on different datasets and settings.
</p>
<a href="http://arxiv.org/abs/2009.04724" target="_blank">arXiv:2009.04724</a> [<a href="http://arxiv.org/pdf/2009.04724" target="_blank">pdf</a>]

<h2>Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion. (arXiv:2009.05757v2 [cs.CV] UPDATED)</h2>
<h3>Jinpeng Wang, Yuting Gao, Ke Li, Jianguo Hu, Xinyang Jiang, Xiaowei Guo, Rongrong Ji, Xing Sun</h3>
<p>One significant factor we expect the video representation learning to
capture, especially in contrast with the image representation learning, is the
object motion. However, we found that in the current mainstream video datasets,
some action categories are highly related with the scene where the action
happens, making the model tend to degrade to a solution where only the scene
information is encoded. For example, a trained model may predict a video as
playing football simply because it sees the field, neglecting that the subject
is dancing as a cheerleader on the field. This is against our original
intention towards the video representation learning and may bring scene bias on
different dataset that can not be ignored. In order to tackle this problem, we
propose to decouple the scene and the motion (DSM) with two simple operations,
so that the model attention towards the motion information is better paid.
Specifically, we construct a positive clip and a negative clip for each video.
Compared to the original video, the positive/negative is
motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance
and Temporal Local Disturbance. Our objective is to pull the positive closer
while pushing the negative farther to the original clip in the latent space. In
this way, the impact of the scene is weakened while the temporal sensitivity of
the network is further enhanced. We conduct experiments on two tasks with
various backbones and different pre-training datasets, and find that our method
surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards
action recognition task on the UCF101 and HMDB51 datasets respectively using
the same backbone.
</p>
<a href="http://arxiv.org/abs/2009.05757" target="_blank">arXiv:2009.05757</a> [<a href="http://arxiv.org/pdf/2009.05757" target="_blank">pdf</a>]

<h2>Large-Scale Intelligent Microservices. (arXiv:2009.08044v2 [cs.AI] UPDATED)</h2>
<h3>Mark Hamilton, Nick Gonsalves, Christina Lee, Anand Raman, Brendan Walsh, Siddhartha Prasad, Dalitso Banda, Lucy Zhang, Lei Zhang, William T. Freeman</h3>
<p>Deploying Machine Learning (ML) algorithms within databases is a challenge
due to the varied computational footprints of modern ML algorithms and the
myriad of database technologies each with its own restrictive syntax. We
introduce an Apache Spark-based micro-service orchestration framework that
extends database operations to include web service primitives. Our system can
orchestrate web services across hundreds of machines and takes full advantage
of cluster, thread, and asynchronous parallelism. Using this framework, we
provide large scale clients for intelligent services such as speech, vision,
search, anomaly detection, and text analysis. This allows users to integrate
ready-to-use intelligence into any datastore with an Apache Spark connector. To
eliminate the majority of overhead from network communication, we also
introduce a low-latency containerized version of our architecture. Finally, we
demonstrate that the services we investigate are competitive on a variety of
benchmarks, and present two applications of this framework to create
intelligent search engines, and real-time auto race analytics systems.
</p>
<a href="http://arxiv.org/abs/2009.08044" target="_blank">arXiv:2009.08044</a> [<a href="http://arxiv.org/pdf/2009.08044" target="_blank">pdf</a>]

<h2>Reinforcement Learning Approaches in Social Robotics. (arXiv:2009.09689v2 [cs.RO] UPDATED)</h2>
<h3>Neziha Akalin, Amy Loutfi</h3>
<p>This article surveys reinforcement learning (RL) approaches in social
robotics. RL is a framework for decision-making problems in which an agent
interacts through trial-and-error with its environment to discover an optimal
behavior. Since interaction is a key component in both RL and social robotics,
it can be a well-suited approach for real-world interactions with physically
embodied social robots. The scope of the paper is focused particularly on
studies that include social physical robots and real-world human-robot
interactions with users. In addition to a survey, we categorize existent RL
approaches based on the design of the reward mechanisms. This categorization
includes three major themes: interactive reinforcement learning, intrinsically
motivated methods, and task performance-driven methods. Thus, this paper aims
to become a starting point for researchers interested to use and apply
reinforcement learning methods in this particular research field.
</p>
<a href="http://arxiv.org/abs/2009.09689" target="_blank">arXiv:2009.09689</a> [<a href="http://arxiv.org/pdf/2009.09689" target="_blank">pdf</a>]

<h2>Streaming Graph Neural Networks via Continual Learning. (arXiv:2009.10951v2 [cs.LG] UPDATED)</h2>
<h3>Junshan Wang, Guojie Song, Yi Wu, Liang Wang</h3>
<p>Graph neural networks (GNNs) have achieved strong performance in various
applications. In the real world, network data is usually formed in a streaming
fashion. The distributions of patterns that refer to neighborhood information
of nodes may shift over time. The GNN model needs to learn the new patterns
that cannot yet be captured. But learning incrementally leads to the
catastrophic forgetting problem that historical knowledge is overwritten by
newly learned knowledge. Therefore, it is important to train GNN model to learn
new patterns and maintain existing patterns simultaneously, which few works
focus on. In this paper, we propose a streaming GNN model based on continual
learning so that the model is trained incrementally and up-to-date node
representations can be obtained at each time step. Firstly, we design an
approximation algorithm to detect new coming patterns efficiently based on
information propagation. Secondly, we combine two perspectives of data
replaying and model regularization for existing pattern consolidation.
Specially, a hierarchy-importance sampling strategy for nodes is designed and a
weighted regularization term for GNN parameters is derived, achieving greater
stability and generalization of knowledge consolidation. Our model is evaluated
on real and synthetic data sets and compared with multiple baselines. The
results of node classification prove that our model can efficiently update
model parameters and achieve comparable performance to model retraining. In
addition, we also conduct a case study on the synthetic data, and carry out
some specific analysis for each part of our model, illustrating its ability to
learn new knowledge and maintain existing knowledge from different
perspectives.
</p>
<a href="http://arxiv.org/abs/2009.10951" target="_blank">arXiv:2009.10951</a> [<a href="http://arxiv.org/pdf/2009.10951" target="_blank">pdf</a>]

<h2>Interventional Few-Shot Learning. (arXiv:2009.13000v2 [cs.LG] UPDATED)</h2>
<h3>Zhongqi Yue, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua</h3>
<p>We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning
(FSL) methods: the pre-trained knowledge is indeed a confounder that limits the
performance. This finding is rooted from our causal assumption: a Structural
Causal Model (SCM) for the causalities among the pre-trained knowledge, sample
features, and labels. Thanks to it, we propose a novel FSL paradigm:
Interventional Few-Shot Learning (IFSL). Specifically, we develop three
effective IFSL algorithmic implementations based on the backdoor adjustment,
which is essentially a causal intervention towards the SCM of many-shot
learning: the upper-bound of FSL in a causal view. It is worth noting that the
contribution of IFSL is orthogonal to existing fine-tuning and meta-learning
based FSL methods, hence IFSL can improve all of them, achieving a new
1-/5-shot state-of-the-art on \textit{mini}ImageNet, \textit{tiered}ImageNet,
and cross-domain CUB. Code is released at https://github.com/yue-zhongqi/ifsl.
</p>
<a href="http://arxiv.org/abs/2009.13000" target="_blank">arXiv:2009.13000</a> [<a href="http://arxiv.org/pdf/2009.13000" target="_blank">pdf</a>]

<h2>Hard Negative Mixing for Contrastive Learning. (arXiv:2010.01028v2 [cs.CV] UPDATED)</h2>
<h3>Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, Diane Larlus</h3>
<p>Contrastive learning has become a key component of self-supervised learning
approaches for computer vision. By learning to embed two augmented versions of
the same image close to each other and to push the embeddings of different
images apart, one can train highly transferable visual representations. As
revealed by recent studies, heavy data augmentation and large sets of negatives
are both crucial in learning such representations. At the same time, data
mixing strategies either at the image or the feature level improve both
supervised and semi-supervised learning by synthesizing novel examples, forcing
networks to learn more robust features. In this paper, we argue that an
important aspect of contrastive learning, i.e., the effect of hard negatives,
has so far been neglected. To get more meaningful negative samples, current top
contrastive self-supervised learning approaches either substantially increase
the batch sizes, or keep very large memory banks; increasing the memory size,
however, leads to diminishing returns in terms of performance. We therefore
start by delving deeper into a top-performing framework and show evidence that
harder negatives are needed to facilitate better and faster learning. Based on
these observations, and motivated by the success of data mixing, we propose
hard negative mixing strategies at the feature level, that can be computed
on-the-fly with a minimal computational overhead. We exhaustively ablate our
approach on linear classification, object detection and instance segmentation
and show that employing our hard negative mixing procedure improves the quality
of visual representations learned by a state-of-the-art self-supervised
learning method.
</p>
<a href="http://arxiv.org/abs/2010.01028" target="_blank">arXiv:2010.01028</a> [<a href="http://arxiv.org/pdf/2010.01028" target="_blank">pdf</a>]

<h2>Sharpness-Aware Minimization for Efficiently Improving Generalization. (arXiv:2010.01412v2 [cs.LG] UPDATED)</h2>
<h3>Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur</h3>
<p>In today's heavily overparameterized models, the value of the training loss
provides few guarantees on model generalization ability. Indeed, optimizing
only the training loss value, as is commonly done, can easily lead to
suboptimal model quality. Motivated by the connection between geometry of the
loss landscape and generalization -- including a generalization bound that we
prove here -- we introduce a novel, effective procedure for instead
simultaneously minimizing loss value and loss sharpness. In particular, our
procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in
neighborhoods having uniformly low loss; this formulation results in a min-max
optimization problem on which gradient descent can be performed efficiently. We
present empirical results showing that SAM improves model generalization across
a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning
tasks) and models, yielding novel state-of-the-art performance for several.
Additionally, we find that SAM natively provides robustness to label noise on
par with that provided by state-of-the-art procedures that specifically target
learning with noisy labels.
</p>
<a href="http://arxiv.org/abs/2010.01412" target="_blank">arXiv:2010.01412</a> [<a href="http://arxiv.org/pdf/2010.01412" target="_blank">pdf</a>]

<h2>Jointly-Trained State-Action Embedding for Efficient Reinforcement Learning. (arXiv:2010.04444v3 [cs.LG] UPDATED)</h2>
<h3>Paul J. Pritz, Liang Ma, Kin K. Leung</h3>
<p>While reinforcement learning has achieved considerable successes in recent
years, state-of-the-art models are often still limited by the size of state and
action spaces. Model-free reinforcement learning approaches use some form of
state representations and the latest work has explored embedding techniques for
actions, both with the aim of achieving better generalization and
applicability. However, these approaches consider only states or actions,
ignoring the interaction between them when generating embedded representations.
In this work, we propose a new approach for jointly learning embeddings for
states and actions that combines aspects of model-free and model-based
reinforcement learning, which can be applied in both discrete and continuous
domains. Specifically, we use a model of the environment to obtain embeddings
for states and actions and present a generic architecture that uses these to
learn a policy. In this way, the embedded representations obtained via our
approach enable better generalization over both states and actions by capturing
similarities in the embedding spaces. Evaluations of our approach on several
gaming, robotic control, and recommender systems show it significantly
outperforms state-of-the-art models in both discrete/continuous domains with
large state/action spaces, thus confirming its efficacy and the overall
superior performance.
</p>
<a href="http://arxiv.org/abs/2010.04444" target="_blank">arXiv:2010.04444</a> [<a href="http://arxiv.org/pdf/2010.04444" target="_blank">pdf</a>]

<h2>Bilevel Optimization: Nonasymptotic Analysis and Faster Algorithms. (arXiv:2010.07962v2 [cs.LG] UPDATED)</h2>
<h3>Kaiyi Ji, Junjie Yang, Yingbin Liang</h3>
<p>Bilevel optimization has arisen as a powerful tool for many machine learning
problems such as meta-learning, hyperparameter optimization, and reinforcement
learning. In this paper, we investigate the nonconvex-strongly-convex bilevel
optimization problem. For deterministic bilevel optimization, we provide a
comprehensive finite-time convergence analysis for two popular algorithms
respectively based on approximate implicit differentiation (AID) and iterative
differentiation (ITD). For the AID-based method, we orderwisely improve the
previous finite-time convergence analysis due to a more practical parameter
selection as well as a warm start strategy, and for the ITD-based method we
establish the first theoretical convergence rate. Our analysis also provides a
quantitative comparison between ITD and AID based approaches. For stochastic
bilevel optimization, we propose a novel algorithm named stocBiO, which
features a sample-efficient hypergradient estimator using efficient Jacobian-
and Hessian-vector product computations. We provide the finite-time convergence
guarantee for stocBiO, and show that stocBiO outperforms the best known
computational complexities orderwisely with respect to the condition number
$\kappa$ and the target accuracy $\epsilon$. We further validate our
theoretical results and demonstrate the efficiency of bilevel optimization
algorithms by the experiments on meta-learning and hyperparameter optimization.
Our code for stocBiO and its comparison to other algorithms is available
online$^1$.
</p>
<a href="http://arxiv.org/abs/2010.07962" target="_blank">arXiv:2010.07962</a> [<a href="http://arxiv.org/pdf/2010.07962" target="_blank">pdf</a>]

<h2>Variational Dynamic Mixtures. (arXiv:2010.10403v2 [cs.LG] UPDATED)</h2>
<h3>Chen Qiu, Stephan Mandt, Maja Rudolph</h3>
<p>Deep probabilistic time series forecasting models have become an integral
part of machine learning. While several powerful generative models have been
proposed, we provide evidence that their associated inference models are
oftentimes too limited and cause the generative model to predict mode-averaged
dynamics. Modeaveraging is problematic since many real-world sequences are
highly multi-modal, and their averaged dynamics are unphysical (e.g., predicted
taxi trajectories might run through buildings on the street map). To better
capture multi-modality, we develop variational dynamic mixtures (VDM): a new
variational family to infer sequential latent variables. The VDM approximate
posterior at each time step is a mixture density network, whose parameters come
from propagating multiple samples through a recurrent architecture. This
results in an expressive multi-modal posterior approximation. In an empirical
study, we show that VDM outperforms competing approaches on highly multi-modal
datasets from different domains.
</p>
<a href="http://arxiv.org/abs/2010.10403" target="_blank">arXiv:2010.10403</a> [<a href="http://arxiv.org/pdf/2010.10403" target="_blank">pdf</a>]

<h2>Explanation Generation for Anomaly Detection Models Applied to the Fuel Consumption of a Vehicle Fleet. (arXiv:2010.16051v4 [cs.LG] UPDATED)</h2>
<h3>Alberto Barbado, &#xd3;scar Corcho</h3>
<p>In this paper we show a complete process for unsupervised anomaly detection
for the fuel consumption of a vehicle fleet, that is able to explain which
variables affect the consumption in terms of feature relevance. We combine
anomaly detection with a surrogate model that is able to provide that feature
relevance. For these surrogate models, we evaluate both whitebox ones from the
literature, as well as novel variations over them, and blackbox models combined
with local posthoc feature relevance techniques.

The evaluation is done using real IoT data, and is measured both in terms of
model performance, as well as using Explainable AI metrics that compare the
explanations generated in terms representativeness, fidelity, stability and
contrastiveness. This provides a complete evaluation both in terms of
predictive power and XAI.

The explanations generate counterfactual recommendations that show what could
have been done to reduce the fuel consumption of a vehicle and turn it into an
inlier. The procedure is combined with domain knowledge expressed in business
rules, and is able to adequate the type of explanations depending on the target
user profile.
</p>
<a href="http://arxiv.org/abs/2010.16051" target="_blank">arXiv:2010.16051</a> [<a href="http://arxiv.org/pdf/2010.16051" target="_blank">pdf</a>]

<h2>Digital Twins: State of the Art Theory and Practice, Challenges, and Open Research Questions. (arXiv:2011.02833v3 [cs.LG] UPDATED)</h2>
<h3>Angira Sharma, Edward Kosasih, Jie Zhang, Alexandra Brintrup, Anisoara Calinescu</h3>
<p>Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.
</p>
<a href="http://arxiv.org/abs/2011.02833" target="_blank">arXiv:2011.02833</a> [<a href="http://arxiv.org/pdf/2011.02833" target="_blank">pdf</a>]

<h2>Towards Optimal Problem Dependent Generalization Error Bounds in Statistical Learning Theory. (arXiv:2011.06186v3 [stat.ML] UPDATED)</h2>
<h3>Yunbei Xu, Assaf Zeevi</h3>
<p>We study problem-dependent rates, i.e., generalization errors that scale
near-optimally with the variance, the effective loss, or the gradient norms
evaluated at the "best hypothesis." We introduce a principled framework dubbed
"uniform localized convergence," and characterize sharp problem-dependent rates
for central statistical learning problems. From a methodological viewpoint, our
framework resolves several fundamental limitations of existing uniform
convergence and localization analysis approaches. It also provides improvements
and some level of unification in the study of localized complexities, one-sided
uniform inequalities, and sample-based iterative algorithms. In the so-called
"slow rate" regime, we provides the first (moment-penalized) estimator that
achieves the optimal variance-dependent rate for general "rich" classes; we
also establish improved loss-dependent rate for standard empirical risk
minimization. In the "fast rate" regime, we establish finite-sample
problem-dependent bounds that are comparable to precise asymptotics. In
addition, we show that efficient algorithms like gradient descent and
first-order Expectation-Maximization can achieve optimal generalization error
in several representative problems across the areas of non-convex learning,
stochastic optimization, and learning with missing data.
</p>
<a href="http://arxiv.org/abs/2011.06186" target="_blank">arXiv:2011.06186</a> [<a href="http://arxiv.org/pdf/2011.06186" target="_blank">pdf</a>]

<h2>LULC classification by semantic segmentation of satellite images using FastFCN. (arXiv:2011.06825v2 [cs.CV] UPDATED)</h2>
<h3>Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, Amreen Anbar, A. K. M. Nazrul Islam, A. K. M. Mahbubur Rahman</h3>
<p>This paper analyses how well a Fast Fully Convolutional Network (FastFCN)
semantically segments satellite images and thus classifies Land Use/Land
Cover(LULC) classes. Fast-FCN was used on Gaofen-2 Image Dataset (GID-2) to
segment them in five different classes: BuiltUp, Meadow, Farmland, Water and
Forest. The results showed better accuracy (0.93), precision (0.99), recall
(0.98) and mean Intersection over Union (mIoU)(0.97) than other approaches like
using FCN-8 or eCognition, a readily available software. We presented a
comparison between the results. We propose FastFCN to be both faster and more
accurate automated method than other existing methods for LULC classification.
</p>
<a href="http://arxiv.org/abs/2011.06825" target="_blank">arXiv:2011.06825</a> [<a href="http://arxiv.org/pdf/2011.06825" target="_blank">pdf</a>]

<h2>Graph-Based Neural Network Models with Multiple Self-Supervised Auxiliary Tasks. (arXiv:2011.07267v2 [cs.LG] UPDATED)</h2>
<h3>Franco Manessi, Alessandro Rozza</h3>
<p>Self-supervised learning is currently gaining a lot of attention, as it
allows neural networks to learn robust representations from large quantities of
unlabeled data. Additionally, multi-task learning can further improve
representation learning by training networks simultaneously on related tasks,
leading to significant performance improvements. In this paper, we propose
three novel self-supervised auxiliary tasks to train graph-based neural network
models in a multi-task fashion. Since Graph Convolutional Networks are among
the most promising approaches for capturing relationships among structured data
points, we use them as a building block to achieve competitive results on
standard semi-supervised graph classification tasks.
</p>
<a href="http://arxiv.org/abs/2011.07267" target="_blank">arXiv:2011.07267</a> [<a href="http://arxiv.org/pdf/2011.07267" target="_blank">pdf</a>]

<h2>A Follow-the-Leader Strategy using Hierarchical Deep Neural Networks with Grouped Convolutions. (arXiv:2011.07948v2 [cs.CV] UPDATED)</h2>
<h3>Jose Solomon, Francois Charette</h3>
<p>The task of following-the-leader is implemented using a hierarchical Deep
Neural Network (DNN) end-to-end driving model to match the direction and speed
of a target pedestrian. The model uses a classifier DNN to determine if the
pedestrian is within the field of view of the camera sensor. If the pedestrian
is present, the image stream from the camera is fed to a regression DNN which
simultaneously adjusts the autonomous vehicle's steering and throttle to keep
cadence with the pedestrian. If the pedestrian is not visible, the vehicle uses
a straightforward exploratory search strategy to reacquire the tracking
objective. The classifier and regression DNNs incorporate grouped convolutions
to boost model performance as well as to significantly reduce parameter count
and compute latency. The models are trained on the Intelligence Processing Unit
(IPU) to leverage its fine-grain compute capabilities in order to minimize
time-to-train. The results indicate very robust tracking behavior on the part
of the autonomous vehicle in terms of its steering and throttle profiles, while
requiring minimal data collection to produce. The throughput in terms of
processing training samples has been boosted by the use of the IPU in
conjunction with grouped convolutions by a factor ~3.5 for training of the
classifier and a factor of ~7 for the regression network. A recording of the
vehicle tracking a pedestrian has been produced and is available on the web.
</p>
<a href="http://arxiv.org/abs/2011.07948" target="_blank">arXiv:2011.07948</a> [<a href="http://arxiv.org/pdf/2011.07948" target="_blank">pdf</a>]

<h2>Feature Sharing and Integration for Cooperative Cognition and Perception with Volumetric Sensors. (arXiv:2011.08317v3 [cs.CV] UPDATED)</h2>
<h3>Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser P.Fallah, Rui Guo, Hongsheng Lu</h3>
<p>The recent advancement in computational and communication systems has led to
the introduction of high-performing neural networks and high-speed wireless
vehicular communication networks. As a result, new technologies such as
cooperative perception and cognition have emerged, addressing the inherent
limitations of sensory devices by providing solutions for the detection of
partially occluded targets and expanding the sensing range. However, designing
a reliable cooperative cognition or perception system requires addressing the
challenges caused by limited network resources and discrepancies between the
data shared by different sources. In this paper, we examine the requirements,
limitations, and performance of different cooperative perception techniques,
and present an in-depth analysis of the notion of Deep Feature Sharing (DFS).
We explore different cooperative object detection designs and evaluate their
performance in terms of average precision. We use the Volony dataset for our
experimental study. The results confirm that the DFS methods are significantly
less sensitive to the localization error caused by GPS noise. Furthermore, the
results attest that detection gain of DFS methods caused by adding more
cooperative participants in the scenes is comparable to raw information sharing
technique while DFS enables flexibility in design toward satisfying
communication requirements.
</p>
<a href="http://arxiv.org/abs/2011.08317" target="_blank">arXiv:2011.08317</a> [<a href="http://arxiv.org/pdf/2011.08317" target="_blank">pdf</a>]

<h2>FROST: Faster and more Robust One-shot Semi-supervised Training. (arXiv:2011.09471v4 [cs.LG] UPDATED)</h2>
<h3>Helena E. Liu, Leslie N. Smith</h3>
<p>Recent advances in one-shot semi-supervised learning have lowered the barrier
for deep learning of new applications. However, the state-of-the-art for
semi-supervised learning is slow to train and the performance is sensitive to
the choices of the labeled data and hyper-parameter values. In this paper, we
present a one-shot semi-supervised learning method that trains up to an order
of magnitude faster and is more robust than state-of-the-art methods.
Specifically, we show that by combining semi-supervised learning with a
one-stage, single network version of self-training, our FROST methodology
trains faster and is more robust to choices for the labeled samples and changes
in hyper-parameters. Our experiments demonstrate FROST's capability to perform
well when the composition of the unlabeled data is unknown; that is when the
unlabeled data contain unequal numbers of each class and can contain
out-of-distribution examples that don't belong to any of the training classes.
High performance, speed of training, and insensitivity to hyper-parameters make
FROST the most practical method for one-shot semi-supervised training. Our code
is available at https://github.com/HelenaELiu/FROST.
</p>
<a href="http://arxiv.org/abs/2011.09471" target="_blank">arXiv:2011.09471</a> [<a href="http://arxiv.org/pdf/2011.09471" target="_blank">pdf</a>]

<h2>Categorical exploratory data analysis on goodness-of-fit issues. (arXiv:2011.09682v2 [stat.ML] UPDATED)</h2>
<h3>Sabrina Enriquez, Fushing Hsieh</h3>
<p>If the aphorism "All models are wrong"- George Box, continues to be true in
data analysis, particularly when analyzing real-world data, then we should
annotate this wisdom with visible and explainable data-driven patterns. Such
annotations can critically shed invaluable light on validity as well as
limitations of statistical modeling as a data analysis approach. In an effort
to avoid holding our real data to potentially unattainable or even unrealistic
theoretical structures, we propose to utilize the data analysis paradigm called
Categorical Exploratory Data Analysis (CEDA). We illustrate the merits of this
proposal with two real-world data sets from the perspective of goodness-of-fit.
In both data sets, the Normal distribution's bell shape seemingly fits rather
well by first glance. We apply CEDA to bring out where and how each data fits
or deviates from the model shape via several important distributional aspects.
We also demonstrate that CEDA affords a version of tree-based p-value, and
compare it with p-values based on traditional statistical approaches. Along our
data analysis, we invest computational efforts in making graphic display to
illuminate the advantages of using CEDA as one primary way of data analysis in
Data Science education.
</p>
<a href="http://arxiv.org/abs/2011.09682" target="_blank">arXiv:2011.09682</a> [<a href="http://arxiv.org/pdf/2011.09682" target="_blank">pdf</a>]

<h2>Social Determinants of Recidivism: A Machine Learning Solution. (arXiv:2011.11483v2 [cs.LG] UPDATED)</h2>
<h3>Vik Shirvaikar, Choudur Lakshminarayan</h3>
<p>Current literature in criminal justice analytics often focuses on predicting
the likelihood of recidivism (repeat offenses committed by released
defendants), but this problem is fraught with ethical missteps ranging from
selection bias in data collection to model interpretability. This paper
re-purposes Machine Learning (ML) in criminal justice to identify social
determinants of recidivism, with contributions along three dimensions. (1) We
shift the focus from predicting which individuals will re-offend to identifying
the broader underlying factors that explain differences in recidivism, with the
goal of providing a reliable framework for preventative policy intervention.
(2) Recidivism models typically agglomerate all individuals into one dataset to
carry out ML tasks. We instead apply unsupervised learning to reduce noise and
extract homogeneous subgroups of individuals, with a novel heuristic to find
the optimal number of subgroups. (3) We subsequently apply supervised learning
within the subgroups to determine statistically significant features that are
correlated to recidivism. It is our view that this new approach to a
long-standing question will serve as a useful guide for the practical
application of ML in policymaking.
</p>
<a href="http://arxiv.org/abs/2011.11483" target="_blank">arXiv:2011.11483</a> [<a href="http://arxiv.org/pdf/2011.11483" target="_blank">pdf</a>]

<h2>Truly shift-invariant convolutional neural networks. (arXiv:2011.14214v3 [cs.CV] UPDATED)</h2>
<h3>Anadi Chaman (1), Ivan Dokmani&#x107; (2) ((1) University of Illinois at Urbana-Champaign, (2) University of Basel)</h3>
<p>Thanks to the use of convolution and pooling layers, convolutional neural
networks were for a long time thought to be shift-invariant. However, recent
works have shown that the output of a CNN can change significantly with small
shifts in input: a problem caused by the presence of downsampling (stride)
layers. The existing solutions rely either on data augmentation or on
anti-aliasing, both of which have limitations and neither of which enables
perfect shift invariance. Additionally, the gains obtained from these methods
do not extend to image patterns not seen during training. To address these
challenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling
scheme that allows convolutional neural networks to achieve 100% consistency in
classification performance under shifts, without any loss in accuracy. With APS
the networks exhibit perfect consistency to shifts even before training, making
it the first approach that makes convolutional neural networks truly shift
invariant.
</p>
<a href="http://arxiv.org/abs/2011.14214" target="_blank">arXiv:2011.14214</a> [<a href="http://arxiv.org/pdf/2011.14214" target="_blank">pdf</a>]

<h2>End-to-End Video Instance Segmentation with Transformers. (arXiv:2011.14503v2 [cs.CV] UPDATED)</h2>
<h3>Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia</h3>
<p>Video instance segmentation (VIS) is the task that requires simultaneously
classifying, segmenting and tracking object instances of interest in video.
Recent methods typically develop sophisticated pipelines to tackle this task.
Here, we propose a new video instance segmentation framework built upon
Transformers, termed VisTR, which views the VIS task as a direct end-to-end
parallel sequence decoding/prediction problem. Given a video clip consisting of
multiple image frames as input, VisTR outputs the sequence of masks for each
instance in the video in order directly. At the core is a new, effective
instance sequence matching and segmentation strategy, which supervises and
segments instances at the sequence level as a whole. VisTR frames the instance
segmentation and tracking in the same perspective of similarity learning, thus
considerably simplifying the overall pipeline and is significantly different
from existing approaches. Without bells and whistles, VisTR achieves the
highest speed among all existing VIS models, and achieves the best result among
methods using single model on the YouTube-VIS dataset. For the first time, we
demonstrate a much simpler and faster video instance segmentation framework
built upon Transformers, achieving competitive accuracy. We hope that VisTR can
motivate future research for more video understanding tasks.
</p>
<a href="http://arxiv.org/abs/2011.14503" target="_blank">arXiv:2011.14503</a> [<a href="http://arxiv.org/pdf/2011.14503" target="_blank">pdf</a>]

<h2>Task Allocation for Asynchronous Mobile Edge Learning with Delay and Energy Constraints. (arXiv:2012.00143v2 [cs.LG] UPDATED)</h2>
<h3>Umair Mohammad, Sameh Sorour, Mohamed Hefeida</h3>
<p>This paper extends the paradigm of "mobile edge learning (MEL)" by designing
an optimal task allocation scheme for training a machine learning model in an
asynchronous manner across mutiple edge nodes or learners connected via a
resource-constrained wireless edge network. The optimization is done such that
the portion of the task allotted to each learner is completed within a given
global delay constraint and a local maximum energy consumption limit. The time
and energy consumed are related directly to the heterogeneous communication and
computational capabilities of the learners; i.e. the proposed model is
heterogeneity aware (HA). Because the resulting optimization is an NP-hard
quadratically-constrained integer linear program (QCILP), a two-step
suggest-and-improve (SAI) solution is proposed based on using the solution of
the relaxed synchronous problem to obtain the solution to the asynchronous
problem. The proposed HA asynchronous (HA-Asyn) approach is compared against
the HA synchronous (HA-Sync) scheme and the heterogeneity unaware (HU) equal
batch allocation scheme. Results from a system of 20 learners tested for
various completion time and energy consumption constraints show that the
proposed HA-Asyn method works better than the HU synchronous/asynchronous
(HU-Sync/Asyn) approach and can provide gains of up-to 25\% compared to the
HA-Sync scheme.
</p>
<a href="http://arxiv.org/abs/2012.00143" target="_blank">arXiv:2012.00143</a> [<a href="http://arxiv.org/pdf/2012.00143" target="_blank">pdf</a>]

<h2>Fast and Robust Bin-picking System for Densely Piled Industrial Objects. (arXiv:2012.00316v2 [cs.RO] UPDATED)</h2>
<h3>Jiaxin Guo, Lian Fu, Mingkai Jia, Kaijun Wang, Shan Liu</h3>
<p>Objects grasping, also known as the bin-picking, is one of the most common
tasks faced by industrial robots. While much work has been done in related
topics, grasping randomly piled objects still remains a challenge because much
of the existing work either lack robustness or costs too much resource. In this
paper, we develop a fast and robust bin-picking system for grasping densely
piled objects adaptively and safely. The proposed system starts with point
cloud segmentation using improved density-based spatial clustering of
application with noise (DBSCAN) algorithm, which is improved by combining the
region growing algorithm and using Octree to speed up the calculation. The
system then uses principle component analysis (PCA) for coarse registration and
iterative closest point (ICP) for fine registration. We propose a grasp risk
score (GRS) to evaluate each object by the collision probability, the stability
of the object, and the whole pile's stability. Through real tests with the Anno
robot, our method is verified to be advanced in speed and robustness.
</p>
<a href="http://arxiv.org/abs/2012.00316" target="_blank">arXiv:2012.00316</a> [<a href="http://arxiv.org/pdf/2012.00316" target="_blank">pdf</a>]

<h2>Learning Spatial Attention for Face Super-Resolution. (arXiv:2012.01211v2 [cs.CV] UPDATED)</h2>
<h3>Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, Kwan-Yee K. Wong</h3>
<p>General image super-resolution techniques have difficulties in recovering
detailed face structures when applying to low resolution face images. Recent
deep learning based methods tailored for face images have achieved improved
performance by jointly trained with additional task such as face parsing and
landmark prediction. However, multi-task learning requires extra manually
labeled data. Besides, most of the existing works can only generate relatively
low resolution face images (e.g., $128\times128$), and their applications are
therefore limited. In this paper, we introduce a novel SPatial Attention
Residual Network (SPARNet) built on our newly proposed Face Attention Units
(FAUs) for face super-resolution. Specifically, we introduce a spatial
attention mechanism to the vanilla residual blocks. This enables the
convolutional layers to adaptively bootstrap features related to the key face
structures and pay less attention to those less feature-rich regions. This
makes the training more effective and efficient as the key face structures only
account for a very small portion of the face image. Visualization of the
attention maps shows that our spatial attention network can capture the key
face structures well even for very low resolution faces (e.g., $16\times16$).
Quantitative comparisons on various kinds of metrics (including PSNR, SSIM,
identity similarity, and landmark detection) demonstrate the superiority of our
method over current state-of-the-arts. We further extend SPARNet with
multi-scale discriminators, named as SPARNetHD, to produce high resolution
results (i.e., $512\times512$). We show that SPARNetHD trained with synthetic
data cannot only produce high quality and high resolution outputs for
synthetically degraded face images, but also show good generalization ability
to real world low quality face images.
</p>
<a href="http://arxiv.org/abs/2012.01211" target="_blank">arXiv:2012.01211</a> [<a href="http://arxiv.org/pdf/2012.01211" target="_blank">pdf</a>]

<h2>Siamese Basis Function Networks for Defect Classification. (arXiv:2012.01338v2 [cs.CV] UPDATED)</h2>
<h3>Tobias Schlagenhauf, Faruk Yildirim, Benedikt Br&#xfc;ckner, J&#xfc;rgen Fleischer</h3>
<p>Defect classification on metallic surfaces is considered a critical issue
since substantial quantities of steel and other metals are processed by the
manufacturing industry on a daily basis. The authors propose a new approach
where they introduce the usage of so called Siamese Kernels in a Basis Function
Network to create the Siamese Basis Function Network (SBF-Network). The
underlying idea is to classify by comparison using similarity scores. This
classification is reinforced through efficient deep learning based feature
extraction methods. First, a center image is assigned to each Siamese Kernel.
The Kernels are then trained to generate encodings in a way that enables them
to distinguish their center from other images in the dataset. Using this
approach the authors created some kind of class-awareness inside the Siamese
Kernels. To classify a given image, each Siamese Kernel generates a feature
vector for its center as well as the given image. These vectors represent
encodings of the respective images in a lower-dimensional space. The distance
between each pair of encodings is then computed using the cosine distance
together with radial basis functions. The distances are fed into a multilayer
neural network to perform the classification. With this approach the authors
achieved outstanding results on the state of the art NEU surface defect
dataset.
</p>
<a href="http://arxiv.org/abs/2012.01338" target="_blank">arXiv:2012.01338</a> [<a href="http://arxiv.org/pdf/2012.01338" target="_blank">pdf</a>]

<h2>Fair Attribute Classification through Latent Space De-biasing. (arXiv:2012.01469v2 [cs.CV] UPDATED)</h2>
<h3>Vikram V. Ramaswamy, Sunnie S. Y. Kim, Olga Russakovsky</h3>
<p>Fairness in visual recognition is becoming a prominent and critical topic of
discussion as recognition systems are deployed at scale in the real world.
Models trained from data in which target labels are correlated with protected
attributes (e.g., gender, race) are known to learn and exploit those
correlations. In this work, we introduce a method for training accurate target
classifiers while mitigating biases that stem from these correlations. We use
GANs to generate realistic-looking images, and perturb these images in the
underlying latent space to generate training data that is balanced for each
protected attribute. We augment the original dataset with this perturbed
generated data, and empirically demonstrate that target classifiers trained on
the augmented dataset exhibit a number of both quantitative and qualitative
benefits. We conduct a thorough evaluation across multiple target labels and
protected attributes in the CelebA dataset, and provide an in-depth analysis
and comparison to existing literature in the space.
</p>
<a href="http://arxiv.org/abs/2012.01469" target="_blank">arXiv:2012.01469</a> [<a href="http://arxiv.org/pdf/2012.01469" target="_blank">pdf</a>]

<h2>Single-shot Path Integrated Panoptic Segmentation. (arXiv:2012.01632v2 [cs.CV] UPDATED)</h2>
<h3>Sukjun Hwang, Seoung Wug Oh, Seon Joo Kim</h3>
<p>Panoptic segmentation, which is a novel task of unifying instance
segmentation and semantic segmentation, has attracted a lot of attention
lately. However, most of the previous methods are composed of multiple pathways
with each pathway specialized to a designated segmentation task. In this paper,
we propose to resolve panoptic segmentation in single-shot by integrating the
execution flows. With the integrated pathway, a unified feature map called
Panoptic-Feature is generated, which includes the information of both things
and stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems
that guide to cluster pixels that belong to the same instance and differentiate
between objects of different classes. A collection of convolutional filters,
where each filter represents either a thing or stuff, is applied to
Panoptic-Feature at once, materializing the single-shot panoptic segmentation.
Taking the advantages of both top-down and bottom-up approaches, our method,
named SPINet, enjoys high efficiency and accuracy on major panoptic
segmentation benchmarks: COCO and Cityscapes.
</p>
<a href="http://arxiv.org/abs/2012.01632" target="_blank">arXiv:2012.01632</a> [<a href="http://arxiv.org/pdf/2012.01632" target="_blank">pdf</a>]

<h2>Motion-based Camera Localization System in Colonoscopy Videos. (arXiv:2012.01690v2 [cs.CV] UPDATED)</h2>
<h3>Heming Yao, Ryan W. Stidham, Jonathan Gryak, Kayvan Najarian</h3>
<p>Optical colonoscopy is an essential diagnostic and prognostic tool for many
gastrointestinal diseases including cancer screening and staging, intestinal
bleeding, diarrhea, abdominal symptom evaluation, and inflammatory bowel
disease assessment. However, the evaluation, classification, and quantification
of findings on colonoscopy are subject to inter-observer variation. Automated
assessment of colonoscopy is of interest considering the subjectivity present
in qualitative human interpretations of colonoscopy findings. Localization of
the camera is an essential element to consider when inferring the meaning and
context of findings for diseases evaluated by colonoscopy. In this study, we
proposed a camera localization system to estimate the approximate anatomic
location of the camera and classify the anatomical colon segment the camera is
in. The camera localization system starts with non-informative frame detection
to remove frames without camera motion information. Then a self-training
end-to-end convolutional neural network was built to estimate the camera
motion. With the estimated camera motion, the camera trajectory can be derived,
and the location index can be calculated. Based on the estimated location
index, anatomical colon segment classification was performed by building the
colon template. The algorithm was trained and validated using colonoscopy
videos collected from routine clinical practice. From our results, the average
accuracy of the classification is 0.759, which is substantially higher than the
performance of using the location index built from other methods.
</p>
<a href="http://arxiv.org/abs/2012.01690" target="_blank">arXiv:2012.01690</a> [<a href="http://arxiv.org/pdf/2012.01690" target="_blank">pdf</a>]

<h2>Dual Refinement Feature Pyramid Networks for Object Detection. (arXiv:2012.01733v2 [cs.CV] UPDATED)</h2>
<h3>Jialiang Ma, Bin Chen</h3>
<p>FPN is a common component used in object detectors, it supplements
multi-scale information by adjacent level features interpolation and summation.
However, due to the existence of nonlinear operations and the convolutional
layers with different output dimensions, the relationship between different
levels is much more complex, the pixel-wise summation is not an efficient
approach. In this paper, we first analyze the design defects from pixel level
and feature map level. Then, we design a novel parameter-free feature pyramid
networks named Dual Refinement Feature Pyramid Networks (DRFPN) for the
problems. Specifically, DRFPN consists of two modules: Spatial Refinement Block
(SRB) and Channel Refinement Block (CRB). SRB learns the location and content
of sampling points based on contextual information between adjacent levels. CRB
learns an adaptive channel merging method based on attention mechanism. Our
proposed DRFPN can be easily plugged into existing FPN-based models. Without
bells and whistles, for two-stage detectors, our model outperforms different
FPN-based counterparts by 1.6 to 2.2 AP on the COCO detection benchmark, and
1.5 to 1.9 AP on the COCO segmentation benchmark. For one-stage detectors,
DRFPN improves anchor-based RetinaNet by 1.9 AP and anchor-free FCOS by 1.3 AP
when using ResNet50 as backbone. Extensive experiments verifies the robustness
and generalization ability of DRFPN. The code will be made publicly available.
</p>
<a href="http://arxiv.org/abs/2012.01733" target="_blank">arXiv:2012.01733</a> [<a href="http://arxiv.org/pdf/2012.01733" target="_blank">pdf</a>]

<h2>Temporal Pyramid Network for Pedestrian Trajectory Prediction with Multi-Supervision. (arXiv:2012.01884v2 [cs.CV] UPDATED)</h2>
<h3>Rongqin Liang, Yuanman Li, Xia Li, yi tang, Jiantao Zhou, Wenbin Zou</h3>
<p>Predicting human motion behavior in a crowd is important for many
applications, ranging from the natural navigation of autonomous vehicles to
intelligent security systems of video surveillance. All the previous works
model and predict the trajectory with a single resolution, which is rather
inefficient and difficult to simultaneously exploit the long-range information
(e.g., the destination of the trajectory), and the short-range information
(e.g., the walking direction and speed at a certain time) of the motion
behavior. In this paper, we propose a temporal pyramid network for pedestrian
trajectory prediction through a squeeze modulation and a dilation modulation.
Our hierarchical framework builds a feature pyramid with increasingly richer
temporal information from top to bottom, which can better capture the motion
behavior at various tempos. Furthermore, we propose a coarse-to-fine fusion
strategy with multi-supervision. By progressively merging the top coarse
features of global context to the bottom fine features of rich local context,
our method can fully exploit both the long-range and short-range information of
the trajectory. Experimental results on several benchmarks demonstrate the
superiority of our method.
</p>
<a href="http://arxiv.org/abs/2012.01884" target="_blank">arXiv:2012.01884</a> [<a href="http://arxiv.org/pdf/2012.01884" target="_blank">pdf</a>]

<h2>Patch2Pix: Epipolar-Guided Pixel-Level Correspondences. (arXiv:2012.01909v2 [cs.CV] UPDATED)</h2>
<h3>Qunjie Zhou, Torsten Sattler, Laura Leal-Taixe</h3>
<p>Deep learning has been applied to a classical matching pipeline which
typically involves three steps: (i) local feature detection and description,
(ii) feature matching, and (iii) outlier rejection. Recently emerged
correspondence networks propose to perform those steps inside a single network
but suffer from low matching resolution due to the memory bottleneck. In this
work, we propose a new perspective to estimate correspondences in a
detect-to-refine manner, where we first predict patch-level match proposals and
then refine them. We present a novel refinement network Patch2Pix that refines
match proposals by regressing pixel-level matches from the local regions
defined by those proposals and jointly rejecting outlier matches with
confidence scores, which is weakly supervised to learn correspondences that are
consistent with the epipolar geometry of an input image pair. We show that our
refinement network significantly improves the performance of correspondence
networks on image matching, homography estimation, and localization tasks. In
addition, we show that our learned refinement generalizes to fully-supervised
methods without re-training, which leads us to state-of-the-art localization
performance.
</p>
<a href="http://arxiv.org/abs/2012.01909" target="_blank">arXiv:2012.01909</a> [<a href="http://arxiv.org/pdf/2012.01909" target="_blank">pdf</a>]

<h2>Accelerating Probabilistic Volumetric Mapping using Ray-Tracing Graphics Hardware. (arXiv:2011.10348v2 [cs.RO] CROSS LISTED)</h2>
<h3>Heajung Min, Kyung Min Han, Young J. Kim</h3>
<p>Probabilistic volumetric mapping (PVM) represents a 3D environmental map for
an autonomous robotic navigational task. A popular implementation such as
Octomap is widely used in the robotics community for such a purpose. The
Octomap relies on octree to represent a PVM and its main bottleneck lies in
massive ray-shooting to determine the occupancy of the underlying volumetric
voxel grids. In this paper, we propose GPU-based ray shooting to drastically
improve the ray shooting performance in Octomap. Our main idea is based on the
use of recent ray-tracing RTX GPU, mainly designed for real-time
photo-realistic computer graphics and the accompanying graphics API, known as
DXR. Our ray-shooting first maps leaf-level voxels in the given octree to a set
of axis-aligned bounding boxes (AABBs) and employ massively parallel ray
shooting on them using GPUs to find free and occupied voxels. These are fed
back into CPU to update the voxel occupancy and restructure the octree. In our
experiments, we have observed more than three-orders-of-magnitude performance
improvement in terms of ray shooting using ray-tracing RTX GPU over a
state-of-the-art Octomap CPU implementation, where the benchmarking
environments consist of more than 77K points and 25K~34K voxel grids.
</p>
<a href="http://arxiv.org/abs/2011.10348" target="_blank">arXiv:2011.10348</a> [<a href="http://arxiv.org/pdf/2011.10348" target="_blank">pdf</a>]

<h2>Robust and Private Learning of Halfspaces. (arXiv:2011.14580v1 [cs.LG] CROSS LISTED)</h2>
<h3>Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Thao Nguyen</h3>
<p>In this work, we study the trade-off between differential privacy and
adversarial robustness under L2-perturbations in the context of learning
halfspaces. We prove nearly tight bounds on the sample complexity of robust
private learning of halfspaces for a large regime of parameters. A highlight of
our results is that robust and private learning is harder than robust or
private learning alone. We complement our theoretical analysis with
experimental results on the MNIST and USPS datasets, for a learning algorithm
that is both differentially private and adversarially robust.
</p>
<a href="http://arxiv.org/abs/2011.14580" target="_blank">arXiv:2011.14580</a> [<a href="http://arxiv.org/pdf/2011.14580" target="_blank">pdf</a>]

